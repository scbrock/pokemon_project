params 0
{'model_type': 'dqn_agent', 'l1_out': 128, 'l2_out': 64, 'gamma': 0.5, 'target_model_update': 1, 'delta_clip': 0.01, 'nb_steps_warmup': 1000, 'enable_dueling_dqn__': False, 'enable_double_dqn__': True, 'dueling_type__': 'avg'}
trial 0
Training for 30000 steps ...
    65/30000: episode: 1, duration: 2.119s, episode steps:  65, steps per second:  31, episode reward: -32.270, mean reward: -0.496 [-31.974,  2.810], mean action: 11.138 [3.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   146/30000: episode: 2, duration: 1.020s, episode steps:  81, steps per second:  79, episode reward: -41.110, mean reward: -0.508 [-32.161,  2.617], mean action: 14.000 [1.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   199/30000: episode: 3, duration: 0.919s, episode steps:  53, steps per second:  58, episode reward: -44.250, mean reward: -0.835 [-32.031,  2.226], mean action: 13.226 [7.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   255/30000: episode: 4, duration: 1.057s, episode steps:  56, steps per second:  53, episode reward: 33.000, mean reward:  0.589 [-2.848, 32.370], mean action: 14.125 [7.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   306/30000: episode: 5, duration: 0.845s, episode steps:  51, steps per second:  60, episode reward: -38.090, mean reward: -0.747 [-32.267,  2.230], mean action: 15.020 [3.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   355/30000: episode: 6, duration: 0.956s, episode steps:  49, steps per second:  51, episode reward: 34.985, mean reward:  0.714 [-3.000, 32.150], mean action: 14.714 [3.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   420/30000: episode: 7, duration: 1.041s, episode steps:  65, steps per second:  62, episode reward: -38.380, mean reward: -0.590 [-32.814,  2.990], mean action: 13.215 [7.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   469/30000: episode: 8, duration: 1.271s, episode steps:  49, steps per second:  39, episode reward: -32.550, mean reward: -0.664 [-31.628,  2.323], mean action: 11.327 [3.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   528/30000: episode: 9, duration: 1.104s, episode steps:  59, steps per second:  53, episode reward: -32.440, mean reward: -0.550 [-31.982,  2.711], mean action: 13.254 [7.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   674/30000: episode: 10, duration: 2.708s, episode steps: 146, steps per second:  54, episode reward: -38.060, mean reward: -0.261 [-32.352,  2.330], mean action: 12.144 [3.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   721/30000: episode: 11, duration: 0.617s, episode steps:  47, steps per second:  76, episode reward: -32.510, mean reward: -0.692 [-31.775,  2.911], mean action: 13.213 [3.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   786/30000: episode: 12, duration: 0.763s, episode steps:  65, steps per second:  85, episode reward: -40.500, mean reward: -0.623 [-32.189,  3.000], mean action: 15.446 [6.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   876/30000: episode: 13, duration: 1.115s, episode steps:  90, steps per second:  81, episode reward: 32.875, mean reward:  0.365 [-3.000, 32.320], mean action: 15.133 [6.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   942/30000: episode: 14, duration: 1.183s, episode steps:  66, steps per second:  56, episode reward: -34.900, mean reward: -0.529 [-32.556,  3.146], mean action: 14.409 [7.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  1007/30000: episode: 15, duration: 3.113s, episode steps:  65, steps per second:  21, episode reward: -42.590, mean reward: -0.655 [-32.084,  3.000], mean action: 13.815 [7.000, 21.000],  loss: 0.014148, mae: 0.446780, mean_q: 0.859520, mean_eps: 0.000000
  1047/30000: episode: 16, duration: 0.967s, episode steps:  40, steps per second:  41, episode reward: -43.640, mean reward: -1.091 [-32.125,  2.339], mean action: 15.150 [3.000, 21.000],  loss: 0.008049, mae: 0.417002, mean_q: 0.187081, mean_eps: 0.000000
  1122/30000: episode: 17, duration: 1.752s, episode steps:  75, steps per second:  43, episode reward: 35.770, mean reward:  0.477 [-2.641, 32.140], mean action: 10.333 [1.000, 21.000],  loss: 0.009725, mae: 0.408460, mean_q: 0.132703, mean_eps: 0.000000
  1168/30000: episode: 18, duration: 1.595s, episode steps:  46, steps per second:  29, episode reward: -32.600, mean reward: -0.709 [-32.519,  3.000], mean action: 11.174 [6.000, 21.000],  loss: 0.010191, mae: 0.408854, mean_q: 0.118797, mean_eps: 0.000000
  1273/30000: episode: 19, duration: 4.662s, episode steps: 105, steps per second:  23, episode reward: 32.685, mean reward:  0.311 [-2.763, 32.050], mean action: 9.562 [1.000, 21.000],  loss: 0.008342, mae: 0.397053, mean_q: 0.094177, mean_eps: 0.000000
  1319/30000: episode: 20, duration: 1.344s, episode steps:  46, steps per second:  34, episode reward: -35.430, mean reward: -0.770 [-32.119,  3.331], mean action: 12.804 [1.000, 21.000],  loss: 0.009272, mae: 0.380281, mean_q: 0.111874, mean_eps: 0.000000
  1458/30000: episode: 21, duration: 3.816s, episode steps: 139, steps per second:  36, episode reward: 38.928, mean reward:  0.280 [-3.000, 32.083], mean action: 9.871 [0.000, 21.000],  loss: 0.009642, mae: 0.389061, mean_q: 0.089281, mean_eps: 0.000000
  1548/30000: episode: 22, duration: 2.057s, episode steps:  90, steps per second:  44, episode reward: -32.740, mean reward: -0.364 [-32.181,  3.000], mean action: 13.322 [1.000, 21.000],  loss: 0.008780, mae: 0.390287, mean_q: 0.092810, mean_eps: 0.000000
  1614/30000: episode: 23, duration: 1.433s, episode steps:  66, steps per second:  46, episode reward: 32.968, mean reward:  0.500 [-2.787, 32.090], mean action: 11.727 [1.000, 21.000],  loss: 0.008188, mae: 0.389601, mean_q: 0.085996, mean_eps: 0.000000
  1661/30000: episode: 24, duration: 1.385s, episode steps:  47, steps per second:  34, episode reward: 37.616, mean reward:  0.800 [-2.470, 31.923], mean action: 7.319 [1.000, 21.000],  loss: 0.010654, mae: 0.370079, mean_q: 0.143344, mean_eps: 0.000000
  1708/30000: episode: 25, duration: 1.467s, episode steps:  47, steps per second:  32, episode reward: 34.874, mean reward:  0.742 [-2.704, 32.118], mean action: 16.298 [6.000, 21.000],  loss: 0.009952, mae: 0.371313, mean_q: 0.139773, mean_eps: 0.000000
  1747/30000: episode: 26, duration: 0.931s, episode steps:  39, steps per second:  42, episode reward: -35.340, mean reward: -0.906 [-32.635,  2.379], mean action: 12.231 [3.000, 21.000],  loss: 0.008709, mae: 0.366537, mean_q: 0.134170, mean_eps: 0.000000
  1837/30000: episode: 27, duration: 3.440s, episode steps:  90, steps per second:  26, episode reward: -35.150, mean reward: -0.391 [-32.593,  3.000], mean action: 12.444 [1.000, 21.000],  loss: 0.009544, mae: 0.371318, mean_q: 0.113398, mean_eps: 0.000000
  1891/30000: episode: 28, duration: 1.409s, episode steps:  54, steps per second:  38, episode reward: 32.284, mean reward:  0.598 [-3.000, 32.500], mean action: 11.667 [1.000, 21.000],  loss: 0.011080, mae: 0.362729, mean_q: 0.142600, mean_eps: 0.000000
  1976/30000: episode: 29, duration: 2.533s, episode steps:  85, steps per second:  34, episode reward: 33.000, mean reward:  0.388 [-3.000, 32.470], mean action: 12.647 [1.000, 21.000],  loss: 0.008138, mae: 0.352127, mean_q: 0.124737, mean_eps: 0.000000
  2034/30000: episode: 30, duration: 2.456s, episode steps:  58, steps per second:  24, episode reward: 35.026, mean reward:  0.604 [-2.693, 32.479], mean action: 9.948 [1.000, 21.000],  loss: 0.009593, mae: 0.343497, mean_q: 0.145570, mean_eps: 0.000000
  2095/30000: episode: 31, duration: 1.461s, episode steps:  61, steps per second:  42, episode reward: -37.900, mean reward: -0.621 [-31.999,  2.240], mean action: 15.525 [1.000, 21.000],  loss: 0.009990, mae: 0.361459, mean_q: 0.126299, mean_eps: 0.000000
  2139/30000: episode: 32, duration: 1.186s, episode steps:  44, steps per second:  37, episode reward: -32.920, mean reward: -0.748 [-32.124,  2.690], mean action: 10.932 [3.000, 21.000],  loss: 0.007300, mae: 0.333767, mean_q: 0.137951, mean_eps: 0.000000
  2177/30000: episode: 33, duration: 1.525s, episode steps:  38, steps per second:  25, episode reward: 37.865, mean reward:  0.996 [-2.302, 32.024], mean action: 11.711 [1.000, 21.000],  loss: 0.010375, mae: 0.347902, mean_q: 0.150021, mean_eps: 0.000000
  2213/30000: episode: 34, duration: 1.284s, episode steps:  36, steps per second:  28, episode reward: 40.572, mean reward:  1.127 [-2.402, 32.070], mean action: 9.972 [1.000, 21.000],  loss: 0.009636, mae: 0.334896, mean_q: 0.196622, mean_eps: 0.000000
  2241/30000: episode: 35, duration: 0.934s, episode steps:  28, steps per second:  30, episode reward: 41.761, mean reward:  1.491 [-2.625, 32.290], mean action: 4.286 [1.000, 17.000],  loss: 0.009532, mae: 0.346611, mean_q: 0.140991, mean_eps: 0.000000
  2264/30000: episode: 36, duration: 0.782s, episode steps:  23, steps per second:  29, episode reward: 41.190, mean reward:  1.791 [-2.248, 32.160], mean action: 4.261 [1.000, 15.000],  loss: 0.010709, mae: 0.375778, mean_q: 0.115166, mean_eps: 0.000000
  2295/30000: episode: 37, duration: 0.789s, episode steps:  31, steps per second:  39, episode reward: 41.451, mean reward:  1.337 [-2.352, 31.768], mean action: 5.484 [1.000, 16.000],  loss: 0.008206, mae: 0.343472, mean_q: 0.171554, mean_eps: 0.000000
  2355/30000: episode: 38, duration: 1.721s, episode steps:  60, steps per second:  35, episode reward: 35.347, mean reward:  0.589 [-2.359, 32.050], mean action: 6.183 [0.000, 21.000],  loss: 0.009448, mae: 0.331289, mean_q: 0.171309, mean_eps: 0.000000
  2415/30000: episode: 39, duration: 1.206s, episode steps:  60, steps per second:  50, episode reward: -43.250, mean reward: -0.721 [-32.186,  2.154], mean action: 11.083 [1.000, 21.000],  loss: 0.009792, mae: 0.328808, mean_q: 0.163820, mean_eps: 0.000000
  2444/30000: episode: 40, duration: 0.713s, episode steps:  29, steps per second:  41, episode reward: 41.467, mean reward:  1.430 [-2.317, 32.300], mean action: 3.759 [1.000, 16.000],  loss: 0.011512, mae: 0.345053, mean_q: 0.167348, mean_eps: 0.000000
  2480/30000: episode: 41, duration: 1.289s, episode steps:  36, steps per second:  28, episode reward: -40.730, mean reward: -1.131 [-32.391,  2.530], mean action: 13.361 [1.000, 21.000],  loss: 0.009588, mae: 0.338552, mean_q: 0.156587, mean_eps: 0.000000
  2536/30000: episode: 42, duration: 1.549s, episode steps:  56, steps per second:  36, episode reward: 38.215, mean reward:  0.682 [-2.674, 32.230], mean action: 5.286 [1.000, 16.000],  loss: 0.008600, mae: 0.326402, mean_q: 0.151274, mean_eps: 0.000000
  2567/30000: episode: 43, duration: 0.745s, episode steps:  31, steps per second:  42, episode reward: 38.053, mean reward:  1.228 [-3.000, 32.474], mean action: 10.065 [1.000, 21.000],  loss: 0.010304, mae: 0.336645, mean_q: 0.158533, mean_eps: 0.000000
  2626/30000: episode: 44, duration: 1.434s, episode steps:  59, steps per second:  41, episode reward: 43.738, mean reward:  0.741 [-2.035, 32.284], mean action: 4.542 [1.000, 21.000],  loss: 0.009881, mae: 0.327332, mean_q: 0.160561, mean_eps: 0.000000
  2649/30000: episode: 45, duration: 0.514s, episode steps:  23, steps per second:  45, episode reward: 44.015, mean reward:  1.914 [-2.153, 32.180], mean action: 4.870 [1.000, 16.000],  loss: 0.010428, mae: 0.329383, mean_q: 0.172519, mean_eps: 0.000000
  2674/30000: episode: 46, duration: 0.905s, episode steps:  25, steps per second:  28, episode reward: 41.078, mean reward:  1.643 [-2.563, 32.002], mean action: 6.880 [1.000, 15.000],  loss: 0.008850, mae: 0.322360, mean_q: 0.162818, mean_eps: 0.000000
  2710/30000: episode: 47, duration: 1.064s, episode steps:  36, steps per second:  34, episode reward: 39.000, mean reward:  1.083 [-2.671, 32.280], mean action: 3.000 [1.000, 18.000],  loss: 0.011498, mae: 0.346606, mean_q: 0.174790, mean_eps: 0.000000
  2754/30000: episode: 48, duration: 1.631s, episode steps:  44, steps per second:  27, episode reward: 35.610, mean reward:  0.809 [-2.427, 32.350], mean action: 5.432 [1.000, 17.000],  loss: 0.011264, mae: 0.340902, mean_q: 0.188417, mean_eps: 0.000000
  2793/30000: episode: 49, duration: 0.863s, episode steps:  39, steps per second:  45, episode reward: 32.470, mean reward:  0.833 [-3.000, 32.151], mean action: 10.179 [3.000, 21.000],  loss: 0.010462, mae: 0.342678, mean_q: 0.173337, mean_eps: 0.000000
  2807/30000: episode: 50, duration: 0.304s, episode steps:  14, steps per second:  46, episode reward: 47.218, mean reward:  3.373 [-0.009, 32.200], mean action: 3.643 [1.000, 15.000],  loss: 0.006971, mae: 0.302517, mean_q: 0.262682, mean_eps: 0.000000
  2842/30000: episode: 51, duration: 0.928s, episode steps:  35, steps per second:  38, episode reward: 36.599, mean reward:  1.046 [-3.000, 32.040], mean action: 4.143 [1.000, 15.000],  loss: 0.011614, mae: 0.332820, mean_q: 0.252367, mean_eps: 0.000000
  2913/30000: episode: 52, duration: 1.491s, episode steps:  71, steps per second:  48, episode reward: 35.596, mean reward:  0.501 [-2.340, 29.887], mean action: 3.352 [1.000, 15.000],  loss: 0.008716, mae: 0.320390, mean_q: 0.274776, mean_eps: 0.000000
  2933/30000: episode: 53, duration: 0.447s, episode steps:  20, steps per second:  45, episode reward: 47.747, mean reward:  2.387 [ 0.000, 32.360], mean action: 3.400 [1.000, 15.000],  loss: 0.014207, mae: 0.342680, mean_q: 0.274771, mean_eps: 0.000000
  2967/30000: episode: 54, duration: 0.637s, episode steps:  34, steps per second:  53, episode reward: 41.448, mean reward:  1.219 [-2.164, 31.528], mean action: 4.912 [1.000, 15.000],  loss: 0.010781, mae: 0.323593, mean_q: 0.259913, mean_eps: 0.000000
  3016/30000: episode: 55, duration: 0.984s, episode steps:  49, steps per second:  50, episode reward: -32.970, mean reward: -0.673 [-32.970,  3.000], mean action: 4.000 [1.000, 21.000],  loss: 0.010246, mae: 0.320556, mean_q: 0.256655, mean_eps: 0.000000
  3099/30000: episode: 56, duration: 2.189s, episode steps:  83, steps per second:  38, episode reward: 32.126, mean reward:  0.387 [-3.000, 32.140], mean action: 10.855 [1.000, 21.000],  loss: 0.011172, mae: 0.326825, mean_q: 0.280581, mean_eps: 0.000000
  3130/30000: episode: 57, duration: 0.736s, episode steps:  31, steps per second:  42, episode reward: 41.135, mean reward:  1.327 [-2.546, 32.020], mean action: 1.968 [1.000, 15.000],  loss: 0.011441, mae: 0.320249, mean_q: 0.342627, mean_eps: 0.000000
  3172/30000: episode: 58, duration: 1.476s, episode steps:  42, steps per second:  28, episode reward: 37.723, mean reward:  0.898 [-2.309, 32.240], mean action: 7.452 [1.000, 16.000],  loss: 0.010782, mae: 0.317586, mean_q: 0.344985, mean_eps: 0.000000
  3216/30000: episode: 59, duration: 1.719s, episode steps:  44, steps per second:  26, episode reward: -38.060, mean reward: -0.865 [-32.089,  2.713], mean action: 8.523 [1.000, 21.000],  loss: 0.009627, mae: 0.305562, mean_q: 0.355052, mean_eps: 0.000000
  3252/30000: episode: 60, duration: 1.118s, episode steps:  36, steps per second:  32, episode reward: 35.093, mean reward:  0.975 [-3.000, 32.269], mean action: 5.194 [1.000, 21.000],  loss: 0.010754, mae: 0.316801, mean_q: 0.337653, mean_eps: 0.000000
  3282/30000: episode: 61, duration: 0.792s, episode steps:  30, steps per second:  38, episode reward: 36.000, mean reward:  1.200 [-2.240, 32.540], mean action: 5.133 [1.000, 16.000],  loss: 0.011518, mae: 0.318426, mean_q: 0.330319, mean_eps: 0.000000
  3349/30000: episode: 62, duration: 2.076s, episode steps:  67, steps per second:  32, episode reward: 32.600, mean reward:  0.487 [-2.480, 32.826], mean action: 13.761 [1.000, 21.000],  loss: 0.011754, mae: 0.330004, mean_q: 0.340906, mean_eps: 0.000000
  3425/30000: episode: 63, duration: 1.483s, episode steps:  76, steps per second:  51, episode reward: 32.782, mean reward:  0.431 [-2.171, 32.350], mean action: 12.158 [1.000, 21.000],  loss: 0.010334, mae: 0.325431, mean_q: 0.303781, mean_eps: 0.000000
  3451/30000: episode: 64, duration: 0.468s, episode steps:  26, steps per second:  56, episode reward: 38.877, mean reward:  1.495 [-3.000, 32.500], mean action: 3.615 [1.000, 16.000],  loss: 0.014041, mae: 0.332121, mean_q: 0.412222, mean_eps: 0.000000
  3506/30000: episode: 65, duration: 0.982s, episode steps:  55, steps per second:  56, episode reward: -32.710, mean reward: -0.595 [-32.302,  2.690], mean action: 8.091 [1.000, 16.000],  loss: 0.010648, mae: 0.321013, mean_q: 0.370156, mean_eps: 0.000000
  3539/30000: episode: 66, duration: 0.598s, episode steps:  33, steps per second:  55, episode reward: 40.216, mean reward:  1.219 [-2.075, 32.243], mean action: 4.121 [1.000, 16.000],  loss: 0.010253, mae: 0.327457, mean_q: 0.264700, mean_eps: 0.000000
  3565/30000: episode: 67, duration: 0.516s, episode steps:  26, steps per second:  50, episode reward: 38.292, mean reward:  1.473 [-2.546, 31.809], mean action: 4.731 [1.000, 17.000],  loss: 0.012529, mae: 0.319723, mean_q: 0.316450, mean_eps: 0.000000
  3598/30000: episode: 68, duration: 0.668s, episode steps:  33, steps per second:  49, episode reward: 41.362, mean reward:  1.253 [-3.000, 32.469], mean action: 3.727 [1.000, 12.000],  loss: 0.009998, mae: 0.324525, mean_q: 0.263602, mean_eps: 0.000000
  3657/30000: episode: 69, duration: 1.121s, episode steps:  59, steps per second:  53, episode reward: -32.140, mean reward: -0.545 [-32.624,  2.532], mean action: 5.441 [1.000, 21.000],  loss: 0.011421, mae: 0.331322, mean_q: 0.280105, mean_eps: 0.000000
  3688/30000: episode: 70, duration: 0.565s, episode steps:  31, steps per second:  55, episode reward: 41.041, mean reward:  1.324 [-2.768, 32.300], mean action: 7.452 [1.000, 21.000],  loss: 0.010775, mae: 0.337786, mean_q: 0.239571, mean_eps: 0.000000
  3709/30000: episode: 71, duration: 0.404s, episode steps:  21, steps per second:  52, episode reward: 41.523, mean reward:  1.977 [-2.048, 32.153], mean action: 3.286 [1.000, 15.000],  loss: 0.010563, mae: 0.334810, mean_q: 0.257200, mean_eps: 0.000000
  3758/30000: episode: 72, duration: 0.984s, episode steps:  49, steps per second:  50, episode reward: 41.748, mean reward:  0.852 [-2.324, 32.198], mean action: 3.571 [1.000, 15.000],  loss: 0.012736, mae: 0.328915, mean_q: 0.273926, mean_eps: 0.000000
  3781/30000: episode: 73, duration: 0.587s, episode steps:  23, steps per second:  39, episode reward: 38.135, mean reward:  1.658 [-2.416, 32.410], mean action: 3.696 [1.000, 14.000],  loss: 0.009498, mae: 0.317905, mean_q: 0.269133, mean_eps: 0.000000
  3831/30000: episode: 74, duration: 1.120s, episode steps:  50, steps per second:  45, episode reward: 32.417, mean reward:  0.648 [-2.817, 32.224], mean action: 7.380 [1.000, 21.000],  loss: 0.009608, mae: 0.321764, mean_q: 0.279678, mean_eps: 0.000000
  3870/30000: episode: 75, duration: 0.907s, episode steps:  39, steps per second:  43, episode reward: 40.615, mean reward:  1.041 [-2.894, 32.140], mean action: 2.718 [1.000, 14.000],  loss: 0.010196, mae: 0.320392, mean_q: 0.293546, mean_eps: 0.000000
  3906/30000: episode: 76, duration: 1.008s, episode steps:  36, steps per second:  36, episode reward: 32.526, mean reward:  0.903 [-3.000, 31.858], mean action: 5.944 [1.000, 18.000],  loss: 0.012375, mae: 0.331433, mean_q: 0.270427, mean_eps: 0.000000
  3938/30000: episode: 77, duration: 0.817s, episode steps:  32, steps per second:  39, episode reward: -36.000, mean reward: -1.125 [-32.128,  2.430], mean action: 5.031 [1.000, 21.000],  loss: 0.011655, mae: 0.316568, mean_q: 0.282433, mean_eps: 0.000000
  3965/30000: episode: 78, duration: 0.948s, episode steps:  27, steps per second:  28, episode reward: 45.921, mean reward:  1.701 [-0.845, 32.040], mean action: 7.222 [1.000, 21.000],  loss: 0.007874, mae: 0.305424, mean_q: 0.268466, mean_eps: 0.000000
  3983/30000: episode: 79, duration: 0.462s, episode steps:  18, steps per second:  39, episode reward: 41.806, mean reward:  2.323 [-2.621, 30.281], mean action: 5.944 [1.000, 21.000],  loss: 0.013019, mae: 0.312238, mean_q: 0.338840, mean_eps: 0.000000
  4023/30000: episode: 80, duration: 1.059s, episode steps:  40, steps per second:  38, episode reward: 37.642, mean reward:  0.941 [-3.000, 32.090], mean action: 7.350 [1.000, 16.000],  loss: 0.009315, mae: 0.309337, mean_q: 0.284621, mean_eps: 0.000000
  4092/30000: episode: 81, duration: 1.560s, episode steps:  69, steps per second:  44, episode reward: 33.000, mean reward:  0.478 [-2.514, 32.280], mean action: 6.580 [1.000, 17.000],  loss: 0.011873, mae: 0.318694, mean_q: 0.262019, mean_eps: 0.000000
  4122/30000: episode: 82, duration: 0.585s, episode steps:  30, steps per second:  51, episode reward: 42.000, mean reward:  1.400 [-3.000, 32.180], mean action: 3.533 [0.000, 14.000],  loss: 0.013326, mae: 0.325312, mean_q: 0.302362, mean_eps: 0.000000
  4168/30000: episode: 83, duration: 0.848s, episode steps:  46, steps per second:  54, episode reward: -32.620, mean reward: -0.709 [-32.506,  2.400], mean action: 7.870 [1.000, 17.000],  loss: 0.010330, mae: 0.316382, mean_q: 0.276071, mean_eps: 0.000000
  4236/30000: episode: 84, duration: 1.543s, episode steps:  68, steps per second:  44, episode reward: -32.820, mean reward: -0.483 [-32.295,  3.000], mean action: 5.853 [1.000, 17.000],  loss: 0.011709, mae: 0.314904, mean_q: 0.269539, mean_eps: 0.000000
  4264/30000: episode: 85, duration: 0.560s, episode steps:  28, steps per second:  50, episode reward: 38.490, mean reward:  1.375 [-2.480, 31.883], mean action: 6.321 [1.000, 15.000],  loss: 0.010752, mae: 0.297726, mean_q: 0.297940, mean_eps: 0.000000
  4297/30000: episode: 86, duration: 0.801s, episode steps:  33, steps per second:  41, episode reward: 38.926, mean reward:  1.180 [-2.435, 32.299], mean action: 4.545 [1.000, 15.000],  loss: 0.012483, mae: 0.314209, mean_q: 0.244326, mean_eps: 0.000000
  4348/30000: episode: 87, duration: 1.163s, episode steps:  51, steps per second:  44, episode reward: -30.000, mean reward: -0.588 [-30.056,  2.499], mean action: 5.392 [1.000, 16.000],  loss: 0.012700, mae: 0.312221, mean_q: 0.267536, mean_eps: 0.000000
  4427/30000: episode: 88, duration: 1.747s, episode steps:  79, steps per second:  45, episode reward: -32.670, mean reward: -0.414 [-33.000,  2.951], mean action: 8.101 [1.000, 17.000],  loss: 0.012359, mae: 0.315320, mean_q: 0.271320, mean_eps: 0.000000
  4451/30000: episode: 89, duration: 0.564s, episode steps:  24, steps per second:  43, episode reward: 38.556, mean reward:  1.606 [-2.429, 32.556], mean action: 2.917 [1.000, 17.000],  loss: 0.014180, mae: 0.330629, mean_q: 0.305724, mean_eps: 0.000000
  4484/30000: episode: 90, duration: 0.830s, episode steps:  33, steps per second:  40, episode reward: 38.137, mean reward:  1.156 [-2.347, 32.020], mean action: 2.970 [1.000, 21.000],  loss: 0.011209, mae: 0.329602, mean_q: 0.206182, mean_eps: 0.000000
  4525/30000: episode: 91, duration: 0.849s, episode steps:  41, steps per second:  48, episode reward: 40.886, mean reward:  0.997 [-3.000, 32.030], mean action: 3.049 [1.000, 16.000],  loss: 0.012256, mae: 0.307945, mean_q: 0.312917, mean_eps: 0.000000
  4547/30000: episode: 92, duration: 0.516s, episode steps:  22, steps per second:  43, episode reward: 38.956, mean reward:  1.771 [-2.801, 32.350], mean action: 4.545 [1.000, 15.000],  loss: 0.011063, mae: 0.311226, mean_q: 0.296541, mean_eps: 0.000000
  4570/30000: episode: 93, duration: 0.433s, episode steps:  23, steps per second:  53, episode reward: 39.000, mean reward:  1.696 [-3.000, 32.130], mean action: 3.826 [1.000, 15.000],  loss: 0.013015, mae: 0.319593, mean_q: 0.257036, mean_eps: 0.000000
  4633/30000: episode: 94, duration: 1.236s, episode steps:  63, steps per second:  51, episode reward: 38.449, mean reward:  0.610 [-2.530, 32.284], mean action: 5.429 [1.000, 15.000],  loss: 0.010940, mae: 0.300283, mean_q: 0.283741, mean_eps: 0.000000
  4672/30000: episode: 95, duration: 0.797s, episode steps:  39, steps per second:  49, episode reward: -34.440, mean reward: -0.883 [-32.223,  2.640], mean action: 6.385 [1.000, 17.000],  loss: 0.012631, mae: 0.311562, mean_q: 0.319957, mean_eps: 0.000000
  4711/30000: episode: 96, duration: 0.792s, episode steps:  39, steps per second:  49, episode reward: -32.290, mean reward: -0.828 [-32.525,  3.000], mean action: 9.205 [1.000, 17.000],  loss: 0.013414, mae: 0.327952, mean_q: 0.251677, mean_eps: 0.000000
  4738/30000: episode: 97, duration: 0.542s, episode steps:  27, steps per second:  50, episode reward: 38.538, mean reward:  1.427 [-2.535, 32.500], mean action: 5.593 [1.000, 21.000],  loss: 0.012292, mae: 0.315962, mean_q: 0.249356, mean_eps: 0.000000
  4754/30000: episode: 98, duration: 0.319s, episode steps:  16, steps per second:  50, episode reward: 41.558, mean reward:  2.597 [-2.099, 31.638], mean action: 3.000 [1.000, 15.000],  loss: 0.010621, mae: 0.302563, mean_q: 0.274056, mean_eps: 0.000000
  4780/30000: episode: 99, duration: 0.538s, episode steps:  26, steps per second:  48, episode reward: 35.691, mean reward:  1.373 [-2.259, 32.021], mean action: 4.000 [1.000, 16.000],  loss: 0.013741, mae: 0.332632, mean_q: 0.229771, mean_eps: 0.000000
  4832/30000: episode: 100, duration: 1.092s, episode steps:  52, steps per second:  48, episode reward: -33.000, mean reward: -0.635 [-32.200,  2.383], mean action: 11.173 [1.000, 21.000],  loss: 0.012361, mae: 0.313652, mean_q: 0.267310, mean_eps: 0.000000
  4876/30000: episode: 101, duration: 0.951s, episode steps:  44, steps per second:  46, episode reward: 39.000, mean reward:  0.886 [-3.000, 32.200], mean action: 2.727 [1.000, 21.000],  loss: 0.012951, mae: 0.315295, mean_q: 0.277713, mean_eps: 0.000000
  4919/30000: episode: 102, duration: 0.863s, episode steps:  43, steps per second:  50, episode reward: 32.569, mean reward:  0.757 [-3.000, 32.110], mean action: 6.233 [1.000, 21.000],  loss: 0.012540, mae: 0.326070, mean_q: 0.212495, mean_eps: 0.000000
  4943/30000: episode: 103, duration: 0.505s, episode steps:  24, steps per second:  48, episode reward: 41.259, mean reward:  1.719 [-2.906, 31.955], mean action: 3.500 [1.000, 15.000],  loss: 0.013313, mae: 0.315958, mean_q: 0.274490, mean_eps: 0.000000
  4974/30000: episode: 104, duration: 0.646s, episode steps:  31, steps per second:  48, episode reward: 33.000, mean reward:  1.065 [-2.806, 32.380], mean action: 6.258 [1.000, 15.000],  loss: 0.013522, mae: 0.310458, mean_q: 0.285975, mean_eps: 0.000000
  5011/30000: episode: 105, duration: 0.761s, episode steps:  37, steps per second:  49, episode reward: 41.054, mean reward:  1.110 [-2.319, 32.248], mean action: 2.622 [1.000, 21.000],  loss: 0.013213, mae: 0.317420, mean_q: 0.292374, mean_eps: 0.000000
  5035/30000: episode: 106, duration: 0.539s, episode steps:  24, steps per second:  45, episode reward: 44.201, mean reward:  1.842 [-2.403, 32.280], mean action: 4.167 [1.000, 20.000],  loss: 0.012458, mae: 0.311294, mean_q: 0.271355, mean_eps: 0.000000
  5059/30000: episode: 107, duration: 0.561s, episode steps:  24, steps per second:  43, episode reward: 43.324, mean reward:  1.805 [-2.054, 32.100], mean action: 4.792 [1.000, 15.000],  loss: 0.011246, mae: 0.311961, mean_q: 0.272563, mean_eps: 0.000000
  5105/30000: episode: 108, duration: 0.965s, episode steps:  46, steps per second:  48, episode reward: 35.848, mean reward:  0.779 [-2.205, 32.230], mean action: 11.370 [1.000, 21.000],  loss: 0.012987, mae: 0.313691, mean_q: 0.295431, mean_eps: 0.000000
  5150/30000: episode: 109, duration: 0.841s, episode steps:  45, steps per second:  53, episode reward: -32.410, mean reward: -0.720 [-32.581,  3.000], mean action: 4.911 [1.000, 17.000],  loss: 0.014704, mae: 0.316041, mean_q: 0.308983, mean_eps: 0.000000
  5172/30000: episode: 110, duration: 0.432s, episode steps:  22, steps per second:  51, episode reward: 42.537, mean reward:  1.934 [-2.690, 31.634], mean action: 5.500 [1.000, 21.000],  loss: 0.012778, mae: 0.297874, mean_q: 0.323995, mean_eps: 0.000000
  5220/30000: episode: 111, duration: 0.952s, episode steps:  48, steps per second:  50, episode reward: -32.250, mean reward: -0.672 [-32.860,  2.581], mean action: 3.979 [1.000, 15.000],  loss: 0.011567, mae: 0.299700, mean_q: 0.288026, mean_eps: 0.000000
  5252/30000: episode: 112, duration: 0.784s, episode steps:  32, steps per second:  41, episode reward: 41.250, mean reward:  1.289 [-2.971, 32.550], mean action: 2.594 [1.000, 21.000],  loss: 0.015410, mae: 0.326838, mean_q: 0.296876, mean_eps: 0.000000
  5282/30000: episode: 113, duration: 0.776s, episode steps:  30, steps per second:  39, episode reward: 39.000, mean reward:  1.300 [-2.631, 32.010], mean action: 4.300 [1.000, 16.000],  loss: 0.012887, mae: 0.318116, mean_q: 0.284832, mean_eps: 0.000000
  5317/30000: episode: 114, duration: 0.835s, episode steps:  35, steps per second:  42, episode reward: 38.777, mean reward:  1.108 [-2.089, 32.130], mean action: 5.514 [1.000, 21.000],  loss: 0.011466, mae: 0.301968, mean_q: 0.309137, mean_eps: 0.000000
  5363/30000: episode: 115, duration: 1.066s, episode steps:  46, steps per second:  43, episode reward: 38.559, mean reward:  0.838 [-2.218, 32.200], mean action: 6.348 [1.000, 21.000],  loss: 0.011642, mae: 0.307792, mean_q: 0.275265, mean_eps: 0.000000
  5414/30000: episode: 116, duration: 1.079s, episode steps:  51, steps per second:  47, episode reward: 37.774, mean reward:  0.741 [-3.000, 32.350], mean action: 5.451 [1.000, 21.000],  loss: 0.012330, mae: 0.297507, mean_q: 0.318658, mean_eps: 0.000000
  5445/30000: episode: 117, duration: 0.545s, episode steps:  31, steps per second:  57, episode reward: 38.700, mean reward:  1.248 [-2.548, 31.979], mean action: 5.419 [1.000, 21.000],  loss: 0.012405, mae: 0.306820, mean_q: 0.267680, mean_eps: 0.000000
  5470/30000: episode: 118, duration: 0.451s, episode steps:  25, steps per second:  55, episode reward: 47.208, mean reward:  1.888 [-0.308, 31.924], mean action: 2.200 [1.000, 8.000],  loss: 0.009891, mae: 0.288063, mean_q: 0.346135, mean_eps: 0.000000
  5497/30000: episode: 119, duration: 0.481s, episode steps:  27, steps per second:  56, episode reward: 44.547, mean reward:  1.650 [-2.155, 32.169], mean action: 3.000 [1.000, 15.000],  loss: 0.011808, mae: 0.315463, mean_q: 0.283967, mean_eps: 0.000000
  5528/30000: episode: 120, duration: 0.638s, episode steps:  31, steps per second:  49, episode reward: 43.128, mean reward:  1.391 [-2.674, 32.020], mean action: 3.129 [1.000, 17.000],  loss: 0.013524, mae: 0.315334, mean_q: 0.298564, mean_eps: 0.000000
  5551/30000: episode: 121, duration: 0.410s, episode steps:  23, steps per second:  56, episode reward: 40.675, mean reward:  1.768 [-2.368, 31.673], mean action: 4.870 [1.000, 21.000],  loss: 0.009881, mae: 0.287390, mean_q: 0.330794, mean_eps: 0.000000
  5594/30000: episode: 122, duration: 0.810s, episode steps:  43, steps per second:  53, episode reward: 35.154, mean reward:  0.818 [-2.453, 32.240], mean action: 7.698 [1.000, 16.000],  loss: 0.011141, mae: 0.288917, mean_q: 0.301458, mean_eps: 0.000000
  5625/30000: episode: 123, duration: 0.695s, episode steps:  31, steps per second:  45, episode reward: 37.345, mean reward:  1.205 [-2.362, 31.908], mean action: 5.387 [1.000, 15.000],  loss: 0.012399, mae: 0.311019, mean_q: 0.243271, mean_eps: 0.000000
  5652/30000: episode: 124, duration: 0.565s, episode steps:  27, steps per second:  48, episode reward: 41.183, mean reward:  1.525 [-3.000, 32.300], mean action: 3.074 [1.000, 10.000],  loss: 0.015308, mae: 0.315249, mean_q: 0.302734, mean_eps: 0.000000
  5671/30000: episode: 125, duration: 0.368s, episode steps:  19, steps per second:  52, episode reward: 44.667, mean reward:  2.351 [-2.004, 32.080], mean action: 1.842 [1.000, 11.000],  loss: 0.014300, mae: 0.317589, mean_q: 0.271015, mean_eps: 0.000000
  5714/30000: episode: 126, duration: 1.025s, episode steps:  43, steps per second:  42, episode reward: 33.000, mean reward:  0.767 [-2.691, 32.110], mean action: 4.558 [1.000, 16.000],  loss: 0.015273, mae: 0.313985, mean_q: 0.325649, mean_eps: 0.000000
  5740/30000: episode: 127, duration: 0.461s, episode steps:  26, steps per second:  56, episode reward: 41.413, mean reward:  1.593 [-2.464, 32.200], mean action: 4.885 [1.000, 15.000],  loss: 0.015634, mae: 0.305433, mean_q: 0.427547, mean_eps: 0.000000
  5769/30000: episode: 128, duration: 0.547s, episode steps:  29, steps per second:  53, episode reward: 35.336, mean reward:  1.218 [-3.000, 32.540], mean action: 5.759 [3.000, 16.000],  loss: 0.011313, mae: 0.304292, mean_q: 0.307057, mean_eps: 0.000000
  5803/30000: episode: 129, duration: 0.654s, episode steps:  34, steps per second:  52, episode reward: 38.117, mean reward:  1.121 [-2.805, 32.901], mean action: 7.765 [1.000, 21.000],  loss: 0.013707, mae: 0.307555, mean_q: 0.325381, mean_eps: 0.000000
  5834/30000: episode: 130, duration: 0.532s, episode steps:  31, steps per second:  58, episode reward: 38.159, mean reward:  1.231 [-2.244, 31.800], mean action: 3.290 [1.000, 14.000],  loss: 0.010379, mae: 0.294654, mean_q: 0.339120, mean_eps: 0.000000
  5879/30000: episode: 131, duration: 0.882s, episode steps:  45, steps per second:  51, episode reward: 40.008, mean reward:  0.889 [-2.121, 32.110], mean action: 2.867 [1.000, 17.000],  loss: 0.014874, mae: 0.312855, mean_q: 0.323618, mean_eps: 0.000000
  5908/30000: episode: 132, duration: 0.797s, episode steps:  29, steps per second:  36, episode reward: 41.753, mean reward:  1.440 [-2.704, 32.274], mean action: 4.448 [1.000, 21.000],  loss: 0.014644, mae: 0.301678, mean_q: 0.361247, mean_eps: 0.000000
  5939/30000: episode: 133, duration: 0.759s, episode steps:  31, steps per second:  41, episode reward: 38.806, mean reward:  1.252 [-2.467, 31.846], mean action: 3.290 [1.000, 16.000],  loss: 0.012994, mae: 0.298766, mean_q: 0.315508, mean_eps: 0.000000
  5969/30000: episode: 134, duration: 0.757s, episode steps:  30, steps per second:  40, episode reward: 41.296, mean reward:  1.377 [-2.101, 32.370], mean action: 3.467 [1.000, 15.000],  loss: 0.015961, mae: 0.316167, mean_q: 0.343889, mean_eps: 0.000000
  6002/30000: episode: 135, duration: 0.730s, episode steps:  33, steps per second:  45, episode reward: -37.850, mean reward: -1.147 [-32.360,  2.050], mean action: 7.970 [1.000, 21.000],  loss: 0.012194, mae: 0.293831, mean_q: 0.302977, mean_eps: 0.000000
  6025/30000: episode: 136, duration: 0.428s, episode steps:  23, steps per second:  54, episode reward: 44.362, mean reward:  1.929 [-2.137, 32.500], mean action: 3.609 [1.000, 16.000],  loss: 0.011697, mae: 0.286183, mean_q: 0.324545, mean_eps: 0.000000
  6068/30000: episode: 137, duration: 0.987s, episode steps:  43, steps per second:  44, episode reward: 35.567, mean reward:  0.827 [-3.000, 31.877], mean action: 3.953 [1.000, 17.000],  loss: 0.013828, mae: 0.304585, mean_q: 0.300662, mean_eps: 0.000000
  6102/30000: episode: 138, duration: 0.855s, episode steps:  34, steps per second:  40, episode reward: 38.214, mean reward:  1.124 [-2.886, 32.110], mean action: 4.765 [1.000, 17.000],  loss: 0.014130, mae: 0.304372, mean_q: 0.334007, mean_eps: 0.000000
  6128/30000: episode: 139, duration: 0.640s, episode steps:  26, steps per second:  41, episode reward: 39.000, mean reward:  1.500 [-2.492, 32.190], mean action: 4.308 [1.000, 21.000],  loss: 0.015396, mae: 0.306642, mean_q: 0.359709, mean_eps: 0.000000
  6212/30000: episode: 140, duration: 1.787s, episode steps:  84, steps per second:  47, episode reward: 40.484, mean reward:  0.482 [-2.067, 32.030], mean action: 4.274 [1.000, 21.000],  loss: 0.012392, mae: 0.291693, mean_q: 0.347145, mean_eps: 0.000000
  6251/30000: episode: 141, duration: 0.779s, episode steps:  39, steps per second:  50, episode reward: 35.046, mean reward:  0.899 [-3.000, 32.160], mean action: 3.590 [1.000, 17.000],  loss: 0.011853, mae: 0.294803, mean_q: 0.294677, mean_eps: 0.000000
  6277/30000: episode: 142, duration: 0.602s, episode steps:  26, steps per second:  43, episode reward: 39.000, mean reward:  1.500 [-2.205, 33.000], mean action: 3.462 [1.000, 15.000],  loss: 0.011430, mae: 0.293020, mean_q: 0.276144, mean_eps: 0.000000
  6313/30000: episode: 143, duration: 0.910s, episode steps:  36, steps per second:  40, episode reward: 40.999, mean reward:  1.139 [-2.618, 32.100], mean action: 9.778 [1.000, 21.000],  loss: 0.009960, mae: 0.281540, mean_q: 0.305297, mean_eps: 0.000000
  6347/30000: episode: 144, duration: 0.732s, episode steps:  34, steps per second:  46, episode reward: 39.807, mean reward:  1.171 [-2.297, 31.971], mean action: 7.353 [1.000, 15.000],  loss: 0.014299, mae: 0.306051, mean_q: 0.280963, mean_eps: 0.000000
  6372/30000: episode: 145, duration: 0.454s, episode steps:  25, steps per second:  55, episode reward: 41.532, mean reward:  1.661 [-3.000, 31.709], mean action: 5.760 [1.000, 18.000],  loss: 0.013555, mae: 0.303037, mean_q: 0.293857, mean_eps: 0.000000
  6430/30000: episode: 146, duration: 1.262s, episode steps:  58, steps per second:  46, episode reward: 32.546, mean reward:  0.561 [-2.444, 32.290], mean action: 3.776 [1.000, 21.000],  loss: 0.012245, mae: 0.291863, mean_q: 0.300504, mean_eps: 0.000000
  6454/30000: episode: 147, duration: 0.906s, episode steps:  24, steps per second:  26, episode reward: 41.801, mean reward:  1.742 [-2.220, 31.851], mean action: 2.875 [1.000, 17.000],  loss: 0.014406, mae: 0.305015, mean_q: 0.322881, mean_eps: 0.000000
  6490/30000: episode: 148, duration: 1.156s, episode steps:  36, steps per second:  31, episode reward: 40.731, mean reward:  1.131 [-2.231, 32.330], mean action: 5.556 [1.000, 17.000],  loss: 0.012479, mae: 0.296836, mean_q: 0.299222, mean_eps: 0.000000
  6517/30000: episode: 149, duration: 0.848s, episode steps:  27, steps per second:  32, episode reward: 42.000, mean reward:  1.556 [-2.317, 32.160], mean action: 3.037 [1.000, 16.000],  loss: 0.014551, mae: 0.296699, mean_q: 0.323382, mean_eps: 0.000000
  6557/30000: episode: 150, duration: 1.609s, episode steps:  40, steps per second:  25, episode reward: -35.810, mean reward: -0.895 [-32.359,  2.600], mean action: 4.500 [1.000, 17.000],  loss: 0.013512, mae: 0.300636, mean_q: 0.308660, mean_eps: 0.000000
  6585/30000: episode: 151, duration: 0.827s, episode steps:  28, steps per second:  34, episode reward: 44.471, mean reward:  1.588 [-2.611, 32.510], mean action: 6.036 [1.000, 21.000],  loss: 0.011068, mae: 0.288264, mean_q: 0.351560, mean_eps: 0.000000
  6624/30000: episode: 152, duration: 1.043s, episode steps:  39, steps per second:  37, episode reward: 37.920, mean reward:  0.972 [-2.162, 32.100], mean action: 6.026 [1.000, 18.000],  loss: 0.014018, mae: 0.300256, mean_q: 0.349247, mean_eps: 0.000000
  6653/30000: episode: 153, duration: 0.558s, episode steps:  29, steps per second:  52, episode reward: 38.979, mean reward:  1.344 [-2.430, 29.390], mean action: 5.310 [1.000, 21.000],  loss: 0.013258, mae: 0.294967, mean_q: 0.347378, mean_eps: 0.000000
  6693/30000: episode: 154, duration: 0.757s, episode steps:  40, steps per second:  53, episode reward: 32.142, mean reward:  0.804 [-2.751, 32.240], mean action: 4.700 [1.000, 16.000],  loss: 0.011363, mae: 0.284835, mean_q: 0.335718, mean_eps: 0.000000
  6721/30000: episode: 155, duration: 0.518s, episode steps:  28, steps per second:  54, episode reward: 40.622, mean reward:  1.451 [-2.783, 32.260], mean action: 4.821 [1.000, 16.000],  loss: 0.013202, mae: 0.297954, mean_q: 0.326850, mean_eps: 0.000000
  6732/30000: episode: 156, duration: 0.235s, episode steps:  11, steps per second:  47, episode reward: 47.457, mean reward:  4.314 [ 0.000, 32.510], mean action: 6.545 [3.000, 10.000],  loss: 0.009644, mae: 0.283802, mean_q: 0.351640, mean_eps: 0.000000
  6757/30000: episode: 157, duration: 0.519s, episode steps:  25, steps per second:  48, episode reward: 44.633, mean reward:  1.785 [-2.304, 32.300], mean action: 3.360 [1.000, 10.000],  loss: 0.012073, mae: 0.294014, mean_q: 0.339652, mean_eps: 0.000000
  6792/30000: episode: 158, duration: 0.812s, episode steps:  35, steps per second:  43, episode reward: -39.440, mean reward: -1.127 [-32.677,  2.160], mean action: 8.943 [1.000, 21.000],  loss: 0.015572, mae: 0.310419, mean_q: 0.342984, mean_eps: 0.000000
  6817/30000: episode: 159, duration: 0.646s, episode steps:  25, steps per second:  39, episode reward: 45.834, mean reward:  1.833 [-0.280, 31.857], mean action: 5.280 [1.000, 14.000],  loss: 0.013880, mae: 0.301234, mean_q: 0.319584, mean_eps: 0.000000
  6849/30000: episode: 160, duration: 1.020s, episode steps:  32, steps per second:  31, episode reward: 41.908, mean reward:  1.310 [-2.190, 31.998], mean action: 3.594 [1.000, 16.000],  loss: 0.015213, mae: 0.303862, mean_q: 0.353772, mean_eps: 0.000000
  6873/30000: episode: 161, duration: 0.600s, episode steps:  24, steps per second:  40, episode reward: 41.023, mean reward:  1.709 [-2.398, 32.090], mean action: 4.500 [1.000, 18.000],  loss: 0.010209, mae: 0.283732, mean_q: 0.358142, mean_eps: 0.000000
  6898/30000: episode: 162, duration: 0.547s, episode steps:  25, steps per second:  46, episode reward: 42.000, mean reward:  1.680 [-2.464, 32.160], mean action: 3.680 [1.000, 8.000],  loss: 0.014166, mae: 0.308814, mean_q: 0.319905, mean_eps: 0.000000
  6918/30000: episode: 163, duration: 0.543s, episode steps:  20, steps per second:  37, episode reward: 43.991, mean reward:  2.200 [-3.000, 32.159], mean action: 4.600 [1.000, 16.000],  loss: 0.012710, mae: 0.297670, mean_q: 0.303892, mean_eps: 0.000000
  6940/30000: episode: 164, duration: 0.578s, episode steps:  22, steps per second:  38, episode reward: 47.142, mean reward:  2.143 [-0.231, 31.604], mean action: 4.045 [1.000, 21.000],  loss: 0.011578, mae: 0.287212, mean_q: 0.322567, mean_eps: 0.000000
  6962/30000: episode: 165, duration: 0.585s, episode steps:  22, steps per second:  38, episode reward: 44.128, mean reward:  2.006 [-3.000, 32.072], mean action: 4.318 [1.000, 16.000],  loss: 0.012455, mae: 0.306604, mean_q: 0.256870, mean_eps: 0.000000
  6987/30000: episode: 166, duration: 1.126s, episode steps:  25, steps per second:  22, episode reward: 41.592, mean reward:  1.664 [-2.119, 32.150], mean action: 3.640 [1.000, 17.000],  loss: 0.016156, mae: 0.303527, mean_q: 0.319169, mean_eps: 0.000000
  7020/30000: episode: 167, duration: 0.743s, episode steps:  33, steps per second:  44, episode reward: 35.656, mean reward:  1.080 [-3.000, 32.130], mean action: 5.848 [1.000, 21.000],  loss: 0.014115, mae: 0.304155, mean_q: 0.338737, mean_eps: 0.000000
  7043/30000: episode: 168, duration: 0.435s, episode steps:  23, steps per second:  53, episode reward: 46.276, mean reward:  2.012 [-0.421, 32.014], mean action: 6.261 [1.000, 21.000],  loss: 0.011572, mae: 0.299402, mean_q: 0.302919, mean_eps: 0.000000
  7107/30000: episode: 169, duration: 1.260s, episode steps:  64, steps per second:  51, episode reward: -34.880, mean reward: -0.545 [-32.122,  3.002], mean action: 5.109 [1.000, 17.000],  loss: 0.012985, mae: 0.299952, mean_q: 0.352584, mean_eps: 0.000000
  7131/30000: episode: 170, duration: 0.992s, episode steps:  24, steps per second:  24, episode reward: 41.395, mean reward:  1.725 [-2.545, 32.350], mean action: 3.958 [1.000, 17.000],  loss: 0.009876, mae: 0.283317, mean_q: 0.303097, mean_eps: 0.000000
  7157/30000: episode: 171, duration: 0.653s, episode steps:  26, steps per second:  40, episode reward: 40.728, mean reward:  1.566 [-3.000, 32.560], mean action: 6.192 [1.000, 21.000],  loss: 0.013017, mae: 0.281207, mean_q: 0.383504, mean_eps: 0.000000
  7200/30000: episode: 172, duration: 1.115s, episode steps:  43, steps per second:  39, episode reward: 38.723, mean reward:  0.901 [-2.302, 32.570], mean action: 4.721 [1.000, 21.000],  loss: 0.011878, mae: 0.286821, mean_q: 0.358739, mean_eps: 0.000000
  7235/30000: episode: 173, duration: 1.108s, episode steps:  35, steps per second:  32, episode reward: 40.880, mean reward:  1.168 [-2.347, 32.600], mean action: 6.971 [1.000, 15.000],  loss: 0.010683, mae: 0.292813, mean_q: 0.293852, mean_eps: 0.000000
  7263/30000: episode: 174, duration: 0.940s, episode steps:  28, steps per second:  30, episode reward: 40.541, mean reward:  1.448 [-2.214, 32.030], mean action: 6.000 [1.000, 21.000],  loss: 0.013165, mae: 0.302069, mean_q: 0.329431, mean_eps: 0.000000
  7295/30000: episode: 175, duration: 1.021s, episode steps:  32, steps per second:  31, episode reward: 32.586, mean reward:  1.018 [-2.801, 32.340], mean action: 4.500 [3.000, 17.000],  loss: 0.013445, mae: 0.305210, mean_q: 0.318716, mean_eps: 0.000000
  7321/30000: episode: 176, duration: 0.666s, episode steps:  26, steps per second:  39, episode reward: 35.902, mean reward:  1.381 [-3.000, 32.172], mean action: 4.885 [1.000, 17.000],  loss: 0.014328, mae: 0.290780, mean_q: 0.384705, mean_eps: 0.000000
  7361/30000: episode: 177, duration: 0.880s, episode steps:  40, steps per second:  45, episode reward: 35.022, mean reward:  0.876 [-3.000, 31.624], mean action: 6.475 [1.000, 16.000],  loss: 0.015326, mae: 0.296338, mean_q: 0.401421, mean_eps: 0.000000
  7408/30000: episode: 178, duration: 1.219s, episode steps:  47, steps per second:  39, episode reward: 38.568, mean reward:  0.821 [-2.756, 32.190], mean action: 3.191 [1.000, 15.000],  loss: 0.011954, mae: 0.302778, mean_q: 0.304718, mean_eps: 0.000000
  7424/30000: episode: 179, duration: 0.449s, episode steps:  16, steps per second:  36, episode reward: 41.907, mean reward:  2.619 [-2.512, 32.200], mean action: 6.938 [1.000, 17.000],  loss: 0.014142, mae: 0.302439, mean_q: 0.348732, mean_eps: 0.000000
  7480/30000: episode: 180, duration: 2.103s, episode steps:  56, steps per second:  27, episode reward: 41.277, mean reward:  0.737 [-2.278, 32.210], mean action: 3.196 [1.000, 17.000],  loss: 0.014533, mae: 0.303865, mean_q: 0.349836, mean_eps: 0.000000
  7521/30000: episode: 181, duration: 1.237s, episode steps:  41, steps per second:  33, episode reward: 37.751, mean reward:  0.921 [-2.324, 32.281], mean action: 5.756 [1.000, 15.000],  loss: 0.011176, mae: 0.285008, mean_q: 0.326959, mean_eps: 0.000000
  7554/30000: episode: 182, duration: 1.663s, episode steps:  33, steps per second:  20, episode reward: 41.340, mean reward:  1.253 [-2.202, 32.101], mean action: 4.667 [1.000, 18.000],  loss: 0.016020, mae: 0.310752, mean_q: 0.368552, mean_eps: 0.000000
  7582/30000: episode: 183, duration: 1.196s, episode steps:  28, steps per second:  23, episode reward: 40.871, mean reward:  1.460 [-3.000, 32.230], mean action: 3.357 [1.000, 8.000],  loss: 0.011547, mae: 0.284631, mean_q: 0.346960, mean_eps: 0.000000
  7638/30000: episode: 184, duration: 1.679s, episode steps:  56, steps per second:  33, episode reward: -32.400, mean reward: -0.579 [-32.167,  2.424], mean action: 3.929 [1.000, 21.000],  loss: 0.013155, mae: 0.295149, mean_q: 0.328642, mean_eps: 0.000000
  7661/30000: episode: 185, duration: 0.428s, episode steps:  23, steps per second:  54, episode reward: 47.666, mean reward:  2.072 [-0.300, 32.170], mean action: 2.217 [1.000, 7.000],  loss: 0.015985, mae: 0.305328, mean_q: 0.281922, mean_eps: 0.000000
  7700/30000: episode: 186, duration: 0.753s, episode steps:  39, steps per second:  52, episode reward: 44.257, mean reward:  1.135 [-2.044, 32.024], mean action: 2.846 [1.000, 15.000],  loss: 0.011525, mae: 0.283985, mean_q: 0.294230, mean_eps: 0.000000
  7737/30000: episode: 187, duration: 0.717s, episode steps:  37, steps per second:  52, episode reward: 35.098, mean reward:  0.949 [-3.000, 32.530], mean action: 6.784 [1.000, 21.000],  loss: 0.013595, mae: 0.278614, mean_q: 0.363408, mean_eps: 0.000000
  7760/30000: episode: 188, duration: 0.503s, episode steps:  23, steps per second:  46, episode reward: 41.867, mean reward:  1.820 [-2.245, 32.340], mean action: 4.217 [1.000, 10.000],  loss: 0.012243, mae: 0.287761, mean_q: 0.230160, mean_eps: 0.000000
  7801/30000: episode: 189, duration: 0.899s, episode steps:  41, steps per second:  46, episode reward: 40.112, mean reward:  0.978 [-2.195, 32.800], mean action: 6.268 [1.000, 21.000],  loss: 0.013229, mae: 0.278502, mean_q: 0.311867, mean_eps: 0.000000
  7846/30000: episode: 190, duration: 0.861s, episode steps:  45, steps per second:  52, episode reward: 32.026, mean reward:  0.712 [-3.000, 32.240], mean action: 9.156 [1.000, 17.000],  loss: 0.010059, mae: 0.268508, mean_q: 0.328031, mean_eps: 0.000000
  7884/30000: episode: 191, duration: 0.737s, episode steps:  38, steps per second:  52, episode reward: 32.793, mean reward:  0.863 [-3.000, 32.123], mean action: 7.053 [1.000, 14.000],  loss: 0.010810, mae: 0.280378, mean_q: 0.310908, mean_eps: 0.000000
  7899/30000: episode: 192, duration: 0.297s, episode steps:  15, steps per second:  51, episode reward: 44.567, mean reward:  2.971 [-2.004, 33.000], mean action: 3.667 [1.000, 11.000],  loss: 0.008276, mae: 0.280430, mean_q: 0.283874, mean_eps: 0.000000
  7949/30000: episode: 193, duration: 0.970s, episode steps:  50, steps per second:  52, episode reward: 38.364, mean reward:  0.767 [-2.702, 32.172], mean action: 5.820 [1.000, 21.000],  loss: 0.013583, mae: 0.296007, mean_q: 0.339499, mean_eps: 0.000000
  7967/30000: episode: 194, duration: 0.617s, episode steps:  18, steps per second:  29, episode reward: 42.000, mean reward:  2.333 [-2.234, 32.340], mean action: 2.000 [1.000, 6.000],  loss: 0.014803, mae: 0.303284, mean_q: 0.331509, mean_eps: 0.000000
  7994/30000: episode: 195, duration: 0.711s, episode steps:  27, steps per second:  38, episode reward: 39.000, mean reward:  1.444 [-2.822, 32.100], mean action: 2.926 [1.000, 17.000],  loss: 0.015492, mae: 0.300105, mean_q: 0.335601, mean_eps: 0.000000
  8019/30000: episode: 196, duration: 0.663s, episode steps:  25, steps per second:  38, episode reward: 43.936, mean reward:  1.757 [-2.639, 32.030], mean action: 6.360 [1.000, 15.000],  loss: 0.015614, mae: 0.295300, mean_q: 0.383502, mean_eps: 0.000000
  8048/30000: episode: 197, duration: 0.703s, episode steps:  29, steps per second:  41, episode reward: 33.000, mean reward:  1.138 [-3.000, 32.350], mean action: 5.069 [1.000, 16.000],  loss: 0.013753, mae: 0.294341, mean_q: 0.339319, mean_eps: 0.000000
  8072/30000: episode: 198, duration: 0.799s, episode steps:  24, steps per second:  30, episode reward: 38.940, mean reward:  1.622 [-2.558, 32.040], mean action: 5.208 [1.000, 17.000],  loss: 0.013558, mae: 0.295750, mean_q: 0.321610, mean_eps: 0.000000
  8098/30000: episode: 199, duration: 0.842s, episode steps:  26, steps per second:  31, episode reward: 44.938, mean reward:  1.728 [-0.178, 29.880], mean action: 5.192 [3.000, 16.000],  loss: 0.010967, mae: 0.267841, mean_q: 0.366504, mean_eps: 0.000000
  8128/30000: episode: 200, duration: 2.373s, episode steps:  30, steps per second:  13, episode reward: 38.145, mean reward:  1.271 [-3.000, 31.934], mean action: 3.633 [1.000, 12.000],  loss: 0.014149, mae: 0.290070, mean_q: 0.291699, mean_eps: 0.000000
  8163/30000: episode: 201, duration: 1.609s, episode steps:  35, steps per second:  22, episode reward: -34.380, mean reward: -0.982 [-32.592,  2.590], mean action: 3.914 [1.000, 14.000],  loss: 0.015518, mae: 0.281377, mean_q: 0.318017, mean_eps: 0.000000
  8181/30000: episode: 202, duration: 0.834s, episode steps:  18, steps per second:  22, episode reward: 44.727, mean reward:  2.485 [-2.601, 32.560], mean action: 6.333 [1.000, 15.000],  loss: 0.016311, mae: 0.291651, mean_q: 0.277162, mean_eps: 0.000000
  8221/30000: episode: 203, duration: 1.229s, episode steps:  40, steps per second:  33, episode reward: 44.407, mean reward:  1.110 [-2.291, 32.329], mean action: 3.825 [3.000, 15.000],  loss: 0.011880, mae: 0.273727, mean_q: 0.300254, mean_eps: 0.000000
  8259/30000: episode: 204, duration: 1.102s, episode steps:  38, steps per second:  34, episode reward: -30.000, mean reward: -0.789 [-30.103,  3.000], mean action: 5.000 [1.000, 21.000],  loss: 0.011003, mae: 0.265458, mean_q: 0.300653, mean_eps: 0.000000
  8290/30000: episode: 205, duration: 0.886s, episode steps:  31, steps per second:  35, episode reward: 44.524, mean reward:  1.436 [-2.025, 32.350], mean action: 2.613 [1.000, 11.000],  loss: 0.012188, mae: 0.267796, mean_q: 0.339544, mean_eps: 0.000000
  8308/30000: episode: 206, duration: 0.638s, episode steps:  18, steps per second:  28, episode reward: 44.008, mean reward:  2.445 [-2.136, 31.918], mean action: 5.556 [1.000, 21.000],  loss: 0.013571, mae: 0.274833, mean_q: 0.362010, mean_eps: 0.000000
  8353/30000: episode: 207, duration: 1.175s, episode steps:  45, steps per second:  38, episode reward: 38.332, mean reward:  0.852 [-2.710, 32.510], mean action: 4.711 [1.000, 21.000],  loss: 0.014950, mae: 0.286004, mean_q: 0.337371, mean_eps: 0.000000
  8384/30000: episode: 208, duration: 1.652s, episode steps:  31, steps per second:  19, episode reward: 35.890, mean reward:  1.158 [-3.000, 32.040], mean action: 8.613 [1.000, 21.000],  loss: 0.014631, mae: 0.277460, mean_q: 0.324791, mean_eps: 0.000000
  8420/30000: episode: 209, duration: 1.130s, episode steps:  36, steps per second:  32, episode reward: 35.598, mean reward:  0.989 [-3.000, 34.008], mean action: 4.333 [1.000, 17.000],  loss: 0.014440, mae: 0.281012, mean_q: 0.321034, mean_eps: 0.000000
  8456/30000: episode: 210, duration: 1.099s, episode steps:  36, steps per second:  33, episode reward: 38.291, mean reward:  1.064 [-2.209, 32.023], mean action: 5.528 [1.000, 15.000],  loss: 0.014797, mae: 0.276133, mean_q: 0.337777, mean_eps: 0.000000
  8489/30000: episode: 211, duration: 0.800s, episode steps:  33, steps per second:  41, episode reward: 38.386, mean reward:  1.163 [-3.000, 32.360], mean action: 2.606 [1.000, 6.000],  loss: 0.014353, mae: 0.276393, mean_q: 0.345648, mean_eps: 0.000000
  8507/30000: episode: 212, duration: 0.427s, episode steps:  18, steps per second:  42, episode reward: 44.438, mean reward:  2.469 [-2.622, 31.789], mean action: 2.889 [1.000, 11.000],  loss: 0.011672, mae: 0.272336, mean_q: 0.276793, mean_eps: 0.000000
  8538/30000: episode: 213, duration: 0.738s, episode steps:  31, steps per second:  42, episode reward: 40.798, mean reward:  1.316 [-2.447, 32.051], mean action: 4.065 [1.000, 21.000],  loss: 0.015258, mae: 0.285095, mean_q: 0.310380, mean_eps: 0.000000
  8564/30000: episode: 214, duration: 0.548s, episode steps:  26, steps per second:  47, episode reward: 44.566, mean reward:  1.714 [-2.532, 32.222], mean action: 3.346 [1.000, 7.000],  loss: 0.012213, mae: 0.273514, mean_q: 0.293710, mean_eps: 0.000000
  8594/30000: episode: 215, duration: 0.587s, episode steps:  30, steps per second:  51, episode reward: 43.291, mean reward:  1.443 [-2.424, 31.853], mean action: 6.600 [1.000, 18.000],  loss: 0.014937, mae: 0.284480, mean_q: 0.320132, mean_eps: 0.000000
  8615/30000: episode: 216, duration: 0.560s, episode steps:  21, steps per second:  37, episode reward: 39.000, mean reward:  1.857 [-3.000, 32.310], mean action: 5.381 [1.000, 21.000],  loss: 0.016328, mae: 0.285902, mean_q: 0.268366, mean_eps: 0.000000
  8644/30000: episode: 217, duration: 0.636s, episode steps:  29, steps per second:  46, episode reward: 44.732, mean reward:  1.542 [-2.163, 32.120], mean action: 3.069 [1.000, 6.000],  loss: 0.013200, mae: 0.264541, mean_q: 0.372689, mean_eps: 0.000000
  8675/30000: episode: 218, duration: 0.714s, episode steps:  31, steps per second:  43, episode reward: 38.152, mean reward:  1.231 [-2.299, 31.706], mean action: 4.774 [1.000, 21.000],  loss: 0.013978, mae: 0.275154, mean_q: 0.324152, mean_eps: 0.000000
  8704/30000: episode: 219, duration: 0.827s, episode steps:  29, steps per second:  35, episode reward: 35.799, mean reward:  1.234 [-2.903, 32.180], mean action: 4.069 [1.000, 11.000],  loss: 0.011880, mae: 0.263317, mean_q: 0.381334, mean_eps: 0.000000
  8735/30000: episode: 220, duration: 0.716s, episode steps:  31, steps per second:  43, episode reward: 39.000, mean reward:  1.258 [-2.515, 32.500], mean action: 7.419 [1.000, 21.000],  loss: 0.013075, mae: 0.274065, mean_q: 0.342663, mean_eps: 0.000000
  8773/30000: episode: 221, duration: 0.853s, episode steps:  38, steps per second:  45, episode reward: -35.490, mean reward: -0.934 [-32.321,  2.490], mean action: 10.553 [1.000, 21.000],  loss: 0.015506, mae: 0.287158, mean_q: 0.357881, mean_eps: 0.000000
  8798/30000: episode: 222, duration: 0.568s, episode steps:  25, steps per second:  44, episode reward: 33.000, mean reward:  1.320 [-3.000, 32.290], mean action: 4.920 [1.000, 16.000],  loss: 0.016225, mae: 0.284476, mean_q: 0.376915, mean_eps: 0.000000
  8833/30000: episode: 223, duration: 0.752s, episode steps:  35, steps per second:  47, episode reward: -32.060, mean reward: -0.916 [-32.900,  2.450], mean action: 5.571 [1.000, 17.000],  loss: 0.017195, mae: 0.299636, mean_q: 0.353979, mean_eps: 0.000000
  8988/30000: episode: 224, duration: 3.344s, episode steps: 155, steps per second:  46, episode reward: -35.850, mean reward: -0.231 [-32.049,  2.773], mean action: 5.439 [1.000, 16.000],  loss: 0.012917, mae: 0.272384, mean_q: 0.322112, mean_eps: 0.000000
  9019/30000: episode: 225, duration: 0.629s, episode steps:  31, steps per second:  49, episode reward: 41.573, mean reward:  1.341 [-2.616, 32.350], mean action: 3.613 [1.000, 12.000],  loss: 0.012252, mae: 0.271202, mean_q: 0.304787, mean_eps: 0.000000
  9048/30000: episode: 226, duration: 0.578s, episode steps:  29, steps per second:  50, episode reward: 41.903, mean reward:  1.445 [-3.000, 32.130], mean action: 4.931 [1.000, 15.000],  loss: 0.011236, mae: 0.264965, mean_q: 0.341151, mean_eps: 0.000000
  9078/30000: episode: 227, duration: 0.628s, episode steps:  30, steps per second:  48, episode reward: 38.779, mean reward:  1.293 [-2.504, 32.710], mean action: 5.700 [1.000, 17.000],  loss: 0.012577, mae: 0.274624, mean_q: 0.309519, mean_eps: 0.000000
  9124/30000: episode: 228, duration: 0.882s, episode steps:  46, steps per second:  52, episode reward: 40.271, mean reward:  0.875 [-2.128, 34.380], mean action: 4.435 [0.000, 20.000],  loss: 0.014402, mae: 0.269889, mean_q: 0.389621, mean_eps: 0.000000
  9181/30000: episode: 229, duration: 1.068s, episode steps:  57, steps per second:  53, episode reward: -35.190, mean reward: -0.617 [-32.861,  2.513], mean action: 6.912 [1.000, 21.000],  loss: 0.014142, mae: 0.279480, mean_q: 0.298790, mean_eps: 0.000000
  9200/30000: episode: 230, duration: 0.365s, episode steps:  19, steps per second:  52, episode reward: 47.330, mean reward:  2.491 [-0.418, 32.080], mean action: 3.842 [1.000, 15.000],  loss: 0.013273, mae: 0.278177, mean_q: 0.311616, mean_eps: 0.000000
  9231/30000: episode: 231, duration: 0.580s, episode steps:  31, steps per second:  53, episode reward: 39.000, mean reward:  1.258 [-2.638, 32.180], mean action: 3.290 [1.000, 15.000],  loss: 0.015448, mae: 0.290808, mean_q: 0.313950, mean_eps: 0.000000
  9286/30000: episode: 232, duration: 0.887s, episode steps:  55, steps per second:  62, episode reward: 43.929, mean reward:  0.799 [-2.381, 32.130], mean action: 2.618 [1.000, 17.000],  loss: 0.012597, mae: 0.272898, mean_q: 0.323948, mean_eps: 0.000000
  9318/30000: episode: 233, duration: 0.559s, episode steps:  32, steps per second:  57, episode reward: 33.000, mean reward:  1.031 [-2.452, 32.680], mean action: 5.812 [1.000, 21.000],  loss: 0.014460, mae: 0.284540, mean_q: 0.287933, mean_eps: 0.000000
  9351/30000: episode: 234, duration: 0.580s, episode steps:  33, steps per second:  57, episode reward: 44.448, mean reward:  1.347 [-2.289, 32.280], mean action: 3.848 [1.000, 12.000],  loss: 0.014709, mae: 0.277988, mean_q: 0.340695, mean_eps: 0.000000
  9375/30000: episode: 235, duration: 0.474s, episode steps:  24, steps per second:  51, episode reward: 35.701, mean reward:  1.488 [-3.000, 32.007], mean action: 5.458 [1.000, 16.000],  loss: 0.013702, mae: 0.286941, mean_q: 0.294685, mean_eps: 0.000000
  9415/30000: episode: 236, duration: 0.649s, episode steps:  40, steps per second:  62, episode reward: 38.138, mean reward:  0.953 [-3.000, 33.917], mean action: 4.850 [1.000, 17.000],  loss: 0.014652, mae: 0.286894, mean_q: 0.318971, mean_eps: 0.000000
  9551/30000: episode: 237, duration: 2.229s, episode steps: 136, steps per second:  61, episode reward: 35.584, mean reward:  0.262 [-3.000, 32.129], mean action: 7.265 [1.000, 21.000],  loss: 0.013745, mae: 0.278966, mean_q: 0.341330, mean_eps: 0.000000
  9585/30000: episode: 238, duration: 0.618s, episode steps:  34, steps per second:  55, episode reward: 35.416, mean reward:  1.042 [-3.000, 32.332], mean action: 4.176 [1.000, 17.000],  loss: 0.014474, mae: 0.272821, mean_q: 0.315138, mean_eps: 0.000000
  9638/30000: episode: 239, duration: 0.959s, episode steps:  53, steps per second:  55, episode reward: 39.000, mean reward:  0.736 [-2.259, 32.520], mean action: 4.811 [1.000, 21.000],  loss: 0.013301, mae: 0.272687, mean_q: 0.317372, mean_eps: 0.000000
  9663/30000: episode: 240, duration: 0.527s, episode steps:  25, steps per second:  47, episode reward: 41.131, mean reward:  1.645 [-2.149, 32.100], mean action: 3.080 [1.000, 16.000],  loss: 0.013984, mae: 0.268652, mean_q: 0.350413, mean_eps: 0.000000
  9701/30000: episode: 241, duration: 0.682s, episode steps:  38, steps per second:  56, episode reward: -35.450, mean reward: -0.933 [-33.000,  2.320], mean action: 4.947 [1.000, 15.000],  loss: 0.012608, mae: 0.270484, mean_q: 0.321706, mean_eps: 0.000000
  9741/30000: episode: 242, duration: 0.728s, episode steps:  40, steps per second:  55, episode reward: 35.265, mean reward:  0.882 [-2.854, 29.267], mean action: 10.975 [1.000, 21.000],  loss: 0.015047, mae: 0.284796, mean_q: 0.343903, mean_eps: 0.000000
  9765/30000: episode: 243, duration: 0.479s, episode steps:  24, steps per second:  50, episode reward: 47.112, mean reward:  1.963 [-0.765, 32.070], mean action: 2.708 [1.000, 10.000],  loss: 0.017056, mae: 0.295890, mean_q: 0.335729, mean_eps: 0.000000
  9796/30000: episode: 244, duration: 0.653s, episode steps:  31, steps per second:  47, episode reward: 41.314, mean reward:  1.333 [-2.189, 32.052], mean action: 2.935 [1.000, 16.000],  loss: 0.015327, mae: 0.288051, mean_q: 0.364566, mean_eps: 0.000000
  9826/30000: episode: 245, duration: 0.543s, episode steps:  30, steps per second:  55, episode reward: 38.886, mean reward:  1.296 [-2.592, 32.208], mean action: 6.433 [1.000, 21.000],  loss: 0.014581, mae: 0.293246, mean_q: 0.312591, mean_eps: 0.000000
  9862/30000: episode: 246, duration: 0.705s, episode steps:  36, steps per second:  51, episode reward: 38.939, mean reward:  1.082 [-3.000, 32.280], mean action: 4.639 [1.000, 18.000],  loss: 0.015267, mae: 0.293680, mean_q: 0.361692, mean_eps: 0.000000
  9889/30000: episode: 247, duration: 0.547s, episode steps:  27, steps per second:  49, episode reward: 46.128, mean reward:  1.708 [-0.345, 32.500], mean action: 4.037 [1.000, 21.000],  loss: 0.015971, mae: 0.312003, mean_q: 0.297188, mean_eps: 0.000000
  9930/30000: episode: 248, duration: 0.746s, episode steps:  41, steps per second:  55, episode reward: 34.991, mean reward:  0.853 [-2.877, 34.337], mean action: 4.951 [0.000, 21.000],  loss: 0.017207, mae: 0.306091, mean_q: 0.364527, mean_eps: 0.000000
  9951/30000: episode: 249, duration: 0.385s, episode steps:  21, steps per second:  55, episode reward: 43.708, mean reward:  2.081 [-2.116, 32.245], mean action: 12.095 [1.000, 21.000],  loss: 0.013723, mae: 0.277953, mean_q: 0.471189, mean_eps: 0.000000
  9971/30000: episode: 250, duration: 0.365s, episode steps:  20, steps per second:  55, episode reward: 43.509, mean reward:  2.175 [-2.569, 32.802], mean action: 8.450 [1.000, 21.000],  loss: 0.012616, mae: 0.282087, mean_q: 0.321551, mean_eps: 0.000000
 10004/30000: episode: 251, duration: 0.609s, episode steps:  33, steps per second:  54, episode reward: 35.555, mean reward:  1.077 [-2.876, 29.454], mean action: 4.606 [1.000, 17.000],  loss: 0.017121, mae: 0.298113, mean_q: 0.330570, mean_eps: 0.000000
 10028/30000: episode: 252, duration: 0.525s, episode steps:  24, steps per second:  46, episode reward: 47.290, mean reward:  1.970 [-0.501, 32.450], mean action: 3.542 [1.000, 15.000],  loss: 0.011885, mae: 0.270893, mean_q: 0.358117, mean_eps: 0.000000
 10052/30000: episode: 253, duration: 0.439s, episode steps:  24, steps per second:  55, episode reward: 37.252, mean reward:  1.552 [-3.000, 31.930], mean action: 5.792 [1.000, 21.000],  loss: 0.015503, mae: 0.288706, mean_q: 0.355876, mean_eps: 0.000000
 10106/30000: episode: 254, duration: 0.929s, episode steps:  54, steps per second:  58, episode reward: -33.000, mean reward: -0.611 [-29.857,  2.331], mean action: 6.722 [1.000, 21.000],  loss: 0.013716, mae: 0.279582, mean_q: 0.335746, mean_eps: 0.000000
 10133/30000: episode: 255, duration: 0.564s, episode steps:  27, steps per second:  48, episode reward: 43.978, mean reward:  1.629 [-2.472, 32.200], mean action: 3.111 [1.000, 15.000],  loss: 0.013294, mae: 0.274555, mean_q: 0.316185, mean_eps: 0.000000
 10161/30000: episode: 256, duration: 0.513s, episode steps:  28, steps per second:  55, episode reward: 41.336, mean reward:  1.476 [-2.323, 32.081], mean action: 3.357 [1.000, 16.000],  loss: 0.012704, mae: 0.276707, mean_q: 0.353020, mean_eps: 0.000000
 10185/30000: episode: 257, duration: 0.456s, episode steps:  24, steps per second:  53, episode reward: 40.944, mean reward:  1.706 [-2.617, 32.240], mean action: 5.583 [1.000, 16.000],  loss: 0.012935, mae: 0.278672, mean_q: 0.383062, mean_eps: 0.000000
 10216/30000: episode: 258, duration: 0.652s, episode steps:  31, steps per second:  48, episode reward: 38.520, mean reward:  1.243 [-2.178, 31.700], mean action: 4.032 [1.000, 17.000],  loss: 0.014563, mae: 0.293311, mean_q: 0.335677, mean_eps: 0.000000
 10268/30000: episode: 259, duration: 1.004s, episode steps:  52, steps per second:  52, episode reward: 32.608, mean reward:  0.627 [-3.000, 32.050], mean action: 8.577 [1.000, 21.000],  loss: 0.014854, mae: 0.293937, mean_q: 0.340150, mean_eps: 0.000000
 10308/30000: episode: 260, duration: 0.743s, episode steps:  40, steps per second:  54, episode reward: 38.255, mean reward:  0.956 [-2.192, 32.180], mean action: 3.850 [1.000, 21.000],  loss: 0.014304, mae: 0.285002, mean_q: 0.365776, mean_eps: 0.000000
 10350/30000: episode: 261, duration: 0.788s, episode steps:  42, steps per second:  53, episode reward: 34.373, mean reward:  0.818 [-2.976, 32.150], mean action: 5.405 [1.000, 17.000],  loss: 0.015271, mae: 0.290197, mean_q: 0.341818, mean_eps: 0.000000
 10387/30000: episode: 262, duration: 0.747s, episode steps:  37, steps per second:  50, episode reward: 36.000, mean reward:  0.973 [-2.430, 32.470], mean action: 4.378 [1.000, 17.000],  loss: 0.015373, mae: 0.293935, mean_q: 0.298019, mean_eps: 0.000000
 10426/30000: episode: 263, duration: 1.062s, episode steps:  39, steps per second:  37, episode reward: 42.000, mean reward:  1.077 [-2.149, 32.050], mean action: 3.692 [3.000, 16.000],  loss: 0.013015, mae: 0.283449, mean_q: 0.315222, mean_eps: 0.000000
 10457/30000: episode: 264, duration: 0.643s, episode steps:  31, steps per second:  48, episode reward: 41.279, mean reward:  1.332 [-2.217, 32.750], mean action: 4.677 [1.000, 14.000],  loss: 0.016142, mae: 0.290316, mean_q: 0.292006, mean_eps: 0.000000
 10489/30000: episode: 265, duration: 0.764s, episode steps:  32, steps per second:  42, episode reward: 43.556, mean reward:  1.361 [-3.000, 32.290], mean action: 4.719 [1.000, 12.000],  loss: 0.015090, mae: 0.272802, mean_q: 0.304382, mean_eps: 0.000000
 10554/30000: episode: 266, duration: 1.505s, episode steps:  65, steps per second:  43, episode reward: -32.270, mean reward: -0.496 [-32.259,  2.150], mean action: 5.292 [1.000, 21.000],  loss: 0.014038, mae: 0.274849, mean_q: 0.330961, mean_eps: 0.000000
 10573/30000: episode: 267, duration: 0.511s, episode steps:  19, steps per second:  37, episode reward: 43.617, mean reward:  2.296 [-2.076, 32.312], mean action: 6.474 [1.000, 15.000],  loss: 0.012868, mae: 0.264620, mean_q: 0.443651, mean_eps: 0.000000
 10610/30000: episode: 268, duration: 0.822s, episode steps:  37, steps per second:  45, episode reward: 38.835, mean reward:  1.050 [-3.000, 31.985], mean action: 4.351 [1.000, 16.000],  loss: 0.013989, mae: 0.281184, mean_q: 0.340130, mean_eps: 0.000000
 10628/30000: episode: 269, duration: 0.453s, episode steps:  18, steps per second:  40, episode reward: 41.622, mean reward:  2.312 [-2.425, 32.360], mean action: 4.556 [1.000, 15.000],  loss: 0.016353, mae: 0.286046, mean_q: 0.360363, mean_eps: 0.000000
 10658/30000: episode: 270, duration: 0.678s, episode steps:  30, steps per second:  44, episode reward: 41.028, mean reward:  1.368 [-2.419, 32.160], mean action: 4.567 [1.000, 15.000],  loss: 0.015503, mae: 0.298383, mean_q: 0.284434, mean_eps: 0.000000
 10691/30000: episode: 271, duration: 0.684s, episode steps:  33, steps per second:  48, episode reward: 40.628, mean reward:  1.231 [-2.416, 32.230], mean action: 5.727 [1.000, 15.000],  loss: 0.012225, mae: 0.278818, mean_q: 0.316720, mean_eps: 0.000000
 10764/30000: episode: 272, duration: 1.489s, episode steps:  73, steps per second:  49, episode reward: -35.370, mean reward: -0.485 [-32.505,  3.000], mean action: 7.123 [1.000, 21.000],  loss: 0.013848, mae: 0.279958, mean_q: 0.294307, mean_eps: 0.000000
 10804/30000: episode: 273, duration: 0.774s, episode steps:  40, steps per second:  52, episode reward: 40.618, mean reward:  1.015 [-2.504, 32.160], mean action: 4.350 [1.000, 15.000],  loss: 0.016210, mae: 0.286119, mean_q: 0.315184, mean_eps: 0.000000
 10844/30000: episode: 274, duration: 1.121s, episode steps:  40, steps per second:  36, episode reward: 38.353, mean reward:  0.959 [-2.358, 31.912], mean action: 8.075 [1.000, 21.000],  loss: 0.013218, mae: 0.274812, mean_q: 0.319393, mean_eps: 0.000000
 10866/30000: episode: 275, duration: 0.694s, episode steps:  22, steps per second:  32, episode reward: 41.106, mean reward:  1.868 [-2.341, 32.240], mean action: 3.727 [1.000, 11.000],  loss: 0.012557, mae: 0.262056, mean_q: 0.301597, mean_eps: 0.000000
 10918/30000: episode: 276, duration: 1.373s, episode steps:  52, steps per second:  38, episode reward: 37.889, mean reward:  0.729 [-2.579, 32.660], mean action: 9.442 [1.000, 21.000],  loss: 0.015670, mae: 0.281738, mean_q: 0.327525, mean_eps: 0.000000
 10941/30000: episode: 277, duration: 0.564s, episode steps:  23, steps per second:  41, episode reward: 39.000, mean reward:  1.696 [-2.366, 32.400], mean action: 3.609 [1.000, 15.000],  loss: 0.013657, mae: 0.272039, mean_q: 0.309706, mean_eps: 0.000000
 10950/30000: episode: 278, duration: 0.273s, episode steps:   9, steps per second:  33, episode reward: 47.390, mean reward:  5.266 [ 0.119, 33.000], mean action: 1.444 [1.000, 3.000],  loss: 0.014072, mae: 0.267060, mean_q: 0.321481, mean_eps: 0.000000
 10979/30000: episode: 279, duration: 0.857s, episode steps:  29, steps per second:  34, episode reward: 38.081, mean reward:  1.313 [-2.429, 32.090], mean action: 5.517 [1.000, 21.000],  loss: 0.015855, mae: 0.279973, mean_q: 0.340233, mean_eps: 0.000000
 10997/30000: episode: 280, duration: 0.424s, episode steps:  18, steps per second:  42, episode reward: 45.000, mean reward:  2.500 [-2.164, 32.450], mean action: 4.278 [1.000, 15.000],  loss: 0.013434, mae: 0.277664, mean_q: 0.303624, mean_eps: 0.000000
 11030/30000: episode: 281, duration: 0.654s, episode steps:  33, steps per second:  50, episode reward: 35.726, mean reward:  1.083 [-2.365, 31.924], mean action: 4.758 [1.000, 17.000],  loss: 0.017509, mae: 0.302921, mean_q: 0.316840, mean_eps: 0.000000
 11070/30000: episode: 282, duration: 0.816s, episode steps:  40, steps per second:  49, episode reward: 38.055, mean reward:  0.951 [-2.254, 32.280], mean action: 5.750 [1.000, 16.000],  loss: 0.013428, mae: 0.276621, mean_q: 0.320240, mean_eps: 0.000000
 11129/30000: episode: 283, duration: 1.235s, episode steps:  59, steps per second:  48, episode reward: 32.520, mean reward:  0.551 [-3.000, 32.210], mean action: 6.661 [3.000, 16.000],  loss: 0.016696, mae: 0.282583, mean_q: 0.347033, mean_eps: 0.000000
 11172/30000: episode: 284, duration: 0.915s, episode steps:  43, steps per second:  47, episode reward: 34.292, mean reward:  0.797 [-3.000, 32.050], mean action: 7.953 [1.000, 21.000],  loss: 0.015675, mae: 0.275909, mean_q: 0.365577, mean_eps: 0.000000
 11190/30000: episode: 285, duration: 0.373s, episode steps:  18, steps per second:  48, episode reward: 44.202, mean reward:  2.456 [-2.420, 32.130], mean action: 3.056 [1.000, 14.000],  loss: 0.016523, mae: 0.281803, mean_q: 0.387955, mean_eps: 0.000000
 11220/30000: episode: 286, duration: 0.674s, episode steps:  30, steps per second:  45, episode reward: 38.727, mean reward:  1.291 [-3.000, 32.100], mean action: 3.533 [1.000, 15.000],  loss: 0.015898, mae: 0.277090, mean_q: 0.332044, mean_eps: 0.000000
 11245/30000: episode: 287, duration: 0.602s, episode steps:  25, steps per second:  42, episode reward: 41.093, mean reward:  1.644 [-2.229, 32.122], mean action: 4.000 [1.000, 15.000],  loss: 0.012063, mae: 0.255407, mean_q: 0.342816, mean_eps: 0.000000
 11288/30000: episode: 288, duration: 0.819s, episode steps:  43, steps per second:  53, episode reward: -34.510, mean reward: -0.803 [-32.840,  3.000], mean action: 6.093 [1.000, 21.000],  loss: 0.018099, mae: 0.291538, mean_q: 0.302395, mean_eps: 0.000000
 11334/30000: episode: 289, duration: 0.853s, episode steps:  46, steps per second:  54, episode reward: 35.220, mean reward:  0.766 [-3.000, 32.360], mean action: 6.587 [1.000, 21.000],  loss: 0.015418, mae: 0.276919, mean_q: 0.309482, mean_eps: 0.000000
 11361/30000: episode: 290, duration: 0.513s, episode steps:  27, steps per second:  53, episode reward: 41.657, mean reward:  1.543 [-2.058, 32.030], mean action: 3.185 [1.000, 16.000],  loss: 0.014557, mae: 0.270820, mean_q: 0.351135, mean_eps: 0.000000
 11390/30000: episode: 291, duration: 0.564s, episode steps:  29, steps per second:  51, episode reward: 38.129, mean reward:  1.315 [-2.372, 32.080], mean action: 9.621 [1.000, 21.000],  loss: 0.017581, mae: 0.301427, mean_q: 0.277644, mean_eps: 0.000000
 11422/30000: episode: 292, duration: 0.564s, episode steps:  32, steps per second:  57, episode reward: 43.553, mean reward:  1.361 [-3.000, 31.631], mean action: 5.219 [1.000, 15.000],  loss: 0.015311, mae: 0.277200, mean_q: 0.321788, mean_eps: 0.000000
 11476/30000: episode: 293, duration: 0.996s, episode steps:  54, steps per second:  54, episode reward: -32.740, mean reward: -0.606 [-32.204,  2.260], mean action: 5.204 [1.000, 12.000],  loss: 0.015663, mae: 0.274709, mean_q: 0.332995, mean_eps: 0.000000
 11502/30000: episode: 294, duration: 0.529s, episode steps:  26, steps per second:  49, episode reward: 38.550, mean reward:  1.483 [-2.506, 32.251], mean action: 4.423 [1.000, 15.000],  loss: 0.013295, mae: 0.273082, mean_q: 0.284711, mean_eps: 0.000000
 11596/30000: episode: 295, duration: 1.779s, episode steps:  94, steps per second:  53, episode reward: 35.777, mean reward:  0.381 [-2.876, 31.945], mean action: 7.915 [1.000, 17.000],  loss: 0.014936, mae: 0.274760, mean_q: 0.329332, mean_eps: 0.000000
 11638/30000: episode: 296, duration: 0.815s, episode steps:  42, steps per second:  52, episode reward: 39.000, mean reward:  0.929 [-2.195, 32.830], mean action: 3.238 [1.000, 16.000],  loss: 0.016603, mae: 0.283626, mean_q: 0.326374, mean_eps: 0.000000
 11669/30000: episode: 297, duration: 0.713s, episode steps:  31, steps per second:  43, episode reward: 37.469, mean reward:  1.209 [-2.860, 31.766], mean action: 6.065 [1.000, 15.000],  loss: 0.012911, mae: 0.276803, mean_q: 0.284644, mean_eps: 0.000000
 11694/30000: episode: 298, duration: 0.666s, episode steps:  25, steps per second:  38, episode reward: 40.623, mean reward:  1.625 [-2.273, 32.060], mean action: 4.560 [1.000, 15.000],  loss: 0.015822, mae: 0.282796, mean_q: 0.309196, mean_eps: 0.000000
 11723/30000: episode: 299, duration: 0.648s, episode steps:  29, steps per second:  45, episode reward: 37.700, mean reward:  1.300 [-2.280, 31.666], mean action: 5.586 [1.000, 17.000],  loss: 0.014981, mae: 0.271934, mean_q: 0.338787, mean_eps: 0.000000
 11746/30000: episode: 300, duration: 0.453s, episode steps:  23, steps per second:  51, episode reward: 41.416, mean reward:  1.801 [-2.864, 32.044], mean action: 2.087 [1.000, 15.000],  loss: 0.019239, mae: 0.310312, mean_q: 0.284697, mean_eps: 0.000000
 11774/30000: episode: 301, duration: 0.602s, episode steps:  28, steps per second:  46, episode reward: 42.852, mean reward:  1.530 [-3.000, 32.090], mean action: 4.286 [1.000, 15.000],  loss: 0.013060, mae: 0.279679, mean_q: 0.271646, mean_eps: 0.000000
 11815/30000: episode: 302, duration: 0.838s, episode steps:  41, steps per second:  49, episode reward: 37.664, mean reward:  0.919 [-3.000, 32.250], mean action: 6.537 [0.000, 21.000],  loss: 0.015653, mae: 0.283153, mean_q: 0.249270, mean_eps: 0.000000
 11855/30000: episode: 303, duration: 0.963s, episode steps:  40, steps per second:  42, episode reward: 35.754, mean reward:  0.894 [-2.901, 32.284], mean action: 5.000 [1.000, 16.000],  loss: 0.014843, mae: 0.278079, mean_q: 0.311372, mean_eps: 0.000000
 11883/30000: episode: 304, duration: 0.684s, episode steps:  28, steps per second:  41, episode reward: 45.653, mean reward:  1.630 [-0.617, 32.066], mean action: 4.000 [1.000, 15.000],  loss: 0.017574, mae: 0.282548, mean_q: 0.371092, mean_eps: 0.000000
 11912/30000: episode: 305, duration: 0.721s, episode steps:  29, steps per second:  40, episode reward: 40.994, mean reward:  1.414 [-3.000, 32.191], mean action: 4.345 [1.000, 14.000],  loss: 0.016510, mae: 0.279304, mean_q: 0.345105, mean_eps: 0.000000
 11940/30000: episode: 306, duration: 0.664s, episode steps:  28, steps per second:  42, episode reward: 42.000, mean reward:  1.500 [-2.394, 32.140], mean action: 2.679 [1.000, 14.000],  loss: 0.014146, mae: 0.269438, mean_q: 0.338137, mean_eps: 0.000000
 11965/30000: episode: 307, duration: 0.594s, episode steps:  25, steps per second:  42, episode reward: 42.000, mean reward:  1.680 [-2.659, 32.764], mean action: 4.880 [1.000, 16.000],  loss: 0.014871, mae: 0.284726, mean_q: 0.296082, mean_eps: 0.000000
 11990/30000: episode: 308, duration: 0.578s, episode steps:  25, steps per second:  43, episode reward: 46.688, mean reward:  1.868 [-0.193, 32.164], mean action: 3.520 [1.000, 8.000],  loss: 0.014671, mae: 0.282369, mean_q: 0.295689, mean_eps: 0.000000
 12035/30000: episode: 309, duration: 0.916s, episode steps:  45, steps per second:  49, episode reward: 41.025, mean reward:  0.912 [-2.940, 31.389], mean action: 2.644 [1.000, 14.000],  loss: 0.013620, mae: 0.276383, mean_q: 0.304708, mean_eps: 0.000000
 12066/30000: episode: 310, duration: 0.819s, episode steps:  31, steps per second:  38, episode reward: 37.648, mean reward:  1.214 [-2.506, 32.230], mean action: 5.484 [1.000, 17.000],  loss: 0.016056, mae: 0.278076, mean_q: 0.364778, mean_eps: 0.000000
 12100/30000: episode: 311, duration: 0.804s, episode steps:  34, steps per second:  42, episode reward: 44.129, mean reward:  1.298 [-2.004, 32.170], mean action: 3.412 [1.000, 10.000],  loss: 0.012814, mae: 0.271143, mean_q: 0.302033, mean_eps: 0.000000
 12129/30000: episode: 312, duration: 0.751s, episode steps:  29, steps per second:  39, episode reward: -32.220, mean reward: -1.111 [-32.004,  2.900], mean action: 9.793 [1.000, 15.000],  loss: 0.015488, mae: 0.282372, mean_q: 0.289764, mean_eps: 0.000000
 12161/30000: episode: 313, duration: 0.874s, episode steps:  32, steps per second:  37, episode reward: 38.307, mean reward:  1.197 [-2.750, 32.241], mean action: 4.844 [1.000, 21.000],  loss: 0.015417, mae: 0.291481, mean_q: 0.308923, mean_eps: 0.000000
 12194/30000: episode: 314, duration: 0.874s, episode steps:  33, steps per second:  38, episode reward: 35.110, mean reward:  1.064 [-2.152, 29.567], mean action: 9.788 [1.000, 21.000],  loss: 0.014874, mae: 0.288556, mean_q: 0.291035, mean_eps: 0.000000
 12230/30000: episode: 315, duration: 0.893s, episode steps:  36, steps per second:  40, episode reward: 38.080, mean reward:  1.058 [-2.311, 31.984], mean action: 3.278 [1.000, 11.000],  loss: 0.014910, mae: 0.283006, mean_q: 0.287226, mean_eps: 0.000000
 12261/30000: episode: 316, duration: 0.801s, episode steps:  31, steps per second:  39, episode reward: 35.541, mean reward:  1.146 [-3.000, 32.150], mean action: 7.548 [1.000, 17.000],  loss: 0.016427, mae: 0.275960, mean_q: 0.329062, mean_eps: 0.000000
 12278/30000: episode: 317, duration: 0.457s, episode steps:  17, steps per second:  37, episode reward: 46.617, mean reward:  2.742 [-0.346, 32.320], mean action: 2.353 [1.000, 12.000],  loss: 0.012665, mae: 0.265563, mean_q: 0.272642, mean_eps: 0.000000
 12321/30000: episode: 318, duration: 0.974s, episode steps:  43, steps per second:  44, episode reward: 35.876, mean reward:  0.834 [-3.000, 32.310], mean action: 3.884 [1.000, 17.000],  loss: 0.016930, mae: 0.277240, mean_q: 0.357239, mean_eps: 0.000000
 12343/30000: episode: 319, duration: 0.547s, episode steps:  22, steps per second:  40, episode reward: 35.622, mean reward:  1.619 [-2.606, 32.160], mean action: 6.318 [1.000, 15.000],  loss: 0.015130, mae: 0.287892, mean_q: 0.306701, mean_eps: 0.000000
 12370/30000: episode: 320, duration: 0.693s, episode steps:  27, steps per second:  39, episode reward: 41.098, mean reward:  1.522 [-2.455, 31.724], mean action: 3.111 [1.000, 16.000],  loss: 0.013796, mae: 0.272847, mean_q: 0.368490, mean_eps: 0.000000
 12393/30000: episode: 321, duration: 0.629s, episode steps:  23, steps per second:  37, episode reward: 42.822, mean reward:  1.862 [-3.000, 32.330], mean action: 3.522 [1.000, 10.000],  loss: 0.012794, mae: 0.285549, mean_q: 0.270336, mean_eps: 0.000000
 12425/30000: episode: 322, duration: 0.734s, episode steps:  32, steps per second:  44, episode reward: 39.000, mean reward:  1.219 [-2.733, 32.270], mean action: 4.062 [1.000, 16.000],  loss: 0.015178, mae: 0.279314, mean_q: 0.340314, mean_eps: 0.000000
 12480/30000: episode: 323, duration: 1.585s, episode steps:  55, steps per second:  35, episode reward: 40.649, mean reward:  0.739 [-3.000, 32.330], mean action: 3.727 [1.000, 12.000],  loss: 0.014882, mae: 0.282453, mean_q: 0.296972, mean_eps: 0.000000
 12519/30000: episode: 324, duration: 0.979s, episode steps:  39, steps per second:  40, episode reward: -32.860, mean reward: -0.843 [-32.019,  2.578], mean action: 6.846 [1.000, 21.000],  loss: 0.013725, mae: 0.267121, mean_q: 0.316753, mean_eps: 0.000000
 12540/30000: episode: 325, duration: 0.530s, episode steps:  21, steps per second:  40, episode reward: 47.151, mean reward:  2.245 [-0.358, 32.140], mean action: 3.714 [3.000, 14.000],  loss: 0.011854, mae: 0.263067, mean_q: 0.295938, mean_eps: 0.000000
 12578/30000: episode: 326, duration: 0.869s, episode steps:  38, steps per second:  44, episode reward: 41.733, mean reward:  1.098 [-2.217, 32.220], mean action: 3.447 [1.000, 17.000],  loss: 0.015965, mae: 0.287390, mean_q: 0.311719, mean_eps: 0.000000
 12600/30000: episode: 327, duration: 0.589s, episode steps:  22, steps per second:  37, episode reward: 42.819, mean reward:  1.946 [-2.809, 32.270], mean action: 5.318 [1.000, 14.000],  loss: 0.019547, mae: 0.292979, mean_q: 0.357642, mean_eps: 0.000000
 12639/30000: episode: 328, duration: 1.172s, episode steps:  39, steps per second:  33, episode reward: 37.222, mean reward:  0.954 [-3.000, 31.623], mean action: 6.692 [1.000, 21.000],  loss: 0.015551, mae: 0.279310, mean_q: 0.314377, mean_eps: 0.000000
 12672/30000: episode: 329, duration: 0.991s, episode steps:  33, steps per second:  33, episode reward: 35.876, mean reward:  1.087 [-2.998, 32.250], mean action: 6.697 [1.000, 21.000],  loss: 0.013392, mae: 0.260471, mean_q: 0.296068, mean_eps: 0.000000
 12695/30000: episode: 330, duration: 0.602s, episode steps:  23, steps per second:  38, episode reward: 41.449, mean reward:  1.802 [-2.461, 32.150], mean action: 3.391 [1.000, 10.000],  loss: 0.012976, mae: 0.260799, mean_q: 0.330778, mean_eps: 0.000000
 12720/30000: episode: 331, duration: 0.653s, episode steps:  25, steps per second:  38, episode reward: 41.456, mean reward:  1.658 [-2.440, 30.946], mean action: 6.280 [1.000, 15.000],  loss: 0.015853, mae: 0.276293, mean_q: 0.312477, mean_eps: 0.000000
 12742/30000: episode: 332, duration: 0.569s, episode steps:  22, steps per second:  39, episode reward: 38.770, mean reward:  1.762 [-2.449, 32.340], mean action: 4.545 [1.000, 21.000],  loss: 0.012229, mae: 0.263321, mean_q: 0.309199, mean_eps: 0.000000
 12775/30000: episode: 333, duration: 0.794s, episode steps:  33, steps per second:  42, episode reward: 37.032, mean reward:  1.122 [-3.000, 31.941], mean action: 3.939 [1.000, 14.000],  loss: 0.016110, mae: 0.285212, mean_q: 0.349387, mean_eps: 0.000000
 12813/30000: episode: 334, duration: 0.842s, episode steps:  38, steps per second:  45, episode reward: -33.000, mean reward: -0.868 [-33.000,  2.890], mean action: 3.421 [1.000, 15.000],  loss: 0.015598, mae: 0.285822, mean_q: 0.338923, mean_eps: 0.000000
 12859/30000: episode: 335, duration: 1.164s, episode steps:  46, steps per second:  40, episode reward: 32.374, mean reward:  0.704 [-2.904, 31.644], mean action: 6.087 [1.000, 21.000],  loss: 0.016063, mae: 0.286264, mean_q: 0.301232, mean_eps: 0.000000
 12881/30000: episode: 336, duration: 0.550s, episode steps:  22, steps per second:  40, episode reward: 41.151, mean reward:  1.870 [-2.781, 32.501], mean action: 6.818 [1.000, 14.000],  loss: 0.016847, mae: 0.275075, mean_q: 0.343356, mean_eps: 0.000000
 12908/30000: episode: 337, duration: 0.548s, episode steps:  27, steps per second:  49, episode reward: 44.452, mean reward:  1.646 [-2.404, 31.821], mean action: 3.630 [3.000, 11.000],  loss: 0.016010, mae: 0.282439, mean_q: 0.290252, mean_eps: 0.000000
 12950/30000: episode: 338, duration: 0.826s, episode steps:  42, steps per second:  51, episode reward: 32.084, mean reward:  0.764 [-3.000, 32.199], mean action: 6.310 [1.000, 17.000],  loss: 0.016984, mae: 0.283358, mean_q: 0.311977, mean_eps: 0.000000
 12980/30000: episode: 339, duration: 0.573s, episode steps:  30, steps per second:  52, episode reward: 38.303, mean reward:  1.277 [-2.756, 32.130], mean action: 5.900 [1.000, 21.000],  loss: 0.014887, mae: 0.270764, mean_q: 0.306114, mean_eps: 0.000000
 13017/30000: episode: 340, duration: 0.688s, episode steps:  37, steps per second:  54, episode reward: 44.198, mean reward:  1.195 [-2.004, 32.440], mean action: 3.514 [1.000, 11.000],  loss: 0.015542, mae: 0.274284, mean_q: 0.322014, mean_eps: 0.000000
 13068/30000: episode: 341, duration: 0.965s, episode steps:  51, steps per second:  53, episode reward: -35.440, mean reward: -0.695 [-32.179,  2.092], mean action: 6.902 [0.000, 21.000],  loss: 0.014123, mae: 0.270229, mean_q: 0.306188, mean_eps: 0.000000
 13092/30000: episode: 342, duration: 0.420s, episode steps:  24, steps per second:  57, episode reward: 38.286, mean reward:  1.595 [-2.578, 32.160], mean action: 4.833 [1.000, 15.000],  loss: 0.014754, mae: 0.309959, mean_q: 0.244972, mean_eps: 0.000000
 13129/30000: episode: 343, duration: 0.675s, episode steps:  37, steps per second:  55, episode reward: 41.218, mean reward:  1.114 [-2.267, 32.601], mean action: 3.757 [1.000, 15.000],  loss: 0.018370, mae: 0.285262, mean_q: 0.363593, mean_eps: 0.000000
 13144/30000: episode: 344, duration: 0.263s, episode steps:  15, steps per second:  57, episode reward: 42.000, mean reward:  2.800 [-2.337, 30.931], mean action: 2.667 [1.000, 12.000],  loss: 0.011642, mae: 0.265871, mean_q: 0.297262, mean_eps: 0.000000
 13172/30000: episode: 345, duration: 0.462s, episode steps:  28, steps per second:  61, episode reward: 34.696, mean reward:  1.239 [-2.645, 32.100], mean action: 8.857 [1.000, 21.000],  loss: 0.014065, mae: 0.265912, mean_q: 0.317015, mean_eps: 0.000000
 13205/30000: episode: 346, duration: 0.540s, episode steps:  33, steps per second:  61, episode reward: 32.901, mean reward:  0.997 [-3.000, 31.921], mean action: 6.758 [1.000, 21.000],  loss: 0.017852, mae: 0.310142, mean_q: 0.281171, mean_eps: 0.000000
 13256/30000: episode: 347, duration: 0.940s, episode steps:  51, steps per second:  54, episode reward: 32.209, mean reward:  0.632 [-2.810, 31.383], mean action: 11.294 [1.000, 21.000],  loss: 0.016196, mae: 0.281902, mean_q: 0.320994, mean_eps: 0.000000
 13289/30000: episode: 348, duration: 0.589s, episode steps:  33, steps per second:  56, episode reward: 32.780, mean reward:  0.993 [-2.970, 32.222], mean action: 3.394 [1.000, 16.000],  loss: 0.018774, mae: 0.294421, mean_q: 0.346520, mean_eps: 0.000000
 13336/30000: episode: 349, duration: 0.822s, episode steps:  47, steps per second:  57, episode reward: -32.550, mean reward: -0.693 [-31.996,  2.450], mean action: 3.830 [1.000, 16.000],  loss: 0.017063, mae: 0.300810, mean_q: 0.324843, mean_eps: 0.000000
 13368/30000: episode: 350, duration: 0.531s, episode steps:  32, steps per second:  60, episode reward: 41.576, mean reward:  1.299 [-2.299, 31.853], mean action: 3.000 [1.000, 15.000],  loss: 0.015141, mae: 0.289630, mean_q: 0.354516, mean_eps: 0.000000
 13389/30000: episode: 351, duration: 0.386s, episode steps:  21, steps per second:  54, episode reward: 40.710, mean reward:  1.939 [-2.876, 32.130], mean action: 5.143 [1.000, 15.000],  loss: 0.018461, mae: 0.317020, mean_q: 0.290201, mean_eps: 0.000000
 13421/30000: episode: 352, duration: 0.580s, episode steps:  32, steps per second:  55, episode reward: 44.254, mean reward:  1.383 [-2.110, 32.240], mean action: 3.562 [0.000, 16.000],  loss: 0.016787, mae: 0.307941, mean_q: 0.302159, mean_eps: 0.000000
 13497/30000: episode: 353, duration: 1.411s, episode steps:  76, steps per second:  54, episode reward: -34.510, mean reward: -0.454 [-32.105,  2.620], mean action: 4.750 [1.000, 14.000],  loss: 0.016533, mae: 0.281183, mean_q: 0.349094, mean_eps: 0.000000
 13547/30000: episode: 354, duration: 0.823s, episode steps:  50, steps per second:  61, episode reward: -32.650, mean reward: -0.653 [-31.938,  2.202], mean action: 4.800 [0.000, 14.000],  loss: 0.017328, mae: 0.278546, mean_q: 0.319411, mean_eps: 0.000000
 13583/30000: episode: 355, duration: 0.649s, episode steps:  36, steps per second:  55, episode reward: 38.122, mean reward:  1.059 [-2.376, 32.091], mean action: 2.139 [1.000, 12.000],  loss: 0.018218, mae: 0.295344, mean_q: 0.358576, mean_eps: 0.000000
 13624/30000: episode: 356, duration: 0.803s, episode steps:  41, steps per second:  51, episode reward: 41.478, mean reward:  1.012 [-2.623, 31.813], mean action: 2.756 [1.000, 11.000],  loss: 0.013237, mae: 0.285852, mean_q: 0.270078, mean_eps: 0.000000
 13648/30000: episode: 357, duration: 0.451s, episode steps:  24, steps per second:  53, episode reward: 41.159, mean reward:  1.715 [-2.342, 31.612], mean action: 2.500 [1.000, 11.000],  loss: 0.016285, mae: 0.266893, mean_q: 0.351495, mean_eps: 0.000000
 13665/30000: episode: 358, duration: 0.343s, episode steps:  17, steps per second:  50, episode reward: 40.928, mean reward:  2.408 [-2.585, 31.981], mean action: 4.118 [1.000, 11.000],  loss: 0.013595, mae: 0.262625, mean_q: 0.311792, mean_eps: 0.000000
 13692/30000: episode: 359, duration: 0.475s, episode steps:  27, steps per second:  57, episode reward: 45.000, mean reward:  1.667 [-2.010, 32.530], mean action: 1.407 [1.000, 12.000],  loss: 0.017884, mae: 0.283119, mean_q: 0.333805, mean_eps: 0.000000
 13745/30000: episode: 360, duration: 1.002s, episode steps:  53, steps per second:  53, episode reward: 32.768, mean reward:  0.618 [-2.304, 31.818], mean action: 4.434 [1.000, 21.000],  loss: 0.014747, mae: 0.281876, mean_q: 0.313240, mean_eps: 0.000000
 13836/30000: episode: 361, duration: 1.631s, episode steps:  91, steps per second:  56, episode reward: 35.269, mean reward:  0.388 [-2.947, 32.300], mean action: 3.945 [1.000, 21.000],  loss: 0.014043, mae: 0.281495, mean_q: 0.309917, mean_eps: 0.000000
 13864/30000: episode: 362, duration: 0.619s, episode steps:  28, steps per second:  45, episode reward: 38.094, mean reward:  1.361 [-2.303, 32.036], mean action: 4.286 [1.000, 21.000],  loss: 0.012549, mae: 0.287624, mean_q: 0.243971, mean_eps: 0.000000
 13901/30000: episode: 363, duration: 0.654s, episode steps:  37, steps per second:  57, episode reward: 33.000, mean reward:  0.892 [-3.000, 32.270], mean action: 4.189 [1.000, 21.000],  loss: 0.016100, mae: 0.286996, mean_q: 0.321225, mean_eps: 0.000000
 13927/30000: episode: 364, duration: 0.478s, episode steps:  26, steps per second:  54, episode reward: 41.223, mean reward:  1.586 [-2.703, 31.923], mean action: 4.538 [3.000, 16.000],  loss: 0.019756, mae: 0.330084, mean_q: 0.294481, mean_eps: 0.000000
 13967/30000: episode: 365, duration: 0.744s, episode steps:  40, steps per second:  54, episode reward: 35.485, mean reward:  0.887 [-3.000, 32.007], mean action: 4.750 [1.000, 21.000],  loss: 0.017760, mae: 0.297925, mean_q: 0.358617, mean_eps: 0.000000
 14000/30000: episode: 366, duration: 0.563s, episode steps:  33, steps per second:  59, episode reward: 38.027, mean reward:  1.152 [-2.402, 31.855], mean action: 6.636 [1.000, 21.000],  loss: 0.014060, mae: 0.286209, mean_q: 0.294866, mean_eps: 0.000000
 14029/30000: episode: 367, duration: 0.493s, episode steps:  29, steps per second:  59, episode reward: 41.286, mean reward:  1.424 [-2.918, 32.043], mean action: 4.655 [1.000, 18.000],  loss: 0.012113, mae: 0.277387, mean_q: 0.305796, mean_eps: 0.000000
 14059/30000: episode: 368, duration: 0.535s, episode steps:  30, steps per second:  56, episode reward: 38.605, mean reward:  1.287 [-2.463, 32.130], mean action: 5.200 [1.000, 16.000],  loss: 0.016780, mae: 0.296395, mean_q: 0.330835, mean_eps: 0.000000
 14085/30000: episode: 369, duration: 0.466s, episode steps:  26, steps per second:  56, episode reward: 43.935, mean reward:  1.690 [-2.130, 32.210], mean action: 3.923 [1.000, 11.000],  loss: 0.014359, mae: 0.291483, mean_q: 0.311988, mean_eps: 0.000000
 14110/30000: episode: 370, duration: 0.432s, episode steps:  25, steps per second:  58, episode reward: 47.059, mean reward:  1.882 [-0.318, 32.015], mean action: 2.840 [1.000, 10.000],  loss: 0.015833, mae: 0.293044, mean_q: 0.383605, mean_eps: 0.000000
 14154/30000: episode: 371, duration: 0.799s, episode steps:  44, steps per second:  55, episode reward: 32.128, mean reward:  0.730 [-2.878, 32.190], mean action: 4.591 [1.000, 17.000],  loss: 0.017613, mae: 0.318469, mean_q: 0.331758, mean_eps: 0.000000
 14204/30000: episode: 372, duration: 0.910s, episode steps:  50, steps per second:  55, episode reward: 35.839, mean reward:  0.717 [-2.423, 32.286], mean action: 11.560 [1.000, 21.000],  loss: 0.017819, mae: 0.312670, mean_q: 0.321641, mean_eps: 0.000000
 14224/30000: episode: 373, duration: 0.393s, episode steps:  20, steps per second:  51, episode reward: 44.341, mean reward:  2.217 [-2.256, 32.410], mean action: 6.550 [1.000, 14.000],  loss: 0.012212, mae: 0.295503, mean_q: 0.342288, mean_eps: 0.000000
 14241/30000: episode: 374, duration: 0.331s, episode steps:  17, steps per second:  51, episode reward: 41.110, mean reward:  2.418 [-2.815, 31.975], mean action: 4.471 [1.000, 17.000],  loss: 0.015179, mae: 0.283516, mean_q: 0.397070, mean_eps: 0.000000
 14279/30000: episode: 375, duration: 0.845s, episode steps:  38, steps per second:  45, episode reward: 39.093, mean reward:  1.029 [-2.473, 32.261], mean action: 7.368 [1.000, 21.000],  loss: 0.015542, mae: 0.315652, mean_q: 0.258601, mean_eps: 0.000000
 14312/30000: episode: 376, duration: 0.682s, episode steps:  33, steps per second:  48, episode reward: 44.681, mean reward:  1.354 [-2.237, 32.500], mean action: 1.909 [1.000, 15.000],  loss: 0.016552, mae: 0.325552, mean_q: 0.290015, mean_eps: 0.000000
 14333/30000: episode: 377, duration: 0.430s, episode steps:  21, steps per second:  49, episode reward: 43.746, mean reward:  2.083 [-3.000, 32.740], mean action: 6.571 [1.000, 17.000],  loss: 0.017552, mae: 0.303194, mean_q: 0.310812, mean_eps: 0.000000
 14373/30000: episode: 378, duration: 0.801s, episode steps:  40, steps per second:  50, episode reward: 38.190, mean reward:  0.955 [-3.000, 32.131], mean action: 7.075 [1.000, 21.000],  loss: 0.015667, mae: 0.299553, mean_q: 0.304473, mean_eps: 0.000000
 14417/30000: episode: 379, duration: 0.807s, episode steps:  44, steps per second:  55, episode reward: 34.550, mean reward:  0.785 [-3.000, 32.110], mean action: 6.023 [1.000, 15.000],  loss: 0.016446, mae: 0.301690, mean_q: 0.292755, mean_eps: 0.000000
 14445/30000: episode: 380, duration: 0.530s, episode steps:  28, steps per second:  53, episode reward: 45.000, mean reward:  1.607 [-2.049, 32.320], mean action: 2.250 [1.000, 15.000],  loss: 0.014254, mae: 0.289405, mean_q: 0.311020, mean_eps: 0.000000
 14502/30000: episode: 381, duration: 1.111s, episode steps:  57, steps per second:  51, episode reward: 34.290, mean reward:  0.602 [-2.634, 32.650], mean action: 7.526 [1.000, 21.000],  loss: 0.016601, mae: 0.294835, mean_q: 0.328768, mean_eps: 0.000000
 14534/30000: episode: 382, duration: 0.653s, episode steps:  32, steps per second:  49, episode reward: 38.463, mean reward:  1.202 [-2.664, 32.110], mean action: 4.156 [1.000, 14.000],  loss: 0.017296, mae: 0.289874, mean_q: 0.328439, mean_eps: 0.000000
 14585/30000: episode: 383, duration: 1.027s, episode steps:  51, steps per second:  50, episode reward: -32.440, mean reward: -0.636 [-32.630,  2.480], mean action: 9.863 [1.000, 21.000],  loss: 0.017052, mae: 0.286142, mean_q: 0.364607, mean_eps: 0.000000
 14632/30000: episode: 384, duration: 0.880s, episode steps:  47, steps per second:  53, episode reward: 32.619, mean reward:  0.694 [-2.598, 32.454], mean action: 4.191 [1.000, 17.000],  loss: 0.015864, mae: 0.280331, mean_q: 0.350836, mean_eps: 0.000000
 14668/30000: episode: 385, duration: 0.632s, episode steps:  36, steps per second:  57, episode reward: 41.385, mean reward:  1.150 [-2.494, 31.595], mean action: 3.639 [1.000, 21.000],  loss: 0.017355, mae: 0.302386, mean_q: 0.302782, mean_eps: 0.000000
 14699/30000: episode: 386, duration: 0.648s, episode steps:  31, steps per second:  48, episode reward: 32.712, mean reward:  1.055 [-2.903, 31.867], mean action: 4.452 [1.000, 17.000],  loss: 0.013862, mae: 0.291658, mean_q: 0.319900, mean_eps: 0.000000
 14721/30000: episode: 387, duration: 0.409s, episode steps:  22, steps per second:  54, episode reward: 38.589, mean reward:  1.754 [-2.710, 31.957], mean action: 5.409 [1.000, 15.000],  loss: 0.015471, mae: 0.291936, mean_q: 0.330594, mean_eps: 0.000000
 14754/30000: episode: 388, duration: 0.632s, episode steps:  33, steps per second:  52, episode reward: 36.000, mean reward:  1.091 [-2.455, 30.213], mean action: 3.758 [1.000, 15.000],  loss: 0.016080, mae: 0.295488, mean_q: 0.316343, mean_eps: 0.000000
 14790/30000: episode: 389, duration: 0.639s, episode steps:  36, steps per second:  56, episode reward: 32.781, mean reward:  0.911 [-2.817, 32.223], mean action: 4.278 [1.000, 16.000],  loss: 0.016497, mae: 0.297291, mean_q: 0.306835, mean_eps: 0.000000
 14815/30000: episode: 390, duration: 0.443s, episode steps:  25, steps per second:  56, episode reward: 35.672, mean reward:  1.427 [-3.000, 32.059], mean action: 7.080 [1.000, 18.000],  loss: 0.016175, mae: 0.290424, mean_q: 0.306841, mean_eps: 0.000000
 14860/30000: episode: 391, duration: 0.854s, episode steps:  45, steps per second:  53, episode reward: 37.879, mean reward:  0.842 [-2.420, 32.209], mean action: 5.422 [1.000, 16.000],  loss: 0.013513, mae: 0.281091, mean_q: 0.322513, mean_eps: 0.000000
 14955/30000: episode: 392, duration: 1.794s, episode steps:  95, steps per second:  53, episode reward: -32.840, mean reward: -0.346 [-32.404,  2.370], mean action: 6.905 [1.000, 21.000],  loss: 0.015240, mae: 0.290612, mean_q: 0.304750, mean_eps: 0.000000
 15012/30000: episode: 393, duration: 1.044s, episode steps:  57, steps per second:  55, episode reward: -34.650, mean reward: -0.608 [-32.343,  2.450], mean action: 14.649 [1.000, 21.000],  loss: 0.016139, mae: 0.288221, mean_q: 0.309661, mean_eps: 0.000000
 15054/30000: episode: 394, duration: 0.769s, episode steps:  42, steps per second:  55, episode reward: 43.655, mean reward:  1.039 [-2.194, 32.292], mean action: 3.976 [1.000, 15.000],  loss: 0.014378, mae: 0.287060, mean_q: 0.298681, mean_eps: 0.000000
 15080/30000: episode: 395, duration: 0.504s, episode steps:  26, steps per second:  52, episode reward: 41.621, mean reward:  1.601 [-2.060, 32.230], mean action: 2.231 [1.000, 12.000],  loss: 0.016934, mae: 0.315237, mean_q: 0.264321, mean_eps: 0.000000
 15121/30000: episode: 396, duration: 0.852s, episode steps:  41, steps per second:  48, episode reward: 40.399, mean reward:  0.985 [-2.641, 32.384], mean action: 3.463 [1.000, 14.000],  loss: 0.016689, mae: 0.306378, mean_q: 0.302727, mean_eps: 0.000000
 15172/30000: episode: 397, duration: 1.121s, episode steps:  51, steps per second:  45, episode reward: 42.000, mean reward:  0.824 [-2.250, 32.090], mean action: 2.863 [1.000, 16.000],  loss: 0.015966, mae: 0.297713, mean_q: 0.309845, mean_eps: 0.000000
 15192/30000: episode: 398, duration: 0.397s, episode steps:  20, steps per second:  50, episode reward: 41.625, mean reward:  2.081 [-2.603, 32.240], mean action: 3.750 [1.000, 21.000],  loss: 0.014367, mae: 0.289721, mean_q: 0.294066, mean_eps: 0.000000
 15241/30000: episode: 399, duration: 0.832s, episode steps:  49, steps per second:  59, episode reward: 35.613, mean reward:  0.727 [-3.000, 32.250], mean action: 3.429 [1.000, 15.000],  loss: 0.015530, mae: 0.291612, mean_q: 0.332002, mean_eps: 0.000000
 15280/30000: episode: 400, duration: 0.646s, episode steps:  39, steps per second:  60, episode reward: 37.887, mean reward:  0.971 [-2.899, 31.494], mean action: 4.949 [1.000, 16.000],  loss: 0.018605, mae: 0.301777, mean_q: 0.311046, mean_eps: 0.000000
 15317/30000: episode: 401, duration: 0.593s, episode steps:  37, steps per second:  62, episode reward: 38.394, mean reward:  1.038 [-2.557, 32.620], mean action: 3.622 [1.000, 16.000],  loss: 0.013167, mae: 0.291963, mean_q: 0.310726, mean_eps: 0.000000
 15371/30000: episode: 402, duration: 0.943s, episode steps:  54, steps per second:  57, episode reward: 40.381, mean reward:  0.748 [-2.302, 32.100], mean action: 2.852 [1.000, 16.000],  loss: 0.013510, mae: 0.289520, mean_q: 0.321443, mean_eps: 0.000000
 15398/30000: episode: 403, duration: 0.471s, episode steps:  27, steps per second:  57, episode reward: 33.000, mean reward:  1.222 [-3.000, 32.160], mean action: 6.148 [1.000, 21.000],  loss: 0.016048, mae: 0.317249, mean_q: 0.282290, mean_eps: 0.000000
 15430/30000: episode: 404, duration: 0.542s, episode steps:  32, steps per second:  59, episode reward: 41.855, mean reward:  1.308 [-3.000, 32.090], mean action: 6.438 [1.000, 16.000],  loss: 0.015623, mae: 0.293744, mean_q: 0.302456, mean_eps: 0.000000
 15452/30000: episode: 405, duration: 0.374s, episode steps:  22, steps per second:  59, episode reward: 44.545, mean reward:  2.025 [-2.069, 31.931], mean action: 2.273 [1.000, 14.000],  loss: 0.012607, mae: 0.281326, mean_q: 0.302595, mean_eps: 0.000000
 15480/30000: episode: 406, duration: 0.518s, episode steps:  28, steps per second:  54, episode reward: 35.558, mean reward:  1.270 [-2.845, 32.120], mean action: 5.429 [1.000, 21.000],  loss: 0.016110, mae: 0.316838, mean_q: 0.266415, mean_eps: 0.000000
 15505/30000: episode: 407, duration: 0.470s, episode steps:  25, steps per second:  53, episode reward: 42.000, mean reward:  1.680 [-2.142, 32.070], mean action: 3.080 [1.000, 11.000],  loss: 0.017163, mae: 0.303997, mean_q: 0.329220, mean_eps: 0.000000
 15532/30000: episode: 408, duration: 0.702s, episode steps:  27, steps per second:  38, episode reward: 43.987, mean reward:  1.629 [-2.443, 31.868], mean action: 4.889 [1.000, 15.000],  loss: 0.017287, mae: 0.315956, mean_q: 0.260494, mean_eps: 0.000000
 15556/30000: episode: 409, duration: 0.459s, episode steps:  24, steps per second:  52, episode reward: 38.042, mean reward:  1.585 [-2.840, 32.316], mean action: 8.250 [1.000, 15.000],  loss: 0.016537, mae: 0.289071, mean_q: 0.330210, mean_eps: 0.000000
 15589/30000: episode: 410, duration: 0.597s, episode steps:  33, steps per second:  55, episode reward: -38.490, mean reward: -1.166 [-32.586,  2.261], mean action: 4.273 [1.000, 16.000],  loss: 0.020102, mae: 0.314520, mean_q: 0.304120, mean_eps: 0.000000
 15609/30000: episode: 411, duration: 0.394s, episode steps:  20, steps per second:  51, episode reward: 43.597, mean reward:  2.180 [-2.039, 34.912], mean action: 5.700 [1.000, 21.000],  loss: 0.015049, mae: 0.296997, mean_q: 0.329669, mean_eps: 0.000000
 15631/30000: episode: 412, duration: 0.466s, episode steps:  22, steps per second:  47, episode reward: 39.000, mean reward:  1.773 [-3.000, 32.030], mean action: 4.045 [1.000, 16.000],  loss: 0.014661, mae: 0.320867, mean_q: 0.262207, mean_eps: 0.000000
 15661/30000: episode: 413, duration: 0.565s, episode steps:  30, steps per second:  53, episode reward: 41.145, mean reward:  1.371 [-2.189, 32.103], mean action: 4.033 [1.000, 15.000],  loss: 0.015282, mae: 0.299985, mean_q: 0.283957, mean_eps: 0.000000
 15716/30000: episode: 414, duration: 1.018s, episode steps:  55, steps per second:  54, episode reward: 37.522, mean reward:  0.682 [-2.567, 32.300], mean action: 3.309 [1.000, 15.000],  loss: 0.016908, mae: 0.296627, mean_q: 0.333610, mean_eps: 0.000000
 15761/30000: episode: 415, duration: 0.738s, episode steps:  45, steps per second:  61, episode reward: 32.840, mean reward:  0.730 [-2.689, 32.300], mean action: 7.556 [3.000, 21.000],  loss: 0.016585, mae: 0.308204, mean_q: 0.296408, mean_eps: 0.000000
 15789/30000: episode: 416, duration: 0.477s, episode steps:  28, steps per second:  59, episode reward: 41.876, mean reward:  1.496 [-2.502, 32.066], mean action: 3.500 [1.000, 15.000],  loss: 0.009941, mae: 0.277313, mean_q: 0.267435, mean_eps: 0.000000
 15804/30000: episode: 417, duration: 0.267s, episode steps:  15, steps per second:  56, episode reward: 47.223, mean reward:  3.148 [-0.218, 32.210], mean action: 1.800 [1.000, 3.000],  loss: 0.013553, mae: 0.290299, mean_q: 0.285346, mean_eps: 0.000000
 15845/30000: episode: 418, duration: 0.753s, episode steps:  41, steps per second:  54, episode reward: 44.662, mean reward:  1.089 [-2.505, 32.044], mean action: 2.854 [1.000, 21.000],  loss: 0.015864, mae: 0.300714, mean_q: 0.350853, mean_eps: 0.000000
 15870/30000: episode: 419, duration: 0.475s, episode steps:  25, steps per second:  53, episode reward: 41.689, mean reward:  1.668 [-2.840, 32.200], mean action: 4.800 [1.000, 14.000],  loss: 0.020430, mae: 0.326987, mean_q: 0.311524, mean_eps: 0.000000
 15908/30000: episode: 420, duration: 0.614s, episode steps:  38, steps per second:  62, episode reward: 41.938, mean reward:  1.104 [-2.263, 32.470], mean action: 3.263 [1.000, 15.000],  loss: 0.014696, mae: 0.294996, mean_q: 0.280444, mean_eps: 0.000000
 15958/30000: episode: 421, duration: 1.003s, episode steps:  50, steps per second:  50, episode reward: 40.615, mean reward:  0.812 [-2.564, 32.020], mean action: 4.660 [1.000, 21.000],  loss: 0.015279, mae: 0.302081, mean_q: 0.270846, mean_eps: 0.000000
 15985/30000: episode: 422, duration: 0.501s, episode steps:  27, steps per second:  54, episode reward: 41.246, mean reward:  1.528 [-2.248, 32.220], mean action: 5.148 [1.000, 21.000],  loss: 0.017835, mae: 0.314733, mean_q: 0.276817, mean_eps: 0.000000
 16013/30000: episode: 423, duration: 0.552s, episode steps:  28, steps per second:  51, episode reward: 41.023, mean reward:  1.465 [-3.000, 31.832], mean action: 7.214 [1.000, 15.000],  loss: 0.015161, mae: 0.306657, mean_q: 0.262425, mean_eps: 0.000000
 16044/30000: episode: 424, duration: 0.555s, episode steps:  31, steps per second:  56, episode reward: 34.926, mean reward:  1.127 [-2.641, 31.953], mean action: 5.548 [0.000, 21.000],  loss: 0.016262, mae: 0.305215, mean_q: 0.308689, mean_eps: 0.000000
 16065/30000: episode: 425, duration: 0.410s, episode steps:  21, steps per second:  51, episode reward: 44.547, mean reward:  2.121 [-2.234, 32.240], mean action: 4.571 [1.000, 15.000],  loss: 0.020114, mae: 0.297061, mean_q: 0.403998, mean_eps: 0.000000
 16092/30000: episode: 426, duration: 0.492s, episode steps:  27, steps per second:  55, episode reward: 33.000, mean reward:  1.222 [-3.000, 33.000], mean action: 4.889 [1.000, 14.000],  loss: 0.016449, mae: 0.290421, mean_q: 0.317143, mean_eps: 0.000000
 16134/30000: episode: 427, duration: 0.762s, episode steps:  42, steps per second:  55, episode reward: -35.500, mean reward: -0.845 [-32.377,  2.180], mean action: 6.738 [1.000, 21.000],  loss: 0.014818, mae: 0.281676, mean_q: 0.338005, mean_eps: 0.000000
 16151/30000: episode: 428, duration: 0.328s, episode steps:  17, steps per second:  52, episode reward: 45.000, mean reward:  2.647 [-2.823, 32.250], mean action: 2.882 [1.000, 21.000],  loss: 0.018140, mae: 0.309067, mean_q: 0.278934, mean_eps: 0.000000
 16177/30000: episode: 429, duration: 0.492s, episode steps:  26, steps per second:  53, episode reward: 44.683, mean reward:  1.719 [-2.698, 32.070], mean action: 1.654 [1.000, 12.000],  loss: 0.017578, mae: 0.311731, mean_q: 0.316864, mean_eps: 0.000000
 16204/30000: episode: 430, duration: 0.474s, episode steps:  27, steps per second:  57, episode reward: 38.326, mean reward:  1.419 [-3.000, 32.010], mean action: 4.778 [1.000, 15.000],  loss: 0.016051, mae: 0.292794, mean_q: 0.338833, mean_eps: 0.000000
 16232/30000: episode: 431, duration: 0.505s, episode steps:  28, steps per second:  55, episode reward: 40.507, mean reward:  1.447 [-2.111, 32.613], mean action: 3.929 [1.000, 15.000],  loss: 0.017109, mae: 0.311225, mean_q: 0.310567, mean_eps: 0.000000
 16254/30000: episode: 432, duration: 0.387s, episode steps:  22, steps per second:  57, episode reward: 38.550, mean reward:  1.752 [-3.000, 32.000], mean action: 4.227 [1.000, 11.000],  loss: 0.014419, mae: 0.292786, mean_q: 0.303122, mean_eps: 0.000000
 16285/30000: episode: 433, duration: 0.624s, episode steps:  31, steps per second:  50, episode reward: 44.059, mean reward:  1.421 [-2.046, 32.088], mean action: 5.226 [1.000, 15.000],  loss: 0.015489, mae: 0.288331, mean_q: 0.298848, mean_eps: 0.000000
 16307/30000: episode: 434, duration: 0.437s, episode steps:  22, steps per second:  50, episode reward: 41.148, mean reward:  1.870 [-2.021, 31.857], mean action: 6.318 [1.000, 15.000],  loss: 0.013138, mae: 0.283553, mean_q: 0.285404, mean_eps: 0.000000
 16356/30000: episode: 435, duration: 0.874s, episode steps:  49, steps per second:  56, episode reward: 35.221, mean reward:  0.719 [-2.947, 32.010], mean action: 4.061 [1.000, 21.000],  loss: 0.017053, mae: 0.290250, mean_q: 0.345704, mean_eps: 0.000000
 16393/30000: episode: 436, duration: 0.710s, episode steps:  37, steps per second:  52, episode reward: 38.396, mean reward:  1.038 [-3.000, 32.195], mean action: 3.784 [1.000, 18.000],  loss: 0.013662, mae: 0.289249, mean_q: 0.303014, mean_eps: 0.000000
 16446/30000: episode: 437, duration: 1.125s, episode steps:  53, steps per second:  47, episode reward: 40.648, mean reward:  0.767 [-2.475, 32.263], mean action: 3.472 [1.000, 15.000],  loss: 0.013630, mae: 0.288826, mean_q: 0.311882, mean_eps: 0.000000
 16482/30000: episode: 438, duration: 0.716s, episode steps:  36, steps per second:  50, episode reward: 35.570, mean reward:  0.988 [-3.000, 32.470], mean action: 6.861 [1.000, 18.000],  loss: 0.017188, mae: 0.288547, mean_q: 0.351586, mean_eps: 0.000000
 16522/30000: episode: 439, duration: 0.685s, episode steps:  40, steps per second:  58, episode reward: 33.000, mean reward:  0.825 [-3.000, 30.060], mean action: 4.575 [1.000, 15.000],  loss: 0.013317, mae: 0.280384, mean_q: 0.286986, mean_eps: 0.000000
 16565/30000: episode: 440, duration: 0.731s, episode steps:  43, steps per second:  59, episode reward: -32.130, mean reward: -0.747 [-32.431,  2.260], mean action: 9.233 [1.000, 21.000],  loss: 0.015862, mae: 0.299806, mean_q: 0.284700, mean_eps: 0.000000
 16599/30000: episode: 441, duration: 0.555s, episode steps:  34, steps per second:  61, episode reward: 32.550, mean reward:  0.957 [-2.876, 32.130], mean action: 4.794 [1.000, 16.000],  loss: 0.013143, mae: 0.289794, mean_q: 0.279986, mean_eps: 0.000000
 16627/30000: episode: 442, duration: 0.491s, episode steps:  28, steps per second:  57, episode reward: 42.000, mean reward:  1.500 [-2.275, 32.170], mean action: 2.143 [0.000, 8.000],  loss: 0.019535, mae: 0.320848, mean_q: 0.263990, mean_eps: 0.000000
 16648/30000: episode: 443, duration: 0.383s, episode steps:  21, steps per second:  55, episode reward: 44.038, mean reward:  2.097 [-2.128, 32.370], mean action: 3.000 [1.000, 8.000],  loss: 0.016971, mae: 0.302836, mean_q: 0.305426, mean_eps: 0.000000
 16674/30000: episode: 444, duration: 0.659s, episode steps:  26, steps per second:  39, episode reward: 35.658, mean reward:  1.371 [-2.664, 31.904], mean action: 8.038 [1.000, 21.000],  loss: 0.013074, mae: 0.277504, mean_q: 0.317639, mean_eps: 0.000000
 16707/30000: episode: 445, duration: 0.723s, episode steps:  33, steps per second:  46, episode reward: 36.000, mean reward:  1.091 [-3.000, 32.320], mean action: 4.758 [1.000, 14.000],  loss: 0.016664, mae: 0.294506, mean_q: 0.351166, mean_eps: 0.000000
 16741/30000: episode: 446, duration: 0.620s, episode steps:  34, steps per second:  55, episode reward: 44.526, mean reward:  1.310 [-2.016, 32.400], mean action: 4.647 [1.000, 21.000],  loss: 0.017100, mae: 0.299483, mean_q: 0.296766, mean_eps: 0.000000
 16753/30000: episode: 447, duration: 0.264s, episode steps:  12, steps per second:  45, episode reward: 47.057, mean reward:  3.921 [-0.604, 32.800], mean action: 8.000 [1.000, 21.000],  loss: 0.016133, mae: 0.301398, mean_q: 0.307409, mean_eps: 0.000000
 16782/30000: episode: 448, duration: 0.538s, episode steps:  29, steps per second:  54, episode reward: 32.187, mean reward:  1.110 [-3.000, 31.812], mean action: 6.034 [0.000, 15.000],  loss: 0.015421, mae: 0.291985, mean_q: 0.308007, mean_eps: 0.000000
 16801/30000: episode: 449, duration: 0.365s, episode steps:  19, steps per second:  52, episode reward: 41.032, mean reward:  2.160 [-2.810, 32.310], mean action: 6.000 [1.000, 17.000],  loss: 0.014803, mae: 0.287113, mean_q: 0.366216, mean_eps: 0.000000
 16833/30000: episode: 450, duration: 0.629s, episode steps:  32, steps per second:  51, episode reward: 36.000, mean reward:  1.125 [-2.303, 32.180], mean action: 4.188 [1.000, 15.000],  loss: 0.014382, mae: 0.301229, mean_q: 0.279086, mean_eps: 0.000000
 16864/30000: episode: 451, duration: 0.643s, episode steps:  31, steps per second:  48, episode reward: 38.967, mean reward:  1.257 [-2.452, 32.100], mean action: 4.194 [1.000, 18.000],  loss: 0.014183, mae: 0.281235, mean_q: 0.328224, mean_eps: 0.000000
 16892/30000: episode: 452, duration: 0.560s, episode steps:  28, steps per second:  50, episode reward: 38.433, mean reward:  1.373 [-2.612, 32.143], mean action: 6.607 [1.000, 15.000],  loss: 0.013926, mae: 0.283737, mean_q: 0.307591, mean_eps: 0.000000
 16927/30000: episode: 453, duration: 0.739s, episode steps:  35, steps per second:  47, episode reward: 40.518, mean reward:  1.158 [-3.000, 31.734], mean action: 2.829 [1.000, 12.000],  loss: 0.015455, mae: 0.304250, mean_q: 0.301411, mean_eps: 0.000000
 16950/30000: episode: 454, duration: 0.447s, episode steps:  23, steps per second:  51, episode reward: 41.426, mean reward:  1.801 [-3.000, 31.871], mean action: 3.478 [1.000, 12.000],  loss: 0.016659, mae: 0.306312, mean_q: 0.298111, mean_eps: 0.000000
 16972/30000: episode: 455, duration: 0.401s, episode steps:  22, steps per second:  55, episode reward: 38.661, mean reward:  1.757 [-3.000, 32.200], mean action: 4.500 [1.000, 11.000],  loss: 0.016412, mae: 0.287553, mean_q: 0.309274, mean_eps: 0.000000
 17010/30000: episode: 456, duration: 0.644s, episode steps:  38, steps per second:  59, episode reward: 35.730, mean reward:  0.940 [-2.777, 31.943], mean action: 3.947 [1.000, 16.000],  loss: 0.014834, mae: 0.291022, mean_q: 0.326814, mean_eps: 0.000000
 17058/30000: episode: 457, duration: 0.830s, episode steps:  48, steps per second:  58, episode reward: 37.704, mean reward:  0.785 [-3.000, 31.456], mean action: 3.583 [1.000, 15.000],  loss: 0.015290, mae: 0.296799, mean_q: 0.283319, mean_eps: 0.000000
 17120/30000: episode: 458, duration: 1.198s, episode steps:  62, steps per second:  52, episode reward: -32.540, mean reward: -0.525 [-32.273,  2.543], mean action: 7.742 [1.000, 17.000],  loss: 0.014959, mae: 0.292504, mean_q: 0.291613, mean_eps: 0.000000
 17151/30000: episode: 459, duration: 0.552s, episode steps:  31, steps per second:  56, episode reward: 38.011, mean reward:  1.226 [-3.000, 32.110], mean action: 3.484 [1.000, 15.000],  loss: 0.015866, mae: 0.290346, mean_q: 0.299326, mean_eps: 0.000000
 17169/30000: episode: 460, duration: 0.331s, episode steps:  18, steps per second:  54, episode reward: 41.798, mean reward:  2.322 [-2.480, 31.998], mean action: 1.778 [1.000, 6.000],  loss: 0.017122, mae: 0.295442, mean_q: 0.294714, mean_eps: 0.000000
 17200/30000: episode: 461, duration: 0.536s, episode steps:  31, steps per second:  58, episode reward: 35.005, mean reward:  1.129 [-2.400, 29.153], mean action: 4.355 [1.000, 15.000],  loss: 0.014279, mae: 0.291003, mean_q: 0.293957, mean_eps: 0.000000
 17247/30000: episode: 462, duration: 0.945s, episode steps:  47, steps per second:  50, episode reward: 42.000, mean reward:  0.894 [-2.644, 32.620], mean action: 2.766 [1.000, 15.000],  loss: 0.016289, mae: 0.311278, mean_q: 0.253871, mean_eps: 0.000000
 17279/30000: episode: 463, duration: 0.585s, episode steps:  32, steps per second:  55, episode reward: -32.090, mean reward: -1.003 [-32.512,  2.240], mean action: 5.062 [1.000, 21.000],  loss: 0.015362, mae: 0.302925, mean_q: 0.261458, mean_eps: 0.000000
 17311/30000: episode: 464, duration: 0.853s, episode steps:  32, steps per second:  38, episode reward: 41.368, mean reward:  1.293 [-2.205, 32.210], mean action: 5.156 [1.000, 16.000],  loss: 0.017443, mae: 0.299209, mean_q: 0.305432, mean_eps: 0.000000
 17340/30000: episode: 465, duration: 0.524s, episode steps:  29, steps per second:  55, episode reward: 39.000, mean reward:  1.345 [-2.710, 29.981], mean action: 4.034 [0.000, 15.000],  loss: 0.016178, mae: 0.293068, mean_q: 0.315395, mean_eps: 0.000000
 17402/30000: episode: 466, duration: 1.094s, episode steps:  62, steps per second:  57, episode reward: 32.860, mean reward:  0.530 [-3.000, 29.640], mean action: 8.645 [1.000, 21.000],  loss: 0.015787, mae: 0.289361, mean_q: 0.305832, mean_eps: 0.000000
 17423/30000: episode: 467, duration: 0.389s, episode steps:  21, steps per second:  54, episode reward: 44.422, mean reward:  2.115 [-2.037, 32.200], mean action: 4.810 [1.000, 15.000],  loss: 0.018545, mae: 0.298381, mean_q: 0.273450, mean_eps: 0.000000
 17462/30000: episode: 468, duration: 0.676s, episode steps:  39, steps per second:  58, episode reward: 41.461, mean reward:  1.063 [-2.191, 32.494], mean action: 4.308 [1.000, 17.000],  loss: 0.017371, mae: 0.283643, mean_q: 0.339870, mean_eps: 0.000000
 17477/30000: episode: 469, duration: 0.312s, episode steps:  15, steps per second:  48, episode reward: 47.351, mean reward:  3.157 [ 0.000, 32.620], mean action: 3.533 [1.000, 17.000],  loss: 0.016390, mae: 0.286316, mean_q: 0.345363, mean_eps: 0.000000
 17501/30000: episode: 470, duration: 0.461s, episode steps:  24, steps per second:  52, episode reward: 38.524, mean reward:  1.605 [-3.000, 32.140], mean action: 4.708 [1.000, 17.000],  loss: 0.014645, mae: 0.279203, mean_q: 0.323636, mean_eps: 0.000000
 17545/30000: episode: 471, duration: 0.849s, episode steps:  44, steps per second:  52, episode reward: -35.030, mean reward: -0.796 [-32.061,  2.360], mean action: 6.136 [0.000, 21.000],  loss: 0.014094, mae: 0.278433, mean_q: 0.292280, mean_eps: 0.000000
 17568/30000: episode: 472, duration: 0.485s, episode steps:  23, steps per second:  47, episode reward: 44.377, mean reward:  1.929 [-2.128, 32.390], mean action: 3.087 [1.000, 16.000],  loss: 0.013609, mae: 0.293458, mean_q: 0.270066, mean_eps: 0.000000
 17583/30000: episode: 473, duration: 0.285s, episode steps:  15, steps per second:  53, episode reward: 39.000, mean reward:  2.600 [-3.000, 32.360], mean action: 6.267 [1.000, 16.000],  loss: 0.018671, mae: 0.297549, mean_q: 0.366675, mean_eps: 0.000000
 17610/30000: episode: 474, duration: 0.506s, episode steps:  27, steps per second:  53, episode reward: 37.777, mean reward:  1.399 [-3.000, 31.922], mean action: 3.963 [1.000, 16.000],  loss: 0.016115, mae: 0.290523, mean_q: 0.314250, mean_eps: 0.000000
 17644/30000: episode: 475, duration: 0.654s, episode steps:  34, steps per second:  52, episode reward: 35.289, mean reward:  1.038 [-2.544, 31.611], mean action: 5.118 [1.000, 21.000],  loss: 0.015275, mae: 0.289573, mean_q: 0.320101, mean_eps: 0.000000
 17677/30000: episode: 476, duration: 0.591s, episode steps:  33, steps per second:  56, episode reward: 38.625, mean reward:  1.170 [-2.360, 32.090], mean action: 3.667 [3.000, 12.000],  loss: 0.018952, mae: 0.300166, mean_q: 0.353004, mean_eps: 0.000000
 17712/30000: episode: 477, duration: 0.583s, episode steps:  35, steps per second:  60, episode reward: 38.013, mean reward:  1.086 [-3.000, 32.123], mean action: 2.429 [1.000, 11.000],  loss: 0.014169, mae: 0.284935, mean_q: 0.324574, mean_eps: 0.000000
 17736/30000: episode: 478, duration: 0.422s, episode steps:  24, steps per second:  57, episode reward: 41.866, mean reward:  1.744 [-2.335, 32.116], mean action: 1.792 [1.000, 11.000],  loss: 0.013932, mae: 0.288602, mean_q: 0.301457, mean_eps: 0.000000
 17763/30000: episode: 479, duration: 0.508s, episode steps:  27, steps per second:  53, episode reward: 43.488, mean reward:  1.611 [-2.259, 32.210], mean action: 3.370 [1.000, 12.000],  loss: 0.015900, mae: 0.298459, mean_q: 0.321886, mean_eps: 0.000000
 17797/30000: episode: 480, duration: 0.622s, episode steps:  34, steps per second:  55, episode reward: 44.646, mean reward:  1.313 [-2.128, 32.293], mean action: 2.559 [1.000, 12.000],  loss: 0.014207, mae: 0.292791, mean_q: 0.326829, mean_eps: 0.000000
 17846/30000: episode: 481, duration: 0.909s, episode steps:  49, steps per second:  54, episode reward: 37.048, mean reward:  0.756 [-3.000, 32.109], mean action: 7.041 [1.000, 21.000],  loss: 0.016178, mae: 0.311598, mean_q: 0.267958, mean_eps: 0.000000
 17953/30000: episode: 482, duration: 1.928s, episode steps: 107, steps per second:  56, episode reward: 32.834, mean reward:  0.307 [-2.460, 32.027], mean action: 15.879 [1.000, 21.000],  loss: 0.015836, mae: 0.300892, mean_q: 0.295388, mean_eps: 0.000000
 17989/30000: episode: 483, duration: 0.798s, episode steps:  36, steps per second:  45, episode reward: 38.095, mean reward:  1.058 [-2.145, 32.780], mean action: 4.056 [1.000, 18.000],  loss: 0.016369, mae: 0.306830, mean_q: 0.286635, mean_eps: 0.000000
 18019/30000: episode: 484, duration: 0.724s, episode steps:  30, steps per second:  41, episode reward: -43.090, mean reward: -1.436 [-32.579,  2.250], mean action: 6.400 [1.000, 14.000],  loss: 0.014811, mae: 0.304325, mean_q: 0.303412, mean_eps: 0.000000
 18055/30000: episode: 485, duration: 0.767s, episode steps:  36, steps per second:  47, episode reward: 40.687, mean reward:  1.130 [-3.000, 31.853], mean action: 4.083 [1.000, 15.000],  loss: 0.014177, mae: 0.303837, mean_q: 0.273491, mean_eps: 0.000000
 18111/30000: episode: 486, duration: 1.113s, episode steps:  56, steps per second:  50, episode reward: 32.641, mean reward:  0.583 [-2.253, 32.390], mean action: 9.679 [1.000, 21.000],  loss: 0.015349, mae: 0.296433, mean_q: 0.309810, mean_eps: 0.000000
 18132/30000: episode: 487, duration: 0.403s, episode steps:  21, steps per second:  52, episode reward: 47.744, mean reward:  2.274 [-0.096, 33.000], mean action: 1.000 [1.000, 1.000],  loss: 0.018105, mae: 0.311696, mean_q: 0.280073, mean_eps: 0.000000
 18187/30000: episode: 488, duration: 1.037s, episode steps:  55, steps per second:  53, episode reward: 41.400, mean reward:  0.753 [-2.468, 32.310], mean action: 5.473 [1.000, 21.000],  loss: 0.015530, mae: 0.294811, mean_q: 0.274269, mean_eps: 0.000000
 18211/30000: episode: 489, duration: 0.446s, episode steps:  24, steps per second:  54, episode reward: 41.533, mean reward:  1.731 [-3.000, 32.100], mean action: 5.125 [1.000, 15.000],  loss: 0.016926, mae: 0.305864, mean_q: 0.305221, mean_eps: 0.000000
 18239/30000: episode: 490, duration: 0.562s, episode steps:  28, steps per second:  50, episode reward: 41.521, mean reward:  1.483 [-2.495, 32.150], mean action: 3.714 [1.000, 16.000],  loss: 0.015520, mae: 0.305110, mean_q: 0.306813, mean_eps: 0.000000
 18261/30000: episode: 491, duration: 0.403s, episode steps:  22, steps per second:  55, episode reward: 38.800, mean reward:  1.764 [-2.614, 32.280], mean action: 4.136 [1.000, 16.000],  loss: 0.017539, mae: 0.324777, mean_q: 0.245556, mean_eps: 0.000000
 18294/30000: episode: 492, duration: 0.656s, episode steps:  33, steps per second:  50, episode reward: 35.231, mean reward:  1.068 [-3.000, 32.800], mean action: 10.576 [1.000, 21.000],  loss: 0.016806, mae: 0.311777, mean_q: 0.273870, mean_eps: 0.000000
 18313/30000: episode: 493, duration: 0.364s, episode steps:  19, steps per second:  52, episode reward: 43.233, mean reward:  2.275 [-2.434, 32.190], mean action: 6.789 [1.000, 21.000],  loss: 0.015131, mae: 0.308139, mean_q: 0.237205, mean_eps: 0.000000
 18335/30000: episode: 494, duration: 0.374s, episode steps:  22, steps per second:  59, episode reward: 38.903, mean reward:  1.768 [-2.679, 32.163], mean action: 3.727 [1.000, 15.000],  loss: 0.015399, mae: 0.304302, mean_q: 0.278746, mean_eps: 0.000000
 18373/30000: episode: 495, duration: 0.692s, episode steps:  38, steps per second:  55, episode reward: 37.896, mean reward:  0.997 [-2.753, 32.210], mean action: 5.684 [1.000, 15.000],  loss: 0.018896, mae: 0.324482, mean_q: 0.268771, mean_eps: 0.000000
 18398/30000: episode: 496, duration: 0.526s, episode steps:  25, steps per second:  48, episode reward: 41.129, mean reward:  1.645 [-2.271, 32.190], mean action: 4.480 [1.000, 15.000],  loss: 0.012691, mae: 0.279991, mean_q: 0.323914, mean_eps: 0.000000
 18429/30000: episode: 497, duration: 0.548s, episode steps:  31, steps per second:  57, episode reward: 38.608, mean reward:  1.245 [-2.625, 32.550], mean action: 4.968 [0.000, 14.000],  loss: 0.014829, mae: 0.292571, mean_q: 0.308346, mean_eps: 0.000000
 18469/30000: episode: 498, duration: 0.691s, episode steps:  40, steps per second:  58, episode reward: 44.083, mean reward:  1.102 [-2.643, 32.090], mean action: 4.025 [1.000, 15.000],  loss: 0.013526, mae: 0.304676, mean_q: 0.254238, mean_eps: 0.000000
 18495/30000: episode: 499, duration: 0.463s, episode steps:  26, steps per second:  56, episode reward: 35.715, mean reward:  1.374 [-2.701, 31.885], mean action: 3.923 [1.000, 12.000],  loss: 0.014484, mae: 0.317830, mean_q: 0.238153, mean_eps: 0.000000
 18525/30000: episode: 500, duration: 0.603s, episode steps:  30, steps per second:  50, episode reward: 44.476, mean reward:  1.483 [-2.288, 32.340], mean action: 4.133 [1.000, 17.000],  loss: 0.012663, mae: 0.288955, mean_q: 0.278205, mean_eps: 0.000000
 18548/30000: episode: 501, duration: 0.460s, episode steps:  23, steps per second:  50, episode reward: 38.640, mean reward:  1.680 [-2.247, 31.894], mean action: 4.435 [1.000, 16.000],  loss: 0.015279, mae: 0.299952, mean_q: 0.293119, mean_eps: 0.000000
 18561/30000: episode: 502, duration: 0.258s, episode steps:  13, steps per second:  50, episode reward: 44.426, mean reward:  3.417 [-2.021, 31.556], mean action: 3.538 [1.000, 11.000],  loss: 0.014438, mae: 0.289697, mean_q: 0.322823, mean_eps: 0.000000
 18612/30000: episode: 503, duration: 0.963s, episode steps:  51, steps per second:  53, episode reward: 34.654, mean reward:  0.679 [-3.000, 32.150], mean action: 5.451 [1.000, 17.000],  loss: 0.015570, mae: 0.310775, mean_q: 0.281075, mean_eps: 0.000000
 18624/30000: episode: 504, duration: 0.245s, episode steps:  12, steps per second:  49, episode reward: 44.023, mean reward:  3.669 [-2.701, 32.250], mean action: 3.750 [3.000, 12.000],  loss: 0.017229, mae: 0.302033, mean_q: 0.355462, mean_eps: 0.000000
 18673/30000: episode: 505, duration: 0.907s, episode steps:  49, steps per second:  54, episode reward: -32.380, mean reward: -0.661 [-31.831,  3.322], mean action: 4.224 [1.000, 12.000],  loss: 0.015291, mae: 0.298419, mean_q: 0.325854, mean_eps: 0.000000
 18693/30000: episode: 506, duration: 0.414s, episode steps:  20, steps per second:  48, episode reward: 45.000, mean reward:  2.250 [-2.500, 32.120], mean action: 1.700 [1.000, 15.000],  loss: 0.013813, mae: 0.297926, mean_q: 0.281450, mean_eps: 0.000000
 18721/30000: episode: 507, duration: 0.539s, episode steps:  28, steps per second:  52, episode reward: 35.201, mean reward:  1.257 [-2.486, 32.070], mean action: 4.643 [1.000, 17.000],  loss: 0.018030, mae: 0.327100, mean_q: 0.287013, mean_eps: 0.000000
 18739/30000: episode: 508, duration: 0.334s, episode steps:  18, steps per second:  54, episode reward: 47.438, mean reward:  2.635 [-0.030, 32.880], mean action: 4.000 [1.000, 14.000],  loss: 0.016135, mae: 0.318795, mean_q: 0.351859, mean_eps: 0.000000
 18774/30000: episode: 509, duration: 0.634s, episode steps:  35, steps per second:  55, episode reward: -32.140, mean reward: -0.918 [-32.045,  3.000], mean action: 5.200 [1.000, 15.000],  loss: 0.015039, mae: 0.319970, mean_q: 0.281833, mean_eps: 0.000000
 18795/30000: episode: 510, duration: 0.429s, episode steps:  21, steps per second:  49, episode reward: 47.584, mean reward:  2.266 [-0.162, 33.000], mean action: 2.762 [1.000, 10.000],  loss: 0.017208, mae: 0.321847, mean_q: 0.335842, mean_eps: 0.000000
 18842/30000: episode: 511, duration: 0.789s, episode steps:  47, steps per second:  60, episode reward: -37.480, mean reward: -0.797 [-32.122,  2.530], mean action: 4.170 [1.000, 15.000],  loss: 0.017964, mae: 0.334552, mean_q: 0.321225, mean_eps: 0.000000
 18875/30000: episode: 512, duration: 0.602s, episode steps:  33, steps per second:  55, episode reward: 34.860, mean reward:  1.056 [-3.000, 31.735], mean action: 4.636 [1.000, 15.000],  loss: 0.014651, mae: 0.300397, mean_q: 0.315845, mean_eps: 0.000000
 18898/30000: episode: 513, duration: 0.762s, episode steps:  23, steps per second:  30, episode reward: 43.842, mean reward:  1.906 [-2.401, 32.490], mean action: 3.348 [1.000, 14.000],  loss: 0.014087, mae: 0.291421, mean_q: 0.329917, mean_eps: 0.000000
 18934/30000: episode: 514, duration: 0.843s, episode steps:  36, steps per second:  43, episode reward: 37.716, mean reward:  1.048 [-3.000, 32.021], mean action: 3.722 [1.000, 16.000],  loss: 0.015540, mae: 0.312388, mean_q: 0.302040, mean_eps: 0.000000
 18961/30000: episode: 515, duration: 0.535s, episode steps:  27, steps per second:  50, episode reward: 41.475, mean reward:  1.536 [-2.341, 33.000], mean action: 4.296 [1.000, 17.000],  loss: 0.013790, mae: 0.304623, mean_q: 0.297673, mean_eps: 0.000000
 19000/30000: episode: 516, duration: 0.756s, episode steps:  39, steps per second:  52, episode reward: 32.903, mean reward:  0.844 [-2.494, 32.493], mean action: 5.564 [1.000, 16.000],  loss: 0.014579, mae: 0.308771, mean_q: 0.302047, mean_eps: 0.000000
 19059/30000: episode: 517, duration: 1.173s, episode steps:  59, steps per second:  50, episode reward: -32.470, mean reward: -0.550 [-32.136,  2.901], mean action: 4.593 [1.000, 15.000],  loss: 0.016307, mae: 0.323960, mean_q: 0.263682, mean_eps: 0.000000
 19083/30000: episode: 518, duration: 0.450s, episode steps:  24, steps per second:  53, episode reward: 37.372, mean reward:  1.557 [-2.335, 32.180], mean action: 5.625 [1.000, 15.000],  loss: 0.013207, mae: 0.300890, mean_q: 0.292044, mean_eps: 0.000000
 19116/30000: episode: 519, duration: 0.747s, episode steps:  33, steps per second:  44, episode reward: -37.450, mean reward: -1.135 [-32.057,  2.930], mean action: 3.333 [1.000, 15.000],  loss: 0.013777, mae: 0.310126, mean_q: 0.323749, mean_eps: 0.000000
 19167/30000: episode: 520, duration: 0.942s, episode steps:  51, steps per second:  54, episode reward: 34.676, mean reward:  0.680 [-3.000, 32.340], mean action: 4.902 [1.000, 16.000],  loss: 0.015053, mae: 0.307382, mean_q: 0.299005, mean_eps: 0.000000
 19196/30000: episode: 521, duration: 0.558s, episode steps:  29, steps per second:  52, episode reward: 35.182, mean reward:  1.213 [-3.000, 32.249], mean action: 4.862 [1.000, 21.000],  loss: 0.015057, mae: 0.314449, mean_q: 0.270231, mean_eps: 0.000000
 19269/30000: episode: 522, duration: 1.312s, episode steps:  73, steps per second:  56, episode reward: -32.640, mean reward: -0.447 [-32.317,  3.000], mean action: 6.877 [1.000, 21.000],  loss: 0.016461, mae: 0.322193, mean_q: 0.270504, mean_eps: 0.000000
 19304/30000: episode: 523, duration: 0.672s, episode steps:  35, steps per second:  52, episode reward: 40.509, mean reward:  1.157 [-2.648, 31.787], mean action: 5.514 [1.000, 16.000],  loss: 0.013775, mae: 0.292855, mean_q: 0.325012, mean_eps: 0.000000
 19329/30000: episode: 524, duration: 0.480s, episode steps:  25, steps per second:  52, episode reward: 38.365, mean reward:  1.535 [-3.000, 32.087], mean action: 4.320 [1.000, 15.000],  loss: 0.017301, mae: 0.337578, mean_q: 0.246072, mean_eps: 0.000000
 19362/30000: episode: 525, duration: 0.623s, episode steps:  33, steps per second:  53, episode reward: -36.000, mean reward: -1.091 [-32.494,  2.740], mean action: 6.515 [1.000, 21.000],  loss: 0.014507, mae: 0.316808, mean_q: 0.259051, mean_eps: 0.000000
 19386/30000: episode: 526, duration: 0.435s, episode steps:  24, steps per second:  55, episode reward: 41.032, mean reward:  1.710 [-2.666, 31.387], mean action: 3.042 [1.000, 11.000],  loss: 0.019786, mae: 0.336608, mean_q: 0.299240, mean_eps: 0.000000
 19408/30000: episode: 527, duration: 0.383s, episode steps:  22, steps per second:  57, episode reward: 41.110, mean reward:  1.869 [-2.402, 32.092], mean action: 5.000 [1.000, 15.000],  loss: 0.015460, mae: 0.321280, mean_q: 0.267466, mean_eps: 0.000000
 19433/30000: episode: 528, duration: 0.412s, episode steps:  25, steps per second:  61, episode reward: 44.181, mean reward:  1.767 [-2.072, 32.270], mean action: 4.800 [3.000, 21.000],  loss: 0.021346, mae: 0.338729, mean_q: 0.276983, mean_eps: 0.000000
 19467/30000: episode: 529, duration: 0.584s, episode steps:  34, steps per second:  58, episode reward: 41.286, mean reward:  1.214 [-2.387, 32.540], mean action: 5.794 [1.000, 17.000],  loss: 0.016872, mae: 0.303805, mean_q: 0.350881, mean_eps: 0.000000
 19507/30000: episode: 530, duration: 0.848s, episode steps:  40, steps per second:  47, episode reward: 43.318, mean reward:  1.083 [-2.451, 31.868], mean action: 2.775 [1.000, 15.000],  loss: 0.016023, mae: 0.310143, mean_q: 0.337641, mean_eps: 0.000000
 19543/30000: episode: 531, duration: 0.600s, episode steps:  36, steps per second:  60, episode reward: 38.913, mean reward:  1.081 [-2.494, 32.113], mean action: 4.000 [1.000, 21.000],  loss: 0.016734, mae: 0.321262, mean_q: 0.294923, mean_eps: 0.000000
 19573/30000: episode: 532, duration: 0.580s, episode steps:  30, steps per second:  52, episode reward: -32.300, mean reward: -1.077 [-31.981,  2.800], mean action: 8.300 [1.000, 16.000],  loss: 0.012889, mae: 0.289517, mean_q: 0.309911, mean_eps: 0.000000
 19608/30000: episode: 533, duration: 0.666s, episode steps:  35, steps per second:  53, episode reward: 38.094, mean reward:  1.088 [-2.424, 32.150], mean action: 4.143 [1.000, 15.000],  loss: 0.015170, mae: 0.302619, mean_q: 0.287872, mean_eps: 0.000000
 19634/30000: episode: 534, duration: 0.499s, episode steps:  26, steps per second:  52, episode reward: 35.815, mean reward:  1.377 [-3.000, 32.090], mean action: 4.615 [1.000, 18.000],  loss: 0.014628, mae: 0.300889, mean_q: 0.331505, mean_eps: 0.000000
 19673/30000: episode: 535, duration: 0.667s, episode steps:  39, steps per second:  58, episode reward: 38.883, mean reward:  0.997 [-2.131, 32.180], mean action: 4.051 [1.000, 15.000],  loss: 0.016824, mae: 0.317324, mean_q: 0.320697, mean_eps: 0.000000
 19718/30000: episode: 536, duration: 0.896s, episode steps:  45, steps per second:  50, episode reward: 35.577, mean reward:  0.791 [-2.633, 32.522], mean action: 8.067 [1.000, 15.000],  loss: 0.015631, mae: 0.308926, mean_q: 0.313334, mean_eps: 0.000000
 19747/30000: episode: 537, duration: 0.521s, episode steps:  29, steps per second:  56, episode reward: 43.375, mean reward:  1.496 [-3.000, 32.260], mean action: 6.310 [1.000, 15.000],  loss: 0.017231, mae: 0.300758, mean_q: 0.354531, mean_eps: 0.000000
 19767/30000: episode: 538, duration: 0.426s, episode steps:  20, steps per second:  47, episode reward: 41.579, mean reward:  2.079 [-2.565, 32.370], mean action: 4.750 [1.000, 10.000],  loss: 0.015409, mae: 0.308306, mean_q: 0.312701, mean_eps: 0.000000
 19790/30000: episode: 539, duration: 0.405s, episode steps:  23, steps per second:  57, episode reward: 41.664, mean reward:  1.811 [-2.197, 32.210], mean action: 3.522 [1.000, 15.000],  loss: 0.014974, mae: 0.318891, mean_q: 0.273830, mean_eps: 0.000000
 19827/30000: episode: 540, duration: 0.700s, episode steps:  37, steps per second:  53, episode reward: 38.087, mean reward:  1.029 [-2.389, 32.070], mean action: 4.054 [1.000, 21.000],  loss: 0.015861, mae: 0.315419, mean_q: 0.314515, mean_eps: 0.000000
 19856/30000: episode: 541, duration: 0.515s, episode steps:  29, steps per second:  56, episode reward: 38.067, mean reward:  1.313 [-2.058, 31.394], mean action: 6.448 [1.000, 17.000],  loss: 0.017120, mae: 0.332098, mean_q: 0.308424, mean_eps: 0.000000
 19891/30000: episode: 542, duration: 0.593s, episode steps:  35, steps per second:  59, episode reward: 38.069, mean reward:  1.088 [-2.082, 31.769], mean action: 4.114 [1.000, 15.000],  loss: 0.018502, mae: 0.338489, mean_q: 0.304306, mean_eps: 0.000000
 19934/30000: episode: 543, duration: 0.822s, episode steps:  43, steps per second:  52, episode reward: -32.450, mean reward: -0.755 [-33.000,  2.590], mean action: 4.302 [1.000, 15.000],  loss: 0.015721, mae: 0.327212, mean_q: 0.279794, mean_eps: 0.000000
 19963/30000: episode: 544, duration: 0.505s, episode steps:  29, steps per second:  57, episode reward: 44.305, mean reward:  1.528 [-2.275, 32.385], mean action: 3.138 [1.000, 15.000],  loss: 0.015626, mae: 0.341599, mean_q: 0.272333, mean_eps: 0.000000
 19990/30000: episode: 545, duration: 0.483s, episode steps:  27, steps per second:  56, episode reward: 38.123, mean reward:  1.412 [-2.453, 32.183], mean action: 3.778 [0.000, 14.000],  loss: 0.014714, mae: 0.314344, mean_q: 0.336741, mean_eps: 0.000000
 20040/30000: episode: 546, duration: 0.926s, episode steps:  50, steps per second:  54, episode reward: -35.000, mean reward: -0.700 [-32.025,  3.000], mean action: 4.720 [0.000, 16.000],  loss: 0.016857, mae: 0.318691, mean_q: 0.317875, mean_eps: 0.000000
 20096/30000: episode: 547, duration: 1.116s, episode steps:  56, steps per second:  50, episode reward: 32.554, mean reward:  0.581 [-2.408, 32.350], mean action: 5.482 [1.000, 16.000],  loss: 0.017032, mae: 0.318238, mean_q: 0.298586, mean_eps: 0.000000
 20128/30000: episode: 548, duration: 0.737s, episode steps:  32, steps per second:  43, episode reward: 32.424, mean reward:  1.013 [-3.000, 31.434], mean action: 6.156 [0.000, 16.000],  loss: 0.016297, mae: 0.313056, mean_q: 0.281546, mean_eps: 0.000000
 20154/30000: episode: 549, duration: 0.469s, episode steps:  26, steps per second:  55, episode reward: 40.401, mean reward:  1.554 [-2.183, 32.000], mean action: 5.423 [1.000, 16.000],  loss: 0.014609, mae: 0.305483, mean_q: 0.306195, mean_eps: 0.000000
 20204/30000: episode: 550, duration: 0.980s, episode steps:  50, steps per second:  51, episode reward: 32.500, mean reward:  0.650 [-3.000, 31.752], mean action: 3.880 [0.000, 16.000],  loss: 0.016633, mae: 0.319155, mean_q: 0.309925, mean_eps: 0.000000
 20233/30000: episode: 551, duration: 0.619s, episode steps:  29, steps per second:  47, episode reward: 44.808, mean reward:  1.545 [-2.142, 32.100], mean action: 6.517 [1.000, 21.000],  loss: 0.016995, mae: 0.315121, mean_q: 0.297063, mean_eps: 0.000000
 20259/30000: episode: 552, duration: 0.558s, episode steps:  26, steps per second:  47, episode reward: 41.646, mean reward:  1.602 [-2.369, 32.100], mean action: 3.192 [1.000, 14.000],  loss: 0.016496, mae: 0.307859, mean_q: 0.334473, mean_eps: 0.000000
 20288/30000: episode: 553, duration: 0.580s, episode steps:  29, steps per second:  50, episode reward: 43.654, mean reward:  1.505 [-2.176, 31.961], mean action: 4.241 [1.000, 15.000],  loss: 0.015580, mae: 0.307994, mean_q: 0.325486, mean_eps: 0.000000
 20372/30000: episode: 554, duration: 1.695s, episode steps:  84, steps per second:  50, episode reward: 37.971, mean reward:  0.452 [-2.755, 32.003], mean action: 2.845 [1.000, 16.000],  loss: 0.014988, mae: 0.311324, mean_q: 0.317717, mean_eps: 0.000000
 20392/30000: episode: 555, duration: 0.525s, episode steps:  20, steps per second:  38, episode reward: 41.182, mean reward:  2.059 [-2.703, 32.153], mean action: 4.300 [3.000, 16.000],  loss: 0.017378, mae: 0.329870, mean_q: 0.329189, mean_eps: 0.000000
 20424/30000: episode: 556, duration: 0.709s, episode steps:  32, steps per second:  45, episode reward: 32.095, mean reward:  1.003 [-2.763, 32.110], mean action: 4.938 [1.000, 16.000],  loss: 0.016541, mae: 0.332379, mean_q: 0.291714, mean_eps: 0.000000
 20455/30000: episode: 557, duration: 0.769s, episode steps:  31, steps per second:  40, episode reward: 40.755, mean reward:  1.315 [-2.618, 32.200], mean action: 5.194 [1.000, 15.000],  loss: 0.016419, mae: 0.319768, mean_q: 0.260790, mean_eps: 0.000000
 20487/30000: episode: 558, duration: 0.677s, episode steps:  32, steps per second:  47, episode reward: 35.827, mean reward:  1.120 [-3.000, 32.010], mean action: 9.469 [1.000, 21.000],  loss: 0.016078, mae: 0.311763, mean_q: 0.305251, mean_eps: 0.000000
 20509/30000: episode: 559, duration: 0.547s, episode steps:  22, steps per second:  40, episode reward: 35.792, mean reward:  1.627 [-2.806, 32.520], mean action: 6.318 [1.000, 15.000],  loss: 0.014232, mae: 0.318949, mean_q: 0.281160, mean_eps: 0.000000
 20548/30000: episode: 560, duration: 1.053s, episode steps:  39, steps per second:  37, episode reward: 35.232, mean reward:  0.903 [-2.805, 33.000], mean action: 4.564 [1.000, 17.000],  loss: 0.012669, mae: 0.313519, mean_q: 0.261508, mean_eps: 0.000000
 20574/30000: episode: 561, duration: 0.596s, episode steps:  26, steps per second:  44, episode reward: 39.000, mean reward:  1.500 [-2.230, 30.090], mean action: 2.692 [1.000, 17.000],  loss: 0.013899, mae: 0.300008, mean_q: 0.340368, mean_eps: 0.000000
 20613/30000: episode: 562, duration: 0.862s, episode steps:  39, steps per second:  45, episode reward: 35.680, mean reward:  0.915 [-2.180, 31.992], mean action: 5.000 [1.000, 16.000],  loss: 0.013158, mae: 0.310518, mean_q: 0.275279, mean_eps: 0.000000
 20649/30000: episode: 563, duration: 0.904s, episode steps:  36, steps per second:  40, episode reward: 32.054, mean reward:  0.890 [-3.000, 32.522], mean action: 3.972 [1.000, 12.000],  loss: 0.014695, mae: 0.322330, mean_q: 0.258188, mean_eps: 0.000000
 20721/30000: episode: 564, duration: 1.772s, episode steps:  72, steps per second:  41, episode reward: -32.920, mean reward: -0.457 [-32.006,  3.000], mean action: 8.736 [1.000, 21.000],  loss: 0.014862, mae: 0.336297, mean_q: 0.270471, mean_eps: 0.000000
 20740/30000: episode: 565, duration: 0.588s, episode steps:  19, steps per second:  32, episode reward: 44.161, mean reward:  2.324 [-3.000, 32.370], mean action: 2.368 [1.000, 14.000],  loss: 0.012901, mae: 0.310546, mean_q: 0.303288, mean_eps: 0.000000
 20758/30000: episode: 566, duration: 0.416s, episode steps:  18, steps per second:  43, episode reward: 44.329, mean reward:  2.463 [-2.089, 32.080], mean action: 4.944 [1.000, 21.000],  loss: 0.012929, mae: 0.308668, mean_q: 0.283833, mean_eps: 0.000000
 20776/30000: episode: 567, duration: 0.353s, episode steps:  18, steps per second:  51, episode reward: 45.000, mean reward:  2.500 [-2.389, 32.413], mean action: 4.333 [1.000, 14.000],  loss: 0.018516, mae: 0.330949, mean_q: 0.336288, mean_eps: 0.000000
 20812/30000: episode: 568, duration: 0.761s, episode steps:  36, steps per second:  47, episode reward: 35.618, mean reward:  0.989 [-2.502, 32.210], mean action: 5.278 [1.000, 15.000],  loss: 0.015529, mae: 0.335957, mean_q: 0.317775, mean_eps: 0.000000
 20831/30000: episode: 569, duration: 0.382s, episode steps:  19, steps per second:  50, episode reward: 38.919, mean reward:  2.048 [-3.000, 32.570], mean action: 5.579 [1.000, 15.000],  loss: 0.015166, mae: 0.342619, mean_q: 0.313196, mean_eps: 0.000000
 20857/30000: episode: 570, duration: 0.585s, episode steps:  26, steps per second:  44, episode reward: 41.816, mean reward:  1.608 [-2.190, 32.380], mean action: 4.038 [1.000, 15.000],  loss: 0.017748, mae: 0.351741, mean_q: 0.258707, mean_eps: 0.000000
 20919/30000: episode: 571, duration: 1.642s, episode steps:  62, steps per second:  38, episode reward: 37.018, mean reward:  0.597 [-2.599, 32.480], mean action: 4.500 [1.000, 21.000],  loss: 0.016481, mae: 0.325320, mean_q: 0.311176, mean_eps: 0.000000
 20978/30000: episode: 572, duration: 1.389s, episode steps:  59, steps per second:  42, episode reward: 30.000, mean reward:  0.508 [-3.000, 29.270], mean action: 5.339 [1.000, 21.000],  loss: 0.015875, mae: 0.327762, mean_q: 0.303877, mean_eps: 0.000000
 21020/30000: episode: 573, duration: 0.985s, episode steps:  42, steps per second:  43, episode reward: 32.177, mean reward:  0.766 [-2.375, 32.460], mean action: 5.500 [0.000, 21.000],  loss: 0.015473, mae: 0.326466, mean_q: 0.323763, mean_eps: 0.000000
 21051/30000: episode: 574, duration: 0.813s, episode steps:  31, steps per second:  38, episode reward: 37.162, mean reward:  1.199 [-3.000, 32.130], mean action: 3.161 [0.000, 16.000],  loss: 0.014570, mae: 0.312346, mean_q: 0.319630, mean_eps: 0.000000
 21108/30000: episode: 575, duration: 1.323s, episode steps:  57, steps per second:  43, episode reward: 33.000, mean reward:  0.579 [-3.000, 32.170], mean action: 3.930 [1.000, 16.000],  loss: 0.015351, mae: 0.324596, mean_q: 0.282306, mean_eps: 0.000000
 21129/30000: episode: 576, duration: 0.533s, episode steps:  21, steps per second:  39, episode reward: 44.027, mean reward:  2.097 [-2.253, 31.981], mean action: 1.762 [1.000, 17.000],  loss: 0.018046, mae: 0.359936, mean_q: 0.239755, mean_eps: 0.000000
 21172/30000: episode: 577, duration: 1.093s, episode steps:  43, steps per second:  39, episode reward: 35.364, mean reward:  0.822 [-2.757, 32.690], mean action: 4.442 [1.000, 17.000],  loss: 0.016687, mae: 0.328569, mean_q: 0.300416, mean_eps: 0.000000
 21194/30000: episode: 578, duration: 0.546s, episode steps:  22, steps per second:  40, episode reward: 38.545, mean reward:  1.752 [-3.000, 32.230], mean action: 7.682 [1.000, 14.000],  loss: 0.014269, mae: 0.321403, mean_q: 0.262380, mean_eps: 0.000000
 21220/30000: episode: 579, duration: 0.595s, episode steps:  26, steps per second:  44, episode reward: 39.000, mean reward:  1.500 [-2.615, 32.380], mean action: 4.346 [1.000, 12.000],  loss: 0.013558, mae: 0.323075, mean_q: 0.288972, mean_eps: 0.000000
 21265/30000: episode: 580, duration: 0.986s, episode steps:  45, steps per second:  46, episode reward: 43.597, mean reward:  0.969 [-2.179, 32.129], mean action: 2.867 [1.000, 11.000],  loss: 0.015299, mae: 0.329708, mean_q: 0.296720, mean_eps: 0.000000
 21306/30000: episode: 581, duration: 0.916s, episode steps:  41, steps per second:  45, episode reward: 35.658, mean reward:  0.870 [-3.000, 32.010], mean action: 3.683 [1.000, 14.000],  loss: 0.017341, mae: 0.345062, mean_q: 0.310821, mean_eps: 0.000000
 21327/30000: episode: 582, duration: 0.529s, episode steps:  21, steps per second:  40, episode reward: 44.287, mean reward:  2.109 [-2.640, 32.287], mean action: 3.810 [1.000, 14.000],  loss: 0.016958, mae: 0.361605, mean_q: 0.257726, mean_eps: 0.000000
 21340/30000: episode: 583, duration: 0.302s, episode steps:  13, steps per second:  43, episode reward: 47.741, mean reward:  3.672 [-0.023, 33.000], mean action: 1.846 [1.000, 12.000],  loss: 0.017916, mae: 0.380224, mean_q: 0.255682, mean_eps: 0.000000
 21369/30000: episode: 584, duration: 0.631s, episode steps:  29, steps per second:  46, episode reward: -34.890, mean reward: -1.203 [-31.729,  2.201], mean action: 5.586 [1.000, 11.000],  loss: 0.016197, mae: 0.349454, mean_q: 0.293953, mean_eps: 0.000000
 21417/30000: episode: 585, duration: 0.998s, episode steps:  48, steps per second:  48, episode reward: -32.570, mean reward: -0.679 [-32.316,  2.901], mean action: 4.167 [1.000, 12.000],  loss: 0.016004, mae: 0.335526, mean_q: 0.293631, mean_eps: 0.000000
 21438/30000: episode: 586, duration: 0.417s, episode steps:  21, steps per second:  50, episode reward: 38.433, mean reward:  1.830 [-3.000, 32.200], mean action: 5.905 [1.000, 21.000],  loss: 0.017617, mae: 0.360424, mean_q: 0.282980, mean_eps: 0.000000
 21471/30000: episode: 587, duration: 0.683s, episode steps:  33, steps per second:  48, episode reward: 38.115, mean reward:  1.155 [-2.589, 32.351], mean action: 3.455 [1.000, 12.000],  loss: 0.017079, mae: 0.351438, mean_q: 0.301701, mean_eps: 0.000000
 21493/30000: episode: 588, duration: 0.396s, episode steps:  22, steps per second:  56, episode reward: 43.974, mean reward:  1.999 [-2.273, 32.131], mean action: 4.318 [1.000, 15.000],  loss: 0.014800, mae: 0.334296, mean_q: 0.307089, mean_eps: 0.000000
 21540/30000: episode: 589, duration: 0.841s, episode steps:  47, steps per second:  56, episode reward: 43.876, mean reward:  0.934 [-2.016, 32.200], mean action: 3.553 [1.000, 15.000],  loss: 0.015623, mae: 0.337872, mean_q: 0.298721, mean_eps: 0.000000
 21555/30000: episode: 590, duration: 0.316s, episode steps:  15, steps per second:  47, episode reward: 38.407, mean reward:  2.560 [-3.000, 32.829], mean action: 5.000 [1.000, 14.000],  loss: 0.013626, mae: 0.328690, mean_q: 0.296156, mean_eps: 0.000000
 21598/30000: episode: 591, duration: 1.104s, episode steps:  43, steps per second:  39, episode reward: 38.010, mean reward:  0.884 [-2.329, 32.290], mean action: 3.349 [1.000, 15.000],  loss: 0.012407, mae: 0.322054, mean_q: 0.306122, mean_eps: 0.000000
 21635/30000: episode: 592, duration: 1.074s, episode steps:  37, steps per second:  34, episode reward: 40.966, mean reward:  1.107 [-2.762, 32.140], mean action: 4.405 [1.000, 15.000],  loss: 0.015803, mae: 0.340105, mean_q: 0.347770, mean_eps: 0.000000
 21669/30000: episode: 593, duration: 0.816s, episode steps:  34, steps per second:  42, episode reward: 32.549, mean reward:  0.957 [-2.622, 32.090], mean action: 5.559 [0.000, 14.000],  loss: 0.014855, mae: 0.335422, mean_q: 0.291115, mean_eps: 0.000000
 21709/30000: episode: 594, duration: 0.988s, episode steps:  40, steps per second:  40, episode reward: 32.543, mean reward:  0.814 [-2.275, 31.623], mean action: 4.400 [0.000, 15.000],  loss: 0.016262, mae: 0.340419, mean_q: 0.317729, mean_eps: 0.000000
 21813/30000: episode: 595, duration: 2.008s, episode steps: 104, steps per second:  52, episode reward: -32.630, mean reward: -0.314 [-31.785,  4.011], mean action: 6.087 [1.000, 17.000],  loss: 0.015599, mae: 0.337403, mean_q: 0.290935, mean_eps: 0.000000
 21838/30000: episode: 596, duration: 0.588s, episode steps:  25, steps per second:  42, episode reward: 41.095, mean reward:  1.644 [-2.560, 31.165], mean action: 2.840 [1.000, 11.000],  loss: 0.015948, mae: 0.328997, mean_q: 0.269604, mean_eps: 0.000000
 21864/30000: episode: 597, duration: 0.648s, episode steps:  26, steps per second:  40, episode reward: 35.624, mean reward:  1.370 [-3.000, 32.180], mean action: 5.346 [1.000, 15.000],  loss: 0.013838, mae: 0.326225, mean_q: 0.246512, mean_eps: 0.000000
 21890/30000: episode: 598, duration: 0.525s, episode steps:  26, steps per second:  49, episode reward: 47.602, mean reward:  1.831 [-0.413, 32.120], mean action: 2.154 [1.000, 3.000],  loss: 0.016061, mae: 0.344346, mean_q: 0.283098, mean_eps: 0.000000
 21916/30000: episode: 599, duration: 0.468s, episode steps:  26, steps per second:  56, episode reward: 35.683, mean reward:  1.372 [-2.797, 31.882], mean action: 3.538 [1.000, 12.000],  loss: 0.019931, mae: 0.353685, mean_q: 0.357906, mean_eps: 0.000000
 21972/30000: episode: 600, duration: 0.962s, episode steps:  56, steps per second:  58, episode reward: 32.305, mean reward:  0.577 [-2.791, 31.735], mean action: 8.018 [1.000, 16.000],  loss: 0.014802, mae: 0.341163, mean_q: 0.285017, mean_eps: 0.000000
 21991/30000: episode: 601, duration: 0.439s, episode steps:  19, steps per second:  43, episode reward: 45.000, mean reward:  2.368 [-0.077, 30.048], mean action: 2.105 [1.000, 14.000],  loss: 0.019024, mae: 0.374058, mean_q: 0.305617, mean_eps: 0.000000
 22014/30000: episode: 602, duration: 0.498s, episode steps:  23, steps per second:  46, episode reward: 43.931, mean reward:  1.910 [-2.513, 32.820], mean action: 4.913 [1.000, 15.000],  loss: 0.016738, mae: 0.356948, mean_q: 0.285875, mean_eps: 0.000000
 22050/30000: episode: 603, duration: 0.705s, episode steps:  36, steps per second:  51, episode reward: 42.000, mean reward:  1.167 [-2.798, 32.170], mean action: 2.917 [1.000, 14.000],  loss: 0.017242, mae: 0.340695, mean_q: 0.327518, mean_eps: 0.000000
 22067/30000: episode: 604, duration: 0.379s, episode steps:  17, steps per second:  45, episode reward: 47.486, mean reward:  2.793 [-0.238, 32.080], mean action: 2.529 [1.000, 14.000],  loss: 0.016985, mae: 0.365745, mean_q: 0.307582, mean_eps: 0.000000
 22090/30000: episode: 605, duration: 0.471s, episode steps:  23, steps per second:  49, episode reward: 47.445, mean reward:  2.063 [-0.210, 32.430], mean action: 4.522 [1.000, 15.000],  loss: 0.015132, mae: 0.361953, mean_q: 0.284862, mean_eps: 0.000000
 22116/30000: episode: 606, duration: 0.502s, episode steps:  26, steps per second:  52, episode reward: 44.480, mean reward:  1.711 [-2.610, 32.030], mean action: 4.885 [3.000, 16.000],  loss: 0.012940, mae: 0.352587, mean_q: 0.292323, mean_eps: 0.000000
 22157/30000: episode: 607, duration: 0.848s, episode steps:  41, steps per second:  48, episode reward: 35.679, mean reward:  0.870 [-3.000, 36.520], mean action: 7.098 [1.000, 21.000],  loss: 0.013954, mae: 0.374896, mean_q: 0.286160, mean_eps: 0.000000
 22202/30000: episode: 608, duration: 0.870s, episode steps:  45, steps per second:  52, episode reward: 33.000, mean reward:  0.733 [-2.552, 32.740], mean action: 4.356 [1.000, 15.000],  loss: 0.015113, mae: 0.360754, mean_q: 0.307587, mean_eps: 0.000000
 22283/30000: episode: 609, duration: 1.730s, episode steps:  81, steps per second:  47, episode reward: 32.827, mean reward:  0.405 [-3.000, 31.867], mean action: 8.753 [1.000, 17.000],  loss: 0.016018, mae: 0.364054, mean_q: 0.304428, mean_eps: 0.000000
 22311/30000: episode: 610, duration: 0.755s, episode steps:  28, steps per second:  37, episode reward: 41.843, mean reward:  1.494 [-2.488, 32.390], mean action: 2.607 [1.000, 11.000],  loss: 0.017491, mae: 0.365756, mean_q: 0.329033, mean_eps: 0.000000
 22356/30000: episode: 611, duration: 0.912s, episode steps:  45, steps per second:  49, episode reward: 40.301, mean reward:  0.896 [-3.000, 32.350], mean action: 4.378 [1.000, 17.000],  loss: 0.014859, mae: 0.345970, mean_q: 0.321012, mean_eps: 0.000000
 22382/30000: episode: 612, duration: 0.733s, episode steps:  26, steps per second:  35, episode reward: 46.606, mean reward:  1.793 [-1.116, 32.260], mean action: 4.962 [1.000, 15.000],  loss: 0.017506, mae: 0.356162, mean_q: 0.286688, mean_eps: 0.000000
 22404/30000: episode: 613, duration: 0.460s, episode steps:  22, steps per second:  48, episode reward: 39.000, mean reward:  1.773 [-2.908, 33.000], mean action: 3.909 [1.000, 14.000],  loss: 0.018857, mae: 0.352000, mean_q: 0.347505, mean_eps: 0.000000
 22451/30000: episode: 614, duration: 1.020s, episode steps:  47, steps per second:  46, episode reward: 35.657, mean reward:  0.759 [-2.527, 32.228], mean action: 6.553 [1.000, 15.000],  loss: 0.016089, mae: 0.359579, mean_q: 0.300311, mean_eps: 0.000000
 22475/30000: episode: 615, duration: 0.679s, episode steps:  24, steps per second:  35, episode reward: 41.494, mean reward:  1.729 [-2.195, 31.972], mean action: 2.792 [1.000, 15.000],  loss: 0.014712, mae: 0.342799, mean_q: 0.317581, mean_eps: 0.000000
 22488/30000: episode: 616, duration: 0.267s, episode steps:  13, steps per second:  49, episode reward: 40.540, mean reward:  3.118 [-3.000, 32.317], mean action: 5.846 [1.000, 15.000],  loss: 0.016985, mae: 0.349769, mean_q: 0.342379, mean_eps: 0.000000
 22513/30000: episode: 617, duration: 0.525s, episode steps:  25, steps per second:  48, episode reward: 44.792, mean reward:  1.792 [-2.113, 32.180], mean action: 3.640 [1.000, 15.000],  loss: 0.012579, mae: 0.351983, mean_q: 0.279730, mean_eps: 0.000000
 22546/30000: episode: 618, duration: 0.760s, episode steps:  33, steps per second:  43, episode reward: 37.633, mean reward:  1.140 [-2.222, 32.140], mean action: 7.061 [1.000, 17.000],  loss: 0.016999, mae: 0.368940, mean_q: 0.274536, mean_eps: 0.000000
 22571/30000: episode: 619, duration: 0.485s, episode steps:  25, steps per second:  52, episode reward: 36.000, mean reward:  1.440 [-3.000, 32.070], mean action: 2.880 [1.000, 6.000],  loss: 0.012956, mae: 0.346305, mean_q: 0.280886, mean_eps: 0.000000
 22588/30000: episode: 620, duration: 0.369s, episode steps:  17, steps per second:  46, episode reward: 41.800, mean reward:  2.459 [-2.573, 32.040], mean action: 4.647 [1.000, 17.000],  loss: 0.013758, mae: 0.368672, mean_q: 0.248689, mean_eps: 0.000000
 22613/30000: episode: 621, duration: 0.528s, episode steps:  25, steps per second:  47, episode reward: 38.555, mean reward:  1.542 [-2.514, 32.220], mean action: 3.720 [1.000, 15.000],  loss: 0.015116, mae: 0.368993, mean_q: 0.253361, mean_eps: 0.000000
 22647/30000: episode: 622, duration: 0.703s, episode steps:  34, steps per second:  48, episode reward: 34.610, mean reward:  1.018 [-2.603, 31.793], mean action: 6.441 [1.000, 16.000],  loss: 0.016370, mae: 0.361251, mean_q: 0.296811, mean_eps: 0.000000
 22668/30000: episode: 623, duration: 0.407s, episode steps:  21, steps per second:  52, episode reward: 43.848, mean reward:  2.088 [-2.008, 32.090], mean action: 4.048 [1.000, 14.000],  loss: 0.015479, mae: 0.368442, mean_q: 0.235858, mean_eps: 0.000000
 22699/30000: episode: 624, duration: 0.655s, episode steps:  31, steps per second:  47, episode reward: 37.806, mean reward:  1.220 [-2.585, 32.120], mean action: 6.774 [1.000, 21.000],  loss: 0.014375, mae: 0.352236, mean_q: 0.262132, mean_eps: 0.000000
 22721/30000: episode: 625, duration: 0.425s, episode steps:  22, steps per second:  52, episode reward: 43.446, mean reward:  1.975 [-2.089, 32.024], mean action: 4.545 [1.000, 14.000],  loss: 0.020341, mae: 0.362763, mean_q: 0.318110, mean_eps: 0.000000
 22751/30000: episode: 626, duration: 0.617s, episode steps:  30, steps per second:  49, episode reward: 41.155, mean reward:  1.372 [-2.539, 32.230], mean action: 6.333 [1.000, 17.000],  loss: 0.016384, mae: 0.354828, mean_q: 0.273203, mean_eps: 0.000000
 22775/30000: episode: 627, duration: 0.507s, episode steps:  24, steps per second:  47, episode reward: 38.110, mean reward:  1.588 [-2.609, 31.655], mean action: 7.250 [1.000, 18.000],  loss: 0.017282, mae: 0.356720, mean_q: 0.284740, mean_eps: 0.000000
 22829/30000: episode: 628, duration: 1.459s, episode steps:  54, steps per second:  37, episode reward: 40.435, mean reward:  0.749 [-2.407, 32.130], mean action: 8.407 [1.000, 21.000],  loss: 0.017697, mae: 0.360690, mean_q: 0.293328, mean_eps: 0.000000
 22855/30000: episode: 629, duration: 0.748s, episode steps:  26, steps per second:  35, episode reward: 42.000, mean reward:  1.615 [-2.297, 32.090], mean action: 3.808 [1.000, 15.000],  loss: 0.015506, mae: 0.333302, mean_q: 0.327668, mean_eps: 0.000000
 22892/30000: episode: 630, duration: 0.972s, episode steps:  37, steps per second:  38, episode reward: 34.334, mean reward:  0.928 [-3.000, 31.856], mean action: 4.351 [1.000, 15.000],  loss: 0.017160, mae: 0.349761, mean_q: 0.316044, mean_eps: 0.000000
 22926/30000: episode: 631, duration: 1.074s, episode steps:  34, steps per second:  32, episode reward: 32.649, mean reward:  0.960 [-2.875, 31.873], mean action: 8.000 [3.000, 17.000],  loss: 0.012438, mae: 0.349949, mean_q: 0.289546, mean_eps: 0.000000
 22947/30000: episode: 632, duration: 0.816s, episode steps:  21, steps per second:  26, episode reward: 44.093, mean reward:  2.100 [-2.304, 32.080], mean action: 4.238 [1.000, 15.000],  loss: 0.017794, mae: 0.377402, mean_q: 0.271817, mean_eps: 0.000000
 22985/30000: episode: 633, duration: 0.859s, episode steps:  38, steps per second:  44, episode reward: 35.362, mean reward:  0.931 [-3.000, 32.146], mean action: 6.184 [1.000, 15.000],  loss: 0.014333, mae: 0.366431, mean_q: 0.256876, mean_eps: 0.000000
 23012/30000: episode: 634, duration: 0.606s, episode steps:  27, steps per second:  45, episode reward: 47.016, mean reward:  1.741 [-0.396, 32.240], mean action: 2.333 [1.000, 14.000],  loss: 0.017534, mae: 0.359922, mean_q: 0.253915, mean_eps: 0.000000
 23032/30000: episode: 635, duration: 0.431s, episode steps:  20, steps per second:  46, episode reward: 41.114, mean reward:  2.056 [-2.185, 32.133], mean action: 5.250 [1.000, 16.000],  loss: 0.017591, mae: 0.355152, mean_q: 0.315838, mean_eps: 0.000000
 23049/30000: episode: 636, duration: 0.367s, episode steps:  17, steps per second:  46, episode reward: 41.446, mean reward:  2.438 [-2.474, 32.346], mean action: 5.000 [3.000, 16.000],  loss: 0.014033, mae: 0.343167, mean_q: 0.304525, mean_eps: 0.000000
 23130/30000: episode: 637, duration: 1.929s, episode steps:  81, steps per second:  42, episode reward: -37.870, mean reward: -0.468 [-32.405,  2.300], mean action: 5.272 [1.000, 15.000],  loss: 0.015791, mae: 0.351315, mean_q: 0.304193, mean_eps: 0.000000
 23172/30000: episode: 638, duration: 1.055s, episode steps:  42, steps per second:  40, episode reward: 38.656, mean reward:  0.920 [-2.292, 31.676], mean action: 2.833 [1.000, 16.000],  loss: 0.017623, mae: 0.359593, mean_q: 0.303457, mean_eps: 0.000000
 23204/30000: episode: 639, duration: 0.788s, episode steps:  32, steps per second:  41, episode reward: 40.666, mean reward:  1.271 [-2.514, 32.320], mean action: 2.938 [1.000, 10.000],  loss: 0.015549, mae: 0.341901, mean_q: 0.370185, mean_eps: 0.000000
 23241/30000: episode: 640, duration: 0.766s, episode steps:  37, steps per second:  48, episode reward: -32.050, mean reward: -0.866 [-33.000,  2.230], mean action: 5.216 [1.000, 17.000],  loss: 0.019891, mae: 0.365793, mean_q: 0.319359, mean_eps: 0.000000
 23314/30000: episode: 641, duration: 1.547s, episode steps:  73, steps per second:  47, episode reward: 32.788, mean reward:  0.449 [-2.617, 32.050], mean action: 5.068 [1.000, 21.000],  loss: 0.015148, mae: 0.353768, mean_q: 0.326684, mean_eps: 0.000000
 23345/30000: episode: 642, duration: 0.720s, episode steps:  31, steps per second:  43, episode reward: 35.573, mean reward:  1.148 [-2.768, 32.220], mean action: 6.161 [1.000, 15.000],  loss: 0.015203, mae: 0.362919, mean_q: 0.315919, mean_eps: 0.000000
 23396/30000: episode: 643, duration: 1.021s, episode steps:  51, steps per second:  50, episode reward: 37.232, mean reward:  0.730 [-3.000, 32.113], mean action: 3.078 [1.000, 15.000],  loss: 0.014955, mae: 0.355944, mean_q: 0.289173, mean_eps: 0.000000
 23428/30000: episode: 644, duration: 0.611s, episode steps:  32, steps per second:  52, episode reward: 47.585, mean reward:  1.487 [-0.264, 32.480], mean action: 3.469 [1.000, 14.000],  loss: 0.014929, mae: 0.354167, mean_q: 0.286732, mean_eps: 0.000000
 23449/30000: episode: 645, duration: 0.563s, episode steps:  21, steps per second:  37, episode reward: 44.043, mean reward:  2.097 [-2.036, 32.140], mean action: 2.714 [1.000, 5.000],  loss: 0.013213, mae: 0.365598, mean_q: 0.257728, mean_eps: 0.000000
 23472/30000: episode: 646, duration: 0.431s, episode steps:  23, steps per second:  53, episode reward: 42.000, mean reward:  1.826 [-2.299, 32.020], mean action: 4.261 [3.000, 14.000],  loss: 0.016371, mae: 0.375751, mean_q: 0.263168, mean_eps: 0.000000
 23491/30000: episode: 647, duration: 0.369s, episode steps:  19, steps per second:  52, episode reward: 41.058, mean reward:  2.161 [-2.402, 32.770], mean action: 5.211 [1.000, 12.000],  loss: 0.013378, mae: 0.345737, mean_q: 0.290707, mean_eps: 0.000000
 23536/30000: episode: 648, duration: 0.832s, episode steps:  45, steps per second:  54, episode reward: 36.524, mean reward:  0.812 [-3.000, 32.240], mean action: 7.044 [1.000, 21.000],  loss: 0.017505, mae: 0.363248, mean_q: 0.317431, mean_eps: 0.000000
 23561/30000: episode: 649, duration: 0.469s, episode steps:  25, steps per second:  53, episode reward: 43.881, mean reward:  1.755 [-2.559, 32.550], mean action: 2.880 [1.000, 17.000],  loss: 0.014192, mae: 0.332627, mean_q: 0.342491, mean_eps: 0.000000
 23587/30000: episode: 650, duration: 0.520s, episode steps:  26, steps per second:  50, episode reward: 44.574, mean reward:  1.714 [-2.126, 32.050], mean action: 1.192 [1.000, 6.000],  loss: 0.018455, mae: 0.359078, mean_q: 0.351731, mean_eps: 0.000000
 23622/30000: episode: 651, duration: 0.679s, episode steps:  35, steps per second:  52, episode reward: 38.759, mean reward:  1.107 [-2.950, 32.110], mean action: 2.771 [1.000, 10.000],  loss: 0.015487, mae: 0.355150, mean_q: 0.296401, mean_eps: 0.000000
 23660/30000: episode: 652, duration: 0.662s, episode steps:  38, steps per second:  57, episode reward: -32.040, mean reward: -0.843 [-32.015,  2.259], mean action: 7.342 [0.000, 18.000],  loss: 0.016104, mae: 0.346955, mean_q: 0.309530, mean_eps: 0.000000
 23704/30000: episode: 653, duration: 0.898s, episode steps:  44, steps per second:  49, episode reward: 37.134, mean reward:  0.844 [-2.155, 32.645], mean action: 5.455 [1.000, 17.000],  loss: 0.013838, mae: 0.329187, mean_q: 0.327652, mean_eps: 0.000000
 23736/30000: episode: 654, duration: 0.713s, episode steps:  32, steps per second:  45, episode reward: 32.706, mean reward:  1.022 [-3.000, 32.712], mean action: 4.719 [1.000, 16.000],  loss: 0.017655, mae: 0.357512, mean_q: 0.313616, mean_eps: 0.000000
 23777/30000: episode: 655, duration: 0.810s, episode steps:  41, steps per second:  51, episode reward: 37.668, mean reward:  0.919 [-2.370, 32.310], mean action: 5.634 [1.000, 15.000],  loss: 0.018531, mae: 0.355984, mean_q: 0.322664, mean_eps: 0.000000
 23806/30000: episode: 656, duration: 0.600s, episode steps:  29, steps per second:  48, episode reward: 37.199, mean reward:  1.283 [-2.680, 32.063], mean action: 5.241 [1.000, 15.000],  loss: 0.017481, mae: 0.346622, mean_q: 0.343433, mean_eps: 0.000000
 23842/30000: episode: 657, duration: 0.682s, episode steps:  36, steps per second:  53, episode reward: 44.719, mean reward:  1.242 [-2.118, 32.010], mean action: 3.056 [1.000, 21.000],  loss: 0.013879, mae: 0.348242, mean_q: 0.290147, mean_eps: 0.000000
 23885/30000: episode: 658, duration: 0.872s, episode steps:  43, steps per second:  49, episode reward: 32.522, mean reward:  0.756 [-2.680, 32.080], mean action: 3.140 [1.000, 12.000],  loss: 0.020182, mae: 0.385849, mean_q: 0.332416, mean_eps: 0.000000
 23909/30000: episode: 659, duration: 0.516s, episode steps:  24, steps per second:  47, episode reward: 38.072, mean reward:  1.586 [-2.548, 31.734], mean action: 4.250 [1.000, 16.000],  loss: 0.015580, mae: 0.359092, mean_q: 0.322947, mean_eps: 0.000000
 23945/30000: episode: 660, duration: 0.816s, episode steps:  36, steps per second:  44, episode reward: 44.227, mean reward:  1.229 [-2.230, 32.300], mean action: 4.417 [1.000, 16.000],  loss: 0.015824, mae: 0.353262, mean_q: 0.327204, mean_eps: 0.000000
 23964/30000: episode: 661, duration: 0.517s, episode steps:  19, steps per second:  37, episode reward: 41.256, mean reward:  2.171 [-2.437, 32.343], mean action: 5.895 [1.000, 15.000],  loss: 0.018038, mae: 0.390580, mean_q: 0.265275, mean_eps: 0.000000
 23988/30000: episode: 662, duration: 0.849s, episode steps:  24, steps per second:  28, episode reward: 45.000, mean reward:  1.875 [-2.084, 32.170], mean action: 2.792 [1.000, 12.000],  loss: 0.014331, mae: 0.353557, mean_q: 0.331624, mean_eps: 0.000000
 24012/30000: episode: 663, duration: 0.643s, episode steps:  24, steps per second:  37, episode reward: 38.473, mean reward:  1.603 [-3.000, 32.080], mean action: 4.042 [1.000, 12.000],  loss: 0.013632, mae: 0.344617, mean_q: 0.337187, mean_eps: 0.000000
 24043/30000: episode: 664, duration: 0.802s, episode steps:  31, steps per second:  39, episode reward: 41.326, mean reward:  1.333 [-2.423, 31.799], mean action: 2.677 [1.000, 15.000],  loss: 0.019261, mae: 0.374342, mean_q: 0.292999, mean_eps: 0.000000
 24086/30000: episode: 665, duration: 1.094s, episode steps:  43, steps per second:  39, episode reward: 35.856, mean reward:  0.834 [-2.674, 29.335], mean action: 5.442 [1.000, 17.000],  loss: 0.015822, mae: 0.352150, mean_q: 0.314576, mean_eps: 0.000000
 24119/30000: episode: 666, duration: 1.114s, episode steps:  33, steps per second:  30, episode reward: 44.282, mean reward:  1.342 [-3.000, 32.420], mean action: 5.242 [0.000, 15.000],  loss: 0.015819, mae: 0.341161, mean_q: 0.321690, mean_eps: 0.000000
 24150/30000: episode: 667, duration: 0.690s, episode steps:  31, steps per second:  45, episode reward: 44.041, mean reward:  1.421 [-2.133, 31.921], mean action: 2.935 [1.000, 12.000],  loss: 0.016713, mae: 0.371527, mean_q: 0.322738, mean_eps: 0.000000
 24183/30000: episode: 668, duration: 0.745s, episode steps:  33, steps per second:  44, episode reward: 43.759, mean reward:  1.326 [-3.000, 32.250], mean action: 3.879 [1.000, 15.000],  loss: 0.016666, mae: 0.375835, mean_q: 0.318901, mean_eps: 0.000000
 24213/30000: episode: 669, duration: 0.522s, episode steps:  30, steps per second:  57, episode reward: 44.508, mean reward:  1.484 [-2.045, 32.270], mean action: 2.200 [1.000, 15.000],  loss: 0.014940, mae: 0.368373, mean_q: 0.319686, mean_eps: 0.000000
 24239/30000: episode: 670, duration: 0.618s, episode steps:  26, steps per second:  42, episode reward: 38.062, mean reward:  1.464 [-3.000, 32.580], mean action: 5.269 [1.000, 16.000],  loss: 0.019105, mae: 0.382864, mean_q: 0.359556, mean_eps: 0.000000
 24268/30000: episode: 671, duration: 0.784s, episode steps:  29, steps per second:  37, episode reward: 38.814, mean reward:  1.338 [-2.370, 32.594], mean action: 2.379 [1.000, 17.000],  loss: 0.016157, mae: 0.359210, mean_q: 0.376904, mean_eps: 0.000000
 24288/30000: episode: 672, duration: 0.476s, episode steps:  20, steps per second:  42, episode reward: 38.756, mean reward:  1.938 [-2.858, 32.026], mean action: 3.450 [1.000, 17.000],  loss: 0.014601, mae: 0.360002, mean_q: 0.323201, mean_eps: 0.000000
 24313/30000: episode: 673, duration: 0.461s, episode steps:  25, steps per second:  54, episode reward: 35.777, mean reward:  1.431 [-2.346, 32.371], mean action: 6.000 [1.000, 17.000],  loss: 0.016662, mae: 0.369476, mean_q: 0.365429, mean_eps: 0.000000
 24353/30000: episode: 674, duration: 0.704s, episode steps:  40, steps per second:  57, episode reward: 38.557, mean reward:  0.964 [-2.365, 31.836], mean action: 4.325 [1.000, 17.000],  loss: 0.017691, mae: 0.388356, mean_q: 0.313597, mean_eps: 0.000000
 24369/30000: episode: 675, duration: 0.383s, episode steps:  16, steps per second:  42, episode reward: 44.181, mean reward:  2.761 [-2.149, 32.130], mean action: 6.125 [3.000, 21.000],  loss: 0.013785, mae: 0.353014, mean_q: 0.366505, mean_eps: 0.000000
 24392/30000: episode: 676, duration: 0.652s, episode steps:  23, steps per second:  35, episode reward: 38.425, mean reward:  1.671 [-2.368, 31.935], mean action: 5.174 [1.000, 17.000],  loss: 0.014842, mae: 0.356667, mean_q: 0.353999, mean_eps: 0.000000
 24417/30000: episode: 677, duration: 0.565s, episode steps:  25, steps per second:  44, episode reward: 41.706, mean reward:  1.668 [-2.426, 32.120], mean action: 3.160 [1.000, 17.000],  loss: 0.014845, mae: 0.357108, mean_q: 0.332901, mean_eps: 0.000000
 24441/30000: episode: 678, duration: 0.463s, episode steps:  24, steps per second:  52, episode reward: 43.995, mean reward:  1.833 [-2.009, 32.940], mean action: 4.333 [1.000, 14.000],  loss: 0.013810, mae: 0.358702, mean_q: 0.318981, mean_eps: 0.000000
 24469/30000: episode: 679, duration: 0.673s, episode steps:  28, steps per second:  42, episode reward: 44.752, mean reward:  1.598 [-2.311, 32.510], mean action: 4.786 [1.000, 10.000],  loss: 0.016469, mae: 0.380650, mean_q: 0.294428, mean_eps: 0.000000
 24519/30000: episode: 680, duration: 1.065s, episode steps:  50, steps per second:  47, episode reward: 33.000, mean reward:  0.660 [-2.273, 32.150], mean action: 5.140 [1.000, 17.000],  loss: 0.016817, mae: 0.372345, mean_q: 0.324334, mean_eps: 0.000000
 24543/30000: episode: 681, duration: 0.511s, episode steps:  24, steps per second:  47, episode reward: 44.738, mean reward:  1.864 [-2.159, 32.210], mean action: 1.750 [1.000, 15.000],  loss: 0.016276, mae: 0.379670, mean_q: 0.309597, mean_eps: 0.000000
 24566/30000: episode: 682, duration: 0.589s, episode steps:  23, steps per second:  39, episode reward: 41.110, mean reward:  1.787 [-2.674, 32.190], mean action: 3.609 [1.000, 12.000],  loss: 0.017411, mae: 0.415868, mean_q: 0.295407, mean_eps: 0.000000
 24595/30000: episode: 683, duration: 1.051s, episode steps:  29, steps per second:  28, episode reward: 41.095, mean reward:  1.417 [-2.441, 32.090], mean action: 4.862 [1.000, 15.000],  loss: 0.016329, mae: 0.375180, mean_q: 0.337715, mean_eps: 0.000000
 24621/30000: episode: 684, duration: 0.864s, episode steps:  26, steps per second:  30, episode reward: 41.734, mean reward:  1.605 [-2.484, 32.240], mean action: 6.077 [1.000, 20.000],  loss: 0.018507, mae: 0.378142, mean_q: 0.331068, mean_eps: 0.000000
 24708/30000: episode: 685, duration: 2.393s, episode steps:  87, steps per second:  36, episode reward: 32.464, mean reward:  0.373 [-3.000, 33.000], mean action: 7.172 [1.000, 17.000],  loss: 0.016078, mae: 0.367152, mean_q: 0.320379, mean_eps: 0.000000
 24756/30000: episode: 686, duration: 1.671s, episode steps:  48, steps per second:  29, episode reward: 32.808, mean reward:  0.684 [-2.945, 32.100], mean action: 7.646 [1.000, 21.000],  loss: 0.016280, mae: 0.381431, mean_q: 0.316536, mean_eps: 0.000000
 24785/30000: episode: 687, duration: 0.839s, episode steps:  29, steps per second:  35, episode reward: 40.805, mean reward:  1.407 [-2.931, 32.460], mean action: 5.828 [1.000, 17.000],  loss: 0.013568, mae: 0.354406, mean_q: 0.378658, mean_eps: 0.000000
 24849/30000: episode: 688, duration: 1.866s, episode steps:  64, steps per second:  34, episode reward: 37.792, mean reward:  0.590 [-2.318, 32.070], mean action: 4.609 [1.000, 17.000],  loss: 0.015777, mae: 0.369674, mean_q: 0.321751, mean_eps: 0.000000
 24879/30000: episode: 689, duration: 0.861s, episode steps:  30, steps per second:  35, episode reward: 34.968, mean reward:  1.166 [-3.000, 33.000], mean action: 7.033 [0.000, 21.000],  loss: 0.013679, mae: 0.363841, mean_q: 0.349215, mean_eps: 0.000000
 24907/30000: episode: 690, duration: 0.708s, episode steps:  28, steps per second:  40, episode reward: 37.948, mean reward:  1.355 [-2.858, 31.943], mean action: 6.786 [1.000, 21.000],  loss: 0.016054, mae: 0.353113, mean_q: 0.384555, mean_eps: 0.000000
 24938/30000: episode: 691, duration: 0.740s, episode steps:  31, steps per second:  42, episode reward: 44.063, mean reward:  1.421 [-2.660, 32.092], mean action: 3.742 [0.000, 17.000],  loss: 0.016569, mae: 0.376461, mean_q: 0.338649, mean_eps: 0.000000
 24981/30000: episode: 692, duration: 1.013s, episode steps:  43, steps per second:  42, episode reward: 39.685, mean reward:  0.923 [-2.330, 31.622], mean action: 3.349 [1.000, 17.000],  loss: 0.012925, mae: 0.346130, mean_q: 0.342536, mean_eps: 0.000000
 24999/30000: episode: 693, duration: 0.395s, episode steps:  18, steps per second:  46, episode reward: 46.042, mean reward:  2.558 [-0.206, 32.053], mean action: 3.333 [1.000, 8.000],  loss: 0.012751, mae: 0.362521, mean_q: 0.292034, mean_eps: 0.000000
 25015/30000: episode: 694, duration: 0.342s, episode steps:  16, steps per second:  47, episode reward: 41.644, mean reward:  2.603 [-3.000, 32.260], mean action: 4.688 [1.000, 17.000],  loss: 0.020771, mae: 0.399802, mean_q: 0.293325, mean_eps: 0.000000
 25047/30000: episode: 695, duration: 0.610s, episode steps:  32, steps per second:  52, episode reward: 36.871, mean reward:  1.152 [-2.746, 32.390], mean action: 7.719 [1.000, 21.000],  loss: 0.013533, mae: 0.362404, mean_q: 0.316353, mean_eps: 0.000000
 25081/30000: episode: 696, duration: 0.622s, episode steps:  34, steps per second:  55, episode reward: 46.943, mean reward:  1.381 [-0.514, 32.010], mean action: 3.176 [0.000, 17.000],  loss: 0.014501, mae: 0.373791, mean_q: 0.290373, mean_eps: 0.000000
 25111/30000: episode: 697, duration: 0.634s, episode steps:  30, steps per second:  47, episode reward: 40.183, mean reward:  1.339 [-2.663, 31.933], mean action: 8.500 [1.000, 17.000],  loss: 0.012205, mae: 0.350201, mean_q: 0.331208, mean_eps: 0.000000
 25139/30000: episode: 698, duration: 0.547s, episode steps:  28, steps per second:  51, episode reward: 36.000, mean reward:  1.286 [-3.000, 32.160], mean action: 4.286 [1.000, 15.000],  loss: 0.016405, mae: 0.376353, mean_q: 0.334920, mean_eps: 0.000000
 25165/30000: episode: 699, duration: 0.448s, episode steps:  26, steps per second:  58, episode reward: 41.271, mean reward:  1.587 [-2.399, 31.612], mean action: 4.423 [1.000, 12.000],  loss: 0.016101, mae: 0.384654, mean_q: 0.328901, mean_eps: 0.000000
 25184/30000: episode: 700, duration: 0.499s, episode steps:  19, steps per second:  38, episode reward: 44.288, mean reward:  2.331 [-2.722, 32.100], mean action: 2.368 [1.000, 15.000],  loss: 0.013737, mae: 0.346743, mean_q: 0.371070, mean_eps: 0.000000
 25222/30000: episode: 701, duration: 1.038s, episode steps:  38, steps per second:  37, episode reward: -35.410, mean reward: -0.932 [-32.125,  2.620], mean action: 7.789 [1.000, 21.000],  loss: 0.016615, mae: 0.383555, mean_q: 0.331776, mean_eps: 0.000000
 25245/30000: episode: 702, duration: 0.392s, episode steps:  23, steps per second:  59, episode reward: 38.860, mean reward:  1.690 [-2.388, 32.390], mean action: 4.478 [1.000, 16.000],  loss: 0.020354, mae: 0.405193, mean_q: 0.302414, mean_eps: 0.000000
 25274/30000: episode: 703, duration: 0.551s, episode steps:  29, steps per second:  53, episode reward: 41.430, mean reward:  1.429 [-3.000, 32.230], mean action: 2.552 [0.000, 14.000],  loss: 0.017499, mae: 0.381074, mean_q: 0.327316, mean_eps: 0.000000
 25323/30000: episode: 704, duration: 0.924s, episode steps:  49, steps per second:  53, episode reward: 35.840, mean reward:  0.731 [-2.516, 32.090], mean action: 6.102 [1.000, 16.000],  loss: 0.017419, mae: 0.375508, mean_q: 0.361735, mean_eps: 0.000000
 25385/30000: episode: 705, duration: 1.076s, episode steps:  62, steps per second:  58, episode reward: -32.430, mean reward: -0.523 [-32.532,  2.330], mean action: 8.468 [1.000, 18.000],  loss: 0.015671, mae: 0.386521, mean_q: 0.337203, mean_eps: 0.000000
 25412/30000: episode: 706, duration: 0.423s, episode steps:  27, steps per second:  64, episode reward: 39.000, mean reward:  1.444 [-2.870, 32.380], mean action: 4.889 [0.000, 17.000],  loss: 0.018203, mae: 0.396107, mean_q: 0.348376, mean_eps: 0.000000
 25445/30000: episode: 707, duration: 0.561s, episode steps:  33, steps per second:  59, episode reward: 43.891, mean reward:  1.330 [-2.415, 32.427], mean action: 4.121 [1.000, 17.000],  loss: 0.016717, mae: 0.391536, mean_q: 0.375688, mean_eps: 0.000000
 25473/30000: episode: 708, duration: 0.517s, episode steps:  28, steps per second:  54, episode reward: 41.254, mean reward:  1.473 [-3.000, 32.040], mean action: 3.607 [1.000, 17.000],  loss: 0.015716, mae: 0.381086, mean_q: 0.342205, mean_eps: 0.000000
 25502/30000: episode: 709, duration: 0.581s, episode steps:  29, steps per second:  50, episode reward: 38.638, mean reward:  1.332 [-2.708, 32.040], mean action: 6.828 [0.000, 21.000],  loss: 0.016856, mae: 0.389247, mean_q: 0.359242, mean_eps: 0.000000
 25537/30000: episode: 710, duration: 0.668s, episode steps:  35, steps per second:  52, episode reward: 34.967, mean reward:  0.999 [-2.807, 31.933], mean action: 7.400 [1.000, 17.000],  loss: 0.015847, mae: 0.391689, mean_q: 0.334965, mean_eps: 0.000000
 25576/30000: episode: 711, duration: 0.738s, episode steps:  39, steps per second:  53, episode reward: 32.386, mean reward:  0.830 [-3.000, 31.844], mean action: 6.282 [3.000, 17.000],  loss: 0.015985, mae: 0.392088, mean_q: 0.364032, mean_eps: 0.000000
 25600/30000: episode: 712, duration: 0.474s, episode steps:  24, steps per second:  51, episode reward: 41.441, mean reward:  1.727 [-3.000, 32.120], mean action: 3.625 [1.000, 12.000],  loss: 0.016951, mae: 0.412513, mean_q: 0.327669, mean_eps: 0.000000
 25627/30000: episode: 713, duration: 0.513s, episode steps:  27, steps per second:  53, episode reward: 41.063, mean reward:  1.521 [-2.256, 32.150], mean action: 3.741 [1.000, 14.000],  loss: 0.016168, mae: 0.396096, mean_q: 0.349509, mean_eps: 0.000000
 25656/30000: episode: 714, duration: 0.610s, episode steps:  29, steps per second:  48, episode reward: 40.769, mean reward:  1.406 [-2.107, 32.020], mean action: 3.379 [1.000, 15.000],  loss: 0.016885, mae: 0.395267, mean_q: 0.327147, mean_eps: 0.000000
 25765/30000: episode: 715, duration: 1.858s, episode steps: 109, steps per second:  59, episode reward: -35.090, mean reward: -0.322 [-32.134,  3.000], mean action: 9.239 [1.000, 17.000],  loss: 0.015681, mae: 0.372172, mean_q: 0.366250, mean_eps: 0.000000
 25827/30000: episode: 716, duration: 1.041s, episode steps:  62, steps per second:  60, episode reward: 35.394, mean reward:  0.571 [-3.000, 31.928], mean action: 7.258 [1.000, 17.000],  loss: 0.015975, mae: 0.381587, mean_q: 0.317517, mean_eps: 0.000000
 25858/30000: episode: 717, duration: 0.523s, episode steps:  31, steps per second:  59, episode reward: 41.824, mean reward:  1.349 [-2.246, 32.200], mean action: 4.677 [1.000, 17.000],  loss: 0.015332, mae: 0.396363, mean_q: 0.318198, mean_eps: 0.000000
 25896/30000: episode: 718, duration: 0.729s, episode steps:  38, steps per second:  52, episode reward: 38.463, mean reward:  1.012 [-2.253, 32.220], mean action: 4.684 [1.000, 17.000],  loss: 0.016016, mae: 0.405208, mean_q: 0.334569, mean_eps: 0.000000
 25958/30000: episode: 719, duration: 1.103s, episode steps:  62, steps per second:  56, episode reward: 32.947, mean reward:  0.531 [-2.492, 32.287], mean action: 6.081 [0.000, 17.000],  loss: 0.015579, mae: 0.395245, mean_q: 0.337218, mean_eps: 0.000000
 26024/30000: episode: 720, duration: 1.261s, episode steps:  66, steps per second:  52, episode reward: 43.976, mean reward:  0.666 [-2.580, 32.000], mean action: 2.652 [1.000, 11.000],  loss: 0.015384, mae: 0.403344, mean_q: 0.305251, mean_eps: 0.000000
 26053/30000: episode: 721, duration: 0.573s, episode steps:  29, steps per second:  51, episode reward: 38.538, mean reward:  1.329 [-2.896, 32.432], mean action: 2.034 [1.000, 8.000],  loss: 0.016009, mae: 0.405730, mean_q: 0.363489, mean_eps: 0.000000
 26070/30000: episode: 722, duration: 0.351s, episode steps:  17, steps per second:  48, episode reward: 47.502, mean reward:  2.794 [-0.070, 32.180], mean action: 1.412 [1.000, 8.000],  loss: 0.017466, mae: 0.412302, mean_q: 0.318197, mean_eps: 0.000000
 26092/30000: episode: 723, duration: 0.425s, episode steps:  22, steps per second:  52, episode reward: 41.057, mean reward:  1.866 [-2.385, 32.940], mean action: 6.773 [1.000, 17.000],  loss: 0.016366, mae: 0.389905, mean_q: 0.332801, mean_eps: 0.000000
 26129/30000: episode: 724, duration: 0.679s, episode steps:  37, steps per second:  54, episode reward: 38.465, mean reward:  1.040 [-2.750, 32.109], mean action: 7.189 [1.000, 21.000],  loss: 0.012232, mae: 0.377031, mean_q: 0.321749, mean_eps: 0.000000
 26177/30000: episode: 725, duration: 1.011s, episode steps:  48, steps per second:  47, episode reward: 37.508, mean reward:  0.781 [-2.379, 32.250], mean action: 7.104 [1.000, 21.000],  loss: 0.015126, mae: 0.403054, mean_q: 0.303567, mean_eps: 0.000000
 26200/30000: episode: 726, duration: 0.431s, episode steps:  23, steps per second:  53, episode reward: 41.165, mean reward:  1.790 [-2.531, 32.261], mean action: 3.696 [1.000, 14.000],  loss: 0.012958, mae: 0.380892, mean_q: 0.343565, mean_eps: 0.000000
 26226/30000: episode: 727, duration: 0.476s, episode steps:  26, steps per second:  55, episode reward: 41.548, mean reward:  1.598 [-2.696, 31.968], mean action: 2.962 [0.000, 16.000],  loss: 0.014626, mae: 0.390022, mean_q: 0.327004, mean_eps: 0.000000
 26245/30000: episode: 728, duration: 0.355s, episode steps:  19, steps per second:  53, episode reward: 44.496, mean reward:  2.342 [-2.194, 31.931], mean action: 2.947 [0.000, 16.000],  loss: 0.014207, mae: 0.389360, mean_q: 0.310297, mean_eps: 0.000000
 26277/30000: episode: 729, duration: 0.549s, episode steps:  32, steps per second:  58, episode reward: 35.900, mean reward:  1.122 [-2.542, 32.300], mean action: 4.969 [1.000, 17.000],  loss: 0.014281, mae: 0.374247, mean_q: 0.336745, mean_eps: 0.000000
 26311/30000: episode: 730, duration: 0.635s, episode steps:  34, steps per second:  54, episode reward: 35.770, mean reward:  1.052 [-2.117, 32.140], mean action: 5.912 [1.000, 17.000],  loss: 0.013721, mae: 0.376443, mean_q: 0.299940, mean_eps: 0.000000
 26332/30000: episode: 731, duration: 0.612s, episode steps:  21, steps per second:  34, episode reward: 43.450, mean reward:  2.069 [-2.392, 31.575], mean action: 5.238 [1.000, 17.000],  loss: 0.014768, mae: 0.390046, mean_q: 0.313057, mean_eps: 0.000000
 26358/30000: episode: 732, duration: 0.470s, episode steps:  26, steps per second:  55, episode reward: 38.910, mean reward:  1.497 [-2.807, 32.090], mean action: 5.346 [1.000, 15.000],  loss: 0.018371, mae: 0.398684, mean_q: 0.319937, mean_eps: 0.000000
 26386/30000: episode: 733, duration: 0.621s, episode steps:  28, steps per second:  45, episode reward: 41.226, mean reward:  1.472 [-2.580, 32.580], mean action: 3.964 [1.000, 16.000],  loss: 0.014585, mae: 0.378564, mean_q: 0.331444, mean_eps: 0.000000
 26462/30000: episode: 734, duration: 1.548s, episode steps:  76, steps per second:  49, episode reward: 35.706, mean reward:  0.470 [-2.200, 32.060], mean action: 3.947 [1.000, 17.000],  loss: 0.016615, mae: 0.386956, mean_q: 0.328971, mean_eps: 0.000000
 26493/30000: episode: 735, duration: 0.579s, episode steps:  31, steps per second:  54, episode reward: 44.725, mean reward:  1.443 [-2.293, 32.890], mean action: 4.032 [1.000, 16.000],  loss: 0.013894, mae: 0.364222, mean_q: 0.354435, mean_eps: 0.000000
 26529/30000: episode: 736, duration: 0.670s, episode steps:  36, steps per second:  54, episode reward: 40.976, mean reward:  1.138 [-2.500, 32.120], mean action: 7.639 [0.000, 21.000],  loss: 0.017014, mae: 0.399703, mean_q: 0.326741, mean_eps: 0.000000
 26563/30000: episode: 737, duration: 0.693s, episode steps:  34, steps per second:  49, episode reward: 35.904, mean reward:  1.056 [-2.320, 32.794], mean action: 2.676 [0.000, 16.000],  loss: 0.014377, mae: 0.393789, mean_q: 0.302738, mean_eps: 0.000000
 26599/30000: episode: 738, duration: 0.773s, episode steps:  36, steps per second:  47, episode reward: 32.776, mean reward:  0.910 [-2.961, 32.290], mean action: 5.417 [1.000, 17.000],  loss: 0.016148, mae: 0.401513, mean_q: 0.299066, mean_eps: 0.000000
 26620/30000: episode: 739, duration: 0.359s, episode steps:  21, steps per second:  58, episode reward: 44.226, mean reward:  2.106 [-2.012, 32.220], mean action: 4.810 [1.000, 21.000],  loss: 0.011923, mae: 0.364425, mean_q: 0.335836, mean_eps: 0.000000
 26639/30000: episode: 740, duration: 0.348s, episode steps:  19, steps per second:  55, episode reward: 47.354, mean reward:  2.492 [-0.032, 32.670], mean action: 3.105 [1.000, 15.000],  loss: 0.014237, mae: 0.397563, mean_q: 0.292051, mean_eps: 0.000000
 26665/30000: episode: 741, duration: 0.474s, episode steps:  26, steps per second:  55, episode reward: 41.730, mean reward:  1.605 [-2.546, 32.020], mean action: 3.154 [1.000, 15.000],  loss: 0.014345, mae: 0.387272, mean_q: 0.326138, mean_eps: 0.000000
 26703/30000: episode: 742, duration: 0.701s, episode steps:  38, steps per second:  54, episode reward: 41.079, mean reward:  1.081 [-2.288, 32.077], mean action: 3.184 [1.000, 14.000],  loss: 0.016181, mae: 0.421406, mean_q: 0.289330, mean_eps: 0.000000
 26733/30000: episode: 743, duration: 0.560s, episode steps:  30, steps per second:  54, episode reward: 41.083, mean reward:  1.369 [-2.309, 32.011], mean action: 2.800 [1.000, 15.000],  loss: 0.017725, mae: 0.413883, mean_q: 0.372313, mean_eps: 0.000000
 26757/30000: episode: 744, duration: 0.446s, episode steps:  24, steps per second:  54, episode reward: 44.407, mean reward:  1.850 [-2.296, 32.288], mean action: 2.417 [1.000, 5.000],  loss: 0.021885, mae: 0.404860, mean_q: 0.394789, mean_eps: 0.000000
 26786/30000: episode: 745, duration: 0.554s, episode steps:  29, steps per second:  52, episode reward: 34.426, mean reward:  1.187 [-3.000, 32.630], mean action: 8.241 [1.000, 15.000],  loss: 0.016913, mae: 0.374251, mean_q: 0.370458, mean_eps: 0.000000
 26835/30000: episode: 746, duration: 0.895s, episode steps:  49, steps per second:  55, episode reward: 35.812, mean reward:  0.731 [-2.704, 32.070], mean action: 3.898 [1.000, 16.000],  loss: 0.017169, mae: 0.399412, mean_q: 0.358063, mean_eps: 0.000000
 26874/30000: episode: 747, duration: 0.839s, episode steps:  39, steps per second:  46, episode reward: 32.778, mean reward:  0.840 [-2.903, 32.120], mean action: 4.256 [1.000, 17.000],  loss: 0.015990, mae: 0.399228, mean_q: 0.329839, mean_eps: 0.000000
 26895/30000: episode: 748, duration: 0.390s, episode steps:  21, steps per second:  54, episode reward: 38.017, mean reward:  1.810 [-3.000, 31.985], mean action: 4.571 [1.000, 14.000],  loss: 0.013966, mae: 0.389212, mean_q: 0.325351, mean_eps: 0.000000
 26932/30000: episode: 749, duration: 0.656s, episode steps:  37, steps per second:  56, episode reward: 44.293, mean reward:  1.197 [-2.352, 32.108], mean action: 3.676 [1.000, 12.000],  loss: 0.015583, mae: 0.396830, mean_q: 0.314694, mean_eps: 0.000000
 26957/30000: episode: 750, duration: 0.469s, episode steps:  25, steps per second:  53, episode reward: 41.388, mean reward:  1.656 [-2.175, 32.160], mean action: 6.320 [1.000, 16.000],  loss: 0.017097, mae: 0.387257, mean_q: 0.318437, mean_eps: 0.000000
 26985/30000: episode: 751, duration: 0.557s, episode steps:  28, steps per second:  50, episode reward: 35.322, mean reward:  1.261 [-3.000, 32.001], mean action: 4.786 [1.000, 17.000],  loss: 0.015733, mae: 0.364513, mean_q: 0.376475, mean_eps: 0.000000
 27015/30000: episode: 752, duration: 0.584s, episode steps:  30, steps per second:  51, episode reward: 40.815, mean reward:  1.360 [-3.000, 32.470], mean action: 11.733 [1.000, 21.000],  loss: 0.015907, mae: 0.368375, mean_q: 0.351292, mean_eps: 0.000000
 27048/30000: episode: 753, duration: 0.657s, episode steps:  33, steps per second:  50, episode reward: 32.671, mean reward:  0.990 [-2.803, 31.901], mean action: 7.212 [1.000, 18.000],  loss: 0.015700, mae: 0.381273, mean_q: 0.318442, mean_eps: 0.000000
 27083/30000: episode: 754, duration: 0.653s, episode steps:  35, steps per second:  54, episode reward: 35.060, mean reward:  1.002 [-2.679, 32.056], mean action: 5.457 [1.000, 17.000],  loss: 0.016384, mae: 0.365485, mean_q: 0.340348, mean_eps: 0.000000
 27121/30000: episode: 755, duration: 0.721s, episode steps:  38, steps per second:  53, episode reward: 40.618, mean reward:  1.069 [-3.000, 32.120], mean action: 3.105 [1.000, 12.000],  loss: 0.016199, mae: 0.366633, mean_q: 0.406011, mean_eps: 0.000000
 27167/30000: episode: 756, duration: 0.823s, episode steps:  46, steps per second:  56, episode reward: -35.580, mean reward: -0.773 [-33.000,  2.332], mean action: 5.348 [1.000, 21.000],  loss: 0.014166, mae: 0.390221, mean_q: 0.261993, mean_eps: 0.000000
 27197/30000: episode: 757, duration: 0.612s, episode steps:  30, steps per second:  49, episode reward: 38.329, mean reward:  1.278 [-2.758, 31.990], mean action: 3.600 [1.000, 18.000],  loss: 0.016159, mae: 0.385002, mean_q: 0.326927, mean_eps: 0.000000
 27229/30000: episode: 758, duration: 0.622s, episode steps:  32, steps per second:  51, episode reward: 38.838, mean reward:  1.214 [-2.165, 32.250], mean action: 4.344 [1.000, 14.000],  loss: 0.014196, mae: 0.367149, mean_q: 0.337123, mean_eps: 0.000000
 27248/30000: episode: 759, duration: 0.401s, episode steps:  19, steps per second:  47, episode reward: 35.694, mean reward:  1.879 [-3.000, 31.804], mean action: 4.526 [1.000, 17.000],  loss: 0.016017, mae: 0.391358, mean_q: 0.339802, mean_eps: 0.000000
 27302/30000: episode: 760, duration: 0.938s, episode steps:  54, steps per second:  58, episode reward: -33.000, mean reward: -0.611 [-32.409,  2.461], mean action: 5.130 [0.000, 17.000],  loss: 0.017436, mae: 0.407859, mean_q: 0.296570, mean_eps: 0.000000
 27347/30000: episode: 761, duration: 0.791s, episode steps:  45, steps per second:  57, episode reward: 32.738, mean reward:  0.728 [-3.000, 32.070], mean action: 5.244 [0.000, 17.000],  loss: 0.014635, mae: 0.378396, mean_q: 0.373286, mean_eps: 0.000000
 27370/30000: episode: 762, duration: 0.484s, episode steps:  23, steps per second:  47, episode reward: 44.058, mean reward:  1.916 [-2.108, 31.356], mean action: 3.957 [1.000, 17.000],  loss: 0.016820, mae: 0.389740, mean_q: 0.326767, mean_eps: 0.000000
 27411/30000: episode: 763, duration: 0.949s, episode steps:  41, steps per second:  43, episode reward: 33.000, mean reward:  0.805 [-2.610, 29.659], mean action: 4.415 [0.000, 21.000],  loss: 0.015263, mae: 0.375957, mean_q: 0.350945, mean_eps: 0.000000
 27441/30000: episode: 764, duration: 0.629s, episode steps:  30, steps per second:  48, episode reward: -32.380, mean reward: -1.079 [-32.019,  2.220], mean action: 6.567 [1.000, 15.000],  loss: 0.019966, mae: 0.405979, mean_q: 0.319330, mean_eps: 0.000000
 27468/30000: episode: 765, duration: 0.518s, episode steps:  27, steps per second:  52, episode reward: 40.781, mean reward:  1.510 [-2.444, 32.190], mean action: 6.000 [1.000, 15.000],  loss: 0.016393, mae: 0.407041, mean_q: 0.326669, mean_eps: 0.000000
 27510/30000: episode: 766, duration: 0.976s, episode steps:  42, steps per second:  43, episode reward: 36.000, mean reward:  0.857 [-2.939, 32.160], mean action: 5.143 [0.000, 17.000],  loss: 0.015649, mae: 0.398188, mean_q: 0.328789, mean_eps: 0.000000
 27543/30000: episode: 767, duration: 0.702s, episode steps:  33, steps per second:  47, episode reward: 40.947, mean reward:  1.241 [-2.508, 32.650], mean action: 2.848 [1.000, 12.000],  loss: 0.015649, mae: 0.398967, mean_q: 0.332055, mean_eps: 0.000000
 27578/30000: episode: 768, duration: 0.953s, episode steps:  35, steps per second:  37, episode reward: 38.383, mean reward:  1.097 [-2.879, 32.090], mean action: 5.057 [0.000, 15.000],  loss: 0.017659, mae: 0.392712, mean_q: 0.383803, mean_eps: 0.000000
 27599/30000: episode: 769, duration: 0.573s, episode steps:  21, steps per second:  37, episode reward: 43.114, mean reward:  2.053 [-2.029, 31.531], mean action: 4.286 [0.000, 16.000],  loss: 0.014306, mae: 0.387952, mean_q: 0.376392, mean_eps: 0.000000
 27641/30000: episode: 770, duration: 0.815s, episode steps:  42, steps per second:  52, episode reward: 41.004, mean reward:  0.976 [-2.028, 31.848], mean action: 2.238 [1.000, 16.000],  loss: 0.014744, mae: 0.393470, mean_q: 0.349712, mean_eps: 0.000000
 27672/30000: episode: 771, duration: 0.604s, episode steps:  31, steps per second:  51, episode reward: 40.342, mean reward:  1.301 [-2.383, 31.675], mean action: 7.516 [1.000, 15.000],  loss: 0.015333, mae: 0.401238, mean_q: 0.322217, mean_eps: 0.000000
 27718/30000: episode: 772, duration: 0.817s, episode steps:  46, steps per second:  56, episode reward: 35.904, mean reward:  0.781 [-3.000, 31.984], mean action: 3.652 [1.000, 17.000],  loss: 0.016011, mae: 0.390041, mean_q: 0.352842, mean_eps: 0.000000
 27755/30000: episode: 773, duration: 0.694s, episode steps:  37, steps per second:  53, episode reward: 35.420, mean reward:  0.957 [-2.373, 32.980], mean action: 6.297 [1.000, 17.000],  loss: 0.016479, mae: 0.398293, mean_q: 0.382712, mean_eps: 0.000000
 27790/30000: episode: 774, duration: 0.809s, episode steps:  35, steps per second:  43, episode reward: 35.903, mean reward:  1.026 [-2.783, 32.063], mean action: 4.200 [1.000, 15.000],  loss: 0.013413, mae: 0.401381, mean_q: 0.288462, mean_eps: 0.000000
 27814/30000: episode: 775, duration: 0.451s, episode steps:  24, steps per second:  53, episode reward: 35.063, mean reward:  1.461 [-2.464, 32.010], mean action: 3.917 [1.000, 14.000],  loss: 0.018973, mae: 0.428371, mean_q: 0.334807, mean_eps: 0.000000
 27842/30000: episode: 776, duration: 0.505s, episode steps:  28, steps per second:  55, episode reward: 41.058, mean reward:  1.466 [-3.000, 31.824], mean action: 5.679 [1.000, 21.000],  loss: 0.016903, mae: 0.404017, mean_q: 0.378507, mean_eps: 0.000000
 27868/30000: episode: 777, duration: 0.500s, episode steps:  26, steps per second:  52, episode reward: 44.341, mean reward:  1.705 [-2.622, 32.430], mean action: 4.192 [1.000, 14.000],  loss: 0.020401, mae: 0.437132, mean_q: 0.370871, mean_eps: 0.000000
 27912/30000: episode: 778, duration: 0.789s, episode steps:  44, steps per second:  56, episode reward: -34.500, mean reward: -0.784 [-32.315,  2.431], mean action: 7.136 [1.000, 17.000],  loss: 0.017224, mae: 0.420514, mean_q: 0.334256, mean_eps: 0.000000
 27941/30000: episode: 779, duration: 0.509s, episode steps:  29, steps per second:  57, episode reward: 41.579, mean reward:  1.434 [-2.252, 31.936], mean action: 4.414 [1.000, 17.000],  loss: 0.016141, mae: 0.423506, mean_q: 0.346930, mean_eps: 0.000000
 27976/30000: episode: 780, duration: 0.625s, episode steps:  35, steps per second:  56, episode reward: 38.615, mean reward:  1.103 [-2.177, 32.249], mean action: 3.600 [1.000, 15.000],  loss: 0.015408, mae: 0.412748, mean_q: 0.326561, mean_eps: 0.000000
 28012/30000: episode: 781, duration: 0.665s, episode steps:  36, steps per second:  54, episode reward: 32.769, mean reward:  0.910 [-3.000, 32.095], mean action: 5.750 [1.000, 17.000],  loss: 0.018196, mae: 0.410061, mean_q: 0.360031, mean_eps: 0.000000
 28043/30000: episode: 782, duration: 0.536s, episode steps:  31, steps per second:  58, episode reward: 40.950, mean reward:  1.321 [-2.947, 31.823], mean action: 6.677 [1.000, 21.000],  loss: 0.017977, mae: 0.405064, mean_q: 0.340419, mean_eps: 0.000000
 28075/30000: episode: 783, duration: 0.559s, episode steps:  32, steps per second:  57, episode reward: 35.299, mean reward:  1.103 [-3.000, 32.080], mean action: 6.469 [1.000, 17.000],  loss: 0.015418, mae: 0.404357, mean_q: 0.294892, mean_eps: 0.000000
 28110/30000: episode: 784, duration: 0.636s, episode steps:  35, steps per second:  55, episode reward: 43.564, mean reward:  1.245 [-2.313, 32.050], mean action: 4.029 [1.000, 14.000],  loss: 0.015650, mae: 0.412972, mean_q: 0.357594, mean_eps: 0.000000
 28141/30000: episode: 785, duration: 0.582s, episode steps:  31, steps per second:  53, episode reward: 38.072, mean reward:  1.228 [-2.805, 31.835], mean action: 3.355 [1.000, 17.000],  loss: 0.019533, mae: 0.439186, mean_q: 0.324098, mean_eps: 0.000000
 28176/30000: episode: 786, duration: 0.703s, episode steps:  35, steps per second:  50, episode reward: 38.395, mean reward:  1.097 [-2.383, 32.373], mean action: 2.914 [1.000, 12.000],  loss: 0.018124, mae: 0.424328, mean_q: 0.359674, mean_eps: 0.000000
 28194/30000: episode: 787, duration: 0.341s, episode steps:  18, steps per second:  53, episode reward: 45.000, mean reward:  2.500 [-2.237, 33.000], mean action: 6.667 [1.000, 21.000],  loss: 0.018340, mae: 0.418858, mean_q: 0.312381, mean_eps: 0.000000
 28226/30000: episode: 788, duration: 0.621s, episode steps:  32, steps per second:  52, episode reward: 39.000, mean reward:  1.219 [-3.000, 32.390], mean action: 3.281 [0.000, 17.000],  loss: 0.015361, mae: 0.422836, mean_q: 0.307895, mean_eps: 0.000000
 28247/30000: episode: 789, duration: 0.564s, episode steps:  21, steps per second:  37, episode reward: 44.180, mean reward:  2.104 [-2.231, 32.486], mean action: 2.048 [1.000, 11.000],  loss: 0.016597, mae: 0.418630, mean_q: 0.359310, mean_eps: 0.000000
 28273/30000: episode: 790, duration: 0.596s, episode steps:  26, steps per second:  44, episode reward: 32.875, mean reward:  1.264 [-3.000, 32.170], mean action: 6.077 [1.000, 17.000],  loss: 0.018486, mae: 0.431846, mean_q: 0.349340, mean_eps: 0.000000
 28302/30000: episode: 791, duration: 0.636s, episode steps:  29, steps per second:  46, episode reward: 35.823, mean reward:  1.235 [-3.000, 32.439], mean action: 4.448 [0.000, 15.000],  loss: 0.014497, mae: 0.408134, mean_q: 0.375038, mean_eps: 0.000000
 28332/30000: episode: 792, duration: 0.595s, episode steps:  30, steps per second:  50, episode reward: 38.203, mean reward:  1.273 [-2.283, 32.310], mean action: 4.667 [1.000, 16.000],  loss: 0.016040, mae: 0.421819, mean_q: 0.315595, mean_eps: 0.000000
 28361/30000: episode: 793, duration: 0.500s, episode steps:  29, steps per second:  58, episode reward: 38.762, mean reward:  1.337 [-2.420, 31.952], mean action: 3.000 [1.000, 15.000],  loss: 0.015333, mae: 0.400597, mean_q: 0.334714, mean_eps: 0.000000
 28383/30000: episode: 794, duration: 0.399s, episode steps:  22, steps per second:  55, episode reward: 41.576, mean reward:  1.890 [-2.276, 32.067], mean action: 3.500 [1.000, 14.000],  loss: 0.017722, mae: 0.426793, mean_q: 0.324399, mean_eps: 0.000000
 28409/30000: episode: 795, duration: 0.460s, episode steps:  26, steps per second:  57, episode reward: 38.345, mean reward:  1.475 [-2.816, 31.816], mean action: 3.923 [1.000, 14.000],  loss: 0.016475, mae: 0.412079, mean_q: 0.359048, mean_eps: 0.000000
 28447/30000: episode: 796, duration: 0.695s, episode steps:  38, steps per second:  55, episode reward: 32.248, mean reward:  0.849 [-2.879, 32.172], mean action: 6.553 [1.000, 15.000],  loss: 0.014907, mae: 0.415504, mean_q: 0.278413, mean_eps: 0.000000
 28477/30000: episode: 797, duration: 0.563s, episode steps:  30, steps per second:  53, episode reward: 40.927, mean reward:  1.364 [-2.391, 32.410], mean action: 7.233 [1.000, 15.000],  loss: 0.018186, mae: 0.434499, mean_q: 0.331712, mean_eps: 0.000000
 28500/30000: episode: 798, duration: 0.462s, episode steps:  23, steps per second:  50, episode reward: 41.556, mean reward:  1.807 [-2.027, 30.545], mean action: 2.826 [1.000, 15.000],  loss: 0.018185, mae: 0.433140, mean_q: 0.345594, mean_eps: 0.000000
 28525/30000: episode: 799, duration: 0.505s, episode steps:  25, steps per second:  50, episode reward: 41.567, mean reward:  1.663 [-2.331, 32.180], mean action: 2.600 [1.000, 12.000],  loss: 0.017224, mae: 0.421211, mean_q: 0.348301, mean_eps: 0.000000
 28572/30000: episode: 800, duration: 0.802s, episode steps:  47, steps per second:  59, episode reward: 39.000, mean reward:  0.830 [-2.365, 32.220], mean action: 2.766 [1.000, 17.000],  loss: 0.013503, mae: 0.402461, mean_q: 0.339885, mean_eps: 0.000000
 28598/30000: episode: 801, duration: 0.537s, episode steps:  26, steps per second:  48, episode reward: 38.010, mean reward:  1.462 [-2.373, 32.220], mean action: 5.846 [1.000, 15.000],  loss: 0.016027, mae: 0.425069, mean_q: 0.329070, mean_eps: 0.000000
 28619/30000: episode: 802, duration: 0.433s, episode steps:  21, steps per second:  48, episode reward: 44.153, mean reward:  2.103 [-2.617, 32.160], mean action: 3.905 [1.000, 17.000],  loss: 0.017182, mae: 0.438836, mean_q: 0.307346, mean_eps: 0.000000
 28642/30000: episode: 803, duration: 0.433s, episode steps:  23, steps per second:  53, episode reward: 41.438, mean reward:  1.802 [-2.220, 32.020], mean action: 1.783 [0.000, 17.000],  loss: 0.015489, mae: 0.436609, mean_q: 0.319768, mean_eps: 0.000000
 28674/30000: episode: 804, duration: 0.579s, episode steps:  32, steps per second:  55, episode reward: 46.970, mean reward:  1.468 [-0.173, 32.462], mean action: 2.594 [0.000, 12.000],  loss: 0.014689, mae: 0.425402, mean_q: 0.320547, mean_eps: 0.000000
 28719/30000: episode: 805, duration: 0.803s, episode steps:  45, steps per second:  56, episode reward: 41.475, mean reward:  0.922 [-2.230, 32.217], mean action: 3.778 [1.000, 12.000],  loss: 0.013557, mae: 0.407098, mean_q: 0.326064, mean_eps: 0.000000
 28742/30000: episode: 806, duration: 0.449s, episode steps:  23, steps per second:  51, episode reward: 41.691, mean reward:  1.813 [-2.650, 33.000], mean action: 2.826 [1.000, 11.000],  loss: 0.016795, mae: 0.433866, mean_q: 0.303718, mean_eps: 0.000000
 28770/30000: episode: 807, duration: 0.541s, episode steps:  28, steps per second:  52, episode reward: 38.039, mean reward:  1.359 [-2.358, 32.230], mean action: 4.571 [0.000, 17.000],  loss: 0.012840, mae: 0.407943, mean_q: 0.299420, mean_eps: 0.000000
 28802/30000: episode: 808, duration: 0.667s, episode steps:  32, steps per second:  48, episode reward: 38.772, mean reward:  1.212 [-3.000, 32.040], mean action: 9.875 [1.000, 21.000],  loss: 0.013653, mae: 0.403228, mean_q: 0.328768, mean_eps: 0.000000
 28829/30000: episode: 809, duration: 0.474s, episode steps:  27, steps per second:  57, episode reward: 42.000, mean reward:  1.556 [-2.514, 32.050], mean action: 1.889 [0.000, 16.000],  loss: 0.015899, mae: 0.418490, mean_q: 0.364736, mean_eps: 0.000000
 28854/30000: episode: 810, duration: 0.460s, episode steps:  25, steps per second:  54, episode reward: 39.000, mean reward:  1.560 [-2.420, 30.032], mean action: 2.640 [1.000, 17.000],  loss: 0.016538, mae: 0.411962, mean_q: 0.350734, mean_eps: 0.000000
 28887/30000: episode: 811, duration: 0.584s, episode steps:  33, steps per second:  56, episode reward: 38.518, mean reward:  1.167 [-2.389, 32.134], mean action: 2.909 [0.000, 17.000],  loss: 0.017896, mae: 0.433098, mean_q: 0.328013, mean_eps: 0.000000
 28919/30000: episode: 812, duration: 0.800s, episode steps:  32, steps per second:  40, episode reward: 34.966, mean reward:  1.093 [-3.000, 32.070], mean action: 5.469 [1.000, 17.000],  loss: 0.017003, mae: 0.420876, mean_q: 0.345844, mean_eps: 0.000000
 28988/30000: episode: 813, duration: 1.207s, episode steps:  69, steps per second:  57, episode reward: -32.950, mean reward: -0.478 [-32.188,  2.430], mean action: 8.159 [1.000, 17.000],  loss: 0.015748, mae: 0.429203, mean_q: 0.317953, mean_eps: 0.000000
 29025/30000: episode: 814, duration: 0.928s, episode steps:  37, steps per second:  40, episode reward: 33.000, mean reward:  0.892 [-3.000, 32.080], mean action: 4.541 [1.000, 17.000],  loss: 0.016066, mae: 0.412926, mean_q: 0.369924, mean_eps: 0.000000
 29041/30000: episode: 815, duration: 0.313s, episode steps:  16, steps per second:  51, episode reward: 45.000, mean reward:  2.812 [-2.440, 32.790], mean action: 3.000 [1.000, 15.000],  loss: 0.016658, mae: 0.434455, mean_q: 0.329008, mean_eps: 0.000000
 29071/30000: episode: 816, duration: 0.546s, episode steps:  30, steps per second:  55, episode reward: 38.362, mean reward:  1.279 [-3.000, 32.230], mean action: 5.033 [1.000, 17.000],  loss: 0.016147, mae: 0.430907, mean_q: 0.313637, mean_eps: 0.000000
 29102/30000: episode: 817, duration: 0.577s, episode steps:  31, steps per second:  54, episode reward: 34.576, mean reward:  1.115 [-2.532, 31.714], mean action: 6.613 [1.000, 17.000],  loss: 0.012488, mae: 0.420634, mean_q: 0.304326, mean_eps: 0.000000
 29137/30000: episode: 818, duration: 0.755s, episode steps:  35, steps per second:  46, episode reward: 41.244, mean reward:  1.178 [-2.585, 32.466], mean action: 1.714 [0.000, 17.000],  loss: 0.019795, mae: 0.455989, mean_q: 0.298840, mean_eps: 0.000000
 29174/30000: episode: 819, duration: 0.701s, episode steps:  37, steps per second:  53, episode reward: 35.741, mean reward:  0.966 [-2.470, 31.871], mean action: 5.324 [1.000, 17.000],  loss: 0.017513, mae: 0.405110, mean_q: 0.409183, mean_eps: 0.000000
 29224/30000: episode: 820, duration: 1.040s, episode steps:  50, steps per second:  48, episode reward: 32.350, mean reward:  0.647 [-2.546, 31.890], mean action: 7.840 [1.000, 17.000],  loss: 0.016622, mae: 0.402630, mean_q: 0.370272, mean_eps: 0.000000
 29262/30000: episode: 821, duration: 0.914s, episode steps:  38, steps per second:  42, episode reward: 38.444, mean reward:  1.012 [-3.000, 32.650], mean action: 3.658 [0.000, 17.000],  loss: 0.017086, mae: 0.406946, mean_q: 0.385284, mean_eps: 0.000000
 29290/30000: episode: 822, duration: 0.552s, episode steps:  28, steps per second:  51, episode reward: 41.457, mean reward:  1.481 [-2.294, 32.130], mean action: 2.000 [1.000, 11.000],  loss: 0.018004, mae: 0.408907, mean_q: 0.351230, mean_eps: 0.000000
 29322/30000: episode: 823, duration: 0.681s, episode steps:  32, steps per second:  47, episode reward: 38.016, mean reward:  1.188 [-2.483, 32.200], mean action: 4.250 [0.000, 15.000],  loss: 0.017486, mae: 0.416062, mean_q: 0.318138, mean_eps: 0.000000
 29351/30000: episode: 824, duration: 0.601s, episode steps:  29, steps per second:  48, episode reward: 41.809, mean reward:  1.442 [-2.174, 32.454], mean action: 4.310 [1.000, 16.000],  loss: 0.015991, mae: 0.430775, mean_q: 0.314994, mean_eps: 0.000000
 29381/30000: episode: 825, duration: 0.564s, episode steps:  30, steps per second:  53, episode reward: 38.741, mean reward:  1.291 [-2.583, 32.140], mean action: 4.433 [1.000, 16.000],  loss: 0.013476, mae: 0.406934, mean_q: 0.328676, mean_eps: 0.000000
 29407/30000: episode: 826, duration: 0.445s, episode steps:  26, steps per second:  58, episode reward: 36.000, mean reward:  1.385 [-2.592, 32.180], mean action: 4.269 [1.000, 17.000],  loss: 0.015990, mae: 0.421537, mean_q: 0.293106, mean_eps: 0.000000
 29468/30000: episode: 827, duration: 1.014s, episode steps:  61, steps per second:  60, episode reward: 35.811, mean reward:  0.587 [-2.622, 32.230], mean action: 8.574 [1.000, 17.000],  loss: 0.014720, mae: 0.421953, mean_q: 0.326627, mean_eps: 0.000000
 29503/30000: episode: 828, duration: 0.648s, episode steps:  35, steps per second:  54, episode reward: 41.690, mean reward:  1.191 [-2.405, 32.110], mean action: 2.457 [1.000, 15.000],  loss: 0.018653, mae: 0.440523, mean_q: 0.326699, mean_eps: 0.000000
 29535/30000: episode: 829, duration: 0.732s, episode steps:  32, steps per second:  44, episode reward: 38.373, mean reward:  1.199 [-2.737, 31.925], mean action: 4.469 [1.000, 17.000],  loss: 0.016947, mae: 0.419409, mean_q: 0.327660, mean_eps: 0.000000
 29562/30000: episode: 830, duration: 0.558s, episode steps:  27, steps per second:  48, episode reward: 41.488, mean reward:  1.537 [-2.513, 31.826], mean action: 4.704 [1.000, 21.000],  loss: 0.013009, mae: 0.393083, mean_q: 0.345680, mean_eps: 0.000000
 29588/30000: episode: 831, duration: 0.525s, episode steps:  26, steps per second:  50, episode reward: 41.727, mean reward:  1.605 [-2.632, 32.280], mean action: 5.269 [3.000, 17.000],  loss: 0.017019, mae: 0.419000, mean_q: 0.337701, mean_eps: 0.000000
 29619/30000: episode: 832, duration: 0.760s, episode steps:  31, steps per second:  41, episode reward: 41.209, mean reward:  1.329 [-2.321, 32.060], mean action: 6.194 [0.000, 17.000],  loss: 0.013611, mae: 0.408289, mean_q: 0.326996, mean_eps: 0.000000
 29644/30000: episode: 833, duration: 0.594s, episode steps:  25, steps per second:  42, episode reward: 41.299, mean reward:  1.652 [-2.317, 32.720], mean action: 6.280 [1.000, 16.000],  loss: 0.016132, mae: 0.422102, mean_q: 0.357928, mean_eps: 0.000000
 29671/30000: episode: 834, duration: 0.552s, episode steps:  27, steps per second:  49, episode reward: 41.131, mean reward:  1.523 [-3.000, 32.373], mean action: 3.926 [0.000, 17.000],  loss: 0.017856, mae: 0.437875, mean_q: 0.292593, mean_eps: 0.000000
 29706/30000: episode: 835, duration: 0.614s, episode steps:  35, steps per second:  57, episode reward: 43.292, mean reward:  1.237 [-2.203, 31.321], mean action: 3.800 [1.000, 15.000],  loss: 0.014748, mae: 0.410133, mean_q: 0.376749, mean_eps: 0.000000
 29743/30000: episode: 836, duration: 0.694s, episode steps:  37, steps per second:  53, episode reward: 43.053, mean reward:  1.164 [-2.381, 32.052], mean action: 3.622 [3.000, 14.000],  loss: 0.018571, mae: 0.443585, mean_q: 0.330989, mean_eps: 0.000000
 29763/30000: episode: 837, duration: 0.354s, episode steps:  20, steps per second:  56, episode reward: 38.827, mean reward:  1.941 [-3.000, 32.700], mean action: 5.800 [1.000, 15.000],  loss: 0.019824, mae: 0.451793, mean_q: 0.348347, mean_eps: 0.000000
 29782/30000: episode: 838, duration: 0.386s, episode steps:  19, steps per second:  49, episode reward: 38.802, mean reward:  2.042 [-3.000, 32.220], mean action: 7.000 [1.000, 15.000],  loss: 0.018456, mae: 0.448303, mean_q: 0.324596, mean_eps: 0.000000
 29822/30000: episode: 839, duration: 0.792s, episode steps:  40, steps per second:  51, episode reward: 44.749, mean reward:  1.119 [-1.809, 32.099], mean action: 3.175 [1.000, 16.000],  loss: 0.015108, mae: 0.431138, mean_q: 0.330560, mean_eps: 0.000000
 29894/30000: episode: 840, duration: 1.354s, episode steps:  72, steps per second:  53, episode reward: 41.163, mean reward:  0.572 [-2.523, 32.495], mean action: 1.806 [1.000, 15.000],  loss: 0.016377, mae: 0.417818, mean_q: 0.347399, mean_eps: 0.000000
 29927/30000: episode: 841, duration: 0.563s, episode steps:  33, steps per second:  59, episode reward: 35.403, mean reward:  1.073 [-2.903, 32.070], mean action: 6.909 [1.000, 15.000],  loss: 0.014942, mae: 0.399223, mean_q: 0.367144, mean_eps: 0.000000
 29959/30000: episode: 842, duration: 0.903s, episode steps:  32, steps per second:  35, episode reward: 38.753, mean reward:  1.211 [-2.338, 32.520], mean action: 3.750 [1.000, 15.000],  loss: 0.019098, mae: 0.435525, mean_q: 0.320173, mean_eps: 0.000000
done, took 656.291 seconds
DQN Evaluation: 747 victories out of 843 episodes
Results against random player:
DQN Evaluation: 271 victories out of 300 episodes

Results against max player:
DQN Evaluation: 168 victories out of 300 episodes
