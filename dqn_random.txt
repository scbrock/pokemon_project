params 0
{'model_type': 'dqn_agent', 'l1_out': 128, 'l2_out': 64, 'gamma': 0.5, 'target_model_update': 1, 'delta_clip': 0.01, 'nb_steps_warmup': 1000, 'enable_dueling_dqn__': False, 'enable_double_dqn__': True, 'dueling_type__': 'avg'}
trial 0
Training for 10000 steps ...
   73/10000: episode: 1, duration: 1.698s, episode steps:  73, steps per second:  43, episode reward: -35.450, mean reward: -0.486 [-32.794,  3.059], mean action: 10.137 [3.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  120/10000: episode: 2, duration: 0.283s, episode steps:  47, steps per second: 166, episode reward: -35.160, mean reward: -0.748 [-31.519,  2.901], mean action: 13.489 [3.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  161/10000: episode: 3, duration: 0.267s, episode steps:  41, steps per second: 154, episode reward: -37.760, mean reward: -0.921 [-32.558,  2.641], mean action: 14.244 [7.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  210/10000: episode: 4, duration: 0.308s, episode steps:  49, steps per second: 159, episode reward: 35.659, mean reward:  0.728 [-3.000, 31.922], mean action: 14.551 [3.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  298/10000: episode: 5, duration: 0.515s, episode steps:  88, steps per second: 171, episode reward: 32.872, mean reward:  0.374 [-2.678, 32.084], mean action: 11.898 [3.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  364/10000: episode: 6, duration: 0.439s, episode steps:  66, steps per second: 150, episode reward: -46.770, mean reward: -0.709 [-32.336,  0.390], mean action: 14.758 [7.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  408/10000: episode: 7, duration: 0.265s, episode steps:  44, steps per second: 166, episode reward: 38.643, mean reward:  0.878 [-2.708, 32.220], mean action: 10.932 [1.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  518/10000: episode: 8, duration: 0.673s, episode steps: 110, steps per second: 163, episode reward: -37.220, mean reward: -0.338 [-33.000,  2.280], mean action: 13.764 [7.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  593/10000: episode: 9, duration: 0.492s, episode steps:  75, steps per second: 153, episode reward: 32.444, mean reward:  0.433 [-2.616, 32.500], mean action: 11.387 [3.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  648/10000: episode: 10, duration: 0.323s, episode steps:  55, steps per second: 170, episode reward: 32.604, mean reward:  0.593 [-3.000, 31.765], mean action: 15.455 [3.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  763/10000: episode: 11, duration: 0.657s, episode steps: 115, steps per second: 175, episode reward: -40.480, mean reward: -0.352 [-32.100,  2.350], mean action: 12.609 [1.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  853/10000: episode: 12, duration: 0.545s, episode steps:  90, steps per second: 165, episode reward: -37.570, mean reward: -0.417 [-32.038,  3.000], mean action: 15.122 [7.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  976/10000: episode: 13, duration: 0.723s, episode steps: 123, steps per second: 170, episode reward: -32.610, mean reward: -0.265 [-32.048,  3.000], mean action: 13.089 [1.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1038/10000: episode: 14, duration: 1.205s, episode steps:  62, steps per second:  51, episode reward: -34.310, mean reward: -0.553 [-32.088,  2.727], mean action: 14.161 [6.000, 21.000],  loss: 0.009000, mae: 0.397534, mean_q: 0.372309, mean_eps: 0.000000
 1078/10000: episode: 15, duration: 0.431s, episode steps:  40, steps per second:  93, episode reward: 38.509, mean reward:  0.963 [-2.712, 30.005], mean action: 9.325 [1.000, 21.000],  loss: 0.007374, mae: 0.346333, mean_q: 0.270069, mean_eps: 0.000000
 1125/10000: episode: 16, duration: 0.492s, episode steps:  47, steps per second:  96, episode reward: 35.387, mean reward:  0.753 [-2.372, 32.270], mean action: 10.149 [1.000, 21.000],  loss: 0.009277, mae: 0.377973, mean_q: 0.184302, mean_eps: 0.000000
 1208/10000: episode: 17, duration: 0.842s, episode steps:  83, steps per second:  99, episode reward: 38.117, mean reward:  0.459 [-3.000, 31.971], mean action: 9.542 [1.000, 21.000],  loss: 0.008713, mae: 0.377937, mean_q: 0.169595, mean_eps: 0.000000
 1265/10000: episode: 18, duration: 0.621s, episode steps:  57, steps per second:  92, episode reward: -32.500, mean reward: -0.570 [-30.171,  2.940], mean action: 10.386 [1.000, 21.000],  loss: 0.007210, mae: 0.366471, mean_q: 0.161927, mean_eps: 0.000000
 1324/10000: episode: 19, duration: 0.693s, episode steps:  59, steps per second:  85, episode reward: -38.080, mean reward: -0.645 [-32.271,  2.360], mean action: 11.966 [3.000, 21.000],  loss: 0.010444, mae: 0.380377, mean_q: 0.153769, mean_eps: 0.000000
 1449/10000: episode: 20, duration: 1.238s, episode steps: 125, steps per second: 101, episode reward: 37.537, mean reward:  0.300 [-2.762, 31.875], mean action: 13.032 [1.000, 21.000],  loss: 0.008460, mae: 0.369522, mean_q: 0.143592, mean_eps: 0.000000
 1517/10000: episode: 21, duration: 0.675s, episode steps:  68, steps per second: 101, episode reward: -32.870, mean reward: -0.483 [-33.000,  2.810], mean action: 10.882 [1.000, 21.000],  loss: 0.006870, mae: 0.361948, mean_q: 0.137789, mean_eps: 0.000000
 1537/10000: episode: 22, duration: 0.220s, episode steps:  20, steps per second:  91, episode reward: 44.043, mean reward:  2.202 [-3.000, 32.371], mean action: 7.350 [1.000, 21.000],  loss: 0.007765, mae: 0.361425, mean_q: 0.139944, mean_eps: 0.000000
 1582/10000: episode: 23, duration: 0.453s, episode steps:  45, steps per second:  99, episode reward: -32.280, mean reward: -0.717 [-32.006,  2.710], mean action: 11.400 [3.000, 21.000],  loss: 0.008352, mae: 0.348131, mean_q: 0.177207, mean_eps: 0.000000
 1628/10000: episode: 24, duration: 0.473s, episode steps:  46, steps per second:  97, episode reward: 32.110, mean reward:  0.698 [-2.620, 32.020], mean action: 11.217 [3.000, 21.000],  loss: 0.009080, mae: 0.349982, mean_q: 0.166437, mean_eps: 0.000000
 1667/10000: episode: 25, duration: 0.386s, episode steps:  39, steps per second: 101, episode reward: -42.420, mean reward: -1.088 [-32.133,  2.050], mean action: 12.692 [3.000, 21.000],  loss: 0.009208, mae: 0.348224, mean_q: 0.179637, mean_eps: 0.000000
 1736/10000: episode: 26, duration: 0.675s, episode steps:  69, steps per second: 102, episode reward: -32.780, mean reward: -0.475 [-31.832,  3.000], mean action: 9.319 [3.000, 21.000],  loss: 0.009159, mae: 0.346850, mean_q: 0.188655, mean_eps: 0.000000
 1798/10000: episode: 27, duration: 0.673s, episode steps:  62, steps per second:  92, episode reward: 35.572, mean reward:  0.574 [-3.000, 32.770], mean action: 6.016 [1.000, 21.000],  loss: 0.008005, mae: 0.326504, mean_q: 0.232362, mean_eps: 0.000000
 1837/10000: episode: 28, duration: 0.410s, episode steps:  39, steps per second:  95, episode reward: 34.537, mean reward:  0.886 [-2.738, 31.735], mean action: 7.718 [1.000, 21.000],  loss: 0.009012, mae: 0.354897, mean_q: 0.166952, mean_eps: 0.000000
 1881/10000: episode: 29, duration: 0.452s, episode steps:  44, steps per second:  97, episode reward: 32.733, mean reward:  0.744 [-2.567, 32.120], mean action: 6.909 [1.000, 21.000],  loss: 0.008789, mae: 0.356807, mean_q: 0.179910, mean_eps: 0.000000
 1918/10000: episode: 30, duration: 0.379s, episode steps:  37, steps per second:  98, episode reward: 43.819, mean reward:  1.184 [-2.181, 32.540], mean action: 4.703 [0.000, 17.000],  loss: 0.008612, mae: 0.350605, mean_q: 0.187930, mean_eps: 0.000000
 1960/10000: episode: 31, duration: 0.431s, episode steps:  42, steps per second:  98, episode reward: -32.350, mean reward: -0.770 [-32.551,  2.898], mean action: 10.452 [1.000, 21.000],  loss: 0.010993, mae: 0.362419, mean_q: 0.182025, mean_eps: 0.000000
 1984/10000: episode: 32, duration: 0.251s, episode steps:  24, steps per second:  95, episode reward: 37.745, mean reward:  1.573 [-2.878, 32.310], mean action: 4.792 [1.000, 21.000],  loss: 0.008263, mae: 0.349435, mean_q: 0.183047, mean_eps: 0.000000
 2027/10000: episode: 33, duration: 0.443s, episode steps:  43, steps per second:  97, episode reward: 37.437, mean reward:  0.871 [-2.611, 32.130], mean action: 3.605 [0.000, 17.000],  loss: 0.008900, mae: 0.359503, mean_q: 0.164349, mean_eps: 0.000000
 2070/10000: episode: 34, duration: 0.438s, episode steps:  43, steps per second:  98, episode reward: -34.590, mean reward: -0.804 [-32.123,  2.970], mean action: 12.070 [1.000, 21.000],  loss: 0.010785, mae: 0.360960, mean_q: 0.183235, mean_eps: 0.000000
 2113/10000: episode: 35, duration: 0.431s, episode steps:  43, steps per second: 100, episode reward: 35.358, mean reward:  0.822 [-2.603, 30.598], mean action: 5.860 [1.000, 21.000],  loss: 0.008831, mae: 0.340096, mean_q: 0.234891, mean_eps: 0.000000
 2165/10000: episode: 36, duration: 0.524s, episode steps:  52, steps per second:  99, episode reward: 35.003, mean reward:  0.673 [-2.537, 32.640], mean action: 10.212 [1.000, 21.000],  loss: 0.010380, mae: 0.370053, mean_q: 0.176641, mean_eps: 0.000000
 2234/10000: episode: 37, duration: 0.716s, episode steps:  69, steps per second:  96, episode reward: 35.058, mean reward:  0.508 [-2.815, 32.360], mean action: 12.217 [1.000, 21.000],  loss: 0.011440, mae: 0.356134, mean_q: 0.228275, mean_eps: 0.000000
 2270/10000: episode: 38, duration: 0.394s, episode steps:  36, steps per second:  91, episode reward: 38.878, mean reward:  1.080 [-2.562, 32.040], mean action: 4.917 [1.000, 21.000],  loss: 0.010845, mae: 0.362596, mean_q: 0.182684, mean_eps: 0.000000
 2315/10000: episode: 39, duration: 0.513s, episode steps:  45, steps per second:  88, episode reward: 32.241, mean reward:  0.716 [-2.511, 32.150], mean action: 5.600 [1.000, 17.000],  loss: 0.008963, mae: 0.351777, mean_q: 0.196653, mean_eps: 0.000000
 2380/10000: episode: 40, duration: 0.672s, episode steps:  65, steps per second:  97, episode reward: 32.826, mean reward:  0.505 [-3.000, 32.832], mean action: 8.769 [1.000, 21.000],  loss: 0.009191, mae: 0.349739, mean_q: 0.224987, mean_eps: 0.000000
 2442/10000: episode: 41, duration: 0.698s, episode steps:  62, steps per second:  89, episode reward: 35.705, mean reward:  0.576 [-2.812, 32.460], mean action: 6.935 [1.000, 21.000],  loss: 0.007783, mae: 0.347389, mean_q: 0.215231, mean_eps: 0.000000
 2462/10000: episode: 42, duration: 0.229s, episode steps:  20, steps per second:  87, episode reward: 46.056, mean reward:  2.303 [-0.210, 32.001], mean action: 7.450 [3.000, 21.000],  loss: 0.008356, mae: 0.354705, mean_q: 0.199740, mean_eps: 0.000000
 2488/10000: episode: 43, duration: 0.287s, episode steps:  26, steps per second:  91, episode reward: 35.148, mean reward:  1.352 [-2.665, 32.340], mean action: 12.808 [3.000, 21.000],  loss: 0.008293, mae: 0.368996, mean_q: 0.154593, mean_eps: 0.000000
 2525/10000: episode: 44, duration: 0.392s, episode steps:  37, steps per second:  94, episode reward: 40.635, mean reward:  1.098 [-2.500, 32.083], mean action: 5.811 [1.000, 21.000],  loss: 0.011265, mae: 0.397355, mean_q: 0.157544, mean_eps: 0.000000
 2561/10000: episode: 45, duration: 0.363s, episode steps:  36, steps per second:  99, episode reward: -32.120, mean reward: -0.892 [-32.256,  3.120], mean action: 9.833 [0.000, 21.000],  loss: 0.010208, mae: 0.358080, mean_q: 0.198314, mean_eps: 0.000000
 2604/10000: episode: 46, duration: 0.438s, episode steps:  43, steps per second:  98, episode reward: -40.290, mean reward: -0.937 [-31.750,  2.286], mean action: 15.000 [3.000, 21.000],  loss: 0.009625, mae: 0.357287, mean_q: 0.188276, mean_eps: 0.000000
 2626/10000: episode: 47, duration: 0.238s, episode steps:  22, steps per second:  93, episode reward: 41.263, mean reward:  1.876 [-2.142, 31.664], mean action: 5.000 [1.000, 21.000],  loss: 0.010497, mae: 0.357403, mean_q: 0.183373, mean_eps: 0.000000
 2645/10000: episode: 48, duration: 0.201s, episode steps:  19, steps per second:  94, episode reward: 43.239, mean reward:  2.276 [-2.220, 32.127], mean action: 8.211 [1.000, 21.000],  loss: 0.009897, mae: 0.360971, mean_q: 0.183792, mean_eps: 0.000000
 2697/10000: episode: 49, duration: 0.570s, episode steps:  52, steps per second:  91, episode reward: 37.946, mean reward:  0.730 [-2.753, 32.150], mean action: 6.558 [3.000, 18.000],  loss: 0.012290, mae: 0.366761, mean_q: 0.214400, mean_eps: 0.000000
 2717/10000: episode: 50, duration: 0.230s, episode steps:  20, steps per second:  87, episode reward: 46.414, mean reward:  2.321 [-0.508, 32.800], mean action: 4.850 [3.000, 21.000],  loss: 0.010136, mae: 0.356763, mean_q: 0.209796, mean_eps: 0.000000
 2769/10000: episode: 51, duration: 0.538s, episode steps:  52, steps per second:  97, episode reward: -32.860, mean reward: -0.632 [-32.361,  2.360], mean action: 6.365 [3.000, 21.000],  loss: 0.010401, mae: 0.345306, mean_q: 0.252977, mean_eps: 0.000000
 2802/10000: episode: 52, duration: 0.369s, episode steps:  33, steps per second:  89, episode reward: 35.014, mean reward:  1.061 [-3.000, 32.394], mean action: 6.788 [1.000, 17.000],  loss: 0.010048, mae: 0.341343, mean_q: 0.252185, mean_eps: 0.000000
 2825/10000: episode: 53, duration: 0.249s, episode steps:  23, steps per second:  92, episode reward: 44.903, mean reward:  1.952 [-2.261, 32.083], mean action: 3.783 [1.000, 10.000],  loss: 0.008077, mae: 0.346677, mean_q: 0.244020, mean_eps: 0.000000
 2865/10000: episode: 54, duration: 0.411s, episode steps:  40, steps per second:  97, episode reward: 38.012, mean reward:  0.950 [-2.350, 32.100], mean action: 5.300 [1.000, 18.000],  loss: 0.010904, mae: 0.344784, mean_q: 0.238028, mean_eps: 0.000000
 2891/10000: episode: 55, duration: 0.296s, episode steps:  26, steps per second:  88, episode reward: 41.425, mean reward:  1.593 [-2.190, 32.070], mean action: 4.462 [1.000, 21.000],  loss: 0.009290, mae: 0.330446, mean_q: 0.238042, mean_eps: 0.000000
 2925/10000: episode: 56, duration: 0.336s, episode steps:  34, steps per second: 101, episode reward: -40.660, mean reward: -1.196 [-32.138,  2.648], mean action: 11.471 [1.000, 21.000],  loss: 0.011044, mae: 0.341014, mean_q: 0.262381, mean_eps: 0.000000
 2974/10000: episode: 57, duration: 0.499s, episode steps:  49, steps per second:  98, episode reward: 32.543, mean reward:  0.664 [-2.686, 32.353], mean action: 8.796 [1.000, 21.000],  loss: 0.012674, mae: 0.344324, mean_q: 0.266697, mean_eps: 0.000000
 3011/10000: episode: 58, duration: 0.391s, episode steps:  37, steps per second:  95, episode reward: 32.021, mean reward:  0.865 [-3.000, 32.170], mean action: 8.703 [1.000, 21.000],  loss: 0.009802, mae: 0.328111, mean_q: 0.271070, mean_eps: 0.000000
 3033/10000: episode: 59, duration: 0.293s, episode steps:  22, steps per second:  75, episode reward: 43.724, mean reward:  1.987 [-2.751, 32.044], mean action: 5.682 [3.000, 15.000],  loss: 0.010584, mae: 0.340696, mean_q: 0.302348, mean_eps: 0.000000
 3052/10000: episode: 60, duration: 0.232s, episode steps:  19, steps per second:  82, episode reward: 44.194, mean reward:  2.326 [-2.041, 32.262], mean action: 6.421 [3.000, 21.000],  loss: 0.009049, mae: 0.344225, mean_q: 0.244895, mean_eps: 0.000000
 3139/10000: episode: 61, duration: 0.949s, episode steps:  87, steps per second:  92, episode reward: -32.880, mean reward: -0.378 [-31.922,  2.130], mean action: 10.747 [1.000, 21.000],  loss: 0.012278, mae: 0.341066, mean_q: 0.276278, mean_eps: 0.000000
 3184/10000: episode: 62, duration: 0.488s, episode steps:  45, steps per second:  92, episode reward: 38.076, mean reward:  0.846 [-2.242, 31.975], mean action: 7.156 [1.000, 21.000],  loss: 0.013737, mae: 0.349922, mean_q: 0.259708, mean_eps: 0.000000
 3203/10000: episode: 63, duration: 0.208s, episode steps:  19, steps per second:  91, episode reward: 43.361, mean reward:  2.282 [-2.069, 31.687], mean action: 2.632 [1.000, 6.000],  loss: 0.011159, mae: 0.347801, mean_q: 0.221958, mean_eps: 0.000000
 3247/10000: episode: 64, duration: 0.453s, episode steps:  44, steps per second:  97, episode reward: 38.539, mean reward:  0.876 [-3.000, 32.150], mean action: 7.273 [3.000, 21.000],  loss: 0.011187, mae: 0.329677, mean_q: 0.308572, mean_eps: 0.000000
 3276/10000: episode: 65, duration: 0.307s, episode steps:  29, steps per second:  95, episode reward: 35.150, mean reward:  1.212 [-2.442, 32.271], mean action: 10.241 [3.000, 21.000],  loss: 0.014006, mae: 0.347758, mean_q: 0.251179, mean_eps: 0.000000
 3312/10000: episode: 66, duration: 0.375s, episode steps:  36, steps per second:  96, episode reward: 35.722, mean reward:  0.992 [-3.000, 32.220], mean action: 7.389 [1.000, 21.000],  loss: 0.012036, mae: 0.348757, mean_q: 0.262052, mean_eps: 0.000000
 3369/10000: episode: 67, duration: 0.575s, episode steps:  57, steps per second:  99, episode reward: -32.140, mean reward: -0.564 [-32.524,  3.000], mean action: 5.649 [1.000, 17.000],  loss: 0.011461, mae: 0.338817, mean_q: 0.275945, mean_eps: 0.000000
 3400/10000: episode: 68, duration: 0.315s, episode steps:  31, steps per second:  98, episode reward: 38.751, mean reward:  1.250 [-2.413, 31.950], mean action: 4.226 [3.000, 15.000],  loss: 0.011090, mae: 0.346946, mean_q: 0.278202, mean_eps: 0.000000
 3422/10000: episode: 69, duration: 0.241s, episode steps:  22, steps per second:  91, episode reward: 47.098, mean reward:  2.141 [-0.068, 32.210], mean action: 4.045 [3.000, 7.000],  loss: 0.011880, mae: 0.346495, mean_q: 0.284137, mean_eps: 0.000000
 3491/10000: episode: 70, duration: 0.717s, episode steps:  69, steps per second:  96, episode reward: 40.564, mean reward:  0.588 [-2.180, 32.070], mean action: 4.623 [0.000, 17.000],  loss: 0.011631, mae: 0.339398, mean_q: 0.317573, mean_eps: 0.000000
 3550/10000: episode: 71, duration: 0.593s, episode steps:  59, steps per second:  99, episode reward: -32.450, mean reward: -0.550 [-32.173,  3.061], mean action: 9.814 [3.000, 21.000],  loss: 0.013326, mae: 0.362644, mean_q: 0.246948, mean_eps: 0.000000
 3577/10000: episode: 72, duration: 0.293s, episode steps:  27, steps per second:  92, episode reward: 43.703, mean reward:  1.619 [-2.319, 31.830], mean action: 4.185 [1.000, 16.000],  loss: 0.012686, mae: 0.350591, mean_q: 0.266525, mean_eps: 0.000000
 3613/10000: episode: 73, duration: 0.387s, episode steps:  36, steps per second:  93, episode reward: 33.000, mean reward:  0.917 [-2.533, 29.748], mean action: 6.194 [3.000, 17.000],  loss: 0.010579, mae: 0.350967, mean_q: 0.208047, mean_eps: 0.000000
 3649/10000: episode: 74, duration: 0.420s, episode steps:  36, steps per second:  86, episode reward: 38.499, mean reward:  1.069 [-3.000, 32.054], mean action: 8.611 [0.000, 21.000],  loss: 0.010382, mae: 0.337226, mean_q: 0.300551, mean_eps: 0.000000
 3672/10000: episode: 75, duration: 0.262s, episode steps:  23, steps per second:  88, episode reward: 43.794, mean reward:  1.904 [-2.500, 32.263], mean action: 3.870 [1.000, 17.000],  loss: 0.012353, mae: 0.357382, mean_q: 0.280815, mean_eps: 0.000000
 3706/10000: episode: 76, duration: 0.347s, episode steps:  34, steps per second:  98, episode reward: -45.360, mean reward: -1.334 [-31.993,  0.610], mean action: 13.529 [3.000, 21.000],  loss: 0.014631, mae: 0.363949, mean_q: 0.256636, mean_eps: 0.000000
 3736/10000: episode: 77, duration: 0.347s, episode steps:  30, steps per second:  86, episode reward: 38.259, mean reward:  1.275 [-2.599, 32.090], mean action: 3.567 [1.000, 16.000],  loss: 0.012831, mae: 0.350318, mean_q: 0.269302, mean_eps: 0.000000
 3798/10000: episode: 78, duration: 0.611s, episode steps:  62, steps per second: 102, episode reward: 34.502, mean reward:  0.556 [-3.000, 32.314], mean action: 7.806 [3.000, 21.000],  loss: 0.011591, mae: 0.349344, mean_q: 0.265188, mean_eps: 0.000000
 3866/10000: episode: 79, duration: 0.715s, episode steps:  68, steps per second:  95, episode reward: -33.000, mean reward: -0.485 [-32.445,  2.280], mean action: 7.368 [0.000, 21.000],  loss: 0.010888, mae: 0.346929, mean_q: 0.236957, mean_eps: 0.000000
 3907/10000: episode: 80, duration: 0.439s, episode steps:  41, steps per second:  93, episode reward: 38.438, mean reward:  0.938 [-2.143, 29.666], mean action: 6.268 [1.000, 21.000],  loss: 0.011612, mae: 0.347813, mean_q: 0.247893, mean_eps: 0.000000
 3943/10000: episode: 81, duration: 0.382s, episode steps:  36, steps per second:  94, episode reward: 40.827, mean reward:  1.134 [-3.000, 32.050], mean action: 6.056 [3.000, 21.000],  loss: 0.012927, mae: 0.359976, mean_q: 0.234823, mean_eps: 0.000000
 3985/10000: episode: 82, duration: 0.454s, episode steps:  42, steps per second:  93, episode reward: 34.756, mean reward:  0.828 [-2.630, 32.470], mean action: 9.000 [0.000, 18.000],  loss: 0.011105, mae: 0.351822, mean_q: 0.209219, mean_eps: 0.000000
 4014/10000: episode: 83, duration: 0.311s, episode steps:  29, steps per second:  93, episode reward: 32.153, mean reward:  1.109 [-3.000, 32.550], mean action: 8.310 [3.000, 21.000],  loss: 0.009767, mae: 0.342174, mean_q: 0.214960, mean_eps: 0.000000
 4055/10000: episode: 84, duration: 0.443s, episode steps:  41, steps per second:  93, episode reward: 38.003, mean reward:  0.927 [-2.532, 32.020], mean action: 7.244 [1.000, 21.000],  loss: 0.012984, mae: 0.351500, mean_q: 0.227031, mean_eps: 0.000000
 4078/10000: episode: 85, duration: 0.243s, episode steps:  23, steps per second:  95, episode reward: 47.093, mean reward:  2.048 [-0.056, 32.110], mean action: 3.000 [3.000, 3.000],  loss: 0.012106, mae: 0.335929, mean_q: 0.291372, mean_eps: 0.000000
 4123/10000: episode: 86, duration: 0.491s, episode steps:  45, steps per second:  92, episode reward: 41.506, mean reward:  0.922 [-2.143, 32.370], mean action: 4.378 [3.000, 11.000],  loss: 0.012416, mae: 0.351143, mean_q: 0.229380, mean_eps: 0.000000
 4173/10000: episode: 87, duration: 0.515s, episode steps:  50, steps per second:  97, episode reward: 41.233, mean reward:  0.825 [-2.704, 32.230], mean action: 3.760 [1.000, 16.000],  loss: 0.012828, mae: 0.346821, mean_q: 0.259512, mean_eps: 0.000000
 4225/10000: episode: 88, duration: 0.531s, episode steps:  52, steps per second:  98, episode reward: -32.260, mean reward: -0.620 [-32.139,  2.920], mean action: 8.115 [3.000, 21.000],  loss: 0.012864, mae: 0.347115, mean_q: 0.315456, mean_eps: 0.000000
 4259/10000: episode: 89, duration: 0.413s, episode steps:  34, steps per second:  82, episode reward: 41.162, mean reward:  1.211 [-3.000, 32.420], mean action: 7.118 [1.000, 18.000],  loss: 0.010285, mae: 0.332687, mean_q: 0.281390, mean_eps: 0.000000
 4306/10000: episode: 90, duration: 0.505s, episode steps:  47, steps per second:  93, episode reward: 37.725, mean reward:  0.803 [-2.364, 32.021], mean action: 6.106 [3.000, 16.000],  loss: 0.012803, mae: 0.351695, mean_q: 0.248279, mean_eps: 0.000000
 4352/10000: episode: 91, duration: 0.472s, episode steps:  46, steps per second:  97, episode reward: 34.780, mean reward:  0.756 [-2.938, 31.981], mean action: 7.435 [3.000, 21.000],  loss: 0.012791, mae: 0.348998, mean_q: 0.240235, mean_eps: 0.000000
 4383/10000: episode: 92, duration: 0.333s, episode steps:  31, steps per second:  93, episode reward: 35.632, mean reward:  1.149 [-2.877, 32.560], mean action: 5.194 [3.000, 16.000],  loss: 0.014437, mae: 0.350919, mean_q: 0.257706, mean_eps: 0.000000
 4441/10000: episode: 93, duration: 0.686s, episode steps:  58, steps per second:  85, episode reward: -32.520, mean reward: -0.561 [-32.004,  2.509], mean action: 10.552 [1.000, 21.000],  loss: 0.011597, mae: 0.339012, mean_q: 0.286587, mean_eps: 0.000000
 4463/10000: episode: 94, duration: 0.246s, episode steps:  22, steps per second:  90, episode reward: 40.796, mean reward:  1.854 [-2.498, 33.000], mean action: 4.682 [1.000, 11.000],  loss: 0.014432, mae: 0.359211, mean_q: 0.276542, mean_eps: 0.000000
 4502/10000: episode: 95, duration: 0.408s, episode steps:  39, steps per second:  96, episode reward: 38.401, mean reward:  0.985 [-3.000, 32.464], mean action: 5.128 [3.000, 17.000],  loss: 0.009907, mae: 0.357417, mean_q: 0.212580, mean_eps: 0.000000
 4534/10000: episode: 96, duration: 0.329s, episode steps:  32, steps per second:  97, episode reward: 38.657, mean reward:  1.208 [-2.326, 29.280], mean action: 4.469 [1.000, 18.000],  loss: 0.010592, mae: 0.348350, mean_q: 0.246868, mean_eps: 0.000000
 4568/10000: episode: 97, duration: 0.361s, episode steps:  34, steps per second:  94, episode reward: 36.000, mean reward:  1.059 [-2.617, 29.632], mean action: 5.382 [3.000, 21.000],  loss: 0.014419, mae: 0.370491, mean_q: 0.253605, mean_eps: 0.000000
 4617/10000: episode: 98, duration: 0.490s, episode steps:  49, steps per second: 100, episode reward: 35.262, mean reward:  0.720 [-2.506, 32.200], mean action: 9.980 [3.000, 21.000],  loss: 0.012034, mae: 0.353802, mean_q: 0.277828, mean_eps: 0.000000
 4668/10000: episode: 99, duration: 0.536s, episode steps:  51, steps per second:  95, episode reward: 35.014, mean reward:  0.687 [-2.779, 32.280], mean action: 8.451 [3.000, 16.000],  loss: 0.013319, mae: 0.362919, mean_q: 0.276812, mean_eps: 0.000000
 4704/10000: episode: 100, duration: 0.374s, episode steps:  36, steps per second:  96, episode reward: -32.040, mean reward: -0.890 [-33.000,  2.310], mean action: 6.667 [1.000, 16.000],  loss: 0.012482, mae: 0.348220, mean_q: 0.304646, mean_eps: 0.000000
 4754/10000: episode: 101, duration: 0.511s, episode steps:  50, steps per second:  98, episode reward: -32.290, mean reward: -0.646 [-32.184,  3.943], mean action: 7.860 [1.000, 21.000],  loss: 0.010526, mae: 0.332029, mean_q: 0.280396, mean_eps: 0.000000
 4845/10000: episode: 102, duration: 0.939s, episode steps:  91, steps per second:  97, episode reward: 32.726, mean reward:  0.360 [-3.000, 32.096], mean action: 6.527 [1.000, 16.000],  loss: 0.013468, mae: 0.360627, mean_q: 0.268759, mean_eps: 0.000000
 4878/10000: episode: 103, duration: 0.363s, episode steps:  33, steps per second:  91, episode reward: 38.479, mean reward:  1.166 [-2.362, 32.160], mean action: 8.515 [3.000, 21.000],  loss: 0.013107, mae: 0.358022, mean_q: 0.291361, mean_eps: 0.000000
 4919/10000: episode: 104, duration: 0.416s, episode steps:  41, steps per second:  99, episode reward: 32.967, mean reward:  0.804 [-2.598, 32.020], mean action: 8.659 [1.000, 21.000],  loss: 0.011766, mae: 0.350190, mean_q: 0.293581, mean_eps: 0.000000
 4960/10000: episode: 105, duration: 0.411s, episode steps:  41, steps per second: 100, episode reward: 34.656, mean reward:  0.845 [-2.939, 32.480], mean action: 5.293 [3.000, 14.000],  loss: 0.013164, mae: 0.360561, mean_q: 0.277865, mean_eps: 0.000000
 4985/10000: episode: 106, duration: 0.280s, episode steps:  25, steps per second:  89, episode reward: 44.131, mean reward:  1.765 [-2.444, 34.053], mean action: 4.120 [1.000, 14.000],  loss: 0.011587, mae: 0.355096, mean_q: 0.266914, mean_eps: 0.000000
 5033/10000: episode: 107, duration: 0.480s, episode steps:  48, steps per second: 100, episode reward: 33.000, mean reward:  0.688 [-3.000, 32.170], mean action: 6.521 [1.000, 21.000],  loss: 0.012656, mae: 0.368030, mean_q: 0.233632, mean_eps: 0.000000
 5120/10000: episode: 108, duration: 1.018s, episode steps:  87, steps per second:  85, episode reward: 32.481, mean reward:  0.373 [-2.570, 31.878], mean action: 13.690 [3.000, 21.000],  loss: 0.012465, mae: 0.355305, mean_q: 0.278088, mean_eps: 0.000000
 5170/10000: episode: 109, duration: 0.526s, episode steps:  50, steps per second:  95, episode reward: 32.136, mean reward:  0.643 [-2.472, 32.050], mean action: 7.640 [3.000, 21.000],  loss: 0.013165, mae: 0.359057, mean_q: 0.264613, mean_eps: 0.000000
 5214/10000: episode: 110, duration: 0.443s, episode steps:  44, steps per second:  99, episode reward: 37.884, mean reward:  0.861 [-3.000, 32.170], mean action: 5.659 [1.000, 15.000],  loss: 0.011715, mae: 0.346935, mean_q: 0.295116, mean_eps: 0.000000
 5235/10000: episode: 111, duration: 0.272s, episode steps:  21, steps per second:  77, episode reward: 41.015, mean reward:  1.953 [-2.416, 31.797], mean action: 5.524 [1.000, 21.000],  loss: 0.010026, mae: 0.351914, mean_q: 0.211327, mean_eps: 0.000000
 5269/10000: episode: 112, duration: 0.342s, episode steps:  34, steps per second: 100, episode reward: 38.046, mean reward:  1.119 [-2.202, 32.253], mean action: 10.059 [3.000, 21.000],  loss: 0.011466, mae: 0.348779, mean_q: 0.267028, mean_eps: 0.000000
 5296/10000: episode: 113, duration: 0.307s, episode steps:  27, steps per second:  88, episode reward: 43.937, mean reward:  1.627 [-2.174, 32.250], mean action: 5.963 [3.000, 18.000],  loss: 0.012432, mae: 0.354266, mean_q: 0.252232, mean_eps: 0.000000
 5346/10000: episode: 114, duration: 0.511s, episode steps:  50, steps per second:  98, episode reward: 35.427, mean reward:  0.709 [-2.698, 32.020], mean action: 4.980 [3.000, 21.000],  loss: 0.013888, mae: 0.372688, mean_q: 0.282195, mean_eps: 0.000000
 5372/10000: episode: 115, duration: 0.277s, episode steps:  26, steps per second:  94, episode reward: 44.906, mean reward:  1.727 [-2.774, 32.016], mean action: 4.654 [3.000, 21.000],  loss: 0.014921, mae: 0.379745, mean_q: 0.280293, mean_eps: 0.000000
 5391/10000: episode: 116, duration: 0.204s, episode steps:  19, steps per second:  93, episode reward: 47.939, mean reward:  2.523 [ 0.000, 32.422], mean action: 5.842 [3.000, 21.000],  loss: 0.013173, mae: 0.370366, mean_q: 0.230743, mean_eps: 0.000000
 5429/10000: episode: 117, duration: 0.432s, episode steps:  38, steps per second:  88, episode reward: -32.710, mean reward: -0.861 [-31.953,  2.601], mean action: 11.737 [1.000, 21.000],  loss: 0.013799, mae: 0.371882, mean_q: 0.219694, mean_eps: 0.000000
 5460/10000: episode: 118, duration: 0.319s, episode steps:  31, steps per second:  97, episode reward: 38.361, mean reward:  1.237 [-2.476, 32.102], mean action: 5.806 [3.000, 15.000],  loss: 0.014621, mae: 0.352388, mean_q: 0.255819, mean_eps: 0.000000
 5495/10000: episode: 119, duration: 0.401s, episode steps:  35, steps per second:  87, episode reward: -32.110, mean reward: -0.917 [-32.029,  2.430], mean action: 9.943 [3.000, 21.000],  loss: 0.011232, mae: 0.340211, mean_q: 0.267295, mean_eps: 0.000000
 5527/10000: episode: 120, duration: 0.359s, episode steps:  32, steps per second:  89, episode reward: 38.669, mean reward:  1.208 [-2.489, 32.320], mean action: 3.344 [1.000, 16.000],  loss: 0.012133, mae: 0.359906, mean_q: 0.252407, mean_eps: 0.000000
 5569/10000: episode: 121, duration: 0.429s, episode steps:  42, steps per second:  98, episode reward: 34.596, mean reward:  0.824 [-3.000, 32.200], mean action: 5.786 [3.000, 21.000],  loss: 0.015145, mae: 0.353822, mean_q: 0.309110, mean_eps: 0.000000
 5603/10000: episode: 122, duration: 0.351s, episode steps:  34, steps per second:  97, episode reward: 38.710, mean reward:  1.139 [-3.000, 32.130], mean action: 8.029 [1.000, 21.000],  loss: 0.012341, mae: 0.352692, mean_q: 0.263837, mean_eps: 0.000000
 5663/10000: episode: 123, duration: 0.614s, episode steps:  60, steps per second:  98, episode reward: 38.465, mean reward:  0.641 [-3.000, 32.540], mean action: 6.033 [2.000, 21.000],  loss: 0.012473, mae: 0.343381, mean_q: 0.302682, mean_eps: 0.000000
 5693/10000: episode: 124, duration: 0.315s, episode steps:  30, steps per second:  95, episode reward: 44.972, mean reward:  1.499 [-2.333, 32.550], mean action: 5.833 [1.000, 21.000],  loss: 0.015340, mae: 0.368249, mean_q: 0.301950, mean_eps: 0.000000
 5719/10000: episode: 125, duration: 0.268s, episode steps:  26, steps per second:  97, episode reward: 41.382, mean reward:  1.592 [-3.000, 32.181], mean action: 7.231 [3.000, 21.000],  loss: 0.012567, mae: 0.361605, mean_q: 0.200668, mean_eps: 0.000000
 5761/10000: episode: 126, duration: 0.534s, episode steps:  42, steps per second:  79, episode reward: 36.567, mean reward:  0.871 [-3.000, 32.110], mean action: 6.214 [1.000, 21.000],  loss: 0.012220, mae: 0.342022, mean_q: 0.286257, mean_eps: 0.000000
 5803/10000: episode: 127, duration: 0.444s, episode steps:  42, steps per second:  95, episode reward: 41.202, mean reward:  0.981 [-3.000, 32.010], mean action: 3.738 [3.000, 11.000],  loss: 0.011961, mae: 0.351737, mean_q: 0.266488, mean_eps: 0.000000
 5831/10000: episode: 128, duration: 0.292s, episode steps:  28, steps per second:  96, episode reward: 41.719, mean reward:  1.490 [-2.251, 31.789], mean action: 3.857 [1.000, 20.000],  loss: 0.011084, mae: 0.334852, mean_q: 0.296916, mean_eps: 0.000000
 5922/10000: episode: 129, duration: 0.920s, episode steps:  91, steps per second:  99, episode reward: -32.810, mean reward: -0.361 [-32.030,  2.630], mean action: 8.703 [1.000, 21.000],  loss: 0.014614, mae: 0.365449, mean_q: 0.261666, mean_eps: 0.000000
 5952/10000: episode: 130, duration: 0.363s, episode steps:  30, steps per second:  83, episode reward: 38.708, mean reward:  1.290 [-2.212, 31.898], mean action: 5.233 [3.000, 17.000],  loss: 0.011044, mae: 0.348953, mean_q: 0.263377, mean_eps: 0.000000
 5995/10000: episode: 131, duration: 0.478s, episode steps:  43, steps per second:  90, episode reward: 37.482, mean reward:  0.872 [-3.000, 32.103], mean action: 8.302 [3.000, 21.000],  loss: 0.011007, mae: 0.342604, mean_q: 0.288093, mean_eps: 0.000000
 6032/10000: episode: 132, duration: 0.400s, episode steps:  37, steps per second:  93, episode reward: 37.590, mean reward:  1.016 [-2.658, 32.160], mean action: 4.703 [1.000, 21.000],  loss: 0.011820, mae: 0.341623, mean_q: 0.264205, mean_eps: 0.000000
 6073/10000: episode: 133, duration: 0.426s, episode steps:  41, steps per second:  96, episode reward: 39.000, mean reward:  0.951 [-2.378, 32.010], mean action: 4.341 [1.000, 18.000],  loss: 0.013380, mae: 0.352590, mean_q: 0.298247, mean_eps: 0.000000
 6090/10000: episode: 134, duration: 0.183s, episode steps:  17, steps per second:  93, episode reward: 44.764, mean reward:  2.633 [-2.171, 32.044], mean action: 2.882 [1.000, 3.000],  loss: 0.012584, mae: 0.358719, mean_q: 0.270745, mean_eps: 0.000000
 6123/10000: episode: 135, duration: 0.373s, episode steps:  33, steps per second:  89, episode reward: 44.079, mean reward:  1.336 [-2.360, 32.170], mean action: 5.576 [3.000, 16.000],  loss: 0.010734, mae: 0.355148, mean_q: 0.247023, mean_eps: 0.000000
 6146/10000: episode: 136, duration: 0.257s, episode steps:  23, steps per second:  90, episode reward: 42.000, mean reward:  1.826 [-2.876, 32.600], mean action: 7.261 [3.000, 21.000],  loss: 0.011445, mae: 0.351275, mean_q: 0.239752, mean_eps: 0.000000
 6183/10000: episode: 137, duration: 0.420s, episode steps:  37, steps per second:  88, episode reward: 41.435, mean reward:  1.120 [-2.619, 32.280], mean action: 5.730 [1.000, 21.000],  loss: 0.013365, mae: 0.363087, mean_q: 0.241319, mean_eps: 0.000000
 6229/10000: episode: 138, duration: 0.503s, episode steps:  46, steps per second:  91, episode reward: 37.200, mean reward:  0.809 [-3.000, 32.253], mean action: 5.652 [1.000, 18.000],  loss: 0.013706, mae: 0.351350, mean_q: 0.301417, mean_eps: 0.000000
 6254/10000: episode: 139, duration: 0.279s, episode steps:  25, steps per second:  90, episode reward: 35.714, mean reward:  1.429 [-3.000, 32.084], mean action: 4.000 [1.000, 17.000],  loss: 0.013301, mae: 0.394654, mean_q: 0.236694, mean_eps: 0.000000
 6322/10000: episode: 140, duration: 0.693s, episode steps:  68, steps per second:  98, episode reward: 35.217, mean reward:  0.518 [-3.000, 32.010], mean action: 4.941 [1.000, 21.000],  loss: 0.012508, mae: 0.356473, mean_q: 0.310752, mean_eps: 0.000000
 6427/10000: episode: 141, duration: 1.093s, episode steps: 105, steps per second:  96, episode reward: 35.479, mean reward:  0.338 [-2.835, 34.230], mean action: 8.305 [1.000, 21.000],  loss: 0.013918, mae: 0.354928, mean_q: 0.277195, mean_eps: 0.000000
 6451/10000: episode: 142, duration: 0.266s, episode steps:  24, steps per second:  90, episode reward: 40.594, mean reward:  1.691 [-3.000, 33.995], mean action: 6.042 [3.000, 11.000],  loss: 0.013180, mae: 0.354803, mean_q: 0.270850, mean_eps: 0.000000
 6543/10000: episode: 143, duration: 0.920s, episode steps:  92, steps per second: 100, episode reward: 32.478, mean reward:  0.353 [-2.780, 32.150], mean action: 7.326 [3.000, 21.000],  loss: 0.014278, mae: 0.354276, mean_q: 0.289801, mean_eps: 0.000000
 6568/10000: episode: 144, duration: 0.259s, episode steps:  25, steps per second:  97, episode reward: 41.638, mean reward:  1.666 [-2.185, 33.000], mean action: 5.680 [1.000, 17.000],  loss: 0.012527, mae: 0.335295, mean_q: 0.328497, mean_eps: 0.000000
 6594/10000: episode: 145, duration: 0.279s, episode steps:  26, steps per second:  93, episode reward: 41.199, mean reward:  1.585 [-3.000, 32.070], mean action: 5.231 [1.000, 15.000],  loss: 0.013374, mae: 0.352869, mean_q: 0.261441, mean_eps: 0.000000
 6628/10000: episode: 146, duration: 0.346s, episode steps:  34, steps per second:  98, episode reward: 39.000, mean reward:  1.147 [-2.271, 29.832], mean action: 3.706 [3.000, 11.000],  loss: 0.014076, mae: 0.345874, mean_q: 0.315558, mean_eps: 0.000000
 6674/10000: episode: 147, duration: 0.462s, episode steps:  46, steps per second: 100, episode reward: -32.750, mean reward: -0.712 [-32.117,  3.059], mean action: 5.848 [1.000, 21.000],  loss: 0.012716, mae: 0.346410, mean_q: 0.288265, mean_eps: 0.000000
 6695/10000: episode: 148, duration: 0.231s, episode steps:  21, steps per second:  91, episode reward: 44.028, mean reward:  2.097 [-2.018, 32.463], mean action: 5.571 [3.000, 21.000],  loss: 0.009754, mae: 0.331653, mean_q: 0.284751, mean_eps: 0.000000
 6728/10000: episode: 149, duration: 0.346s, episode steps:  33, steps per second:  95, episode reward: 35.797, mean reward:  1.085 [-3.000, 31.857], mean action: 6.364 [3.000, 18.000],  loss: 0.013918, mae: 0.360240, mean_q: 0.269969, mean_eps: 0.000000
 6767/10000: episode: 150, duration: 0.393s, episode steps:  39, steps per second:  99, episode reward: 38.371, mean reward:  0.984 [-2.528, 32.287], mean action: 4.821 [1.000, 17.000],  loss: 0.010900, mae: 0.345366, mean_q: 0.305932, mean_eps: 0.000000
 6808/10000: episode: 151, duration: 0.429s, episode steps:  41, steps per second:  95, episode reward: 35.575, mean reward:  0.868 [-2.639, 32.001], mean action: 5.268 [3.000, 17.000],  loss: 0.013427, mae: 0.358517, mean_q: 0.295680, mean_eps: 0.000000
 6838/10000: episode: 152, duration: 0.363s, episode steps:  30, steps per second:  83, episode reward: 37.638, mean reward:  1.255 [-2.192, 32.560], mean action: 8.633 [1.000, 17.000],  loss: 0.009154, mae: 0.339789, mean_q: 0.264408, mean_eps: 0.000000
 6872/10000: episode: 153, duration: 0.355s, episode steps:  34, steps per second:  96, episode reward: 38.494, mean reward:  1.132 [-2.745, 32.850], mean action: 5.853 [1.000, 21.000],  loss: 0.012125, mae: 0.352963, mean_q: 0.294428, mean_eps: 0.000000
 6895/10000: episode: 154, duration: 0.243s, episode steps:  23, steps per second:  95, episode reward: 38.227, mean reward:  1.662 [-2.752, 31.729], mean action: 5.087 [1.000, 21.000],  loss: 0.011154, mae: 0.347031, mean_q: 0.274529, mean_eps: 0.000000
 6928/10000: episode: 155, duration: 0.394s, episode steps:  33, steps per second:  84, episode reward: 40.950, mean reward:  1.241 [-2.177, 32.460], mean action: 4.000 [1.000, 15.000],  loss: 0.014153, mae: 0.374801, mean_q: 0.253900, mean_eps: 0.000000
 6975/10000: episode: 156, duration: 0.481s, episode steps:  47, steps per second:  98, episode reward: 32.035, mean reward:  0.682 [-2.446, 31.786], mean action: 7.979 [3.000, 21.000],  loss: 0.012476, mae: 0.360190, mean_q: 0.269566, mean_eps: 0.000000
 7044/10000: episode: 157, duration: 0.700s, episode steps:  69, steps per second:  99, episode reward: 32.132, mean reward:  0.466 [-3.000, 32.031], mean action: 8.565 [3.000, 21.000],  loss: 0.013601, mae: 0.354673, mean_q: 0.311371, mean_eps: 0.000000
 7080/10000: episode: 158, duration: 0.369s, episode steps:  36, steps per second:  98, episode reward: 36.000, mean reward:  1.000 [-2.356, 32.200], mean action: 5.583 [1.000, 16.000],  loss: 0.013050, mae: 0.357921, mean_q: 0.257150, mean_eps: 0.000000
 7107/10000: episode: 159, duration: 0.270s, episode steps:  27, steps per second: 100, episode reward: 44.407, mean reward:  1.645 [-2.383, 32.290], mean action: 7.852 [1.000, 21.000],  loss: 0.013791, mae: 0.370219, mean_q: 0.232501, mean_eps: 0.000000
 7127/10000: episode: 160, duration: 0.227s, episode steps:  20, steps per second:  88, episode reward: 47.570, mean reward:  2.379 [-0.295, 32.280], mean action: 3.000 [3.000, 3.000],  loss: 0.012309, mae: 0.341714, mean_q: 0.288789, mean_eps: 0.000000
 7190/10000: episode: 161, duration: 0.631s, episode steps:  63, steps per second: 100, episode reward: -35.510, mean reward: -0.564 [-32.263,  2.340], mean action: 5.524 [1.000, 18.000],  loss: 0.013105, mae: 0.359552, mean_q: 0.260337, mean_eps: 0.000000
 7215/10000: episode: 162, duration: 0.304s, episode steps:  25, steps per second:  82, episode reward: 38.476, mean reward:  1.539 [-2.805, 31.846], mean action: 5.200 [1.000, 18.000],  loss: 0.011125, mae: 0.365677, mean_q: 0.232739, mean_eps: 0.000000
 7239/10000: episode: 163, duration: 0.298s, episode steps:  24, steps per second:  81, episode reward: 41.454, mean reward:  1.727 [-2.644, 32.080], mean action: 3.792 [3.000, 11.000],  loss: 0.011434, mae: 0.356699, mean_q: 0.242167, mean_eps: 0.000000
 7276/10000: episode: 164, duration: 0.392s, episode steps:  37, steps per second:  94, episode reward: 35.570, mean reward:  0.961 [-2.688, 31.851], mean action: 5.324 [1.000, 21.000],  loss: 0.013893, mae: 0.361595, mean_q: 0.258099, mean_eps: 0.000000
 7317/10000: episode: 165, duration: 0.430s, episode steps:  41, steps per second:  95, episode reward: 46.444, mean reward:  1.133 [-0.369, 31.823], mean action: 5.463 [1.000, 15.000],  loss: 0.013216, mae: 0.362030, mean_q: 0.264494, mean_eps: 0.000000
 7369/10000: episode: 166, duration: 0.518s, episode steps:  52, steps per second: 100, episode reward: 32.713, mean reward:  0.629 [-3.000, 32.990], mean action: 8.154 [1.000, 18.000],  loss: 0.010925, mae: 0.348679, mean_q: 0.261975, mean_eps: 0.000000
 7415/10000: episode: 167, duration: 0.481s, episode steps:  46, steps per second:  96, episode reward: -32.410, mean reward: -0.705 [-32.104,  2.380], mean action: 7.217 [1.000, 21.000],  loss: 0.012799, mae: 0.373604, mean_q: 0.268865, mean_eps: 0.000000
 7467/10000: episode: 168, duration: 0.529s, episode steps:  52, steps per second:  98, episode reward: 37.250, mean reward:  0.716 [-2.792, 32.190], mean action: 10.019 [1.000, 21.000],  loss: 0.014155, mae: 0.376851, mean_q: 0.233448, mean_eps: 0.000000
 7498/10000: episode: 169, duration: 0.329s, episode steps:  31, steps per second:  94, episode reward: 41.722, mean reward:  1.346 [-2.399, 32.135], mean action: 6.032 [3.000, 16.000],  loss: 0.010069, mae: 0.344547, mean_q: 0.262453, mean_eps: 0.000000
 7541/10000: episode: 170, duration: 0.441s, episode steps:  43, steps per second:  97, episode reward: 44.143, mean reward:  1.027 [-2.078, 32.050], mean action: 4.721 [3.000, 20.000],  loss: 0.010870, mae: 0.350875, mean_q: 0.240687, mean_eps: 0.000000
 7571/10000: episode: 171, duration: 0.317s, episode steps:  30, steps per second:  95, episode reward: 36.845, mean reward:  1.228 [-2.729, 31.729], mean action: 7.267 [3.000, 17.000],  loss: 0.013684, mae: 0.368864, mean_q: 0.242278, mean_eps: 0.000000
 7594/10000: episode: 172, duration: 0.237s, episode steps:  23, steps per second:  97, episode reward: 38.420, mean reward:  1.670 [-3.000, 32.450], mean action: 4.478 [3.000, 16.000],  loss: 0.014023, mae: 0.362088, mean_q: 0.281791, mean_eps: 0.000000
 7649/10000: episode: 173, duration: 0.605s, episode steps:  55, steps per second:  91, episode reward: -32.490, mean reward: -0.591 [-32.322,  2.540], mean action: 6.491 [1.000, 21.000],  loss: 0.013264, mae: 0.365604, mean_q: 0.261540, mean_eps: 0.000000
 7670/10000: episode: 174, duration: 0.219s, episode steps:  21, steps per second:  96, episode reward: 44.263, mean reward:  2.108 [-2.381, 32.180], mean action: 5.048 [3.000, 16.000],  loss: 0.013582, mae: 0.362516, mean_q: 0.261858, mean_eps: 0.000000
 7698/10000: episode: 175, duration: 0.299s, episode steps:  28, steps per second:  94, episode reward: 46.622, mean reward:  1.665 [-0.317, 32.140], mean action: 4.893 [3.000, 20.000],  loss: 0.013420, mae: 0.355369, mean_q: 0.291443, mean_eps: 0.000000
 7710/10000: episode: 176, duration: 0.142s, episode steps:  12, steps per second:  84, episode reward: 44.237, mean reward:  3.686 [-2.112, 32.240], mean action: 7.333 [1.000, 21.000],  loss: 0.009549, mae: 0.326263, mean_q: 0.288817, mean_eps: 0.000000
 7726/10000: episode: 177, duration: 0.178s, episode steps:  16, steps per second:  90, episode reward: 43.589, mean reward:  2.724 [-2.108, 31.865], mean action: 4.375 [3.000, 11.000],  loss: 0.012175, mae: 0.340081, mean_q: 0.282959, mean_eps: 0.000000
 7796/10000: episode: 178, duration: 0.718s, episode steps:  70, steps per second:  97, episode reward: 35.159, mean reward:  0.502 [-3.000, 32.141], mean action: 7.829 [1.000, 21.000],  loss: 0.011039, mae: 0.347130, mean_q: 0.286381, mean_eps: 0.000000
 7833/10000: episode: 179, duration: 0.399s, episode steps:  37, steps per second:  93, episode reward: 35.176, mean reward:  0.951 [-3.000, 32.680], mean action: 6.486 [3.000, 21.000],  loss: 0.015691, mae: 0.366040, mean_q: 0.323331, mean_eps: 0.000000
 7862/10000: episode: 180, duration: 0.318s, episode steps:  29, steps per second:  91, episode reward: 38.475, mean reward:  1.327 [-2.372, 31.953], mean action: 8.276 [3.000, 21.000],  loss: 0.013024, mae: 0.360741, mean_q: 0.263036, mean_eps: 0.000000
 7901/10000: episode: 181, duration: 0.414s, episode steps:  39, steps per second:  94, episode reward: 38.407, mean reward:  0.985 [-2.306, 32.182], mean action: 6.564 [3.000, 21.000],  loss: 0.014828, mae: 0.374031, mean_q: 0.271145, mean_eps: 0.000000
 7924/10000: episode: 182, duration: 0.242s, episode steps:  23, steps per second:  95, episode reward: 35.693, mean reward:  1.552 [-2.781, 32.290], mean action: 7.087 [3.000, 21.000],  loss: 0.013549, mae: 0.365001, mean_q: 0.241393, mean_eps: 0.000000
 7971/10000: episode: 183, duration: 0.497s, episode steps:  47, steps per second:  95, episode reward: 32.644, mean reward:  0.695 [-3.000, 32.026], mean action: 3.894 [1.000, 16.000],  loss: 0.011210, mae: 0.354138, mean_q: 0.276868, mean_eps: 0.000000
 7999/10000: episode: 184, duration: 0.304s, episode steps:  28, steps per second:  92, episode reward: 44.181, mean reward:  1.578 [-2.536, 31.680], mean action: 4.107 [3.000, 21.000],  loss: 0.013697, mae: 0.353975, mean_q: 0.301367, mean_eps: 0.000000
 8031/10000: episode: 185, duration: 0.399s, episode steps:  32, steps per second:  80, episode reward: 37.602, mean reward:  1.175 [-2.390, 32.280], mean action: 6.250 [3.000, 17.000],  loss: 0.013225, mae: 0.367083, mean_q: 0.242939, mean_eps: 0.000000
 8064/10000: episode: 186, duration: 0.338s, episode steps:  33, steps per second:  97, episode reward: 44.432, mean reward:  1.346 [-2.269, 32.410], mean action: 4.030 [1.000, 10.000],  loss: 0.011872, mae: 0.365616, mean_q: 0.253246, mean_eps: 0.000000
 8087/10000: episode: 187, duration: 0.257s, episode steps:  23, steps per second:  90, episode reward: 38.663, mean reward:  1.681 [-3.000, 32.050], mean action: 4.522 [1.000, 15.000],  loss: 0.011587, mae: 0.357694, mean_q: 0.268928, mean_eps: 0.000000
 8114/10000: episode: 188, duration: 0.282s, episode steps:  27, steps per second:  96, episode reward: 41.032, mean reward:  1.520 [-2.288, 31.313], mean action: 4.000 [1.000, 14.000],  loss: 0.011566, mae: 0.356243, mean_q: 0.344413, mean_eps: 0.000000
 8145/10000: episode: 189, duration: 0.324s, episode steps:  31, steps per second:  96, episode reward: 44.129, mean reward:  1.424 [-2.293, 32.110], mean action: 3.903 [3.000, 16.000],  loss: 0.013829, mae: 0.391515, mean_q: 0.247157, mean_eps: 0.000000
 8173/10000: episode: 190, duration: 0.297s, episode steps:  28, steps per second:  94, episode reward: 43.585, mean reward:  1.557 [-2.120, 32.737], mean action: 7.464 [1.000, 18.000],  loss: 0.011708, mae: 0.361823, mean_q: 0.297504, mean_eps: 0.000000
 8201/10000: episode: 191, duration: 0.293s, episode steps:  28, steps per second:  96, episode reward: 40.239, mean reward:  1.437 [-2.238, 32.330], mean action: 4.536 [1.000, 21.000],  loss: 0.013170, mae: 0.388478, mean_q: 0.240305, mean_eps: 0.000000
 8225/10000: episode: 192, duration: 0.269s, episode steps:  24, steps per second:  89, episode reward: 33.000, mean reward:  1.375 [-3.000, 30.072], mean action: 4.833 [3.000, 21.000],  loss: 0.016071, mae: 0.374207, mean_q: 0.276263, mean_eps: 0.000000
 8271/10000: episode: 193, duration: 0.475s, episode steps:  46, steps per second:  97, episode reward: 32.566, mean reward:  0.708 [-2.445, 31.951], mean action: 7.065 [1.000, 21.000],  loss: 0.011498, mae: 0.356161, mean_q: 0.262552, mean_eps: 0.000000
 8312/10000: episode: 194, duration: 0.428s, episode steps:  41, steps per second:  96, episode reward: 32.095, mean reward:  0.783 [-2.998, 32.240], mean action: 8.049 [3.000, 16.000],  loss: 0.014929, mae: 0.383194, mean_q: 0.265437, mean_eps: 0.000000
 8330/10000: episode: 195, duration: 0.213s, episode steps:  18, steps per second:  84, episode reward: 42.000, mean reward:  2.333 [-3.000, 32.290], mean action: 7.389 [1.000, 16.000],  loss: 0.012542, mae: 0.374088, mean_q: 0.244658, mean_eps: 0.000000
 8362/10000: episode: 196, duration: 0.351s, episode steps:  32, steps per second:  91, episode reward: 42.663, mean reward:  1.333 [-2.373, 31.487], mean action: 6.594 [1.000, 15.000],  loss: 0.011284, mae: 0.376518, mean_q: 0.269914, mean_eps: 0.000000
 8392/10000: episode: 197, duration: 0.343s, episode steps:  30, steps per second:  87, episode reward: 35.886, mean reward:  1.196 [-3.000, 32.290], mean action: 5.967 [1.000, 21.000],  loss: 0.012495, mae: 0.359228, mean_q: 0.296621, mean_eps: 0.000000
 8436/10000: episode: 198, duration: 0.456s, episode steps:  44, steps per second:  96, episode reward: -32.610, mean reward: -0.741 [-32.321,  3.059], mean action: 10.159 [3.000, 21.000],  loss: 0.013464, mae: 0.351471, mean_q: 0.332657, mean_eps: 0.000000
 8467/10000: episode: 199, duration: 0.313s, episode steps:  31, steps per second:  99, episode reward: 35.759, mean reward:  1.154 [-3.000, 32.950], mean action: 6.774 [1.000, 21.000],  loss: 0.013083, mae: 0.366074, mean_q: 0.267161, mean_eps: 0.000000
 8508/10000: episode: 200, duration: 0.419s, episode steps:  41, steps per second:  98, episode reward: 32.251, mean reward:  0.787 [-3.000, 32.040], mean action: 10.732 [1.000, 21.000],  loss: 0.014349, mae: 0.373135, mean_q: 0.272063, mean_eps: 0.000000
 8542/10000: episode: 201, duration: 0.351s, episode steps:  34, steps per second:  97, episode reward: 33.000, mean reward:  0.971 [-2.940, 30.011], mean action: 4.676 [1.000, 17.000],  loss: 0.015301, mae: 0.376938, mean_q: 0.284006, mean_eps: 0.000000
 8576/10000: episode: 202, duration: 0.345s, episode steps:  34, steps per second:  98, episode reward: 35.534, mean reward:  1.045 [-2.395, 32.373], mean action: 6.324 [3.000, 21.000],  loss: 0.016146, mae: 0.386033, mean_q: 0.251353, mean_eps: 0.000000
 8612/10000: episode: 203, duration: 0.369s, episode steps:  36, steps per second:  97, episode reward: -32.810, mean reward: -0.911 [-32.287,  2.870], mean action: 5.389 [1.000, 21.000],  loss: 0.013096, mae: 0.364147, mean_q: 0.261827, mean_eps: 0.000000
 8659/10000: episode: 204, duration: 0.478s, episode steps:  47, steps per second:  98, episode reward: 35.011, mean reward:  0.745 [-2.478, 32.136], mean action: 8.809 [3.000, 21.000],  loss: 0.013755, mae: 0.368539, mean_q: 0.264531, mean_eps: 0.000000
 8709/10000: episode: 205, duration: 0.505s, episode steps:  50, steps per second:  99, episode reward: 37.900, mean reward:  0.758 [-2.661, 32.390], mean action: 4.940 [1.000, 21.000],  loss: 0.012274, mae: 0.350486, mean_q: 0.285633, mean_eps: 0.000000
 8745/10000: episode: 206, duration: 0.391s, episode steps:  36, steps per second:  92, episode reward: 35.640, mean reward:  0.990 [-2.489, 31.720], mean action: 4.611 [1.000, 11.000],  loss: 0.011275, mae: 0.353572, mean_q: 0.273747, mean_eps: 0.000000
 8766/10000: episode: 207, duration: 0.222s, episode steps:  21, steps per second:  94, episode reward: 47.436, mean reward:  2.259 [-0.140, 32.170], mean action: 5.381 [3.000, 14.000],  loss: 0.012674, mae: 0.362324, mean_q: 0.248832, mean_eps: 0.000000
 8801/10000: episode: 208, duration: 0.450s, episode steps:  35, steps per second:  78, episode reward: 41.205, mean reward:  1.177 [-2.323, 32.280], mean action: 5.600 [3.000, 15.000],  loss: 0.013766, mae: 0.363912, mean_q: 0.269981, mean_eps: 0.000000
 8846/10000: episode: 209, duration: 0.495s, episode steps:  45, steps per second:  91, episode reward: 32.458, mean reward:  0.721 [-2.460, 32.145], mean action: 4.711 [1.000, 21.000],  loss: 0.014650, mae: 0.371218, mean_q: 0.283726, mean_eps: 0.000000
 8869/10000: episode: 210, duration: 0.344s, episode steps:  23, steps per second:  67, episode reward: 37.895, mean reward:  1.648 [-2.900, 32.270], mean action: 5.043 [3.000, 21.000],  loss: 0.012234, mae: 0.356570, mean_q: 0.219864, mean_eps: 0.000000
 8910/10000: episode: 211, duration: 0.433s, episode steps:  41, steps per second:  95, episode reward: 38.749, mean reward:  0.945 [-2.587, 32.014], mean action: 4.634 [1.000, 14.000],  loss: 0.011873, mae: 0.349589, mean_q: 0.257655, mean_eps: 0.000000
 8939/10000: episode: 212, duration: 0.304s, episode steps:  29, steps per second:  95, episode reward: 47.619, mean reward:  1.642 [-0.309, 32.170], mean action: 3.690 [1.000, 15.000],  loss: 0.012731, mae: 0.344893, mean_q: 0.311158, mean_eps: 0.000000
 8966/10000: episode: 213, duration: 0.303s, episode steps:  27, steps per second:  89, episode reward: 36.846, mean reward:  1.365 [-2.780, 32.440], mean action: 4.704 [1.000, 14.000],  loss: 0.011175, mae: 0.337053, mean_q: 0.305512, mean_eps: 0.000000
 8987/10000: episode: 214, duration: 0.231s, episode steps:  21, steps per second:  91, episode reward: 35.971, mean reward:  1.713 [-3.000, 32.420], mean action: 5.238 [1.000, 15.000],  loss: 0.012530, mae: 0.346568, mean_q: 0.292964, mean_eps: 0.000000
 9009/10000: episode: 215, duration: 0.292s, episode steps:  22, steps per second:  75, episode reward: 43.978, mean reward:  1.999 [-2.018, 32.293], mean action: 4.091 [1.000, 15.000],  loss: 0.013168, mae: 0.346249, mean_q: 0.276527, mean_eps: 0.000000
 9052/10000: episode: 216, duration: 0.481s, episode steps:  43, steps per second:  89, episode reward: 42.986, mean reward:  1.000 [-2.203, 32.190], mean action: 5.558 [1.000, 21.000],  loss: 0.014312, mae: 0.365362, mean_q: 0.245390, mean_eps: 0.000000
 9134/10000: episode: 217, duration: 0.943s, episode steps:  82, steps per second:  87, episode reward: -32.080, mean reward: -0.391 [-32.157,  2.190], mean action: 6.695 [3.000, 16.000],  loss: 0.013006, mae: 0.362183, mean_q: 0.275142, mean_eps: 0.000000
 9197/10000: episode: 218, duration: 0.695s, episode steps:  63, steps per second:  91, episode reward: 34.996, mean reward:  0.555 [-2.556, 32.250], mean action: 7.984 [3.000, 17.000],  loss: 0.013165, mae: 0.361969, mean_q: 0.273841, mean_eps: 0.000000
 9236/10000: episode: 219, duration: 0.421s, episode steps:  39, steps per second:  93, episode reward: 44.185, mean reward:  1.133 [-2.183, 32.070], mean action: 2.410 [1.000, 4.000],  loss: 0.013519, mae: 0.353917, mean_q: 0.290710, mean_eps: 0.000000
 9280/10000: episode: 220, duration: 0.457s, episode steps:  44, steps per second:  96, episode reward: 44.289, mean reward:  1.007 [-2.088, 32.310], mean action: 2.091 [1.000, 3.000],  loss: 0.013903, mae: 0.350332, mean_q: 0.324045, mean_eps: 0.000000
 9301/10000: episode: 221, duration: 0.241s, episode steps:  21, steps per second:  87, episode reward: 38.279, mean reward:  1.823 [-2.198, 32.122], mean action: 6.476 [1.000, 20.000],  loss: 0.014448, mae: 0.354534, mean_q: 0.296303, mean_eps: 0.000000
 9329/10000: episode: 222, duration: 0.323s, episode steps:  28, steps per second:  87, episode reward: 41.333, mean reward:  1.476 [-2.926, 32.340], mean action: 3.750 [1.000, 11.000],  loss: 0.013706, mae: 0.354186, mean_q: 0.295497, mean_eps: 0.000000
 9364/10000: episode: 223, duration: 0.361s, episode steps:  35, steps per second:  97, episode reward: 44.276, mean reward:  1.265 [-2.226, 31.881], mean action: 5.229 [1.000, 19.000],  loss: 0.013687, mae: 0.352360, mean_q: 0.287446, mean_eps: 0.000000
 9397/10000: episode: 224, duration: 0.337s, episode steps:  33, steps per second:  98, episode reward: 41.970, mean reward:  1.272 [-2.287, 32.430], mean action: 4.909 [1.000, 17.000],  loss: 0.013308, mae: 0.345652, mean_q: 0.309850, mean_eps: 0.000000
 9436/10000: episode: 225, duration: 0.409s, episode steps:  39, steps per second:  95, episode reward: 34.724, mean reward:  0.890 [-2.416, 31.901], mean action: 4.641 [1.000, 21.000],  loss: 0.015716, mae: 0.364005, mean_q: 0.307641, mean_eps: 0.000000
 9476/10000: episode: 226, duration: 0.407s, episode steps:  40, steps per second:  98, episode reward: 43.032, mean reward:  1.076 [-2.794, 32.434], mean action: 4.925 [1.000, 21.000],  loss: 0.015645, mae: 0.371901, mean_q: 0.315989, mean_eps: 0.000000
 9491/10000: episode: 227, duration: 0.191s, episode steps:  15, steps per second:  79, episode reward: 44.110, mean reward:  2.941 [-2.127, 32.151], mean action: 3.200 [3.000, 6.000],  loss: 0.014798, mae: 0.364122, mean_q: 0.289447, mean_eps: 0.000000
 9519/10000: episode: 228, duration: 0.295s, episode steps:  28, steps per second:  95, episode reward: 39.000, mean reward:  1.393 [-2.538, 32.410], mean action: 3.857 [3.000, 6.000],  loss: 0.013821, mae: 0.361135, mean_q: 0.318099, mean_eps: 0.000000
 9559/10000: episode: 229, duration: 0.428s, episode steps:  40, steps per second:  93, episode reward: 46.971, mean reward:  1.174 [-0.688, 31.920], mean action: 3.825 [3.000, 15.000],  loss: 0.013991, mae: 0.363693, mean_q: 0.260896, mean_eps: 0.000000
 9571/10000: episode: 230, duration: 0.152s, episode steps:  12, steps per second:  79, episode reward: 46.324, mean reward:  3.860 [-0.250, 31.574], mean action: 6.500 [3.000, 15.000],  loss: 0.018658, mae: 0.384333, mean_q: 0.247961, mean_eps: 0.000000
 9620/10000: episode: 231, duration: 0.539s, episode steps:  49, steps per second:  91, episode reward: -32.920, mean reward: -0.672 [-32.864,  2.660], mean action: 5.837 [1.000, 21.000],  loss: 0.014183, mae: 0.353369, mean_q: 0.316849, mean_eps: 0.000000
 9662/10000: episode: 232, duration: 0.440s, episode steps:  42, steps per second:  95, episode reward: 34.543, mean reward:  0.822 [-2.752, 31.945], mean action: 8.571 [1.000, 21.000],  loss: 0.013473, mae: 0.357538, mean_q: 0.298587, mean_eps: 0.000000
 9713/10000: episode: 233, duration: 0.539s, episode steps:  51, steps per second:  95, episode reward: 32.795, mean reward:  0.643 [-3.000, 32.129], mean action: 7.353 [3.000, 21.000],  loss: 0.013589, mae: 0.356801, mean_q: 0.271140, mean_eps: 0.000000
 9744/10000: episode: 234, duration: 0.332s, episode steps:  31, steps per second:  93, episode reward: 41.643, mean reward:  1.343 [-2.904, 32.220], mean action: 3.161 [1.000, 11.000],  loss: 0.012485, mae: 0.342123, mean_q: 0.320318, mean_eps: 0.000000
 9765/10000: episode: 235, duration: 0.232s, episode steps:  21, steps per second:  90, episode reward: 47.015, mean reward:  2.239 [-0.518, 32.680], mean action: 1.905 [1.000, 4.000],  loss: 0.015086, mae: 0.369432, mean_q: 0.313475, mean_eps: 0.000000
 9793/10000: episode: 236, duration: 0.301s, episode steps:  28, steps per second:  93, episode reward: 34.900, mean reward:  1.246 [-3.000, 32.348], mean action: 5.643 [1.000, 15.000],  loss: 0.012619, mae: 0.365386, mean_q: 0.301387, mean_eps: 0.000000
 9834/10000: episode: 237, duration: 0.418s, episode steps:  41, steps per second:  98, episode reward: 35.467, mean reward:  0.865 [-2.566, 32.220], mean action: 4.049 [1.000, 14.000],  loss: 0.012197, mae: 0.355658, mean_q: 0.321485, mean_eps: 0.000000
 9873/10000: episode: 238, duration: 0.408s, episode steps:  39, steps per second:  96, episode reward: 35.196, mean reward:  0.902 [-3.000, 32.900], mean action: 5.179 [1.000, 15.000],  loss: 0.014873, mae: 0.358801, mean_q: 0.325765, mean_eps: 0.000000
 9907/10000: episode: 239, duration: 0.352s, episode steps:  34, steps per second:  96, episode reward: 43.813, mean reward:  1.289 [-2.026, 31.815], mean action: 7.853 [1.000, 16.000],  loss: 0.017300, mae: 0.365605, mean_q: 0.340762, mean_eps: 0.000000
 9936/10000: episode: 240, duration: 0.308s, episode steps:  29, steps per second:  94, episode reward: 38.278, mean reward:  1.320 [-3.000, 32.220], mean action: 5.517 [1.000, 17.000],  loss: 0.013577, mae: 0.350671, mean_q: 0.320413, mean_eps: 0.000000
 9967/10000: episode: 241, duration: 0.342s, episode steps:  31, steps per second:  91, episode reward: 41.124, mean reward:  1.327 [-2.427, 32.120], mean action: 6.194 [1.000, 17.000],  loss: 0.015014, mae: 0.348116, mean_q: 0.343258, mean_eps: 0.000000
done, took 104.005 seconds
Results against random player:
DQN Evaluation: 249 victories out of 300 episodes

Results against max player:
DQN Evaluation: 145 victories out of 300 episodes
