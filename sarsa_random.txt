params 0
{'model_type': 'sarsa_agent', 'l1_out': 128, 'l2_out': 64, 'gamma': 0.5, 'target_model_update': 1, 'delta_clip': 0.01, 'nb_steps_warmup': 1000, 'enable_dueling_dqn__': False, 'enable_double_dqn__': True, 'dueling_type__': 'avg'}
trial 0
Training for 10000 steps ...
   91/10000: episode: 1, duration: 2.136s, episode steps:  91, steps per second:  43, episode reward: -35.420, mean reward: -0.389 [-32.795,  2.721], mean action: 16.033 [4.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  173/10000: episode: 2, duration: 0.398s, episode steps:  82, steps per second: 206, episode reward: 32.389, mean reward:  0.395 [-2.670, 32.240], mean action: 12.976 [5.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  217/10000: episode: 3, duration: 0.312s, episode steps:  44, steps per second: 141, episode reward: 36.797, mean reward:  0.836 [-3.000, 32.210], mean action: 14.114 [5.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  304/10000: episode: 4, duration: 0.410s, episode steps:  87, steps per second: 212, episode reward: 34.734, mean reward:  0.399 [-2.642, 32.660], mean action: 13.770 [5.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  342/10000: episode: 5, duration: 0.176s, episode steps:  38, steps per second: 215, episode reward: 35.349, mean reward:  0.930 [-3.000, 32.349], mean action: 12.658 [5.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  394/10000: episode: 6, duration: 0.238s, episode steps:  52, steps per second: 218, episode reward: -38.770, mean reward: -0.746 [-32.085,  2.540], mean action: 15.712 [5.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  458/10000: episode: 7, duration: 0.297s, episode steps:  64, steps per second: 215, episode reward: 33.000, mean reward:  0.516 [-2.777, 32.160], mean action: 14.672 [5.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  548/10000: episode: 8, duration: 0.380s, episode steps:  90, steps per second: 237, episode reward: 37.913, mean reward:  0.421 [-3.000, 32.760], mean action: 13.667 [4.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  604/10000: episode: 9, duration: 0.259s, episode steps:  56, steps per second: 216, episode reward: -35.010, mean reward: -0.625 [-32.304,  2.534], mean action: 16.643 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  669/10000: episode: 10, duration: 0.289s, episode steps:  65, steps per second: 225, episode reward: 33.000, mean reward:  0.508 [-2.420, 32.050], mean action: 11.231 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  723/10000: episode: 11, duration: 0.243s, episode steps:  54, steps per second: 222, episode reward: -34.330, mean reward: -0.636 [-32.148,  3.000], mean action: 15.167 [5.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  794/10000: episode: 12, duration: 0.287s, episode steps:  71, steps per second: 247, episode reward: -34.800, mean reward: -0.490 [-31.531,  2.592], mean action: 15.958 [5.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  866/10000: episode: 13, duration: 0.302s, episode steps:  72, steps per second: 238, episode reward: -32.730, mean reward: -0.455 [-32.156,  4.217], mean action: 11.917 [5.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  954/10000: episode: 14, duration: 0.531s, episode steps:  88, steps per second: 166, episode reward: -38.290, mean reward: -0.435 [-31.543,  2.293], mean action: 14.773 [5.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1029/10000: episode: 15, duration: 0.957s, episode steps:  75, steps per second:  78, episode reward: 37.816, mean reward:  0.504 [-2.609, 32.160], mean action: 12.307 [5.000, 20.000],  loss: 0.004962, mae: 0.246360, mean_q: 0.362483, mean_eps: 0.000000
 1087/10000: episode: 16, duration: 0.486s, episode steps:  58, steps per second: 119, episode reward: -32.520, mean reward: -0.561 [-33.000,  2.880], mean action: 12.534 [0.000, 20.000],  loss: 0.006464, mae: 0.301987, mean_q: 0.352008, mean_eps: 0.000000
 1137/10000: episode: 17, duration: 0.447s, episode steps:  50, steps per second: 112, episode reward: -39.380, mean reward: -0.788 [-32.076,  2.170], mean action: 12.520 [2.000, 20.000],  loss: 0.005360, mae: 0.284831, mean_q: 0.253268, mean_eps: 0.000000
 1181/10000: episode: 18, duration: 0.394s, episode steps:  44, steps per second: 112, episode reward: 35.475, mean reward:  0.806 [-2.443, 29.712], mean action: 5.114 [2.000, 18.000],  loss: 0.005830, mae: 0.292608, mean_q: 0.301955, mean_eps: 0.000000
 1219/10000: episode: 19, duration: 0.293s, episode steps:  38, steps per second: 130, episode reward: 33.000, mean reward:  0.868 [-3.000, 32.290], mean action: 3.553 [2.000, 12.000],  loss: 0.008123, mae: 0.342489, mean_q: 0.515928, mean_eps: 0.000000
 1279/10000: episode: 20, duration: 0.445s, episode steps:  60, steps per second: 135, episode reward: 35.761, mean reward:  0.596 [-2.346, 29.554], mean action: 8.850 [2.000, 19.000],  loss: 0.004494, mae: 0.363376, mean_q: 0.176288, mean_eps: 0.000000
 1333/10000: episode: 21, duration: 0.415s, episode steps:  54, steps per second: 130, episode reward: 39.664, mean reward:  0.735 [-3.000, 32.260], mean action: 11.611 [0.000, 20.000],  loss: 0.005119, mae: 0.362981, mean_q: 0.241065, mean_eps: 0.000000
 1385/10000: episode: 22, duration: 0.421s, episode steps:  52, steps per second: 123, episode reward: 34.857, mean reward:  0.670 [-3.000, 32.120], mean action: 10.077 [0.000, 21.000],  loss: 0.005856, mae: 0.337138, mean_q: 0.174866, mean_eps: 0.000000
 1475/10000: episode: 23, duration: 0.663s, episode steps:  90, steps per second: 136, episode reward: 32.249, mean reward:  0.358 [-2.443, 32.270], mean action: 10.633 [0.000, 19.000],  loss: 0.003819, mae: 0.358301, mean_q: 0.097028, mean_eps: 0.000000
 1526/10000: episode: 24, duration: 0.387s, episode steps:  51, steps per second: 132, episode reward: 33.000, mean reward:  0.647 [-2.782, 32.080], mean action: 12.471 [4.000, 13.000],  loss: 0.006357, mae: 0.467796, mean_q: 0.203396, mean_eps: 0.000000
 1588/10000: episode: 25, duration: 0.572s, episode steps:  62, steps per second: 108, episode reward: 32.357, mean reward:  0.522 [-2.720, 32.467], mean action: 11.581 [7.000, 19.000],  loss: 0.005509, mae: 0.443559, mean_q: 0.006778, mean_eps: 0.000000
 1643/10000: episode: 26, duration: 0.541s, episode steps:  55, steps per second: 102, episode reward: -35.080, mean reward: -0.638 [-32.084,  2.240], mean action: 12.073 [4.000, 17.000],  loss: 0.005625, mae: 0.455089, mean_q: -0.029811, mean_eps: 0.000000
 1701/10000: episode: 27, duration: 0.433s, episode steps:  58, steps per second: 134, episode reward: -37.190, mean reward: -0.641 [-32.117,  2.687], mean action: 9.810 [0.000, 18.000],  loss: 0.004767, mae: 0.383628, mean_q: 0.022614, mean_eps: 0.000000
 1739/10000: episode: 28, duration: 0.361s, episode steps:  38, steps per second: 105, episode reward: 43.797, mean reward:  1.153 [-2.142, 32.140], mean action: 10.974 [8.000, 13.000],  loss: 0.005790, mae: 0.520550, mean_q: 0.133619, mean_eps: 0.000000
 1857/10000: episode: 29, duration: 0.835s, episode steps: 118, steps per second: 141, episode reward: 32.751, mean reward:  0.278 [-2.469, 31.854], mean action: 11.297 [0.000, 12.000],  loss: 0.003061, mae: 0.478825, mean_q: 0.050119, mean_eps: 0.000000
 1909/10000: episode: 30, duration: 0.395s, episode steps:  52, steps per second: 132, episode reward: 32.837, mean reward:  0.631 [-2.880, 32.440], mean action: 13.250 [8.000, 21.000],  loss: 0.006324, mae: 0.474985, mean_q: 0.058508, mean_eps: 0.000000
 1948/10000: episode: 31, duration: 0.286s, episode steps:  39, steps per second: 136, episode reward: 32.463, mean reward:  0.832 [-3.000, 32.006], mean action: 15.410 [1.000, 21.000],  loss: 0.008577, mae: 0.524217, mean_q: 0.002922, mean_eps: 0.000000
 2007/10000: episode: 32, duration: 0.477s, episode steps:  59, steps per second: 124, episode reward: 33.000, mean reward:  0.559 [-3.000, 32.070], mean action: 10.695 [0.000, 21.000],  loss: 0.005030, mae: 0.481076, mean_q: -0.016492, mean_eps: 0.000000
 2040/10000: episode: 33, duration: 0.250s, episode steps:  33, steps per second: 132, episode reward: -32.020, mean reward: -0.970 [-32.247,  3.000], mean action: 10.455 [0.000, 21.000],  loss: 0.009591, mae: 0.531665, mean_q: 0.205605, mean_eps: 0.000000
 2137/10000: episode: 34, duration: 0.697s, episode steps:  97, steps per second: 139, episode reward: -35.420, mean reward: -0.365 [-32.004,  2.812], mean action: 12.371 [0.000, 21.000],  loss: 0.003456, mae: 0.535968, mean_q: 0.024267, mean_eps: 0.000000
 2179/10000: episode: 35, duration: 0.383s, episode steps:  42, steps per second: 110, episode reward: 35.100, mean reward:  0.836 [-3.000, 32.150], mean action: 14.976 [4.000, 21.000],  loss: 0.006660, mae: 0.514672, mean_q: -0.025172, mean_eps: 0.000000
 2217/10000: episode: 36, duration: 0.304s, episode steps:  38, steps per second: 125, episode reward: 37.394, mean reward:  0.984 [-2.700, 31.914], mean action: 17.895 [4.000, 21.000],  loss: 0.007474, mae: 0.515324, mean_q: 0.141582, mean_eps: 0.000000
 2292/10000: episode: 37, duration: 0.543s, episode steps:  75, steps per second: 138, episode reward: -32.500, mean reward: -0.433 [-32.005,  3.000], mean action: 15.760 [0.000, 21.000],  loss: 0.004766, mae: 0.425523, mean_q: 0.069742, mean_eps: 0.000000
 2329/10000: episode: 38, duration: 0.266s, episode steps:  37, steps per second: 139, episode reward: -37.180, mean reward: -1.005 [-32.860,  3.000], mean action: 16.946 [0.000, 21.000],  loss: 0.007239, mae: 0.509123, mean_q: -0.032721, mean_eps: 0.000000
 2389/10000: episode: 39, duration: 0.442s, episode steps:  60, steps per second: 136, episode reward: -34.790, mean reward: -0.580 [-32.072,  2.570], mean action: 17.017 [8.000, 21.000],  loss: 0.005069, mae: 0.511567, mean_q: 0.049947, mean_eps: 0.000000
 2439/10000: episode: 40, duration: 0.390s, episode steps:  50, steps per second: 128, episode reward: 32.346, mean reward:  0.647 [-2.745, 31.995], mean action: 16.860 [11.000, 21.000],  loss: 0.006428, mae: 0.521018, mean_q: 0.076633, mean_eps: 0.000000
 2529/10000: episode: 41, duration: 0.635s, episode steps:  90, steps per second: 142, episode reward: 36.588, mean reward:  0.407 [-3.000, 32.170], mean action: 14.467 [14.000, 21.000],  loss: 0.004308, mae: 0.572495, mean_q: 0.129341, mean_eps: 0.000000
 2581/10000: episode: 42, duration: 0.374s, episode steps:  52, steps per second: 139, episode reward: -32.280, mean reward: -0.621 [-31.849,  2.571], mean action: 15.058 [0.000, 21.000],  loss: 0.006295, mae: 0.551458, mean_q: -0.020579, mean_eps: 0.000000
 2632/10000: episode: 43, duration: 0.356s, episode steps:  51, steps per second: 143, episode reward: -37.230, mean reward: -0.730 [-32.088,  3.000], mean action: 11.882 [9.000, 14.000],  loss: 0.005476, mae: 0.494740, mean_q: 0.031357, mean_eps: 0.000000
 2739/10000: episode: 44, duration: 0.796s, episode steps: 107, steps per second: 134, episode reward: -35.310, mean reward: -0.330 [-32.192,  2.658], mean action: 12.598 [5.000, 21.000],  loss: 0.003533, mae: 0.582129, mean_q: 0.036644, mean_eps: 0.000000
 2790/10000: episode: 45, duration: 0.404s, episode steps:  51, steps per second: 126, episode reward: 37.549, mean reward:  0.736 [-3.000, 32.643], mean action: 14.000 [9.000, 17.000],  loss: 0.005617, mae: 0.578381, mean_q: 0.048255, mean_eps: 0.000000
 2867/10000: episode: 46, duration: 0.523s, episode steps:  77, steps per second: 147, episode reward: 34.782, mean reward:  0.452 [-2.538, 31.836], mean action: 14.390 [14.000, 17.000],  loss: 0.004845, mae: 0.620844, mean_q: 0.089873, mean_eps: 0.000000
 2924/10000: episode: 47, duration: 0.420s, episode steps:  57, steps per second: 136, episode reward: -32.020, mean reward: -0.562 [-32.330,  3.062], mean action: 14.368 [14.000, 20.000],  loss: 0.005286, mae: 0.533032, mean_q: 0.006786, mean_eps: 0.000000
 2967/10000: episode: 48, duration: 0.328s, episode steps:  43, steps per second: 131, episode reward: 34.459, mean reward:  0.801 [-2.414, 31.835], mean action: 11.209 [2.000, 19.000],  loss: 0.006077, mae: 0.529168, mean_q: -0.139317, mean_eps: 0.000000
 3045/10000: episode: 49, duration: 0.570s, episode steps:  78, steps per second: 137, episode reward: 32.227, mean reward:  0.413 [-3.000, 32.290], mean action: 14.192 [10.000, 19.000],  loss: 0.004779, mae: 0.588754, mean_q: 0.044594, mean_eps: 0.000000
 3126/10000: episode: 50, duration: 0.580s, episode steps:  81, steps per second: 140, episode reward: -35.250, mean reward: -0.435 [-32.171,  2.642], mean action: 14.185 [9.000, 19.000],  loss: 0.004047, mae: 0.548693, mean_q: 0.012875, mean_eps: 0.000000
 3184/10000: episode: 51, duration: 0.417s, episode steps:  58, steps per second: 139, episode reward: -32.650, mean reward: -0.563 [-32.253,  2.530], mean action: 15.069 [9.000, 19.000],  loss: 0.006299, mae: 0.595120, mean_q: 0.063917, mean_eps: 0.000000
 3235/10000: episode: 52, duration: 0.409s, episode steps:  51, steps per second: 125, episode reward: 32.050, mean reward:  0.628 [-2.800, 31.906], mean action: 11.392 [0.000, 15.000],  loss: 0.006155, mae: 0.549048, mean_q: 0.010810, mean_eps: 0.000000
 3282/10000: episode: 53, duration: 0.357s, episode steps:  47, steps per second: 132, episode reward: -32.990, mean reward: -0.702 [-32.359,  2.750], mean action: 13.596 [9.000, 14.000],  loss: 0.007129, mae: 0.585526, mean_q: 0.055507, mean_eps: 0.000000
 3359/10000: episode: 54, duration: 0.550s, episode steps:  77, steps per second: 140, episode reward: -37.160, mean reward: -0.483 [-32.110,  2.660], mean action: 12.610 [4.000, 15.000],  loss: 0.003789, mae: 0.519578, mean_q: -0.013747, mean_eps: 0.000000
 3399/10000: episode: 55, duration: 0.316s, episode steps:  40, steps per second: 127, episode reward: -32.150, mean reward: -0.804 [-31.688,  3.000], mean action: 12.750 [4.000, 14.000],  loss: 0.007524, mae: 0.575037, mean_q: -0.004414, mean_eps: 0.000000
 3446/10000: episode: 56, duration: 0.392s, episode steps:  47, steps per second: 120, episode reward: -34.510, mean reward: -0.734 [-32.601,  2.330], mean action: 10.021 [4.000, 14.000],  loss: 0.006356, mae: 0.560517, mean_q: 0.056486, mean_eps: 0.000000
 3512/10000: episode: 57, duration: 0.577s, episode steps:  66, steps per second: 114, episode reward: -32.270, mean reward: -0.489 [-32.007,  3.000], mean action: 12.530 [4.000, 14.000],  loss: 0.005463, mae: 0.573140, mean_q: 0.093033, mean_eps: 0.000000
 3559/10000: episode: 58, duration: 0.336s, episode steps:  47, steps per second: 140, episode reward: 34.932, mean reward:  0.743 [-2.900, 32.010], mean action: 11.149 [4.000, 14.000],  loss: 0.006126, mae: 0.547772, mean_q: 0.000414, mean_eps: 0.000000
 3598/10000: episode: 59, duration: 0.310s, episode steps:  39, steps per second: 126, episode reward: 40.353, mean reward:  1.035 [-2.852, 32.869], mean action: 12.256 [4.000, 14.000],  loss: 0.005674, mae: 0.537863, mean_q: 0.072647, mean_eps: 0.000000
 3635/10000: episode: 60, duration: 0.272s, episode steps:  37, steps per second: 136, episode reward: -34.920, mean reward: -0.944 [-32.004,  3.000], mean action: 11.243 [2.000, 14.000],  loss: 0.007248, mae: 0.576461, mean_q: 0.020798, mean_eps: 0.000000
 3754/10000: episode: 61, duration: 0.924s, episode steps: 119, steps per second: 129, episode reward: 32.900, mean reward:  0.276 [-2.529, 32.140], mean action: 11.899 [6.000, 15.000],  loss: 0.003509, mae: 0.531229, mean_q: 0.017251, mean_eps: 0.000000
 3816/10000: episode: 62, duration: 0.443s, episode steps:  62, steps per second: 140, episode reward: -32.070, mean reward: -0.517 [-32.121,  3.000], mean action: 12.081 [7.000, 14.000],  loss: 0.005276, mae: 0.603427, mean_q: -0.042903, mean_eps: 0.000000
 3888/10000: episode: 63, duration: 0.479s, episode steps:  72, steps per second: 150, episode reward: 32.829, mean reward:  0.456 [-3.000, 30.047], mean action: 12.208 [7.000, 14.000],  loss: 0.004745, mae: 0.589758, mean_q: 0.036036, mean_eps: 0.000000
 3923/10000: episode: 64, duration: 0.346s, episode steps:  35, steps per second: 101, episode reward: 41.835, mean reward:  1.195 [-2.065, 32.280], mean action: 9.629 [2.000, 15.000],  loss: 0.006032, mae: 0.555518, mean_q: -0.008349, mean_eps: 0.000000
 3989/10000: episode: 65, duration: 0.466s, episode steps:  66, steps per second: 142, episode reward: -35.030, mean reward: -0.531 [-32.012,  2.703], mean action: 4.848 [2.000, 14.000],  loss: 0.004533, mae: 0.530618, mean_q: 0.088334, mean_eps: 0.000000
 4047/10000: episode: 66, duration: 0.413s, episode steps:  58, steps per second: 140, episode reward: 32.197, mean reward:  0.555 [-2.940, 32.289], mean action: 11.483 [2.000, 14.000],  loss: 0.005366, mae: 0.633010, mean_q: -0.072133, mean_eps: 0.000000
 4084/10000: episode: 67, duration: 0.297s, episode steps:  37, steps per second: 124, episode reward: -35.560, mean reward: -0.961 [-32.342,  2.317], mean action: 12.541 [2.000, 21.000],  loss: 0.006841, mae: 0.566960, mean_q: -0.021219, mean_eps: 0.000000
 4138/10000: episode: 68, duration: 0.374s, episode steps:  54, steps per second: 145, episode reward: 37.005, mean reward:  0.685 [-2.888, 33.000], mean action: 11.481 [2.000, 21.000],  loss: 0.004849, mae: 0.553142, mean_q: 0.021459, mean_eps: 0.000000
 4187/10000: episode: 69, duration: 0.437s, episode steps:  49, steps per second: 112, episode reward: 35.384, mean reward:  0.722 [-2.874, 32.490], mean action: 13.673 [2.000, 21.000],  loss: 0.005892, mae: 0.556025, mean_q: 0.028424, mean_eps: 0.000000
 4226/10000: episode: 70, duration: 0.288s, episode steps:  39, steps per second: 135, episode reward: 34.971, mean reward:  0.897 [-3.000, 32.179], mean action: 15.641 [2.000, 21.000],  loss: 0.007283, mae: 0.619372, mean_q: 0.107961, mean_eps: 0.000000
 4263/10000: episode: 71, duration: 0.296s, episode steps:  37, steps per second: 125, episode reward: -32.140, mean reward: -0.869 [-32.458,  2.460], mean action: 7.027 [2.000, 21.000],  loss: 0.008909, mae: 0.591351, mean_q: 0.207547, mean_eps: 0.000000
 4294/10000: episode: 72, duration: 0.232s, episode steps:  31, steps per second: 134, episode reward: 41.545, mean reward:  1.340 [-2.356, 32.040], mean action: 2.387 [2.000, 8.000],  loss: 0.007631, mae: 0.524503, mean_q: 0.225156, mean_eps: 0.000000
 4360/10000: episode: 73, duration: 0.468s, episode steps:  66, steps per second: 141, episode reward: 32.794, mean reward:  0.497 [-2.585, 32.055], mean action: 6.364 [2.000, 21.000],  loss: 0.004953, mae: 0.571375, mean_q: -0.083673, mean_eps: 0.000000
 4396/10000: episode: 74, duration: 0.259s, episode steps:  36, steps per second: 139, episode reward: 40.926, mean reward:  1.137 [-2.630, 32.640], mean action: 3.139 [2.000, 14.000],  loss: 0.006153, mae: 0.579490, mean_q: 0.325801, mean_eps: 0.000000
 4517/10000: episode: 75, duration: 0.820s, episode steps: 121, steps per second: 148, episode reward: 33.000, mean reward:  0.273 [-3.000, 32.400], mean action: 6.579 [2.000, 21.000],  loss: 0.003174, mae: 0.563996, mean_q: 0.059309, mean_eps: 0.000000
 4553/10000: episode: 76, duration: 0.259s, episode steps:  36, steps per second: 139, episode reward: 35.817, mean reward:  0.995 [-2.308, 32.860], mean action: 4.444 [2.000, 14.000],  loss: 0.008210, mae: 0.607923, mean_q: 0.115595, mean_eps: 0.000000
 4596/10000: episode: 77, duration: 0.306s, episode steps:  43, steps per second: 141, episode reward: 34.929, mean reward:  0.812 [-3.000, 32.090], mean action: 4.512 [2.000, 14.000],  loss: 0.006572, mae: 0.583300, mean_q: 0.051046, mean_eps: 0.000000
 4641/10000: episode: 78, duration: 0.357s, episode steps:  45, steps per second: 126, episode reward: -32.190, mean reward: -0.715 [-32.783,  2.199], mean action: 5.000 [2.000, 14.000],  loss: 0.006816, mae: 0.519390, mean_q: 0.123802, mean_eps: 0.000000
 4698/10000: episode: 79, duration: 0.410s, episode steps:  57, steps per second: 139, episode reward: 32.201, mean reward:  0.565 [-2.437, 32.140], mean action: 5.526 [2.000, 14.000],  loss: 0.006397, mae: 0.544416, mean_q: 0.213905, mean_eps: 0.000000
 4730/10000: episode: 80, duration: 0.234s, episode steps:  32, steps per second: 137, episode reward: -37.920, mean reward: -1.185 [-32.562,  2.282], mean action: 5.000 [2.000, 14.000],  loss: 0.008132, mae: 0.578225, mean_q: 0.189440, mean_eps: 0.000000
 4803/10000: episode: 81, duration: 0.518s, episode steps:  73, steps per second: 141, episode reward: -34.360, mean reward: -0.471 [-32.551,  2.670], mean action: 4.959 [2.000, 14.000],  loss: 0.004736, mae: 0.586641, mean_q: 0.004200, mean_eps: 0.000000
 4857/10000: episode: 82, duration: 0.404s, episode steps:  54, steps per second: 134, episode reward: 37.275, mean reward:  0.690 [-3.000, 31.753], mean action: 4.537 [2.000, 14.000],  loss: 0.005061, mae: 0.524661, mean_q: 0.067891, mean_eps: 0.000000
 4938/10000: episode: 83, duration: 0.741s, episode steps:  81, steps per second: 109, episode reward: -32.880, mean reward: -0.406 [-32.752,  2.430], mean action: 2.877 [2.000, 9.000],  loss: 0.004253, mae: 0.575604, mean_q: 0.173309, mean_eps: 0.000000
 4963/10000: episode: 84, duration: 0.204s, episode steps:  25, steps per second: 123, episode reward: 40.789, mean reward:  1.632 [-2.574, 32.163], mean action: 2.920 [2.000, 9.000],  loss: 0.008837, mae: 0.550257, mean_q: 0.283368, mean_eps: 0.000000
 4990/10000: episode: 85, duration: 0.215s, episode steps:  27, steps per second: 125, episode reward: 41.142, mean reward:  1.524 [-2.164, 31.774], mean action: 2.444 [2.000, 4.000],  loss: 0.007723, mae: 0.552170, mean_q: 0.312809, mean_eps: 0.000000
 5017/10000: episode: 86, duration: 0.195s, episode steps:  27, steps per second: 139, episode reward: 40.856, mean reward:  1.513 [-2.443, 32.153], mean action: 2.370 [2.000, 4.000],  loss: 0.007399, mae: 0.486217, mean_q: 0.339046, mean_eps: 0.000000
 5045/10000: episode: 87, duration: 0.211s, episode steps:  28, steps per second: 133, episode reward: 41.909, mean reward:  1.497 [-2.441, 32.150], mean action: 2.679 [2.000, 9.000],  loss: 0.007576, mae: 0.499270, mean_q: 0.408610, mean_eps: 0.000000
 5107/10000: episode: 88, duration: 0.495s, episode steps:  62, steps per second: 125, episode reward: 39.952, mean reward:  0.644 [-2.538, 32.598], mean action: 3.516 [2.000, 9.000],  loss: 0.004017, mae: 0.528745, mean_q: 0.067904, mean_eps: 0.000000
 5152/10000: episode: 89, duration: 0.331s, episode steps:  45, steps per second: 136, episode reward: 32.669, mean reward:  0.726 [-2.381, 31.939], mean action: 2.778 [2.000, 9.000],  loss: 0.006419, mae: 0.500760, mean_q: 0.225734, mean_eps: 0.000000
 5176/10000: episode: 90, duration: 0.206s, episode steps:  24, steps per second: 116, episode reward: 38.146, mean reward:  1.589 [-3.000, 32.070], mean action: 3.542 [2.000, 9.000],  loss: 0.010429, mae: 0.496235, mean_q: 0.489224, mean_eps: 0.000000
 5194/10000: episode: 91, duration: 0.165s, episode steps:  18, steps per second: 109, episode reward: 42.000, mean reward:  2.333 [-2.122, 30.460], mean action: 2.389 [2.000, 9.000],  loss: 0.008578, mae: 0.571010, mean_q: 0.597518, mean_eps: 0.000000
 5222/10000: episode: 92, duration: 0.200s, episode steps:  28, steps per second: 140, episode reward: 41.238, mean reward:  1.473 [-3.000, 32.178], mean action: 2.857 [2.000, 9.000],  loss: 0.008265, mae: 0.609504, mean_q: 0.215707, mean_eps: 0.000000
 5283/10000: episode: 93, duration: 0.430s, episode steps:  61, steps per second: 142, episode reward: 35.326, mean reward:  0.579 [-2.549, 32.010], mean action: 4.705 [2.000, 9.000],  loss: 0.005201, mae: 0.578460, mean_q: 0.175117, mean_eps: 0.000000
 5323/10000: episode: 94, duration: 0.273s, episode steps:  40, steps per second: 146, episode reward: -37.750, mean reward: -0.944 [-32.163,  2.321], mean action: 5.675 [2.000, 8.000],  loss: 0.006882, mae: 0.515836, mean_q: 0.122532, mean_eps: 0.000000
 5353/10000: episode: 95, duration: 0.222s, episode steps:  30, steps per second: 135, episode reward: 36.000, mean reward:  1.200 [-2.340, 32.150], mean action: 2.867 [2.000, 9.000],  loss: 0.008292, mae: 0.504539, mean_q: 0.169463, mean_eps: 0.000000
 5391/10000: episode: 96, duration: 0.301s, episode steps:  38, steps per second: 126, episode reward: 44.024, mean reward:  1.159 [-2.179, 32.330], mean action: 2.500 [2.000, 9.000],  loss: 0.004804, mae: 0.507354, mean_q: 0.195121, mean_eps: 0.000000
 5449/10000: episode: 97, duration: 0.423s, episode steps:  58, steps per second: 137, episode reward: 32.408, mean reward:  0.559 [-2.707, 32.080], mean action: 5.328 [2.000, 8.000],  loss: 0.006393, mae: 0.524941, mean_q: 0.136385, mean_eps: 0.000000
 5478/10000: episode: 98, duration: 0.241s, episode steps:  29, steps per second: 120, episode reward: 38.425, mean reward:  1.325 [-2.833, 32.061], mean action: 4.276 [2.000, 8.000],  loss: 0.007846, mae: 0.564243, mean_q: 0.284646, mean_eps: 0.000000
 5583/10000: episode: 99, duration: 0.732s, episode steps: 105, steps per second: 143, episode reward: -30.000, mean reward: -0.286 [-30.157,  2.200], mean action: 6.076 [2.000, 8.000],  loss: 0.003658, mae: 0.458552, mean_q: 0.133604, mean_eps: 0.000000
 5620/10000: episode: 100, duration: 0.274s, episode steps:  37, steps per second: 135, episode reward: 38.939, mean reward:  1.052 [-2.593, 31.959], mean action: 6.297 [2.000, 10.000],  loss: 0.007349, mae: 0.595984, mean_q: -0.005058, mean_eps: 0.000000
 5682/10000: episode: 101, duration: 0.452s, episode steps:  62, steps per second: 137, episode reward: -32.940, mean reward: -0.531 [-32.242,  2.668], mean action: 8.065 [2.000, 10.000],  loss: 0.005274, mae: 0.535454, mean_q: 0.023835, mean_eps: 0.000000
 5727/10000: episode: 102, duration: 0.317s, episode steps:  45, steps per second: 142, episode reward: -35.250, mean reward: -0.783 [-32.138,  2.902], mean action: 5.311 [2.000, 8.000],  loss: 0.006914, mae: 0.507315, mean_q: 0.114223, mean_eps: 0.000000
 5764/10000: episode: 103, duration: 0.271s, episode steps:  37, steps per second: 136, episode reward: 41.261, mean reward:  1.115 [-2.664, 32.160], mean action: 3.405 [2.000, 8.000],  loss: 0.006431, mae: 0.504993, mean_q: 0.177276, mean_eps: 0.000000
 5789/10000: episode: 104, duration: 0.182s, episode steps:  25, steps per second: 137, episode reward: 41.663, mean reward:  1.667 [-2.398, 32.448], mean action: 3.680 [2.000, 8.000],  loss: 0.008297, mae: 0.551695, mean_q: 0.257936, mean_eps: 0.000000
 5852/10000: episode: 105, duration: 0.453s, episode steps:  63, steps per second: 139, episode reward: -35.250, mean reward: -0.560 [-32.019,  2.380], mean action: 4.349 [2.000, 8.000],  loss: 0.005712, mae: 0.584478, mean_q: 0.268063, mean_eps: 0.000000
 5899/10000: episode: 106, duration: 0.330s, episode steps:  47, steps per second: 142, episode reward: 40.106, mean reward:  0.853 [-2.232, 32.500], mean action: 4.149 [2.000, 11.000],  loss: 0.004838, mae: 0.617682, mean_q: 0.057582, mean_eps: 0.000000
 5964/10000: episode: 107, duration: 0.480s, episode steps:  65, steps per second: 135, episode reward: 32.152, mean reward:  0.495 [-2.462, 32.260], mean action: 5.431 [2.000, 14.000],  loss: 0.004680, mae: 0.496891, mean_q: 0.064894, mean_eps: 0.000000
 6032/10000: episode: 108, duration: 0.477s, episode steps:  68, steps per second: 143, episode reward: 34.315, mean reward:  0.505 [-2.713, 32.200], mean action: 6.176 [4.000, 12.000],  loss: 0.004400, mae: 0.657271, mean_q: -0.035239, mean_eps: 0.000000
 6060/10000: episode: 109, duration: 0.201s, episode steps:  28, steps per second: 139, episode reward: 32.938, mean reward:  1.176 [-3.000, 30.331], mean action: 5.286 [2.000, 12.000],  loss: 0.009615, mae: 0.593793, mean_q: 0.025403, mean_eps: 0.000000
 6088/10000: episode: 110, duration: 0.202s, episode steps:  28, steps per second: 139, episode reward: 37.853, mean reward:  1.352 [-2.455, 32.140], mean action: 5.071 [2.000, 12.000],  loss: 0.008307, mae: 0.585622, mean_q: 0.159111, mean_eps: 0.000000
 6130/10000: episode: 111, duration: 0.296s, episode steps:  42, steps per second: 142, episode reward: 34.889, mean reward:  0.831 [-2.508, 32.090], mean action: 4.571 [2.000, 12.000],  loss: 0.006814, mae: 0.612506, mean_q: 0.201815, mean_eps: 0.000000
 6174/10000: episode: 112, duration: 0.319s, episode steps:  44, steps per second: 138, episode reward: 32.036, mean reward:  0.728 [-2.901, 34.260], mean action: 4.364 [0.000, 12.000],  loss: 0.006967, mae: 0.555335, mean_q: 0.083824, mean_eps: 0.000000
 6237/10000: episode: 113, duration: 0.437s, episode steps:  63, steps per second: 144, episode reward: 30.063, mean reward:  0.477 [-2.596, 29.739], mean action: 16.206 [2.000, 21.000],  loss: 0.004974, mae: 0.598345, mean_q: -0.048942, mean_eps: 0.000000
 6279/10000: episode: 114, duration: 0.289s, episode steps:  42, steps per second: 145, episode reward: 32.856, mean reward:  0.782 [-3.000, 32.820], mean action: 15.786 [2.000, 21.000],  loss: 0.007800, mae: 0.658507, mean_q: -0.094182, mean_eps: 0.000000
 6354/10000: episode: 115, duration: 0.603s, episode steps:  75, steps per second: 124, episode reward: -33.000, mean reward: -0.440 [-32.244,  3.060], mean action: 15.440 [2.000, 21.000],  loss: 0.005155, mae: 0.512623, mean_q: 0.016434, mean_eps: 0.000000
 6430/10000: episode: 116, duration: 0.515s, episode steps:  76, steps per second: 148, episode reward: 32.151, mean reward:  0.423 [-3.000, 32.180], mean action: 16.434 [2.000, 20.000],  loss: 0.004860, mae: 0.562310, mean_q: 0.075028, mean_eps: 0.000000
 6480/10000: episode: 117, duration: 0.349s, episode steps:  50, steps per second: 143, episode reward: -35.510, mean reward: -0.710 [-32.578,  2.960], mean action: 15.300 [2.000, 20.000],  loss: 0.006286, mae: 0.511728, mean_q: 0.023149, mean_eps: 0.000000
 6521/10000: episode: 118, duration: 0.305s, episode steps:  41, steps per second: 134, episode reward: 32.549, mean reward:  0.794 [-2.515, 31.786], mean action: 9.634 [2.000, 15.000],  loss: 0.007008, mae: 0.571025, mean_q: 0.063940, mean_eps: 0.000000
 6586/10000: episode: 119, duration: 0.456s, episode steps:  65, steps per second: 143, episode reward: 34.851, mean reward:  0.536 [-2.553, 32.050], mean action: 10.323 [2.000, 18.000],  loss: 0.005158, mae: 0.528113, mean_q: 0.127718, mean_eps: 0.000000
 6650/10000: episode: 120, duration: 0.445s, episode steps:  64, steps per second: 144, episode reward: -34.450, mean reward: -0.538 [-31.633,  2.500], mean action: 10.062 [2.000, 15.000],  loss: 0.004919, mae: 0.584924, mean_q: -0.063030, mean_eps: 0.000000
 6686/10000: episode: 121, duration: 0.248s, episode steps:  36, steps per second: 145, episode reward: -34.780, mean reward: -0.966 [-32.213,  3.000], mean action: 13.694 [10.000, 15.000],  loss: 0.008255, mae: 0.660209, mean_q: 0.172297, mean_eps: 0.000000
 6728/10000: episode: 122, duration: 0.295s, episode steps:  42, steps per second: 142, episode reward: 32.506, mean reward:  0.774 [-3.000, 32.083], mean action: 12.500 [2.000, 15.000],  loss: 0.007103, mae: 0.607885, mean_q: 0.053110, mean_eps: 0.000000
 6925/10000: episode: 123, duration: 1.386s, episode steps: 197, steps per second: 142, episode reward: 32.597, mean reward:  0.165 [-2.250, 29.864], mean action: 12.137 [10.000, 17.000],  loss: 0.002512, mae: 0.638011, mean_q: 0.052485, mean_eps: 0.000000
 6970/10000: episode: 124, duration: 0.310s, episode steps:  45, steps per second: 145, episode reward: -35.400, mean reward: -0.787 [-32.597,  3.000], mean action: 12.622 [10.000, 17.000],  loss: 0.006264, mae: 0.600661, mean_q: -0.032743, mean_eps: 0.000000
 7023/10000: episode: 125, duration: 0.387s, episode steps:  53, steps per second: 137, episode reward: -32.320, mean reward: -0.610 [-32.107,  2.921], mean action: 13.377 [4.000, 17.000],  loss: 0.006250, mae: 0.603052, mean_q: 0.051409, mean_eps: 0.000000
 7069/10000: episode: 126, duration: 0.320s, episode steps:  46, steps per second: 144, episode reward: 35.011, mean reward:  0.761 [-2.624, 32.269], mean action: 12.848 [8.000, 15.000],  loss: 0.006365, mae: 0.613822, mean_q: -0.063370, mean_eps: 0.000000
 7105/10000: episode: 127, duration: 0.331s, episode steps:  36, steps per second: 109, episode reward: -37.540, mean reward: -1.043 [-32.089,  2.842], mean action: 13.778 [10.000, 15.000],  loss: 0.006812, mae: 0.601538, mean_q: 0.012454, mean_eps: 0.000000
 7147/10000: episode: 128, duration: 0.284s, episode steps:  42, steps per second: 148, episode reward: -37.190, mean reward: -0.885 [-32.168,  2.600], mean action: 13.524 [2.000, 20.000],  loss: 0.006076, mae: 0.618444, mean_q: -0.034333, mean_eps: 0.000000
 7185/10000: episode: 129, duration: 0.269s, episode steps:  38, steps per second: 141, episode reward: 40.214, mean reward:  1.058 [-2.779, 32.300], mean action: 11.842 [2.000, 16.000],  loss: 0.006477, mae: 0.631852, mean_q: 0.019836, mean_eps: 0.000000
 7215/10000: episode: 130, duration: 0.210s, episode steps:  30, steps per second: 143, episode reward: 43.833, mean reward:  1.461 [-2.034, 32.524], mean action: 7.400 [2.000, 14.000],  loss: 0.006705, mae: 0.587882, mean_q: 0.225742, mean_eps: 0.000000
 7260/10000: episode: 131, duration: 0.311s, episode steps:  45, steps per second: 145, episode reward: -34.910, mean reward: -0.776 [-32.085,  2.430], mean action: 10.111 [2.000, 14.000],  loss: 0.006265, mae: 0.677291, mean_q: 0.084929, mean_eps: 0.000000
 7302/10000: episode: 132, duration: 0.347s, episode steps:  42, steps per second: 121, episode reward: 38.189, mean reward:  0.909 [-3.000, 32.198], mean action: 5.786 [2.000, 15.000],  loss: 0.005881, mae: 0.605979, mean_q: 0.108043, mean_eps: 0.000000
 7342/10000: episode: 133, duration: 0.306s, episode steps:  40, steps per second: 131, episode reward: 43.882, mean reward:  1.097 [-2.395, 32.140], mean action: 4.375 [2.000, 16.000],  loss: 0.005839, mae: 0.553549, mean_q: 0.332670, mean_eps: 0.000000
 7381/10000: episode: 134, duration: 0.276s, episode steps:  39, steps per second: 141, episode reward: -35.240, mean reward: -0.904 [-33.000,  2.272], mean action: 5.821 [2.000, 15.000],  loss: 0.007841, mae: 0.617174, mean_q: 0.401923, mean_eps: 0.000000
 7469/10000: episode: 135, duration: 0.633s, episode steps:  88, steps per second: 139, episode reward: 34.310, mean reward:  0.390 [-2.463, 32.140], mean action: 5.625 [2.000, 15.000],  loss: 0.004360, mae: 0.643886, mean_q: 0.028437, mean_eps: 0.000000
 7516/10000: episode: 136, duration: 0.328s, episode steps:  47, steps per second: 143, episode reward: 30.129, mean reward:  0.641 [-3.000, 31.765], mean action: 5.191 [2.000, 10.000],  loss: 0.007849, mae: 0.576107, mean_q: 0.195930, mean_eps: 0.000000
 7596/10000: episode: 137, duration: 0.555s, episode steps:  80, steps per second: 144, episode reward: 38.083, mean reward:  0.476 [-2.953, 32.270], mean action: 2.825 [2.000, 14.000],  loss: 0.003869, mae: 0.623395, mean_q: 0.188819, mean_eps: 0.000000
 7617/10000: episode: 138, duration: 0.160s, episode steps:  21, steps per second: 131, episode reward: 44.326, mean reward:  2.111 [-2.091, 32.270], mean action: 2.857 [2.000, 8.000],  loss: 0.007761, mae: 0.599035, mean_q: 0.238856, mean_eps: 0.000000
 7683/10000: episode: 139, duration: 0.511s, episode steps:  66, steps per second: 129, episode reward: -35.590, mean reward: -0.539 [-32.132,  2.754], mean action: 6.545 [2.000, 14.000],  loss: 0.005159, mae: 0.575365, mean_q: 0.091272, mean_eps: 0.000000
 7722/10000: episode: 140, duration: 0.276s, episode steps:  39, steps per second: 141, episode reward: -32.730, mean reward: -0.839 [-32.116,  2.540], mean action: 6.051 [2.000, 14.000],  loss: 0.007775, mae: 0.580256, mean_q: 0.098776, mean_eps: 0.000000
 7774/10000: episode: 141, duration: 0.414s, episode steps:  52, steps per second: 125, episode reward: 34.303, mean reward:  0.660 [-2.217, 31.991], mean action: 7.885 [2.000, 14.000],  loss: 0.005708, mae: 0.592401, mean_q: 0.031591, mean_eps: 0.000000
 7806/10000: episode: 142, duration: 0.258s, episode steps:  32, steps per second: 124, episode reward: 35.171, mean reward:  1.099 [-3.000, 31.876], mean action: 5.375 [2.000, 14.000],  loss: 0.009118, mae: 0.580953, mean_q: 0.130332, mean_eps: 0.000000
 7847/10000: episode: 143, duration: 0.301s, episode steps:  41, steps per second: 136, episode reward: 35.089, mean reward:  0.856 [-2.701, 31.465], mean action: 7.854 [2.000, 14.000],  loss: 0.006401, mae: 0.609932, mean_q: 0.230140, mean_eps: 0.000000
 7903/10000: episode: 144, duration: 0.431s, episode steps:  56, steps per second: 130, episode reward: 32.736, mean reward:  0.585 [-2.619, 31.994], mean action: 10.286 [2.000, 14.000],  loss: 0.005502, mae: 0.556099, mean_q: 0.114870, mean_eps: 0.000000
 7964/10000: episode: 145, duration: 0.430s, episode steps:  61, steps per second: 142, episode reward: -35.890, mean reward: -0.588 [-32.111,  2.330], mean action: 9.541 [2.000, 14.000],  loss: 0.004278, mae: 0.601384, mean_q: 0.030894, mean_eps: 0.000000
 8012/10000: episode: 146, duration: 0.346s, episode steps:  48, steps per second: 139, episode reward: 38.253, mean reward:  0.797 [-2.430, 32.152], mean action: 6.729 [2.000, 16.000],  loss: 0.005587, mae: 0.594400, mean_q: -0.005003, mean_eps: 0.000000
 8044/10000: episode: 147, duration: 0.232s, episode steps:  32, steps per second: 138, episode reward: 40.729, mean reward:  1.273 [-2.347, 32.400], mean action: 5.375 [2.000, 8.000],  loss: 0.006536, mae: 0.613911, mean_q: 0.199454, mean_eps: 0.000000
 8070/10000: episode: 148, duration: 0.197s, episode steps:  26, steps per second: 132, episode reward: 43.535, mean reward:  1.674 [-2.812, 32.470], mean action: 3.923 [2.000, 14.000],  loss: 0.006800, mae: 0.525856, mean_q: 0.202393, mean_eps: 0.000000
 8191/10000: episode: 149, duration: 0.874s, episode steps: 121, steps per second: 138, episode reward: 33.000, mean reward:  0.273 [-2.954, 32.480], mean action: 3.983 [2.000, 14.000],  loss: 0.003447, mae: 0.559225, mean_q: 0.150638, mean_eps: 0.000000
 8241/10000: episode: 150, duration: 0.351s, episode steps:  50, steps per second: 143, episode reward: -32.140, mean reward: -0.643 [-32.198,  2.670], mean action: 8.560 [2.000, 20.000],  loss: 0.005888, mae: 0.595031, mean_q: -0.001719, mean_eps: 0.000000
 8267/10000: episode: 151, duration: 0.187s, episode steps:  26, steps per second: 139, episode reward: 37.678, mean reward:  1.449 [-3.000, 32.160], mean action: 8.538 [2.000, 20.000],  loss: 0.009573, mae: 0.665675, mean_q: 0.142147, mean_eps: 0.000000
 8310/10000: episode: 152, duration: 0.302s, episode steps:  43, steps per second: 142, episode reward: 37.334, mean reward:  0.868 [-2.243, 29.921], mean action: 5.860 [2.000, 20.000],  loss: 0.005289, mae: 0.602935, mean_q: 0.194096, mean_eps: 0.000000
 8400/10000: episode: 153, duration: 0.672s, episode steps:  90, steps per second: 134, episode reward: 32.773, mean reward:  0.364 [-2.491, 32.240], mean action: 10.711 [2.000, 20.000],  loss: 0.004387, mae: 0.563427, mean_q: 0.119842, mean_eps: 0.000000
 8451/10000: episode: 154, duration: 0.346s, episode steps:  51, steps per second: 147, episode reward: -34.860, mean reward: -0.684 [-32.156,  3.000], mean action: 11.000 [2.000, 15.000],  loss: 0.006234, mae: 0.607423, mean_q: -0.036712, mean_eps: 0.000000
 8504/10000: episode: 155, duration: 0.401s, episode steps:  53, steps per second: 132, episode reward: 32.426, mean reward:  0.612 [-2.619, 31.942], mean action: 10.434 [2.000, 14.000],  loss: 0.006138, mae: 0.591551, mean_q: 0.062375, mean_eps: 0.000000
 8561/10000: episode: 156, duration: 0.419s, episode steps:  57, steps per second: 136, episode reward: 32.993, mean reward:  0.579 [-3.000, 32.013], mean action: 11.088 [2.000, 14.000],  loss: 0.006338, mae: 0.526199, mean_q: 0.271073, mean_eps: 0.000000
 8618/10000: episode: 157, duration: 0.401s, episode steps:  57, steps per second: 142, episode reward: 41.978, mean reward:  0.736 [-2.235, 32.100], mean action: 8.070 [2.000, 21.000],  loss: 0.003865, mae: 0.580779, mean_q: -0.004969, mean_eps: 0.000000
 8644/10000: episode: 158, duration: 0.204s, episode steps:  26, steps per second: 127, episode reward: 40.795, mean reward:  1.569 [-2.565, 32.391], mean action: 3.962 [2.000, 14.000],  loss: 0.006962, mae: 0.557208, mean_q: 0.304316, mean_eps: 0.000000
 8694/10000: episode: 159, duration: 0.428s, episode steps:  50, steps per second: 117, episode reward: 38.083, mean reward:  0.762 [-2.621, 32.010], mean action: 7.260 [2.000, 21.000],  loss: 0.005626, mae: 0.626732, mean_q: 0.130459, mean_eps: 0.000000
 8716/10000: episode: 160, duration: 0.191s, episode steps:  22, steps per second: 115, episode reward: 44.014, mean reward:  2.001 [-2.657, 31.873], mean action: 7.318 [2.000, 21.000],  loss: 0.008025, mae: 0.624366, mean_q: 0.277652, mean_eps: 0.000000
 8743/10000: episode: 161, duration: 0.205s, episode steps:  27, steps per second: 132, episode reward: 38.320, mean reward:  1.419 [-2.650, 31.839], mean action: 5.296 [2.000, 21.000],  loss: 0.008928, mae: 0.510167, mean_q: 0.464883, mean_eps: 0.000000
 8778/10000: episode: 162, duration: 0.255s, episode steps:  35, steps per second: 138, episode reward: 35.164, mean reward:  1.005 [-3.000, 32.220], mean action: 7.400 [2.000, 21.000],  loss: 0.008914, mae: 0.559912, mean_q: 0.420934, mean_eps: 0.000000
 8849/10000: episode: 163, duration: 0.497s, episode steps:  71, steps per second: 143, episode reward: 32.938, mean reward:  0.464 [-2.762, 32.030], mean action: 11.704 [2.000, 21.000],  loss: 0.005261, mae: 0.590555, mean_q: 0.114072, mean_eps: 0.000000
 8888/10000: episode: 164, duration: 0.283s, episode steps:  39, steps per second: 138, episode reward: 41.320, mean reward:  1.059 [-2.343, 32.040], mean action: 9.256 [2.000, 21.000],  loss: 0.006024, mae: 0.556911, mean_q: 0.273601, mean_eps: 0.000000
 8944/10000: episode: 165, duration: 0.394s, episode steps:  56, steps per second: 142, episode reward: 32.330, mean reward:  0.577 [-2.983, 32.139], mean action: 13.250 [2.000, 21.000],  loss: 0.005979, mae: 0.594709, mean_q: 0.127308, mean_eps: 0.000000
 9002/10000: episode: 166, duration: 0.402s, episode steps:  58, steps per second: 144, episode reward: -32.150, mean reward: -0.554 [-30.180,  3.000], mean action: 12.293 [2.000, 21.000],  loss: 0.005725, mae: 0.653725, mean_q: 0.027993, mean_eps: 0.000000
 9045/10000: episode: 167, duration: 0.315s, episode steps:  43, steps per second: 136, episode reward: 32.167, mean reward:  0.748 [-3.000, 31.730], mean action: 14.349 [2.000, 21.000],  loss: 0.006735, mae: 0.602186, mean_q: -0.059591, mean_eps: 0.000000
 9085/10000: episode: 168, duration: 0.318s, episode steps:  40, steps per second: 126, episode reward: -35.090, mean reward: -0.877 [-32.126,  2.280], mean action: 10.925 [2.000, 21.000],  loss: 0.007014, mae: 0.587339, mean_q: 0.165211, mean_eps: 0.000000
 9139/10000: episode: 169, duration: 0.384s, episode steps:  54, steps per second: 141, episode reward: 37.536, mean reward:  0.695 [-3.000, 32.800], mean action: 7.204 [2.000, 14.000],  loss: 0.005513, mae: 0.619851, mean_q: 0.034863, mean_eps: 0.000000
 9162/10000: episode: 170, duration: 0.178s, episode steps:  23, steps per second: 129, episode reward: 44.667, mean reward:  1.942 [-3.000, 32.500], mean action: 5.130 [2.000, 14.000],  loss: 0.008007, mae: 0.580985, mean_q: 0.245329, mean_eps: 0.000000
 9206/10000: episode: 171, duration: 0.327s, episode steps:  44, steps per second: 135, episode reward: 32.409, mean reward:  0.737 [-2.671, 32.450], mean action: 10.000 [2.000, 14.000],  loss: 0.007389, mae: 0.623573, mean_q: 0.182868, mean_eps: 0.000000
 9241/10000: episode: 172, duration: 0.264s, episode steps:  35, steps per second: 133, episode reward: 38.089, mean reward:  1.088 [-2.658, 32.270], mean action: 4.914 [2.000, 14.000],  loss: 0.007439, mae: 0.569858, mean_q: 0.227765, mean_eps: 0.000000
 9292/10000: episode: 173, duration: 0.376s, episode steps:  51, steps per second: 136, episode reward: 32.811, mean reward:  0.643 [-3.000, 32.100], mean action: 4.588 [2.000, 14.000],  loss: 0.006516, mae: 0.639005, mean_q: 0.139491, mean_eps: 0.000000
 9345/10000: episode: 174, duration: 0.388s, episode steps:  53, steps per second: 137, episode reward: 39.450, mean reward:  0.744 [-2.361, 32.150], mean action: 11.642 [2.000, 15.000],  loss: 0.004494, mae: 0.544336, mean_q: 0.045476, mean_eps: 0.000000
 9369/10000: episode: 175, duration: 0.180s, episode steps:  24, steps per second: 133, episode reward: 45.203, mean reward:  1.883 [-0.848, 32.080], mean action: 5.167 [2.000, 15.000],  loss: 0.007102, mae: 0.568184, mean_q: -0.046837, mean_eps: 0.000000
 9408/10000: episode: 176, duration: 0.294s, episode steps:  39, steps per second: 133, episode reward: 38.377, mean reward:  0.984 [-2.543, 32.250], mean action: 3.513 [2.000, 9.000],  loss: 0.005714, mae: 0.521968, mean_q: 0.262203, mean_eps: 0.000000
 9446/10000: episode: 177, duration: 0.285s, episode steps:  38, steps per second: 133, episode reward: 32.566, mean reward:  0.857 [-3.000, 31.880], mean action: 2.105 [2.000, 4.000],  loss: 0.008116, mae: 0.582011, mean_q: 0.278053, mean_eps: 0.000000
 9469/10000: episode: 178, duration: 0.171s, episode steps:  23, steps per second: 134, episode reward: 40.737, mean reward:  1.771 [-3.000, 32.530], mean action: 3.391 [2.000, 9.000],  loss: 0.009039, mae: 0.619173, mean_q: 0.364045, mean_eps: 0.000000
 9529/10000: episode: 179, duration: 0.484s, episode steps:  60, steps per second: 124, episode reward: -42.930, mean reward: -0.716 [-31.572,  2.130], mean action: 7.767 [2.000, 17.000],  loss: 0.003810, mae: 0.557811, mean_q: 0.082666, mean_eps: 0.000000
 9565/10000: episode: 180, duration: 0.256s, episode steps:  36, steps per second: 141, episode reward: -32.290, mean reward: -0.897 [-31.906,  2.378], mean action: 9.833 [2.000, 21.000],  loss: 0.008549, mae: 0.610385, mean_q: -0.038842, mean_eps: 0.000000
 9601/10000: episode: 181, duration: 0.298s, episode steps:  36, steps per second: 121, episode reward: 32.465, mean reward:  0.902 [-2.998, 31.742], mean action: 11.000 [2.000, 14.000],  loss: 0.008268, mae: 0.585367, mean_q: -0.034296, mean_eps: 0.000000
 9645/10000: episode: 182, duration: 0.325s, episode steps:  44, steps per second: 136, episode reward: 37.781, mean reward:  0.859 [-2.538, 32.190], mean action: 10.227 [2.000, 14.000],  loss: 0.005633, mae: 0.587806, mean_q: 0.181159, mean_eps: 0.000000
 9714/10000: episode: 183, duration: 0.526s, episode steps:  69, steps per second: 131, episode reward: 33.000, mean reward:  0.478 [-2.559, 32.230], mean action: 12.087 [2.000, 14.000],  loss: 0.004859, mae: 0.627812, mean_q: 0.087049, mean_eps: 0.000000
 9808/10000: episode: 184, duration: 0.650s, episode steps:  94, steps per second: 145, episode reward: -32.420, mean reward: -0.345 [-31.943,  2.981], mean action: 13.553 [2.000, 15.000],  loss: 0.003967, mae: 0.604306, mean_q: -0.012166, mean_eps: 0.000000
 9842/10000: episode: 185, duration: 0.249s, episode steps:  34, steps per second: 136, episode reward: 40.716, mean reward:  1.198 [-2.349, 31.943], mean action: 3.765 [2.000, 14.000],  loss: 0.006102, mae: 0.590282, mean_q: 0.155628, mean_eps: 0.000000
 9870/10000: episode: 186, duration: 0.204s, episode steps:  28, steps per second: 137, episode reward: 40.502, mean reward:  1.447 [-2.288, 32.510], mean action: 5.500 [2.000, 16.000],  loss: 0.007405, mae: 0.541944, mean_q: 0.266432, mean_eps: 0.000000
 9926/10000: episode: 187, duration: 0.378s, episode steps:  56, steps per second: 148, episode reward: -32.950, mean reward: -0.588 [-32.510,  3.000], mean action: 9.732 [2.000, 20.000],  loss: 0.005413, mae: 0.603411, mean_q: 0.107434, mean_eps: 0.000000
 9959/10000: episode: 188, duration: 0.228s, episode steps:  33, steps per second: 145, episode reward: 33.000, mean reward:  1.000 [-3.000, 32.370], mean action: 6.364 [2.000, 14.000],  loss: 0.008781, mae: 0.648584, mean_q: 0.036510, mean_eps: 0.000000
done, took 74.149 seconds
Results against random player:
DQN Evaluation: 196 victories out of 300 episodes

Results against max player:
DQN Evaluation: 46 victories out of 300 episodes
