params 0
{'model_type': 'dqn_agent', 'l1_out': 128, 'l2_out': 64, 'gamma': 0.5, 'target_model_update': 1, 'delta_clip': 0.01, 'nb_steps_warmup': 1000, 'enable_dueling_dqn__': False, 'enable_double_dqn__': True, 'dueling_type__': 'avg'}
trial 0
Training for 5000 steps ...
   36/5000: episode: 1, duration: 0.540s, episode steps:  36, steps per second:  67, episode reward: 36.855, mean reward:  1.024 [-3.000, 32.390], mean action: 13.972 [1.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   66/5000: episode: 2, duration: 0.207s, episode steps:  30, steps per second: 145, episode reward: 40.662, mean reward:  1.355 [-2.495, 31.895], mean action: 9.633 [1.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  100/5000: episode: 3, duration: 0.271s, episode steps:  34, steps per second: 125, episode reward: 35.717, mean reward:  1.051 [-2.665, 32.580], mean action: 8.294 [1.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  143/5000: episode: 4, duration: 0.299s, episode steps:  43, steps per second: 144, episode reward: -36.760, mean reward: -0.855 [-32.283,  2.330], mean action: 11.558 [1.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  191/5000: episode: 5, duration: 0.337s, episode steps:  48, steps per second: 142, episode reward: 38.294, mean reward:  0.798 [-2.457, 32.068], mean action: 10.792 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  221/5000: episode: 6, duration: 0.216s, episode steps:  30, steps per second: 139, episode reward: 35.424, mean reward:  1.181 [-3.000, 32.070], mean action: 7.800 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  250/5000: episode: 7, duration: 0.191s, episode steps:  29, steps per second: 152, episode reward: 37.862, mean reward:  1.306 [-3.000, 31.933], mean action: 8.069 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  298/5000: episode: 8, duration: 0.281s, episode steps:  48, steps per second: 171, episode reward: -34.910, mean reward: -0.727 [-31.969,  3.000], mean action: 13.312 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  363/5000: episode: 9, duration: 0.451s, episode steps:  65, steps per second: 144, episode reward: 37.696, mean reward:  0.580 [-2.324, 32.700], mean action: 4.862 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  426/5000: episode: 10, duration: 0.376s, episode steps:  63, steps per second: 167, episode reward: 34.700, mean reward:  0.551 [-2.292, 32.580], mean action: 8.889 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  500/5000: episode: 11, duration: 0.416s, episode steps:  74, steps per second: 178, episode reward: 35.235, mean reward:  0.476 [-2.322, 32.380], mean action: 10.189 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  535/5000: episode: 12, duration: 0.205s, episode steps:  35, steps per second: 171, episode reward: 42.168, mean reward:  1.205 [-2.142, 32.620], mean action: 9.371 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  591/5000: episode: 13, duration: 0.331s, episode steps:  56, steps per second: 169, episode reward: -32.100, mean reward: -0.573 [-32.835,  2.330], mean action: 10.732 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  634/5000: episode: 14, duration: 0.281s, episode steps:  43, steps per second: 153, episode reward: 34.883, mean reward:  0.811 [-2.701, 31.907], mean action: 10.558 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  664/5000: episode: 15, duration: 0.175s, episode steps:  30, steps per second: 171, episode reward: 35.115, mean reward:  1.171 [-3.000, 32.260], mean action: 13.967 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  692/5000: episode: 16, duration: 0.177s, episode steps:  28, steps per second: 158, episode reward: 41.398, mean reward:  1.479 [-2.218, 32.380], mean action: 4.250 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  739/5000: episode: 17, duration: 0.274s, episode steps:  47, steps per second: 172, episode reward: -32.500, mean reward: -0.691 [-31.912,  2.901], mean action: 11.936 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  813/5000: episode: 18, duration: 0.423s, episode steps:  74, steps per second: 175, episode reward: -35.050, mean reward: -0.474 [-32.356,  2.334], mean action: 13.378 [1.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  907/5000: episode: 19, duration: 0.531s, episode steps:  94, steps per second: 177, episode reward: 35.127, mean reward:  0.374 [-2.155, 30.393], mean action: 10.734 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  962/5000: episode: 20, duration: 0.507s, episode steps:  55, steps per second: 108, episode reward: 41.130, mean reward:  0.748 [-3.000, 32.340], mean action: 8.545 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1002/5000: episode: 21, duration: 1.474s, episode steps:  40, steps per second:  27, episode reward: 35.616, mean reward:  0.890 [-2.414, 32.050], mean action: 11.375 [0.000, 20.000],  loss: 0.003863, mae: 0.382420, mean_q: 0.434358, mean_eps: 0.000000
 1041/5000: episode: 22, duration: 0.468s, episode steps:  39, steps per second:  83, episode reward: 32.938, mean reward:  0.845 [-3.000, 33.000], mean action: 13.692 [0.000, 21.000],  loss: 0.011643, mae: 0.510741, mean_q: 0.241369, mean_eps: 0.000000
 1082/5000: episode: 23, duration: 0.570s, episode steps:  41, steps per second:  72, episode reward: 38.675, mean reward:  0.943 [-2.541, 32.070], mean action: 7.049 [0.000, 20.000],  loss: 0.012759, mae: 0.553536, mean_q: 0.152243, mean_eps: 0.000000
 1106/5000: episode: 24, duration: 0.297s, episode steps:  24, steps per second:  81, episode reward: 41.289, mean reward:  1.720 [-2.224, 32.080], mean action: 2.167 [0.000, 15.000],  loss: 0.013894, mae: 0.535593, mean_q: 0.180717, mean_eps: 0.000000
 1149/5000: episode: 25, duration: 0.475s, episode steps:  43, steps per second:  91, episode reward: 43.668, mean reward:  1.016 [-2.030, 32.270], mean action: 7.256 [0.000, 20.000],  loss: 0.012263, mae: 0.511789, mean_q: 0.203385, mean_eps: 0.000000
 1179/5000: episode: 26, duration: 0.336s, episode steps:  30, steps per second:  89, episode reward: 38.684, mean reward:  1.289 [-2.511, 32.060], mean action: 6.500 [0.000, 20.000],  loss: 0.013024, mae: 0.513329, mean_q: 0.213850, mean_eps: 0.000000
 1205/5000: episode: 27, duration: 0.283s, episode steps:  26, steps per second:  92, episode reward: 43.307, mean reward:  1.666 [-2.011, 32.271], mean action: 6.192 [0.000, 15.000],  loss: 0.014166, mae: 0.505218, mean_q: 0.210095, mean_eps: 0.000000
 1234/5000: episode: 28, duration: 0.305s, episode steps:  29, steps per second:  95, episode reward: 38.378, mean reward:  1.323 [-2.438, 31.911], mean action: 8.000 [0.000, 20.000],  loss: 0.011830, mae: 0.480470, mean_q: 0.235734, mean_eps: 0.000000
 1274/5000: episode: 29, duration: 0.711s, episode steps:  40, steps per second:  56, episode reward: 37.915, mean reward:  0.948 [-3.000, 32.198], mean action: 5.750 [0.000, 20.000],  loss: 0.013030, mae: 0.483104, mean_q: 0.235759, mean_eps: 0.000000
 1327/5000: episode: 30, duration: 0.795s, episode steps:  53, steps per second:  67, episode reward: -32.040, mean reward: -0.605 [-32.469,  2.519], mean action: 6.000 [0.000, 20.000],  loss: 0.013753, mae: 0.463259, mean_q: 0.266360, mean_eps: 0.000000
 1392/5000: episode: 31, duration: 1.715s, episode steps:  65, steps per second:  38, episode reward: -32.030, mean reward: -0.493 [-31.783,  2.580], mean action: 11.169 [0.000, 21.000],  loss: 0.012763, mae: 0.472567, mean_q: 0.226939, mean_eps: 0.000000
 1418/5000: episode: 32, duration: 0.464s, episode steps:  26, steps per second:  56, episode reward: 37.924, mean reward:  1.459 [-2.626, 31.783], mean action: 2.385 [0.000, 15.000],  loss: 0.015707, mae: 0.478561, mean_q: 0.221889, mean_eps: 0.000000
 1447/5000: episode: 33, duration: 1.311s, episode steps:  29, steps per second:  22, episode reward: 35.619, mean reward:  1.228 [-2.657, 32.150], mean action: 5.828 [0.000, 21.000],  loss: 0.013504, mae: 0.468138, mean_q: 0.205597, mean_eps: 0.000000
 1487/5000: episode: 34, duration: 0.903s, episode steps:  40, steps per second:  44, episode reward: 40.789, mean reward:  1.020 [-2.481, 31.970], mean action: 7.050 [0.000, 20.000],  loss: 0.012563, mae: 0.467111, mean_q: 0.200360, mean_eps: 0.000000
 1504/5000: episode: 35, duration: 0.195s, episode steps:  17, steps per second:  87, episode reward: 44.163, mean reward:  2.598 [-2.374, 32.254], mean action: 2.941 [0.000, 20.000],  loss: 0.018591, mae: 0.503726, mean_q: 0.197045, mean_eps: 0.000000
 1536/5000: episode: 36, duration: 0.447s, episode steps:  32, steps per second:  72, episode reward: 38.590, mean reward:  1.206 [-2.705, 31.964], mean action: 4.469 [0.000, 20.000],  loss: 0.013889, mae: 0.441593, mean_q: 0.280829, mean_eps: 0.000000
 1571/5000: episode: 37, duration: 0.436s, episode steps:  35, steps per second:  80, episode reward: 32.560, mean reward:  0.930 [-2.753, 30.029], mean action: 9.857 [0.000, 20.000],  loss: 0.012983, mae: 0.439420, mean_q: 0.250964, mean_eps: 0.000000
 1606/5000: episode: 38, duration: 0.411s, episode steps:  35, steps per second:  85, episode reward: 35.396, mean reward:  1.011 [-3.000, 32.340], mean action: 9.943 [0.000, 20.000],  loss: 0.012086, mae: 0.444707, mean_q: 0.254088, mean_eps: 0.000000
 1639/5000: episode: 39, duration: 0.342s, episode steps:  33, steps per second:  96, episode reward: 38.244, mean reward:  1.159 [-2.508, 32.244], mean action: 3.212 [0.000, 15.000],  loss: 0.013413, mae: 0.461172, mean_q: 0.269641, mean_eps: 0.000000
 1667/5000: episode: 40, duration: 0.297s, episode steps:  28, steps per second:  94, episode reward: 32.446, mean reward:  1.159 [-3.000, 32.089], mean action: 5.643 [0.000, 20.000],  loss: 0.013769, mae: 0.445954, mean_q: 0.281761, mean_eps: 0.000000
 1691/5000: episode: 41, duration: 0.262s, episode steps:  24, steps per second:  92, episode reward: 44.220, mean reward:  1.842 [-2.128, 32.331], mean action: 5.167 [0.000, 20.000],  loss: 0.012296, mae: 0.448808, mean_q: 0.298852, mean_eps: 0.000000
 1727/5000: episode: 42, duration: 0.639s, episode steps:  36, steps per second:  56, episode reward: 35.061, mean reward:  0.974 [-2.964, 31.734], mean action: 2.500 [0.000, 12.000],  loss: 0.013609, mae: 0.440224, mean_q: 0.310621, mean_eps: 0.000000
 1756/5000: episode: 43, duration: 0.306s, episode steps:  29, steps per second:  95, episode reward: 35.766, mean reward:  1.233 [-3.000, 32.150], mean action: 5.172 [0.000, 14.000],  loss: 0.014589, mae: 0.447120, mean_q: 0.282189, mean_eps: 0.000000
 1797/5000: episode: 44, duration: 0.422s, episode steps:  41, steps per second:  97, episode reward: 32.275, mean reward:  0.787 [-3.000, 32.230], mean action: 4.854 [0.000, 15.000],  loss: 0.013261, mae: 0.427036, mean_q: 0.319470, mean_eps: 0.000000
 1832/5000: episode: 45, duration: 0.500s, episode steps:  35, steps per second:  70, episode reward: 33.000, mean reward:  0.943 [-3.000, 32.110], mean action: 5.400 [0.000, 21.000],  loss: 0.012821, mae: 0.422119, mean_q: 0.316957, mean_eps: 0.000000
 1855/5000: episode: 46, duration: 0.339s, episode steps:  23, steps per second:  68, episode reward: 44.497, mean reward:  1.935 [-2.204, 32.078], mean action: 6.087 [0.000, 20.000],  loss: 0.015466, mae: 0.456058, mean_q: 0.313143, mean_eps: 0.000000
 1887/5000: episode: 47, duration: 0.364s, episode steps:  32, steps per second:  88, episode reward: 40.600, mean reward:  1.269 [-2.544, 31.935], mean action: 2.594 [0.000, 15.000],  loss: 0.011107, mae: 0.415691, mean_q: 0.340667, mean_eps: 0.000000
 1939/5000: episode: 48, duration: 0.545s, episode steps:  52, steps per second:  95, episode reward: 42.000, mean reward:  0.808 [-2.173, 32.250], mean action: 1.212 [0.000, 12.000],  loss: 0.014156, mae: 0.443565, mean_q: 0.334556, mean_eps: 0.000000
 1977/5000: episode: 49, duration: 0.636s, episode steps:  38, steps per second:  60, episode reward: 37.988, mean reward:  1.000 [-2.414, 32.016], mean action: 2.289 [0.000, 15.000],  loss: 0.013272, mae: 0.417663, mean_q: 0.377273, mean_eps: 0.000000
 2019/5000: episode: 50, duration: 0.576s, episode steps:  42, steps per second:  73, episode reward: 35.051, mean reward:  0.835 [-3.000, 32.220], mean action: 8.429 [0.000, 20.000],  loss: 0.013776, mae: 0.439360, mean_q: 0.340509, mean_eps: 0.000000
 2044/5000: episode: 51, duration: 0.775s, episode steps:  25, steps per second:  32, episode reward: 41.337, mean reward:  1.653 [-2.377, 31.893], mean action: 6.880 [0.000, 20.000],  loss: 0.013666, mae: 0.444506, mean_q: 0.319102, mean_eps: 0.000000
 2076/5000: episode: 52, duration: 0.440s, episode steps:  32, steps per second:  73, episode reward: 38.118, mean reward:  1.191 [-2.579, 31.953], mean action: 9.406 [0.000, 15.000],  loss: 0.015355, mae: 0.429273, mean_q: 0.373959, mean_eps: 0.000000
 2119/5000: episode: 53, duration: 0.505s, episode steps:  43, steps per second:  85, episode reward: -34.920, mean reward: -0.812 [-32.073,  2.357], mean action: 6.140 [0.000, 20.000],  loss: 0.014423, mae: 0.447339, mean_q: 0.316631, mean_eps: 0.000000
 2155/5000: episode: 54, duration: 0.404s, episode steps:  36, steps per second:  89, episode reward: 44.258, mean reward:  1.229 [-2.440, 31.724], mean action: 1.889 [0.000, 14.000],  loss: 0.017412, mae: 0.412559, mean_q: 0.458180, mean_eps: 0.000000
 2188/5000: episode: 55, duration: 0.459s, episode steps:  33, steps per second:  72, episode reward: 40.882, mean reward:  1.239 [-2.469, 29.997], mean action: 5.273 [0.000, 20.000],  loss: 0.014522, mae: 0.409173, mean_q: 0.404402, mean_eps: 0.000000
 2233/5000: episode: 56, duration: 0.561s, episode steps:  45, steps per second:  80, episode reward: -32.540, mean reward: -0.723 [-29.394,  2.200], mean action: 8.156 [0.000, 20.000],  loss: 0.014622, mae: 0.427473, mean_q: 0.350925, mean_eps: 0.000000
 2265/5000: episode: 57, duration: 0.761s, episode steps:  32, steps per second:  42, episode reward: 35.873, mean reward:  1.121 [-2.304, 32.680], mean action: 5.656 [0.000, 20.000],  loss: 0.015380, mae: 0.432348, mean_q: 0.321839, mean_eps: 0.000000
 2285/5000: episode: 58, duration: 0.413s, episode steps:  20, steps per second:  48, episode reward: 41.555, mean reward:  2.078 [-2.517, 32.110], mean action: 5.250 [0.000, 15.000],  loss: 0.012124, mae: 0.430483, mean_q: 0.307845, mean_eps: 0.000000
 2326/5000: episode: 59, duration: 0.482s, episode steps:  41, steps per second:  85, episode reward: 37.477, mean reward:  0.914 [-2.669, 31.485], mean action: 5.220 [0.000, 15.000],  loss: 0.013315, mae: 0.403763, mean_q: 0.371680, mean_eps: 0.000000
 2341/5000: episode: 60, duration: 0.287s, episode steps:  15, steps per second:  52, episode reward: 46.864, mean reward:  3.124 [-0.661, 31.796], mean action: 2.667 [0.000, 14.000],  loss: 0.014712, mae: 0.399359, mean_q: 0.399432, mean_eps: 0.000000
 2370/5000: episode: 61, duration: 0.418s, episode steps:  29, steps per second:  69, episode reward: 39.000, mean reward:  1.345 [-2.601, 32.240], mean action: 4.345 [0.000, 15.000],  loss: 0.016103, mae: 0.401557, mean_q: 0.414037, mean_eps: 0.000000
 2379/5000: episode: 62, duration: 0.354s, episode steps:   9, steps per second:  25, episode reward: 47.362, mean reward:  5.262 [-0.088, 32.450], mean action: 0.000 [0.000, 0.000],  loss: 0.014427, mae: 0.442123, mean_q: 0.308763, mean_eps: 0.000000
 2404/5000: episode: 63, duration: 0.506s, episode steps:  25, steps per second:  49, episode reward: 35.703, mean reward:  1.428 [-2.222, 32.001], mean action: 5.520 [0.000, 20.000],  loss: 0.016574, mae: 0.421935, mean_q: 0.355101, mean_eps: 0.000000
 2434/5000: episode: 64, duration: 0.636s, episode steps:  30, steps per second:  47, episode reward: 41.656, mean reward:  1.389 [-2.769, 32.240], mean action: 4.567 [0.000, 20.000],  loss: 0.016310, mae: 0.412327, mean_q: 0.353028, mean_eps: 0.000000
 2460/5000: episode: 65, duration: 0.434s, episode steps:  26, steps per second:  60, episode reward: 41.438, mean reward:  1.594 [-2.418, 32.188], mean action: 4.308 [0.000, 20.000],  loss: 0.014604, mae: 0.386078, mean_q: 0.430170, mean_eps: 0.000000
 2491/5000: episode: 66, duration: 0.383s, episode steps:  31, steps per second:  81, episode reward: 42.000, mean reward:  1.355 [-2.320, 32.440], mean action: 4.000 [0.000, 20.000],  loss: 0.014349, mae: 0.430679, mean_q: 0.359516, mean_eps: 0.000000
 2510/5000: episode: 67, duration: 0.480s, episode steps:  19, steps per second:  40, episode reward: 41.582, mean reward:  2.189 [-3.000, 32.060], mean action: 6.105 [0.000, 15.000],  loss: 0.017791, mae: 0.444138, mean_q: 0.346538, mean_eps: 0.000000
 2554/5000: episode: 68, duration: 0.721s, episode steps:  44, steps per second:  61, episode reward: 35.051, mean reward:  0.797 [-2.364, 32.478], mean action: 6.795 [0.000, 20.000],  loss: 0.013854, mae: 0.427564, mean_q: 0.333483, mean_eps: 0.000000
 2576/5000: episode: 69, duration: 0.263s, episode steps:  22, steps per second:  84, episode reward: 43.561, mean reward:  1.980 [-2.031, 32.080], mean action: 5.591 [0.000, 20.000],  loss: 0.013377, mae: 0.425771, mean_q: 0.336777, mean_eps: 0.000000
 2607/5000: episode: 70, duration: 0.323s, episode steps:  31, steps per second:  96, episode reward: 38.392, mean reward:  1.238 [-2.866, 32.230], mean action: 2.968 [0.000, 12.000],  loss: 0.015197, mae: 0.430086, mean_q: 0.351103, mean_eps: 0.000000
 2644/5000: episode: 71, duration: 0.459s, episode steps:  37, steps per second:  81, episode reward: 32.504, mean reward:  0.878 [-3.000, 32.175], mean action: 2.595 [0.000, 12.000],  loss: 0.012410, mae: 0.418314, mean_q: 0.335369, mean_eps: 0.000000
 2687/5000: episode: 72, duration: 0.504s, episode steps:  43, steps per second:  85, episode reward: 32.091, mean reward:  0.746 [-3.000, 32.050], mean action: 9.233 [0.000, 20.000],  loss: 0.014665, mae: 0.429725, mean_q: 0.309775, mean_eps: 0.000000
 2716/5000: episode: 73, duration: 0.317s, episode steps:  29, steps per second:  91, episode reward: 41.447, mean reward:  1.429 [-2.353, 32.250], mean action: 4.241 [0.000, 20.000],  loss: 0.014516, mae: 0.436103, mean_q: 0.305128, mean_eps: 0.000000
 2749/5000: episode: 74, duration: 0.366s, episode steps:  33, steps per second:  90, episode reward: 44.853, mean reward:  1.359 [-2.440, 32.310], mean action: 3.000 [0.000, 15.000],  loss: 0.016474, mae: 0.422446, mean_q: 0.412991, mean_eps: 0.000000
 2779/5000: episode: 75, duration: 0.330s, episode steps:  30, steps per second:  91, episode reward: 35.111, mean reward:  1.170 [-2.741, 31.912], mean action: 4.967 [0.000, 15.000],  loss: 0.015474, mae: 0.411664, mean_q: 0.424793, mean_eps: 0.000000
 2798/5000: episode: 76, duration: 0.214s, episode steps:  19, steps per second:  89, episode reward: 44.588, mean reward:  2.347 [-2.570, 32.480], mean action: 3.263 [0.000, 20.000],  loss: 0.014734, mae: 0.423376, mean_q: 0.352319, mean_eps: 0.000000
 2877/5000: episode: 77, duration: 1.869s, episode steps:  79, steps per second:  42, episode reward: 38.029, mean reward:  0.481 [-2.173, 32.160], mean action: 5.076 [0.000, 21.000],  loss: 0.012711, mae: 0.404799, mean_q: 0.396422, mean_eps: 0.000000
 2895/5000: episode: 78, duration: 0.235s, episode steps:  18, steps per second:  77, episode reward: 41.143, mean reward:  2.286 [-2.834, 32.360], mean action: 4.222 [0.000, 20.000],  loss: 0.012168, mae: 0.376124, mean_q: 0.424860, mean_eps: 0.000000
 2909/5000: episode: 79, duration: 0.162s, episode steps:  14, steps per second:  86, episode reward: 47.201, mean reward:  3.372 [-0.040, 32.223], mean action: 2.929 [0.000, 9.000],  loss: 0.015175, mae: 0.424555, mean_q: 0.329418, mean_eps: 0.000000
 2941/5000: episode: 80, duration: 0.593s, episode steps:  32, steps per second:  54, episode reward: 35.803, mean reward:  1.119 [-3.000, 32.021], mean action: 4.188 [0.000, 14.000],  loss: 0.016543, mae: 0.432792, mean_q: 0.341648, mean_eps: 0.000000
 2963/5000: episode: 81, duration: 0.402s, episode steps:  22, steps per second:  55, episode reward: 40.336, mean reward:  1.833 [-2.751, 31.414], mean action: 2.273 [0.000, 15.000],  loss: 0.017119, mae: 0.406862, mean_q: 0.427649, mean_eps: 0.000000
 2992/5000: episode: 82, duration: 0.370s, episode steps:  29, steps per second:  78, episode reward: 46.189, mean reward:  1.593 [-0.211, 32.120], mean action: 3.069 [1.000, 14.000],  loss: 0.013146, mae: 0.406290, mean_q: 0.398080, mean_eps: 0.000000
 3021/5000: episode: 83, duration: 0.344s, episode steps:  29, steps per second:  84, episode reward: 42.000, mean reward:  1.448 [-2.775, 32.550], mean action: 5.034 [0.000, 20.000],  loss: 0.012897, mae: 0.413330, mean_q: 0.338390, mean_eps: 0.000000
 3033/5000: episode: 84, duration: 0.141s, episode steps:  12, steps per second:  85, episode reward: 45.000, mean reward:  3.750 [-2.210, 33.000], mean action: 2.750 [0.000, 15.000],  loss: 0.015192, mae: 0.405880, mean_q: 0.403958, mean_eps: 0.000000
 3069/5000: episode: 85, duration: 0.380s, episode steps:  36, steps per second:  95, episode reward: -37.970, mean reward: -1.055 [-32.292,  2.772], mean action: 7.417 [0.000, 20.000],  loss: 0.015851, mae: 0.415455, mean_q: 0.424482, mean_eps: 0.000000
 3103/5000: episode: 86, duration: 0.364s, episode steps:  34, steps per second:  93, episode reward: 35.559, mean reward:  1.046 [-2.900, 31.875], mean action: 4.765 [0.000, 20.000],  loss: 0.015038, mae: 0.417533, mean_q: 0.411667, mean_eps: 0.000000
 3156/5000: episode: 87, duration: 0.555s, episode steps:  53, steps per second:  95, episode reward: 32.938, mean reward:  0.621 [-2.463, 32.180], mean action: 3.811 [0.000, 15.000],  loss: 0.016249, mae: 0.429598, mean_q: 0.387307, mean_eps: 0.000000
 3183/5000: episode: 88, duration: 0.291s, episode steps:  27, steps per second:  93, episode reward: 41.143, mean reward:  1.524 [-2.111, 31.548], mean action: 2.926 [0.000, 15.000],  loss: 0.015815, mae: 0.414959, mean_q: 0.427051, mean_eps: 0.000000
 3226/5000: episode: 89, duration: 0.528s, episode steps:  43, steps per second:  81, episode reward: 38.354, mean reward:  0.892 [-3.000, 31.981], mean action: 5.233 [0.000, 20.000],  loss: 0.014756, mae: 0.433500, mean_q: 0.334933, mean_eps: 0.000000
 3263/5000: episode: 90, duration: 0.542s, episode steps:  37, steps per second:  68, episode reward: 35.324, mean reward:  0.955 [-3.000, 31.544], mean action: 6.459 [0.000, 20.000],  loss: 0.016303, mae: 0.433200, mean_q: 0.372521, mean_eps: 0.000000
 3289/5000: episode: 91, duration: 0.628s, episode steps:  26, steps per second:  41, episode reward: 38.458, mean reward:  1.479 [-2.903, 31.840], mean action: 4.423 [0.000, 19.000],  loss: 0.014041, mae: 0.431516, mean_q: 0.338374, mean_eps: 0.000000
 3307/5000: episode: 92, duration: 0.226s, episode steps:  18, steps per second:  80, episode reward: 41.989, mean reward:  2.333 [-2.071, 29.140], mean action: 4.389 [0.000, 14.000],  loss: 0.013416, mae: 0.423829, mean_q: 0.339977, mean_eps: 0.000000
 3338/5000: episode: 93, duration: 0.539s, episode steps:  31, steps per second:  58, episode reward: 40.391, mean reward:  1.303 [-3.000, 32.250], mean action: 7.323 [0.000, 20.000],  loss: 0.015406, mae: 0.434754, mean_q: 0.344397, mean_eps: 0.000000
 3379/5000: episode: 94, duration: 0.468s, episode steps:  41, steps per second:  88, episode reward: -34.620, mean reward: -0.844 [-32.617,  2.830], mean action: 6.829 [0.000, 20.000],  loss: 0.012271, mae: 0.396027, mean_q: 0.391771, mean_eps: 0.000000
 3394/5000: episode: 95, duration: 0.197s, episode steps:  15, steps per second:  76, episode reward: 47.184, mean reward:  3.146 [-0.075, 32.110], mean action: 3.333 [0.000, 20.000],  loss: 0.012549, mae: 0.395092, mean_q: 0.421100, mean_eps: 0.000000
 3421/5000: episode: 96, duration: 0.329s, episode steps:  27, steps per second:  82, episode reward: 37.352, mean reward:  1.383 [-2.613, 32.177], mean action: 4.481 [0.000, 20.000],  loss: 0.015051, mae: 0.406535, mean_q: 0.400346, mean_eps: 0.000000
 3446/5000: episode: 97, duration: 0.280s, episode steps:  25, steps per second:  89, episode reward: 38.407, mean reward:  1.536 [-3.000, 31.792], mean action: 3.280 [0.000, 14.000],  loss: 0.014271, mae: 0.421359, mean_q: 0.363431, mean_eps: 0.000000
 3473/5000: episode: 98, duration: 0.426s, episode steps:  27, steps per second:  63, episode reward: 38.529, mean reward:  1.427 [-2.599, 31.974], mean action: 4.148 [0.000, 15.000],  loss: 0.017778, mae: 0.414238, mean_q: 0.436047, mean_eps: 0.000000
 3518/5000: episode: 99, duration: 0.551s, episode steps:  45, steps per second:  82, episode reward: -40.100, mean reward: -0.891 [-32.461,  2.156], mean action: 10.978 [0.000, 21.000],  loss: 0.014724, mae: 0.396640, mean_q: 0.444350, mean_eps: 0.000000
 3537/5000: episode: 100, duration: 0.274s, episode steps:  19, steps per second:  69, episode reward: 42.000, mean reward:  2.211 [-3.000, 32.030], mean action: 2.789 [0.000, 15.000],  loss: 0.016229, mae: 0.412290, mean_q: 0.413831, mean_eps: 0.000000
 3576/5000: episode: 101, duration: 0.689s, episode steps:  39, steps per second:  57, episode reward: 32.429, mean reward:  0.832 [-2.939, 32.221], mean action: 3.538 [0.000, 15.000],  loss: 0.016452, mae: 0.416369, mean_q: 0.358623, mean_eps: 0.000000
 3608/5000: episode: 102, duration: 0.396s, episode steps:  32, steps per second:  81, episode reward: 43.701, mean reward:  1.366 [-2.015, 32.010], mean action: 3.281 [0.000, 13.000],  loss: 0.018516, mae: 0.417161, mean_q: 0.394487, mean_eps: 0.000000
 3638/5000: episode: 103, duration: 0.470s, episode steps:  30, steps per second:  64, episode reward: 41.122, mean reward:  1.371 [-2.819, 32.360], mean action: 4.233 [0.000, 19.000],  loss: 0.013529, mae: 0.386640, mean_q: 0.407158, mean_eps: 0.000000
 3667/5000: episode: 104, duration: 0.489s, episode steps:  29, steps per second:  59, episode reward: 44.346, mean reward:  1.529 [-2.124, 32.170], mean action: 3.241 [0.000, 13.000],  loss: 0.015488, mae: 0.419258, mean_q: 0.329930, mean_eps: 0.000000
 3698/5000: episode: 105, duration: 0.423s, episode steps:  31, steps per second:  73, episode reward: 44.940, mean reward:  1.450 [-2.237, 32.020], mean action: 2.613 [0.000, 15.000],  loss: 0.017191, mae: 0.408634, mean_q: 0.388783, mean_eps: 0.000000
 3739/5000: episode: 106, duration: 0.478s, episode steps:  41, steps per second:  86, episode reward: 36.000, mean reward:  0.878 [-3.000, 32.170], mean action: 5.073 [0.000, 20.000],  loss: 0.014601, mae: 0.399362, mean_q: 0.365318, mean_eps: 0.000000
 3769/5000: episode: 107, duration: 0.583s, episode steps:  30, steps per second:  51, episode reward: 39.000, mean reward:  1.300 [-3.000, 32.120], mean action: 4.367 [0.000, 15.000],  loss: 0.013183, mae: 0.386266, mean_q: 0.403718, mean_eps: 0.000000
 3796/5000: episode: 108, duration: 0.467s, episode steps:  27, steps per second:  58, episode reward: 38.036, mean reward:  1.409 [-3.000, 31.715], mean action: 3.444 [0.000, 15.000],  loss: 0.016962, mae: 0.406445, mean_q: 0.391631, mean_eps: 0.000000
 3826/5000: episode: 109, duration: 0.467s, episode steps:  30, steps per second:  64, episode reward: 41.149, mean reward:  1.372 [-3.000, 31.479], mean action: 2.467 [0.000, 15.000],  loss: 0.015590, mae: 0.409477, mean_q: 0.386808, mean_eps: 0.000000
 3842/5000: episode: 110, duration: 0.199s, episode steps:  16, steps per second:  80, episode reward: 47.132, mean reward:  2.946 [-0.114, 32.230], mean action: 2.375 [1.000, 3.000],  loss: 0.011608, mae: 0.392352, mean_q: 0.412028, mean_eps: 0.000000
 3872/5000: episode: 111, duration: 0.555s, episode steps:  30, steps per second:  54, episode reward: 38.934, mean reward:  1.298 [-2.871, 32.220], mean action: 3.133 [0.000, 14.000],  loss: 0.018393, mae: 0.421850, mean_q: 0.401400, mean_eps: 0.000000
 3900/5000: episode: 112, duration: 0.468s, episode steps:  28, steps per second:  60, episode reward: 41.024, mean reward:  1.465 [-2.088, 32.390], mean action: 5.571 [0.000, 17.000],  loss: 0.016810, mae: 0.426161, mean_q: 0.373580, mean_eps: 0.000000
 3927/5000: episode: 113, duration: 0.314s, episode steps:  27, steps per second:  86, episode reward: 41.574, mean reward:  1.540 [-2.160, 32.390], mean action: 4.963 [0.000, 20.000],  loss: 0.012774, mae: 0.391883, mean_q: 0.404875, mean_eps: 0.000000
 3952/5000: episode: 114, duration: 0.401s, episode steps:  25, steps per second:  62, episode reward: 41.461, mean reward:  1.658 [-2.154, 32.210], mean action: 3.200 [0.000, 15.000],  loss: 0.016180, mae: 0.405225, mean_q: 0.400398, mean_eps: 0.000000
 3983/5000: episode: 115, duration: 0.443s, episode steps:  31, steps per second:  70, episode reward: 32.987, mean reward:  1.064 [-3.000, 32.354], mean action: 4.645 [0.000, 21.000],  loss: 0.019019, mae: 0.409677, mean_q: 0.453334, mean_eps: 0.000000
 4020/5000: episode: 116, duration: 0.781s, episode steps:  37, steps per second:  47, episode reward: 38.440, mean reward:  1.039 [-2.329, 32.810], mean action: 3.405 [0.000, 20.000],  loss: 0.014249, mae: 0.403809, mean_q: 0.386618, mean_eps: 0.000000
 4064/5000: episode: 117, duration: 0.719s, episode steps:  44, steps per second:  61, episode reward: 32.788, mean reward:  0.745 [-3.000, 32.177], mean action: 7.341 [0.000, 21.000],  loss: 0.017082, mae: 0.413594, mean_q: 0.394643, mean_eps: 0.000000
 4085/5000: episode: 118, duration: 0.264s, episode steps:  21, steps per second:  80, episode reward: 39.000, mean reward:  1.857 [-2.939, 32.420], mean action: 4.714 [3.000, 15.000],  loss: 0.016931, mae: 0.405241, mean_q: 0.452633, mean_eps: 0.000000
 4117/5000: episode: 119, duration: 0.477s, episode steps:  32, steps per second:  67, episode reward: 44.543, mean reward:  1.392 [-2.176, 32.520], mean action: 2.375 [0.000, 12.000],  loss: 0.013625, mae: 0.408440, mean_q: 0.366827, mean_eps: 0.000000
 4153/5000: episode: 120, duration: 0.451s, episode steps:  36, steps per second:  80, episode reward: 45.368, mean reward:  1.260 [-0.409, 32.250], mean action: 4.167 [0.000, 20.000],  loss: 0.016670, mae: 0.414849, mean_q: 0.367094, mean_eps: 0.000000
 4186/5000: episode: 121, duration: 0.410s, episode steps:  33, steps per second:  81, episode reward: 45.000, mean reward:  1.364 [-2.506, 32.300], mean action: 4.212 [0.000, 20.000],  loss: 0.015448, mae: 0.393701, mean_q: 0.435364, mean_eps: 0.000000
 4214/5000: episode: 122, duration: 0.305s, episode steps:  28, steps per second:  92, episode reward: 41.531, mean reward:  1.483 [-3.000, 32.360], mean action: 2.786 [0.000, 12.000],  loss: 0.016318, mae: 0.388963, mean_q: 0.462927, mean_eps: 0.000000
 4236/5000: episode: 123, duration: 0.245s, episode steps:  22, steps per second:  90, episode reward: 44.367, mean reward:  2.017 [-2.291, 32.460], mean action: 2.864 [0.000, 12.000],  loss: 0.014592, mae: 0.387852, mean_q: 0.394424, mean_eps: 0.000000
 4264/5000: episode: 124, duration: 0.308s, episode steps:  28, steps per second:  91, episode reward: 41.708, mean reward:  1.490 [-3.000, 32.190], mean action: 3.536 [0.000, 15.000],  loss: 0.016518, mae: 0.409296, mean_q: 0.387773, mean_eps: 0.000000
 4303/5000: episode: 125, duration: 0.429s, episode steps:  39, steps per second:  91, episode reward: -33.000, mean reward: -0.846 [-33.140,  2.401], mean action: 8.333 [0.000, 14.000],  loss: 0.012503, mae: 0.396705, mean_q: 0.393005, mean_eps: 0.000000
 4336/5000: episode: 126, duration: 0.392s, episode steps:  33, steps per second:  84, episode reward: 35.630, mean reward:  1.080 [-3.000, 32.410], mean action: 4.212 [1.000, 15.000],  loss: 0.016187, mae: 0.406429, mean_q: 0.427250, mean_eps: 0.000000
 4365/5000: episode: 127, duration: 0.378s, episode steps:  29, steps per second:  77, episode reward: 41.779, mean reward:  1.441 [-2.269, 31.879], mean action: 3.276 [0.000, 20.000],  loss: 0.015244, mae: 0.393643, mean_q: 0.433617, mean_eps: 0.000000
 4400/5000: episode: 128, duration: 0.765s, episode steps:  35, steps per second:  46, episode reward: 43.819, mean reward:  1.252 [-2.341, 32.150], mean action: 4.057 [0.000, 20.000],  loss: 0.016366, mae: 0.414768, mean_q: 0.402472, mean_eps: 0.000000
 4424/5000: episode: 129, duration: 0.463s, episode steps:  24, steps per second:  52, episode reward: 37.799, mean reward:  1.575 [-2.789, 32.227], mean action: 4.958 [0.000, 21.000],  loss: 0.016339, mae: 0.396509, mean_q: 0.467961, mean_eps: 0.000000
 4448/5000: episode: 130, duration: 0.276s, episode steps:  24, steps per second:  87, episode reward: 38.804, mean reward:  1.617 [-2.350, 32.252], mean action: 2.833 [0.000, 15.000],  loss: 0.014337, mae: 0.397035, mean_q: 0.441662, mean_eps: 0.000000
 4468/5000: episode: 131, duration: 0.385s, episode steps:  20, steps per second:  52, episode reward: 40.920, mean reward:  2.046 [-3.000, 32.241], mean action: 3.350 [0.000, 13.000],  loss: 0.016326, mae: 0.403510, mean_q: 0.412724, mean_eps: 0.000000
 4493/5000: episode: 132, duration: 0.306s, episode steps:  25, steps per second:  82, episode reward: 41.134, mean reward:  1.645 [-2.313, 32.613], mean action: 2.520 [0.000, 11.000],  loss: 0.018195, mae: 0.409083, mean_q: 0.414889, mean_eps: 0.000000
 4536/5000: episode: 133, duration: 0.551s, episode steps:  43, steps per second:  78, episode reward: -37.200, mean reward: -0.865 [-30.250,  2.070], mean action: 6.558 [1.000, 15.000],  loss: 0.018516, mae: 0.397964, mean_q: 0.454096, mean_eps: 0.000000
 4560/5000: episode: 134, duration: 0.258s, episode steps:  24, steps per second:  93, episode reward: 38.867, mean reward:  1.619 [-2.801, 32.380], mean action: 3.958 [0.000, 15.000],  loss: 0.015920, mae: 0.387119, mean_q: 0.425545, mean_eps: 0.000000
 4600/5000: episode: 135, duration: 0.568s, episode steps:  40, steps per second:  70, episode reward: 37.512, mean reward:  0.938 [-2.373, 32.120], mean action: 2.925 [0.000, 12.000],  loss: 0.014196, mae: 0.392029, mean_q: 0.376718, mean_eps: 0.000000
 4622/5000: episode: 136, duration: 0.447s, episode steps:  22, steps per second:  49, episode reward: 42.000, mean reward:  1.909 [-2.126, 32.240], mean action: 3.409 [0.000, 15.000],  loss: 0.013263, mae: 0.375086, mean_q: 0.444004, mean_eps: 0.000000
 4674/5000: episode: 137, duration: 0.993s, episode steps:  52, steps per second:  52, episode reward: 35.842, mean reward:  0.689 [-2.240, 32.060], mean action: 5.000 [0.000, 20.000],  loss: 0.014467, mae: 0.401107, mean_q: 0.404225, mean_eps: 0.000000
 4702/5000: episode: 138, duration: 0.449s, episode steps:  28, steps per second:  62, episode reward: 43.256, mean reward:  1.545 [-3.000, 32.140], mean action: 2.750 [0.000, 13.000],  loss: 0.016251, mae: 0.400172, mean_q: 0.438133, mean_eps: 0.000000
 4719/5000: episode: 139, duration: 0.225s, episode steps:  17, steps per second:  75, episode reward: 47.268, mean reward:  2.780 [-0.419, 32.070], mean action: 1.588 [1.000, 3.000],  loss: 0.017171, mae: 0.405558, mean_q: 0.417184, mean_eps: 0.000000
 4744/5000: episode: 140, duration: 0.342s, episode steps:  25, steps per second:  73, episode reward: 41.584, mean reward:  1.663 [-2.442, 32.100], mean action: 3.280 [0.000, 15.000],  loss: 0.014197, mae: 0.375193, mean_q: 0.498477, mean_eps: 0.000000
 4780/5000: episode: 141, duration: 0.685s, episode steps:  36, steps per second:  53, episode reward: 38.809, mean reward:  1.078 [-2.340, 32.060], mean action: 3.861 [0.000, 14.000],  loss: 0.016366, mae: 0.402789, mean_q: 0.407631, mean_eps: 0.000000
 4801/5000: episode: 142, duration: 0.288s, episode steps:  21, steps per second:  73, episode reward: 44.333, mean reward:  2.111 [-2.547, 32.187], mean action: 4.429 [0.000, 14.000],  loss: 0.016007, mae: 0.406438, mean_q: 0.391476, mean_eps: 0.000000
 4828/5000: episode: 143, duration: 0.473s, episode steps:  27, steps per second:  57, episode reward: 35.724, mean reward:  1.323 [-2.739, 32.330], mean action: 6.815 [0.000, 21.000],  loss: 0.016912, mae: 0.412537, mean_q: 0.376068, mean_eps: 0.000000
 4864/5000: episode: 144, duration: 0.403s, episode steps:  36, steps per second:  89, episode reward: 38.743, mean reward:  1.076 [-2.097, 32.051], mean action: 3.556 [0.000, 14.000],  loss: 0.012266, mae: 0.369208, mean_q: 0.447246, mean_eps: 0.000000
 4901/5000: episode: 145, duration: 0.452s, episode steps:  37, steps per second:  82, episode reward: 34.756, mean reward:  0.939 [-2.939, 32.160], mean action: 5.135 [0.000, 21.000],  loss: 0.016983, mae: 0.396775, mean_q: 0.420890, mean_eps: 0.000000
 4917/5000: episode: 146, duration: 0.200s, episode steps:  16, steps per second:  80, episode reward: 47.615, mean reward:  2.976 [-0.194, 32.130], mean action: 1.000 [1.000, 1.000],  loss: 0.014582, mae: 0.404214, mean_q: 0.380370, mean_eps: 0.000000
 4946/5000: episode: 147, duration: 0.437s, episode steps:  29, steps per second:  66, episode reward: 40.383, mean reward:  1.393 [-2.545, 32.055], mean action: 5.172 [0.000, 20.000],  loss: 0.014945, mae: 0.395872, mean_q: 0.430385, mean_eps: 0.000000
done, took 67.574 seconds
DQN Evaluation: 134 victories out of 148 episodes
Training for 5000 steps ...
   26/5000: episode: 1, duration: 0.265s, episode steps:  26, steps per second:  98, episode reward: 39.000, mean reward:  1.500 [-2.525, 32.680], mean action: 3.154 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   50/5000: episode: 2, duration: 0.175s, episode steps:  24, steps per second: 137, episode reward: -35.410, mean reward: -1.475 [-32.190,  2.440], mean action: 6.875 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   73/5000: episode: 3, duration: 0.148s, episode steps:  23, steps per second: 155, episode reward: 38.960, mean reward:  1.694 [-2.315, 32.160], mean action: 5.957 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   94/5000: episode: 4, duration: 0.139s, episode steps:  21, steps per second: 151, episode reward: 35.272, mean reward:  1.680 [-2.478, 32.060], mean action: 7.857 [1.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  113/5000: episode: 5, duration: 0.229s, episode steps:  19, steps per second:  83, episode reward: 38.589, mean reward:  2.031 [-2.219, 32.438], mean action: 3.211 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  132/5000: episode: 6, duration: 0.140s, episode steps:  19, steps per second: 136, episode reward: 41.207, mean reward:  2.169 [-2.147, 32.293], mean action: 3.368 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  184/5000: episode: 7, duration: 0.481s, episode steps:  52, steps per second: 108, episode reward: -33.000, mean reward: -0.635 [-32.329,  2.370], mean action: 6.462 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  207/5000: episode: 8, duration: 0.193s, episode steps:  23, steps per second: 119, episode reward: -32.020, mean reward: -1.392 [-32.830,  2.303], mean action: 6.174 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  231/5000: episode: 9, duration: 0.184s, episode steps:  24, steps per second: 130, episode reward: 34.523, mean reward:  1.438 [-3.000, 32.170], mean action: 6.500 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  247/5000: episode: 10, duration: 0.105s, episode steps:  16, steps per second: 152, episode reward: 44.604, mean reward:  2.788 [-2.107, 32.061], mean action: 2.812 [1.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  272/5000: episode: 11, duration: 0.201s, episode steps:  25, steps per second: 125, episode reward: 41.222, mean reward:  1.649 [-2.303, 32.280], mean action: 4.120 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  314/5000: episode: 12, duration: 0.322s, episode steps:  42, steps per second: 130, episode reward: -32.630, mean reward: -0.777 [-32.079,  2.340], mean action: 9.190 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  339/5000: episode: 13, duration: 0.157s, episode steps:  25, steps per second: 159, episode reward: -32.590, mean reward: -1.304 [-31.774,  2.530], mean action: 5.880 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  361/5000: episode: 14, duration: 0.312s, episode steps:  22, steps per second:  70, episode reward: -35.910, mean reward: -1.632 [-32.444,  2.790], mean action: 5.182 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  381/5000: episode: 15, duration: 0.158s, episode steps:  20, steps per second: 127, episode reward: 38.020, mean reward:  1.901 [-2.700, 31.695], mean action: 3.100 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  401/5000: episode: 16, duration: 0.200s, episode steps:  20, steps per second: 100, episode reward: -32.230, mean reward: -1.611 [-32.119,  2.903], mean action: 5.850 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  465/5000: episode: 17, duration: 0.391s, episode steps:  64, steps per second: 164, episode reward: -35.700, mean reward: -0.558 [-32.092,  2.417], mean action: 14.109 [1.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  484/5000: episode: 18, duration: 0.118s, episode steps:  19, steps per second: 161, episode reward: 37.611, mean reward:  1.980 [-2.291, 32.450], mean action: 4.789 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  507/5000: episode: 19, duration: 0.141s, episode steps:  23, steps per second: 163, episode reward: 35.404, mean reward:  1.539 [-2.562, 32.528], mean action: 5.130 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  527/5000: episode: 20, duration: 0.126s, episode steps:  20, steps per second: 159, episode reward: -32.170, mean reward: -1.609 [-32.319,  3.000], mean action: 7.200 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  548/5000: episode: 21, duration: 0.161s, episode steps:  21, steps per second: 131, episode reward: 38.083, mean reward:  1.813 [-2.830, 29.858], mean action: 7.333 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  568/5000: episode: 22, duration: 0.141s, episode steps:  20, steps per second: 142, episode reward: 37.822, mean reward:  1.891 [-2.577, 31.883], mean action: 4.600 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  604/5000: episode: 23, duration: 0.216s, episode steps:  36, steps per second: 167, episode reward: 33.000, mean reward:  0.917 [-2.903, 32.160], mean action: 7.917 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  621/5000: episode: 24, duration: 0.130s, episode steps:  17, steps per second: 131, episode reward: 43.621, mean reward:  2.566 [-2.165, 31.755], mean action: 1.882 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  642/5000: episode: 25, duration: 0.169s, episode steps:  21, steps per second: 124, episode reward: 38.809, mean reward:  1.848 [-2.620, 32.040], mean action: 7.333 [1.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  665/5000: episode: 26, duration: 0.185s, episode steps:  23, steps per second: 124, episode reward: 35.121, mean reward:  1.527 [-3.000, 32.360], mean action: 4.217 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  712/5000: episode: 27, duration: 0.293s, episode steps:  47, steps per second: 161, episode reward: 37.528, mean reward:  0.798 [-2.184, 33.000], mean action: 4.489 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  730/5000: episode: 28, duration: 0.148s, episode steps:  18, steps per second: 121, episode reward: 37.579, mean reward:  2.088 [-3.000, 32.807], mean action: 3.056 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  754/5000: episode: 29, duration: 0.223s, episode steps:  24, steps per second: 107, episode reward: 41.431, mean reward:  1.726 [-2.406, 31.909], mean action: 1.958 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  775/5000: episode: 30, duration: 0.306s, episode steps:  21, steps per second:  69, episode reward: 38.429, mean reward:  1.830 [-2.475, 32.021], mean action: 6.762 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  799/5000: episode: 31, duration: 0.245s, episode steps:  24, steps per second:  98, episode reward: 32.242, mean reward:  1.343 [-3.000, 32.610], mean action: 8.417 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  823/5000: episode: 32, duration: 0.228s, episode steps:  24, steps per second: 105, episode reward: -35.610, mean reward: -1.484 [-31.866,  3.000], mean action: 8.875 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  848/5000: episode: 33, duration: 0.396s, episode steps:  25, steps per second:  63, episode reward: -35.620, mean reward: -1.425 [-32.339,  2.440], mean action: 6.680 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  874/5000: episode: 34, duration: 0.226s, episode steps:  26, steps per second: 115, episode reward: 35.690, mean reward:  1.373 [-2.338, 32.151], mean action: 4.962 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  907/5000: episode: 35, duration: 0.221s, episode steps:  33, steps per second: 149, episode reward: -35.830, mean reward: -1.086 [-32.065,  2.217], mean action: 5.909 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  926/5000: episode: 36, duration: 0.136s, episode steps:  19, steps per second: 140, episode reward: 38.435, mean reward:  2.023 [-3.000, 32.250], mean action: 4.316 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  946/5000: episode: 37, duration: 0.161s, episode steps:  20, steps per second: 124, episode reward: 33.000, mean reward:  1.650 [-2.382, 32.320], mean action: 4.900 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  972/5000: episode: 38, duration: 0.355s, episode steps:  26, steps per second:  73, episode reward: -35.540, mean reward: -1.367 [-32.452,  2.903], mean action: 7.885 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  992/5000: episode: 39, duration: 0.459s, episode steps:  20, steps per second:  44, episode reward: 32.057, mean reward:  1.603 [-3.000, 32.602], mean action: 8.000 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1017/5000: episode: 40, duration: 0.595s, episode steps:  25, steps per second:  42, episode reward: 32.952, mean reward:  1.318 [-3.000, 32.662], mean action: 3.640 [0.000, 15.000],  loss: 0.017576, mae: 0.399788, mean_q: 0.389284, mean_eps: 0.000000
 1057/5000: episode: 41, duration: 0.458s, episode steps:  40, steps per second:  87, episode reward: 35.650, mean reward:  0.891 [-2.896, 32.907], mean action: 4.575 [0.000, 19.000],  loss: 0.017241, mae: 0.419832, mean_q: 0.345247, mean_eps: 0.000000
 1086/5000: episode: 42, duration: 0.397s, episode steps:  29, steps per second:  73, episode reward: -35.510, mean reward: -1.224 [-32.072,  2.126], mean action: 5.448 [0.000, 21.000],  loss: 0.015868, mae: 0.389442, mean_q: 0.419842, mean_eps: 0.000000
 1101/5000: episode: 43, duration: 0.198s, episode steps:  15, steps per second:  76, episode reward: 41.303, mean reward:  2.754 [-2.144, 32.389], mean action: 3.067 [0.000, 11.000],  loss: 0.018291, mae: 0.409384, mean_q: 0.359821, mean_eps: 0.000000
 1133/5000: episode: 44, duration: 0.340s, episode steps:  32, steps per second:  94, episode reward: 32.521, mean reward:  1.016 [-2.373, 32.107], mean action: 4.688 [0.000, 21.000],  loss: 0.016375, mae: 0.394692, mean_q: 0.411186, mean_eps: 0.000000
 1156/5000: episode: 45, duration: 0.280s, episode steps:  23, steps per second:  82, episode reward: 35.323, mean reward:  1.536 [-3.000, 32.130], mean action: 3.348 [0.000, 11.000],  loss: 0.019682, mae: 0.414832, mean_q: 0.396686, mean_eps: 0.000000
 1185/5000: episode: 46, duration: 0.362s, episode steps:  29, steps per second:  80, episode reward: 35.160, mean reward:  1.212 [-3.000, 32.579], mean action: 8.310 [0.000, 21.000],  loss: 0.019686, mae: 0.417227, mean_q: 0.403610, mean_eps: 0.000000
 1202/5000: episode: 47, duration: 0.186s, episode steps:  17, steps per second:  91, episode reward: 38.778, mean reward:  2.281 [-2.351, 32.500], mean action: 4.412 [0.000, 15.000],  loss: 0.016479, mae: 0.396019, mean_q: 0.390344, mean_eps: 0.000000
 1224/5000: episode: 48, duration: 0.233s, episode steps:  22, steps per second:  94, episode reward: -39.000, mean reward: -1.773 [-29.949,  2.001], mean action: 7.727 [0.000, 21.000],  loss: 0.017314, mae: 0.422319, mean_q: 0.325564, mean_eps: 0.000000
 1268/5000: episode: 49, duration: 0.474s, episode steps:  44, steps per second:  93, episode reward: -35.470, mean reward: -0.806 [-31.742,  2.500], mean action: 4.955 [0.000, 21.000],  loss: 0.019031, mae: 0.418417, mean_q: 0.375784, mean_eps: 0.000000
 1292/5000: episode: 50, duration: 0.380s, episode steps:  24, steps per second:  63, episode reward: -36.000, mean reward: -1.500 [-33.000,  2.761], mean action: 4.958 [0.000, 14.000],  loss: 0.017597, mae: 0.401244, mean_q: 0.421083, mean_eps: 0.000000
 1313/5000: episode: 51, duration: 0.232s, episode steps:  21, steps per second:  91, episode reward: 35.771, mean reward:  1.703 [-2.613, 31.931], mean action: 4.619 [0.000, 20.000],  loss: 0.019072, mae: 0.428339, mean_q: 0.380663, mean_eps: 0.000000
 1349/5000: episode: 52, duration: 0.377s, episode steps:  36, steps per second:  95, episode reward: -37.590, mean reward: -1.044 [-31.530,  2.303], mean action: 5.833 [0.000, 15.000],  loss: 0.019003, mae: 0.417616, mean_q: 0.380580, mean_eps: 0.000000
 1372/5000: episode: 53, duration: 0.512s, episode steps:  23, steps per second:  45, episode reward: 37.590, mean reward:  1.634 [-2.199, 31.868], mean action: 7.043 [0.000, 14.000],  loss: 0.016658, mae: 0.392142, mean_q: 0.416667, mean_eps: 0.000000
 1399/5000: episode: 54, duration: 0.466s, episode steps:  27, steps per second:  58, episode reward: 38.736, mean reward:  1.435 [-2.561, 32.518], mean action: 4.704 [0.000, 15.000],  loss: 0.019636, mae: 0.416324, mean_q: 0.381309, mean_eps: 0.000000
 1432/5000: episode: 55, duration: 0.408s, episode steps:  33, steps per second:  81, episode reward: -39.000, mean reward: -1.182 [-32.135,  2.070], mean action: 9.576 [0.000, 20.000],  loss: 0.015495, mae: 0.410347, mean_q: 0.344800, mean_eps: 0.000000
 1448/5000: episode: 56, duration: 0.218s, episode steps:  16, steps per second:  73, episode reward: 47.035, mean reward:  2.940 [ 0.155, 32.340], mean action: 2.250 [0.000, 3.000],  loss: 0.017133, mae: 0.416948, mean_q: 0.372753, mean_eps: 0.000000
 1476/5000: episode: 57, duration: 0.497s, episode steps:  28, steps per second:  56, episode reward: -35.550, mean reward: -1.270 [-32.236,  3.000], mean action: 7.464 [3.000, 15.000],  loss: 0.016083, mae: 0.408086, mean_q: 0.386708, mean_eps: 0.000000
 1505/5000: episode: 58, duration: 0.330s, episode steps:  29, steps per second:  88, episode reward: 35.895, mean reward:  1.238 [-2.427, 32.145], mean action: 3.966 [0.000, 20.000],  loss: 0.016113, mae: 0.404801, mean_q: 0.360682, mean_eps: 0.000000
 1525/5000: episode: 59, duration: 0.244s, episode steps:  20, steps per second:  82, episode reward: 40.896, mean reward:  2.045 [-2.342, 31.982], mean action: 4.300 [1.000, 15.000],  loss: 0.013474, mae: 0.402575, mean_q: 0.347837, mean_eps: 0.000000
 1550/5000: episode: 60, duration: 0.418s, episode steps:  25, steps per second:  60, episode reward: 32.345, mean reward:  1.294 [-2.409, 32.420], mean action: 6.680 [0.000, 20.000],  loss: 0.016109, mae: 0.407691, mean_q: 0.341987, mean_eps: 0.000000
 1571/5000: episode: 61, duration: 0.616s, episode steps:  21, steps per second:  34, episode reward: 35.776, mean reward:  1.704 [-3.000, 32.938], mean action: 4.238 [0.000, 14.000],  loss: 0.021894, mae: 0.430046, mean_q: 0.380717, mean_eps: 0.000000
 1588/5000: episode: 62, duration: 0.233s, episode steps:  17, steps per second:  73, episode reward: 40.661, mean reward:  2.392 [-2.443, 32.680], mean action: 3.471 [0.000, 20.000],  loss: 0.018637, mae: 0.401285, mean_q: 0.472619, mean_eps: 0.000000
 1613/5000: episode: 63, duration: 0.283s, episode steps:  25, steps per second:  88, episode reward: 35.120, mean reward:  1.405 [-2.900, 32.700], mean action: 3.920 [0.000, 18.000],  loss: 0.015286, mae: 0.393113, mean_q: 0.436234, mean_eps: 0.000000
 1641/5000: episode: 64, duration: 0.317s, episode steps:  28, steps per second:  88, episode reward: -35.280, mean reward: -1.260 [-31.948,  2.340], mean action: 7.250 [0.000, 18.000],  loss: 0.020354, mae: 0.427312, mean_q: 0.383884, mean_eps: 0.000000
 1664/5000: episode: 65, duration: 0.276s, episode steps:  23, steps per second:  83, episode reward: 35.489, mean reward:  1.543 [-2.701, 32.081], mean action: 6.304 [0.000, 20.000],  loss: 0.017774, mae: 0.421677, mean_q: 0.383105, mean_eps: 0.000000
 1682/5000: episode: 66, duration: 0.200s, episode steps:  18, steps per second:  90, episode reward: 38.164, mean reward:  2.120 [-2.419, 32.130], mean action: 5.389 [0.000, 15.000],  loss: 0.019026, mae: 0.421302, mean_q: 0.418892, mean_eps: 0.000000
 1711/5000: episode: 67, duration: 0.488s, episode steps:  29, steps per second:  59, episode reward: 40.553, mean reward:  1.398 [-2.535, 32.340], mean action: 3.759 [0.000, 12.000],  loss: 0.021025, mae: 0.441376, mean_q: 0.369715, mean_eps: 0.000000
 1733/5000: episode: 68, duration: 0.537s, episode steps:  22, steps per second:  41, episode reward: 35.368, mean reward:  1.608 [-2.903, 32.250], mean action: 4.864 [0.000, 15.000],  loss: 0.017615, mae: 0.433819, mean_q: 0.304847, mean_eps: 0.000000
 1760/5000: episode: 69, duration: 0.430s, episode steps:  27, steps per second:  63, episode reward: 32.014, mean reward:  1.186 [-2.943, 31.844], mean action: 5.333 [0.000, 15.000],  loss: 0.016800, mae: 0.427863, mean_q: 0.307989, mean_eps: 0.000000
 1783/5000: episode: 70, duration: 0.318s, episode steps:  23, steps per second:  72, episode reward: -32.420, mean reward: -1.410 [-32.220,  2.934], mean action: 5.217 [0.000, 15.000],  loss: 0.015920, mae: 0.401531, mean_q: 0.392605, mean_eps: 0.000000
 1806/5000: episode: 71, duration: 0.366s, episode steps:  23, steps per second:  63, episode reward: 33.000, mean reward:  1.435 [-3.000, 32.810], mean action: 7.174 [0.000, 15.000],  loss: 0.017819, mae: 0.402735, mean_q: 0.444183, mean_eps: 0.000000
 1836/5000: episode: 72, duration: 0.335s, episode steps:  30, steps per second:  89, episode reward: -35.100, mean reward: -1.170 [-31.729,  2.140], mean action: 5.967 [0.000, 15.000],  loss: 0.016686, mae: 0.412117, mean_q: 0.393949, mean_eps: 0.000000
 1860/5000: episode: 73, duration: 0.259s, episode steps:  24, steps per second:  93, episode reward: 38.724, mean reward:  1.614 [-2.520, 31.990], mean action: 2.875 [0.000, 12.000],  loss: 0.019295, mae: 0.429842, mean_q: 0.391748, mean_eps: 0.000000
 1882/5000: episode: 74, duration: 0.244s, episode steps:  22, steps per second:  90, episode reward: 38.006, mean reward:  1.728 [-2.532, 32.313], mean action: 4.955 [0.000, 20.000],  loss: 0.017142, mae: 0.428369, mean_q: 0.390299, mean_eps: 0.000000
 1908/5000: episode: 75, duration: 0.326s, episode steps:  26, steps per second:  80, episode reward: 36.000, mean reward:  1.385 [-2.201, 29.701], mean action: 5.538 [0.000, 19.000],  loss: 0.015953, mae: 0.423626, mean_q: 0.390119, mean_eps: 0.000000
 1928/5000: episode: 76, duration: 0.222s, episode steps:  20, steps per second:  90, episode reward: 38.414, mean reward:  1.921 [-2.370, 32.765], mean action: 3.600 [0.000, 12.000],  loss: 0.018740, mae: 0.434386, mean_q: 0.417903, mean_eps: 0.000000
 1982/5000: episode: 77, duration: 0.548s, episode steps:  54, steps per second:  99, episode reward: 41.236, mean reward:  0.764 [-3.000, 31.951], mean action: 6.778 [0.000, 15.000],  loss: 0.017540, mae: 0.432144, mean_q: 0.409004, mean_eps: 0.000000
 2030/5000: episode: 78, duration: 0.543s, episode steps:  48, steps per second:  88, episode reward: -38.790, mean reward: -0.808 [-32.432,  2.280], mean action: 6.104 [0.000, 15.000],  loss: 0.016496, mae: 0.424499, mean_q: 0.395884, mean_eps: 0.000000
 2050/5000: episode: 79, duration: 0.228s, episode steps:  20, steps per second:  88, episode reward: 40.631, mean reward:  2.032 [-2.143, 32.051], mean action: 5.750 [0.000, 15.000],  loss: 0.020235, mae: 0.435354, mean_q: 0.396539, mean_eps: 0.000000
 2066/5000: episode: 80, duration: 0.184s, episode steps:  16, steps per second:  87, episode reward: 41.113, mean reward:  2.570 [-2.485, 32.500], mean action: 2.875 [0.000, 15.000],  loss: 0.017666, mae: 0.420141, mean_q: 0.439677, mean_eps: 0.000000
 2092/5000: episode: 81, duration: 0.283s, episode steps:  26, steps per second:  92, episode reward: 35.481, mean reward:  1.365 [-2.444, 32.100], mean action: 8.192 [1.000, 20.000],  loss: 0.016705, mae: 0.418147, mean_q: 0.408638, mean_eps: 0.000000
 2110/5000: episode: 82, duration: 0.211s, episode steps:  18, steps per second:  85, episode reward: 41.144, mean reward:  2.286 [-2.483, 32.268], mean action: 4.556 [0.000, 15.000],  loss: 0.019962, mae: 0.440855, mean_q: 0.382438, mean_eps: 0.000000
 2131/5000: episode: 83, duration: 0.229s, episode steps:  21, steps per second:  92, episode reward: 38.075, mean reward:  1.813 [-3.000, 32.280], mean action: 3.381 [0.000, 15.000],  loss: 0.019175, mae: 0.421389, mean_q: 0.452057, mean_eps: 0.000000
 2152/5000: episode: 84, duration: 0.226s, episode steps:  21, steps per second:  93, episode reward: 35.730, mean reward:  1.701 [-2.768, 31.941], mean action: 7.381 [0.000, 20.000],  loss: 0.017011, mae: 0.414264, mean_q: 0.424258, mean_eps: 0.000000
 2178/5000: episode: 85, duration: 0.280s, episode steps:  26, steps per second:  93, episode reward: -32.310, mean reward: -1.243 [-33.000,  2.393], mean action: 5.923 [0.000, 15.000],  loss: 0.017682, mae: 0.413771, mean_q: 0.395183, mean_eps: 0.000000
 2197/5000: episode: 86, duration: 0.221s, episode steps:  19, steps per second:  86, episode reward: 41.181, mean reward:  2.167 [-2.300, 33.000], mean action: 4.526 [0.000, 15.000],  loss: 0.019827, mae: 0.446587, mean_q: 0.370697, mean_eps: 0.000000
 2223/5000: episode: 87, duration: 0.278s, episode steps:  26, steps per second:  94, episode reward: -38.830, mean reward: -1.493 [-32.452,  2.460], mean action: 6.692 [0.000, 15.000],  loss: 0.018374, mae: 0.437973, mean_q: 0.375920, mean_eps: 0.000000
 2248/5000: episode: 88, duration: 0.274s, episode steps:  25, steps per second:  91, episode reward: -44.580, mean reward: -1.783 [-32.094,  1.754], mean action: 7.920 [0.000, 20.000],  loss: 0.021910, mae: 0.440237, mean_q: 0.423438, mean_eps: 0.000000
 2280/5000: episode: 89, duration: 0.353s, episode steps:  32, steps per second:  91, episode reward: -32.260, mean reward: -1.008 [-31.851,  3.000], mean action: 11.281 [0.000, 19.000],  loss: 0.017465, mae: 0.414257, mean_q: 0.399324, mean_eps: 0.000000
 2294/5000: episode: 90, duration: 0.160s, episode steps:  14, steps per second:  88, episode reward: 38.215, mean reward:  2.730 [-3.000, 31.975], mean action: 5.143 [1.000, 15.000],  loss: 0.020064, mae: 0.413130, mean_q: 0.468889, mean_eps: 0.000000
 2338/5000: episode: 91, duration: 0.987s, episode steps:  44, steps per second:  45, episode reward: 32.362, mean reward:  0.736 [-2.888, 32.120], mean action: 10.273 [0.000, 21.000],  loss: 0.018257, mae: 0.423205, mean_q: 0.402228, mean_eps: 0.000000
 2368/5000: episode: 92, duration: 0.527s, episode steps:  30, steps per second:  57, episode reward: 32.068, mean reward:  1.069 [-2.232, 30.522], mean action: 5.500 [0.000, 21.000],  loss: 0.012658, mae: 0.400427, mean_q: 0.387354, mean_eps: 0.000000
 2397/5000: episode: 93, duration: 0.478s, episode steps:  29, steps per second:  61, episode reward: 41.150, mean reward:  1.419 [-2.032, 32.862], mean action: 11.862 [0.000, 20.000],  loss: 0.015643, mae: 0.419721, mean_q: 0.339307, mean_eps: 0.000000
 2421/5000: episode: 94, duration: 0.512s, episode steps:  24, steps per second:  47, episode reward: 37.264, mean reward:  1.553 [-3.000, 32.810], mean action: 6.875 [0.000, 14.000],  loss: 0.016238, mae: 0.421726, mean_q: 0.365181, mean_eps: 0.000000
 2446/5000: episode: 95, duration: 0.493s, episode steps:  25, steps per second:  51, episode reward: -35.290, mean reward: -1.412 [-31.933,  2.342], mean action: 5.400 [0.000, 15.000],  loss: 0.018264, mae: 0.441236, mean_q: 0.314152, mean_eps: 0.000000
 2465/5000: episode: 96, duration: 0.219s, episode steps:  19, steps per second:  87, episode reward: 42.897, mean reward:  2.258 [-2.682, 32.490], mean action: 8.895 [0.000, 20.000],  loss: 0.017217, mae: 0.422458, mean_q: 0.378828, mean_eps: 0.000000
 2492/5000: episode: 97, duration: 0.318s, episode steps:  27, steps per second:  85, episode reward: 34.763, mean reward:  1.288 [-2.390, 32.220], mean action: 6.407 [1.000, 20.000],  loss: 0.018168, mae: 0.414089, mean_q: 0.432608, mean_eps: 0.000000
 2510/5000: episode: 98, duration: 0.215s, episode steps:  18, steps per second:  84, episode reward: 36.000, mean reward:  2.000 [-2.425, 32.440], mean action: 6.611 [3.000, 15.000],  loss: 0.017816, mae: 0.420971, mean_q: 0.393802, mean_eps: 0.000000
 2535/5000: episode: 99, duration: 0.332s, episode steps:  25, steps per second:  75, episode reward: 32.671, mean reward:  1.307 [-2.942, 31.821], mean action: 4.480 [0.000, 14.000],  loss: 0.017083, mae: 0.416127, mean_q: 0.423362, mean_eps: 0.000000
 2564/5000: episode: 100, duration: 0.412s, episode steps:  29, steps per second:  70, episode reward: 38.423, mean reward:  1.325 [-3.000, 32.210], mean action: 3.207 [0.000, 14.000],  loss: 0.018731, mae: 0.433824, mean_q: 0.439368, mean_eps: 0.000000
 2599/5000: episode: 101, duration: 0.400s, episode steps:  35, steps per second:  88, episode reward: -32.630, mean reward: -0.932 [-31.866,  2.562], mean action: 6.371 [0.000, 20.000],  loss: 0.016372, mae: 0.403806, mean_q: 0.444269, mean_eps: 0.000000
 2622/5000: episode: 102, duration: 0.278s, episode steps:  23, steps per second:  83, episode reward: 37.490, mean reward:  1.630 [-2.481, 32.140], mean action: 4.087 [0.000, 12.000],  loss: 0.017275, mae: 0.409069, mean_q: 0.432870, mean_eps: 0.000000
 2645/5000: episode: 103, duration: 0.256s, episode steps:  23, steps per second:  90, episode reward: -38.480, mean reward: -1.673 [-31.802,  2.361], mean action: 3.870 [0.000, 20.000],  loss: 0.017522, mae: 0.437714, mean_q: 0.347405, mean_eps: 0.000000
 2669/5000: episode: 104, duration: 0.583s, episode steps:  24, steps per second:  41, episode reward: 35.122, mean reward:  1.463 [-3.000, 32.175], mean action: 5.500 [0.000, 21.000],  loss: 0.016807, mae: 0.443844, mean_q: 0.329221, mean_eps: 0.000000
 2691/5000: episode: 105, duration: 0.359s, episode steps:  22, steps per second:  61, episode reward: 33.000, mean reward:  1.500 [-3.000, 32.250], mean action: 6.409 [0.000, 20.000],  loss: 0.017748, mae: 0.427602, mean_q: 0.394011, mean_eps: 0.000000
 2708/5000: episode: 106, duration: 0.877s, episode steps:  17, steps per second:  19, episode reward: 37.780, mean reward:  2.222 [-3.000, 32.810], mean action: 6.882 [0.000, 20.000],  loss: 0.012726, mae: 0.389572, mean_q: 0.451479, mean_eps: 0.000000
 2736/5000: episode: 107, duration: 1.665s, episode steps:  28, steps per second:  17, episode reward: 37.746, mean reward:  1.348 [-3.000, 32.362], mean action: 6.643 [1.000, 21.000],  loss: 0.016575, mae: 0.423210, mean_q: 0.371619, mean_eps: 0.000000
 2758/5000: episode: 108, duration: 0.709s, episode steps:  22, steps per second:  31, episode reward: -32.500, mean reward: -1.477 [-32.500,  3.000], mean action: 6.909 [0.000, 15.000],  loss: 0.022047, mae: 0.458677, mean_q: 0.351123, mean_eps: 0.000000
 2804/5000: episode: 109, duration: 0.710s, episode steps:  46, steps per second:  65, episode reward: -38.630, mean reward: -0.840 [-32.192,  2.510], mean action: 7.413 [0.000, 21.000],  loss: 0.017845, mae: 0.432435, mean_q: 0.416966, mean_eps: 0.000000
 2827/5000: episode: 110, duration: 0.309s, episode steps:  23, steps per second:  74, episode reward: -36.000, mean reward: -1.565 [-32.196,  2.630], mean action: 11.783 [0.000, 20.000],  loss: 0.019664, mae: 0.438254, mean_q: 0.403418, mean_eps: 0.000000
 2854/5000: episode: 111, duration: 0.396s, episode steps:  27, steps per second:  68, episode reward: -39.000, mean reward: -1.444 [-32.126,  2.310], mean action: 4.481 [0.000, 11.000],  loss: 0.017913, mae: 0.444122, mean_q: 0.366670, mean_eps: 0.000000
 2889/5000: episode: 112, duration: 0.459s, episode steps:  35, steps per second:  76, episode reward: 32.301, mean reward:  0.923 [-2.433, 32.033], mean action: 7.200 [0.000, 15.000],  loss: 0.018341, mae: 0.446255, mean_q: 0.378440, mean_eps: 0.000000
 2917/5000: episode: 113, duration: 0.351s, episode steps:  28, steps per second:  80, episode reward: -39.000, mean reward: -1.393 [-29.994,  2.612], mean action: 8.500 [1.000, 15.000],  loss: 0.019000, mae: 0.428469, mean_q: 0.425105, mean_eps: 0.000000
 2939/5000: episode: 114, duration: 0.299s, episode steps:  22, steps per second:  74, episode reward: 35.596, mean reward:  1.618 [-3.000, 31.846], mean action: 4.364 [0.000, 14.000],  loss: 0.017577, mae: 0.426784, mean_q: 0.455968, mean_eps: 0.000000
 2963/5000: episode: 115, duration: 0.294s, episode steps:  24, steps per second:  82, episode reward: 32.582, mean reward:  1.358 [-3.000, 31.852], mean action: 3.667 [0.000, 15.000],  loss: 0.016072, mae: 0.422177, mean_q: 0.424430, mean_eps: 0.000000
 2988/5000: episode: 116, duration: 0.317s, episode steps:  25, steps per second:  79, episode reward: 35.367, mean reward:  1.415 [-2.448, 31.813], mean action: 4.600 [0.000, 18.000],  loss: 0.018500, mae: 0.458452, mean_q: 0.361808, mean_eps: 0.000000
 3015/5000: episode: 117, duration: 0.341s, episode steps:  27, steps per second:  79, episode reward: 38.190, mean reward:  1.414 [-2.399, 32.090], mean action: 3.630 [0.000, 19.000],  loss: 0.018613, mae: 0.450733, mean_q: 0.377249, mean_eps: 0.000000
 3040/5000: episode: 118, duration: 0.341s, episode steps:  25, steps per second:  73, episode reward: 38.923, mean reward:  1.557 [-3.000, 32.310], mean action: 4.640 [0.000, 15.000],  loss: 0.018829, mae: 0.439668, mean_q: 0.382851, mean_eps: 0.000000
 3074/5000: episode: 119, duration: 1.006s, episode steps:  34, steps per second:  34, episode reward: 32.584, mean reward:  0.958 [-2.843, 32.060], mean action: 8.559 [0.000, 20.000],  loss: 0.014683, mae: 0.429403, mean_q: 0.355853, mean_eps: 0.000000
 3121/5000: episode: 120, duration: 1.382s, episode steps:  47, steps per second:  34, episode reward: 37.121, mean reward:  0.790 [-2.378, 32.680], mean action: 7.532 [0.000, 20.000],  loss: 0.018630, mae: 0.449198, mean_q: 0.374967, mean_eps: 0.000000
 3135/5000: episode: 121, duration: 0.238s, episode steps:  14, steps per second:  59, episode reward: -41.470, mean reward: -2.962 [-31.987,  2.720], mean action: 10.143 [1.000, 20.000],  loss: 0.015627, mae: 0.436372, mean_q: 0.342720, mean_eps: 0.000000
 3164/5000: episode: 122, duration: 0.466s, episode steps:  29, steps per second:  62, episode reward: -30.000, mean reward: -1.034 [-30.101,  2.530], mean action: 8.966 [0.000, 20.000],  loss: 0.024104, mae: 0.472987, mean_q: 0.371338, mean_eps: 0.000000
 3181/5000: episode: 123, duration: 0.280s, episode steps:  17, steps per second:  61, episode reward: 41.202, mean reward:  2.424 [-3.000, 32.390], mean action: 4.353 [0.000, 15.000],  loss: 0.018819, mae: 0.423251, mean_q: 0.443192, mean_eps: 0.000000
 3207/5000: episode: 124, duration: 0.531s, episode steps:  26, steps per second:  49, episode reward: 32.587, mean reward:  1.253 [-3.000, 32.253], mean action: 6.808 [0.000, 15.000],  loss: 0.019765, mae: 0.444027, mean_q: 0.426483, mean_eps: 0.000000
 3239/5000: episode: 125, duration: 0.539s, episode steps:  32, steps per second:  59, episode reward: 32.747, mean reward:  1.023 [-2.502, 31.767], mean action: 4.625 [0.000, 15.000],  loss: 0.020333, mae: 0.444711, mean_q: 0.406381, mean_eps: 0.000000
 3256/5000: episode: 126, duration: 0.296s, episode steps:  17, steps per second:  57, episode reward: 35.622, mean reward:  2.095 [-3.000, 31.899], mean action: 6.882 [0.000, 15.000],  loss: 0.018872, mae: 0.448899, mean_q: 0.370490, mean_eps: 0.000000
 3284/5000: episode: 127, duration: 0.613s, episode steps:  28, steps per second:  46, episode reward: 37.983, mean reward:  1.357 [-2.602, 31.522], mean action: 3.821 [0.000, 19.000],  loss: 0.017393, mae: 0.454782, mean_q: 0.328759, mean_eps: 0.000000
 3299/5000: episode: 128, duration: 0.289s, episode steps:  15, steps per second:  52, episode reward: 45.000, mean reward:  3.000 [-0.050, 30.502], mean action: 3.000 [0.000, 8.000],  loss: 0.017326, mae: 0.438544, mean_q: 0.353330, mean_eps: 0.000000
 3319/5000: episode: 129, duration: 0.637s, episode steps:  20, steps per second:  31, episode reward: 41.305, mean reward:  2.065 [-2.362, 32.022], mean action: 2.950 [0.000, 15.000],  loss: 0.022118, mae: 0.449661, mean_q: 0.419685, mean_eps: 0.000000
 3339/5000: episode: 130, duration: 0.687s, episode steps:  20, steps per second:  29, episode reward: 37.822, mean reward:  1.891 [-2.492, 32.460], mean action: 4.500 [0.000, 13.000],  loss: 0.017272, mae: 0.426599, mean_q: 0.462704, mean_eps: 0.000000
 3371/5000: episode: 131, duration: 0.462s, episode steps:  32, steps per second:  69, episode reward: -32.500, mean reward: -1.016 [-31.622,  2.412], mean action: 7.219 [0.000, 14.000],  loss: 0.019322, mae: 0.443162, mean_q: 0.421603, mean_eps: 0.000000
 3382/5000: episode: 132, duration: 0.139s, episode steps:  11, steps per second:  79, episode reward: 41.729, mean reward:  3.794 [-2.601, 32.777], mean action: 4.273 [0.000, 14.000],  loss: 0.024315, mae: 0.471978, mean_q: 0.381325, mean_eps: 0.000000
 3404/5000: episode: 133, duration: 0.268s, episode steps:  22, steps per second:  82, episode reward: 32.644, mean reward:  1.484 [-2.660, 30.000], mean action: 4.909 [0.000, 20.000],  loss: 0.016089, mae: 0.422688, mean_q: 0.395027, mean_eps: 0.000000
 3435/5000: episode: 134, duration: 0.439s, episode steps:  31, steps per second:  71, episode reward: 32.294, mean reward:  1.042 [-3.000, 32.077], mean action: 9.581 [0.000, 15.000],  loss: 0.018916, mae: 0.439479, mean_q: 0.376384, mean_eps: 0.000000
 3449/5000: episode: 135, duration: 0.190s, episode steps:  14, steps per second:  74, episode reward: 41.517, mean reward:  2.965 [-2.340, 32.103], mean action: 4.429 [0.000, 20.000],  loss: 0.018580, mae: 0.444086, mean_q: 0.420557, mean_eps: 0.000000
 3471/5000: episode: 136, duration: 0.259s, episode steps:  22, steps per second:  85, episode reward: -32.540, mean reward: -1.479 [-31.622,  2.903], mean action: 3.818 [0.000, 15.000],  loss: 0.014618, mae: 0.403459, mean_q: 0.462016, mean_eps: 0.000000
 3485/5000: episode: 137, duration: 0.176s, episode steps:  14, steps per second:  80, episode reward: 43.486, mean reward:  3.106 [-2.806, 33.000], mean action: 6.071 [0.000, 20.000],  loss: 0.013063, mae: 0.395514, mean_q: 0.483324, mean_eps: 0.000000
 3504/5000: episode: 138, duration: 0.224s, episode steps:  19, steps per second:  85, episode reward: 38.360, mean reward:  2.019 [-2.778, 32.411], mean action: 7.632 [1.000, 15.000],  loss: 0.021410, mae: 0.431806, mean_q: 0.441223, mean_eps: 0.000000
 3525/5000: episode: 139, duration: 0.266s, episode steps:  21, steps per second:  79, episode reward: 39.000, mean reward:  1.857 [-2.357, 33.000], mean action: 3.476 [0.000, 11.000],  loss: 0.020945, mae: 0.442328, mean_q: 0.451583, mean_eps: 0.000000
 3547/5000: episode: 140, duration: 0.261s, episode steps:  22, steps per second:  84, episode reward: 36.000, mean reward:  1.636 [-2.298, 32.400], mean action: 3.455 [0.000, 15.000],  loss: 0.018207, mae: 0.437423, mean_q: 0.423728, mean_eps: 0.000000
 3570/5000: episode: 141, duration: 0.255s, episode steps:  23, steps per second:  90, episode reward: 41.409, mean reward:  1.800 [-2.391, 33.000], mean action: 4.391 [0.000, 15.000],  loss: 0.020840, mae: 0.462581, mean_q: 0.417653, mean_eps: 0.000000
 3594/5000: episode: 142, duration: 0.275s, episode steps:  24, steps per second:  87, episode reward: -33.000, mean reward: -1.375 [-30.316,  2.456], mean action: 6.542 [0.000, 15.000],  loss: 0.022021, mae: 0.455514, mean_q: 0.420522, mean_eps: 0.000000
 3611/5000: episode: 143, duration: 0.205s, episode steps:  17, steps per second:  83, episode reward: 35.339, mean reward:  2.079 [-2.901, 32.339], mean action: 5.000 [0.000, 15.000],  loss: 0.020180, mae: 0.439704, mean_q: 0.418108, mean_eps: 0.000000
 3648/5000: episode: 144, duration: 0.415s, episode steps:  37, steps per second:  89, episode reward: -33.000, mean reward: -0.892 [-32.161,  2.500], mean action: 6.595 [0.000, 19.000],  loss: 0.019262, mae: 0.437052, mean_q: 0.421523, mean_eps: 0.000000
 3670/5000: episode: 145, duration: 0.249s, episode steps:  22, steps per second:  88, episode reward: 35.459, mean reward:  1.612 [-2.522, 32.020], mean action: 5.682 [0.000, 20.000],  loss: 0.015698, mae: 0.435604, mean_q: 0.363137, mean_eps: 0.000000
 3684/5000: episode: 146, duration: 0.177s, episode steps:  14, steps per second:  79, episode reward: 41.784, mean reward:  2.985 [-2.286, 32.694], mean action: 2.500 [0.000, 12.000],  loss: 0.017103, mae: 0.438935, mean_q: 0.357516, mean_eps: 0.000000
 3699/5000: episode: 147, duration: 0.219s, episode steps:  15, steps per second:  69, episode reward: 38.362, mean reward:  2.557 [-2.549, 31.562], mean action: 3.733 [0.000, 15.000],  loss: 0.023878, mae: 0.449708, mean_q: 0.420253, mean_eps: 0.000000
 3726/5000: episode: 148, duration: 0.415s, episode steps:  27, steps per second:  65, episode reward: 41.781, mean reward:  1.547 [-2.240, 32.380], mean action: 3.778 [0.000, 13.000],  loss: 0.020161, mae: 0.426018, mean_q: 0.468922, mean_eps: 0.000000
 3745/5000: episode: 149, duration: 0.209s, episode steps:  19, steps per second:  91, episode reward: 35.690, mean reward:  1.878 [-2.609, 32.690], mean action: 3.895 [0.000, 12.000],  loss: 0.019512, mae: 0.432309, mean_q: 0.461677, mean_eps: 0.000000
 3780/5000: episode: 150, duration: 0.472s, episode steps:  35, steps per second:  74, episode reward: 32.815, mean reward:  0.938 [-2.300, 32.675], mean action: 4.686 [0.000, 14.000],  loss: 0.018257, mae: 0.421223, mean_q: 0.466153, mean_eps: 0.000000
 3806/5000: episode: 151, duration: 0.285s, episode steps:  26, steps per second:  91, episode reward: 32.423, mean reward:  1.247 [-3.000, 31.743], mean action: 5.654 [1.000, 13.000],  loss: 0.020491, mae: 0.440593, mean_q: 0.459086, mean_eps: 0.000000
 3826/5000: episode: 152, duration: 0.233s, episode steps:  20, steps per second:  86, episode reward: -35.300, mean reward: -1.765 [-32.300,  3.000], mean action: 5.500 [0.000, 15.000],  loss: 0.019008, mae: 0.444949, mean_q: 0.389799, mean_eps: 0.000000
 3847/5000: episode: 153, duration: 0.248s, episode steps:  21, steps per second:  85, episode reward: 38.649, mean reward:  1.840 [-3.000, 32.136], mean action: 3.857 [0.000, 19.000],  loss: 0.020110, mae: 0.443901, mean_q: 0.393657, mean_eps: 0.000000
 3856/5000: episode: 154, duration: 0.122s, episode steps:   9, steps per second:  74, episode reward: 47.316, mean reward:  5.257 [-0.014, 32.190], mean action: 1.889 [0.000, 12.000],  loss: 0.017634, mae: 0.430006, mean_q: 0.361267, mean_eps: 0.000000
 3880/5000: episode: 155, duration: 0.287s, episode steps:  24, steps per second:  84, episode reward: -35.200, mean reward: -1.467 [-32.420,  2.683], mean action: 5.500 [0.000, 19.000],  loss: 0.016451, mae: 0.439281, mean_q: 0.361373, mean_eps: 0.000000
 3901/5000: episode: 156, duration: 0.237s, episode steps:  21, steps per second:  89, episode reward: -32.250, mean reward: -1.536 [-31.501,  2.669], mean action: 3.524 [0.000, 11.000],  loss: 0.017893, mae: 0.439222, mean_q: 0.415241, mean_eps: 0.000000
 3950/5000: episode: 157, duration: 0.540s, episode steps:  49, steps per second:  91, episode reward: 32.472, mean reward:  0.663 [-2.502, 32.030], mean action: 5.265 [0.000, 13.000],  loss: 0.020460, mae: 0.432856, mean_q: 0.456181, mean_eps: 0.000000
 3971/5000: episode: 158, duration: 0.232s, episode steps:  21, steps per second:  91, episode reward: 33.000, mean reward:  1.571 [-3.000, 32.210], mean action: 3.619 [0.000, 9.000],  loss: 0.018927, mae: 0.412338, mean_q: 0.486764, mean_eps: 0.000000
 3986/5000: episode: 159, duration: 0.189s, episode steps:  15, steps per second:  79, episode reward: 40.477, mean reward:  2.698 [-2.229, 31.778], mean action: 5.733 [0.000, 13.000],  loss: 0.014854, mae: 0.399625, mean_q: 0.479172, mean_eps: 0.000000
 4006/5000: episode: 160, duration: 0.233s, episode steps:  20, steps per second:  86, episode reward: 38.870, mean reward:  1.943 [-2.592, 32.031], mean action: 3.100 [0.000, 9.000],  loss: 0.018078, mae: 0.424814, mean_q: 0.421083, mean_eps: 0.000000
 4030/5000: episode: 161, duration: 0.271s, episode steps:  24, steps per second:  89, episode reward: 37.698, mean reward:  1.571 [-2.636, 33.000], mean action: 5.083 [0.000, 18.000],  loss: 0.017568, mae: 0.422767, mean_q: 0.384055, mean_eps: 0.000000
 4055/5000: episode: 162, duration: 0.293s, episode steps:  25, steps per second:  85, episode reward: -35.910, mean reward: -1.436 [-32.051,  2.430], mean action: 7.640 [0.000, 20.000],  loss: 0.022733, mae: 0.456351, mean_q: 0.421031, mean_eps: 0.000000
 4073/5000: episode: 163, duration: 0.227s, episode steps:  18, steps per second:  79, episode reward: 43.751, mean reward:  2.431 [-2.204, 33.000], mean action: 10.944 [0.000, 15.000],  loss: 0.019117, mae: 0.436601, mean_q: 0.418056, mean_eps: 0.000000
 4096/5000: episode: 164, duration: 0.273s, episode steps:  23, steps per second:  84, episode reward: 35.672, mean reward:  1.551 [-2.964, 32.123], mean action: 3.826 [0.000, 15.000],  loss: 0.019285, mae: 0.435817, mean_q: 0.406369, mean_eps: 0.000000
 4124/5000: episode: 165, duration: 0.336s, episode steps:  28, steps per second:  83, episode reward: 40.749, mean reward:  1.455 [-2.252, 33.000], mean action: 9.536 [0.000, 18.000],  loss: 0.015377, mae: 0.416197, mean_q: 0.402957, mean_eps: 0.000000
 4150/5000: episode: 166, duration: 0.297s, episode steps:  26, steps per second:  88, episode reward: 35.823, mean reward:  1.378 [-2.592, 32.163], mean action: 5.962 [0.000, 15.000],  loss: 0.017425, mae: 0.418288, mean_q: 0.428201, mean_eps: 0.000000
 4178/5000: episode: 167, duration: 0.305s, episode steps:  28, steps per second:  92, episode reward: 40.657, mean reward:  1.452 [-2.187, 32.211], mean action: 6.107 [0.000, 19.000],  loss: 0.019071, mae: 0.449291, mean_q: 0.352024, mean_eps: 0.000000
 4202/5000: episode: 168, duration: 0.267s, episode steps:  24, steps per second:  90, episode reward: 35.247, mean reward:  1.469 [-3.000, 32.065], mean action: 5.125 [0.000, 15.000],  loss: 0.014968, mae: 0.444242, mean_q: 0.344729, mean_eps: 0.000000
 4225/5000: episode: 169, duration: 0.319s, episode steps:  23, steps per second:  72, episode reward: 38.108, mean reward:  1.657 [-2.458, 32.900], mean action: 6.174 [0.000, 20.000],  loss: 0.016671, mae: 0.452500, mean_q: 0.368788, mean_eps: 0.000000
 4249/5000: episode: 170, duration: 0.264s, episode steps:  24, steps per second:  91, episode reward: 35.757, mean reward:  1.490 [-2.446, 31.807], mean action: 4.208 [0.000, 20.000],  loss: 0.016473, mae: 0.425280, mean_q: 0.409443, mean_eps: 0.000000
 4269/5000: episode: 171, duration: 0.230s, episode steps:  20, steps per second:  87, episode reward: 38.671, mean reward:  1.934 [-3.000, 32.110], mean action: 3.200 [0.000, 19.000],  loss: 0.017862, mae: 0.436791, mean_q: 0.387426, mean_eps: 0.000000
 4299/5000: episode: 172, duration: 0.328s, episode steps:  30, steps per second:  92, episode reward: -32.180, mean reward: -1.073 [-32.689,  2.180], mean action: 8.067 [0.000, 19.000],  loss: 0.019833, mae: 0.438036, mean_q: 0.406724, mean_eps: 0.000000
 4320/5000: episode: 173, duration: 0.228s, episode steps:  21, steps per second:  92, episode reward: -38.900, mean reward: -1.852 [-32.535,  2.180], mean action: 3.524 [0.000, 9.000],  loss: 0.017189, mae: 0.427993, mean_q: 0.425578, mean_eps: 0.000000
 4347/5000: episode: 174, duration: 0.289s, episode steps:  27, steps per second:  93, episode reward: -35.620, mean reward: -1.319 [-32.081,  2.420], mean action: 8.000 [0.000, 19.000],  loss: 0.016224, mae: 0.415202, mean_q: 0.417194, mean_eps: 0.000000
 4370/5000: episode: 175, duration: 0.281s, episode steps:  23, steps per second:  82, episode reward: -35.470, mean reward: -1.542 [-32.281,  2.340], mean action: 6.130 [0.000, 15.000],  loss: 0.016749, mae: 0.420157, mean_q: 0.427104, mean_eps: 0.000000
 4385/5000: episode: 176, duration: 0.176s, episode steps:  15, steps per second:  85, episode reward: 44.091, mean reward:  2.939 [-2.231, 32.447], mean action: 2.867 [0.000, 9.000],  loss: 0.021014, mae: 0.442820, mean_q: 0.429499, mean_eps: 0.000000
 4405/5000: episode: 177, duration: 0.224s, episode steps:  20, steps per second:  89, episode reward: 47.352, mean reward:  2.368 [ 0.000, 33.000], mean action: 1.450 [0.000, 19.000],  loss: 0.019362, mae: 0.435609, mean_q: 0.385511, mean_eps: 0.000000
 4434/5000: episode: 178, duration: 0.321s, episode steps:  29, steps per second:  90, episode reward: 32.434, mean reward:  1.118 [-2.903, 32.656], mean action: 7.966 [0.000, 20.000],  loss: 0.017408, mae: 0.428276, mean_q: 0.391340, mean_eps: 0.000000
 4463/5000: episode: 179, duration: 0.330s, episode steps:  29, steps per second:  88, episode reward: -32.210, mean reward: -1.111 [-31.896,  2.210], mean action: 3.759 [0.000, 15.000],  loss: 0.019276, mae: 0.451063, mean_q: 0.377090, mean_eps: 0.000000
 4488/5000: episode: 180, duration: 0.277s, episode steps:  25, steps per second:  90, episode reward: 32.403, mean reward:  1.296 [-2.668, 32.380], mean action: 4.520 [0.000, 9.000],  loss: 0.019145, mae: 0.451484, mean_q: 0.392816, mean_eps: 0.000000
 4511/5000: episode: 181, duration: 0.257s, episode steps:  23, steps per second:  89, episode reward: 38.169, mean reward:  1.660 [-3.000, 32.835], mean action: 2.826 [0.000, 9.000],  loss: 0.020527, mae: 0.444795, mean_q: 0.404481, mean_eps: 0.000000
 4533/5000: episode: 182, duration: 0.279s, episode steps:  22, steps per second:  79, episode reward: 36.000, mean reward:  1.636 [-3.000, 32.120], mean action: 3.364 [0.000, 9.000],  loss: 0.020100, mae: 0.441177, mean_q: 0.392131, mean_eps: 0.000000
 4566/5000: episode: 183, duration: 0.377s, episode steps:  33, steps per second:  88, episode reward: 30.000, mean reward:  0.909 [-2.586, 29.680], mean action: 7.152 [0.000, 20.000],  loss: 0.018638, mae: 0.442226, mean_q: 0.397084, mean_eps: 0.000000
 4581/5000: episode: 184, duration: 0.172s, episode steps:  15, steps per second:  87, episode reward: 41.455, mean reward:  2.764 [-2.563, 31.927], mean action: 2.800 [0.000, 9.000],  loss: 0.013891, mae: 0.426161, mean_q: 0.388082, mean_eps: 0.000000
 4603/5000: episode: 185, duration: 0.251s, episode steps:  22, steps per second:  88, episode reward: -35.420, mean reward: -1.610 [-31.822,  2.676], mean action: 6.273 [0.000, 14.000],  loss: 0.017306, mae: 0.428040, mean_q: 0.438848, mean_eps: 0.000000
 4628/5000: episode: 186, duration: 0.341s, episode steps:  25, steps per second:  73, episode reward: -32.360, mean reward: -1.294 [-32.360,  2.130], mean action: 4.760 [0.000, 14.000],  loss: 0.019900, mae: 0.432650, mean_q: 0.459720, mean_eps: 0.000000
 4651/5000: episode: 187, duration: 0.261s, episode steps:  23, steps per second:  88, episode reward: 38.333, mean reward:  1.667 [-2.474, 32.160], mean action: 2.348 [0.000, 9.000],  loss: 0.019985, mae: 0.437328, mean_q: 0.436191, mean_eps: 0.000000
 4690/5000: episode: 188, duration: 0.458s, episode steps:  39, steps per second:  85, episode reward: 32.341, mean reward:  0.829 [-2.268, 32.020], mean action: 8.103 [1.000, 19.000],  loss: 0.020311, mae: 0.447041, mean_q: 0.445030, mean_eps: 0.000000
 4714/5000: episode: 189, duration: 0.273s, episode steps:  24, steps per second:  88, episode reward: -35.660, mean reward: -1.486 [-32.056,  2.473], mean action: 10.167 [0.000, 21.000],  loss: 0.021846, mae: 0.455201, mean_q: 0.438875, mean_eps: 0.000000
 4732/5000: episode: 190, duration: 0.207s, episode steps:  18, steps per second:  87, episode reward: 36.000, mean reward:  2.000 [-2.334, 32.170], mean action: 4.056 [0.000, 15.000],  loss: 0.019870, mae: 0.444110, mean_q: 0.431812, mean_eps: 0.000000
 4772/5000: episode: 191, duration: 0.433s, episode steps:  40, steps per second:  92, episode reward: -33.000, mean reward: -0.825 [-32.062,  2.413], mean action: 4.250 [0.000, 11.000],  loss: 0.016071, mae: 0.429322, mean_q: 0.399011, mean_eps: 0.000000
 4804/5000: episode: 192, duration: 0.345s, episode steps:  32, steps per second:  93, episode reward: 37.533, mean reward:  1.173 [-2.805, 31.944], mean action: 5.906 [0.000, 15.000],  loss: 0.019642, mae: 0.444263, mean_q: 0.419363, mean_eps: 0.000000
 4830/5000: episode: 193, duration: 0.298s, episode steps:  26, steps per second:  87, episode reward: -39.000, mean reward: -1.500 [-29.984,  1.973], mean action: 11.423 [1.000, 20.000],  loss: 0.017823, mae: 0.431772, mean_q: 0.441649, mean_eps: 0.000000
 4861/5000: episode: 194, duration: 0.338s, episode steps:  31, steps per second:  92, episode reward: 32.324, mean reward:  1.043 [-2.435, 31.922], mean action: 7.839 [0.000, 15.000],  loss: 0.017404, mae: 0.432617, mean_q: 0.454672, mean_eps: 0.000000
 4910/5000: episode: 195, duration: 0.518s, episode steps:  49, steps per second:  95, episode reward: 41.633, mean reward:  0.850 [-2.285, 31.923], mean action: 1.612 [0.000, 15.000],  loss: 0.018633, mae: 0.446058, mean_q: 0.422417, mean_eps: 0.000000
 4931/5000: episode: 196, duration: 0.235s, episode steps:  21, steps per second:  89, episode reward: 38.202, mean reward:  1.819 [-2.533, 31.943], mean action: 3.524 [0.000, 15.000],  loss: 0.019232, mae: 0.467146, mean_q: 0.370507, mean_eps: 0.000000
 4964/5000: episode: 197, duration: 0.365s, episode steps:  33, steps per second:  90, episode reward: 34.479, mean reward:  1.045 [-2.494, 31.980], mean action: 7.152 [0.000, 21.000],  loss: 0.017258, mae: 0.436218, mean_q: 0.442537, mean_eps: 0.000000
 4995/5000: episode: 198, duration: 0.339s, episode steps:  31, steps per second:  92, episode reward: -38.090, mean reward: -1.229 [-32.197,  2.280], mean action: 5.903 [0.000, 20.000],  loss: 0.016114, mae: 0.425539, mean_q: 0.482517, mean_eps: 0.000000
done, took 65.709 seconds
DQN Evaluation: 275 victories out of 347 episodes
Training for 5000 steps ...
   19/5000: episode: 1, duration: 0.133s, episode steps:  19, steps per second: 143, episode reward: 43.710, mean reward:  2.301 [-2.500, 31.931], mean action: 2.263 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   49/5000: episode: 2, duration: 0.190s, episode steps:  30, steps per second: 158, episode reward: 32.875, mean reward:  1.096 [-2.738, 32.580], mean action: 4.367 [0.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   60/5000: episode: 3, duration: 0.083s, episode steps:  11, steps per second: 133, episode reward: 44.155, mean reward:  4.014 [-2.370, 32.570], mean action: 3.000 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   81/5000: episode: 4, duration: 0.137s, episode steps:  21, steps per second: 153, episode reward: 41.160, mean reward:  1.960 [-2.397, 31.913], mean action: 2.048 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  104/5000: episode: 5, duration: 0.146s, episode steps:  23, steps per second: 158, episode reward: 41.970, mean reward:  1.825 [-2.333, 32.280], mean action: 4.000 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  145/5000: episode: 6, duration: 0.243s, episode steps:  41, steps per second: 169, episode reward: 36.000, mean reward:  0.878 [-3.000, 33.000], mean action: 2.707 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  169/5000: episode: 7, duration: 0.153s, episode steps:  24, steps per second: 157, episode reward: 38.471, mean reward:  1.603 [-2.287, 31.541], mean action: 4.917 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  195/5000: episode: 8, duration: 0.172s, episode steps:  26, steps per second: 151, episode reward: 38.529, mean reward:  1.482 [-2.709, 31.979], mean action: 1.731 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  229/5000: episode: 9, duration: 0.206s, episode steps:  34, steps per second: 165, episode reward: 35.799, mean reward:  1.053 [-3.000, 32.228], mean action: 3.147 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  260/5000: episode: 10, duration: 0.194s, episode steps:  31, steps per second: 160, episode reward: 32.190, mean reward:  1.038 [-3.000, 32.140], mean action: 6.452 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  283/5000: episode: 11, duration: 0.152s, episode steps:  23, steps per second: 151, episode reward: 43.669, mean reward:  1.899 [-3.000, 32.009], mean action: 3.087 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  325/5000: episode: 12, duration: 0.314s, episode steps:  42, steps per second: 134, episode reward: 32.483, mean reward:  0.773 [-3.000, 32.030], mean action: 5.167 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  355/5000: episode: 13, duration: 0.202s, episode steps:  30, steps per second: 149, episode reward: 41.229, mean reward:  1.374 [-2.287, 32.250], mean action: 3.733 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  389/5000: episode: 14, duration: 0.224s, episode steps:  34, steps per second: 152, episode reward: 38.249, mean reward:  1.125 [-2.402, 31.879], mean action: 2.765 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  405/5000: episode: 15, duration: 0.105s, episode steps:  16, steps per second: 152, episode reward: 48.000, mean reward:  3.000 [ 0.000, 32.040], mean action: 2.750 [0.000, 8.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  429/5000: episode: 16, duration: 0.162s, episode steps:  24, steps per second: 149, episode reward: 43.299, mean reward:  1.804 [-3.000, 32.361], mean action: 2.000 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  456/5000: episode: 17, duration: 0.168s, episode steps:  27, steps per second: 160, episode reward: 41.608, mean reward:  1.541 [-2.537, 32.120], mean action: 4.111 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  489/5000: episode: 18, duration: 0.199s, episode steps:  33, steps per second: 166, episode reward: 32.098, mean reward:  0.973 [-3.000, 32.198], mean action: 2.939 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  517/5000: episode: 19, duration: 0.178s, episode steps:  28, steps per second: 157, episode reward: 38.329, mean reward:  1.369 [-3.000, 31.925], mean action: 4.500 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  539/5000: episode: 20, duration: 0.154s, episode steps:  22, steps per second: 143, episode reward: 43.649, mean reward:  1.984 [-2.191, 31.807], mean action: 3.045 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  567/5000: episode: 21, duration: 0.181s, episode steps:  28, steps per second: 155, episode reward: 45.614, mean reward:  1.629 [-0.650, 32.100], mean action: 3.643 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  606/5000: episode: 22, duration: 0.252s, episode steps:  39, steps per second: 155, episode reward: -35.000, mean reward: -0.897 [-32.842,  2.570], mean action: 5.692 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  626/5000: episode: 23, duration: 0.133s, episode steps:  20, steps per second: 150, episode reward: 41.122, mean reward:  2.056 [-3.000, 31.903], mean action: 5.150 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  674/5000: episode: 24, duration: 0.301s, episode steps:  48, steps per second: 159, episode reward: 38.695, mean reward:  0.806 [-2.458, 32.502], mean action: 5.521 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  695/5000: episode: 25, duration: 0.139s, episode steps:  21, steps per second: 151, episode reward: 44.073, mean reward:  2.099 [-2.201, 32.250], mean action: 4.286 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  722/5000: episode: 26, duration: 0.167s, episode steps:  27, steps per second: 162, episode reward: 41.877, mean reward:  1.551 [-3.000, 32.160], mean action: 4.815 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  752/5000: episode: 27, duration: 0.300s, episode steps:  30, steps per second: 100, episode reward: 41.888, mean reward:  1.396 [-2.075, 32.370], mean action: 1.600 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  796/5000: episode: 28, duration: 0.271s, episode steps:  44, steps per second: 162, episode reward: 44.065, mean reward:  1.001 [-2.443, 33.000], mean action: 4.091 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  823/5000: episode: 29, duration: 0.172s, episode steps:  27, steps per second: 157, episode reward: 38.752, mean reward:  1.435 [-3.000, 32.110], mean action: 3.926 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  844/5000: episode: 30, duration: 0.135s, episode steps:  21, steps per second: 156, episode reward: 41.734, mean reward:  1.987 [-2.537, 32.370], mean action: 2.762 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  888/5000: episode: 31, duration: 0.280s, episode steps:  44, steps per second: 157, episode reward: 35.911, mean reward:  0.816 [-2.416, 32.190], mean action: 4.955 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  913/5000: episode: 32, duration: 0.165s, episode steps:  25, steps per second: 152, episode reward: 39.000, mean reward:  1.560 [-2.188, 32.340], mean action: 2.400 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  939/5000: episode: 33, duration: 0.168s, episode steps:  26, steps per second: 155, episode reward: 38.010, mean reward:  1.462 [-2.903, 32.574], mean action: 5.154 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  963/5000: episode: 34, duration: 0.154s, episode steps:  24, steps per second: 156, episode reward: 38.446, mean reward:  1.602 [-3.000, 32.105], mean action: 2.208 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  985/5000: episode: 35, duration: 0.142s, episode steps:  22, steps per second: 155, episode reward: 38.721, mean reward:  1.760 [-3.000, 32.440], mean action: 3.591 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1015/5000: episode: 36, duration: 0.446s, episode steps:  30, steps per second:  67, episode reward: 44.406, mean reward:  1.480 [-2.094, 32.340], mean action: 3.600 [0.000, 13.000],  loss: 0.020157, mae: 0.451178, mean_q: 0.448827, mean_eps: 0.000000
 1055/5000: episode: 37, duration: 0.609s, episode steps:  40, steps per second:  66, episode reward: -32.830, mean reward: -0.821 [-32.229,  2.280], mean action: 6.275 [0.000, 17.000],  loss: 0.015955, mae: 0.419062, mean_q: 0.470248, mean_eps: 0.000000
 1082/5000: episode: 38, duration: 0.336s, episode steps:  27, steps per second:  80, episode reward: 40.659, mean reward:  1.506 [-2.065, 31.730], mean action: 6.667 [0.000, 19.000],  loss: 0.018439, mae: 0.426421, mean_q: 0.451967, mean_eps: 0.000000
 1114/5000: episode: 39, duration: 0.527s, episode steps:  32, steps per second:  61, episode reward: 39.000, mean reward:  1.219 [-2.514, 32.150], mean action: 5.188 [1.000, 20.000],  loss: 0.016362, mae: 0.429647, mean_q: 0.380888, mean_eps: 0.000000
 1136/5000: episode: 40, duration: 0.374s, episode steps:  22, steps per second:  59, episode reward: 44.595, mean reward:  2.027 [-2.012, 32.190], mean action: 0.864 [0.000, 11.000],  loss: 0.017723, mae: 0.455603, mean_q: 0.368646, mean_eps: 0.000000
 1169/5000: episode: 41, duration: 0.458s, episode steps:  33, steps per second:  72, episode reward: 38.015, mean reward:  1.152 [-2.519, 31.859], mean action: 7.636 [0.000, 20.000],  loss: 0.018761, mae: 0.444331, mean_q: 0.430907, mean_eps: 0.000000
 1201/5000: episode: 42, duration: 0.347s, episode steps:  32, steps per second:  92, episode reward: 36.000, mean reward:  1.125 [-2.939, 32.330], mean action: 7.125 [1.000, 19.000],  loss: 0.020247, mae: 0.447886, mean_q: 0.425735, mean_eps: 0.000000
 1239/5000: episode: 43, duration: 0.598s, episode steps:  38, steps per second:  64, episode reward: 41.116, mean reward:  1.082 [-2.206, 31.867], mean action: 2.868 [0.000, 20.000],  loss: 0.020204, mae: 0.449495, mean_q: 0.389352, mean_eps: 0.000000
 1272/5000: episode: 44, duration: 0.498s, episode steps:  33, steps per second:  66, episode reward: 47.187, mean reward:  1.430 [-0.257, 32.130], mean action: 2.788 [0.000, 20.000],  loss: 0.018100, mae: 0.442703, mean_q: 0.397529, mean_eps: 0.000000
 1293/5000: episode: 45, duration: 0.239s, episode steps:  21, steps per second:  88, episode reward: 43.990, mean reward:  2.095 [-2.685, 32.390], mean action: 5.238 [0.000, 20.000],  loss: 0.020297, mae: 0.432215, mean_q: 0.435406, mean_eps: 0.000000
 1323/5000: episode: 46, duration: 0.328s, episode steps:  30, steps per second:  92, episode reward: 41.523, mean reward:  1.384 [-2.225, 31.984], mean action: 3.533 [0.000, 13.000],  loss: 0.018662, mae: 0.425092, mean_q: 0.450818, mean_eps: 0.000000
 1344/5000: episode: 47, duration: 0.242s, episode steps:  21, steps per second:  87, episode reward: 47.044, mean reward:  2.240 [-0.233, 32.634], mean action: 3.571 [0.000, 19.000],  loss: 0.019185, mae: 0.435432, mean_q: 0.458131, mean_eps: 0.000000
 1362/5000: episode: 48, duration: 0.205s, episode steps:  18, steps per second:  88, episode reward: 44.074, mean reward:  2.449 [-2.088, 32.270], mean action: 2.167 [0.000, 14.000],  loss: 0.019580, mae: 0.431775, mean_q: 0.450277, mean_eps: 0.000000
 1395/5000: episode: 49, duration: 0.362s, episode steps:  33, steps per second:  91, episode reward: 41.182, mean reward:  1.248 [-2.186, 31.945], mean action: 4.727 [0.000, 20.000],  loss: 0.020455, mae: 0.446452, mean_q: 0.410853, mean_eps: 0.000000
 1419/5000: episode: 50, duration: 0.263s, episode steps:  24, steps per second:  91, episode reward: 35.804, mean reward:  1.492 [-2.915, 31.974], mean action: 5.292 [0.000, 18.000],  loss: 0.022258, mae: 0.441100, mean_q: 0.454161, mean_eps: 0.000000
 1433/5000: episode: 51, duration: 0.158s, episode steps:  14, steps per second:  88, episode reward: 41.268, mean reward:  2.948 [-3.000, 32.470], mean action: 3.786 [0.000, 19.000],  loss: 0.013374, mae: 0.402807, mean_q: 0.483036, mean_eps: 0.000000
 1491/5000: episode: 52, duration: 0.610s, episode steps:  58, steps per second:  95, episode reward: -32.510, mean reward: -0.561 [-32.338,  2.480], mean action: 8.466 [1.000, 20.000],  loss: 0.019785, mae: 0.440320, mean_q: 0.472572, mean_eps: 0.000000
 1521/5000: episode: 53, duration: 0.326s, episode steps:  30, steps per second:  92, episode reward: 39.000, mean reward:  1.300 [-3.000, 32.420], mean action: 5.000 [0.000, 20.000],  loss: 0.020929, mae: 0.460066, mean_q: 0.401053, mean_eps: 0.000000
 1546/5000: episode: 54, duration: 0.292s, episode steps:  25, steps per second:  86, episode reward: 38.210, mean reward:  1.528 [-2.512, 32.360], mean action: 3.160 [0.000, 15.000],  loss: 0.018313, mae: 0.437784, mean_q: 0.424471, mean_eps: 0.000000
 1590/5000: episode: 55, duration: 0.476s, episode steps:  44, steps per second:  92, episode reward: 41.584, mean reward:  0.945 [-2.313, 32.080], mean action: 4.341 [0.000, 15.000],  loss: 0.015573, mae: 0.419960, mean_q: 0.538396, mean_eps: 0.000000
 1613/5000: episode: 56, duration: 0.264s, episode steps:  23, steps per second:  87, episode reward: 40.643, mean reward:  1.767 [-3.000, 32.036], mean action: 3.304 [0.000, 9.000],  loss: 0.015762, mae: 0.439539, mean_q: 0.443959, mean_eps: 0.000000
 1636/5000: episode: 57, duration: 0.256s, episode steps:  23, steps per second:  90, episode reward: 38.494, mean reward:  1.674 [-3.000, 31.664], mean action: 4.957 [0.000, 20.000],  loss: 0.016841, mae: 0.445718, mean_q: 0.393087, mean_eps: 0.000000
 1654/5000: episode: 58, duration: 0.208s, episode steps:  18, steps per second:  87, episode reward: 44.330, mean reward:  2.463 [-2.658, 32.400], mean action: 3.167 [0.000, 9.000],  loss: 0.014364, mae: 0.424031, mean_q: 0.425409, mean_eps: 0.000000
 1674/5000: episode: 59, duration: 0.223s, episode steps:  20, steps per second:  90, episode reward: 41.817, mean reward:  2.091 [-2.608, 32.047], mean action: 5.000 [1.000, 15.000],  loss: 0.020470, mae: 0.437813, mean_q: 0.482902, mean_eps: 0.000000
 1702/5000: episode: 60, duration: 0.303s, episode steps:  28, steps per second:  92, episode reward: 38.805, mean reward:  1.386 [-2.343, 32.132], mean action: 2.786 [0.000, 12.000],  loss: 0.016039, mae: 0.419648, mean_q: 0.459505, mean_eps: 0.000000
 1721/5000: episode: 61, duration: 0.214s, episode steps:  19, steps per second:  89, episode reward: 41.837, mean reward:  2.202 [-2.298, 32.170], mean action: 2.737 [0.000, 12.000],  loss: 0.023721, mae: 0.449696, mean_q: 0.468152, mean_eps: 0.000000
 1739/5000: episode: 62, duration: 0.209s, episode steps:  18, steps per second:  86, episode reward: 41.100, mean reward:  2.283 [-3.000, 32.220], mean action: 4.333 [0.000, 20.000],  loss: 0.019683, mae: 0.425475, mean_q: 0.514250, mean_eps: 0.000000
 1773/5000: episode: 63, duration: 0.397s, episode steps:  34, steps per second:  86, episode reward: -32.780, mean reward: -0.964 [-32.564,  2.461], mean action: 5.000 [0.000, 21.000],  loss: 0.018933, mae: 0.425585, mean_q: 0.466313, mean_eps: 0.000000
 1802/5000: episode: 64, duration: 0.376s, episode steps:  29, steps per second:  77, episode reward: 35.652, mean reward:  1.229 [-2.941, 32.420], mean action: 4.448 [0.000, 20.000],  loss: 0.019373, mae: 0.438429, mean_q: 0.449752, mean_eps: 0.000000
 1851/5000: episode: 65, duration: 0.529s, episode steps:  49, steps per second:  93, episode reward: -32.820, mean reward: -0.670 [-32.043,  3.000], mean action: 12.347 [0.000, 21.000],  loss: 0.021271, mae: 0.442670, mean_q: 0.471771, mean_eps: 0.000000
 1880/5000: episode: 66, duration: 0.323s, episode steps:  29, steps per second:  90, episode reward: 38.579, mean reward:  1.330 [-3.000, 32.380], mean action: 2.483 [0.000, 19.000],  loss: 0.018748, mae: 0.434307, mean_q: 0.478886, mean_eps: 0.000000
 1905/5000: episode: 67, duration: 0.276s, episode steps:  25, steps per second:  91, episode reward: 41.316, mean reward:  1.653 [-3.000, 32.140], mean action: 3.440 [0.000, 14.000],  loss: 0.021326, mae: 0.452987, mean_q: 0.423732, mean_eps: 0.000000
 1948/5000: episode: 68, duration: 0.471s, episode steps:  43, steps per second:  91, episode reward: -32.720, mean reward: -0.761 [-32.141,  3.000], mean action: 7.535 [1.000, 20.000],  loss: 0.016742, mae: 0.433560, mean_q: 0.419368, mean_eps: 0.000000
 1991/5000: episode: 69, duration: 0.464s, episode steps:  43, steps per second:  93, episode reward: 33.000, mean reward:  0.767 [-2.246, 32.620], mean action: 5.651 [0.000, 20.000],  loss: 0.019351, mae: 0.433994, mean_q: 0.478886, mean_eps: 0.000000
 2006/5000: episode: 70, duration: 0.177s, episode steps:  15, steps per second:  85, episode reward: 46.723, mean reward:  3.115 [-0.452, 31.810], mean action: 4.600 [0.000, 9.000],  loss: 0.013860, mae: 0.418527, mean_q: 0.429912, mean_eps: 0.000000
 2027/5000: episode: 71, duration: 0.229s, episode steps:  21, steps per second:  92, episode reward: 41.124, mean reward:  1.958 [-2.430, 32.090], mean action: 2.143 [0.000, 9.000],  loss: 0.015103, mae: 0.427295, mean_q: 0.418810, mean_eps: 0.000000
 2068/5000: episode: 72, duration: 0.503s, episode steps:  41, steps per second:  82, episode reward: 32.797, mean reward:  0.800 [-2.669, 29.622], mean action: 3.878 [0.000, 20.000],  loss: 0.019289, mae: 0.436583, mean_q: 0.451395, mean_eps: 0.000000
 2089/5000: episode: 73, duration: 0.294s, episode steps:  21, steps per second:  71, episode reward: 41.653, mean reward:  1.983 [-2.153, 32.040], mean action: 3.810 [0.000, 20.000],  loss: 0.022809, mae: 0.440333, mean_q: 0.535196, mean_eps: 0.000000
 2103/5000: episode: 74, duration: 0.195s, episode steps:  14, steps per second:  72, episode reward: 44.334, mean reward:  3.167 [-2.461, 31.594], mean action: 1.357 [0.000, 9.000],  loss: 0.020439, mae: 0.435323, mean_q: 0.527060, mean_eps: 0.000000
 2123/5000: episode: 75, duration: 0.293s, episode steps:  20, steps per second:  68, episode reward: 44.046, mean reward:  2.202 [-2.130, 32.146], mean action: 0.900 [0.000, 9.000],  loss: 0.017502, mae: 0.437444, mean_q: 0.467380, mean_eps: 0.000000
 2153/5000: episode: 76, duration: 0.335s, episode steps:  30, steps per second:  90, episode reward: 41.506, mean reward:  1.384 [-2.539, 32.050], mean action: 2.500 [0.000, 9.000],  loss: 0.017616, mae: 0.433412, mean_q: 0.431222, mean_eps: 0.000000
 2187/5000: episode: 77, duration: 0.370s, episode steps:  34, steps per second:  92, episode reward: 35.726, mean reward:  1.051 [-3.000, 32.120], mean action: 4.471 [0.000, 19.000],  loss: 0.016680, mae: 0.419334, mean_q: 0.516706, mean_eps: 0.000000
 2244/5000: episode: 78, duration: 0.622s, episode steps:  57, steps per second:  92, episode reward: 38.161, mean reward:  0.669 [-2.518, 32.260], mean action: 2.614 [0.000, 20.000],  loss: 0.019081, mae: 0.444397, mean_q: 0.459829, mean_eps: 0.000000
 2271/5000: episode: 79, duration: 0.298s, episode steps:  27, steps per second:  91, episode reward: 44.875, mean reward:  1.662 [-2.122, 32.250], mean action: 2.333 [1.000, 9.000],  loss: 0.017598, mae: 0.443563, mean_q: 0.448799, mean_eps: 0.000000
 2305/5000: episode: 80, duration: 0.399s, episode steps:  34, steps per second:  85, episode reward: 32.567, mean reward:  0.958 [-3.000, 32.317], mean action: 3.882 [0.000, 19.000],  loss: 0.020422, mae: 0.457190, mean_q: 0.468137, mean_eps: 0.000000
 2324/5000: episode: 81, duration: 0.214s, episode steps:  19, steps per second:  89, episode reward: 41.034, mean reward:  2.160 [-2.522, 32.100], mean action: 2.895 [0.000, 20.000],  loss: 0.018983, mae: 0.460224, mean_q: 0.436078, mean_eps: 0.000000
 2344/5000: episode: 82, duration: 0.239s, episode steps:  20, steps per second:  84, episode reward: 43.697, mean reward:  2.185 [-2.431, 31.798], mean action: 2.200 [0.000, 9.000],  loss: 0.019516, mae: 0.451645, mean_q: 0.438529, mean_eps: 0.000000
 2361/5000: episode: 83, duration: 0.196s, episode steps:  17, steps per second:  87, episode reward: 43.533, mean reward:  2.561 [-2.156, 32.700], mean action: 1.882 [0.000, 14.000],  loss: 0.017710, mae: 0.429595, mean_q: 0.491305, mean_eps: 0.000000
 2402/5000: episode: 84, duration: 0.450s, episode steps:  41, steps per second:  91, episode reward: 35.163, mean reward:  0.858 [-2.335, 32.138], mean action: 3.951 [0.000, 15.000],  loss: 0.017193, mae: 0.430401, mean_q: 0.444069, mean_eps: 0.000000
 2422/5000: episode: 85, duration: 0.231s, episode steps:  20, steps per second:  87, episode reward: 41.740, mean reward:  2.087 [-2.741, 31.770], mean action: 3.800 [0.000, 15.000],  loss: 0.013831, mae: 0.411271, mean_q: 0.446315, mean_eps: 0.000000
 2443/5000: episode: 86, duration: 0.247s, episode steps:  21, steps per second:  85, episode reward: 44.178, mean reward:  2.104 [-2.858, 32.070], mean action: 2.857 [1.000, 14.000],  loss: 0.017435, mae: 0.448594, mean_q: 0.396708, mean_eps: 0.000000
 2470/5000: episode: 87, duration: 0.324s, episode steps:  27, steps per second:  83, episode reward: 40.517, mean reward:  1.501 [-2.731, 32.080], mean action: 5.741 [0.000, 20.000],  loss: 0.017993, mae: 0.450709, mean_q: 0.443835, mean_eps: 0.000000
 2498/5000: episode: 88, duration: 0.315s, episode steps:  28, steps per second:  89, episode reward: 37.872, mean reward:  1.353 [-2.756, 32.620], mean action: 5.571 [0.000, 19.000],  loss: 0.016386, mae: 0.422257, mean_q: 0.461863, mean_eps: 0.000000
 2526/5000: episode: 89, duration: 0.304s, episode steps:  28, steps per second:  92, episode reward: 40.422, mean reward:  1.444 [-2.711, 32.046], mean action: 3.071 [1.000, 11.000],  loss: 0.021496, mae: 0.436031, mean_q: 0.469522, mean_eps: 0.000000
 2582/5000: episode: 90, duration: 0.585s, episode steps:  56, steps per second:  96, episode reward: 40.251, mean reward:  0.719 [-2.142, 32.080], mean action: 4.018 [1.000, 20.000],  loss: 0.018441, mae: 0.430081, mean_q: 0.479787, mean_eps: 0.000000
 2608/5000: episode: 91, duration: 0.296s, episode steps:  26, steps per second:  88, episode reward: 44.589, mean reward:  1.715 [-2.244, 31.869], mean action: 1.000 [0.000, 15.000],  loss: 0.016810, mae: 0.426369, mean_q: 0.476297, mean_eps: 0.000000
 2635/5000: episode: 92, duration: 0.299s, episode steps:  27, steps per second:  90, episode reward: 41.747, mean reward:  1.546 [-2.408, 33.000], mean action: 3.556 [0.000, 15.000],  loss: 0.017901, mae: 0.425137, mean_q: 0.495945, mean_eps: 0.000000
 2652/5000: episode: 93, duration: 0.251s, episode steps:  17, steps per second:  68, episode reward: 38.803, mean reward:  2.283 [-3.000, 32.191], mean action: 7.294 [0.000, 19.000],  loss: 0.023786, mae: 0.452120, mean_q: 0.491754, mean_eps: 0.000000
 2676/5000: episode: 94, duration: 0.266s, episode steps:  24, steps per second:  90, episode reward: 41.313, mean reward:  1.721 [-2.446, 32.070], mean action: 5.042 [1.000, 12.000],  loss: 0.021486, mae: 0.447604, mean_q: 0.483012, mean_eps: 0.000000
 2709/5000: episode: 95, duration: 0.550s, episode steps:  33, steps per second:  60, episode reward: 41.282, mean reward:  1.251 [-2.557, 32.295], mean action: 2.606 [0.000, 15.000],  loss: 0.020119, mae: 0.439135, mean_q: 0.534913, mean_eps: 0.000000
 2741/5000: episode: 96, duration: 0.358s, episode steps:  32, steps per second:  89, episode reward: 38.300, mean reward:  1.197 [-2.424, 32.036], mean action: 5.625 [0.000, 18.000],  loss: 0.017618, mae: 0.431146, mean_q: 0.492874, mean_eps: 0.000000
 2760/5000: episode: 97, duration: 0.225s, episode steps:  19, steps per second:  85, episode reward: 39.000, mean reward:  2.053 [-2.435, 32.420], mean action: 2.632 [0.000, 15.000],  loss: 0.016153, mae: 0.432039, mean_q: 0.426987, mean_eps: 0.000000
 2791/5000: episode: 98, duration: 0.343s, episode steps:  31, steps per second:  90, episode reward: 37.651, mean reward:  1.215 [-2.751, 32.020], mean action: 5.548 [0.000, 20.000],  loss: 0.019597, mae: 0.463398, mean_q: 0.380189, mean_eps: 0.000000
 2811/5000: episode: 99, duration: 0.224s, episode steps:  20, steps per second:  89, episode reward: 35.903, mean reward:  1.795 [-2.807, 32.163], mean action: 3.100 [0.000, 11.000],  loss: 0.016862, mae: 0.451799, mean_q: 0.405461, mean_eps: 0.000000
 2842/5000: episode: 100, duration: 0.345s, episode steps:  31, steps per second:  90, episode reward: 43.619, mean reward:  1.407 [-2.243, 32.030], mean action: 2.613 [0.000, 11.000],  loss: 0.025343, mae: 0.480894, mean_q: 0.442765, mean_eps: 0.000000
 2864/5000: episode: 101, duration: 0.255s, episode steps:  22, steps per second:  86, episode reward: 42.000, mean reward:  1.909 [-3.000, 32.190], mean action: 1.136 [0.000, 11.000],  loss: 0.019198, mae: 0.444895, mean_q: 0.463050, mean_eps: 0.000000
 2897/5000: episode: 102, duration: 0.360s, episode steps:  33, steps per second:  92, episode reward: 41.070, mean reward:  1.245 [-2.301, 32.099], mean action: 3.061 [0.000, 15.000],  loss: 0.013905, mae: 0.418688, mean_q: 0.493849, mean_eps: 0.000000
 2921/5000: episode: 103, duration: 0.263s, episode steps:  24, steps per second:  91, episode reward: 46.669, mean reward:  1.945 [-0.733, 32.200], mean action: 3.208 [0.000, 14.000],  loss: 0.016519, mae: 0.438855, mean_q: 0.449773, mean_eps: 0.000000
 2955/5000: episode: 104, duration: 0.364s, episode steps:  34, steps per second:  93, episode reward: 41.253, mean reward:  1.213 [-2.602, 31.914], mean action: 2.647 [0.000, 20.000],  loss: 0.018283, mae: 0.443506, mean_q: 0.465099, mean_eps: 0.000000
 2971/5000: episode: 105, duration: 0.189s, episode steps:  16, steps per second:  85, episode reward: 44.552, mean reward:  2.784 [-2.012, 32.170], mean action: 2.688 [0.000, 12.000],  loss: 0.022981, mae: 0.466928, mean_q: 0.468319, mean_eps: 0.000000
 3010/5000: episode: 106, duration: 0.423s, episode steps:  39, steps per second:  92, episode reward: 41.180, mean reward:  1.056 [-2.173, 32.230], mean action: 4.846 [0.000, 20.000],  loss: 0.019885, mae: 0.465514, mean_q: 0.430670, mean_eps: 0.000000
 3063/5000: episode: 107, duration: 0.564s, episode steps:  53, steps per second:  94, episode reward: -32.640, mean reward: -0.616 [-32.078,  2.450], mean action: 6.717 [1.000, 21.000],  loss: 0.019248, mae: 0.458032, mean_q: 0.433072, mean_eps: 0.000000
 3109/5000: episode: 108, duration: 0.492s, episode steps:  46, steps per second:  93, episode reward: 35.462, mean reward:  0.771 [-2.724, 31.950], mean action: 2.913 [0.000, 15.000],  loss: 0.018188, mae: 0.439000, mean_q: 0.471678, mean_eps: 0.000000
 3135/5000: episode: 109, duration: 0.299s, episode steps:  26, steps per second:  87, episode reward: 41.128, mean reward:  1.582 [-2.592, 31.685], mean action: 4.885 [0.000, 14.000],  loss: 0.021176, mae: 0.452030, mean_q: 0.494162, mean_eps: 0.000000
 3169/5000: episode: 110, duration: 0.404s, episode steps:  34, steps per second:  84, episode reward: 41.116, mean reward:  1.209 [-2.116, 32.380], mean action: 3.500 [1.000, 12.000],  loss: 0.017598, mae: 0.442191, mean_q: 0.448404, mean_eps: 0.000000
 3189/5000: episode: 111, duration: 0.227s, episode steps:  20, steps per second:  88, episode reward: 44.921, mean reward:  2.246 [-2.272, 32.171], mean action: 1.150 [0.000, 9.000],  loss: 0.019387, mae: 0.460141, mean_q: 0.423088, mean_eps: 0.000000
 3227/5000: episode: 112, duration: 0.420s, episode steps:  38, steps per second:  90, episode reward: 37.408, mean reward:  0.984 [-3.000, 32.290], mean action: 3.737 [0.000, 18.000],  loss: 0.018368, mae: 0.452081, mean_q: 0.447456, mean_eps: 0.000000
 3254/5000: episode: 113, duration: 0.300s, episode steps:  27, steps per second:  90, episode reward: 38.776, mean reward:  1.436 [-2.862, 32.495], mean action: 1.852 [0.000, 9.000],  loss: 0.019871, mae: 0.455868, mean_q: 0.446382, mean_eps: 0.000000
 3286/5000: episode: 114, duration: 0.350s, episode steps:  32, steps per second:  92, episode reward: 40.281, mean reward:  1.259 [-2.622, 32.150], mean action: 2.188 [0.000, 12.000],  loss: 0.020593, mae: 0.463299, mean_q: 0.484473, mean_eps: 0.000000
 3307/5000: episode: 115, duration: 0.270s, episode steps:  21, steps per second:  78, episode reward: 40.752, mean reward:  1.941 [-2.443, 32.011], mean action: 4.667 [0.000, 15.000],  loss: 0.015427, mae: 0.442615, mean_q: 0.439323, mean_eps: 0.000000
 3346/5000: episode: 116, duration: 0.424s, episode steps:  39, steps per second:  92, episode reward: 38.698, mean reward:  0.992 [-2.764, 32.847], mean action: 2.821 [0.000, 20.000],  loss: 0.016750, mae: 0.440968, mean_q: 0.486838, mean_eps: 0.000000
 3359/5000: episode: 117, duration: 0.158s, episode steps:  13, steps per second:  82, episode reward: 47.009, mean reward:  3.616 [-0.030, 32.330], mean action: 0.077 [0.000, 1.000],  loss: 0.017534, mae: 0.437418, mean_q: 0.471988, mean_eps: 0.000000
 3388/5000: episode: 118, duration: 0.315s, episode steps:  29, steps per second:  92, episode reward: 38.753, mean reward:  1.336 [-2.378, 32.160], mean action: 3.897 [0.000, 15.000],  loss: 0.021735, mae: 0.443670, mean_q: 0.515657, mean_eps: 0.000000
 3412/5000: episode: 119, duration: 0.263s, episode steps:  24, steps per second:  91, episode reward: 38.752, mean reward:  1.615 [-2.403, 31.903], mean action: 3.625 [0.000, 15.000],  loss: 0.020156, mae: 0.442455, mean_q: 0.532948, mean_eps: 0.000000
 3423/5000: episode: 120, duration: 0.139s, episode steps:  11, steps per second:  79, episode reward: 44.604, mean reward:  4.055 [-2.549, 32.450], mean action: 2.636 [1.000, 9.000],  loss: 0.019505, mae: 0.441271, mean_q: 0.491771, mean_eps: 0.000000
 3446/5000: episode: 121, duration: 0.259s, episode steps:  23, steps per second:  89, episode reward: 43.422, mean reward:  1.888 [-2.182, 32.743], mean action: 5.696 [0.000, 20.000],  loss: 0.017801, mae: 0.432310, mean_q: 0.537918, mean_eps: 0.000000
 3475/5000: episode: 122, duration: 0.311s, episode steps:  29, steps per second:  93, episode reward: -32.220, mean reward: -1.111 [-32.350,  2.901], mean action: 7.345 [0.000, 20.000],  loss: 0.019776, mae: 0.429540, mean_q: 0.511445, mean_eps: 0.000000
 3493/5000: episode: 123, duration: 0.218s, episode steps:  18, steps per second:  83, episode reward: 41.791, mean reward:  2.322 [-2.946, 32.408], mean action: 5.222 [0.000, 20.000],  loss: 0.022682, mae: 0.461627, mean_q: 0.475038, mean_eps: 0.000000
 3544/5000: episode: 124, duration: 0.607s, episode steps:  51, steps per second:  84, episode reward: 43.756, mean reward:  0.858 [-2.057, 32.484], mean action: 2.196 [0.000, 13.000],  loss: 0.020744, mae: 0.462045, mean_q: 0.470519, mean_eps: 0.000000
 3560/5000: episode: 125, duration: 0.186s, episode steps:  16, steps per second:  86, episode reward: 46.155, mean reward:  2.885 [-0.211, 32.525], mean action: 4.188 [0.000, 20.000],  loss: 0.024677, mae: 0.475012, mean_q: 0.446219, mean_eps: 0.000000
 3583/5000: episode: 126, duration: 0.270s, episode steps:  23, steps per second:  85, episode reward: 44.373, mean reward:  1.929 [-2.219, 32.903], mean action: 4.826 [3.000, 14.000],  loss: 0.018381, mae: 0.453580, mean_q: 0.450880, mean_eps: 0.000000
 3612/5000: episode: 127, duration: 0.319s, episode steps:  29, steps per second:  91, episode reward: 40.951, mean reward:  1.412 [-2.280, 32.170], mean action: 3.103 [0.000, 12.000],  loss: 0.017711, mae: 0.432109, mean_q: 0.503835, mean_eps: 0.000000
 3634/5000: episode: 128, duration: 0.244s, episode steps:  22, steps per second:  90, episode reward: 41.187, mean reward:  1.872 [-2.708, 31.814], mean action: 1.500 [0.000, 9.000],  loss: 0.020541, mae: 0.464330, mean_q: 0.416893, mean_eps: 0.000000
 3662/5000: episode: 129, duration: 0.307s, episode steps:  28, steps per second:  91, episode reward: 41.815, mean reward:  1.493 [-2.555, 32.560], mean action: 5.536 [0.000, 19.000],  loss: 0.019786, mae: 0.461493, mean_q: 0.493758, mean_eps: 0.000000
 3687/5000: episode: 130, duration: 0.275s, episode steps:  25, steps per second:  91, episode reward: 43.324, mean reward:  1.733 [-2.563, 32.150], mean action: 3.360 [0.000, 13.000],  loss: 0.020586, mae: 0.445267, mean_q: 0.532494, mean_eps: 0.000000
 3706/5000: episode: 131, duration: 0.219s, episode steps:  19, steps per second:  87, episode reward: 44.653, mean reward:  2.350 [-2.256, 32.510], mean action: 2.053 [0.000, 9.000],  loss: 0.015435, mae: 0.412660, mean_q: 0.522294, mean_eps: 0.000000
 3757/5000: episode: 132, duration: 0.537s, episode steps:  51, steps per second:  95, episode reward: 35.819, mean reward:  0.702 [-3.000, 32.060], mean action: 3.882 [0.000, 14.000],  loss: 0.018907, mae: 0.439000, mean_q: 0.458168, mean_eps: 0.000000
 3786/5000: episode: 133, duration: 0.330s, episode steps:  29, steps per second:  88, episode reward: 41.614, mean reward:  1.435 [-2.155, 29.263], mean action: 5.276 [0.000, 20.000],  loss: 0.018147, mae: 0.438402, mean_q: 0.469598, mean_eps: 0.000000
 3813/5000: episode: 134, duration: 0.297s, episode steps:  27, steps per second:  91, episode reward: 38.075, mean reward:  1.410 [-3.000, 31.830], mean action: 3.630 [0.000, 21.000],  loss: 0.018291, mae: 0.432382, mean_q: 0.486171, mean_eps: 0.000000
 3845/5000: episode: 135, duration: 0.355s, episode steps:  32, steps per second:  90, episode reward: 44.518, mean reward:  1.391 [-2.674, 32.378], mean action: 2.906 [0.000, 20.000],  loss: 0.019273, mae: 0.437601, mean_q: 0.509354, mean_eps: 0.000000
 3864/5000: episode: 136, duration: 0.214s, episode steps:  19, steps per second:  89, episode reward: 38.004, mean reward:  2.000 [-3.000, 31.778], mean action: 2.947 [0.000, 15.000],  loss: 0.018210, mae: 0.440890, mean_q: 0.483438, mean_eps: 0.000000
 3894/5000: episode: 137, duration: 0.360s, episode steps:  30, steps per second:  83, episode reward: 44.082, mean reward:  1.469 [-2.232, 32.069], mean action: 2.500 [0.000, 9.000],  loss: 0.018733, mae: 0.448846, mean_q: 0.441995, mean_eps: 0.000000
 3921/5000: episode: 138, duration: 0.297s, episode steps:  27, steps per second:  91, episode reward: 44.668, mean reward:  1.654 [-2.382, 32.450], mean action: 4.222 [0.000, 20.000],  loss: 0.017387, mae: 0.452863, mean_q: 0.399419, mean_eps: 0.000000
 3943/5000: episode: 139, duration: 0.304s, episode steps:  22, steps per second:  72, episode reward: 41.103, mean reward:  1.868 [-2.324, 31.790], mean action: 5.682 [0.000, 15.000],  loss: 0.016523, mae: 0.427980, mean_q: 0.435924, mean_eps: 0.000000
 3982/5000: episode: 140, duration: 0.436s, episode steps:  39, steps per second:  89, episode reward: 41.121, mean reward:  1.054 [-2.628, 31.982], mean action: 4.231 [0.000, 15.000],  loss: 0.016150, mae: 0.433324, mean_q: 0.436405, mean_eps: 0.000000
 4008/5000: episode: 141, duration: 0.285s, episode steps:  26, steps per second:  91, episode reward: 38.875, mean reward:  1.495 [-2.444, 32.330], mean action: 2.423 [0.000, 12.000],  loss: 0.017080, mae: 0.436230, mean_q: 0.458940, mean_eps: 0.000000
 4048/5000: episode: 142, duration: 0.430s, episode steps:  40, steps per second:  93, episode reward: 38.241, mean reward:  0.956 [-2.290, 32.020], mean action: 5.275 [0.000, 20.000],  loss: 0.017364, mae: 0.430017, mean_q: 0.479972, mean_eps: 0.000000
 4088/5000: episode: 143, duration: 0.425s, episode steps:  40, steps per second:  94, episode reward: 39.172, mean reward:  0.979 [-2.256, 32.701], mean action: 8.050 [0.000, 20.000],  loss: 0.018408, mae: 0.439553, mean_q: 0.464394, mean_eps: 0.000000
 4129/5000: episode: 144, duration: 0.441s, episode steps:  41, steps per second:  93, episode reward: 32.424, mean reward:  0.791 [-2.603, 32.180], mean action: 5.439 [0.000, 19.000],  loss: 0.015759, mae: 0.429346, mean_q: 0.408927, mean_eps: 0.000000
 4153/5000: episode: 145, duration: 0.274s, episode steps:  24, steps per second:  88, episode reward: 38.017, mean reward:  1.584 [-2.879, 31.900], mean action: 3.792 [0.000, 15.000],  loss: 0.018829, mae: 0.448926, mean_q: 0.438675, mean_eps: 0.000000
 4183/5000: episode: 146, duration: 0.322s, episode steps:  30, steps per second:  93, episode reward: 38.409, mean reward:  1.280 [-3.000, 32.413], mean action: 2.400 [0.000, 9.000],  loss: 0.016527, mae: 0.444990, mean_q: 0.419121, mean_eps: 0.000000
 4205/5000: episode: 147, duration: 0.259s, episode steps:  22, steps per second:  85, episode reward: 44.344, mean reward:  2.016 [-2.480, 32.080], mean action: 1.636 [0.000, 9.000],  loss: 0.018387, mae: 0.456943, mean_q: 0.379571, mean_eps: 0.000000
 4233/5000: episode: 148, duration: 0.313s, episode steps:  28, steps per second:  89, episode reward: 39.000, mean reward:  1.393 [-2.634, 32.830], mean action: 2.679 [0.000, 14.000],  loss: 0.019219, mae: 0.446665, mean_q: 0.421241, mean_eps: 0.000000
 4260/5000: episode: 149, duration: 0.301s, episode steps:  27, steps per second:  90, episode reward: 40.820, mean reward:  1.512 [-2.934, 31.503], mean action: 3.259 [0.000, 14.000],  loss: 0.016825, mae: 0.416488, mean_q: 0.507968, mean_eps: 0.000000
 4276/5000: episode: 150, duration: 0.184s, episode steps:  16, steps per second:  87, episode reward: 45.000, mean reward:  2.812 [-2.143, 32.500], mean action: 2.562 [0.000, 14.000],  loss: 0.017491, mae: 0.415323, mean_q: 0.522073, mean_eps: 0.000000
 4299/5000: episode: 151, duration: 0.264s, episode steps:  23, steps per second:  87, episode reward: 41.901, mean reward:  1.822 [-2.765, 32.171], mean action: 1.217 [0.000, 11.000],  loss: 0.017032, mae: 0.427781, mean_q: 0.451307, mean_eps: 0.000000
 4348/5000: episode: 152, duration: 0.533s, episode steps:  49, steps per second:  92, episode reward: 33.000, mean reward:  0.673 [-3.000, 32.290], mean action: 2.286 [0.000, 11.000],  loss: 0.018747, mae: 0.449121, mean_q: 0.411493, mean_eps: 0.000000
 4365/5000: episode: 153, duration: 0.239s, episode steps:  17, steps per second:  71, episode reward: 41.139, mean reward:  2.420 [-2.441, 31.561], mean action: 2.647 [0.000, 15.000],  loss: 0.014723, mae: 0.404055, mean_q: 0.509059, mean_eps: 0.000000
 4389/5000: episode: 154, duration: 0.281s, episode steps:  24, steps per second:  85, episode reward: 41.792, mean reward:  1.741 [-2.340, 32.112], mean action: 4.375 [0.000, 20.000],  loss: 0.017708, mae: 0.423022, mean_q: 0.472998, mean_eps: 0.000000
 4423/5000: episode: 155, duration: 0.513s, episode steps:  34, steps per second:  66, episode reward: 36.000, mean reward:  1.059 [-3.000, 32.330], mean action: 3.971 [0.000, 14.000],  loss: 0.018196, mae: 0.430029, mean_q: 0.462353, mean_eps: 0.000000
 4446/5000: episode: 156, duration: 0.288s, episode steps:  23, steps per second:  80, episode reward: 44.416, mean reward:  1.931 [-2.086, 33.000], mean action: 1.826 [0.000, 12.000],  loss: 0.017436, mae: 0.425522, mean_q: 0.498378, mean_eps: 0.000000
 4464/5000: episode: 157, duration: 0.200s, episode steps:  18, steps per second:  90, episode reward: 39.000, mean reward:  2.167 [-2.755, 32.150], mean action: 2.222 [0.000, 9.000],  loss: 0.013236, mae: 0.405405, mean_q: 0.490540, mean_eps: 0.000000
 4496/5000: episode: 158, duration: 0.353s, episode steps:  32, steps per second:  91, episode reward: 40.090, mean reward:  1.253 [-2.529, 31.478], mean action: 3.500 [0.000, 14.000],  loss: 0.020551, mae: 0.436653, mean_q: 0.476539, mean_eps: 0.000000
 4524/5000: episode: 159, duration: 0.303s, episode steps:  28, steps per second:  92, episode reward: 38.564, mean reward:  1.377 [-2.725, 31.886], mean action: 3.607 [0.000, 14.000],  loss: 0.017438, mae: 0.420809, mean_q: 0.520146, mean_eps: 0.000000
 4554/5000: episode: 160, duration: 0.332s, episode steps:  30, steps per second:  90, episode reward: 43.564, mean reward:  1.452 [-3.000, 32.121], mean action: 2.667 [0.000, 12.000],  loss: 0.020691, mae: 0.436119, mean_q: 0.490368, mean_eps: 0.000000
 4582/5000: episode: 161, duration: 0.309s, episode steps:  28, steps per second:  91, episode reward: 32.589, mean reward:  1.164 [-3.000, 31.659], mean action: 5.607 [0.000, 19.000],  loss: 0.017748, mae: 0.416306, mean_q: 0.535284, mean_eps: 0.000000
 4605/5000: episode: 162, duration: 0.254s, episode steps:  23, steps per second:  90, episode reward: 35.694, mean reward:  1.552 [-2.851, 32.655], mean action: 3.348 [0.000, 9.000],  loss: 0.018204, mae: 0.425942, mean_q: 0.515005, mean_eps: 0.000000
 4632/5000: episode: 163, duration: 0.307s, episode steps:  27, steps per second:  88, episode reward: 40.856, mean reward:  1.513 [-2.879, 32.320], mean action: 3.333 [0.000, 14.000],  loss: 0.018450, mae: 0.441808, mean_q: 0.481182, mean_eps: 0.000000
 4656/5000: episode: 164, duration: 0.272s, episode steps:  24, steps per second:  88, episode reward: 41.153, mean reward:  1.715 [-3.000, 32.073], mean action: 3.917 [0.000, 15.000],  loss: 0.019349, mae: 0.437718, mean_q: 0.449831, mean_eps: 0.000000
 4684/5000: episode: 165, duration: 0.308s, episode steps:  28, steps per second:  91, episode reward: 38.197, mean reward:  1.364 [-2.626, 32.330], mean action: 5.107 [0.000, 20.000],  loss: 0.021306, mae: 0.445704, mean_q: 0.439913, mean_eps: 0.000000
 4702/5000: episode: 166, duration: 0.198s, episode steps:  18, steps per second:  91, episode reward: 39.000, mean reward:  2.167 [-2.614, 29.855], mean action: 2.444 [1.000, 9.000],  loss: 0.016557, mae: 0.421843, mean_q: 0.497754, mean_eps: 0.000000
 4732/5000: episode: 167, duration: 0.325s, episode steps:  30, steps per second:  92, episode reward: 35.831, mean reward:  1.194 [-3.000, 30.073], mean action: 6.033 [0.000, 20.000],  loss: 0.017633, mae: 0.443755, mean_q: 0.497749, mean_eps: 0.000000
 4756/5000: episode: 168, duration: 0.264s, episode steps:  24, steps per second:  91, episode reward: 35.875, mean reward:  1.495 [-3.000, 32.410], mean action: 5.417 [0.000, 20.000],  loss: 0.020530, mae: 0.464493, mean_q: 0.440809, mean_eps: 0.000000
 4779/5000: episode: 169, duration: 0.256s, episode steps:  23, steps per second:  90, episode reward: 41.708, mean reward:  1.813 [-3.000, 32.378], mean action: 5.609 [1.000, 21.000],  loss: 0.016146, mae: 0.422361, mean_q: 0.531346, mean_eps: 0.000000
 4843/5000: episode: 170, duration: 0.738s, episode steps:  64, steps per second:  87, episode reward: 32.710, mean reward:  0.511 [-2.872, 32.013], mean action: 4.250 [0.000, 20.000],  loss: 0.018027, mae: 0.437024, mean_q: 0.502346, mean_eps: 0.000000
 4868/5000: episode: 171, duration: 0.278s, episode steps:  25, steps per second:  90, episode reward: 47.205, mean reward:  1.888 [-0.073, 31.934], mean action: 2.360 [0.000, 12.000],  loss: 0.018644, mae: 0.457665, mean_q: 0.438005, mean_eps: 0.000000
 4902/5000: episode: 172, duration: 0.363s, episode steps:  34, steps per second:  94, episode reward: 41.349, mean reward:  1.216 [-2.412, 32.330], mean action: 4.265 [0.000, 15.000],  loss: 0.017462, mae: 0.444812, mean_q: 0.461933, mean_eps: 0.000000
 4927/5000: episode: 173, duration: 0.281s, episode steps:  25, steps per second:  89, episode reward: 39.000, mean reward:  1.560 [-2.815, 32.120], mean action: 3.400 [0.000, 15.000],  loss: 0.017620, mae: 0.426472, mean_q: 0.475350, mean_eps: 0.000000
 4948/5000: episode: 174, duration: 0.231s, episode steps:  21, steps per second:  91, episode reward: 38.900, mean reward:  1.852 [-2.347, 32.060], mean action: 4.714 [0.000, 14.000],  loss: 0.014531, mae: 0.409318, mean_q: 0.490220, mean_eps: 0.000000
 4987/5000: episode: 175, duration: 0.454s, episode steps:  39, steps per second:  86, episode reward: 42.000, mean reward:  1.077 [-2.431, 32.100], mean action: 3.205 [0.000, 13.000],  loss: 0.016552, mae: 0.420657, mean_q: 0.502620, mean_eps: 0.000000
done, took 53.115 seconds
DQN Evaluation: 443 victories out of 523 episodes
Training for 5000 steps ...
   20/5000: episode: 1, duration: 0.140s, episode steps:  20, steps per second: 142, episode reward: -35.910, mean reward: -1.795 [-32.511,  2.794], mean action: 5.150 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   38/5000: episode: 2, duration: 0.119s, episode steps:  18, steps per second: 152, episode reward: 38.288, mean reward:  2.127 [-3.000, 32.542], mean action: 4.611 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   59/5000: episode: 3, duration: 0.138s, episode steps:  21, steps per second: 153, episode reward: 35.385, mean reward:  1.685 [-2.535, 32.210], mean action: 4.095 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   95/5000: episode: 4, duration: 0.221s, episode steps:  36, steps per second: 163, episode reward: 35.921, mean reward:  0.998 [-2.198, 31.968], mean action: 5.972 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  121/5000: episode: 5, duration: 0.158s, episode steps:  26, steps per second: 164, episode reward: -35.330, mean reward: -1.359 [-31.943,  2.340], mean action: 7.038 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  134/5000: episode: 6, duration: 0.239s, episode steps:  13, steps per second:  54, episode reward: 41.552, mean reward:  3.196 [-3.000, 31.562], mean action: 4.077 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  154/5000: episode: 7, duration: 0.128s, episode steps:  20, steps per second: 156, episode reward: 41.030, mean reward:  2.052 [-2.158, 32.010], mean action: 3.850 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  179/5000: episode: 8, duration: 0.163s, episode steps:  25, steps per second: 154, episode reward: 36.000, mean reward:  1.440 [-2.416, 32.280], mean action: 4.200 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  206/5000: episode: 9, duration: 0.167s, episode steps:  27, steps per second: 162, episode reward: -35.810, mean reward: -1.326 [-32.054,  3.000], mean action: 9.259 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  226/5000: episode: 10, duration: 0.133s, episode steps:  20, steps per second: 150, episode reward: 38.045, mean reward:  1.902 [-2.486, 32.210], mean action: 4.700 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  240/5000: episode: 11, duration: 0.096s, episode steps:  14, steps per second: 146, episode reward: 41.680, mean reward:  2.977 [-2.092, 32.260], mean action: 3.214 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  311/5000: episode: 12, duration: 0.409s, episode steps:  71, steps per second: 174, episode reward: -32.200, mean reward: -0.454 [-32.352,  2.780], mean action: 4.141 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  330/5000: episode: 13, duration: 0.129s, episode steps:  19, steps per second: 147, episode reward: 44.722, mean reward:  2.354 [-2.224, 32.309], mean action: 0.947 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  353/5000: episode: 14, duration: 0.146s, episode steps:  23, steps per second: 158, episode reward: -33.000, mean reward: -1.435 [-32.408,  2.444], mean action: 4.304 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  380/5000: episode: 15, duration: 0.164s, episode steps:  27, steps per second: 164, episode reward: 30.000, mean reward:  1.111 [-3.000, 30.431], mean action: 6.889 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  400/5000: episode: 16, duration: 0.125s, episode steps:  20, steps per second: 160, episode reward: 32.763, mean reward:  1.638 [-3.000, 32.952], mean action: 6.000 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  416/5000: episode: 17, duration: 0.110s, episode steps:  16, steps per second: 145, episode reward: 38.208, mean reward:  2.388 [-2.539, 32.551], mean action: 3.438 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  439/5000: episode: 18, duration: 0.143s, episode steps:  23, steps per second: 161, episode reward: 35.500, mean reward:  1.543 [-3.000, 32.020], mean action: 2.957 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  469/5000: episode: 19, duration: 0.185s, episode steps:  30, steps per second: 162, episode reward: -35.610, mean reward: -1.187 [-32.081,  3.062], mean action: 5.167 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  494/5000: episode: 20, duration: 0.158s, episode steps:  25, steps per second: 158, episode reward: 35.084, mean reward:  1.403 [-2.256, 32.554], mean action: 4.840 [1.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  514/5000: episode: 21, duration: 0.132s, episode steps:  20, steps per second: 152, episode reward: 35.386, mean reward:  1.769 [-3.000, 32.250], mean action: 3.550 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  534/5000: episode: 22, duration: 0.130s, episode steps:  20, steps per second: 153, episode reward: 41.764, mean reward:  2.088 [-2.380, 32.910], mean action: 3.350 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  547/5000: episode: 23, duration: 0.090s, episode steps:  13, steps per second: 144, episode reward: 44.543, mean reward:  3.426 [-2.555, 33.000], mean action: 2.154 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  572/5000: episode: 24, duration: 0.154s, episode steps:  25, steps per second: 162, episode reward: 32.511, mean reward:  1.300 [-3.000, 32.009], mean action: 5.720 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  591/5000: episode: 25, duration: 0.124s, episode steps:  19, steps per second: 154, episode reward: 35.094, mean reward:  1.847 [-3.000, 32.839], mean action: 5.316 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  606/5000: episode: 26, duration: 0.105s, episode steps:  15, steps per second: 143, episode reward: 38.853, mean reward:  2.590 [-3.000, 31.983], mean action: 3.733 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  631/5000: episode: 27, duration: 0.159s, episode steps:  25, steps per second: 157, episode reward: 33.000, mean reward:  1.320 [-2.411, 32.030], mean action: 3.480 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  649/5000: episode: 28, duration: 0.120s, episode steps:  18, steps per second: 150, episode reward: 35.771, mean reward:  1.987 [-3.000, 33.000], mean action: 4.722 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  674/5000: episode: 29, duration: 0.155s, episode steps:  25, steps per second: 161, episode reward: 35.285, mean reward:  1.411 [-2.402, 32.360], mean action: 3.360 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  700/5000: episode: 30, duration: 0.168s, episode steps:  26, steps per second: 155, episode reward: 34.785, mean reward:  1.338 [-2.422, 32.080], mean action: 6.423 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  721/5000: episode: 31, duration: 0.137s, episode steps:  21, steps per second: 154, episode reward: 41.057, mean reward:  1.955 [-2.305, 32.090], mean action: 2.143 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  752/5000: episode: 32, duration: 0.189s, episode steps:  31, steps per second: 164, episode reward: 32.268, mean reward:  1.041 [-2.364, 32.013], mean action: 5.258 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  772/5000: episode: 33, duration: 0.128s, episode steps:  20, steps per second: 157, episode reward: 38.218, mean reward:  1.911 [-2.260, 32.218], mean action: 2.750 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  794/5000: episode: 34, duration: 0.181s, episode steps:  22, steps per second: 121, episode reward: 38.903, mean reward:  1.768 [-2.541, 32.173], mean action: 3.000 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  823/5000: episode: 35, duration: 0.197s, episode steps:  29, steps per second: 147, episode reward: -32.910, mean reward: -1.135 [-32.074,  2.210], mean action: 5.172 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  844/5000: episode: 36, duration: 0.135s, episode steps:  21, steps per second: 155, episode reward: 38.862, mean reward:  1.851 [-2.160, 32.500], mean action: 2.190 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  878/5000: episode: 37, duration: 0.221s, episode steps:  34, steps per second: 154, episode reward: 33.000, mean reward:  0.971 [-2.530, 33.000], mean action: 2.971 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  908/5000: episode: 38, duration: 0.182s, episode steps:  30, steps per second: 165, episode reward: 34.211, mean reward:  1.140 [-3.000, 32.360], mean action: 5.967 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  929/5000: episode: 39, duration: 0.137s, episode steps:  21, steps per second: 153, episode reward: 35.508, mean reward:  1.691 [-2.817, 32.040], mean action: 2.429 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  953/5000: episode: 40, duration: 0.149s, episode steps:  24, steps per second: 161, episode reward: 38.525, mean reward:  1.605 [-2.389, 32.115], mean action: 4.958 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  999/5000: episode: 41, duration: 0.260s, episode steps:  46, steps per second: 177, episode reward: 35.753, mean reward:  0.777 [-2.517, 32.426], mean action: 4.261 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1018/5000: episode: 42, duration: 0.218s, episode steps:  19, steps per second:  87, episode reward: 39.000, mean reward:  2.053 [-3.000, 32.510], mean action: 2.789 [0.000, 9.000],  loss: 0.020314, mae: 0.438079, mean_q: 0.489916, mean_eps: 0.000000
 1041/5000: episode: 43, duration: 0.265s, episode steps:  23, steps per second:  87, episode reward: 37.991, mean reward:  1.652 [-2.388, 32.431], mean action: 6.522 [0.000, 20.000],  loss: 0.017575, mae: 0.429774, mean_q: 0.477733, mean_eps: 0.000000
 1067/5000: episode: 44, duration: 0.281s, episode steps:  26, steps per second:  93, episode reward: -36.000, mean reward: -1.385 [-32.639,  3.000], mean action: 9.154 [0.000, 20.000],  loss: 0.019732, mae: 0.443323, mean_q: 0.536369, mean_eps: 0.000000
 1093/5000: episode: 45, duration: 0.286s, episode steps:  26, steps per second:  91, episode reward: 39.000, mean reward:  1.500 [-2.404, 32.530], mean action: 2.346 [0.000, 12.000],  loss: 0.019082, mae: 0.431066, mean_q: 0.476228, mean_eps: 0.000000
 1123/5000: episode: 46, duration: 0.342s, episode steps:  30, steps per second:  88, episode reward: 37.471, mean reward:  1.249 [-2.291, 32.010], mean action: 5.333 [0.000, 20.000],  loss: 0.017708, mae: 0.437348, mean_q: 0.444607, mean_eps: 0.000000
 1150/5000: episode: 47, duration: 0.293s, episode steps:  27, steps per second:  92, episode reward: 35.050, mean reward:  1.298 [-2.450, 32.380], mean action: 5.333 [1.000, 15.000],  loss: 0.020029, mae: 0.448291, mean_q: 0.481425, mean_eps: 0.000000
 1174/5000: episode: 48, duration: 0.274s, episode steps:  24, steps per second:  88, episode reward: 40.386, mean reward:  1.683 [-2.310, 32.180], mean action: 4.083 [0.000, 20.000],  loss: 0.019256, mae: 0.445285, mean_q: 0.476408, mean_eps: 0.000000
 1191/5000: episode: 49, duration: 0.194s, episode steps:  17, steps per second:  88, episode reward: 41.239, mean reward:  2.426 [-2.123, 31.938], mean action: 3.647 [0.000, 20.000],  loss: 0.016649, mae: 0.434817, mean_q: 0.487236, mean_eps: 0.000000
 1218/5000: episode: 50, duration: 0.299s, episode steps:  27, steps per second:  90, episode reward: 35.368, mean reward:  1.310 [-2.944, 32.080], mean action: 4.704 [0.000, 14.000],  loss: 0.019953, mae: 0.454802, mean_q: 0.444468, mean_eps: 0.000000
 1240/5000: episode: 51, duration: 0.291s, episode steps:  22, steps per second:  76, episode reward: 38.072, mean reward:  1.731 [-2.638, 32.470], mean action: 5.182 [0.000, 15.000],  loss: 0.018892, mae: 0.436537, mean_q: 0.509641, mean_eps: 0.000000
 1264/5000: episode: 52, duration: 0.264s, episode steps:  24, steps per second:  91, episode reward: 35.800, mean reward:  1.492 [-2.174, 32.350], mean action: 4.292 [0.000, 15.000],  loss: 0.019725, mae: 0.432712, mean_q: 0.528354, mean_eps: 0.000000
 1278/5000: episode: 53, duration: 0.161s, episode steps:  14, steps per second:  87, episode reward: 42.000, mean reward:  3.000 [-2.348, 32.180], mean action: 3.000 [0.000, 12.000],  loss: 0.015664, mae: 0.419214, mean_q: 0.472926, mean_eps: 0.000000
 1305/5000: episode: 54, duration: 0.296s, episode steps:  27, steps per second:  91, episode reward: 32.539, mean reward:  1.205 [-3.000, 32.090], mean action: 5.852 [0.000, 18.000],  loss: 0.019317, mae: 0.451204, mean_q: 0.439258, mean_eps: 0.000000
 1327/5000: episode: 55, duration: 0.233s, episode steps:  22, steps per second:  94, episode reward: 35.898, mean reward:  1.632 [-3.000, 32.660], mean action: 4.636 [0.000, 15.000],  loss: 0.019190, mae: 0.448250, mean_q: 0.430517, mean_eps: 0.000000
 1347/5000: episode: 56, duration: 0.220s, episode steps:  20, steps per second:  91, episode reward: 35.898, mean reward:  1.795 [-2.632, 32.910], mean action: 5.700 [1.000, 14.000],  loss: 0.018752, mae: 0.446903, mean_q: 0.457576, mean_eps: 0.000000
 1362/5000: episode: 57, duration: 0.162s, episode steps:  15, steps per second:  93, episode reward: -44.550, mean reward: -2.970 [-33.000,  2.240], mean action: 6.467 [0.000, 15.000],  loss: 0.017091, mae: 0.431832, mean_q: 0.513451, mean_eps: 0.000000
 1380/5000: episode: 58, duration: 0.199s, episode steps:  18, steps per second:  91, episode reward: 41.636, mean reward:  2.313 [-2.760, 32.136], mean action: 3.000 [1.000, 15.000],  loss: 0.016579, mae: 0.429561, mean_q: 0.499374, mean_eps: 0.000000
 1406/5000: episode: 59, duration: 0.287s, episode steps:  26, steps per second:  91, episode reward: 36.000, mean reward:  1.385 [-3.000, 30.660], mean action: 3.154 [0.000, 15.000],  loss: 0.020960, mae: 0.453323, mean_q: 0.501992, mean_eps: 0.000000
 1425/5000: episode: 60, duration: 0.215s, episode steps:  19, steps per second:  88, episode reward: 44.803, mean reward:  2.358 [-2.198, 33.000], mean action: 6.105 [3.000, 14.000],  loss: 0.016936, mae: 0.435300, mean_q: 0.496849, mean_eps: 0.000000
 1451/5000: episode: 61, duration: 0.280s, episode steps:  26, steps per second:  93, episode reward: -35.180, mean reward: -1.353 [-31.820,  2.952], mean action: 5.692 [0.000, 20.000],  loss: 0.022651, mae: 0.447154, mean_q: 0.524729, mean_eps: 0.000000
 1473/5000: episode: 62, duration: 0.244s, episode steps:  22, steps per second:  90, episode reward: 35.114, mean reward:  1.596 [-2.717, 32.811], mean action: 4.955 [0.000, 14.000],  loss: 0.021427, mae: 0.441398, mean_q: 0.507684, mean_eps: 0.000000
 1497/5000: episode: 63, duration: 0.266s, episode steps:  24, steps per second:  90, episode reward: -32.800, mean reward: -1.367 [-31.967,  2.686], mean action: 3.917 [0.000, 11.000],  loss: 0.022457, mae: 0.455739, mean_q: 0.441047, mean_eps: 0.000000
 1519/5000: episode: 64, duration: 0.253s, episode steps:  22, steps per second:  87, episode reward: 35.450, mean reward:  1.611 [-3.000, 32.721], mean action: 5.545 [0.000, 14.000],  loss: 0.016499, mae: 0.415322, mean_q: 0.482866, mean_eps: 0.000000
 1539/5000: episode: 65, duration: 0.223s, episode steps:  20, steps per second:  90, episode reward: 35.088, mean reward:  1.754 [-3.000, 31.948], mean action: 3.300 [0.000, 14.000],  loss: 0.017853, mae: 0.434394, mean_q: 0.481677, mean_eps: 0.000000
 1558/5000: episode: 66, duration: 0.212s, episode steps:  19, steps per second:  89, episode reward: 41.106, mean reward:  2.163 [-2.496, 32.165], mean action: 2.842 [0.000, 9.000],  loss: 0.017560, mae: 0.432933, mean_q: 0.445176, mean_eps: 0.000000
 1583/5000: episode: 67, duration: 0.280s, episode steps:  25, steps per second:  89, episode reward: 35.411, mean reward:  1.416 [-3.000, 32.037], mean action: 5.560 [0.000, 18.000],  loss: 0.017556, mae: 0.416468, mean_q: 0.547086, mean_eps: 0.000000
 1605/5000: episode: 68, duration: 0.244s, episode steps:  22, steps per second:  90, episode reward: 35.809, mean reward:  1.628 [-2.424, 32.952], mean action: 4.409 [0.000, 15.000],  loss: 0.017391, mae: 0.411540, mean_q: 0.544368, mean_eps: 0.000000
 1624/5000: episode: 69, duration: 0.207s, episode steps:  19, steps per second:  92, episode reward: 41.134, mean reward:  2.165 [-2.404, 33.000], mean action: 5.053 [0.000, 15.000],  loss: 0.020449, mae: 0.434882, mean_q: 0.506614, mean_eps: 0.000000
 1652/5000: episode: 70, duration: 0.308s, episode steps:  28, steps per second:  91, episode reward: 35.284, mean reward:  1.260 [-2.876, 32.160], mean action: 4.821 [0.000, 15.000],  loss: 0.020774, mae: 0.436157, mean_q: 0.538849, mean_eps: 0.000000
 1673/5000: episode: 71, duration: 0.250s, episode steps:  21, steps per second:  84, episode reward: -35.740, mean reward: -1.702 [-33.000,  2.690], mean action: 6.714 [0.000, 19.000],  loss: 0.020186, mae: 0.428435, mean_q: 0.506181, mean_eps: 0.000000
 1716/5000: episode: 72, duration: 0.484s, episode steps:  43, steps per second:  89, episode reward: -32.450, mean reward: -0.755 [-31.637,  2.687], mean action: 8.023 [0.000, 20.000],  loss: 0.017129, mae: 0.439833, mean_q: 0.470483, mean_eps: 0.000000
 1746/5000: episode: 73, duration: 0.320s, episode steps:  30, steps per second:  94, episode reward: -32.200, mean reward: -1.073 [-32.008,  2.431], mean action: 6.067 [0.000, 19.000],  loss: 0.016191, mae: 0.421386, mean_q: 0.513267, mean_eps: 0.000000
 1766/5000: episode: 74, duration: 0.229s, episode steps:  20, steps per second:  87, episode reward: 44.232, mean reward:  2.212 [-2.502, 32.080], mean action: 4.750 [1.000, 13.000],  loss: 0.017681, mae: 0.439148, mean_q: 0.476785, mean_eps: 0.000000
 1797/5000: episode: 75, duration: 0.393s, episode steps:  31, steps per second:  79, episode reward: -32.150, mean reward: -1.037 [-31.946,  2.279], mean action: 6.387 [0.000, 15.000],  loss: 0.016855, mae: 0.430511, mean_q: 0.486241, mean_eps: 0.000000
 1812/5000: episode: 76, duration: 0.177s, episode steps:  15, steps per second:  85, episode reward: 36.000, mean reward:  2.400 [-3.000, 29.590], mean action: 3.800 [0.000, 15.000],  loss: 0.021624, mae: 0.447053, mean_q: 0.555649, mean_eps: 0.000000
 1832/5000: episode: 77, duration: 0.228s, episode steps:  20, steps per second:  88, episode reward: 32.264, mean reward:  1.613 [-3.000, 32.529], mean action: 8.350 [1.000, 20.000],  loss: 0.019661, mae: 0.429560, mean_q: 0.538613, mean_eps: 0.000000
 1895/5000: episode: 78, duration: 0.679s, episode steps:  63, steps per second:  93, episode reward: -32.430, mean reward: -0.515 [-31.765,  2.282], mean action: 12.079 [0.000, 21.000],  loss: 0.020540, mae: 0.449988, mean_q: 0.482849, mean_eps: 0.000000
 1918/5000: episode: 79, duration: 0.249s, episode steps:  23, steps per second:  92, episode reward: 37.757, mean reward:  1.642 [-3.000, 32.070], mean action: 3.739 [0.000, 20.000],  loss: 0.021976, mae: 0.453426, mean_q: 0.522547, mean_eps: 0.000000
 1943/5000: episode: 80, duration: 0.275s, episode steps:  25, steps per second:  91, episode reward: 36.000, mean reward:  1.440 [-2.629, 32.350], mean action: 3.720 [0.000, 19.000],  loss: 0.021659, mae: 0.469694, mean_q: 0.476845, mean_eps: 0.000000
 1965/5000: episode: 81, duration: 0.240s, episode steps:  22, steps per second:  92, episode reward: 32.613, mean reward:  1.482 [-3.000, 32.164], mean action: 5.273 [0.000, 19.000],  loss: 0.017760, mae: 0.454663, mean_q: 0.397232, mean_eps: 0.000000
 1990/5000: episode: 82, duration: 0.274s, episode steps:  25, steps per second:  91, episode reward: -36.000, mean reward: -1.440 [-32.171,  2.300], mean action: 6.440 [0.000, 19.000],  loss: 0.019736, mae: 0.451811, mean_q: 0.490304, mean_eps: 0.000000
 2014/5000: episode: 83, duration: 0.274s, episode steps:  24, steps per second:  87, episode reward: 37.089, mean reward:  1.545 [-3.000, 32.250], mean action: 4.667 [0.000, 15.000],  loss: 0.021964, mae: 0.463501, mean_q: 0.449635, mean_eps: 0.000000
 2043/5000: episode: 84, duration: 0.313s, episode steps:  29, steps per second:  93, episode reward: 34.743, mean reward:  1.198 [-2.744, 32.650], mean action: 7.138 [0.000, 20.000],  loss: 0.019989, mae: 0.457585, mean_q: 0.457943, mean_eps: 0.000000
 2068/5000: episode: 85, duration: 0.280s, episode steps:  25, steps per second:  89, episode reward: 35.827, mean reward:  1.433 [-3.000, 32.810], mean action: 4.680 [0.000, 15.000],  loss: 0.015933, mae: 0.427068, mean_q: 0.449262, mean_eps: 0.000000
 2089/5000: episode: 86, duration: 0.249s, episode steps:  21, steps per second:  84, episode reward: 40.718, mean reward:  1.939 [-2.058, 32.400], mean action: 2.286 [0.000, 12.000],  loss: 0.020896, mae: 0.453594, mean_q: 0.443566, mean_eps: 0.000000
 2112/5000: episode: 87, duration: 0.301s, episode steps:  23, steps per second:  77, episode reward: 35.323, mean reward:  1.536 [-2.902, 32.168], mean action: 3.652 [0.000, 12.000],  loss: 0.021176, mae: 0.456142, mean_q: 0.450959, mean_eps: 0.000000
 2131/5000: episode: 88, duration: 0.219s, episode steps:  19, steps per second:  87, episode reward: 38.333, mean reward:  2.018 [-3.000, 31.958], mean action: 4.579 [0.000, 15.000],  loss: 0.017469, mae: 0.439485, mean_q: 0.429218, mean_eps: 0.000000
 2152/5000: episode: 89, duration: 0.233s, episode steps:  21, steps per second:  90, episode reward: 38.655, mean reward:  1.841 [-2.549, 32.310], mean action: 4.095 [0.000, 14.000],  loss: 0.016553, mae: 0.439982, mean_q: 0.442072, mean_eps: 0.000000
 2181/5000: episode: 90, duration: 0.317s, episode steps:  29, steps per second:  92, episode reward: 36.000, mean reward:  1.241 [-2.952, 32.260], mean action: 4.310 [1.000, 15.000],  loss: 0.017384, mae: 0.429002, mean_q: 0.502316, mean_eps: 0.000000
 2206/5000: episode: 91, duration: 0.284s, episode steps:  25, steps per second:  88, episode reward: 38.134, mean reward:  1.525 [-2.449, 32.095], mean action: 3.480 [0.000, 14.000],  loss: 0.017558, mae: 0.432334, mean_q: 0.494589, mean_eps: 0.000000
 2229/5000: episode: 92, duration: 0.255s, episode steps:  23, steps per second:  90, episode reward: 35.363, mean reward:  1.538 [-3.000, 32.220], mean action: 5.174 [0.000, 21.000],  loss: 0.021306, mae: 0.437814, mean_q: 0.491307, mean_eps: 0.000000
 2265/5000: episode: 93, duration: 0.397s, episode steps:  36, steps per second:  91, episode reward: 32.681, mean reward:  0.908 [-2.459, 29.765], mean action: 7.361 [0.000, 21.000],  loss: 0.019522, mae: 0.436234, mean_q: 0.526042, mean_eps: 0.000000
 2291/5000: episode: 94, duration: 0.282s, episode steps:  26, steps per second:  92, episode reward: 38.496, mean reward:  1.481 [-2.903, 31.876], mean action: 3.231 [0.000, 9.000],  loss: 0.017357, mae: 0.432876, mean_q: 0.461818, mean_eps: 0.000000
 2315/5000: episode: 95, duration: 0.262s, episode steps:  24, steps per second:  91, episode reward: -32.930, mean reward: -1.372 [-32.093,  2.571], mean action: 4.542 [0.000, 14.000],  loss: 0.021104, mae: 0.442839, mean_q: 0.489034, mean_eps: 0.000000
 2345/5000: episode: 96, duration: 0.321s, episode steps:  30, steps per second:  93, episode reward: 35.430, mean reward:  1.181 [-2.930, 32.309], mean action: 5.933 [0.000, 15.000],  loss: 0.016560, mae: 0.425787, mean_q: 0.488458, mean_eps: 0.000000
 2358/5000: episode: 97, duration: 0.155s, episode steps:  13, steps per second:  84, episode reward: 41.635, mean reward:  3.203 [-2.403, 32.041], mean action: 4.923 [0.000, 15.000],  loss: 0.018504, mae: 0.437043, mean_q: 0.459069, mean_eps: 0.000000
 2380/5000: episode: 98, duration: 0.239s, episode steps:  22, steps per second:  92, episode reward: 35.152, mean reward:  1.598 [-3.000, 33.000], mean action: 6.091 [0.000, 15.000],  loss: 0.020598, mae: 0.451877, mean_q: 0.401644, mean_eps: 0.000000
 2403/5000: episode: 99, duration: 0.319s, episode steps:  23, steps per second:  72, episode reward: -32.270, mean reward: -1.403 [-32.163,  2.903], mean action: 8.696 [0.000, 20.000],  loss: 0.019098, mae: 0.437569, mean_q: 0.424994, mean_eps: 0.000000
 2426/5000: episode: 100, duration: 0.253s, episode steps:  23, steps per second:  91, episode reward: 32.786, mean reward:  1.425 [-2.901, 32.786], mean action: 4.696 [0.000, 19.000],  loss: 0.018478, mae: 0.426848, mean_q: 0.518274, mean_eps: 0.000000
 2461/5000: episode: 101, duration: 0.372s, episode steps:  35, steps per second:  94, episode reward: -35.320, mean reward: -1.009 [-32.018,  2.480], mean action: 9.657 [0.000, 15.000],  loss: 0.019845, mae: 0.436168, mean_q: 0.497534, mean_eps: 0.000000
 2486/5000: episode: 102, duration: 0.285s, episode steps:  25, steps per second:  88, episode reward: -33.000, mean reward: -1.320 [-32.243,  2.340], mean action: 7.640 [0.000, 21.000],  loss: 0.024367, mae: 0.471794, mean_q: 0.471320, mean_eps: 0.000000
 2506/5000: episode: 103, duration: 0.222s, episode steps:  20, steps per second:  90, episode reward: 39.000, mean reward:  1.950 [-2.528, 32.470], mean action: 3.700 [1.000, 14.000],  loss: 0.021275, mae: 0.448988, mean_q: 0.472267, mean_eps: 0.000000
 2535/5000: episode: 104, duration: 0.315s, episode steps:  29, steps per second:  92, episode reward: 32.086, mean reward:  1.106 [-2.480, 32.080], mean action: 12.724 [1.000, 21.000],  loss: 0.017278, mae: 0.432120, mean_q: 0.438206, mean_eps: 0.000000
 2555/5000: episode: 105, duration: 0.229s, episode steps:  20, steps per second:  87, episode reward: 38.775, mean reward:  1.939 [-2.683, 32.390], mean action: 2.800 [0.000, 12.000],  loss: 0.018890, mae: 0.427788, mean_q: 0.497997, mean_eps: 0.000000
 2581/5000: episode: 106, duration: 0.283s, episode steps:  26, steps per second:  92, episode reward: -32.400, mean reward: -1.246 [-32.116,  3.000], mean action: 4.500 [0.000, 15.000],  loss: 0.019152, mae: 0.433945, mean_q: 0.493942, mean_eps: 0.000000
 2604/5000: episode: 107, duration: 0.247s, episode steps:  23, steps per second:  93, episode reward: -32.900, mean reward: -1.430 [-32.879,  2.962], mean action: 4.826 [0.000, 15.000],  loss: 0.018630, mae: 0.437140, mean_q: 0.460965, mean_eps: 0.000000
 2623/5000: episode: 108, duration: 0.211s, episode steps:  19, steps per second:  90, episode reward: 33.000, mean reward:  1.737 [-3.000, 32.360], mean action: 5.053 [0.000, 15.000],  loss: 0.021779, mae: 0.448838, mean_q: 0.483164, mean_eps: 0.000000
 2643/5000: episode: 109, duration: 0.226s, episode steps:  20, steps per second:  88, episode reward: 33.000, mean reward:  1.650 [-3.000, 32.270], mean action: 5.550 [0.000, 15.000],  loss: 0.020608, mae: 0.442471, mean_q: 0.454784, mean_eps: 0.000000
 2665/5000: episode: 110, duration: 0.244s, episode steps:  22, steps per second:  90, episode reward: -32.670, mean reward: -1.485 [-32.670,  2.900], mean action: 5.455 [0.000, 20.000],  loss: 0.026420, mae: 0.466682, mean_q: 0.491117, mean_eps: 0.000000
 2680/5000: episode: 111, duration: 0.174s, episode steps:  15, steps per second:  86, episode reward: 45.000, mean reward:  3.000 [-2.072, 32.504], mean action: 2.667 [0.000, 9.000],  loss: 0.014895, mae: 0.403356, mean_q: 0.531709, mean_eps: 0.000000
 2711/5000: episode: 112, duration: 0.330s, episode steps:  31, steps per second:  94, episode reward: -32.200, mean reward: -1.039 [-32.028,  2.240], mean action: 7.935 [0.000, 20.000],  loss: 0.018991, mae: 0.428674, mean_q: 0.459509, mean_eps: 0.000000
 2727/5000: episode: 113, duration: 0.172s, episode steps:  16, steps per second:  93, episode reward: -44.670, mean reward: -2.792 [-33.000,  2.320], mean action: 7.312 [0.000, 14.000],  loss: 0.019764, mae: 0.432642, mean_q: 0.521517, mean_eps: 0.000000
 2745/5000: episode: 114, duration: 0.199s, episode steps:  18, steps per second:  90, episode reward: 39.000, mean reward:  2.167 [-2.514, 32.180], mean action: 2.944 [0.000, 9.000],  loss: 0.016825, mae: 0.423611, mean_q: 0.549195, mean_eps: 0.000000
 2766/5000: episode: 115, duration: 0.229s, episode steps:  21, steps per second:  92, episode reward: -33.000, mean reward: -1.571 [-30.000,  2.314], mean action: 5.714 [0.000, 15.000],  loss: 0.018173, mae: 0.446429, mean_q: 0.444954, mean_eps: 0.000000
 2789/5000: episode: 116, duration: 0.263s, episode steps:  23, steps per second:  87, episode reward: 35.420, mean reward:  1.540 [-2.346, 32.110], mean action: 7.609 [0.000, 21.000],  loss: 0.015844, mae: 0.431093, mean_q: 0.419456, mean_eps: 0.000000
 2813/5000: episode: 117, duration: 0.263s, episode steps:  24, steps per second:  91, episode reward: -35.520, mean reward: -1.480 [-33.000,  3.000], mean action: 4.000 [0.000, 12.000],  loss: 0.020443, mae: 0.431759, mean_q: 0.496885, mean_eps: 0.000000
 2835/5000: episode: 118, duration: 0.244s, episode steps:  22, steps per second:  90, episode reward: 35.216, mean reward:  1.601 [-2.660, 32.490], mean action: 3.273 [0.000, 12.000],  loss: 0.019528, mae: 0.426866, mean_q: 0.476327, mean_eps: 0.000000
 2858/5000: episode: 119, duration: 0.255s, episode steps:  23, steps per second:  90, episode reward: 38.195, mean reward:  1.661 [-2.242, 31.570], mean action: 4.217 [0.000, 15.000],  loss: 0.019985, mae: 0.428335, mean_q: 0.467011, mean_eps: 0.000000
 2880/5000: episode: 120, duration: 0.235s, episode steps:  22, steps per second:  93, episode reward: 36.000, mean reward:  1.636 [-2.292, 32.110], mean action: 5.182 [1.000, 12.000],  loss: 0.016518, mae: 0.421886, mean_q: 0.440212, mean_eps: 0.000000
 2898/5000: episode: 121, duration: 0.211s, episode steps:  18, steps per second:  85, episode reward: 40.774, mean reward:  2.265 [-2.302, 32.160], mean action: 3.444 [0.000, 12.000],  loss: 0.019465, mae: 0.431243, mean_q: 0.452140, mean_eps: 0.000000
 2916/5000: episode: 122, duration: 0.202s, episode steps:  18, steps per second:  89, episode reward: -32.260, mean reward: -1.792 [-32.410,  3.000], mean action: 5.556 [0.000, 19.000],  loss: 0.019717, mae: 0.428493, mean_q: 0.496916, mean_eps: 0.000000
 2934/5000: episode: 123, duration: 0.194s, episode steps:  18, steps per second:  93, episode reward: -38.790, mean reward: -2.155 [-32.146,  2.309], mean action: 7.000 [0.000, 15.000],  loss: 0.016142, mae: 0.420458, mean_q: 0.432109, mean_eps: 0.000000
 2964/5000: episode: 124, duration: 0.392s, episode steps:  30, steps per second:  76, episode reward: 33.000, mean reward:  1.100 [-3.000, 32.850], mean action: 5.933 [0.000, 19.000],  loss: 0.019639, mae: 0.436677, mean_q: 0.451345, mean_eps: 0.000000
 3006/5000: episode: 125, duration: 0.462s, episode steps:  42, steps per second:  91, episode reward: 32.558, mean reward:  0.775 [-2.470, 32.080], mean action: 7.048 [0.000, 15.000],  loss: 0.019610, mae: 0.435258, mean_q: 0.476877, mean_eps: 0.000000
 3038/5000: episode: 126, duration: 0.339s, episode steps:  32, steps per second:  94, episode reward: 35.365, mean reward:  1.105 [-2.235, 31.935], mean action: 5.188 [0.000, 15.000],  loss: 0.018603, mae: 0.428333, mean_q: 0.520798, mean_eps: 0.000000
 3061/5000: episode: 127, duration: 0.255s, episode steps:  23, steps per second:  90, episode reward: 35.387, mean reward:  1.539 [-3.000, 32.440], mean action: 3.261 [0.000, 15.000],  loss: 0.018116, mae: 0.424470, mean_q: 0.460506, mean_eps: 0.000000
 3098/5000: episode: 128, duration: 0.410s, episode steps:  37, steps per second:  90, episode reward: 32.901, mean reward:  0.889 [-2.465, 32.951], mean action: 3.189 [0.000, 15.000],  loss: 0.020516, mae: 0.427981, mean_q: 0.508391, mean_eps: 0.000000
 3116/5000: episode: 129, duration: 0.211s, episode steps:  18, steps per second:  85, episode reward: 37.676, mean reward:  2.093 [-2.724, 31.794], mean action: 3.722 [0.000, 20.000],  loss: 0.016843, mae: 0.432887, mean_q: 0.407180, mean_eps: 0.000000
 3144/5000: episode: 130, duration: 0.309s, episode steps:  28, steps per second:  91, episode reward: 35.494, mean reward:  1.268 [-2.290, 32.483], mean action: 5.964 [0.000, 20.000],  loss: 0.018809, mae: 0.432402, mean_q: 0.432210, mean_eps: 0.000000
 3166/5000: episode: 131, duration: 0.240s, episode steps:  22, steps per second:  92, episode reward: -41.910, mean reward: -1.905 [-32.261,  2.300], mean action: 6.045 [0.000, 20.000],  loss: 0.021733, mae: 0.449658, mean_q: 0.489446, mean_eps: 0.000000
 3184/5000: episode: 132, duration: 0.206s, episode steps:  18, steps per second:  88, episode reward: 35.563, mean reward:  1.976 [-2.900, 32.475], mean action: 4.000 [0.000, 12.000],  loss: 0.019242, mae: 0.428413, mean_q: 0.512116, mean_eps: 0.000000
 3203/5000: episode: 133, duration: 0.214s, episode steps:  19, steps per second:  89, episode reward: 37.732, mean reward:  1.986 [-2.309, 31.899], mean action: 4.211 [0.000, 13.000],  loss: 0.023526, mae: 0.457947, mean_q: 0.464100, mean_eps: 0.000000
 3226/5000: episode: 134, duration: 0.253s, episode steps:  23, steps per second:  91, episode reward: 33.000, mean reward:  1.435 [-2.103, 30.058], mean action: 3.261 [0.000, 11.000],  loss: 0.020368, mae: 0.436016, mean_q: 0.510930, mean_eps: 0.000000
 3247/5000: episode: 135, duration: 0.267s, episode steps:  21, steps per second:  79, episode reward: 33.000, mean reward:  1.571 [-3.000, 30.207], mean action: 2.571 [0.000, 9.000],  loss: 0.017956, mae: 0.421054, mean_q: 0.540224, mean_eps: 0.000000
 3276/5000: episode: 136, duration: 0.317s, episode steps:  29, steps per second:  91, episode reward: 36.000, mean reward:  1.241 [-2.900, 32.070], mean action: 1.483 [0.000, 14.000],  loss: 0.022316, mae: 0.442687, mean_q: 0.484271, mean_eps: 0.000000
 3324/5000: episode: 137, duration: 0.513s, episode steps:  48, steps per second:  94, episode reward: 32.332, mean reward:  0.674 [-2.328, 31.874], mean action: 12.542 [0.000, 21.000],  loss: 0.018585, mae: 0.430825, mean_q: 0.463813, mean_eps: 0.000000
 3355/5000: episode: 138, duration: 0.343s, episode steps:  31, steps per second:  90, episode reward: 32.861, mean reward:  1.060 [-2.780, 32.293], mean action: 10.839 [0.000, 21.000],  loss: 0.021130, mae: 0.447992, mean_q: 0.429808, mean_eps: 0.000000
 3377/5000: episode: 139, duration: 0.246s, episode steps:  22, steps per second:  90, episode reward: -32.470, mean reward: -1.476 [-31.784,  2.530], mean action: 5.364 [0.000, 15.000],  loss: 0.022015, mae: 0.441174, mean_q: 0.480317, mean_eps: 0.000000
 3416/5000: episode: 140, duration: 0.416s, episode steps:  39, steps per second:  94, episode reward: -38.050, mean reward: -0.976 [-32.152,  1.999], mean action: 10.795 [0.000, 17.000],  loss: 0.017169, mae: 0.413751, mean_q: 0.504399, mean_eps: 0.000000
 3443/5000: episode: 141, duration: 0.297s, episode steps:  27, steps per second:  91, episode reward: -32.350, mean reward: -1.198 [-31.653,  2.903], mean action: 6.407 [0.000, 20.000],  loss: 0.018427, mae: 0.434196, mean_q: 0.434294, mean_eps: 0.000000
 3462/5000: episode: 142, duration: 0.217s, episode steps:  19, steps per second:  88, episode reward: -39.000, mean reward: -2.053 [-32.901,  2.400], mean action: 7.947 [0.000, 15.000],  loss: 0.016785, mae: 0.423587, mean_q: 0.442061, mean_eps: 0.000000
 3469/5000: episode: 143, duration: 0.091s, episode steps:   7, steps per second:  77, episode reward: 47.201, mean reward:  6.743 [ 0.000, 32.201], mean action: 2.857 [0.000, 20.000],  loss: 0.018454, mae: 0.436806, mean_q: 0.431364, mean_eps: 0.000000
 3486/5000: episode: 144, duration: 0.192s, episode steps:  17, steps per second:  89, episode reward: -35.830, mean reward: -2.108 [-32.021,  2.770], mean action: 7.000 [0.000, 19.000],  loss: 0.022481, mae: 0.451014, mean_q: 0.415138, mean_eps: 0.000000
 3510/5000: episode: 145, duration: 0.341s, episode steps:  24, steps per second:  70, episode reward: 34.919, mean reward:  1.455 [-2.506, 31.800], mean action: 5.583 [0.000, 15.000],  loss: 0.019735, mae: 0.438944, mean_q: 0.459659, mean_eps: 0.000000
 3532/5000: episode: 146, duration: 0.241s, episode steps:  22, steps per second:  91, episode reward: 32.066, mean reward:  1.458 [-3.000, 32.849], mean action: 3.636 [0.000, 15.000],  loss: 0.021626, mae: 0.440419, mean_q: 0.548121, mean_eps: 0.000000
 3557/5000: episode: 147, duration: 0.277s, episode steps:  25, steps per second:  90, episode reward: 33.000, mean reward:  1.320 [-2.494, 32.670], mean action: 6.000 [0.000, 18.000],  loss: 0.021272, mae: 0.427140, mean_q: 0.545098, mean_eps: 0.000000
 3576/5000: episode: 148, duration: 0.217s, episode steps:  19, steps per second:  88, episode reward: 34.742, mean reward:  1.829 [-3.000, 32.042], mean action: 6.632 [0.000, 15.000],  loss: 0.021321, mae: 0.450442, mean_q: 0.485769, mean_eps: 0.000000
 3624/5000: episode: 149, duration: 0.515s, episode steps:  48, steps per second:  93, episode reward: -32.670, mean reward: -0.681 [-32.155,  3.000], mean action: 3.292 [0.000, 15.000],  loss: 0.018641, mae: 0.435577, mean_q: 0.452234, mean_eps: 0.000000
 3641/5000: episode: 150, duration: 0.204s, episode steps:  17, steps per second:  83, episode reward: 41.315, mean reward:  2.430 [-3.000, 32.193], mean action: 5.235 [0.000, 15.000],  loss: 0.018843, mae: 0.432798, mean_q: 0.460853, mean_eps: 0.000000
 3667/5000: episode: 151, duration: 0.286s, episode steps:  26, steps per second:  91, episode reward: 35.535, mean reward:  1.367 [-3.000, 32.270], mean action: 3.962 [0.000, 15.000],  loss: 0.019137, mae: 0.437327, mean_q: 0.455376, mean_eps: 0.000000
 3698/5000: episode: 152, duration: 0.334s, episode steps:  31, steps per second:  93, episode reward: 38.457, mean reward:  1.241 [-2.406, 31.975], mean action: 4.355 [1.000, 13.000],  loss: 0.018322, mae: 0.436060, mean_q: 0.439195, mean_eps: 0.000000
 3723/5000: episode: 153, duration: 0.270s, episode steps:  25, steps per second:  93, episode reward: -35.560, mean reward: -1.422 [-31.865,  2.241], mean action: 6.240 [0.000, 15.000],  loss: 0.023352, mae: 0.456580, mean_q: 0.469024, mean_eps: 0.000000
 3739/5000: episode: 154, duration: 0.193s, episode steps:  16, steps per second:  83, episode reward: 39.000, mean reward:  2.438 [-2.678, 33.000], mean action: 4.562 [0.000, 15.000],  loss: 0.021275, mae: 0.443406, mean_q: 0.494672, mean_eps: 0.000000
 3775/5000: episode: 155, duration: 0.386s, episode steps:  36, steps per second:  93, episode reward: 35.214, mean reward:  0.978 [-2.337, 32.016], mean action: 10.444 [0.000, 20.000],  loss: 0.020019, mae: 0.442495, mean_q: 0.512214, mean_eps: 0.000000
 3804/5000: episode: 156, duration: 0.318s, episode steps:  29, steps per second:  91, episode reward: 35.145, mean reward:  1.212 [-2.588, 31.995], mean action: 6.759 [0.000, 15.000],  loss: 0.020089, mae: 0.452142, mean_q: 0.460947, mean_eps: 0.000000
 3830/5000: episode: 157, duration: 0.285s, episode steps:  26, steps per second:  91, episode reward: -38.190, mean reward: -1.469 [-32.220,  2.704], mean action: 6.615 [0.000, 15.000],  loss: 0.017918, mae: 0.435945, mean_q: 0.482472, mean_eps: 0.000000
 3854/5000: episode: 158, duration: 0.263s, episode steps:  24, steps per second:  91, episode reward: 35.495, mean reward:  1.479 [-2.351, 32.470], mean action: 3.542 [0.000, 13.000],  loss: 0.020706, mae: 0.437723, mean_q: 0.579449, mean_eps: 0.000000
 3875/5000: episode: 159, duration: 0.248s, episode steps:  21, steps per second:  85, episode reward: 38.923, mean reward:  1.853 [-2.398, 32.760], mean action: 6.333 [0.000, 19.000],  loss: 0.016189, mae: 0.438348, mean_q: 0.470445, mean_eps: 0.000000
 3894/5000: episode: 160, duration: 0.207s, episode steps:  19, steps per second:  92, episode reward: -41.850, mean reward: -2.203 [-32.322,  2.260], mean action: 9.421 [1.000, 20.000],  loss: 0.023522, mae: 0.479062, mean_q: 0.450297, mean_eps: 0.000000
 3912/5000: episode: 161, duration: 0.204s, episode steps:  18, steps per second:  88, episode reward: -39.000, mean reward: -2.167 [-33.000,  3.000], mean action: 4.000 [0.000, 9.000],  loss: 0.022954, mae: 0.459499, mean_q: 0.498431, mean_eps: 0.000000
 3933/5000: episode: 162, duration: 0.228s, episode steps:  21, steps per second:  92, episode reward: 37.793, mean reward:  1.800 [-3.000, 32.180], mean action: 5.286 [1.000, 14.000],  loss: 0.016653, mae: 0.435839, mean_q: 0.493255, mean_eps: 0.000000
 3958/5000: episode: 163, duration: 0.274s, episode steps:  25, steps per second:  91, episode reward: 37.337, mean reward:  1.493 [-2.442, 31.944], mean action: 4.160 [0.000, 13.000],  loss: 0.019371, mae: 0.441256, mean_q: 0.510031, mean_eps: 0.000000
 3977/5000: episode: 164, duration: 0.214s, episode steps:  19, steps per second:  89, episode reward: 38.720, mean reward:  2.038 [-2.356, 32.070], mean action: 3.579 [0.000, 14.000],  loss: 0.017224, mae: 0.439099, mean_q: 0.448615, mean_eps: 0.000000
 4005/5000: episode: 165, duration: 0.317s, episode steps:  28, steps per second:  88, episode reward: -32.050, mean reward: -1.145 [-32.138,  2.900], mean action: 4.857 [0.000, 15.000],  loss: 0.016804, mae: 0.436520, mean_q: 0.460414, mean_eps: 0.000000
 4061/5000: episode: 166, duration: 0.593s, episode steps:  56, steps per second:  94, episode reward: -32.540, mean reward: -0.581 [-32.486,  2.430], mean action: 2.089 [0.000, 11.000],  loss: 0.019294, mae: 0.433220, mean_q: 0.496152, mean_eps: 0.000000
 4095/5000: episode: 167, duration: 0.434s, episode steps:  34, steps per second:  78, episode reward: 32.698, mean reward:  0.962 [-2.580, 33.000], mean action: 4.882 [0.000, 19.000],  loss: 0.019484, mae: 0.449143, mean_q: 0.474939, mean_eps: 0.000000
 4118/5000: episode: 168, duration: 0.252s, episode steps:  23, steps per second:  91, episode reward: -38.420, mean reward: -1.670 [-33.000,  2.780], mean action: 6.652 [0.000, 20.000],  loss: 0.020122, mae: 0.470764, mean_q: 0.418527, mean_eps: 0.000000
 4142/5000: episode: 169, duration: 0.264s, episode steps:  24, steps per second:  91, episode reward: 36.000, mean reward:  1.500 [-2.312, 32.280], mean action: 3.375 [0.000, 9.000],  loss: 0.017193, mae: 0.473029, mean_q: 0.377727, mean_eps: 0.000000
 4171/5000: episode: 170, duration: 0.324s, episode steps:  29, steps per second:  89, episode reward: 40.946, mean reward:  1.412 [-2.318, 32.470], mean action: 5.207 [0.000, 20.000],  loss: 0.020331, mae: 0.471321, mean_q: 0.407364, mean_eps: 0.000000
 4199/5000: episode: 171, duration: 0.308s, episode steps:  28, steps per second:  91, episode reward: 35.339, mean reward:  1.262 [-3.000, 32.230], mean action: 6.107 [0.000, 15.000],  loss: 0.018035, mae: 0.448926, mean_q: 0.476616, mean_eps: 0.000000
 4221/5000: episode: 172, duration: 0.257s, episode steps:  22, steps per second:  86, episode reward: 30.000, mean reward:  1.364 [-3.000, 30.195], mean action: 8.500 [0.000, 19.000],  loss: 0.015986, mae: 0.443905, mean_q: 0.463665, mean_eps: 0.000000
 4254/5000: episode: 173, duration: 0.359s, episode steps:  33, steps per second:  92, episode reward: -35.250, mean reward: -1.068 [-31.828,  2.269], mean action: 6.879 [1.000, 15.000],  loss: 0.020175, mae: 0.458489, mean_q: 0.453200, mean_eps: 0.000000
 4275/5000: episode: 174, duration: 0.240s, episode steps:  21, steps per second:  88, episode reward: 35.901, mean reward:  1.710 [-3.000, 32.901], mean action: 7.667 [0.000, 14.000],  loss: 0.017695, mae: 0.451367, mean_q: 0.440667, mean_eps: 0.000000
 4289/5000: episode: 175, duration: 0.160s, episode steps:  14, steps per second:  87, episode reward: 41.825, mean reward:  2.987 [-2.230, 32.030], mean action: 2.714 [0.000, 11.000],  loss: 0.015409, mae: 0.427637, mean_q: 0.445065, mean_eps: 0.000000
 4324/5000: episode: 176, duration: 0.374s, episode steps:  35, steps per second:  93, episode reward: -35.210, mean reward: -1.006 [-32.013,  2.844], mean action: 12.600 [0.000, 15.000],  loss: 0.018798, mae: 0.443537, mean_q: 0.472801, mean_eps: 0.000000
 4350/5000: episode: 177, duration: 0.309s, episode steps:  26, steps per second:  84, episode reward: -32.790, mean reward: -1.261 [-32.880,  2.963], mean action: 5.769 [0.000, 14.000],  loss: 0.016702, mae: 0.440689, mean_q: 0.440932, mean_eps: 0.000000
 4368/5000: episode: 178, duration: 0.204s, episode steps:  18, steps per second:  88, episode reward: 41.387, mean reward:  2.299 [-2.814, 33.000], mean action: 3.444 [0.000, 14.000],  loss: 0.025306, mae: 0.481384, mean_q: 0.428328, mean_eps: 0.000000
 4391/5000: episode: 179, duration: 0.254s, episode steps:  23, steps per second:  90, episode reward: 35.368, mean reward:  1.538 [-3.000, 32.330], mean action: 6.522 [0.000, 20.000],  loss: 0.020266, mae: 0.454150, mean_q: 0.484784, mean_eps: 0.000000
 4414/5000: episode: 180, duration: 0.257s, episode steps:  23, steps per second:  90, episode reward: 32.442, mean reward:  1.411 [-3.000, 32.200], mean action: 2.870 [0.000, 15.000],  loss: 0.018655, mae: 0.451230, mean_q: 0.481002, mean_eps: 0.000000
 4434/5000: episode: 181, duration: 0.231s, episode steps:  20, steps per second:  87, episode reward: 41.288, mean reward:  2.064 [-2.080, 32.680], mean action: 2.800 [0.000, 11.000],  loss: 0.021335, mae: 0.483002, mean_q: 0.405418, mean_eps: 0.000000
 4463/5000: episode: 182, duration: 0.314s, episode steps:  29, steps per second:  92, episode reward: 32.137, mean reward:  1.108 [-2.901, 33.000], mean action: 6.448 [0.000, 15.000],  loss: 0.020938, mae: 0.480024, mean_q: 0.449341, mean_eps: 0.000000
 4491/5000: episode: 183, duration: 0.298s, episode steps:  28, steps per second:  94, episode reward: -41.630, mean reward: -1.487 [-32.133,  2.421], mean action: 6.321 [1.000, 14.000],  loss: 0.016284, mae: 0.454152, mean_q: 0.436181, mean_eps: 0.000000
 4514/5000: episode: 184, duration: 0.251s, episode steps:  23, steps per second:  91, episode reward: 35.184, mean reward:  1.530 [-2.738, 32.230], mean action: 4.087 [0.000, 19.000],  loss: 0.018437, mae: 0.456629, mean_q: 0.462481, mean_eps: 0.000000
 4572/5000: episode: 185, duration: 0.618s, episode steps:  58, steps per second:  94, episode reward: 32.921, mean reward:  0.568 [-2.455, 33.000], mean action: 2.845 [0.000, 15.000],  loss: 0.022334, mae: 0.482709, mean_q: 0.471995, mean_eps: 0.000000
 4589/5000: episode: 186, duration: 0.197s, episode steps:  17, steps per second:  86, episode reward: 38.164, mean reward:  2.245 [-3.000, 32.150], mean action: 3.824 [0.000, 15.000],  loss: 0.017534, mae: 0.440498, mean_q: 0.559412, mean_eps: 0.000000
 4611/5000: episode: 187, duration: 0.297s, episode steps:  22, steps per second:  74, episode reward: 36.000, mean reward:  1.636 [-3.000, 32.630], mean action: 5.227 [0.000, 20.000],  loss: 0.022551, mae: 0.461772, mean_q: 0.597798, mean_eps: 0.000000
 4639/5000: episode: 188, duration: 0.316s, episode steps:  28, steps per second:  89, episode reward: 32.376, mean reward:  1.156 [-2.462, 31.927], mean action: 6.071 [0.000, 20.000],  loss: 0.017695, mae: 0.449974, mean_q: 0.468208, mean_eps: 0.000000
 4675/5000: episode: 189, duration: 0.382s, episode steps:  36, steps per second:  94, episode reward: 37.914, mean reward:  1.053 [-2.455, 31.972], mean action: 5.917 [0.000, 15.000],  loss: 0.020917, mae: 0.467133, mean_q: 0.425526, mean_eps: 0.000000
 4699/5000: episode: 190, duration: 0.280s, episode steps:  24, steps per second:  86, episode reward: 32.199, mean reward:  1.342 [-2.470, 32.230], mean action: 5.000 [0.000, 20.000],  loss: 0.020964, mae: 0.454357, mean_q: 0.479239, mean_eps: 0.000000
 4730/5000: episode: 191, duration: 0.336s, episode steps:  31, steps per second:  92, episode reward: -35.460, mean reward: -1.144 [-32.158,  2.539], mean action: 4.000 [0.000, 13.000],  loss: 0.017331, mae: 0.426289, mean_q: 0.514071, mean_eps: 0.000000
 4747/5000: episode: 192, duration: 0.190s, episode steps:  17, steps per second:  90, episode reward: 36.000, mean reward:  2.118 [-3.000, 32.070], mean action: 2.588 [0.000, 9.000],  loss: 0.018034, mae: 0.432396, mean_q: 0.451876, mean_eps: 0.000000
 4764/5000: episode: 193, duration: 0.197s, episode steps:  17, steps per second:  86, episode reward: 41.452, mean reward:  2.438 [-2.605, 32.420], mean action: 5.824 [1.000, 20.000],  loss: 0.023617, mae: 0.464599, mean_q: 0.487620, mean_eps: 0.000000
 4784/5000: episode: 194, duration: 0.235s, episode steps:  20, steps per second:  85, episode reward: 38.340, mean reward:  1.917 [-2.450, 32.095], mean action: 5.800 [0.000, 15.000],  loss: 0.016108, mae: 0.439376, mean_q: 0.432519, mean_eps: 0.000000
 4803/5000: episode: 195, duration: 0.215s, episode steps:  19, steps per second:  88, episode reward: 35.623, mean reward:  1.875 [-2.900, 32.230], mean action: 4.632 [0.000, 20.000],  loss: 0.023618, mae: 0.477058, mean_q: 0.418795, mean_eps: 0.000000
 4835/5000: episode: 196, duration: 0.351s, episode steps:  32, steps per second:  91, episode reward: 35.312, mean reward:  1.104 [-2.321, 31.853], mean action: 4.312 [1.000, 19.000],  loss: 0.021870, mae: 0.470689, mean_q: 0.449387, mean_eps: 0.000000
 4873/5000: episode: 197, duration: 0.400s, episode steps:  38, steps per second:  95, episode reward: 34.516, mean reward:  0.908 [-2.950, 32.200], mean action: 6.553 [0.000, 20.000],  loss: 0.018210, mae: 0.463483, mean_q: 0.419758, mean_eps: 0.000000
 4900/5000: episode: 198, duration: 0.296s, episode steps:  27, steps per second:  91, episode reward: -33.000, mean reward: -1.222 [-30.750,  2.370], mean action: 3.444 [0.000, 14.000],  loss: 0.019470, mae: 0.473517, mean_q: 0.423200, mean_eps: 0.000000
 4927/5000: episode: 199, duration: 0.295s, episode steps:  27, steps per second:  92, episode reward: 35.605, mean reward:  1.319 [-3.000, 33.000], mean action: 6.333 [0.000, 15.000],  loss: 0.018688, mae: 0.454476, mean_q: 0.456884, mean_eps: 0.000000
 4946/5000: episode: 200, duration: 0.214s, episode steps:  19, steps per second:  89, episode reward: 41.569, mean reward:  2.188 [-2.181, 31.970], mean action: 4.211 [0.000, 20.000],  loss: 0.019200, mae: 0.456607, mean_q: 0.464550, mean_eps: 0.000000
 4975/5000: episode: 201, duration: 0.311s, episode steps:  29, steps per second:  93, episode reward: -30.000, mean reward: -1.034 [-30.520,  3.000], mean action: 8.414 [0.000, 21.000],  loss: 0.021327, mae: 0.464164, mean_q: 0.498683, mean_eps: 0.000000
done, took 51.370 seconds
DQN Evaluation: 593 victories out of 725 episodes
Training for 5000 steps ...
   39/5000: episode: 1, duration: 0.245s, episode steps:  39, steps per second: 159, episode reward: -32.880, mean reward: -0.843 [-32.447,  2.560], mean action: 9.538 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   61/5000: episode: 2, duration: 0.140s, episode steps:  22, steps per second: 157, episode reward: 38.938, mean reward:  1.770 [-2.699, 32.160], mean action: 3.864 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   88/5000: episode: 3, duration: 0.177s, episode steps:  27, steps per second: 152, episode reward: 38.844, mean reward:  1.439 [-3.000, 32.720], mean action: 3.889 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  126/5000: episode: 4, duration: 0.234s, episode steps:  38, steps per second: 162, episode reward: 35.191, mean reward:  0.926 [-2.688, 32.123], mean action: 3.658 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  152/5000: episode: 5, duration: 0.195s, episode steps:  26, steps per second: 133, episode reward: 36.000, mean reward:  1.385 [-2.672, 30.124], mean action: 6.423 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  168/5000: episode: 6, duration: 0.145s, episode steps:  16, steps per second: 111, episode reward: 44.177, mean reward:  2.761 [-2.175, 33.000], mean action: 3.438 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  188/5000: episode: 7, duration: 0.129s, episode steps:  20, steps per second: 155, episode reward: 44.906, mean reward:  2.245 [-2.153, 32.326], mean action: 3.750 [1.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  211/5000: episode: 8, duration: 0.153s, episode steps:  23, steps per second: 150, episode reward: 35.876, mean reward:  1.560 [-2.259, 30.418], mean action: 5.174 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  245/5000: episode: 9, duration: 0.206s, episode steps:  34, steps per second: 165, episode reward: 38.539, mean reward:  1.134 [-2.264, 32.115], mean action: 4.882 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  283/5000: episode: 10, duration: 0.233s, episode steps:  38, steps per second: 163, episode reward: 35.150, mean reward:  0.925 [-2.427, 31.942], mean action: 6.816 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  301/5000: episode: 11, duration: 0.287s, episode steps:  18, steps per second:  63, episode reward: 41.907, mean reward:  2.328 [-2.901, 32.540], mean action: 4.667 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  329/5000: episode: 12, duration: 0.176s, episode steps:  28, steps per second: 159, episode reward: 33.000, mean reward:  1.179 [-2.426, 29.784], mean action: 4.321 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  346/5000: episode: 13, duration: 0.116s, episode steps:  17, steps per second: 146, episode reward: 47.409, mean reward:  2.789 [-0.270, 32.253], mean action: 4.118 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  367/5000: episode: 14, duration: 0.147s, episode steps:  21, steps per second: 143, episode reward: 38.346, mean reward:  1.826 [-2.509, 32.310], mean action: 3.571 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  400/5000: episode: 15, duration: 0.204s, episode steps:  33, steps per second: 162, episode reward: 38.778, mean reward:  1.175 [-2.335, 32.580], mean action: 2.606 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  435/5000: episode: 16, duration: 0.214s, episode steps:  35, steps per second: 164, episode reward: 41.296, mean reward:  1.180 [-3.000, 32.620], mean action: 4.114 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  462/5000: episode: 17, duration: 0.168s, episode steps:  27, steps per second: 161, episode reward: 35.835, mean reward:  1.327 [-3.000, 32.300], mean action: 3.963 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  488/5000: episode: 18, duration: 0.163s, episode steps:  26, steps per second: 159, episode reward: 44.153, mean reward:  1.698 [-2.528, 32.220], mean action: 2.423 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  512/5000: episode: 19, duration: 0.159s, episode steps:  24, steps per second: 151, episode reward: 43.315, mean reward:  1.805 [-2.232, 32.340], mean action: 2.583 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  522/5000: episode: 20, duration: 0.075s, episode steps:  10, steps per second: 134, episode reward: 47.498, mean reward:  4.750 [-0.025, 33.000], mean action: 2.900 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  563/5000: episode: 21, duration: 0.246s, episode steps:  41, steps per second: 167, episode reward: 35.690, mean reward:  0.870 [-3.000, 32.520], mean action: 2.732 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  599/5000: episode: 22, duration: 0.222s, episode steps:  36, steps per second: 162, episode reward: -35.200, mean reward: -0.978 [-32.282,  2.206], mean action: 6.611 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  618/5000: episode: 23, duration: 0.114s, episode steps:  19, steps per second: 166, episode reward: 44.373, mean reward:  2.335 [-2.184, 33.000], mean action: 2.579 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  637/5000: episode: 24, duration: 0.192s, episode steps:  19, steps per second:  99, episode reward: 43.638, mean reward:  2.297 [-2.154, 31.841], mean action: 4.316 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  677/5000: episode: 25, duration: 0.317s, episode steps:  40, steps per second: 126, episode reward: 37.393, mean reward:  0.935 [-3.000, 31.592], mean action: 8.825 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  722/5000: episode: 26, duration: 0.294s, episode steps:  45, steps per second: 153, episode reward: 41.728, mean reward:  0.927 [-3.000, 32.310], mean action: 3.956 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  757/5000: episode: 27, duration: 0.221s, episode steps:  35, steps per second: 159, episode reward: 39.965, mean reward:  1.142 [-2.321, 32.520], mean action: 5.257 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  773/5000: episode: 28, duration: 0.109s, episode steps:  16, steps per second: 146, episode reward: 47.397, mean reward:  2.962 [ 0.000, 32.290], mean action: 1.812 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  789/5000: episode: 29, duration: 0.109s, episode steps:  16, steps per second: 146, episode reward: 45.763, mean reward:  2.860 [-1.101, 32.350], mean action: 3.625 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  819/5000: episode: 30, duration: 0.186s, episode steps:  30, steps per second: 161, episode reward: 35.904, mean reward:  1.197 [-3.000, 32.631], mean action: 2.967 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  837/5000: episode: 31, duration: 0.119s, episode steps:  18, steps per second: 151, episode reward: 41.584, mean reward:  2.310 [-3.000, 32.760], mean action: 6.778 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  850/5000: episode: 32, duration: 0.104s, episode steps:  13, steps per second: 124, episode reward: 47.134, mean reward:  3.626 [-0.652, 33.000], mean action: 4.385 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  891/5000: episode: 33, duration: 0.243s, episode steps:  41, steps per second: 169, episode reward: 40.857, mean reward:  0.997 [-3.000, 32.050], mean action: 6.610 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  922/5000: episode: 34, duration: 0.196s, episode steps:  31, steps per second: 158, episode reward: 37.979, mean reward:  1.225 [-2.177, 32.130], mean action: 7.548 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  940/5000: episode: 35, duration: 0.134s, episode steps:  18, steps per second: 135, episode reward: 41.208, mean reward:  2.289 [-2.676, 32.229], mean action: 2.889 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  959/5000: episode: 36, duration: 0.132s, episode steps:  19, steps per second: 144, episode reward: 44.076, mean reward:  2.320 [-2.045, 32.390], mean action: 2.474 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  975/5000: episode: 37, duration: 0.105s, episode steps:  16, steps per second: 152, episode reward: 42.000, mean reward:  2.625 [-2.307, 32.490], mean action: 4.312 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1006/5000: episode: 38, duration: 0.224s, episode steps:  31, steps per second: 139, episode reward: 38.009, mean reward:  1.226 [-3.000, 31.691], mean action: 6.839 [1.000, 21.000],  loss: 0.018889, mae: 0.474044, mean_q: 0.408548, mean_eps: 0.000000
 1058/5000: episode: 39, duration: 0.579s, episode steps:  52, steps per second:  90, episode reward: 44.284, mean reward:  0.852 [-2.090, 32.250], mean action: 3.231 [0.000, 20.000],  loss: 0.021360, mae: 0.471548, mean_q: 0.445015, mean_eps: 0.000000
 1072/5000: episode: 40, duration: 0.166s, episode steps:  14, steps per second:  84, episode reward: 47.554, mean reward:  3.397 [ 0.160, 32.350], mean action: 1.357 [0.000, 3.000],  loss: 0.015541, mae: 0.436131, mean_q: 0.462717, mean_eps: 0.000000
 1136/5000: episode: 41, duration: 0.701s, episode steps:  64, steps per second:  91, episode reward: 38.531, mean reward:  0.602 [-2.390, 32.592], mean action: 3.719 [0.000, 20.000],  loss: 0.020578, mae: 0.455389, mean_q: 0.488254, mean_eps: 0.000000
 1170/5000: episode: 42, duration: 0.379s, episode steps:  34, steps per second:  90, episode reward: 39.000, mean reward:  1.147 [-2.360, 32.340], mean action: 2.647 [0.000, 15.000],  loss: 0.021750, mae: 0.454740, mean_q: 0.536160, mean_eps: 0.000000
 1197/5000: episode: 43, duration: 0.302s, episode steps:  27, steps per second:  89, episode reward: 35.031, mean reward:  1.297 [-3.000, 31.878], mean action: 7.444 [0.000, 20.000],  loss: 0.019438, mae: 0.441501, mean_q: 0.499648, mean_eps: 0.000000
 1240/5000: episode: 44, duration: 0.526s, episode steps:  43, steps per second:  82, episode reward: 35.939, mean reward:  0.836 [-2.286, 32.120], mean action: 4.163 [0.000, 21.000],  loss: 0.020647, mae: 0.453263, mean_q: 0.467654, mean_eps: 0.000000
 1269/5000: episode: 45, duration: 0.329s, episode steps:  29, steps per second:  88, episode reward: 42.000, mean reward:  1.448 [-2.447, 32.570], mean action: 2.793 [0.000, 11.000],  loss: 0.022134, mae: 0.466672, mean_q: 0.516652, mean_eps: 0.000000
 1304/5000: episode: 46, duration: 0.378s, episode steps:  35, steps per second:  93, episode reward: 38.967, mean reward:  1.113 [-2.564, 32.792], mean action: 6.686 [0.000, 20.000],  loss: 0.020796, mae: 0.464713, mean_q: 0.497941, mean_eps: 0.000000
 1347/5000: episode: 47, duration: 0.460s, episode steps:  43, steps per second:  93, episode reward: 35.843, mean reward:  0.834 [-3.000, 32.213], mean action: 6.581 [0.000, 20.000],  loss: 0.021695, mae: 0.457087, mean_q: 0.496971, mean_eps: 0.000000
 1376/5000: episode: 48, duration: 0.325s, episode steps:  29, steps per second:  89, episode reward: 34.903, mean reward:  1.204 [-2.746, 32.110], mean action: 6.966 [1.000, 19.000],  loss: 0.019423, mae: 0.449821, mean_q: 0.450304, mean_eps: 0.000000
 1393/5000: episode: 49, duration: 0.191s, episode steps:  17, steps per second:  89, episode reward: 41.489, mean reward:  2.441 [-2.420, 32.027], mean action: 3.176 [0.000, 12.000],  loss: 0.015036, mae: 0.430280, mean_q: 0.451742, mean_eps: 0.000000
 1433/5000: episode: 50, duration: 0.521s, episode steps:  40, steps per second:  77, episode reward: 37.625, mean reward:  0.941 [-2.732, 32.020], mean action: 6.125 [0.000, 14.000],  loss: 0.018613, mae: 0.439663, mean_q: 0.488723, mean_eps: 0.000000
 1460/5000: episode: 51, duration: 0.309s, episode steps:  27, steps per second:  87, episode reward: 35.089, mean reward:  1.300 [-2.284, 32.490], mean action: 5.259 [0.000, 14.000],  loss: 0.019134, mae: 0.431375, mean_q: 0.523763, mean_eps: 0.000000
 1504/5000: episode: 52, duration: 0.481s, episode steps:  44, steps per second:  92, episode reward: -32.770, mean reward: -0.745 [-32.099,  2.400], mean action: 5.068 [0.000, 20.000],  loss: 0.022702, mae: 0.455769, mean_q: 0.439146, mean_eps: 0.000000
 1532/5000: episode: 53, duration: 0.328s, episode steps:  28, steps per second:  85, episode reward: 41.008, mean reward:  1.465 [-2.903, 32.350], mean action: 4.000 [0.000, 15.000],  loss: 0.017905, mae: 0.434551, mean_q: 0.479114, mean_eps: 0.000000
 1568/5000: episode: 54, duration: 0.393s, episode steps:  36, steps per second:  92, episode reward: 32.718, mean reward:  0.909 [-3.000, 32.480], mean action: 5.667 [0.000, 15.000],  loss: 0.017364, mae: 0.445804, mean_q: 0.435021, mean_eps: 0.000000
 1594/5000: episode: 55, duration: 0.292s, episode steps:  26, steps per second:  89, episode reward: 42.000, mean reward:  1.615 [-3.000, 33.000], mean action: 0.962 [0.000, 9.000],  loss: 0.015566, mae: 0.439712, mean_q: 0.409346, mean_eps: 0.000000
 1611/5000: episode: 56, duration: 0.195s, episode steps:  17, steps per second:  87, episode reward: 44.252, mean reward:  2.603 [-2.137, 32.060], mean action: 1.412 [0.000, 9.000],  loss: 0.024529, mae: 0.477130, mean_q: 0.389963, mean_eps: 0.000000
 1625/5000: episode: 57, duration: 0.165s, episode steps:  14, steps per second:  85, episode reward: 45.000, mean reward:  3.214 [-2.168, 32.470], mean action: 3.143 [0.000, 13.000],  loss: 0.017284, mae: 0.441336, mean_q: 0.433921, mean_eps: 0.000000
 1641/5000: episode: 58, duration: 0.182s, episode steps:  16, steps per second:  88, episode reward: 45.000, mean reward:  2.812 [-3.000, 32.470], mean action: 4.062 [0.000, 20.000],  loss: 0.019353, mae: 0.451008, mean_q: 0.442591, mean_eps: 0.000000
 1666/5000: episode: 59, duration: 0.281s, episode steps:  25, steps per second:  89, episode reward: 47.175, mean reward:  1.887 [-0.341, 32.234], mean action: 3.880 [0.000, 20.000],  loss: 0.017604, mae: 0.451620, mean_q: 0.466697, mean_eps: 0.000000
 1696/5000: episode: 60, duration: 0.333s, episode steps:  30, steps per second:  90, episode reward: 41.036, mean reward:  1.368 [-3.000, 32.210], mean action: 3.167 [0.000, 14.000],  loss: 0.021830, mae: 0.464012, mean_q: 0.480890, mean_eps: 0.000000
 1757/5000: episode: 61, duration: 0.653s, episode steps:  61, steps per second:  93, episode reward: 35.048, mean reward:  0.575 [-2.893, 32.480], mean action: 4.557 [0.000, 21.000],  loss: 0.018857, mae: 0.452024, mean_q: 0.445072, mean_eps: 0.000000
 1789/5000: episode: 62, duration: 0.352s, episode steps:  32, steps per second:  91, episode reward: 41.533, mean reward:  1.298 [-2.142, 32.101], mean action: 5.219 [0.000, 20.000],  loss: 0.017388, mae: 0.452723, mean_q: 0.472494, mean_eps: 0.000000
 1812/5000: episode: 63, duration: 0.256s, episode steps:  23, steps per second:  90, episode reward: 35.896, mean reward:  1.561 [-3.000, 32.180], mean action: 3.739 [0.000, 15.000],  loss: 0.019226, mae: 0.474239, mean_q: 0.411927, mean_eps: 0.000000
 1842/5000: episode: 64, duration: 0.335s, episode steps:  30, steps per second:  90, episode reward: 38.252, mean reward:  1.275 [-3.000, 32.290], mean action: 4.267 [0.000, 12.000],  loss: 0.020489, mae: 0.463644, mean_q: 0.434883, mean_eps: 0.000000
 1862/5000: episode: 65, duration: 0.244s, episode steps:  20, steps per second:  82, episode reward: 43.803, mean reward:  2.190 [-2.099, 32.517], mean action: 3.100 [0.000, 9.000],  loss: 0.019314, mae: 0.463812, mean_q: 0.430814, mean_eps: 0.000000
 1899/5000: episode: 66, duration: 0.467s, episode steps:  37, steps per second:  79, episode reward: 35.544, mean reward:  0.961 [-2.718, 31.886], mean action: 5.865 [0.000, 15.000],  loss: 0.017296, mae: 0.437345, mean_q: 0.472751, mean_eps: 0.000000
 1919/5000: episode: 67, duration: 0.282s, episode steps:  20, steps per second:  71, episode reward: 43.110, mean reward:  2.156 [-2.087, 32.141], mean action: 4.050 [0.000, 15.000],  loss: 0.017596, mae: 0.432792, mean_q: 0.462575, mean_eps: 0.000000
 1954/5000: episode: 68, duration: 0.489s, episode steps:  35, steps per second:  72, episode reward: 41.299, mean reward:  1.180 [-3.000, 32.102], mean action: 4.057 [0.000, 19.000],  loss: 0.019645, mae: 0.446088, mean_q: 0.436997, mean_eps: 0.000000
 1976/5000: episode: 69, duration: 0.255s, episode steps:  22, steps per second:  86, episode reward: 38.220, mean reward:  1.737 [-3.000, 31.799], mean action: 5.000 [0.000, 14.000],  loss: 0.021807, mae: 0.450040, mean_q: 0.454765, mean_eps: 0.000000
 2020/5000: episode: 70, duration: 0.479s, episode steps:  44, steps per second:  92, episode reward: 34.893, mean reward:  0.793 [-2.874, 29.181], mean action: 8.773 [0.000, 21.000],  loss: 0.019236, mae: 0.438725, mean_q: 0.432502, mean_eps: 0.000000
 2061/5000: episode: 71, duration: 0.451s, episode steps:  41, steps per second:  91, episode reward: 40.655, mean reward:  0.992 [-2.695, 32.230], mean action: 6.390 [0.000, 20.000],  loss: 0.022176, mae: 0.457837, mean_q: 0.464644, mean_eps: 0.000000
 2079/5000: episode: 72, duration: 0.209s, episode steps:  18, steps per second:  86, episode reward: 41.974, mean reward:  2.332 [-2.741, 32.690], mean action: 4.722 [0.000, 14.000],  loss: 0.017972, mae: 0.444177, mean_q: 0.459335, mean_eps: 0.000000
 2101/5000: episode: 73, duration: 0.325s, episode steps:  22, steps per second:  68, episode reward: 44.015, mean reward:  2.001 [-2.557, 32.232], mean action: 4.182 [0.000, 15.000],  loss: 0.019392, mae: 0.453492, mean_q: 0.416155, mean_eps: 0.000000
 2121/5000: episode: 74, duration: 0.230s, episode steps:  20, steps per second:  87, episode reward: 41.298, mean reward:  2.065 [-2.174, 32.200], mean action: 3.300 [0.000, 15.000],  loss: 0.024802, mae: 0.468270, mean_q: 0.464246, mean_eps: 0.000000
 2174/5000: episode: 75, duration: 0.566s, episode steps:  53, steps per second:  94, episode reward: 38.321, mean reward:  0.723 [-2.901, 32.630], mean action: 4.226 [0.000, 19.000],  loss: 0.018917, mae: 0.442222, mean_q: 0.465980, mean_eps: 0.000000
 2196/5000: episode: 76, duration: 0.245s, episode steps:  22, steps per second:  90, episode reward: 38.196, mean reward:  1.736 [-2.403, 31.585], mean action: 3.636 [0.000, 15.000],  loss: 0.016430, mae: 0.441869, mean_q: 0.468101, mean_eps: 0.000000
 2241/5000: episode: 77, duration: 0.485s, episode steps:  45, steps per second:  93, episode reward: 34.942, mean reward:  0.776 [-2.605, 34.491], mean action: 4.667 [0.000, 15.000],  loss: 0.015170, mae: 0.436166, mean_q: 0.440843, mean_eps: 0.000000
 2283/5000: episode: 78, duration: 0.463s, episode steps:  42, steps per second:  91, episode reward: 38.903, mean reward:  0.926 [-2.381, 32.270], mean action: 4.119 [0.000, 15.000],  loss: 0.020793, mae: 0.448428, mean_q: 0.496276, mean_eps: 0.000000
 2311/5000: episode: 79, duration: 0.308s, episode steps:  28, steps per second:  91, episode reward: 38.683, mean reward:  1.382 [-2.158, 31.874], mean action: 3.750 [0.000, 15.000],  loss: 0.020942, mae: 0.439949, mean_q: 0.543388, mean_eps: 0.000000
 2368/5000: episode: 80, duration: 0.599s, episode steps:  57, steps per second:  95, episode reward: -35.070, mean reward: -0.615 [-32.031,  2.200], mean action: 7.860 [0.000, 21.000],  loss: 0.022433, mae: 0.456606, mean_q: 0.470404, mean_eps: 0.000000
 2394/5000: episode: 81, duration: 0.289s, episode steps:  26, steps per second:  90, episode reward: 42.000, mean reward:  1.615 [-2.606, 32.700], mean action: 2.731 [0.000, 20.000],  loss: 0.022992, mae: 0.460666, mean_q: 0.460509, mean_eps: 0.000000
 2407/5000: episode: 82, duration: 0.159s, episode steps:  13, steps per second:  82, episode reward: 44.520, mean reward:  3.425 [-2.166, 32.075], mean action: 3.462 [0.000, 19.000],  loss: 0.022109, mae: 0.457294, mean_q: 0.483325, mean_eps: 0.000000
 2438/5000: episode: 83, duration: 0.339s, episode steps:  31, steps per second:  91, episode reward: 32.878, mean reward:  1.061 [-3.000, 32.138], mean action: 4.710 [0.000, 19.000],  loss: 0.016746, mae: 0.448026, mean_q: 0.396535, mean_eps: 0.000000
 2467/5000: episode: 84, duration: 0.313s, episode steps:  29, steps per second:  93, episode reward: 38.828, mean reward:  1.339 [-2.191, 32.390], mean action: 4.862 [0.000, 20.000],  loss: 0.017746, mae: 0.444604, mean_q: 0.434315, mean_eps: 0.000000
 2494/5000: episode: 85, duration: 0.303s, episode steps:  27, steps per second:  89, episode reward: 41.517, mean reward:  1.538 [-2.199, 32.133], mean action: 3.519 [0.000, 15.000],  loss: 0.019247, mae: 0.449214, mean_q: 0.454985, mean_eps: 0.000000
 2540/5000: episode: 86, duration: 0.519s, episode steps:  46, steps per second:  89, episode reward: 41.778, mean reward:  0.908 [-2.141, 32.240], mean action: 3.500 [0.000, 20.000],  loss: 0.018488, mae: 0.446744, mean_q: 0.488231, mean_eps: 0.000000
 2566/5000: episode: 87, duration: 0.367s, episode steps:  26, steps per second:  71, episode reward: 41.473, mean reward:  1.595 [-2.612, 32.700], mean action: 1.885 [0.000, 9.000],  loss: 0.018699, mae: 0.452800, mean_q: 0.414659, mean_eps: 0.000000
 2586/5000: episode: 88, duration: 0.228s, episode steps:  20, steps per second:  88, episode reward: 41.552, mean reward:  2.078 [-3.000, 32.370], mean action: 4.600 [0.000, 14.000],  loss: 0.022663, mae: 0.464519, mean_q: 0.449679, mean_eps: 0.000000
 2622/5000: episode: 89, duration: 0.387s, episode steps:  36, steps per second:  93, episode reward: -32.200, mean reward: -0.894 [-32.051,  2.480], mean action: 7.667 [0.000, 20.000],  loss: 0.019233, mae: 0.431929, mean_q: 0.519861, mean_eps: 0.000000
 2654/5000: episode: 90, duration: 0.900s, episode steps:  32, steps per second:  36, episode reward: 34.919, mean reward:  1.091 [-3.000, 32.200], mean action: 6.438 [0.000, 19.000],  loss: 0.020712, mae: 0.434656, mean_q: 0.522360, mean_eps: 0.000000
 2681/5000: episode: 91, duration: 0.387s, episode steps:  27, steps per second:  70, episode reward: 41.658, mean reward:  1.543 [-2.700, 32.510], mean action: 4.222 [0.000, 15.000],  loss: 0.020455, mae: 0.440835, mean_q: 0.508173, mean_eps: 0.000000
 2730/5000: episode: 92, duration: 0.537s, episode steps:  49, steps per second:  91, episode reward: -32.100, mean reward: -0.655 [-31.883,  2.842], mean action: 8.184 [0.000, 20.000],  loss: 0.018775, mae: 0.434837, mean_q: 0.502837, mean_eps: 0.000000
 2762/5000: episode: 93, duration: 0.368s, episode steps:  32, steps per second:  87, episode reward: 41.091, mean reward:  1.284 [-2.879, 31.952], mean action: 3.531 [0.000, 20.000],  loss: 0.021504, mae: 0.463128, mean_q: 0.405833, mean_eps: 0.000000
 2791/5000: episode: 94, duration: 0.422s, episode steps:  29, steps per second:  69, episode reward: 43.460, mean reward:  1.499 [-2.004, 32.440], mean action: 3.069 [0.000, 19.000],  loss: 0.021676, mae: 0.463983, mean_q: 0.446013, mean_eps: 0.000000
 2807/5000: episode: 95, duration: 0.197s, episode steps:  16, steps per second:  81, episode reward: 44.333, mean reward:  2.771 [-2.447, 32.290], mean action: 2.625 [0.000, 14.000],  loss: 0.019200, mae: 0.456649, mean_q: 0.449441, mean_eps: 0.000000
 2828/5000: episode: 96, duration: 0.255s, episode steps:  21, steps per second:  82, episode reward: 41.465, mean reward:  1.975 [-3.000, 32.107], mean action: 2.000 [0.000, 11.000],  loss: 0.020645, mae: 0.456867, mean_q: 0.492909, mean_eps: 0.000000
 2858/5000: episode: 97, duration: 0.332s, episode steps:  30, steps per second:  90, episode reward: 36.000, mean reward:  1.200 [-2.605, 32.230], mean action: 4.433 [0.000, 14.000],  loss: 0.017092, mae: 0.439918, mean_q: 0.491991, mean_eps: 0.000000
 2888/5000: episode: 98, duration: 0.333s, episode steps:  30, steps per second:  90, episode reward: 38.160, mean reward:  1.272 [-2.514, 32.010], mean action: 4.633 [0.000, 15.000],  loss: 0.020133, mae: 0.457983, mean_q: 0.446091, mean_eps: 0.000000
 2910/5000: episode: 99, duration: 0.251s, episode steps:  22, steps per second:  88, episode reward: 44.824, mean reward:  2.037 [-2.056, 32.240], mean action: 2.727 [0.000, 15.000],  loss: 0.019954, mae: 0.452075, mean_q: 0.467870, mean_eps: 0.000000
 2927/5000: episode: 100, duration: 0.193s, episode steps:  17, steps per second:  88, episode reward: 44.613, mean reward:  2.624 [-2.089, 32.670], mean action: 5.882 [0.000, 19.000],  loss: 0.020783, mae: 0.455459, mean_q: 0.491137, mean_eps: 0.000000
 2982/5000: episode: 101, duration: 0.575s, episode steps:  55, steps per second:  96, episode reward: -42.740, mean reward: -0.777 [-32.081,  1.611], mean action: 11.327 [0.000, 15.000],  loss: 0.020345, mae: 0.456930, mean_q: 0.470408, mean_eps: 0.000000
 3028/5000: episode: 102, duration: 0.494s, episode steps:  46, steps per second:  93, episode reward: 38.490, mean reward:  0.837 [-2.167, 32.120], mean action: 3.978 [0.000, 18.000],  loss: 0.017254, mae: 0.448040, mean_q: 0.473929, mean_eps: 0.000000
 3071/5000: episode: 103, duration: 0.471s, episode steps:  43, steps per second:  91, episode reward: 38.891, mean reward:  0.904 [-2.685, 32.410], mean action: 4.512 [0.000, 19.000],  loss: 0.018911, mae: 0.448326, mean_q: 0.472512, mean_eps: 0.000000
 3100/5000: episode: 104, duration: 0.317s, episode steps:  29, steps per second:  91, episode reward: 38.843, mean reward:  1.339 [-2.289, 31.983], mean action: 4.276 [1.000, 14.000],  loss: 0.018093, mae: 0.443895, mean_q: 0.483499, mean_eps: 0.000000
 3128/5000: episode: 105, duration: 0.309s, episode steps:  28, steps per second:  91, episode reward: 41.657, mean reward:  1.488 [-3.000, 32.227], mean action: 3.214 [0.000, 20.000],  loss: 0.018761, mae: 0.447284, mean_q: 0.457146, mean_eps: 0.000000
 3157/5000: episode: 106, duration: 0.339s, episode steps:  29, steps per second:  85, episode reward: 44.015, mean reward:  1.518 [-2.095, 31.611], mean action: 5.241 [0.000, 19.000],  loss: 0.021758, mae: 0.472468, mean_q: 0.436832, mean_eps: 0.000000
 3183/5000: episode: 107, duration: 0.443s, episode steps:  26, steps per second:  59, episode reward: 37.423, mean reward:  1.439 [-2.560, 32.003], mean action: 4.731 [0.000, 18.000],  loss: 0.021091, mae: 0.458563, mean_q: 0.489005, mean_eps: 0.000000
 3226/5000: episode: 108, duration: 0.562s, episode steps:  43, steps per second:  76, episode reward: -32.660, mean reward: -0.760 [-32.178,  2.951], mean action: 5.233 [0.000, 15.000],  loss: 0.016250, mae: 0.449983, mean_q: 0.438816, mean_eps: 0.000000
 3246/5000: episode: 109, duration: 0.267s, episode steps:  20, steps per second:  75, episode reward: 38.638, mean reward:  1.932 [-3.000, 33.000], mean action: 6.950 [0.000, 20.000],  loss: 0.015977, mae: 0.432181, mean_q: 0.477265, mean_eps: 0.000000
 3276/5000: episode: 110, duration: 0.366s, episode steps:  30, steps per second:  82, episode reward: 38.202, mean reward:  1.273 [-2.310, 32.330], mean action: 3.100 [0.000, 15.000],  loss: 0.016077, mae: 0.440267, mean_q: 0.454102, mean_eps: 0.000000
 3299/5000: episode: 111, duration: 0.269s, episode steps:  23, steps per second:  86, episode reward: 44.275, mean reward:  1.925 [-2.274, 32.290], mean action: 2.957 [0.000, 11.000],  loss: 0.019146, mae: 0.457568, mean_q: 0.459433, mean_eps: 0.000000
 3335/5000: episode: 112, duration: 0.438s, episode steps:  36, steps per second:  82, episode reward: 34.756, mean reward:  0.965 [-2.401, 32.120], mean action: 4.000 [0.000, 19.000],  loss: 0.020407, mae: 0.459445, mean_q: 0.465803, mean_eps: 0.000000
 3362/5000: episode: 113, duration: 0.330s, episode steps:  27, steps per second:  82, episode reward: 41.861, mean reward:  1.550 [-2.105, 32.591], mean action: 1.778 [0.000, 12.000],  loss: 0.019010, mae: 0.444994, mean_q: 0.477411, mean_eps: 0.000000
 3395/5000: episode: 114, duration: 0.371s, episode steps:  33, steps per second:  89, episode reward: 41.044, mean reward:  1.244 [-2.299, 32.800], mean action: 3.848 [0.000, 15.000],  loss: 0.020344, mae: 0.447685, mean_q: 0.483251, mean_eps: 0.000000
 3421/5000: episode: 115, duration: 0.296s, episode steps:  26, steps per second:  88, episode reward: 35.903, mean reward:  1.381 [-3.000, 32.230], mean action: 3.846 [0.000, 15.000],  loss: 0.018964, mae: 0.443485, mean_q: 0.520562, mean_eps: 0.000000
 3447/5000: episode: 116, duration: 0.298s, episode steps:  26, steps per second:  87, episode reward: 44.123, mean reward:  1.697 [-2.221, 32.010], mean action: 5.538 [0.000, 20.000],  loss: 0.016423, mae: 0.424705, mean_q: 0.475680, mean_eps: 0.000000
 3477/5000: episode: 117, duration: 0.342s, episode steps:  30, steps per second:  88, episode reward: 41.690, mean reward:  1.390 [-3.000, 32.490], mean action: 3.367 [0.000, 15.000],  loss: 0.017342, mae: 0.424386, mean_q: 0.507947, mean_eps: 0.000000
 3499/5000: episode: 118, duration: 0.257s, episode steps:  22, steps per second:  86, episode reward: 43.939, mean reward:  1.997 [-2.003, 32.130], mean action: 2.591 [0.000, 19.000],  loss: 0.016754, mae: 0.424641, mean_q: 0.487969, mean_eps: 0.000000
 3532/5000: episode: 119, duration: 0.373s, episode steps:  33, steps per second:  88, episode reward: 44.117, mean reward:  1.337 [-3.000, 32.001], mean action: 2.727 [0.000, 20.000],  loss: 0.018388, mae: 0.441654, mean_q: 0.464285, mean_eps: 0.000000
 3558/5000: episode: 120, duration: 0.477s, episode steps:  26, steps per second:  54, episode reward: 35.981, mean reward:  1.384 [-2.412, 29.895], mean action: 6.385 [0.000, 20.000],  loss: 0.017560, mae: 0.427388, mean_q: 0.465803, mean_eps: 0.000000
 3578/5000: episode: 121, duration: 0.230s, episode steps:  20, steps per second:  87, episode reward: 44.469, mean reward:  2.223 [-2.140, 32.056], mean action: 4.650 [0.000, 20.000],  loss: 0.020415, mae: 0.444885, mean_q: 0.445729, mean_eps: 0.000000
 3597/5000: episode: 122, duration: 0.234s, episode steps:  19, steps per second:  81, episode reward: 40.283, mean reward:  2.120 [-3.000, 32.060], mean action: 4.421 [0.000, 15.000],  loss: 0.016900, mae: 0.435748, mean_q: 0.455889, mean_eps: 0.000000
 3621/5000: episode: 123, duration: 0.269s, episode steps:  24, steps per second:  89, episode reward: 35.117, mean reward:  1.463 [-3.000, 32.390], mean action: 5.583 [0.000, 15.000],  loss: 0.016011, mae: 0.438580, mean_q: 0.483438, mean_eps: 0.000000
 3649/5000: episode: 124, duration: 0.322s, episode steps:  28, steps per second:  87, episode reward: 38.398, mean reward:  1.371 [-2.607, 31.667], mean action: 2.536 [0.000, 12.000],  loss: 0.017823, mae: 0.452519, mean_q: 0.462828, mean_eps: 0.000000
 3691/5000: episode: 125, duration: 0.470s, episode steps:  42, steps per second:  89, episode reward: 32.383, mean reward:  0.771 [-3.000, 32.270], mean action: 7.452 [0.000, 18.000],  loss: 0.016640, mae: 0.441280, mean_q: 0.428353, mean_eps: 0.000000
 3713/5000: episode: 126, duration: 0.339s, episode steps:  22, steps per second:  65, episode reward: 47.174, mean reward:  2.144 [-0.682, 32.100], mean action: 3.955 [1.000, 14.000],  loss: 0.020764, mae: 0.440355, mean_q: 0.435434, mean_eps: 0.000000
 3741/5000: episode: 127, duration: 0.330s, episode steps:  28, steps per second:  85, episode reward: 44.095, mean reward:  1.575 [-2.205, 32.150], mean action: 1.250 [0.000, 13.000],  loss: 0.018629, mae: 0.437431, mean_q: 0.475502, mean_eps: 0.000000
 3768/5000: episode: 128, duration: 0.308s, episode steps:  27, steps per second:  88, episode reward: 41.344, mean reward:  1.531 [-2.481, 32.090], mean action: 4.963 [0.000, 14.000],  loss: 0.018064, mae: 0.445138, mean_q: 0.446011, mean_eps: 0.000000
 3790/5000: episode: 129, duration: 0.262s, episode steps:  22, steps per second:  84, episode reward: 41.130, mean reward:  1.870 [-2.463, 32.233], mean action: 3.545 [1.000, 12.000],  loss: 0.014825, mae: 0.433838, mean_q: 0.370844, mean_eps: 0.000000
 3808/5000: episode: 130, duration: 0.220s, episode steps:  18, steps per second:  82, episode reward: 41.718, mean reward:  2.318 [-2.141, 29.565], mean action: 4.444 [1.000, 15.000],  loss: 0.020358, mae: 0.454120, mean_q: 0.402810, mean_eps: 0.000000
 3834/5000: episode: 131, duration: 0.292s, episode steps:  26, steps per second:  89, episode reward: 38.803, mean reward:  1.492 [-2.710, 32.751], mean action: 3.231 [0.000, 14.000],  loss: 0.020523, mae: 0.450630, mean_q: 0.424472, mean_eps: 0.000000
 3847/5000: episode: 132, duration: 0.154s, episode steps:  13, steps per second:  84, episode reward: 45.000, mean reward:  3.462 [-2.163, 33.000], mean action: 0.923 [0.000, 12.000],  loss: 0.024563, mae: 0.466401, mean_q: 0.431696, mean_eps: 0.000000
 3872/5000: episode: 133, duration: 0.287s, episode steps:  25, steps per second:  87, episode reward: 44.547, mean reward:  1.782 [-2.124, 32.222], mean action: 5.600 [3.000, 15.000],  loss: 0.019437, mae: 0.439300, mean_q: 0.438173, mean_eps: 0.000000
 3897/5000: episode: 134, duration: 0.286s, episode steps:  25, steps per second:  87, episode reward: 38.171, mean reward:  1.527 [-3.000, 31.991], mean action: 5.120 [0.000, 20.000],  loss: 0.021024, mae: 0.438420, mean_q: 0.464601, mean_eps: 0.000000
 3946/5000: episode: 135, duration: 0.526s, episode steps:  49, steps per second:  93, episode reward: -35.000, mean reward: -0.714 [-32.571,  2.820], mean action: 6.469 [0.000, 15.000],  loss: 0.023051, mae: 0.450970, mean_q: 0.543206, mean_eps: 0.000000
 3975/5000: episode: 136, duration: 0.322s, episode steps:  29, steps per second:  90, episode reward: 41.757, mean reward:  1.440 [-2.155, 31.937], mean action: 3.517 [0.000, 14.000],  loss: 0.022058, mae: 0.448837, mean_q: 0.520243, mean_eps: 0.000000
 4006/5000: episode: 137, duration: 0.340s, episode steps:  31, steps per second:  91, episode reward: 38.940, mean reward:  1.256 [-2.457, 32.310], mean action: 2.290 [0.000, 11.000],  loss: 0.018237, mae: 0.434010, mean_q: 0.444142, mean_eps: 0.000000
 4021/5000: episode: 138, duration: 0.176s, episode steps:  15, steps per second:  85, episode reward: 44.250, mean reward:  2.950 [-2.060, 32.190], mean action: 3.133 [1.000, 11.000],  loss: 0.019130, mae: 0.449855, mean_q: 0.425436, mean_eps: 0.000000
 4040/5000: episode: 139, duration: 0.216s, episode steps:  19, steps per second:  88, episode reward: 41.186, mean reward:  2.168 [-2.525, 32.360], mean action: 2.684 [0.000, 14.000],  loss: 0.021353, mae: 0.451564, mean_q: 0.460237, mean_eps: 0.000000
 4063/5000: episode: 140, duration: 0.272s, episode steps:  23, steps per second:  85, episode reward: 41.837, mean reward:  1.819 [-2.709, 32.010], mean action: 2.478 [1.000, 14.000],  loss: 0.021402, mae: 0.448450, mean_q: 0.531838, mean_eps: 0.000000
 4086/5000: episode: 141, duration: 0.253s, episode steps:  23, steps per second:  91, episode reward: 38.032, mean reward:  1.654 [-3.000, 32.384], mean action: 5.174 [0.000, 18.000],  loss: 0.017615, mae: 0.422274, mean_q: 0.527148, mean_eps: 0.000000
 4114/5000: episode: 142, duration: 0.307s, episode steps:  28, steps per second:  91, episode reward: 41.638, mean reward:  1.487 [-2.199, 32.352], mean action: 4.036 [1.000, 19.000],  loss: 0.020507, mae: 0.440053, mean_q: 0.463256, mean_eps: 0.000000
 4165/5000: episode: 143, duration: 0.561s, episode steps:  51, steps per second:  91, episode reward: 38.841, mean reward:  0.762 [-2.332, 31.973], mean action: 3.608 [0.000, 19.000],  loss: 0.016167, mae: 0.428947, mean_q: 0.424970, mean_eps: 0.000000
 4217/5000: episode: 144, duration: 0.569s, episode steps:  52, steps per second:  91, episode reward: 41.835, mean reward:  0.805 [-3.000, 32.340], mean action: 2.865 [0.000, 18.000],  loss: 0.018497, mae: 0.443487, mean_q: 0.389727, mean_eps: 0.000000
 4253/5000: episode: 145, duration: 0.481s, episode steps:  36, steps per second:  75, episode reward: 33.000, mean reward:  0.917 [-2.704, 32.340], mean action: 2.833 [0.000, 9.000],  loss: 0.018018, mae: 0.431077, mean_q: 0.468952, mean_eps: 0.000000
 4272/5000: episode: 146, duration: 0.221s, episode steps:  19, steps per second:  86, episode reward: 38.336, mean reward:  2.018 [-3.000, 32.290], mean action: 3.421 [0.000, 19.000],  loss: 0.022527, mae: 0.456774, mean_q: 0.455305, mean_eps: 0.000000
 4299/5000: episode: 147, duration: 0.328s, episode steps:  27, steps per second:  82, episode reward: 41.185, mean reward:  1.525 [-2.235, 32.420], mean action: 4.444 [0.000, 20.000],  loss: 0.020331, mae: 0.443407, mean_q: 0.422228, mean_eps: 0.000000
 4327/5000: episode: 148, duration: 0.334s, episode steps:  28, steps per second:  84, episode reward: 41.081, mean reward:  1.467 [-2.118, 31.991], mean action: 2.964 [0.000, 15.000],  loss: 0.018940, mae: 0.438810, mean_q: 0.446338, mean_eps: 0.000000
 4342/5000: episode: 149, duration: 0.575s, episode steps:  15, steps per second:  26, episode reward: 47.978, mean reward:  3.199 [-0.221, 32.090], mean action: 2.067 [1.000, 3.000],  loss: 0.020781, mae: 0.454224, mean_q: 0.399945, mean_eps: 0.000000
 4377/5000: episode: 150, duration: 0.469s, episode steps:  35, steps per second:  75, episode reward: 35.938, mean reward:  1.027 [-3.000, 32.030], mean action: 4.229 [0.000, 20.000],  loss: 0.017888, mae: 0.429457, mean_q: 0.451792, mean_eps: 0.000000
 4393/5000: episode: 151, duration: 0.192s, episode steps:  16, steps per second:  83, episode reward: 41.550, mean reward:  2.597 [-3.000, 32.059], mean action: 1.938 [0.000, 15.000],  loss: 0.019084, mae: 0.436370, mean_q: 0.502640, mean_eps: 0.000000
 4415/5000: episode: 152, duration: 0.649s, episode steps:  22, steps per second:  34, episode reward: 44.641, mean reward:  2.029 [-2.525, 32.090], mean action: 4.182 [1.000, 14.000],  loss: 0.021945, mae: 0.450160, mean_q: 0.459559, mean_eps: 0.000000
 4466/5000: episode: 153, duration: 0.626s, episode steps:  51, steps per second:  81, episode reward: 33.000, mean reward:  0.647 [-2.868, 32.230], mean action: 3.333 [0.000, 19.000],  loss: 0.019305, mae: 0.437696, mean_q: 0.463828, mean_eps: 0.000000
 4493/5000: episode: 154, duration: 0.313s, episode steps:  27, steps per second:  86, episode reward: 44.354, mean reward:  1.643 [-2.558, 32.003], mean action: 2.111 [0.000, 19.000],  loss: 0.021500, mae: 0.446878, mean_q: 0.472512, mean_eps: 0.000000
 4518/5000: episode: 155, duration: 0.293s, episode steps:  25, steps per second:  85, episode reward: 43.722, mean reward:  1.749 [-2.611, 32.130], mean action: 3.600 [0.000, 20.000],  loss: 0.019300, mae: 0.436904, mean_q: 0.472733, mean_eps: 0.000000
 4562/5000: episode: 156, duration: 0.519s, episode steps:  44, steps per second:  85, episode reward: 44.398, mean reward:  1.009 [-2.363, 32.100], mean action: 2.273 [1.000, 11.000],  loss: 0.021130, mae: 0.446433, mean_q: 0.479634, mean_eps: 0.000000
 4578/5000: episode: 157, duration: 0.190s, episode steps:  16, steps per second:  84, episode reward: 41.210, mean reward:  2.576 [-2.450, 32.334], mean action: 4.375 [0.000, 20.000],  loss: 0.018010, mae: 0.435590, mean_q: 0.458938, mean_eps: 0.000000
 4630/5000: episode: 158, duration: 0.561s, episode steps:  52, steps per second:  93, episode reward: 32.095, mean reward:  0.617 [-3.000, 31.953], mean action: 5.885 [0.000, 21.000],  loss: 0.020034, mae: 0.443037, mean_q: 0.493377, mean_eps: 0.000000
 4669/5000: episode: 159, duration: 0.437s, episode steps:  39, steps per second:  89, episode reward: 35.395, mean reward:  0.908 [-2.653, 31.949], mean action: 4.641 [0.000, 12.000],  loss: 0.019510, mae: 0.428323, mean_q: 0.502599, mean_eps: 0.000000
 4712/5000: episode: 160, duration: 0.479s, episode steps:  43, steps per second:  90, episode reward: 37.447, mean reward:  0.871 [-2.617, 32.560], mean action: 4.907 [0.000, 14.000],  loss: 0.018166, mae: 0.434918, mean_q: 0.478685, mean_eps: 0.000000
 4748/5000: episode: 161, duration: 0.425s, episode steps:  36, steps per second:  85, episode reward: 38.020, mean reward:  1.056 [-2.366, 32.200], mean action: 3.111 [0.000, 11.000],  loss: 0.019391, mae: 0.437985, mean_q: 0.466040, mean_eps: 0.000000
 4769/5000: episode: 162, duration: 0.237s, episode steps:  21, steps per second:  89, episode reward: 42.000, mean reward:  2.000 [-2.511, 32.440], mean action: 1.857 [0.000, 9.000],  loss: 0.022459, mae: 0.448093, mean_q: 0.456819, mean_eps: 0.000000
 4796/5000: episode: 163, duration: 0.399s, episode steps:  27, steps per second:  68, episode reward: 40.845, mean reward:  1.513 [-3.000, 32.160], mean action: 3.556 [0.000, 13.000],  loss: 0.023391, mae: 0.450337, mean_q: 0.436717, mean_eps: 0.000000
 4825/5000: episode: 164, duration: 0.328s, episode steps:  29, steps per second:  89, episode reward: 41.798, mean reward:  1.441 [-3.000, 32.501], mean action: 4.690 [0.000, 20.000],  loss: 0.019084, mae: 0.436555, mean_q: 0.455246, mean_eps: 0.000000
 4854/5000: episode: 165, duration: 0.320s, episode steps:  29, steps per second:  91, episode reward: 44.938, mean reward:  1.550 [-2.163, 32.010], mean action: 2.690 [1.000, 15.000],  loss: 0.020664, mae: 0.440228, mean_q: 0.473884, mean_eps: 0.000000
 4889/5000: episode: 166, duration: 0.382s, episode steps:  35, steps per second:  92, episode reward: 38.210, mean reward:  1.092 [-3.000, 32.050], mean action: 6.629 [0.000, 18.000],  loss: 0.018080, mae: 0.432286, mean_q: 0.499307, mean_eps: 0.000000
 4915/5000: episode: 167, duration: 0.297s, episode steps:  26, steps per second:  88, episode reward: 41.168, mean reward:  1.583 [-2.598, 32.250], mean action: 4.462 [0.000, 18.000],  loss: 0.019047, mae: 0.425269, mean_q: 0.473239, mean_eps: 0.000000
 4934/5000: episode: 168, duration: 0.219s, episode steps:  19, steps per second:  87, episode reward: 37.884, mean reward:  1.994 [-3.000, 32.420], mean action: 3.684 [0.000, 18.000],  loss: 0.019302, mae: 0.422514, mean_q: 0.557325, mean_eps: 0.000000
 4949/5000: episode: 169, duration: 0.184s, episode steps:  15, steps per second:  82, episode reward: 44.790, mean reward:  2.986 [-2.185, 32.414], mean action: 2.600 [0.000, 9.000],  loss: 0.020053, mae: 0.421880, mean_q: 0.519826, mean_eps: 0.000000
 4984/5000: episode: 170, duration: 0.383s, episode steps:  35, steps per second:  91, episode reward: 41.298, mean reward:  1.180 [-2.598, 32.230], mean action: 5.029 [1.000, 15.000],  loss: 0.016764, mae: 0.417659, mean_q: 0.528001, mean_eps: 0.000000
done, took 54.662 seconds
DQN Evaluation: 755 victories out of 896 episodes
Training for 5000 steps ...
   34/5000: episode: 1, duration: 0.216s, episode steps:  34, steps per second: 158, episode reward: 32.567, mean reward:  0.958 [-3.000, 32.220], mean action: 9.441 [1.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   52/5000: episode: 2, duration: 0.126s, episode steps:  18, steps per second: 143, episode reward: 44.351, mean reward:  2.464 [-2.247, 32.010], mean action: 3.722 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   73/5000: episode: 3, duration: 0.140s, episode steps:  21, steps per second: 150, episode reward: 38.347, mean reward:  1.826 [-2.445, 32.350], mean action: 5.762 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   95/5000: episode: 4, duration: 0.149s, episode steps:  22, steps per second: 148, episode reward: 35.251, mean reward:  1.602 [-2.844, 31.795], mean action: 3.636 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  125/5000: episode: 5, duration: 0.187s, episode steps:  30, steps per second: 160, episode reward: -35.180, mean reward: -1.173 [-31.560,  2.480], mean action: 7.500 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  141/5000: episode: 6, duration: 0.107s, episode steps:  16, steps per second: 149, episode reward: 48.000, mean reward:  3.000 [ 0.250, 32.020], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  161/5000: episode: 7, duration: 0.134s, episode steps:  20, steps per second: 150, episode reward: 38.136, mean reward:  1.907 [-2.802, 31.536], mean action: 3.900 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  198/5000: episode: 8, duration: 0.215s, episode steps:  37, steps per second: 172, episode reward: -32.120, mean reward: -0.868 [-32.104,  3.059], mean action: 5.324 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  228/5000: episode: 9, duration: 0.193s, episode steps:  30, steps per second: 155, episode reward: -38.710, mean reward: -1.290 [-32.317,  3.088], mean action: 8.133 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  256/5000: episode: 10, duration: 0.183s, episode steps:  28, steps per second: 153, episode reward: 35.293, mean reward:  1.260 [-2.625, 32.200], mean action: 5.143 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  282/5000: episode: 11, duration: 0.167s, episode steps:  26, steps per second: 156, episode reward: 36.000, mean reward:  1.385 [-2.327, 32.380], mean action: 2.538 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  296/5000: episode: 12, duration: 0.100s, episode steps:  14, steps per second: 140, episode reward: 39.000, mean reward:  2.786 [-3.000, 32.350], mean action: 3.571 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  328/5000: episode: 13, duration: 0.200s, episode steps:  32, steps per second: 160, episode reward: -32.740, mean reward: -1.023 [-32.080,  2.330], mean action: 9.188 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  348/5000: episode: 14, duration: 0.131s, episode steps:  20, steps per second: 152, episode reward: 41.453, mean reward:  2.073 [-2.283, 30.458], mean action: 3.800 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  362/5000: episode: 15, duration: 0.093s, episode steps:  14, steps per second: 151, episode reward: 41.912, mean reward:  2.994 [-2.903, 32.162], mean action: 3.571 [1.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  390/5000: episode: 16, duration: 0.170s, episode steps:  28, steps per second: 165, episode reward: -35.550, mean reward: -1.270 [-31.995,  2.403], mean action: 5.143 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  415/5000: episode: 17, duration: 0.165s, episode steps:  25, steps per second: 151, episode reward: 32.773, mean reward:  1.311 [-3.000, 32.229], mean action: 7.400 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  433/5000: episode: 18, duration: 0.117s, episode steps:  18, steps per second: 154, episode reward: 38.245, mean reward:  2.125 [-3.000, 32.227], mean action: 5.056 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  452/5000: episode: 19, duration: 0.132s, episode steps:  19, steps per second: 144, episode reward: 39.000, mean reward:  2.053 [-3.000, 32.790], mean action: 3.368 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  476/5000: episode: 20, duration: 0.150s, episode steps:  24, steps per second: 160, episode reward: 32.526, mean reward:  1.355 [-2.445, 31.896], mean action: 7.625 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  506/5000: episode: 21, duration: 0.183s, episode steps:  30, steps per second: 164, episode reward: -32.480, mean reward: -1.083 [-31.997,  2.653], mean action: 7.867 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  530/5000: episode: 22, duration: 0.150s, episode steps:  24, steps per second: 160, episode reward: 35.756, mean reward:  1.490 [-3.000, 32.180], mean action: 4.750 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  546/5000: episode: 23, duration: 0.109s, episode steps:  16, steps per second: 147, episode reward: 42.000, mean reward:  2.625 [-2.262, 32.110], mean action: 4.062 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  565/5000: episode: 24, duration: 0.122s, episode steps:  19, steps per second: 155, episode reward: 34.891, mean reward:  1.836 [-3.000, 31.752], mean action: 4.421 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  586/5000: episode: 25, duration: 0.139s, episode steps:  21, steps per second: 151, episode reward: 35.798, mean reward:  1.705 [-2.949, 31.938], mean action: 6.190 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  615/5000: episode: 26, duration: 0.172s, episode steps:  29, steps per second: 169, episode reward: -35.610, mean reward: -1.228 [-32.610,  2.560], mean action: 3.931 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  641/5000: episode: 27, duration: 0.162s, episode steps:  26, steps per second: 161, episode reward: 38.707, mean reward:  1.489 [-2.375, 32.024], mean action: 5.077 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  663/5000: episode: 28, duration: 0.137s, episode steps:  22, steps per second: 160, episode reward: 35.798, mean reward:  1.627 [-2.454, 32.038], mean action: 3.909 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  679/5000: episode: 29, duration: 0.112s, episode steps:  16, steps per second: 143, episode reward: 41.443, mean reward:  2.590 [-2.107, 31.931], mean action: 3.125 [1.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  703/5000: episode: 30, duration: 0.145s, episode steps:  24, steps per second: 165, episode reward: 35.305, mean reward:  1.471 [-3.000, 31.911], mean action: 6.667 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  722/5000: episode: 31, duration: 0.132s, episode steps:  19, steps per second: 144, episode reward: 39.891, mean reward:  2.100 [-2.644, 32.000], mean action: 4.579 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  745/5000: episode: 32, duration: 0.145s, episode steps:  23, steps per second: 159, episode reward: 46.674, mean reward:  2.029 [-0.171, 33.000], mean action: 4.565 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  768/5000: episode: 33, duration: 0.150s, episode steps:  23, steps per second: 154, episode reward: 38.252, mean reward:  1.663 [-3.000, 32.093], mean action: 3.391 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  798/5000: episode: 34, duration: 0.275s, episode steps:  30, steps per second: 109, episode reward: 35.512, mean reward:  1.184 [-2.890, 32.040], mean action: 7.500 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  827/5000: episode: 35, duration: 0.188s, episode steps:  29, steps per second: 154, episode reward: 38.176, mean reward:  1.316 [-2.387, 32.050], mean action: 5.069 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  846/5000: episode: 36, duration: 0.133s, episode steps:  19, steps per second: 142, episode reward: 39.000, mean reward:  2.053 [-2.250, 33.000], mean action: 5.211 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  870/5000: episode: 37, duration: 0.149s, episode steps:  24, steps per second: 161, episode reward: 32.683, mean reward:  1.362 [-2.153, 32.370], mean action: 6.500 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  887/5000: episode: 38, duration: 0.111s, episode steps:  17, steps per second: 153, episode reward: 38.419, mean reward:  2.260 [-2.938, 32.383], mean action: 6.235 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  910/5000: episode: 39, duration: 0.142s, episode steps:  23, steps per second: 162, episode reward: 37.978, mean reward:  1.651 [-2.668, 31.252], mean action: 4.130 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  933/5000: episode: 40, duration: 0.141s, episode steps:  23, steps per second: 163, episode reward: 35.387, mean reward:  1.539 [-2.540, 31.741], mean action: 7.217 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  956/5000: episode: 41, duration: 0.141s, episode steps:  23, steps per second: 163, episode reward: 40.908, mean reward:  1.779 [-2.236, 32.073], mean action: 5.957 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  981/5000: episode: 42, duration: 0.159s, episode steps:  25, steps per second: 158, episode reward: 35.275, mean reward:  1.411 [-2.429, 33.000], mean action: 4.320 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1010/5000: episode: 43, duration: 0.246s, episode steps:  29, steps per second: 118, episode reward: 32.616, mean reward:  1.125 [-2.316, 32.074], mean action: 7.069 [0.000, 18.000],  loss: 0.016855, mae: 0.418084, mean_q: 0.448061, mean_eps: 0.000000
 1032/5000: episode: 44, duration: 0.247s, episode steps:  22, steps per second:  89, episode reward: 44.372, mean reward:  2.017 [-2.014, 32.490], mean action: 1.545 [0.000, 9.000],  loss: 0.015456, mae: 0.414052, mean_q: 0.475395, mean_eps: 0.000000
 1055/5000: episode: 45, duration: 0.259s, episode steps:  23, steps per second:  89, episode reward: 40.816, mean reward:  1.775 [-2.194, 32.410], mean action: 6.522 [0.000, 20.000],  loss: 0.017661, mae: 0.434661, mean_q: 0.425180, mean_eps: 0.000000
 1072/5000: episode: 46, duration: 0.197s, episode steps:  17, steps per second:  86, episode reward: 38.901, mean reward:  2.288 [-2.880, 32.901], mean action: 5.588 [0.000, 14.000],  loss: 0.018778, mae: 0.453647, mean_q: 0.448943, mean_eps: 0.000000
 1110/5000: episode: 47, duration: 0.417s, episode steps:  38, steps per second:  91, episode reward: 38.053, mean reward:  1.001 [-2.255, 32.093], mean action: 5.658 [1.000, 11.000],  loss: 0.018625, mae: 0.445292, mean_q: 0.469683, mean_eps: 0.000000
 1125/5000: episode: 48, duration: 0.174s, episode steps:  15, steps per second:  86, episode reward: 40.713, mean reward:  2.714 [-2.290, 32.061], mean action: 5.467 [1.000, 20.000],  loss: 0.019049, mae: 0.454621, mean_q: 0.452233, mean_eps: 0.000000
 1147/5000: episode: 49, duration: 0.244s, episode steps:  22, steps per second:  90, episode reward: 38.390, mean reward:  1.745 [-2.614, 32.043], mean action: 7.955 [0.000, 20.000],  loss: 0.019733, mae: 0.443000, mean_q: 0.480729, mean_eps: 0.000000
 1172/5000: episode: 50, duration: 0.273s, episode steps:  25, steps per second:  91, episode reward: 35.554, mean reward:  1.422 [-2.542, 31.929], mean action: 5.840 [0.000, 14.000],  loss: 0.016894, mae: 0.423653, mean_q: 0.555574, mean_eps: 0.000000
 1199/5000: episode: 51, duration: 0.301s, episode steps:  27, steps per second:  90, episode reward: -35.470, mean reward: -1.314 [-32.245,  2.909], mean action: 6.000 [0.000, 15.000],  loss: 0.016110, mae: 0.427882, mean_q: 0.494496, mean_eps: 0.000000
 1220/5000: episode: 52, duration: 0.235s, episode steps:  21, steps per second:  89, episode reward: 32.503, mean reward:  1.548 [-2.560, 31.643], mean action: 3.619 [0.000, 12.000],  loss: 0.023990, mae: 0.480989, mean_q: 0.446628, mean_eps: 0.000000
 1246/5000: episode: 53, duration: 0.330s, episode steps:  26, steps per second:  79, episode reward: 35.191, mean reward:  1.353 [-2.658, 31.878], mean action: 5.038 [0.000, 20.000],  loss: 0.017792, mae: 0.451410, mean_q: 0.432538, mean_eps: 0.000000
 1265/5000: episode: 54, duration: 0.222s, episode steps:  19, steps per second:  86, episode reward: 35.143, mean reward:  1.850 [-3.000, 32.698], mean action: 4.105 [0.000, 14.000],  loss: 0.018422, mae: 0.441605, mean_q: 0.480445, mean_eps: 0.000000
 1285/5000: episode: 55, duration: 0.224s, episode steps:  20, steps per second:  89, episode reward: 35.208, mean reward:  1.760 [-2.998, 32.760], mean action: 4.800 [0.000, 15.000],  loss: 0.018663, mae: 0.451213, mean_q: 0.476264, mean_eps: 0.000000
 1305/5000: episode: 56, duration: 0.226s, episode steps:  20, steps per second:  88, episode reward: 43.203, mean reward:  2.160 [-2.597, 32.110], mean action: 4.050 [0.000, 14.000],  loss: 0.021128, mae: 0.465971, mean_q: 0.447967, mean_eps: 0.000000
 1328/5000: episode: 57, duration: 0.259s, episode steps:  23, steps per second:  89, episode reward: 38.661, mean reward:  1.681 [-2.505, 32.331], mean action: 2.609 [0.000, 14.000],  loss: 0.018078, mae: 0.441395, mean_q: 0.448328, mean_eps: 0.000000
 1349/5000: episode: 58, duration: 0.234s, episode steps:  21, steps per second:  90, episode reward: 32.818, mean reward:  1.563 [-2.842, 32.818], mean action: 5.143 [0.000, 17.000],  loss: 0.017648, mae: 0.439559, mean_q: 0.539240, mean_eps: 0.000000
 1381/5000: episode: 59, duration: 0.354s, episode steps:  32, steps per second:  90, episode reward: -32.190, mean reward: -1.006 [-32.146,  2.350], mean action: 8.344 [0.000, 15.000],  loss: 0.019563, mae: 0.441447, mean_q: 0.497007, mean_eps: 0.000000
 1400/5000: episode: 60, duration: 0.222s, episode steps:  19, steps per second:  86, episode reward: 40.894, mean reward:  2.152 [-2.126, 31.988], mean action: 4.000 [0.000, 13.000],  loss: 0.020848, mae: 0.454886, mean_q: 0.512269, mean_eps: 0.000000
 1421/5000: episode: 61, duration: 0.234s, episode steps:  21, steps per second:  90, episode reward: 35.900, mean reward:  1.710 [-2.468, 31.930], mean action: 3.333 [0.000, 15.000],  loss: 0.023368, mae: 0.468247, mean_q: 0.479042, mean_eps: 0.000000
 1451/5000: episode: 62, duration: 0.333s, episode steps:  30, steps per second:  90, episode reward: 32.379, mean reward:  1.079 [-2.615, 31.699], mean action: 4.967 [0.000, 14.000],  loss: 0.022732, mae: 0.465221, mean_q: 0.507235, mean_eps: 0.000000
 1473/5000: episode: 63, duration: 0.249s, episode steps:  22, steps per second:  88, episode reward: 35.230, mean reward:  1.601 [-3.000, 32.345], mean action: 3.818 [1.000, 15.000],  loss: 0.016914, mae: 0.449781, mean_q: 0.482361, mean_eps: 0.000000
 1483/5000: episode: 64, duration: 0.125s, episode steps:  10, steps per second:  80, episode reward: 45.000, mean reward:  4.500 [-0.082, 30.702], mean action: 1.400 [1.000, 3.000],  loss: 0.018913, mae: 0.453373, mean_q: 0.473182, mean_eps: 0.000000
 1508/5000: episode: 65, duration: 0.278s, episode steps:  25, steps per second:  90, episode reward: 39.000, mean reward:  1.560 [-2.156, 32.300], mean action: 2.800 [0.000, 15.000],  loss: 0.016871, mae: 0.433971, mean_q: 0.528000, mean_eps: 0.000000
 1525/5000: episode: 66, duration: 0.201s, episode steps:  17, steps per second:  84, episode reward: 43.461, mean reward:  2.557 [-2.316, 32.460], mean action: 3.235 [0.000, 15.000],  loss: 0.016660, mae: 0.427303, mean_q: 0.500226, mean_eps: 0.000000
 1545/5000: episode: 67, duration: 0.235s, episode steps:  20, steps per second:  85, episode reward: 41.579, mean reward:  2.079 [-2.652, 32.240], mean action: 3.600 [0.000, 14.000],  loss: 0.020768, mae: 0.445921, mean_q: 0.476664, mean_eps: 0.000000
 1573/5000: episode: 68, duration: 0.321s, episode steps:  28, steps per second:  87, episode reward: 34.618, mean reward:  1.236 [-2.380, 32.020], mean action: 8.286 [0.000, 21.000],  loss: 0.017131, mae: 0.439356, mean_q: 0.471661, mean_eps: 0.000000
 1602/5000: episode: 69, duration: 0.319s, episode steps:  29, steps per second:  91, episode reward: -35.430, mean reward: -1.222 [-32.457,  2.690], mean action: 7.448 [0.000, 15.000],  loss: 0.019369, mae: 0.452333, mean_q: 0.463468, mean_eps: 0.000000
 1628/5000: episode: 70, duration: 0.289s, episode steps:  26, steps per second:  90, episode reward: -35.910, mean reward: -1.381 [-31.987,  2.570], mean action: 8.192 [0.000, 20.000],  loss: 0.016750, mae: 0.432983, mean_q: 0.504349, mean_eps: 0.000000
 1646/5000: episode: 71, duration: 0.205s, episode steps:  18, steps per second:  88, episode reward: 38.489, mean reward:  2.138 [-2.904, 33.000], mean action: 5.278 [0.000, 15.000],  loss: 0.022875, mae: 0.465710, mean_q: 0.432062, mean_eps: 0.000000
 1683/5000: episode: 72, duration: 0.411s, episode steps:  37, steps per second:  90, episode reward: -32.240, mean reward: -0.871 [-32.052,  2.581], mean action: 4.703 [0.000, 19.000],  loss: 0.018615, mae: 0.433692, mean_q: 0.449142, mean_eps: 0.000000
 1708/5000: episode: 73, duration: 0.285s, episode steps:  25, steps per second:  88, episode reward: 35.044, mean reward:  1.402 [-3.000, 32.045], mean action: 5.880 [0.000, 20.000],  loss: 0.019338, mae: 0.439848, mean_q: 0.488079, mean_eps: 0.000000
 1739/5000: episode: 74, duration: 0.340s, episode steps:  31, steps per second:  91, episode reward: 32.245, mean reward:  1.040 [-2.459, 32.212], mean action: 7.097 [1.000, 21.000],  loss: 0.020008, mae: 0.441674, mean_q: 0.521405, mean_eps: 0.000000
 1763/5000: episode: 75, duration: 0.270s, episode steps:  24, steps per second:  89, episode reward: 39.000, mean reward:  1.625 [-2.594, 32.260], mean action: 4.208 [0.000, 15.000],  loss: 0.019596, mae: 0.448687, mean_q: 0.494445, mean_eps: 0.000000
 1806/5000: episode: 76, duration: 0.474s, episode steps:  43, steps per second:  91, episode reward: -35.590, mean reward: -0.828 [-32.097,  2.788], mean action: 5.558 [0.000, 14.000],  loss: 0.022587, mae: 0.471129, mean_q: 0.453298, mean_eps: 0.000000
 1836/5000: episode: 77, duration: 0.534s, episode steps:  30, steps per second:  56, episode reward: 32.374, mean reward:  1.079 [-2.526, 33.000], mean action: 4.233 [0.000, 14.000],  loss: 0.017592, mae: 0.436252, mean_q: 0.508057, mean_eps: 0.000000
 1877/5000: episode: 78, duration: 0.490s, episode steps:  41, steps per second:  84, episode reward: 35.743, mean reward:  0.872 [-2.126, 32.121], mean action: 5.146 [0.000, 15.000],  loss: 0.019213, mae: 0.443132, mean_q: 0.469153, mean_eps: 0.000000
 1894/5000: episode: 79, duration: 0.260s, episode steps:  17, steps per second:  65, episode reward: 38.820, mean reward:  2.284 [-2.886, 32.487], mean action: 3.529 [0.000, 15.000],  loss: 0.023288, mae: 0.449218, mean_q: 0.510983, mean_eps: 0.000000
 1914/5000: episode: 80, duration: 0.230s, episode steps:  20, steps per second:  87, episode reward: 35.900, mean reward:  1.795 [-2.361, 32.010], mean action: 3.150 [0.000, 15.000],  loss: 0.021810, mae: 0.443744, mean_q: 0.469804, mean_eps: 0.000000
 1949/5000: episode: 81, duration: 0.396s, episode steps:  35, steps per second:  88, episode reward: -32.330, mean reward: -0.924 [-32.033,  3.000], mean action: 9.571 [0.000, 19.000],  loss: 0.015413, mae: 0.416848, mean_q: 0.463432, mean_eps: 0.000000
 1973/5000: episode: 82, duration: 0.260s, episode steps:  24, steps per second:  92, episode reward: -36.000, mean reward: -1.500 [-33.000,  2.710], mean action: 6.958 [0.000, 15.000],  loss: 0.018776, mae: 0.440533, mean_q: 0.436440, mean_eps: 0.000000
 1990/5000: episode: 83, duration: 0.196s, episode steps:  17, steps per second:  87, episode reward: 38.841, mean reward:  2.285 [-2.472, 32.390], mean action: 4.765 [0.000, 15.000],  loss: 0.019101, mae: 0.436178, mean_q: 0.495164, mean_eps: 0.000000
 2013/5000: episode: 84, duration: 0.261s, episode steps:  23, steps per second:  88, episode reward: 35.221, mean reward:  1.531 [-2.600, 31.887], mean action: 4.913 [1.000, 20.000],  loss: 0.019080, mae: 0.444604, mean_q: 0.437606, mean_eps: 0.000000
 2034/5000: episode: 85, duration: 0.243s, episode steps:  21, steps per second:  86, episode reward: 38.380, mean reward:  1.828 [-2.429, 31.991], mean action: 3.619 [0.000, 15.000],  loss: 0.021601, mae: 0.471109, mean_q: 0.445985, mean_eps: 0.000000
 2081/5000: episode: 86, duration: 0.506s, episode steps:  47, steps per second:  93, episode reward: -35.080, mean reward: -0.746 [-32.596,  3.385], mean action: 10.766 [3.000, 15.000],  loss: 0.016861, mae: 0.450304, mean_q: 0.395850, mean_eps: 0.000000
 2127/5000: episode: 87, duration: 0.486s, episode steps:  46, steps per second:  95, episode reward: 36.000, mean reward:  0.783 [-2.225, 32.370], mean action: 4.283 [1.000, 12.000],  loss: 0.016986, mae: 0.450812, mean_q: 0.382796, mean_eps: 0.000000
 2154/5000: episode: 88, duration: 0.321s, episode steps:  27, steps per second:  84, episode reward: -35.810, mean reward: -1.326 [-32.163,  2.241], mean action: 7.815 [0.000, 20.000],  loss: 0.018079, mae: 0.448202, mean_q: 0.425977, mean_eps: 0.000000
 2179/5000: episode: 89, duration: 0.346s, episode steps:  25, steps per second:  72, episode reward: -33.000, mean reward: -1.320 [-32.155,  2.410], mean action: 7.800 [0.000, 16.000],  loss: 0.016286, mae: 0.447388, mean_q: 0.413350, mean_eps: 0.000000
 2206/5000: episode: 90, duration: 0.370s, episode steps:  27, steps per second:  73, episode reward: -32.450, mean reward: -1.202 [-32.265,  2.823], mean action: 4.630 [0.000, 20.000],  loss: 0.017769, mae: 0.449720, mean_q: 0.449304, mean_eps: 0.000000
 2227/5000: episode: 91, duration: 0.231s, episode steps:  21, steps per second:  91, episode reward: 34.615, mean reward:  1.648 [-3.000, 32.700], mean action: 7.429 [0.000, 19.000],  loss: 0.020262, mae: 0.458370, mean_q: 0.440655, mean_eps: 0.000000
 2259/5000: episode: 92, duration: 0.349s, episode steps:  32, steps per second:  92, episode reward: 32.745, mean reward:  1.023 [-3.000, 32.111], mean action: 6.125 [0.000, 15.000],  loss: 0.018918, mae: 0.466353, mean_q: 0.368836, mean_eps: 0.000000
 2289/5000: episode: 93, duration: 0.350s, episode steps:  30, steps per second:  86, episode reward: 37.539, mean reward:  1.251 [-2.438, 32.220], mean action: 4.667 [0.000, 19.000],  loss: 0.019497, mae: 0.464900, mean_q: 0.423849, mean_eps: 0.000000
 2339/5000: episode: 94, duration: 0.930s, episode steps:  50, steps per second:  54, episode reward: 33.000, mean reward:  0.660 [-2.901, 32.220], mean action: 11.440 [0.000, 20.000],  loss: 0.018756, mae: 0.445349, mean_q: 0.472227, mean_eps: 0.000000
 2365/5000: episode: 95, duration: 0.530s, episode steps:  26, steps per second:  49, episode reward: -33.000, mean reward: -1.269 [-32.305,  2.590], mean action: 6.462 [0.000, 19.000],  loss: 0.018596, mae: 0.435689, mean_q: 0.467674, mean_eps: 0.000000
 2389/5000: episode: 96, duration: 0.294s, episode steps:  24, steps per second:  82, episode reward: -32.160, mean reward: -1.340 [-32.521,  2.635], mean action: 7.458 [0.000, 14.000],  loss: 0.019505, mae: 0.445151, mean_q: 0.467378, mean_eps: 0.000000
 2416/5000: episode: 97, duration: 0.338s, episode steps:  27, steps per second:  80, episode reward: -32.910, mean reward: -1.219 [-32.596,  2.309], mean action: 7.407 [0.000, 17.000],  loss: 0.019971, mae: 0.467243, mean_q: 0.442886, mean_eps: 0.000000
 2440/5000: episode: 98, duration: 0.275s, episode steps:  24, steps per second:  87, episode reward: -32.910, mean reward: -1.371 [-32.243,  3.000], mean action: 2.625 [0.000, 9.000],  loss: 0.023114, mae: 0.463117, mean_q: 0.472764, mean_eps: 0.000000
 2469/5000: episode: 99, duration: 0.326s, episode steps:  29, steps per second:  89, episode reward: 41.481, mean reward:  1.430 [-2.376, 33.000], mean action: 3.862 [0.000, 15.000],  loss: 0.019362, mae: 0.437442, mean_q: 0.497731, mean_eps: 0.000000
 2495/5000: episode: 100, duration: 0.288s, episode steps:  26, steps per second:  90, episode reward: -34.610, mean reward: -1.331 [-32.088,  2.572], mean action: 7.846 [0.000, 15.000],  loss: 0.016356, mae: 0.435086, mean_q: 0.466502, mean_eps: 0.000000
 2523/5000: episode: 101, duration: 0.308s, episode steps:  28, steps per second:  91, episode reward: 35.553, mean reward:  1.270 [-2.502, 32.041], mean action: 3.500 [0.000, 15.000],  loss: 0.018893, mae: 0.450241, mean_q: 0.478992, mean_eps: 0.000000
 2545/5000: episode: 102, duration: 0.253s, episode steps:  22, steps per second:  87, episode reward: 38.220, mean reward:  1.737 [-3.000, 32.154], mean action: 6.136 [0.000, 20.000],  loss: 0.017769, mae: 0.434860, mean_q: 0.475375, mean_eps: 0.000000
 2567/5000: episode: 103, duration: 0.243s, episode steps:  22, steps per second:  91, episode reward: 36.000, mean reward:  1.636 [-2.686, 32.530], mean action: 4.364 [0.000, 15.000],  loss: 0.020811, mae: 0.455405, mean_q: 0.442359, mean_eps: 0.000000
 2588/5000: episode: 104, duration: 0.236s, episode steps:  21, steps per second:  89, episode reward: 41.801, mean reward:  1.991 [-2.110, 32.320], mean action: 4.333 [0.000, 15.000],  loss: 0.022398, mae: 0.473306, mean_q: 0.433859, mean_eps: 0.000000
 2637/5000: episode: 105, duration: 0.529s, episode steps:  49, steps per second:  93, episode reward: -32.640, mean reward: -0.666 [-32.130,  2.330], mean action: 8.633 [0.000, 21.000],  loss: 0.020689, mae: 0.460632, mean_q: 0.430548, mean_eps: 0.000000
 2664/5000: episode: 106, duration: 0.322s, episode steps:  27, steps per second:  84, episode reward: 36.000, mean reward:  1.333 [-3.000, 30.670], mean action: 3.481 [0.000, 15.000],  loss: 0.018438, mae: 0.457320, mean_q: 0.394950, mean_eps: 0.000000
 2690/5000: episode: 107, duration: 0.324s, episode steps:  26, steps per second:  80, episode reward: -38.150, mean reward: -1.467 [-31.630,  2.131], mean action: 6.769 [0.000, 20.000],  loss: 0.017152, mae: 0.454490, mean_q: 0.409541, mean_eps: 0.000000
 2718/5000: episode: 108, duration: 0.311s, episode steps:  28, steps per second:  90, episode reward: 37.005, mean reward:  1.322 [-2.412, 32.005], mean action: 4.214 [0.000, 15.000],  loss: 0.019996, mae: 0.454246, mean_q: 0.429071, mean_eps: 0.000000
 2739/5000: episode: 109, duration: 0.235s, episode steps:  21, steps per second:  89, episode reward: 32.698, mean reward:  1.557 [-3.000, 32.797], mean action: 4.762 [0.000, 17.000],  loss: 0.022060, mae: 0.476301, mean_q: 0.396219, mean_eps: 0.000000
 2771/5000: episode: 110, duration: 0.343s, episode steps:  32, steps per second:  93, episode reward: -35.250, mean reward: -1.102 [-32.250,  2.140], mean action: 7.656 [0.000, 20.000],  loss: 0.021942, mae: 0.461050, mean_q: 0.485800, mean_eps: 0.000000
 2802/5000: episode: 111, duration: 0.348s, episode steps:  31, steps per second:  89, episode reward: -41.730, mean reward: -1.346 [-32.265,  2.230], mean action: 5.839 [0.000, 15.000],  loss: 0.020067, mae: 0.447330, mean_q: 0.514989, mean_eps: 0.000000
 2836/5000: episode: 112, duration: 0.461s, episode steps:  34, steps per second:  74, episode reward: 32.691, mean reward:  0.961 [-2.469, 29.848], mean action: 4.941 [0.000, 15.000],  loss: 0.020497, mae: 0.446185, mean_q: 0.513151, mean_eps: 0.000000
 2853/5000: episode: 113, duration: 0.192s, episode steps:  17, steps per second:  89, episode reward: 38.758, mean reward:  2.280 [-2.502, 32.160], mean action: 5.059 [0.000, 15.000],  loss: 0.019647, mae: 0.449158, mean_q: 0.455114, mean_eps: 0.000000
 2881/5000: episode: 114, duration: 0.317s, episode steps:  28, steps per second:  88, episode reward: 35.546, mean reward:  1.270 [-2.407, 32.470], mean action: 2.893 [0.000, 9.000],  loss: 0.018293, mae: 0.455078, mean_q: 0.449868, mean_eps: 0.000000
 2903/5000: episode: 115, duration: 0.249s, episode steps:  22, steps per second:  88, episode reward: 35.497, mean reward:  1.614 [-2.651, 32.050], mean action: 8.773 [1.000, 21.000],  loss: 0.019521, mae: 0.470741, mean_q: 0.411041, mean_eps: 0.000000
 2929/5000: episode: 116, duration: 0.282s, episode steps:  26, steps per second:  92, episode reward: -35.940, mean reward: -1.382 [-32.170,  2.500], mean action: 7.885 [0.000, 20.000],  loss: 0.019227, mae: 0.465158, mean_q: 0.404584, mean_eps: 0.000000
 2953/5000: episode: 117, duration: 0.277s, episode steps:  24, steps per second:  86, episode reward: 38.368, mean reward:  1.599 [-2.124, 33.000], mean action: 4.042 [0.000, 12.000],  loss: 0.020604, mae: 0.454140, mean_q: 0.508206, mean_eps: 0.000000
 2976/5000: episode: 118, duration: 0.258s, episode steps:  23, steps per second:  89, episode reward: 32.250, mean reward:  1.402 [-3.000, 31.939], mean action: 4.261 [0.000, 14.000],  loss: 0.017418, mae: 0.447637, mean_q: 0.490184, mean_eps: 0.000000
 3001/5000: episode: 119, duration: 0.283s, episode steps:  25, steps per second:  88, episode reward: 32.475, mean reward:  1.299 [-2.275, 32.280], mean action: 6.480 [1.000, 15.000],  loss: 0.025607, mae: 0.498396, mean_q: 0.435067, mean_eps: 0.000000
 3018/5000: episode: 120, duration: 0.203s, episode steps:  17, steps per second:  84, episode reward: 39.000, mean reward:  2.294 [-2.246, 33.000], mean action: 4.176 [0.000, 14.000],  loss: 0.020083, mae: 0.475675, mean_q: 0.385874, mean_eps: 0.000000
 3040/5000: episode: 121, duration: 0.258s, episode steps:  22, steps per second:  85, episode reward: 38.905, mean reward:  1.768 [-2.474, 32.101], mean action: 7.864 [0.000, 20.000],  loss: 0.019440, mae: 0.473370, mean_q: 0.397276, mean_eps: 0.000000
 3053/5000: episode: 122, duration: 0.190s, episode steps:  13, steps per second:  68, episode reward: 44.047, mean reward:  3.388 [-2.078, 32.100], mean action: 3.154 [0.000, 12.000],  loss: 0.021329, mae: 0.451101, mean_q: 0.494784, mean_eps: 0.000000
 3079/5000: episode: 123, duration: 0.440s, episode steps:  26, steps per second:  59, episode reward: 35.082, mean reward:  1.349 [-3.000, 32.193], mean action: 7.808 [0.000, 15.000],  loss: 0.023476, mae: 0.454828, mean_q: 0.559568, mean_eps: 0.000000
 3105/5000: episode: 124, duration: 0.401s, episode steps:  26, steps per second:  65, episode reward: 32.176, mean reward:  1.238 [-3.000, 32.140], mean action: 5.269 [0.000, 14.000],  loss: 0.020036, mae: 0.431365, mean_q: 0.537920, mean_eps: 0.000000
 3157/5000: episode: 125, duration: 0.597s, episode steps:  52, steps per second:  87, episode reward: 38.548, mean reward:  0.741 [-2.903, 32.397], mean action: 1.923 [0.000, 14.000],  loss: 0.021994, mae: 0.464269, mean_q: 0.450135, mean_eps: 0.000000
 3180/5000: episode: 126, duration: 0.256s, episode steps:  23, steps per second:  90, episode reward: 32.369, mean reward:  1.407 [-3.000, 32.369], mean action: 3.609 [0.000, 15.000],  loss: 0.020138, mae: 0.462104, mean_q: 0.464656, mean_eps: 0.000000
 3200/5000: episode: 127, duration: 0.352s, episode steps:  20, steps per second:  57, episode reward: -35.770, mean reward: -1.788 [-33.000,  2.904], mean action: 5.250 [1.000, 15.000],  loss: 0.022423, mae: 0.469773, mean_q: 0.475852, mean_eps: 0.000000
 3226/5000: episode: 128, duration: 0.301s, episode steps:  26, steps per second:  86, episode reward: 35.565, mean reward:  1.368 [-2.732, 32.580], mean action: 6.538 [0.000, 14.000],  loss: 0.018236, mae: 0.449007, mean_q: 0.503956, mean_eps: 0.000000
 3246/5000: episode: 129, duration: 0.227s, episode steps:  20, steps per second:  88, episode reward: -35.880, mean reward: -1.794 [-32.269,  2.510], mean action: 7.200 [0.000, 15.000],  loss: 0.020537, mae: 0.448821, mean_q: 0.564194, mean_eps: 0.000000
 3271/5000: episode: 130, duration: 0.283s, episode steps:  25, steps per second:  88, episode reward: 37.633, mean reward:  1.505 [-2.268, 32.070], mean action: 5.080 [0.000, 18.000],  loss: 0.016602, mae: 0.437696, mean_q: 0.496389, mean_eps: 0.000000
 3306/5000: episode: 131, duration: 0.376s, episode steps:  35, steps per second:  93, episode reward: 41.087, mean reward:  1.174 [-2.686, 32.952], mean action: 4.971 [0.000, 20.000],  loss: 0.019956, mae: 0.452699, mean_q: 0.465010, mean_eps: 0.000000
 3326/5000: episode: 132, duration: 0.229s, episode steps:  20, steps per second:  87, episode reward: 40.119, mean reward:  2.006 [-2.187, 33.000], mean action: 5.550 [0.000, 15.000],  loss: 0.020502, mae: 0.445037, mean_q: 0.498458, mean_eps: 0.000000
 3365/5000: episode: 133, duration: 0.480s, episode steps:  39, steps per second:  81, episode reward: -32.510, mean reward: -0.834 [-32.004,  2.330], mean action: 9.513 [0.000, 20.000],  loss: 0.024332, mae: 0.465393, mean_q: 0.462319, mean_eps: 0.000000
 3409/5000: episode: 134, duration: 0.491s, episode steps:  44, steps per second:  90, episode reward: 37.698, mean reward:  0.857 [-2.323, 31.928], mean action: 3.091 [0.000, 13.000],  loss: 0.018243, mae: 0.443402, mean_q: 0.466904, mean_eps: 0.000000
 3428/5000: episode: 135, duration: 0.222s, episode steps:  19, steps per second:  86, episode reward: 40.538, mean reward:  2.134 [-2.158, 32.687], mean action: 5.684 [0.000, 20.000],  loss: 0.022572, mae: 0.468723, mean_q: 0.421218, mean_eps: 0.000000
 3448/5000: episode: 136, duration: 0.228s, episode steps:  20, steps per second:  88, episode reward: 35.825, mean reward:  1.791 [-2.966, 32.830], mean action: 6.900 [0.000, 15.000],  loss: 0.021016, mae: 0.463453, mean_q: 0.411323, mean_eps: 0.000000
 3475/5000: episode: 137, duration: 0.478s, episode steps:  27, steps per second:  57, episode reward: -35.240, mean reward: -1.305 [-32.305,  2.420], mean action: 9.259 [0.000, 18.000],  loss: 0.018091, mae: 0.443138, mean_q: 0.469743, mean_eps: 0.000000
 3497/5000: episode: 138, duration: 0.266s, episode steps:  22, steps per second:  83, episode reward: 32.686, mean reward:  1.486 [-3.000, 32.686], mean action: 4.318 [0.000, 15.000],  loss: 0.017495, mae: 0.438798, mean_q: 0.530192, mean_eps: 0.000000
 3523/5000: episode: 139, duration: 0.360s, episode steps:  26, steps per second:  72, episode reward: 32.587, mean reward:  1.253 [-2.781, 32.580], mean action: 7.346 [0.000, 14.000],  loss: 0.018213, mae: 0.435898, mean_q: 0.487937, mean_eps: 0.000000
 3539/5000: episode: 140, duration: 0.189s, episode steps:  16, steps per second:  85, episode reward: 38.007, mean reward:  2.375 [-3.000, 32.901], mean action: 5.875 [0.000, 14.000],  loss: 0.016902, mae: 0.437713, mean_q: 0.441378, mean_eps: 0.000000
 3559/5000: episode: 141, duration: 0.224s, episode steps:  20, steps per second:  89, episode reward: 35.708, mean reward:  1.785 [-2.531, 32.248], mean action: 5.600 [0.000, 20.000],  loss: 0.022243, mae: 0.458322, mean_q: 0.466717, mean_eps: 0.000000
 3580/5000: episode: 142, duration: 0.234s, episode steps:  21, steps per second:  90, episode reward: 35.734, mean reward:  1.702 [-2.315, 31.764], mean action: 2.952 [0.000, 11.000],  loss: 0.020200, mae: 0.448429, mean_q: 0.465113, mean_eps: 0.000000
 3601/5000: episode: 143, duration: 0.247s, episode steps:  21, steps per second:  85, episode reward: 38.890, mean reward:  1.852 [-2.319, 32.150], mean action: 3.952 [0.000, 15.000],  loss: 0.017084, mae: 0.434426, mean_q: 0.477702, mean_eps: 0.000000
 3630/5000: episode: 144, duration: 0.321s, episode steps:  29, steps per second:  90, episode reward: 36.000, mean reward:  1.241 [-2.463, 32.060], mean action: 4.069 [0.000, 14.000],  loss: 0.020966, mae: 0.449235, mean_q: 0.478588, mean_eps: 0.000000
 3652/5000: episode: 145, duration: 0.248s, episode steps:  22, steps per second:  89, episode reward: 37.162, mean reward:  1.689 [-2.452, 32.056], mean action: 3.909 [0.000, 14.000],  loss: 0.022015, mae: 0.459108, mean_q: 0.455810, mean_eps: 0.000000
 3744/5000: episode: 146, duration: 0.960s, episode steps:  92, steps per second:  96, episode reward: -32.620, mean reward: -0.355 [-32.038,  2.400], mean action: 13.022 [0.000, 17.000],  loss: 0.019208, mae: 0.444326, mean_q: 0.461692, mean_eps: 0.000000
 3760/5000: episode: 147, duration: 0.190s, episode steps:  16, steps per second:  84, episode reward: 41.072, mean reward:  2.567 [-2.143, 32.128], mean action: 6.250 [0.000, 19.000],  loss: 0.021709, mae: 0.455284, mean_q: 0.415145, mean_eps: 0.000000
 3781/5000: episode: 148, duration: 0.243s, episode steps:  21, steps per second:  86, episode reward: 32.756, mean reward:  1.560 [-3.000, 32.290], mean action: 5.381 [0.000, 19.000],  loss: 0.020333, mae: 0.435753, mean_q: 0.487316, mean_eps: 0.000000
 3806/5000: episode: 149, duration: 0.430s, episode steps:  25, steps per second:  58, episode reward: 37.391, mean reward:  1.496 [-2.688, 32.620], mean action: 4.520 [0.000, 15.000],  loss: 0.025281, mae: 0.460570, mean_q: 0.552588, mean_eps: 0.000000
 3826/5000: episode: 150, duration: 0.232s, episode steps:  20, steps per second:  86, episode reward: 41.233, mean reward:  2.062 [-2.641, 32.010], mean action: 3.850 [0.000, 14.000],  loss: 0.023965, mae: 0.459939, mean_q: 0.548169, mean_eps: 0.000000
 3847/5000: episode: 151, duration: 0.336s, episode steps:  21, steps per second:  62, episode reward: 38.031, mean reward:  1.811 [-2.196, 32.066], mean action: 3.095 [0.000, 9.000],  loss: 0.019347, mae: 0.436157, mean_q: 0.559079, mean_eps: 0.000000
 3878/5000: episode: 152, duration: 0.356s, episode steps:  31, steps per second:  87, episode reward: 32.460, mean reward:  1.047 [-2.445, 31.690], mean action: 6.774 [0.000, 21.000],  loss: 0.020421, mae: 0.446351, mean_q: 0.538205, mean_eps: 0.000000
 3906/5000: episode: 153, duration: 0.306s, episode steps:  28, steps per second:  91, episode reward: 33.000, mean reward:  1.179 [-2.901, 32.270], mean action: 6.000 [1.000, 12.000],  loss: 0.020232, mae: 0.451311, mean_q: 0.510827, mean_eps: 0.000000
 3960/5000: episode: 154, duration: 0.588s, episode steps:  54, steps per second:  92, episode reward: 35.537, mean reward:  0.658 [-2.484, 32.115], mean action: 10.944 [0.000, 21.000],  loss: 0.017225, mae: 0.442975, mean_q: 0.483867, mean_eps: 0.000000
 3985/5000: episode: 155, duration: 0.281s, episode steps:  25, steps per second:  89, episode reward: 37.203, mean reward:  1.488 [-3.000, 33.000], mean action: 8.840 [0.000, 20.000],  loss: 0.020787, mae: 0.457648, mean_q: 0.540999, mean_eps: 0.000000
 3999/5000: episode: 156, duration: 0.166s, episode steps:  14, steps per second:  84, episode reward: 43.842, mean reward:  3.132 [-2.674, 32.901], mean action: 4.571 [0.000, 20.000],  loss: 0.015884, mae: 0.445023, mean_q: 0.515469, mean_eps: 0.000000
 4014/5000: episode: 157, duration: 0.170s, episode steps:  15, steps per second:  88, episode reward: 38.704, mean reward:  2.580 [-3.000, 32.901], mean action: 4.267 [0.000, 15.000],  loss: 0.024263, mae: 0.491353, mean_q: 0.398710, mean_eps: 0.000000
 4038/5000: episode: 158, duration: 0.268s, episode steps:  24, steps per second:  90, episode reward: 35.285, mean reward:  1.470 [-2.631, 32.222], mean action: 3.458 [0.000, 19.000],  loss: 0.017361, mae: 0.449508, mean_q: 0.403617, mean_eps: 0.000000
 4068/5000: episode: 159, duration: 0.330s, episode steps:  30, steps per second:  91, episode reward: 32.410, mean reward:  1.080 [-2.390, 31.980], mean action: 4.400 [0.000, 19.000],  loss: 0.018147, mae: 0.430853, mean_q: 0.489624, mean_eps: 0.000000
 4093/5000: episode: 160, duration: 0.277s, episode steps:  25, steps per second:  90, episode reward: -35.050, mean reward: -1.402 [-32.037,  2.919], mean action: 6.680 [0.000, 21.000],  loss: 0.019857, mae: 0.460574, mean_q: 0.430707, mean_eps: 0.000000
 4108/5000: episode: 161, duration: 0.176s, episode steps:  15, steps per second:  85, episode reward: 38.849, mean reward:  2.590 [-2.361, 33.000], mean action: 4.600 [0.000, 19.000],  loss: 0.018046, mae: 0.440794, mean_q: 0.459868, mean_eps: 0.000000
 4136/5000: episode: 162, duration: 0.310s, episode steps:  28, steps per second:  90, episode reward: 38.187, mean reward:  1.364 [-2.407, 31.888], mean action: 3.286 [0.000, 19.000],  loss: 0.019349, mae: 0.449923, mean_q: 0.425684, mean_eps: 0.000000
 4157/5000: episode: 163, duration: 0.236s, episode steps:  21, steps per second:  89, episode reward: 38.066, mean reward:  1.813 [-3.000, 32.250], mean action: 5.286 [0.000, 15.000],  loss: 0.021083, mae: 0.455592, mean_q: 0.517007, mean_eps: 0.000000
 4201/5000: episode: 164, duration: 0.480s, episode steps:  44, steps per second:  92, episode reward: 35.734, mean reward:  0.812 [-2.428, 31.992], mean action: 5.841 [0.000, 20.000],  loss: 0.019882, mae: 0.450249, mean_q: 0.469544, mean_eps: 0.000000
 4224/5000: episode: 165, duration: 0.249s, episode steps:  23, steps per second:  92, episode reward: -38.680, mean reward: -1.682 [-31.816,  2.366], mean action: 3.565 [0.000, 20.000],  loss: 0.018383, mae: 0.444028, mean_q: 0.498933, mean_eps: 0.000000
 4247/5000: episode: 166, duration: 0.255s, episode steps:  23, steps per second:  90, episode reward: -38.820, mean reward: -1.688 [-32.035,  2.250], mean action: 3.087 [0.000, 14.000],  loss: 0.023143, mae: 0.458826, mean_q: 0.485874, mean_eps: 0.000000
 4299/5000: episode: 167, duration: 0.555s, episode steps:  52, steps per second:  94, episode reward: -36.000, mean reward: -0.692 [-32.776,  2.744], mean action: 7.019 [0.000, 17.000],  loss: 0.020208, mae: 0.449018, mean_q: 0.466124, mean_eps: 0.000000
 4322/5000: episode: 168, duration: 0.257s, episode steps:  23, steps per second:  90, episode reward: 41.918, mean reward:  1.823 [-2.322, 32.400], mean action: 4.391 [1.000, 20.000],  loss: 0.018356, mae: 0.446436, mean_q: 0.476234, mean_eps: 0.000000
 4370/5000: episode: 169, duration: 0.513s, episode steps:  48, steps per second:  94, episode reward: 33.000, mean reward:  0.687 [-3.000, 32.540], mean action: 5.312 [0.000, 16.000],  loss: 0.022820, mae: 0.464846, mean_q: 0.501385, mean_eps: 0.000000
 4394/5000: episode: 170, duration: 0.259s, episode steps:  24, steps per second:  93, episode reward: -35.190, mean reward: -1.466 [-32.413,  3.000], mean action: 10.542 [0.000, 20.000],  loss: 0.020126, mae: 0.453729, mean_q: 0.474571, mean_eps: 0.000000
 4414/5000: episode: 171, duration: 0.232s, episode steps:  20, steps per second:  86, episode reward: 38.207, mean reward:  1.910 [-2.416, 32.029], mean action: 7.050 [0.000, 21.000],  loss: 0.017118, mae: 0.441814, mean_q: 0.420454, mean_eps: 0.000000
 4439/5000: episode: 172, duration: 0.296s, episode steps:  25, steps per second:  84, episode reward: 33.000, mean reward:  1.320 [-2.900, 32.120], mean action: 6.720 [0.000, 17.000],  loss: 0.017294, mae: 0.439234, mean_q: 0.448289, mean_eps: 0.000000
 4496/5000: episode: 173, duration: 0.627s, episode steps:  57, steps per second:  91, episode reward: 38.068, mean reward:  0.668 [-3.000, 33.000], mean action: 2.877 [0.000, 20.000],  loss: 0.020341, mae: 0.443834, mean_q: 0.495855, mean_eps: 0.000000
 4529/5000: episode: 174, duration: 0.362s, episode steps:  33, steps per second:  91, episode reward: 32.253, mean reward:  0.977 [-3.000, 32.840], mean action: 7.667 [0.000, 19.000],  loss: 0.021130, mae: 0.450865, mean_q: 0.461235, mean_eps: 0.000000
 4553/5000: episode: 175, duration: 0.271s, episode steps:  24, steps per second:  88, episode reward: -35.080, mean reward: -1.462 [-32.070,  2.699], mean action: 6.708 [0.000, 20.000],  loss: 0.019121, mae: 0.450617, mean_q: 0.458642, mean_eps: 0.000000
 4572/5000: episode: 176, duration: 0.222s, episode steps:  19, steps per second:  86, episode reward: 38.620, mean reward:  2.033 [-2.802, 32.250], mean action: 4.000 [0.000, 14.000],  loss: 0.021354, mae: 0.443926, mean_q: 0.488154, mean_eps: 0.000000
 4598/5000: episode: 177, duration: 0.299s, episode steps:  26, steps per second:  87, episode reward: -33.000, mean reward: -1.269 [-32.193,  2.902], mean action: 5.154 [0.000, 17.000],  loss: 0.020978, mae: 0.443039, mean_q: 0.491323, mean_eps: 0.000000
 4622/5000: episode: 178, duration: 0.372s, episode steps:  24, steps per second:  65, episode reward: 40.362, mean reward:  1.682 [-2.203, 32.149], mean action: 5.542 [0.000, 21.000],  loss: 0.020590, mae: 0.449447, mean_q: 0.500832, mean_eps: 0.000000
 4645/5000: episode: 179, duration: 0.264s, episode steps:  23, steps per second:  87, episode reward: 32.142, mean reward:  1.397 [-3.000, 32.880], mean action: 7.217 [0.000, 14.000],  loss: 0.018641, mae: 0.442229, mean_q: 0.509606, mean_eps: 0.000000
 4664/5000: episode: 180, duration: 0.230s, episode steps:  19, steps per second:  83, episode reward: 38.392, mean reward:  2.021 [-2.667, 31.982], mean action: 4.421 [0.000, 20.000],  loss: 0.017166, mae: 0.436496, mean_q: 0.491865, mean_eps: 0.000000
 4689/5000: episode: 181, duration: 0.282s, episode steps:  25, steps per second:  89, episode reward: -32.910, mean reward: -1.316 [-32.204,  2.591], mean action: 5.600 [0.000, 14.000],  loss: 0.020827, mae: 0.468097, mean_q: 0.434230, mean_eps: 0.000000
 4716/5000: episode: 182, duration: 0.302s, episode steps:  27, steps per second:  89, episode reward: 36.000, mean reward:  1.333 [-2.429, 32.220], mean action: 3.185 [0.000, 15.000],  loss: 0.020688, mae: 0.479795, mean_q: 0.400166, mean_eps: 0.000000
 4739/5000: episode: 183, duration: 0.261s, episode steps:  23, steps per second:  88, episode reward: 41.120, mean reward:  1.788 [-2.294, 32.050], mean action: 3.696 [1.000, 15.000],  loss: 0.018107, mae: 0.451995, mean_q: 0.429477, mean_eps: 0.000000
 4769/5000: episode: 184, duration: 0.331s, episode steps:  30, steps per second:  91, episode reward: -32.880, mean reward: -1.096 [-32.136,  2.320], mean action: 4.967 [0.000, 18.000],  loss: 0.022922, mae: 0.464594, mean_q: 0.471350, mean_eps: 0.000000
 4797/5000: episode: 185, duration: 0.324s, episode steps:  28, steps per second:  86, episode reward: -33.000, mean reward: -1.179 [-32.406,  2.800], mean action: 8.143 [0.000, 20.000],  loss: 0.022178, mae: 0.472619, mean_q: 0.470023, mean_eps: 0.000000
 4829/5000: episode: 186, duration: 0.357s, episode steps:  32, steps per second:  90, episode reward: -32.400, mean reward: -1.012 [-32.268,  2.604], mean action: 3.812 [0.000, 15.000],  loss: 0.020291, mae: 0.461833, mean_q: 0.501260, mean_eps: 0.000000
 4854/5000: episode: 187, duration: 0.280s, episode steps:  25, steps per second:  89, episode reward: -35.490, mean reward: -1.420 [-32.181,  2.460], mean action: 5.640 [0.000, 15.000],  loss: 0.019482, mae: 0.458826, mean_q: 0.465951, mean_eps: 0.000000
 4880/5000: episode: 188, duration: 0.290s, episode steps:  26, steps per second:  90, episode reward: 37.620, mean reward:  1.447 [-2.426, 33.000], mean action: 5.692 [0.000, 20.000],  loss: 0.018503, mae: 0.453328, mean_q: 0.496436, mean_eps: 0.000000
 4901/5000: episode: 189, duration: 0.244s, episode steps:  21, steps per second:  86, episode reward: 37.819, mean reward:  1.801 [-2.339, 29.976], mean action: 5.238 [0.000, 20.000],  loss: 0.020534, mae: 0.468034, mean_q: 0.466163, mean_eps: 0.000000
 4924/5000: episode: 190, duration: 0.255s, episode steps:  23, steps per second:  90, episode reward: 35.820, mean reward:  1.557 [-2.562, 32.702], mean action: 3.609 [0.000, 15.000],  loss: 0.017637, mae: 0.452787, mean_q: 0.426434, mean_eps: 0.000000
 4951/5000: episode: 191, duration: 0.300s, episode steps:  27, steps per second:  90, episode reward: -32.540, mean reward: -1.205 [-31.866,  3.000], mean action: 4.333 [0.000, 17.000],  loss: 0.022067, mae: 0.481355, mean_q: 0.407467, mean_eps: 0.000000
 4976/5000: episode: 192, duration: 0.282s, episode steps:  25, steps per second:  89, episode reward: 33.000, mean reward:  1.320 [-2.366, 32.160], mean action: 5.720 [0.000, 15.000],  loss: 0.021750, mae: 0.467341, mean_q: 0.449138, mean_eps: 0.000000
done, took 53.743 seconds
DQN Evaluation: 901 victories out of 1089 episodes
Training for 5000 steps ...
   39/5000: episode: 1, duration: 0.250s, episode steps:  39, steps per second: 156, episode reward: 35.128, mean reward:  0.901 [-2.824, 31.926], mean action: 2.897 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   65/5000: episode: 2, duration: 0.171s, episode steps:  26, steps per second: 152, episode reward: 41.670, mean reward:  1.603 [-3.000, 32.080], mean action: 3.846 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   80/5000: episode: 3, duration: 0.113s, episode steps:  15, steps per second: 133, episode reward: 46.809, mean reward:  3.121 [-0.040, 32.130], mean action: 4.133 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  101/5000: episode: 4, duration: 0.149s, episode steps:  21, steps per second: 141, episode reward: 46.758, mean reward:  2.227 [-0.722, 32.390], mean action: 2.238 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  152/5000: episode: 5, duration: 0.296s, episode steps:  51, steps per second: 172, episode reward: 34.623, mean reward:  0.679 [-3.000, 31.810], mean action: 6.196 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  173/5000: episode: 6, duration: 0.139s, episode steps:  21, steps per second: 151, episode reward: 41.384, mean reward:  1.971 [-2.518, 31.896], mean action: 2.571 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  204/5000: episode: 7, duration: 0.190s, episode steps:  31, steps per second: 163, episode reward: 38.166, mean reward:  1.231 [-2.479, 31.885], mean action: 4.935 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  230/5000: episode: 8, duration: 0.160s, episode steps:  26, steps per second: 162, episode reward: 40.714, mean reward:  1.566 [-2.300, 32.343], mean action: 2.077 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  260/5000: episode: 9, duration: 0.195s, episode steps:  30, steps per second: 154, episode reward: 43.135, mean reward:  1.438 [-2.644, 32.050], mean action: 3.967 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  288/5000: episode: 10, duration: 0.185s, episode steps:  28, steps per second: 152, episode reward: 44.340, mean reward:  1.584 [-2.643, 32.227], mean action: 2.464 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  309/5000: episode: 11, duration: 0.134s, episode steps:  21, steps per second: 157, episode reward: 41.369, mean reward:  1.970 [-2.297, 33.000], mean action: 2.762 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  377/5000: episode: 12, duration: 0.386s, episode steps:  68, steps per second: 176, episode reward: 37.874, mean reward:  0.557 [-2.505, 32.243], mean action: 5.059 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  408/5000: episode: 13, duration: 0.192s, episode steps:  31, steps per second: 161, episode reward: 37.816, mean reward:  1.220 [-3.000, 32.278], mean action: 3.581 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  433/5000: episode: 14, duration: 0.158s, episode steps:  25, steps per second: 159, episode reward: 37.320, mean reward:  1.493 [-2.781, 31.920], mean action: 5.560 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  458/5000: episode: 15, duration: 0.158s, episode steps:  25, steps per second: 159, episode reward: 41.151, mean reward:  1.646 [-3.000, 32.120], mean action: 4.200 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  483/5000: episode: 16, duration: 0.160s, episode steps:  25, steps per second: 157, episode reward: 44.484, mean reward:  1.779 [-3.000, 32.128], mean action: 5.040 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  508/5000: episode: 17, duration: 0.153s, episode steps:  25, steps per second: 164, episode reward: 40.815, mean reward:  1.633 [-2.938, 31.727], mean action: 3.720 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  542/5000: episode: 18, duration: 0.219s, episode steps:  34, steps per second: 155, episode reward: 41.322, mean reward:  1.215 [-2.287, 32.650], mean action: 3.765 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  568/5000: episode: 19, duration: 0.160s, episode steps:  26, steps per second: 163, episode reward: 37.924, mean reward:  1.459 [-2.362, 32.131], mean action: 4.423 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  607/5000: episode: 20, duration: 0.236s, episode steps:  39, steps per second: 166, episode reward: 32.620, mean reward:  0.836 [-2.690, 29.410], mean action: 4.077 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  629/5000: episode: 21, duration: 0.150s, episode steps:  22, steps per second: 147, episode reward: 42.000, mean reward:  1.909 [-2.401, 32.590], mean action: 2.500 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  653/5000: episode: 22, duration: 0.149s, episode steps:  24, steps per second: 161, episode reward: 38.113, mean reward:  1.588 [-3.000, 31.652], mean action: 3.208 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  680/5000: episode: 23, duration: 0.180s, episode steps:  27, steps per second: 150, episode reward: 38.089, mean reward:  1.411 [-2.283, 31.794], mean action: 3.370 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  708/5000: episode: 24, duration: 0.192s, episode steps:  28, steps per second: 145, episode reward: 41.109, mean reward:  1.468 [-2.050, 32.440], mean action: 3.500 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  741/5000: episode: 25, duration: 0.200s, episode steps:  33, steps per second: 165, episode reward: 39.000, mean reward:  1.182 [-3.000, 32.300], mean action: 2.636 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  757/5000: episode: 26, duration: 0.200s, episode steps:  16, steps per second:  80, episode reward: 44.460, mean reward:  2.779 [-2.101, 32.397], mean action: 2.375 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  786/5000: episode: 27, duration: 0.181s, episode steps:  29, steps per second: 160, episode reward: 41.155, mean reward:  1.419 [-2.663, 32.310], mean action: 7.517 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  822/5000: episode: 28, duration: 0.225s, episode steps:  36, steps per second: 160, episode reward: 38.647, mean reward:  1.074 [-2.811, 32.280], mean action: 2.667 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  852/5000: episode: 29, duration: 0.189s, episode steps:  30, steps per second: 158, episode reward: 35.036, mean reward:  1.168 [-3.000, 31.570], mean action: 4.067 [0.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  881/5000: episode: 30, duration: 0.269s, episode steps:  29, steps per second: 108, episode reward: 43.612, mean reward:  1.504 [-2.170, 31.663], mean action: 4.448 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  904/5000: episode: 31, duration: 0.154s, episode steps:  23, steps per second: 149, episode reward: 35.406, mean reward:  1.539 [-2.623, 31.718], mean action: 4.217 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  919/5000: episode: 32, duration: 0.106s, episode steps:  15, steps per second: 142, episode reward: 41.236, mean reward:  2.749 [-2.480, 32.060], mean action: 3.000 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  943/5000: episode: 33, duration: 0.160s, episode steps:  24, steps per second: 150, episode reward: 44.097, mean reward:  1.837 [-2.363, 32.357], mean action: 3.792 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  972/5000: episode: 34, duration: 0.191s, episode steps:  29, steps per second: 152, episode reward: -32.240, mean reward: -1.112 [-32.650,  2.940], mean action: 6.241 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  996/5000: episode: 35, duration: 0.154s, episode steps:  24, steps per second: 155, episode reward: 40.333, mean reward:  1.681 [-2.202, 32.820], mean action: 4.208 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1020/5000: episode: 36, duration: 0.261s, episode steps:  24, steps per second:  92, episode reward: 43.884, mean reward:  1.828 [-2.765, 32.170], mean action: 3.208 [0.000, 14.000],  loss: 0.019912, mae: 0.448650, mean_q: 0.477134, mean_eps: 0.000000
 1039/5000: episode: 37, duration: 0.219s, episode steps:  19, steps per second:  87, episode reward: 44.105, mean reward:  2.321 [-2.166, 32.580], mean action: 4.579 [0.000, 15.000],  loss: 0.023596, mae: 0.469296, mean_q: 0.467886, mean_eps: 0.000000
 1059/5000: episode: 38, duration: 0.269s, episode steps:  20, steps per second:  74, episode reward: 38.722, mean reward:  1.936 [-2.469, 32.130], mean action: 3.150 [0.000, 9.000],  loss: 0.020732, mae: 0.449773, mean_q: 0.529674, mean_eps: 0.000000
 1082/5000: episode: 39, duration: 0.267s, episode steps:  23, steps per second:  86, episode reward: 38.069, mean reward:  1.655 [-2.941, 32.320], mean action: 3.435 [0.000, 9.000],  loss: 0.020698, mae: 0.447305, mean_q: 0.559345, mean_eps: 0.000000
 1105/5000: episode: 40, duration: 0.290s, episode steps:  23, steps per second:  79, episode reward: 38.055, mean reward:  1.655 [-2.976, 32.100], mean action: 4.348 [0.000, 20.000],  loss: 0.019176, mae: 0.447242, mean_q: 0.510933, mean_eps: 0.000000
 1145/5000: episode: 41, duration: 0.450s, episode steps:  40, steps per second:  89, episode reward: 37.744, mean reward:  0.944 [-3.000, 32.190], mean action: 4.850 [0.000, 20.000],  loss: 0.019080, mae: 0.456371, mean_q: 0.441049, mean_eps: 0.000000
 1164/5000: episode: 42, duration: 0.231s, episode steps:  19, steps per second:  82, episode reward: 38.595, mean reward:  2.031 [-2.877, 32.410], mean action: 5.421 [0.000, 18.000],  loss: 0.019773, mae: 0.452890, mean_q: 0.523656, mean_eps: 0.000000
 1189/5000: episode: 43, duration: 0.291s, episode steps:  25, steps per second:  86, episode reward: 41.284, mean reward:  1.651 [-2.301, 32.360], mean action: 3.760 [0.000, 20.000],  loss: 0.022543, mae: 0.465674, mean_q: 0.540624, mean_eps: 0.000000
 1214/5000: episode: 44, duration: 0.288s, episode steps:  25, steps per second:  87, episode reward: 40.900, mean reward:  1.636 [-2.903, 32.069], mean action: 3.440 [0.000, 14.000],  loss: 0.022206, mae: 0.465854, mean_q: 0.588115, mean_eps: 0.000000
 1239/5000: episode: 45, duration: 0.297s, episode steps:  25, steps per second:  84, episode reward: 44.037, mean reward:  1.761 [-2.035, 31.772], mean action: 2.120 [0.000, 14.000],  loss: 0.016200, mae: 0.450209, mean_q: 0.486540, mean_eps: 0.000000
 1257/5000: episode: 46, duration: 0.273s, episode steps:  18, steps per second:  66, episode reward: 43.554, mean reward:  2.420 [-2.355, 32.450], mean action: 3.000 [0.000, 9.000],  loss: 0.021658, mae: 0.476887, mean_q: 0.468224, mean_eps: 0.000000
 1301/5000: episode: 47, duration: 0.487s, episode steps:  44, steps per second:  90, episode reward: 35.879, mean reward:  0.815 [-3.000, 32.370], mean action: 6.295 [0.000, 15.000],  loss: 0.019033, mae: 0.476340, mean_q: 0.408799, mean_eps: 0.000000
 1327/5000: episode: 48, duration: 0.290s, episode steps:  26, steps per second:  90, episode reward: 41.868, mean reward:  1.610 [-3.000, 32.280], mean action: 2.731 [0.000, 9.000],  loss: 0.020804, mae: 0.470399, mean_q: 0.472707, mean_eps: 0.000000
 1370/5000: episode: 49, duration: 0.476s, episode steps:  43, steps per second:  90, episode reward: 32.318, mean reward:  0.752 [-2.633, 31.685], mean action: 6.233 [0.000, 17.000],  loss: 0.018917, mae: 0.451329, mean_q: 0.457688, mean_eps: 0.000000
 1449/5000: episode: 50, duration: 0.835s, episode steps:  79, steps per second:  95, episode reward: -35.230, mean reward: -0.446 [-32.369,  2.240], mean action: 9.772 [0.000, 15.000],  loss: 0.019666, mae: 0.456332, mean_q: 0.468827, mean_eps: 0.000000
 1474/5000: episode: 51, duration: 0.289s, episode steps:  25, steps per second:  87, episode reward: 42.000, mean reward:  1.680 [-2.338, 32.260], mean action: 4.000 [0.000, 20.000],  loss: 0.018963, mae: 0.465821, mean_q: 0.414250, mean_eps: 0.000000
 1500/5000: episode: 52, duration: 0.298s, episode steps:  26, steps per second:  87, episode reward: 35.460, mean reward:  1.364 [-2.622, 31.590], mean action: 3.615 [0.000, 15.000],  loss: 0.019119, mae: 0.460338, mean_q: 0.452151, mean_eps: 0.000000
 1535/5000: episode: 53, duration: 0.385s, episode steps:  35, steps per second:  91, episode reward: 38.463, mean reward:  1.099 [-2.502, 31.903], mean action: 2.286 [0.000, 15.000],  loss: 0.017914, mae: 0.448582, mean_q: 0.510828, mean_eps: 0.000000
 1555/5000: episode: 54, duration: 0.228s, episode steps:  20, steps per second:  88, episode reward: 38.507, mean reward:  1.925 [-3.000, 32.160], mean action: 4.550 [0.000, 15.000],  loss: 0.022360, mae: 0.481477, mean_q: 0.470743, mean_eps: 0.000000
 1577/5000: episode: 55, duration: 0.255s, episode steps:  22, steps per second:  86, episode reward: 46.030, mean reward:  2.092 [-0.922, 32.250], mean action: 3.409 [0.000, 20.000],  loss: 0.021155, mae: 0.483288, mean_q: 0.431166, mean_eps: 0.000000
 1601/5000: episode: 56, duration: 0.273s, episode steps:  24, steps per second:  88, episode reward: 40.686, mean reward:  1.695 [-2.415, 33.000], mean action: 7.500 [0.000, 20.000],  loss: 0.019531, mae: 0.464203, mean_q: 0.490436, mean_eps: 0.000000
 1619/5000: episode: 57, duration: 0.211s, episode steps:  18, steps per second:  85, episode reward: 44.203, mean reward:  2.456 [-2.578, 32.180], mean action: 1.667 [0.000, 12.000],  loss: 0.016100, mae: 0.447144, mean_q: 0.479727, mean_eps: 0.000000
 1641/5000: episode: 58, duration: 0.247s, episode steps:  22, steps per second:  89, episode reward: 38.437, mean reward:  1.747 [-3.000, 32.050], mean action: 4.773 [0.000, 11.000],  loss: 0.020229, mae: 0.473705, mean_q: 0.463985, mean_eps: 0.000000
 1679/5000: episode: 59, duration: 0.432s, episode steps:  38, steps per second:  88, episode reward: -34.710, mean reward: -0.913 [-31.972,  3.000], mean action: 6.632 [0.000, 17.000],  loss: 0.018213, mae: 0.457858, mean_q: 0.492383, mean_eps: 0.000000
 1715/5000: episode: 60, duration: 0.408s, episode steps:  36, steps per second:  88, episode reward: 35.782, mean reward:  0.994 [-3.000, 32.372], mean action: 7.250 [0.000, 20.000],  loss: 0.021625, mae: 0.466456, mean_q: 0.483494, mean_eps: 0.000000
 1740/5000: episode: 61, duration: 0.281s, episode steps:  25, steps per second:  89, episode reward: 41.780, mean reward:  1.671 [-2.170, 31.993], mean action: 2.880 [0.000, 15.000],  loss: 0.019252, mae: 0.462914, mean_q: 0.438426, mean_eps: 0.000000
 1770/5000: episode: 62, duration: 0.325s, episode steps:  30, steps per second:  92, episode reward: 35.324, mean reward:  1.177 [-3.000, 32.250], mean action: 8.200 [0.000, 20.000],  loss: 0.018317, mae: 0.457229, mean_q: 0.486820, mean_eps: 0.000000
 1800/5000: episode: 63, duration: 0.334s, episode steps:  30, steps per second:  90, episode reward: 41.429, mean reward:  1.381 [-3.000, 32.950], mean action: 4.033 [0.000, 20.000],  loss: 0.017765, mae: 0.462713, mean_q: 0.495511, mean_eps: 0.000000
 1843/5000: episode: 64, duration: 0.464s, episode steps:  43, steps per second:  93, episode reward: -32.470, mean reward: -0.755 [-32.093,  2.360], mean action: 7.814 [0.000, 20.000],  loss: 0.020339, mae: 0.478591, mean_q: 0.468231, mean_eps: 0.000000
 1883/5000: episode: 65, duration: 0.434s, episode steps:  40, steps per second:  92, episode reward: 38.466, mean reward:  0.962 [-3.000, 32.193], mean action: 4.450 [0.000, 18.000],  loss: 0.020064, mae: 0.482282, mean_q: 0.417925, mean_eps: 0.000000
 1923/5000: episode: 66, duration: 0.432s, episode steps:  40, steps per second:  93, episode reward: 40.414, mean reward:  1.010 [-2.301, 31.983], mean action: 4.175 [0.000, 20.000],  loss: 0.016466, mae: 0.458145, mean_q: 0.417044, mean_eps: 0.000000
 1945/5000: episode: 67, duration: 0.248s, episode steps:  22, steps per second:  89, episode reward: 38.903, mean reward:  1.768 [-2.474, 31.983], mean action: 2.955 [0.000, 12.000],  loss: 0.017755, mae: 0.453345, mean_q: 0.466285, mean_eps: 0.000000
 1969/5000: episode: 68, duration: 0.274s, episode steps:  24, steps per second:  88, episode reward: 42.879, mean reward:  1.787 [-2.280, 32.340], mean action: 3.792 [0.000, 20.000],  loss: 0.024080, mae: 0.489392, mean_q: 0.457950, mean_eps: 0.000000
 2001/5000: episode: 69, duration: 0.413s, episode steps:  32, steps per second:  78, episode reward: 38.185, mean reward:  1.193 [-2.857, 32.140], mean action: 4.812 [0.000, 19.000],  loss: 0.017698, mae: 0.450789, mean_q: 0.471680, mean_eps: 0.000000
 2025/5000: episode: 70, duration: 0.309s, episode steps:  24, steps per second:  78, episode reward: 41.262, mean reward:  1.719 [-2.807, 32.270], mean action: 2.458 [0.000, 9.000],  loss: 0.017769, mae: 0.453964, mean_q: 0.475792, mean_eps: 0.000000
 2057/5000: episode: 71, duration: 0.360s, episode steps:  32, steps per second:  89, episode reward: 41.550, mean reward:  1.298 [-2.254, 31.570], mean action: 2.094 [0.000, 11.000],  loss: 0.018600, mae: 0.455251, mean_q: 0.506386, mean_eps: 0.000000
 2079/5000: episode: 72, duration: 0.247s, episode steps:  22, steps per second:  89, episode reward: 39.000, mean reward:  1.773 [-3.000, 30.343], mean action: 4.727 [0.000, 20.000],  loss: 0.018522, mae: 0.466565, mean_q: 0.424525, mean_eps: 0.000000
 2117/5000: episode: 73, duration: 0.423s, episode steps:  38, steps per second:  90, episode reward: 34.494, mean reward:  0.908 [-3.000, 32.800], mean action: 8.500 [0.000, 21.000],  loss: 0.020585, mae: 0.462781, mean_q: 0.475720, mean_eps: 0.000000
 2144/5000: episode: 74, duration: 0.310s, episode steps:  27, steps per second:  87, episode reward: 42.758, mean reward:  1.584 [-2.325, 31.791], mean action: 8.444 [0.000, 20.000],  loss: 0.019459, mae: 0.450316, mean_q: 0.495757, mean_eps: 0.000000
 2170/5000: episode: 75, duration: 0.313s, episode steps:  26, steps per second:  83, episode reward: 47.325, mean reward:  1.820 [-0.220, 32.103], mean action: 1.385 [0.000, 3.000],  loss: 0.018306, mae: 0.450610, mean_q: 0.456430, mean_eps: 0.000000
 2190/5000: episode: 76, duration: 0.234s, episode steps:  20, steps per second:  85, episode reward: 44.717, mean reward:  2.236 [-2.531, 31.747], mean action: 2.450 [0.000, 14.000],  loss: 0.015999, mae: 0.449904, mean_q: 0.430522, mean_eps: 0.000000
 2232/5000: episode: 77, duration: 0.458s, episode steps:  42, steps per second:  92, episode reward: 38.544, mean reward:  0.918 [-2.249, 32.220], mean action: 6.738 [0.000, 20.000],  loss: 0.016473, mae: 0.472197, mean_q: 0.383223, mean_eps: 0.000000
 2264/5000: episode: 78, duration: 0.351s, episode steps:  32, steps per second:  91, episode reward: 41.286, mean reward:  1.290 [-3.000, 31.546], mean action: 4.500 [0.000, 20.000],  loss: 0.016721, mae: 0.470974, mean_q: 0.398340, mean_eps: 0.000000
 2295/5000: episode: 79, duration: 0.366s, episode steps:  31, steps per second:  85, episode reward: 35.313, mean reward:  1.139 [-2.941, 31.635], mean action: 3.194 [0.000, 15.000],  loss: 0.018457, mae: 0.463987, mean_q: 0.452319, mean_eps: 0.000000
 2315/5000: episode: 80, duration: 0.228s, episode steps:  20, steps per second:  88, episode reward: 41.446, mean reward:  2.072 [-2.612, 31.773], mean action: 1.950 [0.000, 12.000],  loss: 0.022995, mae: 0.477498, mean_q: 0.457170, mean_eps: 0.000000
 2342/5000: episode: 81, duration: 0.296s, episode steps:  27, steps per second:  91, episode reward: 38.630, mean reward:  1.431 [-2.943, 32.370], mean action: 5.556 [0.000, 14.000],  loss: 0.018074, mae: 0.442460, mean_q: 0.468600, mean_eps: 0.000000
 2358/5000: episode: 82, duration: 0.188s, episode steps:  16, steps per second:  85, episode reward: 44.459, mean reward:  2.779 [-2.056, 32.550], mean action: 4.125 [0.000, 14.000],  loss: 0.018476, mae: 0.457669, mean_q: 0.435180, mean_eps: 0.000000
 2402/5000: episode: 83, duration: 0.480s, episode steps:  44, steps per second:  92, episode reward: 35.809, mean reward:  0.814 [-3.000, 32.250], mean action: 5.136 [0.000, 21.000],  loss: 0.022058, mae: 0.476326, mean_q: 0.439993, mean_eps: 0.000000
 2430/5000: episode: 84, duration: 0.306s, episode steps:  28, steps per second:  91, episode reward: 35.925, mean reward:  1.283 [-2.701, 31.965], mean action: 2.036 [0.000, 9.000],  loss: 0.021762, mae: 0.474888, mean_q: 0.454358, mean_eps: 0.000000
 2460/5000: episode: 85, duration: 0.412s, episode steps:  30, steps per second:  73, episode reward: 38.233, mean reward:  1.274 [-2.275, 32.506], mean action: 2.933 [0.000, 9.000],  loss: 0.021109, mae: 0.478430, mean_q: 0.416881, mean_eps: 0.000000
 2481/5000: episode: 86, duration: 0.243s, episode steps:  21, steps per second:  86, episode reward: 41.502, mean reward:  1.976 [-2.461, 32.220], mean action: 2.619 [0.000, 9.000],  loss: 0.014506, mae: 0.467102, mean_q: 0.384280, mean_eps: 0.000000
 2499/5000: episode: 87, duration: 0.215s, episode steps:  18, steps per second:  84, episode reward: 46.723, mean reward:  2.596 [-0.067, 32.150], mean action: 2.111 [1.000, 9.000],  loss: 0.019421, mae: 0.473552, mean_q: 0.442004, mean_eps: 0.000000
 2530/5000: episode: 88, duration: 0.340s, episode steps:  31, steps per second:  91, episode reward: 38.508, mean reward:  1.242 [-2.807, 32.140], mean action: 4.710 [0.000, 15.000],  loss: 0.022050, mae: 0.472897, mean_q: 0.470323, mean_eps: 0.000000
 2562/5000: episode: 89, duration: 0.580s, episode steps:  32, steps per second:  55, episode reward: 40.653, mean reward:  1.270 [-2.203, 31.991], mean action: 4.469 [0.000, 15.000],  loss: 0.021258, mae: 0.475114, mean_q: 0.497423, mean_eps: 0.000000
 2606/5000: episode: 90, duration: 0.487s, episode steps:  44, steps per second:  90, episode reward: 38.326, mean reward:  0.871 [-3.000, 31.910], mean action: 3.591 [0.000, 20.000],  loss: 0.018742, mae: 0.464913, mean_q: 0.496256, mean_eps: 0.000000
 2632/5000: episode: 91, duration: 0.296s, episode steps:  26, steps per second:  88, episode reward: 38.515, mean reward:  1.481 [-3.000, 32.290], mean action: 4.654 [0.000, 14.000],  loss: 0.017231, mae: 0.446531, mean_q: 0.502459, mean_eps: 0.000000
 2657/5000: episode: 92, duration: 0.312s, episode steps:  25, steps per second:  80, episode reward: 38.395, mean reward:  1.536 [-2.315, 32.020], mean action: 3.560 [0.000, 12.000],  loss: 0.022070, mae: 0.465970, mean_q: 0.493351, mean_eps: 0.000000
 2687/5000: episode: 93, duration: 0.353s, episode steps:  30, steps per second:  85, episode reward: 38.836, mean reward:  1.295 [-2.840, 32.336], mean action: 4.100 [0.000, 20.000],  loss: 0.019247, mae: 0.459691, mean_q: 0.435679, mean_eps: 0.000000
 2738/5000: episode: 94, duration: 0.576s, episode steps:  51, steps per second:  89, episode reward: 39.000, mean reward:  0.765 [-2.487, 32.270], mean action: 3.686 [1.000, 20.000],  loss: 0.017438, mae: 0.458037, mean_q: 0.435616, mean_eps: 0.000000
 2761/5000: episode: 95, duration: 0.279s, episode steps:  23, steps per second:  82, episode reward: 41.521, mean reward:  1.805 [-2.581, 32.296], mean action: 2.522 [0.000, 15.000],  loss: 0.018432, mae: 0.458035, mean_q: 0.483496, mean_eps: 0.000000
 2788/5000: episode: 96, duration: 0.309s, episode steps:  27, steps per second:  87, episode reward: 38.230, mean reward:  1.416 [-2.686, 32.200], mean action: 3.556 [0.000, 17.000],  loss: 0.017024, mae: 0.444638, mean_q: 0.528160, mean_eps: 0.000000
 2814/5000: episode: 97, duration: 0.297s, episode steps:  26, steps per second:  87, episode reward: 43.891, mean reward:  1.688 [-2.504, 31.940], mean action: 3.308 [0.000, 15.000],  loss: 0.018859, mae: 0.459356, mean_q: 0.454844, mean_eps: 0.000000
 2828/5000: episode: 98, duration: 0.173s, episode steps:  14, steps per second:  81, episode reward: 44.586, mean reward:  3.185 [-3.000, 32.904], mean action: 4.143 [0.000, 15.000],  loss: 0.020538, mae: 0.477883, mean_q: 0.405888, mean_eps: 0.000000
 2850/5000: episode: 99, duration: 0.250s, episode steps:  22, steps per second:  88, episode reward: 38.230, mean reward:  1.738 [-2.884, 32.530], mean action: 3.955 [0.000, 15.000],  loss: 0.021914, mae: 0.491590, mean_q: 0.392050, mean_eps: 0.000000
 2869/5000: episode: 100, duration: 0.222s, episode steps:  19, steps per second:  86, episode reward: 38.135, mean reward:  2.007 [-2.343, 32.657], mean action: 3.105 [0.000, 11.000],  loss: 0.016955, mae: 0.464003, mean_q: 0.406430, mean_eps: 0.000000
 2898/5000: episode: 101, duration: 0.376s, episode steps:  29, steps per second:  77, episode reward: 38.493, mean reward:  1.327 [-2.589, 32.210], mean action: 3.172 [0.000, 15.000],  loss: 0.022655, mae: 0.483583, mean_q: 0.431076, mean_eps: 0.000000
 2934/5000: episode: 102, duration: 0.415s, episode steps:  36, steps per second:  87, episode reward: 35.660, mean reward:  0.991 [-2.778, 32.080], mean action: 5.528 [0.000, 15.000],  loss: 0.020973, mae: 0.469034, mean_q: 0.464052, mean_eps: 0.000000
 2962/5000: episode: 103, duration: 0.329s, episode steps:  28, steps per second:  85, episode reward: 42.000, mean reward:  1.500 [-2.062, 30.590], mean action: 2.143 [0.000, 9.000],  loss: 0.018807, mae: 0.451512, mean_q: 0.498055, mean_eps: 0.000000
 2986/5000: episode: 104, duration: 0.273s, episode steps:  24, steps per second:  88, episode reward: 41.495, mean reward:  1.729 [-2.818, 32.190], mean action: 3.292 [0.000, 9.000],  loss: 0.020937, mae: 0.469546, mean_q: 0.492969, mean_eps: 0.000000
 3017/5000: episode: 105, duration: 0.345s, episode steps:  31, steps per second:  90, episode reward: 38.346, mean reward:  1.237 [-3.000, 32.136], mean action: 3.290 [0.000, 14.000],  loss: 0.024826, mae: 0.485208, mean_q: 0.515568, mean_eps: 0.000000
 3037/5000: episode: 106, duration: 0.228s, episode steps:  20, steps per second:  88, episode reward: 42.000, mean reward:  2.100 [-2.837, 33.000], mean action: 1.100 [0.000, 9.000],  loss: 0.020118, mae: 0.466554, mean_q: 0.481515, mean_eps: 0.000000
 3074/5000: episode: 107, duration: 0.405s, episode steps:  37, steps per second:  91, episode reward: 38.203, mean reward:  1.033 [-2.415, 32.450], mean action: 2.378 [0.000, 14.000],  loss: 0.020389, mae: 0.462624, mean_q: 0.494736, mean_eps: 0.000000
 3094/5000: episode: 108, duration: 0.229s, episode steps:  20, steps per second:  87, episode reward: 46.009, mean reward:  2.300 [-0.452, 32.200], mean action: 6.000 [0.000, 20.000],  loss: 0.022076, mae: 0.471628, mean_q: 0.487772, mean_eps: 0.000000
 3122/5000: episode: 109, duration: 0.312s, episode steps:  28, steps per second:  90, episode reward: 38.101, mean reward:  1.361 [-3.000, 32.080], mean action: 3.107 [0.000, 14.000],  loss: 0.017086, mae: 0.444384, mean_q: 0.486801, mean_eps: 0.000000
 3158/5000: episode: 110, duration: 0.403s, episode steps:  36, steps per second:  89, episode reward: 34.254, mean reward:  0.951 [-2.566, 31.343], mean action: 5.472 [1.000, 14.000],  loss: 0.018733, mae: 0.452987, mean_q: 0.468799, mean_eps: 0.000000
 3182/5000: episode: 111, duration: 0.269s, episode steps:  24, steps per second:  89, episode reward: 35.970, mean reward:  1.499 [-3.000, 32.700], mean action: 5.208 [0.000, 12.000],  loss: 0.021193, mae: 0.455500, mean_q: 0.502957, mean_eps: 0.000000
 3211/5000: episode: 112, duration: 0.335s, episode steps:  29, steps per second:  87, episode reward: 35.607, mean reward:  1.228 [-2.582, 32.140], mean action: 4.172 [0.000, 15.000],  loss: 0.016895, mae: 0.434553, mean_q: 0.470349, mean_eps: 0.000000
 3241/5000: episode: 113, duration: 0.401s, episode steps:  30, steps per second:  75, episode reward: 40.776, mean reward:  1.359 [-2.560, 32.354], mean action: 5.667 [0.000, 20.000],  loss: 0.019352, mae: 0.450264, mean_q: 0.460423, mean_eps: 0.000000
 3280/5000: episode: 114, duration: 0.430s, episode steps:  39, steps per second:  91, episode reward: 43.497, mean reward:  1.115 [-2.461, 32.190], mean action: 1.462 [0.000, 9.000],  loss: 0.021576, mae: 0.474667, mean_q: 0.411222, mean_eps: 0.000000
 3303/5000: episode: 115, duration: 0.367s, episode steps:  23, steps per second:  63, episode reward: 38.741, mean reward:  1.684 [-2.728, 32.421], mean action: 5.478 [0.000, 19.000],  loss: 0.018640, mae: 0.456736, mean_q: 0.426401, mean_eps: 0.000000
 3331/5000: episode: 116, duration: 0.351s, episode steps:  28, steps per second:  80, episode reward: 41.400, mean reward:  1.479 [-2.151, 32.270], mean action: 1.857 [0.000, 19.000],  loss: 0.016665, mae: 0.428814, mean_q: 0.454609, mean_eps: 0.000000
 3357/5000: episode: 117, duration: 0.296s, episode steps:  26, steps per second:  88, episode reward: 41.381, mean reward:  1.592 [-2.691, 32.030], mean action: 2.462 [1.000, 15.000],  loss: 0.019100, mae: 0.445314, mean_q: 0.462249, mean_eps: 0.000000
 3367/5000: episode: 118, duration: 0.123s, episode steps:  10, steps per second:  82, episode reward: 45.000, mean reward:  4.500 [-2.125, 32.230], mean action: 4.000 [0.000, 14.000],  loss: 0.017656, mae: 0.437970, mean_q: 0.475652, mean_eps: 0.000000
 3391/5000: episode: 119, duration: 0.274s, episode steps:  24, steps per second:  87, episode reward: 41.895, mean reward:  1.746 [-2.534, 31.925], mean action: 1.833 [0.000, 9.000],  loss: 0.019503, mae: 0.463874, mean_q: 0.396319, mean_eps: 0.000000
 3443/5000: episode: 120, duration: 0.605s, episode steps:  52, steps per second:  86, episode reward: 43.587, mean reward:  0.838 [-2.309, 32.382], mean action: 3.346 [0.000, 18.000],  loss: 0.021037, mae: 0.469383, mean_q: 0.439865, mean_eps: 0.000000
 3459/5000: episode: 121, duration: 0.194s, episode steps:  16, steps per second:  82, episode reward: 44.143, mean reward:  2.759 [-2.011, 31.842], mean action: 2.625 [0.000, 14.000],  loss: 0.018590, mae: 0.454547, mean_q: 0.529785, mean_eps: 0.000000
 3489/5000: episode: 122, duration: 0.329s, episode steps:  30, steps per second:  91, episode reward: 38.799, mean reward:  1.293 [-2.631, 32.430], mean action: 3.033 [0.000, 9.000],  loss: 0.016709, mae: 0.457153, mean_q: 0.458924, mean_eps: 0.000000
 3524/5000: episode: 123, duration: 0.383s, episode steps:  35, steps per second:  91, episode reward: 40.460, mean reward:  1.156 [-2.194, 34.640], mean action: 3.114 [0.000, 15.000],  loss: 0.022904, mae: 0.478349, mean_q: 0.492611, mean_eps: 0.000000
 3547/5000: episode: 124, duration: 0.261s, episode steps:  23, steps per second:  88, episode reward: 46.813, mean reward:  2.035 [-0.502, 31.921], mean action: 4.000 [0.000, 18.000],  loss: 0.024259, mae: 0.483647, mean_q: 0.466336, mean_eps: 0.000000
 3565/5000: episode: 125, duration: 0.212s, episode steps:  18, steps per second:  85, episode reward: 42.000, mean reward:  2.333 [-2.580, 32.010], mean action: 3.333 [0.000, 13.000],  loss: 0.022082, mae: 0.471580, mean_q: 0.452016, mean_eps: 0.000000
 3591/5000: episode: 126, duration: 0.290s, episode steps:  26, steps per second:  90, episode reward: 41.270, mean reward:  1.587 [-2.137, 32.129], mean action: 3.615 [0.000, 19.000],  loss: 0.019319, mae: 0.468767, mean_q: 0.401016, mean_eps: 0.000000
 3613/5000: episode: 127, duration: 0.240s, episode steps:  22, steps per second:  92, episode reward: 32.406, mean reward:  1.473 [-3.000, 31.806], mean action: 6.955 [0.000, 20.000],  loss: 0.018231, mae: 0.468044, mean_q: 0.419854, mean_eps: 0.000000
 3636/5000: episode: 128, duration: 0.257s, episode steps:  23, steps per second:  89, episode reward: 43.329, mean reward:  1.884 [-2.107, 32.140], mean action: 5.348 [0.000, 20.000],  loss: 0.019488, mae: 0.454862, mean_q: 0.437234, mean_eps: 0.000000
 3663/5000: episode: 129, duration: 0.302s, episode steps:  27, steps per second:  89, episode reward: 35.817, mean reward:  1.327 [-3.000, 32.110], mean action: 3.630 [0.000, 15.000],  loss: 0.016821, mae: 0.445849, mean_q: 0.473840, mean_eps: 0.000000
 3687/5000: episode: 130, duration: 0.269s, episode steps:  24, steps per second:  89, episode reward: 41.161, mean reward:  1.715 [-2.425, 33.000], mean action: 5.333 [0.000, 15.000],  loss: 0.021891, mae: 0.478424, mean_q: 0.464912, mean_eps: 0.000000
 3708/5000: episode: 131, duration: 0.235s, episode steps:  21, steps per second:  89, episode reward: 38.561, mean reward:  1.836 [-2.624, 32.089], mean action: 4.524 [0.000, 15.000],  loss: 0.017098, mae: 0.449072, mean_q: 0.493501, mean_eps: 0.000000
 3735/5000: episode: 132, duration: 0.307s, episode steps:  27, steps per second:  88, episode reward: 44.773, mean reward:  1.658 [-3.000, 32.150], mean action: 1.370 [0.000, 15.000],  loss: 0.022190, mae: 0.481549, mean_q: 0.492437, mean_eps: 0.000000
 3779/5000: episode: 133, duration: 0.515s, episode steps:  44, steps per second:  85, episode reward: -33.000, mean reward: -0.750 [-33.021,  3.000], mean action: 9.591 [0.000, 20.000],  loss: 0.019802, mae: 0.462349, mean_q: 0.478154, mean_eps: 0.000000
 3801/5000: episode: 134, duration: 0.245s, episode steps:  22, steps per second:  90, episode reward: 38.477, mean reward:  1.749 [-3.000, 32.477], mean action: 3.364 [0.000, 11.000],  loss: 0.019013, mae: 0.462284, mean_q: 0.496659, mean_eps: 0.000000
 3819/5000: episode: 135, duration: 0.208s, episode steps:  18, steps per second:  86, episode reward: 44.236, mean reward:  2.458 [-2.003, 31.468], mean action: 5.444 [0.000, 14.000],  loss: 0.021356, mae: 0.466196, mean_q: 0.544033, mean_eps: 0.000000
 3848/5000: episode: 136, duration: 0.323s, episode steps:  29, steps per second:  90, episode reward: 38.811, mean reward:  1.338 [-3.000, 32.210], mean action: 3.000 [0.000, 15.000],  loss: 0.016997, mae: 0.435637, mean_q: 0.483995, mean_eps: 0.000000
 3867/5000: episode: 137, duration: 0.239s, episode steps:  19, steps per second:  79, episode reward: 44.330, mean reward:  2.333 [-2.531, 32.470], mean action: 3.789 [0.000, 14.000],  loss: 0.022915, mae: 0.474799, mean_q: 0.439381, mean_eps: 0.000000
 3896/5000: episode: 138, duration: 0.326s, episode steps:  29, steps per second:  89, episode reward: 40.848, mean reward:  1.409 [-2.404, 32.250], mean action: 4.241 [0.000, 19.000],  loss: 0.013625, mae: 0.444042, mean_q: 0.396084, mean_eps: 0.000000
 3924/5000: episode: 139, duration: 0.333s, episode steps:  28, steps per second:  84, episode reward: 41.576, mean reward:  1.485 [-2.712, 32.170], mean action: 4.214 [0.000, 20.000],  loss: 0.020365, mae: 0.472551, mean_q: 0.426618, mean_eps: 0.000000
 3960/5000: episode: 140, duration: 0.401s, episode steps:  36, steps per second:  90, episode reward: 38.903, mean reward:  1.081 [-3.000, 31.983], mean action: 2.861 [0.000, 15.000],  loss: 0.019296, mae: 0.455537, mean_q: 0.471053, mean_eps: 0.000000
 3981/5000: episode: 141, duration: 0.243s, episode steps:  21, steps per second:  87, episode reward: 41.650, mean reward:  1.983 [-2.101, 32.096], mean action: 3.429 [0.000, 20.000],  loss: 0.016245, mae: 0.449372, mean_q: 0.392488, mean_eps: 0.000000
 4034/5000: episode: 142, duration: 0.590s, episode steps:  53, steps per second:  90, episode reward: 41.941, mean reward:  0.791 [-2.433, 32.250], mean action: 1.868 [0.000, 14.000],  loss: 0.019387, mae: 0.477879, mean_q: 0.398261, mean_eps: 0.000000
 4064/5000: episode: 143, duration: 0.327s, episode steps:  30, steps per second:  92, episode reward: -34.990, mean reward: -1.166 [-33.000,  2.220], mean action: 3.833 [0.000, 14.000],  loss: 0.020645, mae: 0.469644, mean_q: 0.454847, mean_eps: 0.000000
 4086/5000: episode: 144, duration: 0.247s, episode steps:  22, steps per second:  89, episode reward: 41.155, mean reward:  1.871 [-2.217, 32.900], mean action: 7.091 [0.000, 20.000],  loss: 0.022062, mae: 0.466972, mean_q: 0.478705, mean_eps: 0.000000
 4109/5000: episode: 145, duration: 0.270s, episode steps:  23, steps per second:  85, episode reward: 41.697, mean reward:  1.813 [-2.051, 32.200], mean action: 5.609 [0.000, 20.000],  loss: 0.017409, mae: 0.450307, mean_q: 0.439018, mean_eps: 0.000000
 4131/5000: episode: 146, duration: 0.248s, episode steps:  22, steps per second:  89, episode reward: 40.702, mean reward:  1.850 [-2.798, 32.244], mean action: 3.818 [0.000, 20.000],  loss: 0.016914, mae: 0.463314, mean_q: 0.446516, mean_eps: 0.000000
 4166/5000: episode: 147, duration: 0.521s, episode steps:  35, steps per second:  67, episode reward: 35.097, mean reward:  1.003 [-2.622, 32.113], mean action: 7.000 [0.000, 15.000],  loss: 0.022671, mae: 0.483850, mean_q: 0.454562, mean_eps: 0.000000
 4194/5000: episode: 148, duration: 0.322s, episode steps:  28, steps per second:  87, episode reward: 38.455, mean reward:  1.373 [-3.000, 32.150], mean action: 3.143 [0.000, 15.000],  loss: 0.015621, mae: 0.449008, mean_q: 0.425423, mean_eps: 0.000000
 4232/5000: episode: 149, duration: 0.433s, episode steps:  38, steps per second:  88, episode reward: 37.713, mean reward:  0.992 [-2.646, 31.605], mean action: 4.263 [0.000, 20.000],  loss: 0.021932, mae: 0.467605, mean_q: 0.445198, mean_eps: 0.000000
 4256/5000: episode: 150, duration: 0.273s, episode steps:  24, steps per second:  88, episode reward: 38.164, mean reward:  1.590 [-2.150, 32.539], mean action: 3.417 [0.000, 15.000],  loss: 0.017938, mae: 0.452388, mean_q: 0.440184, mean_eps: 0.000000
 4296/5000: episode: 151, duration: 0.459s, episode steps:  40, steps per second:  87, episode reward: 38.530, mean reward:  0.963 [-2.176, 31.847], mean action: 4.625 [0.000, 20.000],  loss: 0.018679, mae: 0.461792, mean_q: 0.413546, mean_eps: 0.000000
 4322/5000: episode: 152, duration: 0.290s, episode steps:  26, steps per second:  90, episode reward: 43.810, mean reward:  1.685 [-2.049, 32.300], mean action: 1.423 [0.000, 12.000],  loss: 0.020624, mae: 0.461003, mean_q: 0.486144, mean_eps: 0.000000
 4375/5000: episode: 153, duration: 0.567s, episode steps:  53, steps per second:  94, episode reward: 32.520, mean reward:  0.614 [-2.519, 32.290], mean action: 4.830 [0.000, 15.000],  loss: 0.020091, mae: 0.465608, mean_q: 0.492908, mean_eps: 0.000000
 4398/5000: episode: 154, duration: 0.264s, episode steps:  23, steps per second:  87, episode reward: 38.215, mean reward:  1.662 [-2.125, 31.908], mean action: 2.304 [0.000, 9.000],  loss: 0.016641, mae: 0.449233, mean_q: 0.483114, mean_eps: 0.000000
 4432/5000: episode: 155, duration: 0.377s, episode steps:  34, steps per second:  90, episode reward: 41.126, mean reward:  1.210 [-2.465, 32.170], mean action: 1.676 [0.000, 15.000],  loss: 0.020590, mae: 0.456501, mean_q: 0.517569, mean_eps: 0.000000
 4465/5000: episode: 156, duration: 0.378s, episode steps:  33, steps per second:  87, episode reward: 35.254, mean reward:  1.068 [-2.304, 31.274], mean action: 3.000 [0.000, 14.000],  loss: 0.019488, mae: 0.457750, mean_q: 0.511672, mean_eps: 0.000000
 4491/5000: episode: 157, duration: 0.289s, episode steps:  26, steps per second:  90, episode reward: 43.146, mean reward:  1.659 [-2.000, 32.061], mean action: 2.885 [0.000, 11.000],  loss: 0.020382, mae: 0.469503, mean_q: 0.430244, mean_eps: 0.000000
 4510/5000: episode: 158, duration: 0.224s, episode steps:  19, steps per second:  85, episode reward: 46.755, mean reward:  2.461 [-0.060, 32.270], mean action: 3.421 [0.000, 16.000],  loss: 0.014713, mae: 0.433163, mean_q: 0.454672, mean_eps: 0.000000
 4549/5000: episode: 159, duration: 0.453s, episode steps:  39, steps per second:  86, episode reward: 41.171, mean reward:  1.056 [-2.215, 32.064], mean action: 2.923 [0.000, 20.000],  loss: 0.018044, mae: 0.441374, mean_q: 0.494675, mean_eps: 0.000000
 4576/5000: episode: 160, duration: 0.309s, episode steps:  27, steps per second:  87, episode reward: 35.938, mean reward:  1.331 [-3.000, 32.090], mean action: 4.444 [0.000, 15.000],  loss: 0.018909, mae: 0.448527, mean_q: 0.487050, mean_eps: 0.000000
 4592/5000: episode: 161, duration: 0.187s, episode steps:  16, steps per second:  85, episode reward: 44.234, mean reward:  2.765 [-2.322, 31.714], mean action: 3.750 [3.000, 15.000],  loss: 0.019692, mae: 0.458512, mean_q: 0.458012, mean_eps: 0.000000
 4635/5000: episode: 162, duration: 0.476s, episode steps:  43, steps per second:  90, episode reward: 34.551, mean reward:  0.804 [-3.000, 31.953], mean action: 6.093 [0.000, 15.000],  loss: 0.018749, mae: 0.457113, mean_q: 0.421510, mean_eps: 0.000000
 4666/5000: episode: 163, duration: 0.348s, episode steps:  31, steps per second:  89, episode reward: 43.966, mean reward:  1.418 [-2.244, 32.110], mean action: 2.226 [0.000, 13.000],  loss: 0.019047, mae: 0.460601, mean_q: 0.385448, mean_eps: 0.000000
 4690/5000: episode: 164, duration: 0.269s, episode steps:  24, steps per second:  89, episode reward: 41.176, mean reward:  1.716 [-2.816, 32.080], mean action: 2.958 [0.000, 15.000],  loss: 0.022302, mae: 0.470693, mean_q: 0.409575, mean_eps: 0.000000
 4729/5000: episode: 165, duration: 0.428s, episode steps:  39, steps per second:  91, episode reward: -32.160, mean reward: -0.825 [-32.803,  3.000], mean action: 6.744 [0.000, 15.000],  loss: 0.018422, mae: 0.446463, mean_q: 0.476052, mean_eps: 0.000000
 4753/5000: episode: 166, duration: 0.276s, episode steps:  24, steps per second:  87, episode reward: 39.967, mean reward:  1.665 [-2.668, 32.143], mean action: 3.333 [0.000, 9.000],  loss: 0.022304, mae: 0.484528, mean_q: 0.422419, mean_eps: 0.000000
 4777/5000: episode: 167, duration: 0.276s, episode steps:  24, steps per second:  87, episode reward: 38.014, mean reward:  1.584 [-2.462, 32.108], mean action: 3.583 [0.000, 14.000],  loss: 0.018500, mae: 0.467987, mean_q: 0.426092, mean_eps: 0.000000
 4804/5000: episode: 168, duration: 0.328s, episode steps:  27, steps per second:  82, episode reward: 32.531, mean reward:  1.205 [-3.000, 29.040], mean action: 7.593 [0.000, 20.000],  loss: 0.020576, mae: 0.478137, mean_q: 0.426772, mean_eps: 0.000000
 4829/5000: episode: 169, duration: 0.654s, episode steps:  25, steps per second:  38, episode reward: 41.169, mean reward:  1.647 [-2.109, 32.129], mean action: 3.320 [0.000, 15.000],  loss: 0.019604, mae: 0.463783, mean_q: 0.475649, mean_eps: 0.000000
 4868/5000: episode: 170, duration: 0.471s, episode steps:  39, steps per second:  83, episode reward: 33.000, mean reward:  0.846 [-3.000, 32.130], mean action: 6.538 [0.000, 21.000],  loss: 0.021977, mae: 0.471738, mean_q: 0.485907, mean_eps: 0.000000
 4894/5000: episode: 171, duration: 0.300s, episode steps:  26, steps per second:  87, episode reward: 38.824, mean reward:  1.493 [-2.568, 31.844], mean action: 3.692 [0.000, 15.000],  loss: 0.015696, mae: 0.448727, mean_q: 0.499995, mean_eps: 0.000000
 4917/5000: episode: 172, duration: 0.328s, episode steps:  23, steps per second:  70, episode reward: 44.468, mean reward:  1.933 [-2.845, 32.160], mean action: 1.087 [0.000, 11.000],  loss: 0.022562, mae: 0.481378, mean_q: 0.426943, mean_eps: 0.000000
 4951/5000: episode: 173, duration: 0.656s, episode steps:  34, steps per second:  52, episode reward: 38.844, mean reward:  1.142 [-2.429, 32.120], mean action: 5.000 [0.000, 21.000],  loss: 0.020378, mae: 0.469337, mean_q: 0.420718, mean_eps: 0.000000
 4983/5000: episode: 174, duration: 0.478s, episode steps:  32, steps per second:  67, episode reward: 40.926, mean reward:  1.279 [-2.377, 32.530], mean action: 4.969 [1.000, 18.000],  loss: 0.022256, mae: 0.478256, mean_q: 0.408389, mean_eps: 0.000000
done, took 53.731 seconds
DQN Evaluation: 1069 victories out of 1264 episodes
Training for 5000 steps ...
   20/5000: episode: 1, duration: 0.385s, episode steps:  20, steps per second:  52, episode reward: 35.112, mean reward:  1.756 [-3.000, 33.000], mean action: 6.500 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   44/5000: episode: 2, duration: 0.151s, episode steps:  24, steps per second: 159, episode reward: -35.560, mean reward: -1.482 [-32.443,  2.476], mean action: 6.792 [0.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   70/5000: episode: 3, duration: 0.171s, episode steps:  26, steps per second: 152, episode reward: 32.416, mean reward:  1.247 [-3.000, 32.120], mean action: 7.615 [2.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   99/5000: episode: 4, duration: 0.184s, episode steps:  29, steps per second: 158, episode reward: 38.305, mean reward:  1.321 [-2.705, 32.130], mean action: 3.862 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  130/5000: episode: 5, duration: 0.189s, episode steps:  31, steps per second: 164, episode reward: -32.730, mean reward: -1.056 [-32.107,  2.308], mean action: 6.581 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  171/5000: episode: 6, duration: 0.242s, episode steps:  41, steps per second: 170, episode reward: 32.582, mean reward:  0.795 [-3.000, 33.000], mean action: 9.463 [1.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  190/5000: episode: 7, duration: 0.132s, episode steps:  19, steps per second: 144, episode reward: 43.758, mean reward:  2.303 [-2.004, 31.831], mean action: 3.000 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  214/5000: episode: 8, duration: 0.146s, episode steps:  24, steps per second: 164, episode reward: -38.610, mean reward: -1.609 [-31.860,  2.571], mean action: 8.292 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  242/5000: episode: 9, duration: 0.178s, episode steps:  28, steps per second: 158, episode reward: -30.000, mean reward: -1.071 [-29.694,  3.010], mean action: 6.071 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  260/5000: episode: 10, duration: 0.118s, episode steps:  18, steps per second: 152, episode reward: 41.457, mean reward:  2.303 [-2.208, 32.490], mean action: 5.111 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  278/5000: episode: 11, duration: 0.122s, episode steps:  18, steps per second: 148, episode reward: 41.062, mean reward:  2.281 [-2.835, 32.902], mean action: 4.944 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  302/5000: episode: 12, duration: 0.152s, episode steps:  24, steps per second: 158, episode reward: 35.851, mean reward:  1.494 [-2.350, 32.210], mean action: 5.958 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  324/5000: episode: 13, duration: 0.143s, episode steps:  22, steps per second: 153, episode reward: 40.440, mean reward:  1.838 [-2.218, 32.109], mean action: 4.500 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  347/5000: episode: 14, duration: 0.145s, episode steps:  23, steps per second: 158, episode reward: 34.890, mean reward:  1.517 [-2.523, 32.370], mean action: 5.913 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  372/5000: episode: 15, duration: 0.154s, episode steps:  25, steps per second: 163, episode reward: 35.232, mean reward:  1.409 [-2.439, 32.200], mean action: 7.400 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  395/5000: episode: 16, duration: 0.143s, episode steps:  23, steps per second: 160, episode reward: -35.590, mean reward: -1.547 [-32.292,  2.360], mean action: 3.522 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  421/5000: episode: 17, duration: 0.163s, episode steps:  26, steps per second: 160, episode reward: 38.290, mean reward:  1.473 [-2.705, 32.160], mean action: 3.885 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  452/5000: episode: 18, duration: 0.186s, episode steps:  31, steps per second: 167, episode reward: 33.218, mean reward:  1.072 [-2.901, 32.120], mean action: 4.484 [0.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  476/5000: episode: 19, duration: 0.148s, episode steps:  24, steps per second: 163, episode reward: 35.742, mean reward:  1.489 [-3.000, 32.180], mean action: 4.333 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  498/5000: episode: 20, duration: 0.137s, episode steps:  22, steps per second: 161, episode reward: -32.710, mean reward: -1.487 [-31.889,  2.603], mean action: 6.136 [0.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  542/5000: episode: 21, duration: 0.263s, episode steps:  44, steps per second: 167, episode reward: 32.771, mean reward:  0.745 [-3.000, 32.875], mean action: 8.818 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  563/5000: episode: 22, duration: 0.133s, episode steps:  21, steps per second: 158, episode reward: 32.684, mean reward:  1.556 [-3.000, 32.049], mean action: 7.286 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  589/5000: episode: 23, duration: 0.161s, episode steps:  26, steps per second: 161, episode reward: -32.320, mean reward: -1.243 [-32.028,  2.390], mean action: 8.962 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  616/5000: episode: 24, duration: 0.165s, episode steps:  27, steps per second: 164, episode reward: -35.160, mean reward: -1.302 [-32.720,  2.391], mean action: 4.593 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  648/5000: episode: 25, duration: 0.199s, episode steps:  32, steps per second: 161, episode reward: 32.246, mean reward:  1.008 [-3.000, 32.490], mean action: 6.250 [0.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  670/5000: episode: 26, duration: 0.139s, episode steps:  22, steps per second: 159, episode reward: -36.000, mean reward: -1.636 [-32.320,  2.403], mean action: 10.045 [0.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  697/5000: episode: 27, duration: 0.162s, episode steps:  27, steps per second: 167, episode reward: -35.390, mean reward: -1.311 [-31.941,  3.000], mean action: 6.852 [0.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  741/5000: episode: 28, duration: 0.267s, episode steps:  44, steps per second: 165, episode reward: 34.976, mean reward:  0.795 [-2.941, 32.160], mean action: 4.841 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  773/5000: episode: 29, duration: 0.204s, episode steps:  32, steps per second: 157, episode reward: 32.618, mean reward:  1.019 [-3.000, 32.090], mean action: 8.000 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  804/5000: episode: 30, duration: 0.186s, episode steps:  31, steps per second: 166, episode reward: 32.610, mean reward:  1.052 [-3.000, 32.150], mean action: 4.065 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  822/5000: episode: 31, duration: 0.122s, episode steps:  18, steps per second: 148, episode reward: 41.131, mean reward:  2.285 [-2.336, 32.350], mean action: 3.111 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  852/5000: episode: 32, duration: 0.187s, episode steps:  30, steps per second: 161, episode reward: -32.730, mean reward: -1.091 [-31.860,  2.460], mean action: 8.833 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  883/5000: episode: 33, duration: 0.197s, episode steps:  31, steps per second: 157, episode reward: -32.290, mean reward: -1.042 [-32.532,  3.000], mean action: 9.161 [0.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  902/5000: episode: 34, duration: 0.118s, episode steps:  19, steps per second: 161, episode reward: 35.626, mean reward:  1.875 [-2.482, 32.036], mean action: 5.368 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  933/5000: episode: 35, duration: 0.189s, episode steps:  31, steps per second: 164, episode reward: 32.331, mean reward:  1.043 [-2.450, 31.883], mean action: 6.323 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  953/5000: episode: 36, duration: 0.123s, episode steps:  20, steps per second: 162, episode reward: 38.813, mean reward:  1.941 [-2.563, 32.320], mean action: 2.450 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  989/5000: episode: 37, duration: 0.236s, episode steps:  36, steps per second: 152, episode reward: 32.828, mean reward:  0.912 [-3.000, 31.898], mean action: 7.056 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1012/5000: episode: 38, duration: 0.216s, episode steps:  23, steps per second: 107, episode reward: 44.155, mean reward:  1.920 [-2.255, 32.321], mean action: 4.087 [0.000, 20.000],  loss: 0.020433, mae: 0.471792, mean_q: 0.410369, mean_eps: 0.000000
 1036/5000: episode: 39, duration: 0.306s, episode steps:  24, steps per second:  78, episode reward: 32.640, mean reward:  1.360 [-3.000, 31.865], mean action: 5.375 [0.000, 12.000],  loss: 0.017545, mae: 0.461862, mean_q: 0.378237, mean_eps: 0.000000
 1060/5000: episode: 40, duration: 0.267s, episode steps:  24, steps per second:  90, episode reward: -32.070, mean reward: -1.336 [-31.858,  2.617], mean action: 4.667 [0.000, 17.000],  loss: 0.016733, mae: 0.441573, mean_q: 0.459239, mean_eps: 0.000000
 1092/5000: episode: 41, duration: 0.352s, episode steps:  32, steps per second:  91, episode reward: -32.580, mean reward: -1.018 [-32.041,  2.712], mean action: 5.438 [0.000, 17.000],  loss: 0.019697, mae: 0.443388, mean_q: 0.469410, mean_eps: 0.000000
 1122/5000: episode: 42, duration: 0.363s, episode steps:  30, steps per second:  83, episode reward: 33.000, mean reward:  1.100 [-2.555, 32.040], mean action: 8.933 [0.000, 21.000],  loss: 0.019547, mae: 0.459567, mean_q: 0.433068, mean_eps: 0.000000
 1144/5000: episode: 43, duration: 0.247s, episode steps:  22, steps per second:  89, episode reward: 33.000, mean reward:  1.500 [-3.000, 33.292], mean action: 6.136 [0.000, 15.000],  loss: 0.021135, mae: 0.457396, mean_q: 0.485542, mean_eps: 0.000000
 1164/5000: episode: 44, duration: 0.226s, episode steps:  20, steps per second:  89, episode reward: -36.000, mean reward: -1.800 [-32.440,  2.804], mean action: 4.950 [0.000, 14.000],  loss: 0.021296, mae: 0.453386, mean_q: 0.421656, mean_eps: 0.000000
 1187/5000: episode: 45, duration: 0.255s, episode steps:  23, steps per second:  90, episode reward: 35.753, mean reward:  1.554 [-3.000, 32.560], mean action: 6.565 [0.000, 14.000],  loss: 0.019865, mae: 0.449902, mean_q: 0.450739, mean_eps: 0.000000
 1222/5000: episode: 46, duration: 0.445s, episode steps:  35, steps per second:  79, episode reward: -33.000, mean reward: -0.943 [-33.000,  2.512], mean action: 6.514 [0.000, 21.000],  loss: 0.017427, mae: 0.443408, mean_q: 0.450409, mean_eps: 0.000000
 1242/5000: episode: 47, duration: 0.266s, episode steps:  20, steps per second:  75, episode reward: 39.000, mean reward:  1.950 [-2.246, 32.330], mean action: 2.700 [0.000, 11.000],  loss: 0.020040, mae: 0.466072, mean_q: 0.465473, mean_eps: 0.000000
 1277/5000: episode: 48, duration: 0.394s, episode steps:  35, steps per second:  89, episode reward: 32.777, mean reward:  0.936 [-2.735, 32.130], mean action: 5.057 [0.000, 14.000],  loss: 0.016015, mae: 0.435585, mean_q: 0.488956, mean_eps: 0.000000
 1304/5000: episode: 49, duration: 0.302s, episode steps:  27, steps per second:  89, episode reward: -32.910, mean reward: -1.219 [-32.177,  2.500], mean action: 8.815 [0.000, 17.000],  loss: 0.018906, mae: 0.455054, mean_q: 0.442235, mean_eps: 0.000000
 1340/5000: episode: 50, duration: 0.395s, episode steps:  36, steps per second:  91, episode reward: -35.850, mean reward: -0.996 [-32.497,  2.710], mean action: 7.778 [1.000, 14.000],  loss: 0.018346, mae: 0.460132, mean_q: 0.451707, mean_eps: 0.000000
 1367/5000: episode: 51, duration: 0.303s, episode steps:  27, steps per second:  89, episode reward: 33.000, mean reward:  1.222 [-2.537, 32.080], mean action: 5.296 [0.000, 17.000],  loss: 0.018583, mae: 0.463103, mean_q: 0.464426, mean_eps: 0.000000
 1390/5000: episode: 52, duration: 0.255s, episode steps:  23, steps per second:  90, episode reward: -44.300, mean reward: -1.926 [-32.326,  1.957], mean action: 5.435 [0.000, 20.000],  loss: 0.019160, mae: 0.454118, mean_q: 0.469236, mean_eps: 0.000000
 1416/5000: episode: 53, duration: 0.292s, episode steps:  26, steps per second:  89, episode reward: -32.120, mean reward: -1.235 [-31.962,  2.270], mean action: 2.654 [0.000, 12.000],  loss: 0.019971, mae: 0.461327, mean_q: 0.448866, mean_eps: 0.000000
 1437/5000: episode: 54, duration: 0.242s, episode steps:  21, steps per second:  87, episode reward: -33.000, mean reward: -1.571 [-30.470,  2.240], mean action: 4.952 [0.000, 20.000],  loss: 0.016406, mae: 0.458796, mean_q: 0.412811, mean_eps: 0.000000
 1460/5000: episode: 55, duration: 0.258s, episode steps:  23, steps per second:  89, episode reward: 32.567, mean reward:  1.416 [-3.000, 31.747], mean action: 4.130 [0.000, 15.000],  loss: 0.020576, mae: 0.481080, mean_q: 0.469202, mean_eps: 0.000000
 1478/5000: episode: 56, duration: 0.212s, episode steps:  18, steps per second:  85, episode reward: 38.298, mean reward:  2.128 [-2.460, 32.073], mean action: 3.667 [0.000, 14.000],  loss: 0.017896, mae: 0.457193, mean_q: 0.483660, mean_eps: 0.000000
 1497/5000: episode: 57, duration: 0.226s, episode steps:  19, steps per second:  84, episode reward: 43.876, mean reward:  2.309 [-2.234, 32.780], mean action: 3.421 [0.000, 19.000],  loss: 0.018888, mae: 0.475363, mean_q: 0.470872, mean_eps: 0.000000
 1519/5000: episode: 58, duration: 0.246s, episode steps:  22, steps per second:  89, episode reward: 32.782, mean reward:  1.490 [-3.000, 32.782], mean action: 5.545 [0.000, 15.000],  loss: 0.020860, mae: 0.475128, mean_q: 0.399614, mean_eps: 0.000000
 1545/5000: episode: 59, duration: 0.290s, episode steps:  26, steps per second:  90, episode reward: 38.715, mean reward:  1.489 [-2.447, 31.972], mean action: 3.385 [0.000, 15.000],  loss: 0.020901, mae: 0.477475, mean_q: 0.419647, mean_eps: 0.000000
 1568/5000: episode: 60, duration: 0.294s, episode steps:  23, steps per second:  78, episode reward: 38.627, mean reward:  1.679 [-2.138, 32.080], mean action: 2.696 [0.000, 9.000],  loss: 0.020117, mae: 0.455255, mean_q: 0.462496, mean_eps: 0.000000
 1601/5000: episode: 61, duration: 0.391s, episode steps:  33, steps per second:  84, episode reward: -32.800, mean reward: -0.994 [-32.142,  3.000], mean action: 7.970 [0.000, 19.000],  loss: 0.025917, mae: 0.488120, mean_q: 0.475273, mean_eps: 0.000000
 1629/5000: episode: 62, duration: 0.311s, episode steps:  28, steps per second:  90, episode reward: -32.630, mean reward: -1.165 [-31.856,  2.690], mean action: 7.500 [0.000, 19.000],  loss: 0.021090, mae: 0.461167, mean_q: 0.476326, mean_eps: 0.000000
 1646/5000: episode: 63, duration: 0.191s, episode steps:  17, steps per second:  89, episode reward: 35.901, mean reward:  2.112 [-2.538, 32.811], mean action: 4.176 [0.000, 12.000],  loss: 0.022588, mae: 0.481790, mean_q: 0.423231, mean_eps: 0.000000
 1665/5000: episode: 64, duration: 0.215s, episode steps:  19, steps per second:  89, episode reward: 32.692, mean reward:  1.721 [-3.000, 33.000], mean action: 3.789 [0.000, 9.000],  loss: 0.015376, mae: 0.450597, mean_q: 0.401583, mean_eps: 0.000000
 1692/5000: episode: 65, duration: 0.311s, episode steps:  27, steps per second:  87, episode reward: -30.000, mean reward: -1.111 [-29.411,  2.637], mean action: 4.630 [0.000, 19.000],  loss: 0.021911, mae: 0.485933, mean_q: 0.413942, mean_eps: 0.000000
 1710/5000: episode: 66, duration: 0.211s, episode steps:  18, steps per second:  85, episode reward: 33.000, mean reward:  1.833 [-3.000, 29.620], mean action: 6.056 [0.000, 20.000],  loss: 0.015882, mae: 0.458605, mean_q: 0.382580, mean_eps: 0.000000
 1728/5000: episode: 67, duration: 0.214s, episode steps:  18, steps per second:  84, episode reward: 40.947, mean reward:  2.275 [-2.539, 32.290], mean action: 6.111 [0.000, 20.000],  loss: 0.023346, mae: 0.490479, mean_q: 0.353880, mean_eps: 0.000000
 1755/5000: episode: 68, duration: 0.305s, episode steps:  27, steps per second:  89, episode reward: 32.458, mean reward:  1.202 [-3.000, 32.094], mean action: 5.296 [0.000, 20.000],  loss: 0.017678, mae: 0.462090, mean_q: 0.418171, mean_eps: 0.000000
 1774/5000: episode: 69, duration: 0.222s, episode steps:  19, steps per second:  86, episode reward: 35.706, mean reward:  1.879 [-2.374, 32.830], mean action: 4.368 [0.000, 14.000],  loss: 0.015844, mae: 0.456259, mean_q: 0.400487, mean_eps: 0.000000
 1808/5000: episode: 70, duration: 0.379s, episode steps:  34, steps per second:  90, episode reward: 33.000, mean reward:  0.971 [-2.351, 32.100], mean action: 8.559 [0.000, 21.000],  loss: 0.023508, mae: 0.487579, mean_q: 0.369897, mean_eps: 0.000000
 1826/5000: episode: 71, duration: 0.205s, episode steps:  18, steps per second:  88, episode reward: 37.607, mean reward:  2.089 [-3.000, 32.280], mean action: 4.778 [0.000, 20.000],  loss: 0.019773, mae: 0.450743, mean_q: 0.465809, mean_eps: 0.000000
 1849/5000: episode: 72, duration: 0.261s, episode steps:  23, steps per second:  88, episode reward: 35.087, mean reward:  1.526 [-2.416, 33.000], mean action: 4.913 [0.000, 15.000],  loss: 0.018582, mae: 0.459259, mean_q: 0.435299, mean_eps: 0.000000
 1871/5000: episode: 73, duration: 0.254s, episode steps:  22, steps per second:  87, episode reward: 38.364, mean reward:  1.744 [-2.174, 32.070], mean action: 4.000 [0.000, 15.000],  loss: 0.020113, mae: 0.474473, mean_q: 0.392849, mean_eps: 0.000000
 1887/5000: episode: 74, duration: 0.189s, episode steps:  16, steps per second:  85, episode reward: 35.072, mean reward:  2.192 [-3.000, 32.670], mean action: 5.250 [0.000, 15.000],  loss: 0.022391, mae: 0.474684, mean_q: 0.469858, mean_eps: 0.000000
 1902/5000: episode: 75, duration: 0.173s, episode steps:  15, steps per second:  87, episode reward: 38.007, mean reward:  2.534 [-2.696, 32.319], mean action: 3.800 [0.000, 12.000],  loss: 0.015720, mae: 0.428369, mean_q: 0.507509, mean_eps: 0.000000
 1921/5000: episode: 76, duration: 0.281s, episode steps:  19, steps per second:  68, episode reward: 38.626, mean reward:  2.033 [-2.322, 32.470], mean action: 2.105 [0.000, 9.000],  loss: 0.019023, mae: 0.447540, mean_q: 0.528927, mean_eps: 0.000000
 1940/5000: episode: 77, duration: 0.261s, episode steps:  19, steps per second:  73, episode reward: -35.340, mean reward: -1.860 [-32.904,  2.555], mean action: 7.105 [0.000, 20.000],  loss: 0.019488, mae: 0.461985, mean_q: 0.447816, mean_eps: 0.000000
 1960/5000: episode: 78, duration: 0.241s, episode steps:  20, steps per second:  83, episode reward: 38.596, mean reward:  1.930 [-2.136, 32.218], mean action: 3.850 [0.000, 14.000],  loss: 0.018600, mae: 0.466982, mean_q: 0.442577, mean_eps: 0.000000
 1984/5000: episode: 79, duration: 0.280s, episode steps:  24, steps per second:  86, episode reward: 37.193, mean reward:  1.550 [-2.296, 32.190], mean action: 5.208 [0.000, 14.000],  loss: 0.020082, mae: 0.467392, mean_q: 0.427015, mean_eps: 0.000000
 2007/5000: episode: 80, duration: 0.261s, episode steps:  23, steps per second:  88, episode reward: -33.000, mean reward: -1.435 [-32.978,  2.679], mean action: 6.652 [0.000, 14.000],  loss: 0.015022, mae: 0.438448, mean_q: 0.444231, mean_eps: 0.000000
 2022/5000: episode: 81, duration: 0.176s, episode steps:  15, steps per second:  85, episode reward: 38.253, mean reward:  2.550 [-2.580, 32.410], mean action: 2.467 [0.000, 12.000],  loss: 0.015915, mae: 0.440959, mean_q: 0.465235, mean_eps: 0.000000
 2047/5000: episode: 82, duration: 0.286s, episode steps:  25, steps per second:  87, episode reward: 35.357, mean reward:  1.414 [-2.574, 31.993], mean action: 2.960 [0.000, 14.000],  loss: 0.015739, mae: 0.438822, mean_q: 0.456027, mean_eps: 0.000000
 2071/5000: episode: 83, duration: 0.265s, episode steps:  24, steps per second:  90, episode reward: -32.020, mean reward: -1.334 [-32.070,  2.363], mean action: 3.667 [0.000, 12.000],  loss: 0.017565, mae: 0.449187, mean_q: 0.393596, mean_eps: 0.000000
 2091/5000: episode: 84, duration: 0.226s, episode steps:  20, steps per second:  88, episode reward: 32.689, mean reward:  1.634 [-3.000, 32.072], mean action: 7.950 [0.000, 14.000],  loss: 0.019227, mae: 0.457471, mean_q: 0.428563, mean_eps: 0.000000
 2123/5000: episode: 85, duration: 0.354s, episode steps:  32, steps per second:  90, episode reward: 32.630, mean reward:  1.020 [-3.000, 32.190], mean action: 4.281 [0.000, 9.000],  loss: 0.017653, mae: 0.445545, mean_q: 0.429379, mean_eps: 0.000000
 2151/5000: episode: 86, duration: 0.311s, episode steps:  28, steps per second:  90, episode reward: 35.455, mean reward:  1.266 [-2.663, 32.410], mean action: 5.679 [0.000, 15.000],  loss: 0.022799, mae: 0.466011, mean_q: 0.483531, mean_eps: 0.000000
 2180/5000: episode: 87, duration: 0.317s, episode steps:  29, steps per second:  92, episode reward: -35.140, mean reward: -1.212 [-32.760,  3.000], mean action: 5.379 [0.000, 14.000],  loss: 0.017575, mae: 0.469657, mean_q: 0.396849, mean_eps: 0.000000
 2206/5000: episode: 88, duration: 0.292s, episode steps:  26, steps per second:  89, episode reward: 32.904, mean reward:  1.266 [-2.623, 32.214], mean action: 2.077 [0.000, 9.000],  loss: 0.019641, mae: 0.469578, mean_q: 0.418853, mean_eps: 0.000000
 2230/5000: episode: 89, duration: 0.277s, episode steps:  24, steps per second:  87, episode reward: 37.538, mean reward:  1.564 [-3.000, 31.343], mean action: 6.708 [3.000, 14.000],  loss: 0.020836, mae: 0.460012, mean_q: 0.446155, mean_eps: 0.000000
 2256/5000: episode: 90, duration: 0.291s, episode steps:  26, steps per second:  89, episode reward: 38.592, mean reward:  1.484 [-2.203, 32.724], mean action: 3.115 [0.000, 15.000],  loss: 0.020706, mae: 0.471184, mean_q: 0.420329, mean_eps: 0.000000
 2267/5000: episode: 91, duration: 0.134s, episode steps:  11, steps per second:  82, episode reward: 44.054, mean reward:  4.005 [-2.031, 32.812], mean action: 2.727 [1.000, 12.000],  loss: 0.024143, mae: 0.495944, mean_q: 0.352247, mean_eps: 0.000000
 2301/5000: episode: 92, duration: 0.382s, episode steps:  34, steps per second:  89, episode reward: -32.800, mean reward: -0.965 [-32.069,  2.470], mean action: 5.500 [1.000, 15.000],  loss: 0.019146, mae: 0.479634, mean_q: 0.396083, mean_eps: 0.000000
 2321/5000: episode: 93, duration: 0.231s, episode steps:  20, steps per second:  87, episode reward: 41.369, mean reward:  2.068 [-2.207, 32.268], mean action: 3.500 [0.000, 19.000],  loss: 0.021411, mae: 0.475650, mean_q: 0.430305, mean_eps: 0.000000
 2340/5000: episode: 94, duration: 0.219s, episode steps:  19, steps per second:  87, episode reward: 38.169, mean reward:  2.009 [-3.000, 32.877], mean action: 5.316 [0.000, 15.000],  loss: 0.020866, mae: 0.474596, mean_q: 0.434777, mean_eps: 0.000000
 2360/5000: episode: 95, duration: 0.226s, episode steps:  20, steps per second:  89, episode reward: -38.870, mean reward: -1.944 [-32.091,  2.340], mean action: 7.800 [0.000, 21.000],  loss: 0.014958, mae: 0.433529, mean_q: 0.477304, mean_eps: 0.000000
 2379/5000: episode: 96, duration: 0.219s, episode steps:  19, steps per second:  87, episode reward: 38.880, mean reward:  2.046 [-3.000, 33.000], mean action: 4.105 [0.000, 15.000],  loss: 0.018399, mae: 0.451363, mean_q: 0.480671, mean_eps: 0.000000
 2405/5000: episode: 97, duration: 0.290s, episode steps:  26, steps per second:  90, episode reward: 35.629, mean reward:  1.370 [-2.837, 32.420], mean action: 3.654 [0.000, 15.000],  loss: 0.021492, mae: 0.471854, mean_q: 0.476622, mean_eps: 0.000000
 2429/5000: episode: 98, duration: 0.267s, episode steps:  24, steps per second:  90, episode reward: 37.999, mean reward:  1.583 [-2.500, 32.120], mean action: 4.292 [0.000, 15.000],  loss: 0.021156, mae: 0.457652, mean_q: 0.499366, mean_eps: 0.000000
 2459/5000: episode: 99, duration: 0.339s, episode steps:  30, steps per second:  89, episode reward: 39.000, mean reward:  1.300 [-2.438, 32.010], mean action: 3.700 [0.000, 15.000],  loss: 0.021762, mae: 0.458105, mean_q: 0.467626, mean_eps: 0.000000
 2479/5000: episode: 100, duration: 0.223s, episode steps:  20, steps per second:  90, episode reward: 39.000, mean reward:  1.950 [-3.000, 32.170], mean action: 2.800 [0.000, 9.000],  loss: 0.019285, mae: 0.464188, mean_q: 0.432787, mean_eps: 0.000000
 2502/5000: episode: 101, duration: 0.262s, episode steps:  23, steps per second:  88, episode reward: 37.630, mean reward:  1.636 [-2.453, 32.106], mean action: 3.217 [0.000, 11.000],  loss: 0.023167, mae: 0.489352, mean_q: 0.424502, mean_eps: 0.000000
 2534/5000: episode: 102, duration: 0.355s, episode steps:  32, steps per second:  90, episode reward: 39.061, mean reward:  1.221 [-2.302, 32.680], mean action: 7.062 [0.000, 20.000],  loss: 0.021737, mae: 0.472557, mean_q: 0.449453, mean_eps: 0.000000
 2553/5000: episode: 103, duration: 0.221s, episode steps:  19, steps per second:  86, episode reward: 38.915, mean reward:  2.048 [-2.593, 32.440], mean action: 4.211 [0.000, 15.000],  loss: 0.019135, mae: 0.462199, mean_q: 0.429013, mean_eps: 0.000000
 2564/5000: episode: 104, duration: 0.146s, episode steps:  11, steps per second:  76, episode reward: 39.000, mean reward:  3.545 [-3.000, 30.775], mean action: 3.818 [0.000, 15.000],  loss: 0.023755, mae: 0.472564, mean_q: 0.483682, mean_eps: 0.000000
 2592/5000: episode: 105, duration: 0.308s, episode steps:  28, steps per second:  91, episode reward: 36.000, mean reward:  1.286 [-3.000, 32.100], mean action: 2.786 [1.000, 9.000],  loss: 0.023319, mae: 0.480698, mean_q: 0.440658, mean_eps: 0.000000
 2616/5000: episode: 106, duration: 0.274s, episode steps:  24, steps per second:  88, episode reward: 35.276, mean reward:  1.470 [-3.000, 32.220], mean action: 5.500 [0.000, 14.000],  loss: 0.015959, mae: 0.450825, mean_q: 0.409897, mean_eps: 0.000000
 2632/5000: episode: 107, duration: 0.188s, episode steps:  16, steps per second:  85, episode reward: 44.292, mean reward:  2.768 [-2.135, 32.580], mean action: 4.312 [1.000, 14.000],  loss: 0.018722, mae: 0.443140, mean_q: 0.454088, mean_eps: 0.000000
 2656/5000: episode: 108, duration: 0.275s, episode steps:  24, steps per second:  87, episode reward: -35.100, mean reward: -1.463 [-32.335,  2.810], mean action: 5.167 [0.000, 20.000],  loss: 0.023654, mae: 0.470953, mean_q: 0.458993, mean_eps: 0.000000
 2675/5000: episode: 109, duration: 0.223s, episode steps:  19, steps per second:  85, episode reward: 41.407, mean reward:  2.179 [-2.393, 31.817], mean action: 4.368 [0.000, 14.000],  loss: 0.019526, mae: 0.450333, mean_q: 0.484193, mean_eps: 0.000000
 2700/5000: episode: 110, duration: 0.280s, episode steps:  25, steps per second:  89, episode reward: 38.079, mean reward:  1.523 [-3.000, 32.270], mean action: 3.040 [0.000, 9.000],  loss: 0.018851, mae: 0.447800, mean_q: 0.480543, mean_eps: 0.000000
 2724/5000: episode: 111, duration: 0.283s, episode steps:  24, steps per second:  85, episode reward: 37.849, mean reward:  1.577 [-2.486, 32.272], mean action: 5.458 [0.000, 20.000],  loss: 0.017217, mae: 0.432600, mean_q: 0.476874, mean_eps: 0.000000
 2751/5000: episode: 112, duration: 0.300s, episode steps:  27, steps per second:  90, episode reward: 34.540, mean reward:  1.279 [-3.000, 32.507], mean action: 4.259 [0.000, 20.000],  loss: 0.020680, mae: 0.473776, mean_q: 0.466226, mean_eps: 0.000000
 2773/5000: episode: 113, duration: 0.253s, episode steps:  22, steps per second:  87, episode reward: 41.536, mean reward:  1.888 [-2.157, 32.290], mean action: 2.045 [0.000, 15.000],  loss: 0.021320, mae: 0.466158, mean_q: 0.493006, mean_eps: 0.000000
 2801/5000: episode: 114, duration: 0.313s, episode steps:  28, steps per second:  89, episode reward: -32.700, mean reward: -1.168 [-31.923,  2.698], mean action: 6.393 [0.000, 17.000],  loss: 0.017550, mae: 0.459781, mean_q: 0.499103, mean_eps: 0.000000
 2815/5000: episode: 115, duration: 0.166s, episode steps:  14, steps per second:  84, episode reward: 41.777, mean reward:  2.984 [-2.268, 32.087], mean action: 3.571 [0.000, 15.000],  loss: 0.020934, mae: 0.481282, mean_q: 0.434315, mean_eps: 0.000000
 2844/5000: episode: 116, duration: 0.318s, episode steps:  29, steps per second:  91, episode reward: -32.580, mean reward: -1.123 [-32.088,  3.000], mean action: 4.931 [1.000, 20.000],  loss: 0.021669, mae: 0.487897, mean_q: 0.473177, mean_eps: 0.000000
 2866/5000: episode: 117, duration: 0.249s, episode steps:  22, steps per second:  88, episode reward: 38.142, mean reward:  1.734 [-3.000, 31.989], mean action: 2.773 [0.000, 15.000],  loss: 0.018125, mae: 0.469191, mean_q: 0.465421, mean_eps: 0.000000
 2895/5000: episode: 118, duration: 0.323s, episode steps:  29, steps per second:  90, episode reward: -32.060, mean reward: -1.106 [-31.731,  2.261], mean action: 3.793 [1.000, 12.000],  loss: 0.017060, mae: 0.467038, mean_q: 0.445170, mean_eps: 0.000000
 2912/5000: episode: 119, duration: 0.204s, episode steps:  17, steps per second:  83, episode reward: 38.682, mean reward:  2.275 [-3.000, 32.120], mean action: 4.471 [0.000, 21.000],  loss: 0.017850, mae: 0.458249, mean_q: 0.462160, mean_eps: 0.000000
 2932/5000: episode: 120, duration: 0.239s, episode steps:  20, steps per second:  84, episode reward: 41.371, mean reward:  2.069 [-2.275, 32.250], mean action: 3.850 [0.000, 15.000],  loss: 0.020952, mae: 0.476100, mean_q: 0.463282, mean_eps: 0.000000
 2955/5000: episode: 121, duration: 0.259s, episode steps:  23, steps per second:  89, episode reward: 32.094, mean reward:  1.395 [-3.000, 32.530], mean action: 6.522 [0.000, 15.000],  loss: 0.021563, mae: 0.476008, mean_q: 0.465399, mean_eps: 0.000000
 2978/5000: episode: 122, duration: 0.257s, episode steps:  23, steps per second:  89, episode reward: -32.530, mean reward: -1.414 [-32.230,  2.439], mean action: 6.000 [0.000, 15.000],  loss: 0.020649, mae: 0.477615, mean_q: 0.458800, mean_eps: 0.000000
 2995/5000: episode: 123, duration: 0.199s, episode steps:  17, steps per second:  85, episode reward: 38.679, mean reward:  2.275 [-2.751, 33.000], mean action: 4.294 [0.000, 15.000],  loss: 0.019584, mae: 0.466072, mean_q: 0.444878, mean_eps: 0.000000
 3016/5000: episode: 124, duration: 0.244s, episode steps:  21, steps per second:  86, episode reward: 38.084, mean reward:  1.814 [-2.283, 32.132], mean action: 3.476 [0.000, 13.000],  loss: 0.017959, mae: 0.457430, mean_q: 0.457990, mean_eps: 0.000000
 3036/5000: episode: 125, duration: 0.220s, episode steps:  20, steps per second:  91, episode reward: -38.120, mean reward: -1.906 [-32.456,  3.000], mean action: 6.950 [0.000, 20.000],  loss: 0.021204, mae: 0.482855, mean_q: 0.457496, mean_eps: 0.000000
 3067/5000: episode: 126, duration: 0.448s, episode steps:  31, steps per second:  69, episode reward: 38.868, mean reward:  1.254 [-2.269, 32.160], mean action: 3.000 [0.000, 11.000],  loss: 0.017582, mae: 0.448462, mean_q: 0.483035, mean_eps: 0.000000
 3097/5000: episode: 127, duration: 0.339s, episode steps:  30, steps per second:  89, episode reward: -35.470, mean reward: -1.182 [-32.297,  2.241], mean action: 5.533 [0.000, 14.000],  loss: 0.016793, mae: 0.445724, mean_q: 0.444242, mean_eps: 0.000000
 3122/5000: episode: 128, duration: 0.282s, episode steps:  25, steps per second:  89, episode reward: 30.000, mean reward:  1.200 [-2.503, 30.000], mean action: 6.720 [0.000, 14.000],  loss: 0.019974, mae: 0.450593, mean_q: 0.476209, mean_eps: 0.000000
 3148/5000: episode: 129, duration: 0.288s, episode steps:  26, steps per second:  90, episode reward: 33.000, mean reward:  1.269 [-3.000, 32.530], mean action: 4.885 [0.000, 21.000],  loss: 0.020336, mae: 0.464397, mean_q: 0.425141, mean_eps: 0.000000
 3169/5000: episode: 130, duration: 0.237s, episode steps:  21, steps per second:  89, episode reward: 39.000, mean reward:  1.857 [-2.555, 33.000], mean action: 4.381 [0.000, 15.000],  loss: 0.016729, mae: 0.439575, mean_q: 0.419931, mean_eps: 0.000000
 3190/5000: episode: 131, duration: 0.243s, episode steps:  21, steps per second:  86, episode reward: 35.440, mean reward:  1.688 [-2.578, 32.200], mean action: 5.095 [0.000, 20.000],  loss: 0.020280, mae: 0.457794, mean_q: 0.456082, mean_eps: 0.000000
 3205/5000: episode: 132, duration: 0.173s, episode steps:  15, steps per second:  87, episode reward: 38.622, mean reward:  2.575 [-2.758, 32.072], mean action: 3.733 [0.000, 12.000],  loss: 0.023932, mae: 0.460661, mean_q: 0.515797, mean_eps: 0.000000
 3228/5000: episode: 133, duration: 0.263s, episode steps:  23, steps per second:  88, episode reward: -32.610, mean reward: -1.418 [-33.000,  2.330], mean action: 5.696 [0.000, 15.000],  loss: 0.021641, mae: 0.456765, mean_q: 0.537843, mean_eps: 0.000000
 3250/5000: episode: 134, duration: 0.305s, episode steps:  22, steps per second:  72, episode reward: -35.540, mean reward: -1.615 [-31.801,  2.140], mean action: 3.182 [0.000, 9.000],  loss: 0.017560, mae: 0.452454, mean_q: 0.519637, mean_eps: 0.000000
 3274/5000: episode: 135, duration: 0.269s, episode steps:  24, steps per second:  89, episode reward: -36.000, mean reward: -1.500 [-33.000,  2.460], mean action: 5.292 [0.000, 14.000],  loss: 0.021827, mae: 0.476511, mean_q: 0.465241, mean_eps: 0.000000
 3309/5000: episode: 136, duration: 0.383s, episode steps:  35, steps per second:  91, episode reward: -32.600, mean reward: -0.931 [-32.370,  2.610], mean action: 9.429 [0.000, 21.000],  loss: 0.018660, mae: 0.464812, mean_q: 0.442762, mean_eps: 0.000000
 3348/5000: episode: 137, duration: 0.424s, episode steps:  39, steps per second:  92, episode reward: -35.290, mean reward: -0.905 [-32.035,  2.775], mean action: 7.487 [1.000, 20.000],  loss: 0.021796, mae: 0.468457, mean_q: 0.454988, mean_eps: 0.000000
 3364/5000: episode: 138, duration: 0.188s, episode steps:  16, steps per second:  85, episode reward: 38.311, mean reward:  2.394 [-2.295, 32.024], mean action: 4.125 [0.000, 15.000],  loss: 0.024683, mae: 0.476054, mean_q: 0.479102, mean_eps: 0.000000
 3389/5000: episode: 139, duration: 0.275s, episode steps:  25, steps per second:  91, episode reward: -35.340, mean reward: -1.414 [-31.979,  2.601], mean action: 6.760 [0.000, 20.000],  loss: 0.017136, mae: 0.438785, mean_q: 0.504894, mean_eps: 0.000000
 3429/5000: episode: 140, duration: 0.461s, episode steps:  40, steps per second:  87, episode reward: -32.390, mean reward: -0.810 [-32.707,  2.522], mean action: 5.925 [0.000, 21.000],  loss: 0.016651, mae: 0.442020, mean_q: 0.486438, mean_eps: 0.000000
 3444/5000: episode: 141, duration: 0.175s, episode steps:  15, steps per second:  86, episode reward: 35.864, mean reward:  2.391 [-2.585, 30.967], mean action: 4.667 [0.000, 21.000],  loss: 0.021855, mae: 0.468916, mean_q: 0.480418, mean_eps: 0.000000
 3473/5000: episode: 142, duration: 0.328s, episode steps:  29, steps per second:  88, episode reward: 32.475, mean reward:  1.120 [-3.000, 32.290], mean action: 5.448 [0.000, 21.000],  loss: 0.021370, mae: 0.457908, mean_q: 0.486055, mean_eps: 0.000000
 3492/5000: episode: 143, duration: 0.212s, episode steps:  19, steps per second:  90, episode reward: -32.020, mean reward: -1.685 [-32.650,  3.000], mean action: 4.526 [0.000, 19.000],  loss: 0.020333, mae: 0.455429, mean_q: 0.450105, mean_eps: 0.000000
 3518/5000: episode: 144, duration: 0.290s, episode steps:  26, steps per second:  90, episode reward: 35.008, mean reward:  1.346 [-2.579, 32.230], mean action: 7.385 [0.000, 15.000],  loss: 0.021941, mae: 0.457805, mean_q: 0.459527, mean_eps: 0.000000
 3546/5000: episode: 145, duration: 0.310s, episode steps:  28, steps per second:  90, episode reward: -33.000, mean reward: -1.179 [-32.620,  2.551], mean action: 7.071 [1.000, 14.000],  loss: 0.019425, mae: 0.456352, mean_q: 0.439801, mean_eps: 0.000000
 3561/5000: episode: 146, duration: 0.174s, episode steps:  15, steps per second:  86, episode reward: 41.099, mean reward:  2.740 [-2.807, 33.000], mean action: 2.800 [0.000, 11.000],  loss: 0.020705, mae: 0.458300, mean_q: 0.437373, mean_eps: 0.000000
 3584/5000: episode: 147, duration: 0.258s, episode steps:  23, steps per second:  89, episode reward: 32.319, mean reward:  1.405 [-2.608, 32.067], mean action: 6.130 [0.000, 19.000],  loss: 0.021021, mae: 0.446931, mean_q: 0.456609, mean_eps: 0.000000
 3602/5000: episode: 148, duration: 0.212s, episode steps:  18, steps per second:  85, episode reward: 40.777, mean reward:  2.265 [-2.329, 32.888], mean action: 6.556 [0.000, 14.000],  loss: 0.020595, mae: 0.437941, mean_q: 0.492740, mean_eps: 0.000000
 3633/5000: episode: 149, duration: 0.344s, episode steps:  31, steps per second:  90, episode reward: -35.790, mean reward: -1.155 [-31.942,  2.330], mean action: 7.194 [0.000, 19.000],  loss: 0.021683, mae: 0.447071, mean_q: 0.455170, mean_eps: 0.000000
 3662/5000: episode: 150, duration: 0.326s, episode steps:  29, steps per second:  89, episode reward: 33.000, mean reward:  1.138 [-3.000, 32.170], mean action: 7.793 [0.000, 15.000],  loss: 0.016438, mae: 0.431035, mean_q: 0.420435, mean_eps: 0.000000
 3681/5000: episode: 151, duration: 0.218s, episode steps:  19, steps per second:  87, episode reward: 41.523, mean reward:  2.185 [-2.149, 31.988], mean action: 4.316 [0.000, 15.000],  loss: 0.022418, mae: 0.457896, mean_q: 0.392694, mean_eps: 0.000000
 3712/5000: episode: 152, duration: 0.342s, episode steps:  31, steps per second:  91, episode reward: 32.748, mean reward:  1.056 [-2.369, 32.110], mean action: 7.452 [1.000, 19.000],  loss: 0.022976, mae: 0.468979, mean_q: 0.431003, mean_eps: 0.000000
 3731/5000: episode: 153, duration: 0.219s, episode steps:  19, steps per second:  87, episode reward: 38.821, mean reward:  2.043 [-2.883, 32.500], mean action: 4.737 [0.000, 20.000],  loss: 0.019558, mae: 0.451946, mean_q: 0.476764, mean_eps: 0.000000
 3753/5000: episode: 154, duration: 0.250s, episode steps:  22, steps per second:  88, episode reward: 32.556, mean reward:  1.480 [-3.000, 32.130], mean action: 4.455 [0.000, 15.000],  loss: 0.020097, mae: 0.457381, mean_q: 0.488439, mean_eps: 0.000000
 3825/5000: episode: 155, duration: 0.774s, episode steps:  72, steps per second:  93, episode reward: 32.898, mean reward:  0.457 [-2.883, 32.170], mean action: 6.167 [0.000, 15.000],  loss: 0.020377, mae: 0.455183, mean_q: 0.449086, mean_eps: 0.000000
 3854/5000: episode: 156, duration: 0.310s, episode steps:  29, steps per second:  94, episode reward: 41.286, mean reward:  1.424 [-2.355, 32.063], mean action: 2.034 [0.000, 9.000],  loss: 0.020084, mae: 0.455147, mean_q: 0.411905, mean_eps: 0.000000
 3880/5000: episode: 157, duration: 0.288s, episode steps:  26, steps per second:  90, episode reward: -35.510, mean reward: -1.366 [-32.398,  2.450], mean action: 5.885 [3.000, 14.000],  loss: 0.018973, mae: 0.447453, mean_q: 0.432338, mean_eps: 0.000000
 3897/5000: episode: 158, duration: 0.195s, episode steps:  17, steps per second:  87, episode reward: 42.000, mean reward:  2.471 [-2.410, 33.000], mean action: 5.471 [1.000, 15.000],  loss: 0.018148, mae: 0.439460, mean_q: 0.486786, mean_eps: 0.000000
 3922/5000: episode: 159, duration: 0.283s, episode steps:  25, steps per second:  88, episode reward: 40.451, mean reward:  1.618 [-2.279, 32.010], mean action: 6.600 [0.000, 20.000],  loss: 0.020935, mae: 0.445482, mean_q: 0.481895, mean_eps: 0.000000
 3952/5000: episode: 160, duration: 0.330s, episode steps:  30, steps per second:  91, episode reward: 32.463, mean reward:  1.082 [-2.704, 32.212], mean action: 8.300 [0.000, 21.000],  loss: 0.022346, mae: 0.469771, mean_q: 0.458028, mean_eps: 0.000000
 3969/5000: episode: 161, duration: 0.207s, episode steps:  17, steps per second:  82, episode reward: 39.000, mean reward:  2.294 [-2.510, 32.380], mean action: 3.471 [0.000, 19.000],  loss: 0.021918, mae: 0.490300, mean_q: 0.383018, mean_eps: 0.000000
 3993/5000: episode: 162, duration: 0.274s, episode steps:  24, steps per second:  88, episode reward: 34.826, mean reward:  1.451 [-2.533, 31.526], mean action: 5.083 [1.000, 19.000],  loss: 0.022159, mae: 0.493581, mean_q: 0.374845, mean_eps: 0.000000
 4012/5000: episode: 163, duration: 0.216s, episode steps:  19, steps per second:  88, episode reward: -35.270, mean reward: -1.856 [-32.264,  2.604], mean action: 5.368 [0.000, 14.000],  loss: 0.016302, mae: 0.447651, mean_q: 0.414592, mean_eps: 0.000000
 4039/5000: episode: 164, duration: 0.298s, episode steps:  27, steps per second:  90, episode reward: -33.000, mean reward: -1.222 [-32.294,  2.621], mean action: 3.185 [0.000, 15.000],  loss: 0.019739, mae: 0.462019, mean_q: 0.455915, mean_eps: 0.000000
 4059/5000: episode: 165, duration: 0.226s, episode steps:  20, steps per second:  89, episode reward: -35.120, mean reward: -1.756 [-32.339,  2.661], mean action: 4.800 [0.000, 15.000],  loss: 0.017537, mae: 0.444446, mean_q: 0.532086, mean_eps: 0.000000
 4079/5000: episode: 166, duration: 0.237s, episode steps:  20, steps per second:  84, episode reward: 38.452, mean reward:  1.923 [-2.502, 32.741], mean action: 4.300 [0.000, 15.000],  loss: 0.021951, mae: 0.470658, mean_q: 0.462574, mean_eps: 0.000000
 4125/5000: episode: 167, duration: 0.500s, episode steps:  46, steps per second:  92, episode reward: 36.000, mean reward:  0.783 [-2.363, 30.070], mean action: 4.413 [0.000, 21.000],  loss: 0.018862, mae: 0.471589, mean_q: 0.387438, mean_eps: 0.000000
 4147/5000: episode: 168, duration: 0.261s, episode steps:  22, steps per second:  84, episode reward: 35.848, mean reward:  1.629 [-2.210, 32.278], mean action: 3.864 [0.000, 15.000],  loss: 0.024174, mae: 0.485214, mean_q: 0.449498, mean_eps: 0.000000
 4174/5000: episode: 169, duration: 0.306s, episode steps:  27, steps per second:  88, episode reward: -33.000, mean reward: -1.222 [-32.080,  2.440], mean action: 4.630 [0.000, 19.000],  loss: 0.019316, mae: 0.462643, mean_q: 0.452598, mean_eps: 0.000000
 4197/5000: episode: 170, duration: 0.262s, episode steps:  23, steps per second:  88, episode reward: 34.941, mean reward:  1.519 [-3.000, 32.120], mean action: 5.391 [0.000, 14.000],  loss: 0.018310, mae: 0.471800, mean_q: 0.426322, mean_eps: 0.000000
 4214/5000: episode: 171, duration: 0.195s, episode steps:  17, steps per second:  87, episode reward: 41.443, mean reward:  2.438 [-2.231, 32.066], mean action: 3.353 [0.000, 12.000],  loss: 0.018297, mae: 0.467385, mean_q: 0.430439, mean_eps: 0.000000
 4231/5000: episode: 172, duration: 0.216s, episode steps:  17, steps per second:  79, episode reward: 39.000, mean reward:  2.294 [-3.000, 32.030], mean action: 4.176 [0.000, 15.000],  loss: 0.022345, mae: 0.487236, mean_q: 0.405744, mean_eps: 0.000000
 4259/5000: episode: 173, duration: 0.350s, episode steps:  28, steps per second:  80, episode reward: -35.110, mean reward: -1.254 [-32.110,  2.522], mean action: 5.429 [0.000, 15.000],  loss: 0.017452, mae: 0.465906, mean_q: 0.420856, mean_eps: 0.000000
 4277/5000: episode: 174, duration: 0.287s, episode steps:  18, steps per second:  63, episode reward: 38.043, mean reward:  2.114 [-2.609, 32.430], mean action: 3.056 [0.000, 11.000],  loss: 0.018209, mae: 0.479709, mean_q: 0.402441, mean_eps: 0.000000
 4307/5000: episode: 175, duration: 0.372s, episode steps:  30, steps per second:  81, episode reward: 34.582, mean reward:  1.153 [-3.000, 32.361], mean action: 4.733 [0.000, 15.000],  loss: 0.018013, mae: 0.463381, mean_q: 0.419061, mean_eps: 0.000000
 4330/5000: episode: 176, duration: 0.267s, episode steps:  23, steps per second:  86, episode reward: -35.130, mean reward: -1.527 [-31.725,  2.320], mean action: 3.739 [0.000, 12.000],  loss: 0.022327, mae: 0.479166, mean_q: 0.474076, mean_eps: 0.000000
 4355/5000: episode: 177, duration: 0.280s, episode steps:  25, steps per second:  89, episode reward: 32.255, mean reward:  1.290 [-2.514, 32.210], mean action: 3.200 [0.000, 19.000],  loss: 0.021656, mae: 0.471511, mean_q: 0.459064, mean_eps: 0.000000
 4389/5000: episode: 178, duration: 0.382s, episode steps:  34, steps per second:  89, episode reward: 34.649, mean reward:  1.019 [-2.448, 32.120], mean action: 7.471 [0.000, 21.000],  loss: 0.018472, mae: 0.461392, mean_q: 0.448102, mean_eps: 0.000000
 4417/5000: episode: 179, duration: 0.323s, episode steps:  28, steps per second:  87, episode reward: -32.550, mean reward: -1.162 [-31.637,  2.900], mean action: 8.071 [0.000, 19.000],  loss: 0.023586, mae: 0.478528, mean_q: 0.381869, mean_eps: 0.000000
 4441/5000: episode: 180, duration: 0.263s, episode steps:  24, steps per second:  91, episode reward: -38.690, mean reward: -1.612 [-31.860,  2.460], mean action: 10.208 [0.000, 19.000],  loss: 0.022095, mae: 0.475429, mean_q: 0.414138, mean_eps: 0.000000
 4468/5000: episode: 181, duration: 0.314s, episode steps:  27, steps per second:  86, episode reward: -32.490, mean reward: -1.203 [-31.811,  2.534], mean action: 7.037 [0.000, 19.000],  loss: 0.017722, mae: 0.453627, mean_q: 0.481160, mean_eps: 0.000000
 4485/5000: episode: 182, duration: 0.196s, episode steps:  17, steps per second:  87, episode reward: -38.490, mean reward: -2.264 [-32.500,  2.250], mean action: 6.529 [0.000, 15.000],  loss: 0.018622, mae: 0.455792, mean_q: 0.460379, mean_eps: 0.000000
 4506/5000: episode: 183, duration: 0.242s, episode steps:  21, steps per second:  87, episode reward: 35.852, mean reward:  1.707 [-3.000, 32.531], mean action: 5.952 [0.000, 19.000],  loss: 0.020313, mae: 0.471657, mean_q: 0.447131, mean_eps: 0.000000
 4549/5000: episode: 184, duration: 0.471s, episode steps:  43, steps per second:  91, episode reward: 32.698, mean reward:  0.760 [-2.238, 32.470], mean action: 2.488 [0.000, 15.000],  loss: 0.017631, mae: 0.454405, mean_q: 0.435890, mean_eps: 0.000000
 4580/5000: episode: 185, duration: 0.345s, episode steps:  31, steps per second:  90, episode reward: 32.817, mean reward:  1.059 [-2.507, 31.979], mean action: 3.516 [0.000, 19.000],  loss: 0.017994, mae: 0.463860, mean_q: 0.409773, mean_eps: 0.000000
 4596/5000: episode: 186, duration: 0.187s, episode steps:  16, steps per second:  86, episode reward: 44.629, mean reward:  2.789 [-2.201, 32.080], mean action: 4.375 [0.000, 15.000],  loss: 0.019439, mae: 0.482563, mean_q: 0.405596, mean_eps: 0.000000
 4640/5000: episode: 187, duration: 0.479s, episode steps:  44, steps per second:  92, episode reward: 32.472, mean reward:  0.738 [-3.000, 31.931], mean action: 4.227 [0.000, 20.000],  loss: 0.019140, mae: 0.478330, mean_q: 0.435062, mean_eps: 0.000000
 4675/5000: episode: 188, duration: 0.392s, episode steps:  35, steps per second:  89, episode reward: 38.368, mean reward:  1.096 [-2.215, 32.714], mean action: 2.971 [0.000, 15.000],  loss: 0.022308, mae: 0.481120, mean_q: 0.432557, mean_eps: 0.000000
 4701/5000: episode: 189, duration: 0.294s, episode steps:  26, steps per second:  88, episode reward: -32.640, mean reward: -1.255 [-32.502,  2.340], mean action: 5.654 [0.000, 19.000],  loss: 0.018781, mae: 0.468467, mean_q: 0.399804, mean_eps: 0.000000
 4725/5000: episode: 190, duration: 0.272s, episode steps:  24, steps per second:  88, episode reward: 38.497, mean reward:  1.604 [-2.738, 32.560], mean action: 4.792 [0.000, 14.000],  loss: 0.019275, mae: 0.467825, mean_q: 0.363456, mean_eps: 0.000000
 4756/5000: episode: 191, duration: 0.342s, episode steps:  31, steps per second:  91, episode reward: -38.720, mean reward: -1.249 [-32.082,  2.124], mean action: 9.387 [0.000, 15.000],  loss: 0.018022, mae: 0.466496, mean_q: 0.382499, mean_eps: 0.000000
 4787/5000: episode: 192, duration: 0.353s, episode steps:  31, steps per second:  88, episode reward: 32.787, mean reward:  1.058 [-2.489, 32.100], mean action: 10.097 [1.000, 15.000],  loss: 0.021176, mae: 0.469450, mean_q: 0.426416, mean_eps: 0.000000
 4807/5000: episode: 193, duration: 0.228s, episode steps:  20, steps per second:  88, episode reward: 33.000, mean reward:  1.650 [-3.000, 32.260], mean action: 3.950 [0.000, 11.000],  loss: 0.018067, mae: 0.449107, mean_q: 0.427202, mean_eps: 0.000000
 4830/5000: episode: 194, duration: 0.260s, episode steps:  23, steps per second:  88, episode reward: 32.255, mean reward:  1.402 [-3.000, 32.390], mean action: 4.043 [0.000, 17.000],  loss: 0.020936, mae: 0.467578, mean_q: 0.444627, mean_eps: 0.000000
 4854/5000: episode: 195, duration: 0.270s, episode steps:  24, steps per second:  89, episode reward: 32.822, mean reward:  1.368 [-2.395, 32.560], mean action: 3.833 [0.000, 19.000],  loss: 0.018922, mae: 0.456403, mean_q: 0.426918, mean_eps: 0.000000
 4874/5000: episode: 196, duration: 0.226s, episode steps:  20, steps per second:  88, episode reward: 35.418, mean reward:  1.771 [-3.000, 32.757], mean action: 3.850 [0.000, 19.000],  loss: 0.019249, mae: 0.458045, mean_q: 0.509471, mean_eps: 0.000000
 4899/5000: episode: 197, duration: 0.281s, episode steps:  25, steps per second:  89, episode reward: 32.179, mean reward:  1.287 [-2.584, 32.152], mean action: 5.760 [0.000, 14.000],  loss: 0.017187, mae: 0.437125, mean_q: 0.462215, mean_eps: 0.000000
 4921/5000: episode: 198, duration: 0.244s, episode steps:  22, steps per second:  90, episode reward: 33.000, mean reward:  1.500 [-3.000, 32.570], mean action: 4.636 [0.000, 19.000],  loss: 0.020951, mae: 0.462773, mean_q: 0.461533, mean_eps: 0.000000
 4939/5000: episode: 199, duration: 0.207s, episode steps:  18, steps per second:  87, episode reward: 38.788, mean reward:  2.155 [-2.520, 32.951], mean action: 3.222 [0.000, 15.000],  loss: 0.021647, mae: 0.464491, mean_q: 0.413135, mean_eps: 0.000000
 4958/5000: episode: 200, duration: 0.221s, episode steps:  19, steps per second:  86, episode reward: 38.697, mean reward:  2.037 [-2.647, 31.737], mean action: 2.632 [0.000, 19.000],  loss: 0.020123, mae: 0.461264, mean_q: 0.406742, mean_eps: 0.000000
 4978/5000: episode: 201, duration: 0.232s, episode steps:  20, steps per second:  86, episode reward: 36.000, mean reward:  1.800 [-2.357, 32.250], mean action: 4.450 [0.000, 19.000],  loss: 0.018982, mae: 0.451654, mean_q: 0.467551, mean_eps: 0.000000
done, took 52.440 seconds
DQN Evaluation: 1210 victories out of 1466 episodes
Training for 5000 steps ...
   18/5000: episode: 1, duration: 0.130s, episode steps:  18, steps per second: 139, episode reward: 47.600, mean reward:  2.644 [-0.231, 31.900], mean action: 1.722 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   32/5000: episode: 2, duration: 0.106s, episode steps:  14, steps per second: 132, episode reward: 41.562, mean reward:  2.969 [-2.480, 31.802], mean action: 3.643 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   51/5000: episode: 3, duration: 0.133s, episode steps:  19, steps per second: 142, episode reward: 44.180, mean reward:  2.325 [-2.248, 32.038], mean action: 3.105 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   68/5000: episode: 4, duration: 0.123s, episode steps:  17, steps per second: 138, episode reward: 39.000, mean reward:  2.294 [-3.000, 32.690], mean action: 5.647 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   84/5000: episode: 5, duration: 0.117s, episode steps:  16, steps per second: 137, episode reward: 41.867, mean reward:  2.617 [-2.221, 30.374], mean action: 4.062 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  115/5000: episode: 6, duration: 0.196s, episode steps:  31, steps per second: 158, episode reward: 43.433, mean reward:  1.401 [-2.042, 31.843], mean action: 3.290 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  146/5000: episode: 7, duration: 0.200s, episode steps:  31, steps per second: 155, episode reward: 39.000, mean reward:  1.258 [-2.825, 32.080], mean action: 2.613 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  170/5000: episode: 8, duration: 0.153s, episode steps:  24, steps per second: 157, episode reward: 45.000, mean reward:  1.875 [-2.464, 32.670], mean action: 5.333 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  194/5000: episode: 9, duration: 0.155s, episode steps:  24, steps per second: 155, episode reward: 41.258, mean reward:  1.719 [-2.938, 32.070], mean action: 3.625 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  224/5000: episode: 10, duration: 0.194s, episode steps:  30, steps per second: 155, episode reward: 41.582, mean reward:  1.386 [-2.105, 31.942], mean action: 2.033 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  253/5000: episode: 11, duration: 0.172s, episode steps:  29, steps per second: 168, episode reward: 32.613, mean reward:  1.125 [-3.000, 32.613], mean action: 7.241 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  284/5000: episode: 12, duration: 0.191s, episode steps:  31, steps per second: 162, episode reward: 37.684, mean reward:  1.216 [-2.412, 31.712], mean action: 4.355 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  318/5000: episode: 13, duration: 0.218s, episode steps:  34, steps per second: 156, episode reward: 40.515, mean reward:  1.192 [-2.876, 32.160], mean action: 5.382 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  345/5000: episode: 14, duration: 0.167s, episode steps:  27, steps per second: 162, episode reward: 38.850, mean reward:  1.439 [-2.536, 32.115], mean action: 3.185 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  369/5000: episode: 15, duration: 0.154s, episode steps:  24, steps per second: 156, episode reward: 38.676, mean reward:  1.611 [-2.603, 32.120], mean action: 4.333 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  395/5000: episode: 16, duration: 0.167s, episode steps:  26, steps per second: 155, episode reward: 43.808, mean reward:  1.685 [-2.619, 31.788], mean action: 3.154 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  427/5000: episode: 17, duration: 0.201s, episode steps:  32, steps per second: 159, episode reward: 42.000, mean reward:  1.313 [-2.278, 32.060], mean action: 3.000 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  450/5000: episode: 18, duration: 0.144s, episode steps:  23, steps per second: 160, episode reward: 41.281, mean reward:  1.795 [-2.757, 31.958], mean action: 5.043 [1.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  487/5000: episode: 19, duration: 0.227s, episode steps:  37, steps per second: 163, episode reward: 35.827, mean reward:  0.968 [-2.247, 32.110], mean action: 5.919 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  508/5000: episode: 20, duration: 0.147s, episode steps:  21, steps per second: 143, episode reward: 44.408, mean reward:  2.115 [-2.000, 32.210], mean action: 1.714 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  526/5000: episode: 21, duration: 0.129s, episode steps:  18, steps per second: 139, episode reward: 41.163, mean reward:  2.287 [-2.730, 31.785], mean action: 3.500 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  556/5000: episode: 22, duration: 0.194s, episode steps:  30, steps per second: 155, episode reward: 41.442, mean reward:  1.381 [-2.261, 32.300], mean action: 2.667 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  582/5000: episode: 23, duration: 0.171s, episode steps:  26, steps per second: 152, episode reward: 41.440, mean reward:  1.594 [-2.554, 31.979], mean action: 4.231 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  604/5000: episode: 24, duration: 0.156s, episode steps:  22, steps per second: 141, episode reward: 41.566, mean reward:  1.889 [-2.095, 31.929], mean action: 4.818 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  631/5000: episode: 25, duration: 0.170s, episode steps:  27, steps per second: 159, episode reward: 41.015, mean reward:  1.519 [-2.507, 32.840], mean action: 4.593 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  647/5000: episode: 26, duration: 0.122s, episode steps:  16, steps per second: 131, episode reward: 41.057, mean reward:  2.566 [-3.000, 31.737], mean action: 2.750 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  678/5000: episode: 27, duration: 0.213s, episode steps:  31, steps per second: 145, episode reward: 37.507, mean reward:  1.210 [-2.497, 31.724], mean action: 5.742 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  703/5000: episode: 28, duration: 0.285s, episode steps:  25, steps per second:  88, episode reward: 41.603, mean reward:  1.664 [-2.737, 32.350], mean action: 2.720 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  738/5000: episode: 29, duration: 0.216s, episode steps:  35, steps per second: 162, episode reward: 41.368, mean reward:  1.182 [-2.425, 32.515], mean action: 4.257 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  759/5000: episode: 30, duration: 0.144s, episode steps:  21, steps per second: 145, episode reward: 41.310, mean reward:  1.967 [-2.695, 32.020], mean action: 4.000 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  783/5000: episode: 31, duration: 0.160s, episode steps:  24, steps per second: 150, episode reward: 44.531, mean reward:  1.855 [-2.212, 32.027], mean action: 3.875 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  816/5000: episode: 32, duration: 0.200s, episode steps:  33, steps per second: 165, episode reward: 35.000, mean reward:  1.061 [-2.837, 31.743], mean action: 3.667 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  833/5000: episode: 33, duration: 0.111s, episode steps:  17, steps per second: 153, episode reward: 47.584, mean reward:  2.799 [-0.136, 32.120], mean action: 1.353 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  856/5000: episode: 34, duration: 0.149s, episode steps:  23, steps per second: 154, episode reward: 45.000, mean reward:  1.957 [-2.549, 32.170], mean action: 2.609 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  881/5000: episode: 35, duration: 0.165s, episode steps:  25, steps per second: 152, episode reward: 41.352, mean reward:  1.654 [-3.000, 32.150], mean action: 4.680 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  908/5000: episode: 36, duration: 0.177s, episode steps:  27, steps per second: 153, episode reward: 38.662, mean reward:  1.432 [-2.901, 31.895], mean action: 4.333 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  936/5000: episode: 37, duration: 0.175s, episode steps:  28, steps per second: 160, episode reward: 35.124, mean reward:  1.254 [-3.000, 31.875], mean action: 5.429 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  985/5000: episode: 38, duration: 0.295s, episode steps:  49, steps per second: 166, episode reward: 34.777, mean reward:  0.710 [-3.000, 32.080], mean action: 5.755 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1018/5000: episode: 39, duration: 0.317s, episode steps:  33, steps per second: 104, episode reward: 41.639, mean reward:  1.262 [-2.256, 32.029], mean action: 2.455 [0.000, 20.000],  loss: 0.017434, mae: 0.438797, mean_q: 0.487683, mean_eps: 0.000000
 1046/5000: episode: 40, duration: 0.334s, episode steps:  28, steps per second:  84, episode reward: 38.622, mean reward:  1.379 [-2.360, 31.822], mean action: 1.929 [0.000, 19.000],  loss: 0.018880, mae: 0.454995, mean_q: 0.481784, mean_eps: 0.000000
 1084/5000: episode: 41, duration: 0.425s, episode steps:  38, steps per second:  89, episode reward: 41.353, mean reward:  1.088 [-2.160, 32.180], mean action: 2.237 [0.000, 19.000],  loss: 0.016331, mae: 0.446452, mean_q: 0.465091, mean_eps: 0.000000
 1113/5000: episode: 42, duration: 0.327s, episode steps:  29, steps per second:  89, episode reward: 35.083, mean reward:  1.210 [-2.274, 31.889], mean action: 5.207 [0.000, 19.000],  loss: 0.019970, mae: 0.456967, mean_q: 0.487318, mean_eps: 0.000000
 1138/5000: episode: 43, duration: 0.288s, episode steps:  25, steps per second:  87, episode reward: 44.302, mean reward:  1.772 [-2.055, 31.753], mean action: 1.640 [0.000, 14.000],  loss: 0.020848, mae: 0.465346, mean_q: 0.522136, mean_eps: 0.000000
 1161/5000: episode: 44, duration: 0.260s, episode steps:  23, steps per second:  88, episode reward: 44.367, mean reward:  1.929 [-2.339, 32.240], mean action: 0.652 [0.000, 9.000],  loss: 0.023247, mae: 0.485735, mean_q: 0.520372, mean_eps: 0.000000
 1193/5000: episode: 45, duration: 0.352s, episode steps:  32, steps per second:  91, episode reward: 41.925, mean reward:  1.310 [-3.000, 32.025], mean action: 2.188 [0.000, 14.000],  loss: 0.019923, mae: 0.454572, mean_q: 0.488170, mean_eps: 0.000000
 1212/5000: episode: 46, duration: 0.224s, episode steps:  19, steps per second:  85, episode reward: 44.262, mean reward:  2.330 [-2.243, 32.022], mean action: 2.842 [0.000, 14.000],  loss: 0.022568, mae: 0.468906, mean_q: 0.480945, mean_eps: 0.000000
 1233/5000: episode: 47, duration: 0.249s, episode steps:  21, steps per second:  84, episode reward: 41.578, mean reward:  1.980 [-2.722, 31.888], mean action: 3.905 [0.000, 20.000],  loss: 0.016345, mae: 0.432743, mean_q: 0.504629, mean_eps: 0.000000
 1254/5000: episode: 48, duration: 0.238s, episode steps:  21, steps per second:  88, episode reward: 41.638, mean reward:  1.983 [-2.720, 32.230], mean action: 4.952 [0.000, 19.000],  loss: 0.020208, mae: 0.450570, mean_q: 0.519159, mean_eps: 0.000000
 1269/5000: episode: 49, duration: 0.184s, episode steps:  15, steps per second:  81, episode reward: 41.876, mean reward:  2.792 [-2.151, 32.230], mean action: 4.267 [0.000, 14.000],  loss: 0.019891, mae: 0.451065, mean_q: 0.516292, mean_eps: 0.000000
 1289/5000: episode: 50, duration: 0.229s, episode steps:  20, steps per second:  87, episode reward: 44.684, mean reward:  2.234 [-2.327, 32.070], mean action: 5.350 [0.000, 18.000],  loss: 0.020072, mae: 0.452852, mean_q: 0.471373, mean_eps: 0.000000
 1302/5000: episode: 51, duration: 0.155s, episode steps:  13, steps per second:  84, episode reward: 44.140, mean reward:  3.395 [-2.458, 32.016], mean action: 2.154 [0.000, 9.000],  loss: 0.015200, mae: 0.436528, mean_q: 0.469446, mean_eps: 0.000000
 1331/5000: episode: 52, duration: 0.331s, episode steps:  29, steps per second:  88, episode reward: 37.797, mean reward:  1.303 [-2.619, 31.958], mean action: 4.172 [0.000, 14.000],  loss: 0.016917, mae: 0.448220, mean_q: 0.420684, mean_eps: 0.000000
 1358/5000: episode: 53, duration: 0.316s, episode steps:  27, steps per second:  85, episode reward: 44.633, mean reward:  1.653 [-2.302, 31.923], mean action: 1.370 [0.000, 9.000],  loss: 0.019102, mae: 0.452862, mean_q: 0.446523, mean_eps: 0.000000
 1380/5000: episode: 54, duration: 0.255s, episode steps:  22, steps per second:  86, episode reward: 44.484, mean reward:  2.022 [-2.454, 32.076], mean action: 1.773 [0.000, 9.000],  loss: 0.024728, mae: 0.471935, mean_q: 0.510961, mean_eps: 0.000000
 1413/5000: episode: 55, duration: 0.370s, episode steps:  33, steps per second:  89, episode reward: 41.242, mean reward:  1.250 [-2.066, 31.934], mean action: 5.273 [0.000, 20.000],  loss: 0.019622, mae: 0.446257, mean_q: 0.501595, mean_eps: 0.000000
 1445/5000: episode: 56, duration: 0.352s, episode steps:  32, steps per second:  91, episode reward: 41.225, mean reward:  1.288 [-2.324, 32.273], mean action: 2.344 [0.000, 12.000],  loss: 0.021657, mae: 0.455684, mean_q: 0.503875, mean_eps: 0.000000
 1457/5000: episode: 57, duration: 0.149s, episode steps:  12, steps per second:  81, episode reward: 47.752, mean reward:  3.979 [ 0.000, 33.000], mean action: 5.833 [1.000, 14.000],  loss: 0.024041, mae: 0.469507, mean_q: 0.454018, mean_eps: 0.000000
 1492/5000: episode: 58, duration: 0.399s, episode steps:  35, steps per second:  88, episode reward: 37.032, mean reward:  1.058 [-2.379, 31.658], mean action: 5.229 [0.000, 15.000],  loss: 0.018117, mae: 0.456204, mean_q: 0.400151, mean_eps: 0.000000
 1517/5000: episode: 59, duration: 0.296s, episode steps:  25, steps per second:  84, episode reward: 44.888, mean reward:  1.796 [-2.200, 32.170], mean action: 1.920 [0.000, 11.000],  loss: 0.020444, mae: 0.476407, mean_q: 0.388312, mean_eps: 0.000000
 1539/5000: episode: 60, duration: 0.247s, episode steps:  22, steps per second:  89, episode reward: 38.356, mean reward:  1.743 [-3.000, 32.910], mean action: 3.773 [0.000, 12.000],  loss: 0.023210, mae: 0.483431, mean_q: 0.421557, mean_eps: 0.000000
 1567/5000: episode: 61, duration: 0.322s, episode steps:  28, steps per second:  87, episode reward: 44.454, mean reward:  1.588 [-2.437, 32.041], mean action: 4.714 [0.000, 15.000],  loss: 0.017399, mae: 0.454603, mean_q: 0.432186, mean_eps: 0.000000
 1594/5000: episode: 62, duration: 0.307s, episode steps:  27, steps per second:  88, episode reward: 42.000, mean reward:  1.556 [-3.000, 32.050], mean action: 3.963 [0.000, 15.000],  loss: 0.016214, mae: 0.450418, mean_q: 0.426539, mean_eps: 0.000000
 1617/5000: episode: 63, duration: 0.257s, episode steps:  23, steps per second:  89, episode reward: 35.272, mean reward:  1.534 [-3.000, 31.761], mean action: 5.826 [0.000, 20.000],  loss: 0.016778, mae: 0.447804, mean_q: 0.440429, mean_eps: 0.000000
 1651/5000: episode: 64, duration: 0.390s, episode steps:  34, steps per second:  87, episode reward: 35.278, mean reward:  1.038 [-2.938, 32.320], mean action: 5.118 [0.000, 20.000],  loss: 0.020791, mae: 0.459895, mean_q: 0.506889, mean_eps: 0.000000
 1671/5000: episode: 65, duration: 0.233s, episode steps:  20, steps per second:  86, episode reward: 44.498, mean reward:  2.225 [-2.831, 32.238], mean action: 0.450 [0.000, 9.000],  loss: 0.023245, mae: 0.468071, mean_q: 0.523955, mean_eps: 0.000000
 1689/5000: episode: 66, duration: 0.207s, episode steps:  18, steps per second:  87, episode reward: 41.236, mean reward:  2.291 [-3.000, 32.460], mean action: 1.722 [0.000, 11.000],  loss: 0.019532, mae: 0.453111, mean_q: 0.501801, mean_eps: 0.000000
 1734/5000: episode: 67, duration: 0.510s, episode steps:  45, steps per second:  88, episode reward: 37.671, mean reward:  0.837 [-3.000, 31.731], mean action: 3.711 [0.000, 14.000],  loss: 0.018526, mae: 0.459049, mean_q: 0.483993, mean_eps: 0.000000
 1760/5000: episode: 68, duration: 0.291s, episode steps:  26, steps per second:  89, episode reward: 41.051, mean reward:  1.579 [-2.254, 31.412], mean action: 2.692 [0.000, 13.000],  loss: 0.019124, mae: 0.460914, mean_q: 0.421689, mean_eps: 0.000000
 1787/5000: episode: 69, duration: 0.306s, episode steps:  27, steps per second:  88, episode reward: 43.976, mean reward:  1.629 [-2.014, 32.018], mean action: 3.519 [0.000, 14.000],  loss: 0.016739, mae: 0.444731, mean_q: 0.426493, mean_eps: 0.000000
 1807/5000: episode: 70, duration: 0.235s, episode steps:  20, steps per second:  85, episode reward: 43.781, mean reward:  2.189 [-2.033, 32.410], mean action: 2.700 [0.000, 13.000],  loss: 0.017667, mae: 0.449922, mean_q: 0.422421, mean_eps: 0.000000
 1821/5000: episode: 71, duration: 0.168s, episode steps:  14, steps per second:  83, episode reward: 47.341, mean reward:  3.381 [ 0.010, 33.000], mean action: 0.929 [0.000, 3.000],  loss: 0.018326, mae: 0.459960, mean_q: 0.484993, mean_eps: 0.000000
 1850/5000: episode: 72, duration: 0.326s, episode steps:  29, steps per second:  89, episode reward: 41.596, mean reward:  1.434 [-2.250, 30.096], mean action: 3.586 [0.000, 15.000],  loss: 0.018498, mae: 0.466402, mean_q: 0.464182, mean_eps: 0.000000
 1866/5000: episode: 73, duration: 0.192s, episode steps:  16, steps per second:  83, episode reward: 44.417, mean reward:  2.776 [-2.446, 32.903], mean action: 0.938 [0.000, 9.000],  loss: 0.018215, mae: 0.456752, mean_q: 0.477301, mean_eps: 0.000000
 1903/5000: episode: 74, duration: 0.413s, episode steps:  37, steps per second:  90, episode reward: 37.124, mean reward:  1.003 [-3.000, 31.635], mean action: 3.730 [0.000, 20.000],  loss: 0.020995, mae: 0.476439, mean_q: 0.440247, mean_eps: 0.000000
 1931/5000: episode: 75, duration: 0.432s, episode steps:  28, steps per second:  65, episode reward: 38.404, mean reward:  1.372 [-2.811, 32.160], mean action: 4.857 [0.000, 15.000],  loss: 0.022052, mae: 0.486992, mean_q: 0.436829, mean_eps: 0.000000
 1978/5000: episode: 76, duration: 0.547s, episode steps:  47, steps per second:  86, episode reward: 37.826, mean reward:  0.805 [-2.819, 32.200], mean action: 4.064 [0.000, 15.000],  loss: 0.017565, mae: 0.457158, mean_q: 0.440169, mean_eps: 0.000000
 2003/5000: episode: 77, duration: 0.284s, episode steps:  25, steps per second:  88, episode reward: 40.878, mean reward:  1.635 [-2.197, 32.090], mean action: 3.600 [0.000, 14.000],  loss: 0.017397, mae: 0.459673, mean_q: 0.460461, mean_eps: 0.000000
 2018/5000: episode: 78, duration: 0.179s, episode steps:  15, steps per second:  84, episode reward: 44.209, mean reward:  2.947 [-2.440, 31.639], mean action: 1.600 [0.000, 9.000],  loss: 0.020271, mae: 0.466872, mean_q: 0.459671, mean_eps: 0.000000
 2042/5000: episode: 79, duration: 0.271s, episode steps:  24, steps per second:  89, episode reward: 41.377, mean reward:  1.724 [-2.644, 32.070], mean action: 3.792 [0.000, 15.000],  loss: 0.021158, mae: 0.466249, mean_q: 0.494257, mean_eps: 0.000000
 2077/5000: episode: 80, duration: 0.451s, episode steps:  35, steps per second:  78, episode reward: 44.370, mean reward:  1.268 [-2.026, 32.300], mean action: 1.486 [0.000, 9.000],  loss: 0.020162, mae: 0.466376, mean_q: 0.479518, mean_eps: 0.000000
 2100/5000: episode: 81, duration: 0.257s, episode steps:  23, steps per second:  90, episode reward: 44.018, mean reward:  1.914 [-2.048, 31.492], mean action: 2.130 [0.000, 14.000],  loss: 0.018163, mae: 0.464018, mean_q: 0.499385, mean_eps: 0.000000
 2115/5000: episode: 82, duration: 0.179s, episode steps:  15, steps per second:  84, episode reward: 44.171, mean reward:  2.945 [-2.288, 32.510], mean action: 3.667 [0.000, 14.000],  loss: 0.018836, mae: 0.454947, mean_q: 0.466339, mean_eps: 0.000000
 2153/5000: episode: 83, duration: 0.424s, episode steps:  38, steps per second:  90, episode reward: 38.684, mean reward:  1.018 [-2.440, 32.303], mean action: 2.711 [0.000, 15.000],  loss: 0.019662, mae: 0.467113, mean_q: 0.450623, mean_eps: 0.000000
 2168/5000: episode: 84, duration: 0.182s, episode steps:  15, steps per second:  82, episode reward: 47.314, mean reward:  3.154 [ 0.000, 33.000], mean action: 2.600 [1.000, 3.000],  loss: 0.019132, mae: 0.462627, mean_q: 0.455229, mean_eps: 0.000000
 2198/5000: episode: 85, duration: 0.333s, episode steps:  30, steps per second:  90, episode reward: 42.000, mean reward:  1.400 [-2.106, 32.160], mean action: 1.800 [0.000, 20.000],  loss: 0.020619, mae: 0.473014, mean_q: 0.455689, mean_eps: 0.000000
 2209/5000: episode: 86, duration: 0.135s, episode steps:  11, steps per second:  81, episode reward: 44.903, mean reward:  4.082 [-2.383, 33.000], mean action: 2.727 [0.000, 11.000],  loss: 0.018171, mae: 0.456158, mean_q: 0.402786, mean_eps: 0.000000
 2231/5000: episode: 87, duration: 0.248s, episode steps:  22, steps per second:  89, episode reward: 44.076, mean reward:  2.003 [-2.094, 32.109], mean action: 3.364 [1.000, 15.000],  loss: 0.022215, mae: 0.474246, mean_q: 0.444306, mean_eps: 0.000000
 2258/5000: episode: 88, duration: 0.321s, episode steps:  27, steps per second:  84, episode reward: 41.344, mean reward:  1.531 [-2.557, 31.757], mean action: 3.074 [0.000, 15.000],  loss: 0.021850, mae: 0.464506, mean_q: 0.473340, mean_eps: 0.000000
 2283/5000: episode: 89, duration: 0.278s, episode steps:  25, steps per second:  90, episode reward: 35.289, mean reward:  1.412 [-3.000, 31.319], mean action: 2.360 [0.000, 11.000],  loss: 0.019029, mae: 0.460664, mean_q: 0.408222, mean_eps: 0.000000
 2324/5000: episode: 90, duration: 0.460s, episode steps:  41, steps per second:  89, episode reward: -32.240, mean reward: -0.786 [-32.429,  2.580], mean action: 7.439 [0.000, 15.000],  loss: 0.022684, mae: 0.472315, mean_q: 0.417676, mean_eps: 0.000000
 2347/5000: episode: 91, duration: 0.267s, episode steps:  23, steps per second:  86, episode reward: 38.281, mean reward:  1.664 [-2.778, 31.953], mean action: 6.000 [0.000, 21.000],  loss: 0.017918, mae: 0.445894, mean_q: 0.441877, mean_eps: 0.000000
 2374/5000: episode: 92, duration: 0.304s, episode steps:  27, steps per second:  89, episode reward: 35.900, mean reward:  1.330 [-2.404, 32.280], mean action: 2.556 [0.000, 15.000],  loss: 0.018409, mae: 0.446366, mean_q: 0.443765, mean_eps: 0.000000
 2391/5000: episode: 93, duration: 0.206s, episode steps:  17, steps per second:  83, episode reward: 38.538, mean reward:  2.267 [-2.307, 32.470], mean action: 2.529 [0.000, 9.000],  loss: 0.021877, mae: 0.473268, mean_q: 0.463385, mean_eps: 0.000000
 2412/5000: episode: 94, duration: 0.243s, episode steps:  21, steps per second:  86, episode reward: 44.975, mean reward:  2.142 [-2.137, 32.110], mean action: 2.905 [0.000, 9.000],  loss: 0.017214, mae: 0.456433, mean_q: 0.453997, mean_eps: 0.000000
 2441/5000: episode: 95, duration: 0.325s, episode steps:  29, steps per second:  89, episode reward: 41.167, mean reward:  1.420 [-2.775, 32.560], mean action: 3.241 [0.000, 19.000],  loss: 0.017952, mae: 0.459657, mean_q: 0.426070, mean_eps: 0.000000
 2455/5000: episode: 96, duration: 0.171s, episode steps:  14, steps per second:  82, episode reward: 42.000, mean reward:  3.000 [-2.294, 32.910], mean action: 4.571 [1.000, 19.000],  loss: 0.017927, mae: 0.453057, mean_q: 0.410406, mean_eps: 0.000000
 2468/5000: episode: 97, duration: 0.161s, episode steps:  13, steps per second:  81, episode reward: 47.296, mean reward:  3.638 [-0.062, 32.590], mean action: 4.231 [0.000, 14.000],  loss: 0.018570, mae: 0.460581, mean_q: 0.452884, mean_eps: 0.000000
 2485/5000: episode: 98, duration: 0.201s, episode steps:  17, steps per second:  85, episode reward: 41.565, mean reward:  2.445 [-2.135, 32.731], mean action: 5.824 [1.000, 20.000],  loss: 0.020965, mae: 0.476707, mean_q: 0.459709, mean_eps: 0.000000
 2509/5000: episode: 99, duration: 0.283s, episode steps:  24, steps per second:  85, episode reward: 41.606, mean reward:  1.734 [-3.000, 32.004], mean action: 3.333 [0.000, 19.000],  loss: 0.023795, mae: 0.490122, mean_q: 0.433507, mean_eps: 0.000000
 2530/5000: episode: 100, duration: 0.238s, episode steps:  21, steps per second:  88, episode reward: 39.000, mean reward:  1.857 [-2.438, 32.060], mean action: 3.524 [0.000, 15.000],  loss: 0.021275, mae: 0.474174, mean_q: 0.459357, mean_eps: 0.000000
 2575/5000: episode: 101, duration: 0.501s, episode steps:  45, steps per second:  90, episode reward: 43.982, mean reward:  0.977 [-2.184, 32.420], mean action: 2.644 [0.000, 13.000],  loss: 0.020576, mae: 0.470290, mean_q: 0.466464, mean_eps: 0.000000
 2604/5000: episode: 102, duration: 0.328s, episode steps:  29, steps per second:  88, episode reward: 38.245, mean reward:  1.319 [-2.703, 32.440], mean action: 3.345 [0.000, 19.000],  loss: 0.020457, mae: 0.467251, mean_q: 0.453870, mean_eps: 0.000000
 2633/5000: episode: 103, duration: 0.327s, episode steps:  29, steps per second:  89, episode reward: 41.907, mean reward:  1.445 [-2.207, 32.400], mean action: 3.552 [1.000, 13.000],  loss: 0.018458, mae: 0.463287, mean_q: 0.442396, mean_eps: 0.000000
 2667/5000: episode: 104, duration: 0.376s, episode steps:  34, steps per second:  90, episode reward: 41.253, mean reward:  1.213 [-2.427, 31.934], mean action: 5.206 [0.000, 15.000],  loss: 0.022214, mae: 0.488098, mean_q: 0.434756, mean_eps: 0.000000
 2688/5000: episode: 105, duration: 0.243s, episode steps:  21, steps per second:  86, episode reward: 41.361, mean reward:  1.970 [-2.442, 32.013], mean action: 3.952 [0.000, 14.000],  loss: 0.017540, mae: 0.454319, mean_q: 0.486134, mean_eps: 0.000000
 2705/5000: episode: 106, duration: 0.204s, episode steps:  17, steps per second:  83, episode reward: 44.621, mean reward:  2.625 [-2.271, 32.110], mean action: 4.294 [0.000, 14.000],  loss: 0.021304, mae: 0.468824, mean_q: 0.442641, mean_eps: 0.000000
 2739/5000: episode: 107, duration: 0.455s, episode steps:  34, steps per second:  75, episode reward: 32.264, mean reward:  0.949 [-3.000, 32.340], mean action: 8.559 [1.000, 19.000],  loss: 0.018914, mae: 0.462814, mean_q: 0.388580, mean_eps: 0.000000
 2786/5000: episode: 108, duration: 0.519s, episode steps:  47, steps per second:  91, episode reward: -32.540, mean reward: -0.692 [-32.049,  2.352], mean action: 3.979 [0.000, 19.000],  loss: 0.017120, mae: 0.438864, mean_q: 0.430106, mean_eps: 0.000000
 2819/5000: episode: 109, duration: 0.371s, episode steps:  33, steps per second:  89, episode reward: 35.053, mean reward:  1.062 [-3.000, 31.879], mean action: 5.667 [0.000, 20.000],  loss: 0.019416, mae: 0.443378, mean_q: 0.453750, mean_eps: 0.000000
 2840/5000: episode: 110, duration: 0.249s, episode steps:  21, steps per second:  84, episode reward: 44.801, mean reward:  2.133 [-2.076, 33.000], mean action: 4.429 [0.000, 14.000],  loss: 0.018719, mae: 0.439503, mean_q: 0.446816, mean_eps: 0.000000
 2869/5000: episode: 111, duration: 0.319s, episode steps:  29, steps per second:  91, episode reward: 41.781, mean reward:  1.441 [-2.402, 32.010], mean action: 2.621 [0.000, 11.000],  loss: 0.021102, mae: 0.468739, mean_q: 0.378613, mean_eps: 0.000000
 2932/5000: episode: 112, duration: 0.793s, episode steps:  63, steps per second:  79, episode reward: -32.090, mean reward: -0.509 [-32.239,  2.620], mean action: 6.349 [0.000, 20.000],  loss: 0.019353, mae: 0.449612, mean_q: 0.462084, mean_eps: 0.000000
 2958/5000: episode: 113, duration: 0.300s, episode steps:  26, steps per second:  87, episode reward: 41.697, mean reward:  1.604 [-2.880, 32.040], mean action: 5.269 [3.000, 15.000],  loss: 0.018057, mae: 0.437569, mean_q: 0.446554, mean_eps: 0.000000
 2975/5000: episode: 114, duration: 0.198s, episode steps:  17, steps per second:  86, episode reward: 44.287, mean reward:  2.605 [-2.367, 32.285], mean action: 1.000 [0.000, 9.000],  loss: 0.021266, mae: 0.457209, mean_q: 0.451090, mean_eps: 0.000000
 3005/5000: episode: 115, duration: 0.345s, episode steps:  30, steps per second:  87, episode reward: 41.671, mean reward:  1.389 [-2.441, 34.660], mean action: 3.800 [0.000, 15.000],  loss: 0.018995, mae: 0.452536, mean_q: 0.427566, mean_eps: 0.000000
 3031/5000: episode: 116, duration: 0.296s, episode steps:  26, steps per second:  88, episode reward: 35.065, mean reward:  1.349 [-2.821, 32.020], mean action: 6.808 [0.000, 15.000],  loss: 0.018520, mae: 0.462516, mean_q: 0.372541, mean_eps: 0.000000
 3056/5000: episode: 117, duration: 0.283s, episode steps:  25, steps per second:  88, episode reward: 38.710, mean reward:  1.548 [-2.579, 32.020], mean action: 4.600 [0.000, 14.000],  loss: 0.022220, mae: 0.466162, mean_q: 0.443668, mean_eps: 0.000000
 3082/5000: episode: 118, duration: 0.284s, episode steps:  26, steps per second:  92, episode reward: -37.820, mean reward: -1.455 [-32.134,  2.711], mean action: 7.308 [1.000, 15.000],  loss: 0.020118, mae: 0.447371, mean_q: 0.428575, mean_eps: 0.000000
 3115/5000: episode: 119, duration: 0.378s, episode steps:  33, steps per second:  87, episode reward: 43.905, mean reward:  1.330 [-2.059, 31.995], mean action: 2.333 [0.000, 13.000],  loss: 0.016893, mae: 0.434911, mean_q: 0.449289, mean_eps: 0.000000
 3138/5000: episode: 120, duration: 0.258s, episode steps:  23, steps per second:  89, episode reward: 37.645, mean reward:  1.637 [-3.000, 32.470], mean action: 4.609 [1.000, 12.000],  loss: 0.019003, mae: 0.446653, mean_q: 0.411598, mean_eps: 0.000000
 3157/5000: episode: 121, duration: 0.218s, episode steps:  19, steps per second:  87, episode reward: 47.623, mean reward:  2.506 [-0.204, 32.080], mean action: 5.947 [1.000, 20.000],  loss: 0.024549, mae: 0.481168, mean_q: 0.354429, mean_eps: 0.000000
 3173/5000: episode: 122, duration: 0.188s, episode steps:  16, steps per second:  85, episode reward: 47.555, mean reward:  2.972 [-0.550, 33.000], mean action: 3.938 [0.000, 14.000],  loss: 0.016706, mae: 0.442029, mean_q: 0.393011, mean_eps: 0.000000
 3191/5000: episode: 123, duration: 0.209s, episode steps:  18, steps per second:  86, episode reward: 45.000, mean reward:  2.500 [-2.035, 32.210], mean action: 3.222 [0.000, 14.000],  loss: 0.021578, mae: 0.468642, mean_q: 0.466625, mean_eps: 0.000000
 3214/5000: episode: 124, duration: 0.270s, episode steps:  23, steps per second:  85, episode reward: 38.268, mean reward:  1.664 [-2.606, 32.448], mean action: 2.130 [0.000, 9.000],  loss: 0.017987, mae: 0.454480, mean_q: 0.440081, mean_eps: 0.000000
 3240/5000: episode: 125, duration: 0.296s, episode steps:  26, steps per second:  88, episode reward: 40.771, mean reward:  1.568 [-2.488, 31.753], mean action: 5.846 [0.000, 15.000],  loss: 0.020664, mae: 0.451013, mean_q: 0.476671, mean_eps: 0.000000
 3277/5000: episode: 126, duration: 0.474s, episode steps:  37, steps per second:  78, episode reward: -33.000, mean reward: -0.892 [-32.156,  3.000], mean action: 6.784 [0.000, 21.000],  loss: 0.019980, mae: 0.444801, mean_q: 0.454701, mean_eps: 0.000000
 3316/5000: episode: 127, duration: 0.431s, episode steps:  39, steps per second:  90, episode reward: 35.512, mean reward:  0.911 [-2.216, 32.062], mean action: 3.333 [0.000, 20.000],  loss: 0.023632, mae: 0.479649, mean_q: 0.421111, mean_eps: 0.000000
 3339/5000: episode: 128, duration: 0.263s, episode steps:  23, steps per second:  87, episode reward: 40.898, mean reward:  1.778 [-2.542, 32.192], mean action: 2.826 [0.000, 14.000],  loss: 0.021165, mae: 0.474617, mean_q: 0.377764, mean_eps: 0.000000
 3353/5000: episode: 129, duration: 0.172s, episode steps:  14, steps per second:  82, episode reward: 44.256, mean reward:  3.161 [-2.794, 31.349], mean action: 2.714 [1.000, 9.000],  loss: 0.020168, mae: 0.466274, mean_q: 0.421908, mean_eps: 0.000000
 3375/5000: episode: 130, duration: 0.250s, episode steps:  22, steps per second:  88, episode reward: 39.000, mean reward:  1.773 [-2.328, 32.140], mean action: 3.000 [0.000, 14.000],  loss: 0.018245, mae: 0.451020, mean_q: 0.443144, mean_eps: 0.000000
 3396/5000: episode: 131, duration: 0.245s, episode steps:  21, steps per second:  86, episode reward: 43.553, mean reward:  2.074 [-2.156, 32.480], mean action: 2.762 [0.000, 14.000],  loss: 0.016848, mae: 0.444964, mean_q: 0.429149, mean_eps: 0.000000
 3447/5000: episode: 132, duration: 0.580s, episode steps:  51, steps per second:  88, episode reward: 41.274, mean reward:  0.809 [-2.017, 32.790], mean action: 2.157 [0.000, 14.000],  loss: 0.020583, mae: 0.468693, mean_q: 0.418093, mean_eps: 0.000000
 3473/5000: episode: 133, duration: 0.298s, episode steps:  26, steps per second:  87, episode reward: 44.529, mean reward:  1.713 [-2.084, 32.020], mean action: 2.923 [0.000, 14.000],  loss: 0.021764, mae: 0.477363, mean_q: 0.447071, mean_eps: 0.000000
 3497/5000: episode: 134, duration: 0.276s, episode steps:  24, steps per second:  87, episode reward: 44.492, mean reward:  1.854 [-2.251, 32.580], mean action: 1.542 [0.000, 14.000],  loss: 0.018940, mae: 0.441929, mean_q: 0.515609, mean_eps: 0.000000
 3511/5000: episode: 135, duration: 0.169s, episode steps:  14, steps per second:  83, episode reward: 44.424, mean reward:  3.173 [-2.291, 33.000], mean action: 2.214 [0.000, 9.000],  loss: 0.018412, mae: 0.459135, mean_q: 0.512517, mean_eps: 0.000000
 3542/5000: episode: 136, duration: 0.352s, episode steps:  31, steps per second:  88, episode reward: 32.047, mean reward:  1.034 [-3.000, 31.850], mean action: 4.226 [0.000, 19.000],  loss: 0.019502, mae: 0.462311, mean_q: 0.453706, mean_eps: 0.000000
 3573/5000: episode: 137, duration: 0.340s, episode steps:  31, steps per second:  91, episode reward: 41.373, mean reward:  1.335 [-3.000, 32.180], mean action: 4.419 [0.000, 20.000],  loss: 0.018703, mae: 0.457835, mean_q: 0.474848, mean_eps: 0.000000
 3604/5000: episode: 138, duration: 0.345s, episode steps:  31, steps per second:  90, episode reward: 35.283, mean reward:  1.138 [-2.498, 32.170], mean action: 3.871 [0.000, 15.000],  loss: 0.017102, mae: 0.444765, mean_q: 0.508100, mean_eps: 0.000000
 3656/5000: episode: 139, duration: 0.580s, episode steps:  52, steps per second:  90, episode reward: 41.279, mean reward:  0.794 [-2.263, 32.280], mean action: 3.346 [0.000, 14.000],  loss: 0.018829, mae: 0.451543, mean_q: 0.469946, mean_eps: 0.000000
 3695/5000: episode: 140, duration: 0.439s, episode steps:  39, steps per second:  89, episode reward: 35.334, mean reward:  0.906 [-2.272, 32.290], mean action: 5.641 [0.000, 15.000],  loss: 0.021152, mae: 0.459915, mean_q: 0.495629, mean_eps: 0.000000
 3727/5000: episode: 141, duration: 0.378s, episode steps:  32, steps per second:  85, episode reward: 41.767, mean reward:  1.305 [-2.382, 32.153], mean action: 3.562 [0.000, 15.000],  loss: 0.018154, mae: 0.454980, mean_q: 0.433751, mean_eps: 0.000000
 3760/5000: episode: 142, duration: 0.367s, episode steps:  33, steps per second:  90, episode reward: 35.840, mean reward:  1.086 [-3.000, 31.980], mean action: 3.879 [0.000, 21.000],  loss: 0.016366, mae: 0.441714, mean_q: 0.464032, mean_eps: 0.000000
 3817/5000: episode: 143, duration: 0.625s, episode steps:  57, steps per second:  91, episode reward: -33.000, mean reward: -0.579 [-32.127,  2.410], mean action: 5.789 [0.000, 16.000],  loss: 0.022262, mae: 0.481556, mean_q: 0.462019, mean_eps: 0.000000
 3832/5000: episode: 144, duration: 0.177s, episode steps:  15, steps per second:  85, episode reward: 44.627, mean reward:  2.975 [-2.063, 32.327], mean action: 2.933 [1.000, 12.000],  loss: 0.016600, mae: 0.440799, mean_q: 0.481104, mean_eps: 0.000000
 3850/5000: episode: 145, duration: 0.233s, episode steps:  18, steps per second:  77, episode reward: 44.158, mean reward:  2.453 [-2.401, 32.180], mean action: 2.278 [0.000, 12.000],  loss: 0.020567, mae: 0.458414, mean_q: 0.454578, mean_eps: 0.000000
 3879/5000: episode: 146, duration: 0.346s, episode steps:  29, steps per second:  84, episode reward: 42.000, mean reward:  1.448 [-3.000, 32.190], mean action: 2.621 [0.000, 12.000],  loss: 0.016888, mae: 0.442765, mean_q: 0.500154, mean_eps: 0.000000
 3920/5000: episode: 147, duration: 0.461s, episode steps:  41, steps per second:  89, episode reward: 41.910, mean reward:  1.022 [-2.456, 32.240], mean action: 4.244 [0.000, 14.000],  loss: 0.020969, mae: 0.466703, mean_q: 0.501180, mean_eps: 0.000000
 3958/5000: episode: 148, duration: 0.426s, episode steps:  38, steps per second:  89, episode reward: 38.470, mean reward:  1.012 [-2.647, 32.490], mean action: 3.842 [0.000, 19.000],  loss: 0.021315, mae: 0.466675, mean_q: 0.479907, mean_eps: 0.000000
 4013/5000: episode: 149, duration: 0.605s, episode steps:  55, steps per second:  91, episode reward: -32.430, mean reward: -0.590 [-32.270,  3.000], mean action: 8.236 [0.000, 17.000],  loss: 0.020278, mae: 0.459687, mean_q: 0.472130, mean_eps: 0.000000
 4049/5000: episode: 150, duration: 0.518s, episode steps:  36, steps per second:  69, episode reward: 32.430, mean reward:  0.901 [-2.489, 32.020], mean action: 6.528 [0.000, 18.000],  loss: 0.019457, mae: 0.447577, mean_q: 0.487312, mean_eps: 0.000000
 4071/5000: episode: 151, duration: 0.256s, episode steps:  22, steps per second:  86, episode reward: 40.441, mean reward:  1.838 [-3.000, 32.029], mean action: 2.500 [0.000, 15.000],  loss: 0.016526, mae: 0.449938, mean_q: 0.439456, mean_eps: 0.000000
 4091/5000: episode: 152, duration: 0.235s, episode steps:  20, steps per second:  85, episode reward: 44.733, mean reward:  2.237 [-2.597, 32.640], mean action: 7.950 [0.000, 14.000],  loss: 0.019697, mae: 0.457688, mean_q: 0.533113, mean_eps: 0.000000
 4126/5000: episode: 153, duration: 0.420s, episode steps:  35, steps per second:  83, episode reward: 34.803, mean reward:  0.994 [-3.000, 31.970], mean action: 4.000 [0.000, 15.000],  loss: 0.021370, mae: 0.468825, mean_q: 0.515627, mean_eps: 0.000000
 4154/5000: episode: 154, duration: 0.315s, episode steps:  28, steps per second:  89, episode reward: 41.115, mean reward:  1.468 [-2.466, 32.468], mean action: 3.393 [0.000, 15.000],  loss: 0.020785, mae: 0.478036, mean_q: 0.440020, mean_eps: 0.000000
 4170/5000: episode: 155, duration: 0.186s, episode steps:  16, steps per second:  86, episode reward: 41.611, mean reward:  2.601 [-2.697, 31.821], mean action: 3.625 [0.000, 11.000],  loss: 0.016498, mae: 0.457443, mean_q: 0.431312, mean_eps: 0.000000
 4198/5000: episode: 156, duration: 0.317s, episode steps:  28, steps per second:  88, episode reward: 38.783, mean reward:  1.385 [-2.506, 32.170], mean action: 3.179 [0.000, 15.000],  loss: 0.024388, mae: 0.486915, mean_q: 0.465542, mean_eps: 0.000000
 4222/5000: episode: 157, duration: 0.281s, episode steps:  24, steps per second:  86, episode reward: 44.224, mean reward:  1.843 [-2.354, 32.470], mean action: 2.000 [0.000, 12.000],  loss: 0.019116, mae: 0.458115, mean_q: 0.505705, mean_eps: 0.000000
 4243/5000: episode: 158, duration: 0.243s, episode steps:  21, steps per second:  86, episode reward: 44.102, mean reward:  2.100 [-2.107, 33.000], mean action: 2.476 [0.000, 12.000],  loss: 0.018175, mae: 0.450035, mean_q: 0.489476, mean_eps: 0.000000
 4271/5000: episode: 159, duration: 0.315s, episode steps:  28, steps per second:  89, episode reward: 41.821, mean reward:  1.494 [-2.042, 31.841], mean action: 4.500 [1.000, 18.000],  loss: 0.017906, mae: 0.452432, mean_q: 0.494897, mean_eps: 0.000000
 4301/5000: episode: 160, duration: 0.343s, episode steps:  30, steps per second:  87, episode reward: 41.138, mean reward:  1.371 [-2.250, 32.220], mean action: 2.200 [0.000, 9.000],  loss: 0.017962, mae: 0.448661, mean_q: 0.504784, mean_eps: 0.000000
 4324/5000: episode: 161, duration: 0.261s, episode steps:  23, steps per second:  88, episode reward: 35.627, mean reward:  1.549 [-2.866, 31.787], mean action: 5.130 [1.000, 14.000],  loss: 0.019881, mae: 0.456826, mean_q: 0.473294, mean_eps: 0.000000
 4336/5000: episode: 162, duration: 0.151s, episode steps:  12, steps per second:  80, episode reward: 47.622, mean reward:  3.968 [ 0.370, 32.240], mean action: 2.833 [0.000, 14.000],  loss: 0.015349, mae: 0.432847, mean_q: 0.439301, mean_eps: 0.000000
 4364/5000: episode: 163, duration: 0.318s, episode steps:  28, steps per second:  88, episode reward: 41.956, mean reward:  1.498 [-2.968, 32.180], mean action: 2.536 [0.000, 14.000],  loss: 0.022040, mae: 0.460504, mean_q: 0.473625, mean_eps: 0.000000
 4388/5000: episode: 164, duration: 0.275s, episode steps:  24, steps per second:  87, episode reward: 38.630, mean reward:  1.610 [-2.552, 32.310], mean action: 3.500 [0.000, 20.000],  loss: 0.021981, mae: 0.454710, mean_q: 0.508131, mean_eps: 0.000000
 4421/5000: episode: 165, duration: 0.369s, episode steps:  33, steps per second:  90, episode reward: 37.952, mean reward:  1.150 [-2.831, 31.973], mean action: 4.242 [0.000, 18.000],  loss: 0.017517, mae: 0.438753, mean_q: 0.486202, mean_eps: 0.000000
 4455/5000: episode: 166, duration: 0.379s, episode steps:  34, steps per second:  90, episode reward: 34.477, mean reward:  1.014 [-2.548, 32.330], mean action: 3.235 [0.000, 14.000],  loss: 0.018305, mae: 0.449912, mean_q: 0.491695, mean_eps: 0.000000
 4493/5000: episode: 167, duration: 0.426s, episode steps:  38, steps per second:  89, episode reward: 41.738, mean reward:  1.098 [-2.635, 32.170], mean action: 2.974 [0.000, 14.000],  loss: 0.019449, mae: 0.456156, mean_q: 0.491206, mean_eps: 0.000000
 4514/5000: episode: 168, duration: 0.244s, episode steps:  21, steps per second:  86, episode reward: 47.138, mean reward:  2.245 [-0.230, 32.510], mean action: 0.667 [0.000, 3.000],  loss: 0.020436, mae: 0.461745, mean_q: 0.486713, mean_eps: 0.000000
 4543/5000: episode: 169, duration: 0.324s, episode steps:  29, steps per second:  89, episode reward: 35.608, mean reward:  1.228 [-3.000, 31.978], mean action: 4.103 [0.000, 14.000],  loss: 0.020431, mae: 0.460675, mean_q: 0.532384, mean_eps: 0.000000
 4568/5000: episode: 170, duration: 0.286s, episode steps:  25, steps per second:  87, episode reward: 44.202, mean reward:  1.768 [-2.250, 32.004], mean action: 3.880 [1.000, 14.000],  loss: 0.017779, mae: 0.463976, mean_q: 0.472115, mean_eps: 0.000000
 4595/5000: episode: 171, duration: 0.303s, episode steps:  27, steps per second:  89, episode reward: 40.560, mean reward:  1.502 [-2.263, 32.069], mean action: 5.852 [0.000, 18.000],  loss: 0.019914, mae: 0.459063, mean_q: 0.477500, mean_eps: 0.000000
 4624/5000: episode: 172, duration: 0.332s, episode steps:  29, steps per second:  87, episode reward: 38.204, mean reward:  1.317 [-2.438, 32.130], mean action: 3.621 [0.000, 19.000],  loss: 0.019240, mae: 0.444619, mean_q: 0.475004, mean_eps: 0.000000
 4648/5000: episode: 173, duration: 0.271s, episode steps:  24, steps per second:  89, episode reward: 41.931, mean reward:  1.747 [-2.643, 32.471], mean action: 2.542 [1.000, 12.000],  loss: 0.017543, mae: 0.431097, mean_q: 0.484216, mean_eps: 0.000000
 4676/5000: episode: 174, duration: 0.323s, episode steps:  28, steps per second:  87, episode reward: 38.259, mean reward:  1.366 [-2.312, 31.799], mean action: 3.786 [1.000, 9.000],  loss: 0.018674, mae: 0.442586, mean_q: 0.500058, mean_eps: 0.000000
 4709/5000: episode: 175, duration: 0.368s, episode steps:  33, steps per second:  90, episode reward: 39.938, mean reward:  1.210 [-2.288, 31.823], mean action: 4.515 [0.000, 14.000],  loss: 0.021838, mae: 0.461683, mean_q: 0.483435, mean_eps: 0.000000
 4731/5000: episode: 176, duration: 0.253s, episode steps:  22, steps per second:  87, episode reward: 41.204, mean reward:  1.873 [-2.415, 32.540], mean action: 3.182 [0.000, 16.000],  loss: 0.020571, mae: 0.460414, mean_q: 0.440964, mean_eps: 0.000000
 4764/5000: episode: 177, duration: 0.371s, episode steps:  33, steps per second:  89, episode reward: 41.171, mean reward:  1.248 [-2.270, 32.090], mean action: 1.212 [0.000, 9.000],  loss: 0.018249, mae: 0.459094, mean_q: 0.447946, mean_eps: 0.000000
 4793/5000: episode: 178, duration: 0.321s, episode steps:  29, steps per second:  90, episode reward: -32.440, mean reward: -1.119 [-33.000,  2.430], mean action: 5.448 [0.000, 15.000],  loss: 0.018453, mae: 0.453161, mean_q: 0.483285, mean_eps: 0.000000
 4832/5000: episode: 179, duration: 0.425s, episode steps:  39, steps per second:  92, episode reward: 34.384, mean reward:  0.882 [-2.678, 32.140], mean action: 4.667 [0.000, 14.000],  loss: 0.022135, mae: 0.475430, mean_q: 0.467315, mean_eps: 0.000000
 4875/5000: episode: 180, duration: 0.489s, episode steps:  43, steps per second:  88, episode reward: 41.513, mean reward:  0.965 [-2.917, 32.190], mean action: 1.721 [0.000, 20.000],  loss: 0.022253, mae: 0.468507, mean_q: 0.463589, mean_eps: 0.000000
 4919/5000: episode: 181, duration: 0.806s, episode steps:  44, steps per second:  55, episode reward: -32.490, mean reward: -0.738 [-31.697,  2.440], mean action: 7.614 [0.000, 20.000],  loss: 0.020660, mae: 0.473799, mean_q: 0.427936, mean_eps: 0.000000
 4937/5000: episode: 182, duration: 0.215s, episode steps:  18, steps per second:  84, episode reward: 47.939, mean reward:  2.663 [-0.180, 32.170], mean action: 1.000 [1.000, 1.000],  loss: 0.018946, mae: 0.452477, mean_q: 0.516768, mean_eps: 0.000000
 4965/5000: episode: 183, duration: 0.312s, episode steps:  28, steps per second:  90, episode reward: 33.000, mean reward:  1.179 [-3.000, 32.310], mean action: 4.036 [0.000, 15.000],  loss: 0.017692, mae: 0.453675, mean_q: 0.447120, mean_eps: 0.000000
 4980/5000: episode: 184, duration: 0.190s, episode steps:  15, steps per second:  79, episode reward: 44.524, mean reward:  2.968 [-2.152, 32.073], mean action: 4.067 [0.000, 14.000],  loss: 0.023985, mae: 0.475635, mean_q: 0.456820, mean_eps: 0.000000
done, took 53.162 seconds
DQN Evaluation: 1386 victories out of 1651 episodes
Training for 5000 steps ...
   19/5000: episode: 1, duration: 0.143s, episode steps:  19, steps per second: 133, episode reward: 41.033, mean reward:  2.160 [-2.283, 31.862], mean action: 1.684 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   39/5000: episode: 2, duration: 0.131s, episode steps:  20, steps per second: 152, episode reward: -41.210, mean reward: -2.060 [-32.064,  2.228], mean action: 5.350 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   80/5000: episode: 3, duration: 0.252s, episode steps:  41, steps per second: 163, episode reward: 35.296, mean reward:  0.861 [-2.719, 31.866], mean action: 1.927 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  106/5000: episode: 4, duration: 0.165s, episode steps:  26, steps per second: 158, episode reward: -35.570, mean reward: -1.368 [-32.568,  2.470], mean action: 5.231 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  126/5000: episode: 5, duration: 0.133s, episode steps:  20, steps per second: 150, episode reward: 35.218, mean reward:  1.761 [-2.938, 32.621], mean action: 4.150 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  149/5000: episode: 6, duration: 0.144s, episode steps:  23, steps per second: 159, episode reward: -38.970, mean reward: -1.694 [-32.176,  2.193], mean action: 8.783 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  175/5000: episode: 7, duration: 0.165s, episode steps:  26, steps per second: 158, episode reward: -32.810, mean reward: -1.262 [-32.157,  2.600], mean action: 6.462 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  194/5000: episode: 8, duration: 0.125s, episode steps:  19, steps per second: 151, episode reward: 37.291, mean reward:  1.963 [-2.752, 32.410], mean action: 4.053 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  218/5000: episode: 9, duration: 0.159s, episode steps:  24, steps per second: 151, episode reward: 32.416, mean reward:  1.351 [-3.000, 33.000], mean action: 3.375 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  265/5000: episode: 10, duration: 0.297s, episode steps:  47, steps per second: 158, episode reward: 40.667, mean reward:  0.865 [-2.457, 32.820], mean action: 5.340 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  288/5000: episode: 11, duration: 0.150s, episode steps:  23, steps per second: 153, episode reward: 32.186, mean reward:  1.399 [-3.000, 32.606], mean action: 4.130 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  313/5000: episode: 12, duration: 0.155s, episode steps:  25, steps per second: 162, episode reward: -38.620, mean reward: -1.545 [-32.060,  2.650], mean action: 6.480 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  343/5000: episode: 13, duration: 0.196s, episode steps:  30, steps per second: 153, episode reward: 33.000, mean reward:  1.100 [-3.000, 32.190], mean action: 3.767 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  366/5000: episode: 14, duration: 0.150s, episode steps:  23, steps per second: 154, episode reward: 32.424, mean reward:  1.410 [-3.000, 32.065], mean action: 8.304 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  385/5000: episode: 15, duration: 0.126s, episode steps:  19, steps per second: 151, episode reward: 38.659, mean reward:  2.035 [-2.496, 32.330], mean action: 5.421 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  406/5000: episode: 16, duration: 0.139s, episode steps:  21, steps per second: 151, episode reward: 33.000, mean reward:  1.571 [-3.000, 30.462], mean action: 3.810 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  433/5000: episode: 17, duration: 0.169s, episode steps:  27, steps per second: 160, episode reward: -32.260, mean reward: -1.195 [-31.695,  2.881], mean action: 2.963 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  465/5000: episode: 18, duration: 0.233s, episode steps:  32, steps per second: 137, episode reward: -32.280, mean reward: -1.009 [-32.227,  2.356], mean action: 5.812 [1.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  490/5000: episode: 19, duration: 0.176s, episode steps:  25, steps per second: 142, episode reward: -35.810, mean reward: -1.432 [-31.886,  2.261], mean action: 4.640 [0.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  519/5000: episode: 20, duration: 0.579s, episode steps:  29, steps per second:  50, episode reward: 35.374, mean reward:  1.220 [-2.373, 30.171], mean action: 3.310 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  546/5000: episode: 21, duration: 0.337s, episode steps:  27, steps per second:  80, episode reward: 37.487, mean reward:  1.388 [-3.000, 32.344], mean action: 6.185 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  596/5000: episode: 22, duration: 0.414s, episode steps:  50, steps per second: 121, episode reward: 41.154, mean reward:  0.823 [-2.168, 32.150], mean action: 1.640 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  622/5000: episode: 23, duration: 0.276s, episode steps:  26, steps per second:  94, episode reward: -32.330, mean reward: -1.243 [-32.240,  2.580], mean action: 5.038 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  640/5000: episode: 24, duration: 0.134s, episode steps:  18, steps per second: 134, episode reward: 38.562, mean reward:  2.142 [-2.424, 32.562], mean action: 3.833 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  658/5000: episode: 25, duration: 0.146s, episode steps:  18, steps per second: 123, episode reward: 41.102, mean reward:  2.283 [-2.478, 32.220], mean action: 2.333 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  680/5000: episode: 26, duration: 0.143s, episode steps:  22, steps per second: 154, episode reward: 39.000, mean reward:  1.773 [-2.339, 32.140], mean action: 2.909 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  696/5000: episode: 27, duration: 0.269s, episode steps:  16, steps per second:  59, episode reward: 40.829, mean reward:  2.552 [-2.862, 32.064], mean action: 4.375 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  728/5000: episode: 28, duration: 0.411s, episode steps:  32, steps per second:  78, episode reward: -32.910, mean reward: -1.028 [-31.953,  2.320], mean action: 5.438 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  750/5000: episode: 29, duration: 0.265s, episode steps:  22, steps per second:  83, episode reward: -32.910, mean reward: -1.496 [-32.077,  2.194], mean action: 4.182 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  775/5000: episode: 30, duration: 0.169s, episode steps:  25, steps per second: 147, episode reward: 34.454, mean reward:  1.378 [-3.000, 31.989], mean action: 4.280 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  801/5000: episode: 31, duration: 0.199s, episode steps:  26, steps per second: 130, episode reward: 38.407, mean reward:  1.477 [-2.455, 32.240], mean action: 4.038 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  824/5000: episode: 32, duration: 0.189s, episode steps:  23, steps per second: 122, episode reward: 36.000, mean reward:  1.565 [-2.244, 32.390], mean action: 3.783 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  847/5000: episode: 33, duration: 0.154s, episode steps:  23, steps per second: 149, episode reward: 32.856, mean reward:  1.429 [-2.894, 32.320], mean action: 4.304 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  883/5000: episode: 34, duration: 0.210s, episode steps:  36, steps per second: 171, episode reward: 38.641, mean reward:  1.073 [-2.415, 32.510], mean action: 2.361 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  902/5000: episode: 35, duration: 0.145s, episode steps:  19, steps per second: 131, episode reward: 37.274, mean reward:  1.962 [-2.544, 32.036], mean action: 6.368 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  922/5000: episode: 36, duration: 0.131s, episode steps:  20, steps per second: 153, episode reward: 35.446, mean reward:  1.772 [-2.462, 32.250], mean action: 3.500 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  943/5000: episode: 37, duration: 0.134s, episode steps:  21, steps per second: 157, episode reward: 35.901, mean reward:  1.710 [-2.495, 32.511], mean action: 3.714 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  967/5000: episode: 38, duration: 0.146s, episode steps:  24, steps per second: 165, episode reward: -38.320, mean reward: -1.597 [-33.000,  2.251], mean action: 6.375 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  986/5000: episode: 39, duration: 0.126s, episode steps:  19, steps per second: 150, episode reward: 39.000, mean reward:  2.053 [-2.216, 32.080], mean action: 4.000 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1027/5000: episode: 40, duration: 0.426s, episode steps:  41, steps per second:  96, episode reward: 33.000, mean reward:  0.805 [-2.601, 32.010], mean action: 7.927 [0.000, 21.000],  loss: 0.022010, mae: 0.472672, mean_q: 0.418143, mean_eps: 0.000000
 1052/5000: episode: 41, duration: 0.292s, episode steps:  25, steps per second:  86, episode reward: 32.398, mean reward:  1.296 [-3.000, 32.060], mean action: 6.240 [0.000, 17.000],  loss: 0.018902, mae: 0.462822, mean_q: 0.387124, mean_eps: 0.000000
 1068/5000: episode: 42, duration: 0.192s, episode steps:  16, steps per second:  83, episode reward: 35.351, mean reward:  2.209 [-3.000, 32.351], mean action: 5.375 [0.000, 16.000],  loss: 0.021131, mae: 0.463944, mean_q: 0.464841, mean_eps: 0.000000
 1090/5000: episode: 43, duration: 0.257s, episode steps:  22, steps per second:  86, episode reward: 38.139, mean reward:  1.734 [-2.803, 31.332], mean action: 5.318 [0.000, 14.000],  loss: 0.021605, mae: 0.460244, mean_q: 0.464242, mean_eps: 0.000000
 1121/5000: episode: 44, duration: 0.355s, episode steps:  31, steps per second:  87, episode reward: 37.989, mean reward:  1.225 [-2.317, 32.330], mean action: 4.258 [0.000, 14.000],  loss: 0.019742, mae: 0.457034, mean_q: 0.478019, mean_eps: 0.000000
 1141/5000: episode: 45, duration: 0.247s, episode steps:  20, steps per second:  81, episode reward: -38.600, mean reward: -1.930 [-32.221,  2.719], mean action: 6.550 [0.000, 15.000],  loss: 0.020956, mae: 0.466987, mean_q: 0.509962, mean_eps: 0.000000
 1155/5000: episode: 46, duration: 0.218s, episode steps:  14, steps per second:  64, episode reward: 42.000, mean reward:  3.000 [-2.610, 33.000], mean action: 4.929 [0.000, 15.000],  loss: 0.019719, mae: 0.458526, mean_q: 0.450717, mean_eps: 0.000000
 1173/5000: episode: 47, duration: 0.601s, episode steps:  18, steps per second:  30, episode reward: 38.918, mean reward:  2.162 [-2.578, 32.918], mean action: 5.222 [0.000, 19.000],  loss: 0.017140, mae: 0.444975, mean_q: 0.436809, mean_eps: 0.000000
 1200/5000: episode: 48, duration: 0.401s, episode steps:  27, steps per second:  67, episode reward: 41.014, mean reward:  1.519 [-2.489, 32.895], mean action: 4.407 [0.000, 15.000],  loss: 0.017690, mae: 0.450764, mean_q: 0.396432, mean_eps: 0.000000
 1225/5000: episode: 49, duration: 0.460s, episode steps:  25, steps per second:  54, episode reward: 43.366, mean reward:  1.735 [-2.086, 32.295], mean action: 5.360 [0.000, 14.000],  loss: 0.021750, mae: 0.467341, mean_q: 0.421964, mean_eps: 0.000000
 1242/5000: episode: 50, duration: 0.373s, episode steps:  17, steps per second:  46, episode reward: 34.779, mean reward:  2.046 [-2.998, 31.825], mean action: 6.176 [0.000, 15.000],  loss: 0.018971, mae: 0.446129, mean_q: 0.451898, mean_eps: 0.000000
 1255/5000: episode: 51, duration: 0.183s, episode steps:  13, steps per second:  71, episode reward: 44.305, mean reward:  3.408 [-2.131, 32.540], mean action: 2.154 [0.000, 14.000],  loss: 0.019033, mae: 0.434042, mean_q: 0.520540, mean_eps: 0.000000
 1289/5000: episode: 52, duration: 0.453s, episode steps:  34, steps per second:  75, episode reward: 32.012, mean reward:  0.942 [-2.699, 32.004], mean action: 9.559 [0.000, 21.000],  loss: 0.017885, mae: 0.436509, mean_q: 0.447955, mean_eps: 0.000000
 1315/5000: episode: 53, duration: 0.301s, episode steps:  26, steps per second:  86, episode reward: -32.670, mean reward: -1.257 [-32.150,  3.000], mean action: 5.423 [0.000, 20.000],  loss: 0.018647, mae: 0.447101, mean_q: 0.446480, mean_eps: 0.000000
 1343/5000: episode: 54, duration: 0.580s, episode steps:  28, steps per second:  48, episode reward: -32.620, mean reward: -1.165 [-29.891,  2.230], mean action: 4.464 [0.000, 17.000],  loss: 0.020924, mae: 0.459020, mean_q: 0.423060, mean_eps: 0.000000
 1362/5000: episode: 55, duration: 0.313s, episode steps:  19, steps per second:  61, episode reward: 32.455, mean reward:  1.708 [-2.903, 32.500], mean action: 5.947 [0.000, 17.000],  loss: 0.017016, mae: 0.451620, mean_q: 0.357834, mean_eps: 0.000000
 1378/5000: episode: 56, duration: 0.479s, episode steps:  16, steps per second:  33, episode reward: 41.166, mean reward:  2.573 [-2.900, 32.098], mean action: 1.938 [0.000, 12.000],  loss: 0.019117, mae: 0.446417, mean_q: 0.387909, mean_eps: 0.000000
 1394/5000: episode: 57, duration: 0.201s, episode steps:  16, steps per second:  80, episode reward: 44.457, mean reward:  2.779 [-2.131, 32.260], mean action: 0.750 [0.000, 9.000],  loss: 0.021272, mae: 0.459487, mean_q: 0.457477, mean_eps: 0.000000
 1413/5000: episode: 58, duration: 0.238s, episode steps:  19, steps per second:  80, episode reward: 41.384, mean reward:  2.178 [-2.480, 33.000], mean action: 2.211 [0.000, 15.000],  loss: 0.020090, mae: 0.449268, mean_q: 0.426846, mean_eps: 0.000000
 1434/5000: episode: 59, duration: 0.784s, episode steps:  21, steps per second:  27, episode reward: 35.568, mean reward:  1.694 [-3.000, 33.000], mean action: 6.286 [0.000, 15.000],  loss: 0.018701, mae: 0.454763, mean_q: 0.421341, mean_eps: 0.000000
 1458/5000: episode: 60, duration: 0.279s, episode steps:  24, steps per second:  86, episode reward: 35.341, mean reward:  1.473 [-2.655, 31.701], mean action: 3.083 [0.000, 11.000],  loss: 0.020524, mae: 0.469615, mean_q: 0.422278, mean_eps: 0.000000
 1490/5000: episode: 61, duration: 0.357s, episode steps:  32, steps per second:  90, episode reward: 35.195, mean reward:  1.100 [-3.000, 32.103], mean action: 5.188 [0.000, 14.000],  loss: 0.017490, mae: 0.462980, mean_q: 0.407597, mean_eps: 0.000000
 1511/5000: episode: 62, duration: 0.247s, episode steps:  21, steps per second:  85, episode reward: 32.930, mean reward:  1.568 [-3.000, 32.930], mean action: 4.333 [0.000, 14.000],  loss: 0.019164, mae: 0.460007, mean_q: 0.447585, mean_eps: 0.000000
 1548/5000: episode: 63, duration: 0.408s, episode steps:  37, steps per second:  91, episode reward: 32.383, mean reward:  0.875 [-3.000, 32.330], mean action: 5.919 [0.000, 14.000],  loss: 0.021092, mae: 0.464054, mean_q: 0.463005, mean_eps: 0.000000
 1568/5000: episode: 64, duration: 0.225s, episode steps:  20, steps per second:  89, episode reward: -35.790, mean reward: -1.789 [-32.790,  3.000], mean action: 7.200 [0.000, 21.000],  loss: 0.013329, mae: 0.420428, mean_q: 0.502434, mean_eps: 0.000000
 1583/5000: episode: 65, duration: 0.181s, episode steps:  15, steps per second:  83, episode reward: 44.064, mean reward:  2.938 [-2.081, 32.330], mean action: 2.267 [0.000, 12.000],  loss: 0.025624, mae: 0.479528, mean_q: 0.455718, mean_eps: 0.000000
 1603/5000: episode: 66, duration: 0.235s, episode steps:  20, steps per second:  85, episode reward: 40.724, mean reward:  2.036 [-3.000, 31.984], mean action: 3.800 [0.000, 14.000],  loss: 0.020487, mae: 0.454964, mean_q: 0.417240, mean_eps: 0.000000
 1627/5000: episode: 67, duration: 0.277s, episode steps:  24, steps per second:  87, episode reward: -32.460, mean reward: -1.353 [-32.391,  3.000], mean action: 8.042 [0.000, 19.000],  loss: 0.017984, mae: 0.440798, mean_q: 0.404494, mean_eps: 0.000000
 1649/5000: episode: 68, duration: 0.260s, episode steps:  22, steps per second:  84, episode reward: 35.180, mean reward:  1.599 [-2.344, 32.017], mean action: 6.136 [0.000, 19.000],  loss: 0.020250, mae: 0.460618, mean_q: 0.397806, mean_eps: 0.000000
 1671/5000: episode: 69, duration: 0.250s, episode steps:  22, steps per second:  88, episode reward: 35.592, mean reward:  1.618 [-2.604, 32.592], mean action: 5.136 [0.000, 19.000],  loss: 0.016453, mae: 0.444212, mean_q: 0.476848, mean_eps: 0.000000
 1688/5000: episode: 70, duration: 0.201s, episode steps:  17, steps per second:  85, episode reward: 40.812, mean reward:  2.401 [-2.412, 33.000], mean action: 4.118 [0.000, 15.000],  loss: 0.023597, mae: 0.478103, mean_q: 0.426915, mean_eps: 0.000000
 1717/5000: episode: 71, duration: 0.328s, episode steps:  29, steps per second:  89, episode reward: -32.230, mean reward: -1.111 [-32.008,  2.360], mean action: 6.655 [0.000, 21.000],  loss: 0.023001, mae: 0.467359, mean_q: 0.473270, mean_eps: 0.000000
 1735/5000: episode: 72, duration: 0.213s, episode steps:  18, steps per second:  84, episode reward: 38.708, mean reward:  2.150 [-2.393, 32.230], mean action: 3.944 [0.000, 14.000],  loss: 0.022861, mae: 0.451197, mean_q: 0.464783, mean_eps: 0.000000
 1852/5000: episode: 73, duration: 1.793s, episode steps: 117, steps per second:  65, episode reward: -38.820, mean reward: -0.332 [-32.155,  2.340], mean action: 11.838 [0.000, 17.000],  loss: 0.020568, mae: 0.446966, mean_q: 0.462854, mean_eps: 0.000000
 1874/5000: episode: 74, duration: 0.656s, episode steps:  22, steps per second:  34, episode reward: -35.420, mean reward: -1.610 [-32.399,  2.550], mean action: 3.227 [0.000, 12.000],  loss: 0.021790, mae: 0.452898, mean_q: 0.467443, mean_eps: 0.000000
 1900/5000: episode: 75, duration: 0.401s, episode steps:  26, steps per second:  65, episode reward: 35.249, mean reward:  1.356 [-2.171, 33.000], mean action: 5.731 [0.000, 20.000],  loss: 0.019639, mae: 0.463190, mean_q: 0.450270, mean_eps: 0.000000
 1931/5000: episode: 76, duration: 1.520s, episode steps:  31, steps per second:  20, episode reward: -32.790, mean reward: -1.058 [-32.093,  3.062], mean action: 5.774 [0.000, 14.000],  loss: 0.018933, mae: 0.447599, mean_q: 0.463841, mean_eps: 0.000000
 1957/5000: episode: 77, duration: 0.545s, episode steps:  26, steps per second:  48, episode reward: 38.120, mean reward:  1.466 [-2.599, 32.300], mean action: 2.231 [0.000, 11.000],  loss: 0.022600, mae: 0.468992, mean_q: 0.466280, mean_eps: 0.000000
 1982/5000: episode: 78, duration: 0.827s, episode steps:  25, steps per second:  30, episode reward: -38.140, mean reward: -1.526 [-32.062,  2.269], mean action: 4.080 [0.000, 15.000],  loss: 0.013193, mae: 0.422936, mean_q: 0.452325, mean_eps: 0.000000
 2005/5000: episode: 79, duration: 0.271s, episode steps:  23, steps per second:  85, episode reward: 32.197, mean reward:  1.400 [-2.903, 32.257], mean action: 5.217 [0.000, 18.000],  loss: 0.017613, mae: 0.458815, mean_q: 0.406429, mean_eps: 0.000000
 2023/5000: episode: 80, duration: 0.215s, episode steps:  18, steps per second:  84, episode reward: 40.848, mean reward:  2.269 [-2.498, 32.150], mean action: 5.944 [0.000, 14.000],  loss: 0.019263, mae: 0.468892, mean_q: 0.427986, mean_eps: 0.000000
 2048/5000: episode: 81, duration: 0.294s, episode steps:  25, steps per second:  85, episode reward: 35.059, mean reward:  1.402 [-3.000, 31.925], mean action: 5.600 [1.000, 20.000],  loss: 0.022648, mae: 0.463419, mean_q: 0.486562, mean_eps: 0.000000
 2068/5000: episode: 82, duration: 0.256s, episode steps:  20, steps per second:  78, episode reward: 39.778, mean reward:  1.989 [-3.000, 32.189], mean action: 5.100 [0.000, 14.000],  loss: 0.019003, mae: 0.448141, mean_q: 0.453943, mean_eps: 0.000000
 2086/5000: episode: 83, duration: 0.486s, episode steps:  18, steps per second:  37, episode reward: 38.672, mean reward:  2.148 [-2.904, 32.769], mean action: 3.556 [0.000, 19.000],  loss: 0.025949, mae: 0.481199, mean_q: 0.436585, mean_eps: 0.000000
 2109/5000: episode: 84, duration: 0.302s, episode steps:  23, steps per second:  76, episode reward: 35.749, mean reward:  1.554 [-3.000, 32.280], mean action: 6.217 [0.000, 19.000],  loss: 0.020340, mae: 0.445967, mean_q: 0.528084, mean_eps: 0.000000
 2131/5000: episode: 85, duration: 0.510s, episode steps:  22, steps per second:  43, episode reward: 41.088, mean reward:  1.868 [-2.187, 32.310], mean action: 3.727 [0.000, 15.000],  loss: 0.019395, mae: 0.441959, mean_q: 0.531323, mean_eps: 0.000000
 2150/5000: episode: 86, duration: 0.705s, episode steps:  19, steps per second:  27, episode reward: 38.694, mean reward:  2.037 [-3.000, 32.140], mean action: 3.842 [0.000, 15.000],  loss: 0.026574, mae: 0.482269, mean_q: 0.489880, mean_eps: 0.000000
 2169/5000: episode: 87, duration: 0.252s, episode steps:  19, steps per second:  75, episode reward: 38.766, mean reward:  2.040 [-2.318, 31.991], mean action: 3.789 [0.000, 15.000],  loss: 0.021087, mae: 0.463646, mean_q: 0.472485, mean_eps: 0.000000
 2209/5000: episode: 88, duration: 0.789s, episode steps:  40, steps per second:  51, episode reward: 38.770, mean reward:  0.969 [-2.535, 32.350], mean action: 3.700 [0.000, 21.000],  loss: 0.022158, mae: 0.474616, mean_q: 0.448404, mean_eps: 0.000000
 2260/5000: episode: 89, duration: 1.420s, episode steps:  51, steps per second:  36, episode reward: -32.450, mean reward: -0.636 [-31.988,  2.901], mean action: 5.098 [0.000, 17.000],  loss: 0.020532, mae: 0.452908, mean_q: 0.481806, mean_eps: 0.000000
 2281/5000: episode: 90, duration: 0.320s, episode steps:  21, steps per second:  66, episode reward: 37.994, mean reward:  1.809 [-3.000, 31.914], mean action: 5.095 [0.000, 14.000],  loss: 0.019317, mae: 0.449641, mean_q: 0.420661, mean_eps: 0.000000
 2312/5000: episode: 91, duration: 0.521s, episode steps:  31, steps per second:  59, episode reward: -32.420, mean reward: -1.046 [-32.092,  2.390], mean action: 6.548 [0.000, 21.000],  loss: 0.018465, mae: 0.436053, mean_q: 0.457295, mean_eps: 0.000000
 2338/5000: episode: 92, duration: 0.503s, episode steps:  26, steps per second:  52, episode reward: -35.570, mean reward: -1.368 [-32.120,  2.511], mean action: 7.385 [1.000, 20.000],  loss: 0.019251, mae: 0.429518, mean_q: 0.475944, mean_eps: 0.000000
 2355/5000: episode: 93, duration: 0.230s, episode steps:  17, steps per second:  74, episode reward: 35.157, mean reward:  2.068 [-3.000, 33.000], mean action: 3.118 [0.000, 9.000],  loss: 0.023205, mae: 0.450151, mean_q: 0.527698, mean_eps: 0.000000
 2391/5000: episode: 94, duration: 0.504s, episode steps:  36, steps per second:  71, episode reward: 35.704, mean reward:  0.992 [-2.333, 32.057], mean action: 4.000 [0.000, 14.000],  loss: 0.021992, mae: 0.451542, mean_q: 0.463925, mean_eps: 0.000000
 2419/5000: episode: 95, duration: 0.334s, episode steps:  28, steps per second:  84, episode reward: -32.420, mean reward: -1.158 [-32.004,  2.616], mean action: 7.107 [0.000, 17.000],  loss: 0.020918, mae: 0.451474, mean_q: 0.483862, mean_eps: 0.000000
 2480/5000: episode: 96, duration: 1.906s, episode steps:  61, steps per second:  32, episode reward: 37.528, mean reward:  0.615 [-2.197, 32.431], mean action: 3.344 [0.000, 14.000],  loss: 0.018589, mae: 0.447858, mean_q: 0.398265, mean_eps: 0.000000
 2504/5000: episode: 97, duration: 0.288s, episode steps:  24, steps per second:  83, episode reward: 41.336, mean reward:  1.722 [-2.226, 32.224], mean action: 4.000 [0.000, 19.000],  loss: 0.021275, mae: 0.462832, mean_q: 0.397264, mean_eps: 0.000000
 2532/5000: episode: 98, duration: 0.332s, episode steps:  28, steps per second:  84, episode reward: -35.520, mean reward: -1.269 [-32.275,  2.630], mean action: 6.964 [0.000, 19.000],  loss: 0.019644, mae: 0.454496, mean_q: 0.400484, mean_eps: 0.000000
 2557/5000: episode: 99, duration: 0.292s, episode steps:  25, steps per second:  86, episode reward: -38.990, mean reward: -1.560 [-32.242,  2.236], mean action: 2.560 [0.000, 12.000],  loss: 0.018183, mae: 0.443117, mean_q: 0.412003, mean_eps: 0.000000
 2577/5000: episode: 100, duration: 0.241s, episode steps:  20, steps per second:  83, episode reward: 39.000, mean reward:  1.950 [-2.902, 32.140], mean action: 4.350 [0.000, 19.000],  loss: 0.020181, mae: 0.450091, mean_q: 0.411827, mean_eps: 0.000000
 2587/5000: episode: 101, duration: 0.129s, episode steps:  10, steps per second:  77, episode reward: 46.766, mean reward:  4.677 [-0.231, 32.080], mean action: 5.000 [0.000, 18.000],  loss: 0.017338, mae: 0.432799, mean_q: 0.424105, mean_eps: 0.000000
 2612/5000: episode: 102, duration: 0.299s, episode steps:  25, steps per second:  84, episode reward: 38.676, mean reward:  1.547 [-2.362, 31.926], mean action: 2.840 [0.000, 9.000],  loss: 0.022326, mae: 0.455160, mean_q: 0.473103, mean_eps: 0.000000
 2646/5000: episode: 103, duration: 0.379s, episode steps:  34, steps per second:  90, episode reward: 35.099, mean reward:  1.032 [-2.565, 32.210], mean action: 3.265 [0.000, 19.000],  loss: 0.020663, mae: 0.456618, mean_q: 0.497121, mean_eps: 0.000000
 2679/5000: episode: 104, duration: 0.385s, episode steps:  33, steps per second:  86, episode reward: 38.903, mean reward:  1.179 [-2.175, 32.083], mean action: 2.364 [0.000, 14.000],  loss: 0.019297, mae: 0.463086, mean_q: 0.430006, mean_eps: 0.000000
 2693/5000: episode: 105, duration: 0.170s, episode steps:  14, steps per second:  83, episode reward: 38.800, mean reward:  2.771 [-2.373, 33.000], mean action: 5.714 [0.000, 14.000],  loss: 0.017700, mae: 0.466096, mean_q: 0.432549, mean_eps: 0.000000
 2714/5000: episode: 106, duration: 0.249s, episode steps:  21, steps per second:  84, episode reward: -38.490, mean reward: -1.833 [-32.462,  2.200], mean action: 8.333 [1.000, 15.000],  loss: 0.019304, mae: 0.462304, mean_q: 0.455466, mean_eps: 0.000000
 2731/5000: episode: 107, duration: 0.200s, episode steps:  17, steps per second:  85, episode reward: 40.771, mean reward:  2.398 [-2.362, 31.830], mean action: 6.294 [0.000, 19.000],  loss: 0.014947, mae: 0.435222, mean_q: 0.449192, mean_eps: 0.000000
 2790/5000: episode: 108, duration: 0.643s, episode steps:  59, steps per second:  92, episode reward: -35.100, mean reward: -0.595 [-32.562,  2.532], mean action: 4.136 [0.000, 20.000],  loss: 0.018858, mae: 0.460062, mean_q: 0.417566, mean_eps: 0.000000
 2817/5000: episode: 109, duration: 0.427s, episode steps:  27, steps per second:  63, episode reward: 36.000, mean reward:  1.333 [-2.807, 32.300], mean action: 3.370 [0.000, 13.000],  loss: 0.019198, mae: 0.460609, mean_q: 0.411982, mean_eps: 0.000000
 2841/5000: episode: 110, duration: 0.289s, episode steps:  24, steps per second:  83, episode reward: 41.074, mean reward:  1.711 [-2.580, 32.440], mean action: 7.083 [0.000, 20.000],  loss: 0.021307, mae: 0.469021, mean_q: 0.398254, mean_eps: 0.000000
 2866/5000: episode: 111, duration: 0.305s, episode steps:  25, steps per second:  82, episode reward: 37.940, mean reward:  1.518 [-3.000, 33.000], mean action: 5.560 [0.000, 19.000],  loss: 0.021707, mae: 0.469476, mean_q: 0.456226, mean_eps: 0.000000
 2882/5000: episode: 112, duration: 0.191s, episode steps:  16, steps per second:  84, episode reward: 44.774, mean reward:  2.798 [-2.234, 33.000], mean action: 2.438 [0.000, 9.000],  loss: 0.014089, mae: 0.418844, mean_q: 0.508702, mean_eps: 0.000000
 2909/5000: episode: 113, duration: 0.384s, episode steps:  27, steps per second:  70, episode reward: 32.779, mean reward:  1.214 [-3.000, 32.080], mean action: 9.556 [0.000, 15.000],  loss: 0.021379, mae: 0.453968, mean_q: 0.460101, mean_eps: 0.000000
 2935/5000: episode: 114, duration: 0.302s, episode steps:  26, steps per second:  86, episode reward: 35.044, mean reward:  1.348 [-3.000, 32.264], mean action: 3.615 [0.000, 15.000],  loss: 0.020697, mae: 0.451035, mean_q: 0.479621, mean_eps: 0.000000
 2960/5000: episode: 115, duration: 0.285s, episode steps:  25, steps per second:  88, episode reward: 32.389, mean reward:  1.296 [-2.701, 32.233], mean action: 5.720 [1.000, 15.000],  loss: 0.021544, mae: 0.455181, mean_q: 0.492599, mean_eps: 0.000000
 2994/5000: episode: 116, duration: 0.377s, episode steps:  34, steps per second:  90, episode reward: 36.000, mean reward:  1.059 [-3.000, 33.000], mean action: 3.647 [0.000, 15.000],  loss: 0.023081, mae: 0.454744, mean_q: 0.471468, mean_eps: 0.000000
 3024/5000: episode: 117, duration: 0.352s, episode steps:  30, steps per second:  85, episode reward: -32.930, mean reward: -1.098 [-32.039,  2.812], mean action: 7.433 [0.000, 15.000],  loss: 0.019291, mae: 0.447772, mean_q: 0.422486, mean_eps: 0.000000
 3044/5000: episode: 118, duration: 0.226s, episode steps:  20, steps per second:  88, episode reward: 32.876, mean reward:  1.644 [-2.676, 33.000], mean action: 4.400 [0.000, 15.000],  loss: 0.022666, mae: 0.460129, mean_q: 0.431662, mean_eps: 0.000000
 3070/5000: episode: 119, duration: 0.296s, episode steps:  26, steps per second:  88, episode reward: -36.000, mean reward: -1.385 [-32.148,  2.536], mean action: 7.423 [0.000, 17.000],  loss: 0.019127, mae: 0.437409, mean_q: 0.481215, mean_eps: 0.000000
 3097/5000: episode: 120, duration: 0.302s, episode steps:  27, steps per second:  89, episode reward: 34.292, mean reward:  1.270 [-2.525, 32.280], mean action: 7.185 [0.000, 20.000],  loss: 0.016141, mae: 0.427623, mean_q: 0.468223, mean_eps: 0.000000
 3115/5000: episode: 121, duration: 0.211s, episode steps:  18, steps per second:  85, episode reward: 37.996, mean reward:  2.111 [-3.000, 33.344], mean action: 5.000 [0.000, 15.000],  loss: 0.018215, mae: 0.444971, mean_q: 0.405954, mean_eps: 0.000000
 3143/5000: episode: 122, duration: 0.313s, episode steps:  28, steps per second:  89, episode reward: -32.520, mean reward: -1.161 [-32.460,  2.875], mean action: 6.929 [0.000, 15.000],  loss: 0.024708, mae: 0.471058, mean_q: 0.427573, mean_eps: 0.000000
 3166/5000: episode: 123, duration: 0.315s, episode steps:  23, steps per second:  73, episode reward: 38.391, mean reward:  1.669 [-2.440, 32.210], mean action: 2.522 [0.000, 9.000],  loss: 0.018116, mae: 0.434040, mean_q: 0.501743, mean_eps: 0.000000
 3187/5000: episode: 124, duration: 0.252s, episode steps:  21, steps per second:  83, episode reward: 37.980, mean reward:  1.809 [-2.239, 31.897], mean action: 2.143 [0.000, 9.000],  loss: 0.017603, mae: 0.428703, mean_q: 0.463643, mean_eps: 0.000000
 3216/5000: episode: 125, duration: 0.351s, episode steps:  29, steps per second:  83, episode reward: 44.516, mean reward:  1.535 [-2.037, 31.984], mean action: 3.897 [1.000, 14.000],  loss: 0.020198, mae: 0.447799, mean_q: 0.447153, mean_eps: 0.000000
 3247/5000: episode: 126, duration: 0.385s, episode steps:  31, steps per second:  81, episode reward: 35.800, mean reward:  1.155 [-2.867, 32.648], mean action: 4.581 [0.000, 15.000],  loss: 0.019307, mae: 0.439855, mean_q: 0.463941, mean_eps: 0.000000
 3272/5000: episode: 127, duration: 0.791s, episode steps:  25, steps per second:  32, episode reward: 34.753, mean reward:  1.390 [-2.296, 33.000], mean action: 4.560 [0.000, 16.000],  loss: 0.021140, mae: 0.457642, mean_q: 0.435072, mean_eps: 0.000000
 3299/5000: episode: 128, duration: 0.412s, episode steps:  27, steps per second:  65, episode reward: -35.240, mean reward: -1.305 [-31.887,  3.000], mean action: 6.222 [0.000, 17.000],  loss: 0.020804, mae: 0.447823, mean_q: 0.453699, mean_eps: 0.000000
 3329/5000: episode: 129, duration: 0.637s, episode steps:  30, steps per second:  47, episode reward: 35.757, mean reward:  1.192 [-3.000, 32.350], mean action: 4.967 [0.000, 15.000],  loss: 0.021794, mae: 0.451570, mean_q: 0.545167, mean_eps: 0.000000
 3357/5000: episode: 130, duration: 1.030s, episode steps:  28, steps per second:  27, episode reward: -35.580, mean reward: -1.271 [-32.492,  3.000], mean action: 8.250 [0.000, 15.000],  loss: 0.020880, mae: 0.456342, mean_q: 0.511053, mean_eps: 0.000000
 3404/5000: episode: 131, duration: 0.711s, episode steps:  47, steps per second:  66, episode reward: -35.140, mean reward: -0.748 [-32.246,  2.390], mean action: 6.085 [0.000, 19.000],  loss: 0.018224, mae: 0.447041, mean_q: 0.496502, mean_eps: 0.000000
 3432/5000: episode: 132, duration: 0.362s, episode steps:  28, steps per second:  77, episode reward: 36.000, mean reward:  1.286 [-2.351, 32.630], mean action: 3.821 [0.000, 19.000],  loss: 0.020032, mae: 0.459831, mean_q: 0.479893, mean_eps: 0.000000
 3454/5000: episode: 133, duration: 0.324s, episode steps:  22, steps per second:  68, episode reward: 41.194, mean reward:  1.872 [-2.803, 32.500], mean action: 5.318 [0.000, 14.000],  loss: 0.019307, mae: 0.462803, mean_q: 0.439637, mean_eps: 0.000000
 3481/5000: episode: 134, duration: 0.321s, episode steps:  27, steps per second:  84, episode reward: -32.600, mean reward: -1.207 [-32.301,  2.879], mean action: 5.815 [0.000, 14.000],  loss: 0.024142, mae: 0.477880, mean_q: 0.440854, mean_eps: 0.000000
 3501/5000: episode: 135, duration: 0.251s, episode steps:  20, steps per second:  80, episode reward: 34.809, mean reward:  1.740 [-2.604, 31.534], mean action: 6.100 [0.000, 18.000],  loss: 0.016114, mae: 0.429367, mean_q: 0.506200, mean_eps: 0.000000
 3523/5000: episode: 136, duration: 0.266s, episode steps:  22, steps per second:  83, episode reward: 43.449, mean reward:  1.975 [-2.116, 32.010], mean action: 3.591 [0.000, 14.000],  loss: 0.020774, mae: 0.459528, mean_q: 0.442323, mean_eps: 0.000000
 3542/5000: episode: 137, duration: 0.306s, episode steps:  19, steps per second:  62, episode reward: 38.115, mean reward:  2.006 [-2.576, 33.000], mean action: 3.737 [0.000, 14.000],  loss: 0.022247, mae: 0.466704, mean_q: 0.446572, mean_eps: 0.000000
 3573/5000: episode: 138, duration: 1.058s, episode steps:  31, steps per second:  29, episode reward: 37.918, mean reward:  1.223 [-2.267, 31.771], mean action: 4.484 [0.000, 20.000],  loss: 0.018699, mae: 0.451856, mean_q: 0.461591, mean_eps: 0.000000
 3599/5000: episode: 139, duration: 1.623s, episode steps:  26, steps per second:  16, episode reward: 32.478, mean reward:  1.249 [-3.000, 32.034], mean action: 6.731 [1.000, 15.000],  loss: 0.017259, mae: 0.449702, mean_q: 0.479714, mean_eps: 0.000000
 3614/5000: episode: 140, duration: 0.291s, episode steps:  15, steps per second:  51, episode reward: 38.710, mean reward:  2.581 [-2.590, 32.310], mean action: 4.533 [0.000, 14.000],  loss: 0.017912, mae: 0.451932, mean_q: 0.490517, mean_eps: 0.000000
 3638/5000: episode: 141, duration: 0.353s, episode steps:  24, steps per second:  68, episode reward: 38.012, mean reward:  1.584 [-3.000, 31.981], mean action: 3.875 [0.000, 14.000],  loss: 0.018872, mae: 0.458090, mean_q: 0.434401, mean_eps: 0.000000
 3666/5000: episode: 142, duration: 0.544s, episode steps:  28, steps per second:  52, episode reward: 30.000, mean reward:  1.071 [-2.551, 30.090], mean action: 6.357 [0.000, 15.000],  loss: 0.019142, mae: 0.467919, mean_q: 0.453470, mean_eps: 0.000000
 3688/5000: episode: 143, duration: 0.266s, episode steps:  22, steps per second:  83, episode reward: 32.581, mean reward:  1.481 [-3.000, 32.480], mean action: 7.045 [0.000, 21.000],  loss: 0.019139, mae: 0.459943, mean_q: 0.466807, mean_eps: 0.000000
 3715/5000: episode: 144, duration: 0.609s, episode steps:  27, steps per second:  44, episode reward: -33.000, mean reward: -1.222 [-30.174,  2.190], mean action: 5.481 [0.000, 15.000],  loss: 0.020013, mae: 0.454922, mean_q: 0.448703, mean_eps: 0.000000
 3740/5000: episode: 145, duration: 0.622s, episode steps:  25, steps per second:  40, episode reward: 35.597, mean reward:  1.424 [-3.000, 32.132], mean action: 3.000 [0.000, 12.000],  loss: 0.020809, mae: 0.451664, mean_q: 0.535314, mean_eps: 0.000000
 3755/5000: episode: 146, duration: 0.692s, episode steps:  15, steps per second:  22, episode reward: -44.900, mean reward: -2.993 [-31.943,  1.583], mean action: 6.000 [1.000, 15.000],  loss: 0.018338, mae: 0.432372, mean_q: 0.518034, mean_eps: 0.000000
 3772/5000: episode: 147, duration: 0.308s, episode steps:  17, steps per second:  55, episode reward: 40.819, mean reward:  2.401 [-2.136, 32.220], mean action: 4.294 [0.000, 14.000],  loss: 0.019745, mae: 0.445734, mean_q: 0.471827, mean_eps: 0.000000
 3788/5000: episode: 148, duration: 0.203s, episode steps:  16, steps per second:  79, episode reward: 41.494, mean reward:  2.593 [-2.321, 32.470], mean action: 3.125 [0.000, 12.000],  loss: 0.020812, mae: 0.447885, mean_q: 0.485797, mean_eps: 0.000000
 3810/5000: episode: 149, duration: 0.265s, episode steps:  22, steps per second:  83, episode reward: 37.770, mean reward:  1.717 [-2.902, 32.140], mean action: 4.682 [0.000, 13.000],  loss: 0.017124, mae: 0.432958, mean_q: 0.422525, mean_eps: 0.000000
 3834/5000: episode: 150, duration: 0.271s, episode steps:  24, steps per second:  89, episode reward: 35.959, mean reward:  1.498 [-2.611, 32.120], mean action: 6.583 [0.000, 15.000],  loss: 0.022037, mae: 0.461975, mean_q: 0.435205, mean_eps: 0.000000
 3872/5000: episode: 151, duration: 0.438s, episode steps:  38, steps per second:  87, episode reward: -36.000, mean reward: -0.947 [-29.571,  2.240], mean action: 8.079 [0.000, 15.000],  loss: 0.021789, mae: 0.463584, mean_q: 0.467262, mean_eps: 0.000000
 3901/5000: episode: 152, duration: 0.340s, episode steps:  29, steps per second:  85, episode reward: 32.824, mean reward:  1.132 [-2.412, 31.874], mean action: 5.241 [0.000, 15.000],  loss: 0.018184, mae: 0.448872, mean_q: 0.462156, mean_eps: 0.000000
 3927/5000: episode: 153, duration: 0.314s, episode steps:  26, steps per second:  83, episode reward: -32.440, mean reward: -1.248 [-32.459,  2.805], mean action: 7.500 [0.000, 15.000],  loss: 0.018309, mae: 0.449245, mean_q: 0.491271, mean_eps: 0.000000
 3946/5000: episode: 154, duration: 0.218s, episode steps:  19, steps per second:  87, episode reward: -38.940, mean reward: -2.049 [-32.667,  2.069], mean action: 8.000 [0.000, 20.000],  loss: 0.020048, mae: 0.457505, mean_q: 0.483539, mean_eps: 0.000000
 3988/5000: episode: 155, duration: 0.500s, episode steps:  42, steps per second:  84, episode reward: 32.373, mean reward:  0.771 [-2.292, 31.758], mean action: 5.643 [0.000, 21.000],  loss: 0.019197, mae: 0.455004, mean_q: 0.457912, mean_eps: 0.000000
 4007/5000: episode: 156, duration: 0.272s, episode steps:  19, steps per second:  70, episode reward: 34.584, mean reward:  1.820 [-2.717, 32.277], mean action: 5.895 [0.000, 20.000],  loss: 0.016430, mae: 0.443866, mean_q: 0.466157, mean_eps: 0.000000
 4031/5000: episode: 157, duration: 0.281s, episode steps:  24, steps per second:  86, episode reward: 35.309, mean reward:  1.471 [-2.451, 32.546], mean action: 6.250 [0.000, 18.000],  loss: 0.019051, mae: 0.450449, mean_q: 0.434610, mean_eps: 0.000000
 4062/5000: episode: 158, duration: 0.690s, episode steps:  31, steps per second:  45, episode reward: 38.145, mean reward:  1.230 [-2.425, 32.430], mean action: 5.258 [1.000, 14.000],  loss: 0.017421, mae: 0.450305, mean_q: 0.445284, mean_eps: 0.000000
 4091/5000: episode: 159, duration: 0.351s, episode steps:  29, steps per second:  83, episode reward: 35.588, mean reward:  1.227 [-3.000, 32.100], mean action: 11.345 [0.000, 21.000],  loss: 0.022647, mae: 0.470997, mean_q: 0.436279, mean_eps: 0.000000
 4123/5000: episode: 160, duration: 0.358s, episode steps:  32, steps per second:  89, episode reward: 32.292, mean reward:  1.009 [-2.903, 32.280], mean action: 7.562 [0.000, 19.000],  loss: 0.020800, mae: 0.461998, mean_q: 0.455383, mean_eps: 0.000000
 4136/5000: episode: 161, duration: 0.158s, episode steps:  13, steps per second:  82, episode reward: -47.030, mean reward: -3.618 [-33.000,  0.620], mean action: 9.692 [0.000, 15.000],  loss: 0.014118, mae: 0.439249, mean_q: 0.460559, mean_eps: 0.000000
 4148/5000: episode: 162, duration: 0.230s, episode steps:  12, steps per second:  52, episode reward: 42.000, mean reward:  3.500 [-2.202, 32.420], mean action: 4.833 [0.000, 19.000],  loss: 0.020316, mae: 0.471301, mean_q: 0.467188, mean_eps: 0.000000
 4175/5000: episode: 163, duration: 0.752s, episode steps:  27, steps per second:  36, episode reward: -35.280, mean reward: -1.307 [-32.755,  2.480], mean action: 7.444 [3.000, 21.000],  loss: 0.016968, mae: 0.440052, mean_q: 0.482569, mean_eps: 0.000000
 4207/5000: episode: 164, duration: 1.157s, episode steps:  32, steps per second:  28, episode reward: 35.872, mean reward:  1.121 [-3.000, 32.304], mean action: 4.562 [0.000, 18.000],  loss: 0.019427, mae: 0.456382, mean_q: 0.488958, mean_eps: 0.000000
 4265/5000: episode: 165, duration: 0.674s, episode steps:  58, steps per second:  86, episode reward: 35.878, mean reward:  0.619 [-2.878, 32.410], mean action: 4.155 [0.000, 21.000],  loss: 0.019694, mae: 0.456919, mean_q: 0.414118, mean_eps: 0.000000
 4298/5000: episode: 166, duration: 0.373s, episode steps:  33, steps per second:  88, episode reward: -32.370, mean reward: -0.981 [-32.016,  3.032], mean action: 8.545 [0.000, 18.000],  loss: 0.020134, mae: 0.461521, mean_q: 0.495244, mean_eps: 0.000000
 4318/5000: episode: 167, duration: 0.237s, episode steps:  20, steps per second:  85, episode reward: 38.057, mean reward:  1.903 [-2.655, 32.594], mean action: 4.550 [0.000, 21.000],  loss: 0.021021, mae: 0.468495, mean_q: 0.466570, mean_eps: 0.000000
 4336/5000: episode: 168, duration: 0.292s, episode steps:  18, steps per second:  62, episode reward: 35.189, mean reward:  1.955 [-3.000, 31.729], mean action: 7.111 [0.000, 20.000],  loss: 0.014754, mae: 0.441081, mean_q: 0.446331, mean_eps: 0.000000
 4371/5000: episode: 169, duration: 1.327s, episode steps:  35, steps per second:  26, episode reward: 38.744, mean reward:  1.107 [-2.771, 32.140], mean action: 3.000 [0.000, 21.000],  loss: 0.019940, mae: 0.469920, mean_q: 0.437293, mean_eps: 0.000000
 4397/5000: episode: 170, duration: 0.852s, episode steps:  26, steps per second:  30, episode reward: 38.001, mean reward:  1.462 [-2.323, 31.910], mean action: 6.385 [1.000, 15.000],  loss: 0.019063, mae: 0.464280, mean_q: 0.461799, mean_eps: 0.000000
 4424/5000: episode: 171, duration: 0.380s, episode steps:  27, steps per second:  71, episode reward: -35.690, mean reward: -1.322 [-32.253,  2.792], mean action: 6.963 [0.000, 16.000],  loss: 0.019003, mae: 0.463615, mean_q: 0.461126, mean_eps: 0.000000
 4449/5000: episode: 172, duration: 0.328s, episode steps:  25, steps per second:  76, episode reward: 40.123, mean reward:  1.605 [-2.137, 31.932], mean action: 5.840 [0.000, 21.000],  loss: 0.023001, mae: 0.471614, mean_q: 0.495779, mean_eps: 0.000000
 4478/5000: episode: 173, duration: 0.426s, episode steps:  29, steps per second:  68, episode reward: 35.796, mean reward:  1.234 [-2.465, 33.000], mean action: 2.828 [0.000, 14.000],  loss: 0.019937, mae: 0.468715, mean_q: 0.442657, mean_eps: 0.000000
 4498/5000: episode: 174, duration: 0.282s, episode steps:  20, steps per second:  71, episode reward: 35.086, mean reward:  1.754 [-2.285, 32.080], mean action: 4.850 [0.000, 14.000],  loss: 0.020830, mae: 0.483401, mean_q: 0.384270, mean_eps: 0.000000
 4552/5000: episode: 175, duration: 0.599s, episode steps:  54, steps per second:  90, episode reward: -44.380, mean reward: -0.822 [-32.294,  1.845], mean action: 10.870 [1.000, 20.000],  loss: 0.018242, mae: 0.448970, mean_q: 0.470728, mean_eps: 0.000000
 4570/5000: episode: 176, duration: 0.207s, episode steps:  18, steps per second:  87, episode reward: 38.903, mean reward:  2.161 [-2.302, 32.563], mean action: 4.333 [0.000, 19.000],  loss: 0.023395, mae: 0.468812, mean_q: 0.492787, mean_eps: 0.000000
 4594/5000: episode: 177, duration: 0.788s, episode steps:  24, steps per second:  30, episode reward: 38.374, mean reward:  1.599 [-2.202, 32.270], mean action: 5.958 [0.000, 14.000],  loss: 0.017753, mae: 0.450360, mean_q: 0.442228, mean_eps: 0.000000
 4614/5000: episode: 178, duration: 0.310s, episode steps:  20, steps per second:  64, episode reward: 38.149, mean reward:  1.907 [-2.510, 32.754], mean action: 5.100 [0.000, 14.000],  loss: 0.020474, mae: 0.450729, mean_q: 0.485916, mean_eps: 0.000000
 4640/5000: episode: 179, duration: 0.290s, episode steps:  26, steps per second:  90, episode reward: 36.000, mean reward:  1.385 [-2.688, 32.100], mean action: 5.538 [0.000, 18.000],  loss: 0.017801, mae: 0.433938, mean_q: 0.451910, mean_eps: 0.000000
 4664/5000: episode: 180, duration: 0.683s, episode steps:  24, steps per second:  35, episode reward: 34.692, mean reward:  1.445 [-2.458, 32.053], mean action: 7.208 [0.000, 21.000],  loss: 0.020128, mae: 0.453883, mean_q: 0.442671, mean_eps: 0.000000
 4686/5000: episode: 181, duration: 0.500s, episode steps:  22, steps per second:  44, episode reward: -33.000, mean reward: -1.500 [-32.335,  2.875], mean action: 8.136 [0.000, 19.000],  loss: 0.021882, mae: 0.463636, mean_q: 0.478199, mean_eps: 0.000000
 4708/5000: episode: 182, duration: 0.250s, episode steps:  22, steps per second:  88, episode reward: 36.000, mean reward:  1.636 [-2.549, 32.350], mean action: 4.045 [0.000, 14.000],  loss: 0.020190, mae: 0.449328, mean_q: 0.452539, mean_eps: 0.000000
 4737/5000: episode: 183, duration: 0.411s, episode steps:  29, steps per second:  71, episode reward: 35.110, mean reward:  1.211 [-2.628, 32.034], mean action: 3.897 [0.000, 15.000],  loss: 0.021727, mae: 0.464868, mean_q: 0.427528, mean_eps: 0.000000
 4758/5000: episode: 184, duration: 0.246s, episode steps:  21, steps per second:  85, episode reward: 43.059, mean reward:  2.050 [-2.039, 32.200], mean action: 2.952 [0.000, 14.000],  loss: 0.020032, mae: 0.456235, mean_q: 0.387569, mean_eps: 0.000000
 4786/5000: episode: 185, duration: 0.442s, episode steps:  28, steps per second:  63, episode reward: 35.332, mean reward:  1.262 [-2.575, 31.782], mean action: 9.107 [0.000, 19.000],  loss: 0.021088, mae: 0.459647, mean_q: 0.438628, mean_eps: 0.000000
 4806/5000: episode: 186, duration: 0.609s, episode steps:  20, steps per second:  33, episode reward: 44.362, mean reward:  2.218 [-2.212, 32.180], mean action: 3.850 [1.000, 19.000],  loss: 0.023185, mae: 0.462596, mean_q: 0.495709, mean_eps: 0.000000
 4828/5000: episode: 187, duration: 0.844s, episode steps:  22, steps per second:  26, episode reward: 41.352, mean reward:  1.880 [-2.702, 32.100], mean action: 4.045 [0.000, 21.000],  loss: 0.018714, mae: 0.442853, mean_q: 0.467391, mean_eps: 0.000000
 4843/5000: episode: 188, duration: 0.388s, episode steps:  15, steps per second:  39, episode reward: 38.384, mean reward:  2.559 [-3.000, 32.058], mean action: 4.200 [0.000, 9.000],  loss: 0.024210, mae: 0.471164, mean_q: 0.437380, mean_eps: 0.000000
 4865/5000: episode: 189, duration: 0.390s, episode steps:  22, steps per second:  56, episode reward: 35.697, mean reward:  1.623 [-2.397, 32.280], mean action: 3.273 [0.000, 15.000],  loss: 0.019901, mae: 0.444201, mean_q: 0.470982, mean_eps: 0.000000
 4888/5000: episode: 190, duration: 0.668s, episode steps:  23, steps per second:  34, episode reward: 35.363, mean reward:  1.538 [-3.000, 32.070], mean action: 3.870 [0.000, 16.000],  loss: 0.023988, mae: 0.471507, mean_q: 0.473019, mean_eps: 0.000000
 4902/5000: episode: 191, duration: 0.182s, episode steps:  14, steps per second:  77, episode reward: 44.207, mean reward:  3.158 [-2.303, 33.000], mean action: 3.429 [0.000, 13.000],  loss: 0.018621, mae: 0.439764, mean_q: 0.473192, mean_eps: 0.000000
 4945/5000: episode: 192, duration: 0.600s, episode steps:  43, steps per second:  72, episode reward: 41.407, mean reward:  0.963 [-2.516, 32.240], mean action: 2.349 [0.000, 12.000],  loss: 0.021857, mae: 0.458409, mean_q: 0.476840, mean_eps: 0.000000
 4970/5000: episode: 193, duration: 1.092s, episode steps:  25, steps per second:  23, episode reward: -36.000, mean reward: -1.440 [-32.464,  2.403], mean action: 4.280 [0.000, 14.000],  loss: 0.020878, mae: 0.450125, mean_q: 0.489917, mean_eps: 0.000000
done, took 77.092 seconds
DQN Evaluation: 1531 victories out of 1845 episodes
Training for 5000 steps ...
   36/5000: episode: 1, duration: 0.227s, episode steps:  36, steps per second: 159, episode reward: 35.808, mean reward:  0.995 [-2.590, 32.668], mean action: 3.778 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   66/5000: episode: 2, duration: 0.195s, episode steps:  30, steps per second: 154, episode reward: 38.624, mean reward:  1.287 [-2.373, 32.270], mean action: 3.100 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   92/5000: episode: 3, duration: 0.169s, episode steps:  26, steps per second: 154, episode reward: 41.691, mean reward:  1.604 [-2.159, 32.311], mean action: 3.692 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  125/5000: episode: 4, duration: 0.216s, episode steps:  33, steps per second: 153, episode reward: 35.742, mean reward:  1.083 [-3.000, 32.412], mean action: 6.091 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  165/5000: episode: 5, duration: 0.256s, episode steps:  40, steps per second: 156, episode reward: 44.554, mean reward:  1.114 [-2.136, 32.160], mean action: 1.100 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  180/5000: episode: 6, duration: 0.103s, episode steps:  15, steps per second: 146, episode reward: 41.119, mean reward:  2.741 [-3.000, 31.660], mean action: 3.600 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  205/5000: episode: 7, duration: 0.163s, episode steps:  25, steps per second: 153, episode reward: 44.752, mean reward:  1.790 [-2.325, 32.370], mean action: 3.320 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  222/5000: episode: 8, duration: 0.122s, episode steps:  17, steps per second: 139, episode reward: 44.401, mean reward:  2.612 [-2.112, 32.340], mean action: 0.882 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  241/5000: episode: 9, duration: 0.122s, episode steps:  19, steps per second: 156, episode reward: 43.747, mean reward:  2.302 [-2.659, 32.149], mean action: 5.316 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  266/5000: episode: 10, duration: 0.172s, episode steps:  25, steps per second: 146, episode reward: 39.644, mean reward:  1.586 [-2.066, 32.411], mean action: 4.680 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  285/5000: episode: 11, duration: 0.125s, episode steps:  19, steps per second: 152, episode reward: 42.000, mean reward:  2.211 [-2.925, 32.020], mean action: 3.421 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  317/5000: episode: 12, duration: 0.197s, episode steps:  32, steps per second: 163, episode reward: 41.730, mean reward:  1.304 [-2.861, 32.886], mean action: 5.969 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  337/5000: episode: 13, duration: 0.133s, episode steps:  20, steps per second: 150, episode reward: 44.181, mean reward:  2.209 [-2.189, 32.090], mean action: 3.700 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  359/5000: episode: 14, duration: 0.144s, episode steps:  22, steps per second: 153, episode reward: 41.144, mean reward:  1.870 [-2.715, 32.180], mean action: 2.818 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  377/5000: episode: 15, duration: 0.125s, episode steps:  18, steps per second: 144, episode reward: 44.146, mean reward:  2.453 [-2.090, 32.254], mean action: 4.389 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  401/5000: episode: 16, duration: 0.161s, episode steps:  24, steps per second: 149, episode reward: 38.123, mean reward:  1.588 [-2.805, 32.270], mean action: 1.750 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  425/5000: episode: 17, duration: 0.159s, episode steps:  24, steps per second: 150, episode reward: 43.711, mean reward:  1.821 [-2.046, 32.251], mean action: 2.417 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  455/5000: episode: 18, duration: 0.190s, episode steps:  30, steps per second: 158, episode reward: 38.012, mean reward:  1.267 [-3.000, 31.975], mean action: 2.600 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  481/5000: episode: 19, duration: 0.162s, episode steps:  26, steps per second: 160, episode reward: 38.559, mean reward:  1.483 [-2.479, 32.758], mean action: 4.192 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  515/5000: episode: 20, duration: 0.199s, episode steps:  34, steps per second: 171, episode reward: 38.825, mean reward:  1.142 [-2.791, 31.935], mean action: 2.118 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  547/5000: episode: 21, duration: 0.205s, episode steps:  32, steps per second: 156, episode reward: 38.668, mean reward:  1.208 [-3.000, 32.028], mean action: 1.625 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  570/5000: episode: 22, duration: 0.154s, episode steps:  23, steps per second: 150, episode reward: 44.895, mean reward:  1.952 [-2.035, 32.423], mean action: 2.913 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  587/5000: episode: 23, duration: 0.121s, episode steps:  17, steps per second: 141, episode reward: 47.412, mean reward:  2.789 [ 0.029, 31.930], mean action: 1.353 [1.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  619/5000: episode: 24, duration: 0.199s, episode steps:  32, steps per second: 161, episode reward: 38.042, mean reward:  1.189 [-3.000, 32.096], mean action: 2.750 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  642/5000: episode: 25, duration: 0.147s, episode steps:  23, steps per second: 157, episode reward: 41.172, mean reward:  1.790 [-2.081, 31.775], mean action: 3.391 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  676/5000: episode: 26, duration: 0.240s, episode steps:  34, steps per second: 142, episode reward: 38.614, mean reward:  1.136 [-2.395, 32.173], mean action: 3.000 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  700/5000: episode: 27, duration: 0.178s, episode steps:  24, steps per second: 135, episode reward: 38.518, mean reward:  1.605 [-2.354, 32.518], mean action: 3.000 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  715/5000: episode: 28, duration: 0.113s, episode steps:  15, steps per second: 133, episode reward: 44.414, mean reward:  2.961 [-2.138, 32.360], mean action: 2.867 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  750/5000: episode: 29, duration: 0.225s, episode steps:  35, steps per second: 155, episode reward: 41.762, mean reward:  1.193 [-2.213, 31.952], mean action: 3.114 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  781/5000: episode: 30, duration: 0.195s, episode steps:  31, steps per second: 159, episode reward: -32.780, mean reward: -1.057 [-32.091,  2.250], mean action: 6.742 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  802/5000: episode: 31, duration: 0.134s, episode steps:  21, steps per second: 157, episode reward: 41.541, mean reward:  1.978 [-2.044, 32.051], mean action: 2.810 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  829/5000: episode: 32, duration: 0.335s, episode steps:  27, steps per second:  81, episode reward: 42.000, mean reward:  1.556 [-2.168, 32.220], mean action: 4.741 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  851/5000: episode: 33, duration: 0.231s, episode steps:  22, steps per second:  95, episode reward: 44.741, mean reward:  2.034 [-2.903, 32.270], mean action: 4.273 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  874/5000: episode: 34, duration: 0.152s, episode steps:  23, steps per second: 151, episode reward: 41.674, mean reward:  1.812 [-2.489, 32.174], mean action: 6.435 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  907/5000: episode: 35, duration: 0.197s, episode steps:  33, steps per second: 168, episode reward: 38.203, mean reward:  1.158 [-2.544, 31.868], mean action: 3.697 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  936/5000: episode: 36, duration: 0.177s, episode steps:  29, steps per second: 164, episode reward: 44.394, mean reward:  1.531 [-2.277, 32.051], mean action: 3.690 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  949/5000: episode: 37, duration: 0.092s, episode steps:  13, steps per second: 141, episode reward: 40.551, mean reward:  3.119 [-3.000, 32.744], mean action: 4.615 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1035/5000: episode: 38, duration: 0.664s, episode steps:  86, steps per second: 129, episode reward: 35.307, mean reward:  0.411 [-2.228, 32.110], mean action: 5.221 [0.000, 20.000],  loss: 0.019413, mae: 0.449572, mean_q: 0.526101, mean_eps: 0.000000
 1062/5000: episode: 39, duration: 0.318s, episode steps:  27, steps per second:  85, episode reward: 41.615, mean reward:  1.541 [-2.343, 32.440], mean action: 3.593 [0.000, 15.000],  loss: 0.020388, mae: 0.457727, mean_q: 0.467805, mean_eps: 0.000000
 1094/5000: episode: 40, duration: 0.363s, episode steps:  32, steps per second:  88, episode reward: 41.753, mean reward:  1.305 [-2.736, 32.430], mean action: 2.312 [0.000, 15.000],  loss: 0.018163, mae: 0.449499, mean_q: 0.436784, mean_eps: 0.000000
 1140/5000: episode: 41, duration: 0.518s, episode steps:  46, steps per second:  89, episode reward: -32.160, mean reward: -0.699 [-32.054,  2.350], mean action: 2.761 [0.000, 15.000],  loss: 0.022154, mae: 0.471162, mean_q: 0.436807, mean_eps: 0.000000
 1161/5000: episode: 42, duration: 0.243s, episode steps:  21, steps per second:  86, episode reward: 35.312, mean reward:  1.682 [-2.480, 31.562], mean action: 3.762 [0.000, 15.000],  loss: 0.021077, mae: 0.461758, mean_q: 0.429725, mean_eps: 0.000000
 1180/5000: episode: 43, duration: 0.222s, episode steps:  19, steps per second:  86, episode reward: 39.000, mean reward:  2.053 [-3.000, 32.220], mean action: 5.526 [0.000, 20.000],  loss: 0.019366, mae: 0.450599, mean_q: 0.428452, mean_eps: 0.000000
 1222/5000: episode: 44, duration: 0.464s, episode steps:  42, steps per second:  91, episode reward: 36.000, mean reward:  0.857 [-2.668, 29.748], mean action: 3.238 [0.000, 13.000],  loss: 0.021302, mae: 0.460552, mean_q: 0.420985, mean_eps: 0.000000
 1252/5000: episode: 45, duration: 0.335s, episode steps:  30, steps per second:  90, episode reward: 30.000, mean reward:  1.000 [-3.000, 30.098], mean action: 5.633 [0.000, 19.000],  loss: 0.021632, mae: 0.459186, mean_q: 0.446897, mean_eps: 0.000000
 1287/5000: episode: 46, duration: 0.398s, episode steps:  35, steps per second:  88, episode reward: 44.902, mean reward:  1.283 [-2.206, 32.072], mean action: 1.429 [0.000, 9.000],  loss: 0.016668, mae: 0.436375, mean_q: 0.459297, mean_eps: 0.000000
 1315/5000: episode: 47, duration: 0.315s, episode steps:  28, steps per second:  89, episode reward: 35.940, mean reward:  1.284 [-3.000, 32.360], mean action: 3.500 [0.000, 15.000],  loss: 0.019470, mae: 0.464824, mean_q: 0.424804, mean_eps: 0.000000
 1331/5000: episode: 48, duration: 0.193s, episode steps:  16, steps per second:  83, episode reward: 41.391, mean reward:  2.587 [-2.504, 32.250], mean action: 1.750 [0.000, 11.000],  loss: 0.024444, mae: 0.482759, mean_q: 0.447935, mean_eps: 0.000000
 1358/5000: episode: 49, duration: 0.306s, episode steps:  27, steps per second:  88, episode reward: 41.230, mean reward:  1.527 [-2.298, 32.100], mean action: 2.074 [0.000, 15.000],  loss: 0.020673, mae: 0.457503, mean_q: 0.471291, mean_eps: 0.000000
 1381/5000: episode: 50, duration: 0.277s, episode steps:  23, steps per second:  83, episode reward: 44.152, mean reward:  1.920 [-2.803, 32.370], mean action: 3.348 [0.000, 21.000],  loss: 0.018044, mae: 0.447850, mean_q: 0.399910, mean_eps: 0.000000
 1401/5000: episode: 51, duration: 0.234s, episode steps:  20, steps per second:  86, episode reward: 35.776, mean reward:  1.789 [-2.343, 31.896], mean action: 5.050 [1.000, 14.000],  loss: 0.021741, mae: 0.460079, mean_q: 0.453300, mean_eps: 0.000000
 1425/5000: episode: 52, duration: 0.277s, episode steps:  24, steps per second:  86, episode reward: 44.390, mean reward:  1.850 [-2.080, 32.170], mean action: 3.000 [0.000, 12.000],  loss: 0.015047, mae: 0.433354, mean_q: 0.405150, mean_eps: 0.000000
 1470/5000: episode: 53, duration: 0.706s, episode steps:  45, steps per second:  64, episode reward: 41.735, mean reward:  0.927 [-2.086, 32.300], mean action: 2.800 [0.000, 14.000],  loss: 0.019175, mae: 0.457986, mean_q: 0.447770, mean_eps: 0.000000
 1490/5000: episode: 54, duration: 0.387s, episode steps:  20, steps per second:  52, episode reward: 44.406, mean reward:  2.220 [-2.036, 32.490], mean action: 4.900 [0.000, 21.000],  loss: 0.021871, mae: 0.459579, mean_q: 0.442258, mean_eps: 0.000000
 1524/5000: episode: 55, duration: 0.430s, episode steps:  34, steps per second:  79, episode reward: 41.226, mean reward:  1.213 [-2.253, 32.006], mean action: 2.882 [0.000, 19.000],  loss: 0.017069, mae: 0.437885, mean_q: 0.461590, mean_eps: 0.000000
 1556/5000: episode: 56, duration: 0.363s, episode steps:  32, steps per second:  88, episode reward: 36.000, mean reward:  1.125 [-2.556, 32.340], mean action: 9.875 [0.000, 21.000],  loss: 0.017919, mae: 0.443937, mean_q: 0.443913, mean_eps: 0.000000
 1577/5000: episode: 57, duration: 0.250s, episode steps:  21, steps per second:  84, episode reward: 44.210, mean reward:  2.105 [-2.026, 32.180], mean action: 5.095 [0.000, 19.000],  loss: 0.019981, mae: 0.447632, mean_q: 0.425120, mean_eps: 0.000000
 1602/5000: episode: 58, duration: 0.284s, episode steps:  25, steps per second:  88, episode reward: 40.198, mean reward:  1.608 [-2.120, 31.741], mean action: 3.800 [0.000, 19.000],  loss: 0.020729, mae: 0.447719, mean_q: 0.449169, mean_eps: 0.000000
 1630/5000: episode: 59, duration: 0.316s, episode steps:  28, steps per second:  89, episode reward: 40.943, mean reward:  1.462 [-3.000, 32.443], mean action: 3.500 [0.000, 15.000],  loss: 0.018294, mae: 0.428476, mean_q: 0.524098, mean_eps: 0.000000
 1651/5000: episode: 60, duration: 0.246s, episode steps:  21, steps per second:  85, episode reward: 44.177, mean reward:  2.104 [-2.914, 32.090], mean action: 3.762 [3.000, 19.000],  loss: 0.018431, mae: 0.432373, mean_q: 0.500917, mean_eps: 0.000000
 1695/5000: episode: 61, duration: 0.506s, episode steps:  44, steps per second:  87, episode reward: 41.280, mean reward:  0.938 [-2.396, 31.896], mean action: 3.568 [0.000, 19.000],  loss: 0.017897, mae: 0.446254, mean_q: 0.449694, mean_eps: 0.000000
 1721/5000: episode: 62, duration: 0.302s, episode steps:  26, steps per second:  86, episode reward: 40.653, mean reward:  1.564 [-2.343, 32.160], mean action: 3.923 [0.000, 14.000],  loss: 0.019085, mae: 0.458562, mean_q: 0.401224, mean_eps: 0.000000
 1736/5000: episode: 63, duration: 0.187s, episode steps:  15, steps per second:  80, episode reward: 47.025, mean reward:  3.135 [-0.266, 33.000], mean action: 0.067 [0.000, 1.000],  loss: 0.017849, mae: 0.447622, mean_q: 0.429497, mean_eps: 0.000000
 1761/5000: episode: 64, duration: 0.291s, episode steps:  25, steps per second:  86, episode reward: 44.156, mean reward:  1.766 [-2.425, 32.400], mean action: 3.440 [0.000, 19.000],  loss: 0.020055, mae: 0.455778, mean_q: 0.447677, mean_eps: 0.000000
 1784/5000: episode: 65, duration: 0.288s, episode steps:  23, steps per second:  80, episode reward: 39.000, mean reward:  1.696 [-2.392, 29.983], mean action: 4.522 [1.000, 14.000],  loss: 0.019692, mae: 0.457448, mean_q: 0.450856, mean_eps: 0.000000
 1812/5000: episode: 66, duration: 0.322s, episode steps:  28, steps per second:  87, episode reward: 41.693, mean reward:  1.489 [-3.000, 32.320], mean action: 6.429 [1.000, 19.000],  loss: 0.017024, mae: 0.445410, mean_q: 0.396009, mean_eps: 0.000000
 1828/5000: episode: 67, duration: 0.190s, episode steps:  16, steps per second:  84, episode reward: 44.875, mean reward:  2.805 [-2.244, 32.290], mean action: 2.562 [0.000, 19.000],  loss: 0.019845, mae: 0.452407, mean_q: 0.425449, mean_eps: 0.000000
 1873/5000: episode: 68, duration: 0.498s, episode steps:  45, steps per second:  90, episode reward: -32.600, mean reward: -0.724 [-32.086,  2.300], mean action: 6.600 [0.000, 17.000],  loss: 0.020189, mae: 0.445976, mean_q: 0.456395, mean_eps: 0.000000
 1906/5000: episode: 69, duration: 0.370s, episode steps:  33, steps per second:  89, episode reward: 36.000, mean reward:  1.091 [-2.448, 32.380], mean action: 5.364 [0.000, 19.000],  loss: 0.014976, mae: 0.428533, mean_q: 0.412582, mean_eps: 0.000000
 1930/5000: episode: 70, duration: 0.276s, episode steps:  24, steps per second:  87, episode reward: 38.259, mean reward:  1.594 [-2.747, 32.043], mean action: 2.750 [0.000, 11.000],  loss: 0.018267, mae: 0.441613, mean_q: 0.414923, mean_eps: 0.000000
 1971/5000: episode: 71, duration: 0.449s, episode steps:  41, steps per second:  91, episode reward: 38.555, mean reward:  0.940 [-3.000, 32.060], mean action: 1.976 [0.000, 20.000],  loss: 0.020095, mae: 0.452594, mean_q: 0.521105, mean_eps: 0.000000
 2004/5000: episode: 72, duration: 0.377s, episode steps:  33, steps per second:  88, episode reward: 41.208, mean reward:  1.249 [-2.757, 32.583], mean action: 2.212 [0.000, 9.000],  loss: 0.019438, mae: 0.461584, mean_q: 0.442214, mean_eps: 0.000000
 2040/5000: episode: 73, duration: 0.550s, episode steps:  36, steps per second:  65, episode reward: 44.435, mean reward:  1.234 [-2.242, 32.120], mean action: 3.861 [0.000, 19.000],  loss: 0.018225, mae: 0.460633, mean_q: 0.382172, mean_eps: 0.000000
 2063/5000: episode: 74, duration: 0.270s, episode steps:  23, steps per second:  85, episode reward: 41.468, mean reward:  1.803 [-2.413, 31.849], mean action: 6.739 [1.000, 19.000],  loss: 0.018117, mae: 0.453128, mean_q: 0.412636, mean_eps: 0.000000
 2092/5000: episode: 75, duration: 0.393s, episode steps:  29, steps per second:  74, episode reward: 41.230, mean reward:  1.422 [-2.361, 31.653], mean action: 2.759 [1.000, 15.000],  loss: 0.019481, mae: 0.458941, mean_q: 0.433668, mean_eps: 0.000000
 2112/5000: episode: 76, duration: 0.251s, episode steps:  20, steps per second:  80, episode reward: 41.635, mean reward:  2.082 [-2.058, 32.430], mean action: 4.000 [0.000, 15.000],  loss: 0.020871, mae: 0.458784, mean_q: 0.431573, mean_eps: 0.000000
 2135/5000: episode: 77, duration: 0.271s, episode steps:  23, steps per second:  85, episode reward: 41.306, mean reward:  1.796 [-2.910, 32.310], mean action: 3.522 [0.000, 15.000],  loss: 0.019160, mae: 0.459775, mean_q: 0.421586, mean_eps: 0.000000
 2161/5000: episode: 78, duration: 0.298s, episode steps:  26, steps per second:  87, episode reward: 38.337, mean reward:  1.475 [-2.605, 32.280], mean action: 4.346 [0.000, 15.000],  loss: 0.021910, mae: 0.470380, mean_q: 0.484400, mean_eps: 0.000000
 2178/5000: episode: 79, duration: 0.207s, episode steps:  17, steps per second:  82, episode reward: 41.367, mean reward:  2.433 [-2.592, 32.190], mean action: 2.118 [0.000, 15.000],  loss: 0.017547, mae: 0.450481, mean_q: 0.481799, mean_eps: 0.000000
 2194/5000: episode: 80, duration: 0.191s, episode steps:  16, steps per second:  84, episode reward: 47.400, mean reward:  2.962 [-0.079, 32.590], mean action: 2.125 [1.000, 3.000],  loss: 0.020045, mae: 0.464004, mean_q: 0.460172, mean_eps: 0.000000
 2219/5000: episode: 81, duration: 0.294s, episode steps:  25, steps per second:  85, episode reward: 38.198, mean reward:  1.528 [-3.000, 31.898], mean action: 5.200 [0.000, 20.000],  loss: 0.016321, mae: 0.446324, mean_q: 0.418020, mean_eps: 0.000000
 2249/5000: episode: 82, duration: 0.341s, episode steps:  30, steps per second:  88, episode reward: 40.398, mean reward:  1.347 [-2.197, 32.039], mean action: 3.967 [0.000, 15.000],  loss: 0.018766, mae: 0.460778, mean_q: 0.370920, mean_eps: 0.000000
 2289/5000: episode: 83, duration: 0.487s, episode steps:  40, steps per second:  82, episode reward: 39.000, mean reward:  0.975 [-2.342, 32.350], mean action: 2.800 [0.000, 15.000],  loss: 0.018662, mae: 0.450724, mean_q: 0.394398, mean_eps: 0.000000
 2331/5000: episode: 84, duration: 0.467s, episode steps:  42, steps per second:  90, episode reward: 32.664, mean reward:  0.778 [-2.635, 32.180], mean action: 6.762 [0.000, 20.000],  loss: 0.018221, mae: 0.437991, mean_q: 0.445705, mean_eps: 0.000000
 2357/5000: episode: 85, duration: 0.306s, episode steps:  26, steps per second:  85, episode reward: 38.959, mean reward:  1.498 [-2.664, 32.099], mean action: 2.885 [0.000, 18.000],  loss: 0.017807, mae: 0.442570, mean_q: 0.464840, mean_eps: 0.000000
 2374/5000: episode: 86, duration: 0.219s, episode steps:  17, steps per second:  78, episode reward: 47.738, mean reward:  2.808 [ 0.000, 33.000], mean action: 4.588 [0.000, 14.000],  loss: 0.021086, mae: 0.449052, mean_q: 0.504478, mean_eps: 0.000000
 2405/5000: episode: 87, duration: 0.358s, episode steps:  31, steps per second:  87, episode reward: 43.619, mean reward:  1.407 [-1.678, 32.172], mean action: 2.871 [0.000, 14.000],  loss: 0.017086, mae: 0.434815, mean_q: 0.479893, mean_eps: 0.000000
 2429/5000: episode: 88, duration: 0.275s, episode steps:  24, steps per second:  87, episode reward: 38.753, mean reward:  1.615 [-2.407, 32.100], mean action: 3.833 [0.000, 18.000],  loss: 0.018407, mae: 0.439907, mean_q: 0.463100, mean_eps: 0.000000
 2463/5000: episode: 89, duration: 0.391s, episode steps:  34, steps per second:  87, episode reward: 32.753, mean reward:  0.963 [-2.938, 32.080], mean action: 7.353 [0.000, 21.000],  loss: 0.020026, mae: 0.450960, mean_q: 0.464161, mean_eps: 0.000000
 2493/5000: episode: 90, duration: 0.343s, episode steps:  30, steps per second:  88, episode reward: -35.150, mean reward: -1.172 [-32.498,  2.272], mean action: 3.467 [1.000, 11.000],  loss: 0.018509, mae: 0.446055, mean_q: 0.436461, mean_eps: 0.000000
 2521/5000: episode: 91, duration: 0.310s, episode steps:  28, steps per second:  90, episode reward: 38.305, mean reward:  1.368 [-2.784, 31.575], mean action: 3.714 [0.000, 12.000],  loss: 0.018049, mae: 0.444204, mean_q: 0.439323, mean_eps: 0.000000
 2543/5000: episode: 92, duration: 0.257s, episode steps:  22, steps per second:  86, episode reward: 39.000, mean reward:  1.773 [-2.583, 32.260], mean action: 2.091 [0.000, 11.000],  loss: 0.020721, mae: 0.458298, mean_q: 0.489382, mean_eps: 0.000000
 2575/5000: episode: 93, duration: 0.370s, episode steps:  32, steps per second:  87, episode reward: 38.858, mean reward:  1.214 [-2.422, 32.030], mean action: 4.094 [0.000, 21.000],  loss: 0.023195, mae: 0.466090, mean_q: 0.452604, mean_eps: 0.000000
 2617/5000: episode: 94, duration: 0.504s, episode steps:  42, steps per second:  83, episode reward: 34.370, mean reward:  0.818 [-2.861, 31.786], mean action: 4.690 [0.000, 18.000],  loss: 0.021557, mae: 0.448805, mean_q: 0.513906, mean_eps: 0.000000
 2649/5000: episode: 95, duration: 0.370s, episode steps:  32, steps per second:  86, episode reward: 35.430, mean reward:  1.107 [-2.847, 32.370], mean action: 2.906 [0.000, 14.000],  loss: 0.022338, mae: 0.448758, mean_q: 0.502862, mean_eps: 0.000000
 2681/5000: episode: 96, duration: 0.374s, episode steps:  32, steps per second:  86, episode reward: 41.824, mean reward:  1.307 [-2.225, 32.054], mean action: 2.344 [0.000, 9.000],  loss: 0.020064, mae: 0.448913, mean_q: 0.460390, mean_eps: 0.000000
 2746/5000: episode: 97, duration: 0.747s, episode steps:  65, steps per second:  87, episode reward: 32.070, mean reward:  0.493 [-2.452, 32.580], mean action: 8.677 [0.000, 18.000],  loss: 0.017910, mae: 0.442910, mean_q: 0.474574, mean_eps: 0.000000
 2776/5000: episode: 98, duration: 0.356s, episode steps:  30, steps per second:  84, episode reward: 35.614, mean reward:  1.187 [-2.945, 32.724], mean action: 3.533 [0.000, 14.000],  loss: 0.018430, mae: 0.451100, mean_q: 0.472509, mean_eps: 0.000000
 2798/5000: episode: 99, duration: 0.263s, episode steps:  22, steps per second:  84, episode reward: 38.248, mean reward:  1.739 [-2.645, 31.960], mean action: 2.682 [0.000, 20.000],  loss: 0.023829, mae: 0.476962, mean_q: 0.449673, mean_eps: 0.000000
 2818/5000: episode: 100, duration: 0.243s, episode steps:  20, steps per second:  82, episode reward: 35.739, mean reward:  1.787 [-2.902, 32.739], mean action: 4.100 [0.000, 18.000],  loss: 0.021293, mae: 0.455045, mean_q: 0.456518, mean_eps: 0.000000
 2837/5000: episode: 101, duration: 0.228s, episode steps:  19, steps per second:  83, episode reward: 44.682, mean reward:  2.352 [-2.255, 32.282], mean action: 1.842 [0.000, 14.000],  loss: 0.015294, mae: 0.435001, mean_q: 0.423153, mean_eps: 0.000000
 2852/5000: episode: 102, duration: 0.187s, episode steps:  15, steps per second:  80, episode reward: 43.470, mean reward:  2.898 [-2.195, 31.903], mean action: 6.200 [0.000, 14.000],  loss: 0.020477, mae: 0.468404, mean_q: 0.411053, mean_eps: 0.000000
 2887/5000: episode: 103, duration: 0.488s, episode steps:  35, steps per second:  72, episode reward: 42.000, mean reward:  1.200 [-2.306, 32.070], mean action: 2.143 [1.000, 11.000],  loss: 0.018184, mae: 0.445360, mean_q: 0.478143, mean_eps: 0.000000
 2914/5000: episode: 104, duration: 0.314s, episode steps:  27, steps per second:  86, episode reward: 41.627, mean reward:  1.542 [-2.532, 31.846], mean action: 3.037 [0.000, 14.000],  loss: 0.020716, mae: 0.446518, mean_q: 0.472587, mean_eps: 0.000000
 2938/5000: episode: 105, duration: 0.277s, episode steps:  24, steps per second:  86, episode reward: 39.000, mean reward:  1.625 [-2.246, 32.100], mean action: 3.542 [0.000, 14.000],  loss: 0.025036, mae: 0.471201, mean_q: 0.452779, mean_eps: 0.000000
 2968/5000: episode: 106, duration: 0.344s, episode steps:  30, steps per second:  87, episode reward: 41.840, mean reward:  1.395 [-3.000, 32.230], mean action: 3.833 [0.000, 14.000],  loss: 0.025847, mae: 0.464840, mean_q: 0.481996, mean_eps: 0.000000
 2989/5000: episode: 107, duration: 0.245s, episode steps:  21, steps per second:  86, episode reward: 44.124, mean reward:  2.101 [-2.717, 32.370], mean action: 2.143 [0.000, 15.000],  loss: 0.015706, mae: 0.416233, mean_q: 0.466722, mean_eps: 0.000000
 3009/5000: episode: 108, duration: 0.238s, episode steps:  20, steps per second:  84, episode reward: 37.267, mean reward:  1.863 [-3.000, 31.623], mean action: 2.350 [0.000, 15.000],  loss: 0.017025, mae: 0.421752, mean_q: 0.478132, mean_eps: 0.000000
 3037/5000: episode: 109, duration: 0.325s, episode steps:  28, steps per second:  86, episode reward: -36.580, mean reward: -1.306 [-33.000,  2.154], mean action: 6.679 [0.000, 15.000],  loss: 0.018890, mae: 0.440726, mean_q: 0.473275, mean_eps: 0.000000
 3064/5000: episode: 110, duration: 0.308s, episode steps:  27, steps per second:  88, episode reward: 38.354, mean reward:  1.421 [-3.000, 32.060], mean action: 2.481 [0.000, 11.000],  loss: 0.021884, mae: 0.460465, mean_q: 0.468555, mean_eps: 0.000000
 3080/5000: episode: 111, duration: 0.190s, episode steps:  16, steps per second:  84, episode reward: 44.428, mean reward:  2.777 [-3.000, 32.510], mean action: 2.438 [1.000, 9.000],  loss: 0.020133, mae: 0.441701, mean_q: 0.500604, mean_eps: 0.000000
 3103/5000: episode: 112, duration: 0.266s, episode steps:  23, steps per second:  86, episode reward: 44.545, mean reward:  1.937 [-2.330, 32.140], mean action: 5.087 [0.000, 14.000],  loss: 0.018479, mae: 0.441509, mean_q: 0.453863, mean_eps: 0.000000
 3128/5000: episode: 113, duration: 0.359s, episode steps:  25, steps per second:  70, episode reward: 41.514, mean reward:  1.661 [-2.151, 31.754], mean action: 1.600 [0.000, 15.000],  loss: 0.019242, mae: 0.443295, mean_q: 0.480281, mean_eps: 0.000000
 3159/5000: episode: 114, duration: 0.420s, episode steps:  31, steps per second:  74, episode reward: 41.808, mean reward:  1.349 [-2.456, 32.190], mean action: 3.774 [1.000, 14.000],  loss: 0.018849, mae: 0.451195, mean_q: 0.440956, mean_eps: 0.000000
 3191/5000: episode: 115, duration: 0.507s, episode steps:  32, steps per second:  63, episode reward: 36.000, mean reward:  1.125 [-2.649, 32.340], mean action: 3.781 [0.000, 15.000],  loss: 0.016492, mae: 0.445700, mean_q: 0.459448, mean_eps: 0.000000
 3209/5000: episode: 116, duration: 0.223s, episode steps:  18, steps per second:  81, episode reward: 44.030, mean reward:  2.446 [-2.333, 32.143], mean action: 0.889 [0.000, 9.000],  loss: 0.023213, mae: 0.458752, mean_q: 0.487195, mean_eps: 0.000000
 3229/5000: episode: 117, duration: 0.279s, episode steps:  20, steps per second:  72, episode reward: 44.963, mean reward:  2.248 [-2.297, 32.093], mean action: 1.000 [0.000, 9.000],  loss: 0.019144, mae: 0.442961, mean_q: 0.534196, mean_eps: 0.000000
 3261/5000: episode: 118, duration: 0.679s, episode steps:  32, steps per second:  47, episode reward: 34.715, mean reward:  1.085 [-2.821, 31.793], mean action: 4.812 [0.000, 14.000],  loss: 0.019176, mae: 0.436766, mean_q: 0.538602, mean_eps: 0.000000
 3298/5000: episode: 119, duration: 0.429s, episode steps:  37, steps per second:  86, episode reward: 37.584, mean reward:  1.016 [-3.000, 31.646], mean action: 7.027 [0.000, 21.000],  loss: 0.017508, mae: 0.425736, mean_q: 0.479444, mean_eps: 0.000000
 3316/5000: episode: 120, duration: 0.356s, episode steps:  18, steps per second:  50, episode reward: 44.184, mean reward:  2.455 [-2.405, 32.009], mean action: 2.444 [0.000, 15.000],  loss: 0.016138, mae: 0.414948, mean_q: 0.470033, mean_eps: 0.000000
 3349/5000: episode: 121, duration: 0.466s, episode steps:  33, steps per second:  71, episode reward: 41.141, mean reward:  1.247 [-2.574, 32.710], mean action: 8.576 [0.000, 15.000],  loss: 0.019139, mae: 0.440259, mean_q: 0.467592, mean_eps: 0.000000
 3388/5000: episode: 122, duration: 0.439s, episode steps:  39, steps per second:  89, episode reward: 35.813, mean reward:  0.918 [-3.000, 32.345], mean action: 8.128 [0.000, 15.000],  loss: 0.020744, mae: 0.454270, mean_q: 0.406374, mean_eps: 0.000000
 3406/5000: episode: 123, duration: 0.220s, episode steps:  18, steps per second:  82, episode reward: 47.220, mean reward:  2.623 [-0.899, 32.440], mean action: 3.000 [0.000, 15.000],  loss: 0.021332, mae: 0.460061, mean_q: 0.450156, mean_eps: 0.000000
 3421/5000: episode: 124, duration: 0.191s, episode steps:  15, steps per second:  78, episode reward: 47.881, mean reward:  3.192 [ 0.000, 32.310], mean action: 1.933 [1.000, 3.000],  loss: 0.017798, mae: 0.434802, mean_q: 0.475941, mean_eps: 0.000000
 3453/5000: episode: 125, duration: 0.361s, episode steps:  32, steps per second:  89, episode reward: 42.000, mean reward:  1.312 [-3.000, 32.800], mean action: 2.062 [0.000, 12.000],  loss: 0.018325, mae: 0.438824, mean_q: 0.489300, mean_eps: 0.000000
 3493/5000: episode: 126, duration: 0.459s, episode steps:  40, steps per second:  87, episode reward: 44.490, mean reward:  1.112 [-2.294, 32.190], mean action: 3.125 [1.000, 11.000],  loss: 0.020104, mae: 0.444695, mean_q: 0.449179, mean_eps: 0.000000
 3528/5000: episode: 127, duration: 0.401s, episode steps:  35, steps per second:  87, episode reward: 38.172, mean reward:  1.091 [-2.231, 32.029], mean action: 2.543 [0.000, 14.000],  loss: 0.020590, mae: 0.437701, mean_q: 0.474765, mean_eps: 0.000000
 3554/5000: episode: 128, duration: 0.319s, episode steps:  26, steps per second:  82, episode reward: 39.536, mean reward:  1.521 [-3.000, 32.013], mean action: 3.923 [0.000, 15.000],  loss: 0.018647, mae: 0.436065, mean_q: 0.478393, mean_eps: 0.000000
 3576/5000: episode: 129, duration: 0.254s, episode steps:  22, steps per second:  86, episode reward: 40.767, mean reward:  1.853 [-2.610, 32.399], mean action: 2.727 [0.000, 19.000],  loss: 0.020773, mae: 0.441503, mean_q: 0.492358, mean_eps: 0.000000
 3601/5000: episode: 130, duration: 0.358s, episode steps:  25, steps per second:  70, episode reward: 44.287, mean reward:  1.771 [-2.051, 32.136], mean action: 3.120 [0.000, 19.000],  loss: 0.022499, mae: 0.456480, mean_q: 0.478088, mean_eps: 0.000000
 3622/5000: episode: 131, duration: 0.333s, episode steps:  21, steps per second:  63, episode reward: 41.770, mean reward:  1.989 [-2.996, 32.053], mean action: 5.619 [0.000, 20.000],  loss: 0.024564, mae: 0.465674, mean_q: 0.476721, mean_eps: 0.000000
 3638/5000: episode: 132, duration: 0.323s, episode steps:  16, steps per second:  50, episode reward: 44.726, mean reward:  2.795 [-2.096, 32.298], mean action: 1.688 [0.000, 19.000],  loss: 0.021817, mae: 0.451589, mean_q: 0.455279, mean_eps: 0.000000
 3666/5000: episode: 133, duration: 0.363s, episode steps:  28, steps per second:  77, episode reward: 42.000, mean reward:  1.500 [-2.581, 32.080], mean action: 1.464 [0.000, 11.000],  loss: 0.018490, mae: 0.443561, mean_q: 0.457472, mean_eps: 0.000000
 3687/5000: episode: 134, duration: 0.275s, episode steps:  21, steps per second:  76, episode reward: 44.405, mean reward:  2.115 [-2.057, 31.929], mean action: 1.762 [0.000, 15.000],  loss: 0.017585, mae: 0.441505, mean_q: 0.481852, mean_eps: 0.000000
 3718/5000: episode: 135, duration: 0.402s, episode steps:  31, steps per second:  77, episode reward: 38.174, mean reward:  1.231 [-2.665, 32.230], mean action: 3.968 [0.000, 19.000],  loss: 0.021008, mae: 0.453561, mean_q: 0.469234, mean_eps: 0.000000
 3798/5000: episode: 136, duration: 1.633s, episode steps:  80, steps per second:  49, episode reward: -33.000, mean reward: -0.412 [-32.289,  2.624], mean action: 5.700 [0.000, 15.000],  loss: 0.018069, mae: 0.442208, mean_q: 0.418562, mean_eps: 0.000000
 3818/5000: episode: 137, duration: 0.293s, episode steps:  20, steps per second:  68, episode reward: 41.931, mean reward:  2.097 [-3.000, 32.155], mean action: 3.400 [0.000, 21.000],  loss: 0.016472, mae: 0.425370, mean_q: 0.445512, mean_eps: 0.000000
 3852/5000: episode: 138, duration: 0.490s, episode steps:  34, steps per second:  69, episode reward: 46.357, mean reward:  1.363 [-0.155, 32.233], mean action: 4.059 [0.000, 15.000],  loss: 0.019202, mae: 0.440464, mean_q: 0.435337, mean_eps: 0.000000
 3885/5000: episode: 139, duration: 0.367s, episode steps:  33, steps per second:  90, episode reward: 38.502, mean reward:  1.167 [-3.000, 31.950], mean action: 3.667 [0.000, 15.000],  loss: 0.019634, mae: 0.452384, mean_q: 0.435813, mean_eps: 0.000000
 3903/5000: episode: 140, duration: 0.218s, episode steps:  18, steps per second:  82, episode reward: 41.335, mean reward:  2.296 [-3.000, 32.330], mean action: 2.889 [0.000, 11.000],  loss: 0.021659, mae: 0.452087, mean_q: 0.495994, mean_eps: 0.000000
 3930/5000: episode: 141, duration: 0.317s, episode steps:  27, steps per second:  85, episode reward: 38.082, mean reward:  1.410 [-2.696, 31.645], mean action: 4.444 [0.000, 21.000],  loss: 0.020117, mae: 0.433941, mean_q: 0.447735, mean_eps: 0.000000
 3974/5000: episode: 142, duration: 0.494s, episode steps:  44, steps per second:  89, episode reward: 32.563, mean reward:  0.740 [-3.000, 31.703], mean action: 6.659 [0.000, 17.000],  loss: 0.023520, mae: 0.449943, mean_q: 0.471759, mean_eps: 0.000000
 3996/5000: episode: 143, duration: 0.249s, episode steps:  22, steps per second:  88, episode reward: 42.000, mean reward:  1.909 [-2.372, 32.490], mean action: 4.364 [0.000, 14.000],  loss: 0.023596, mae: 0.455103, mean_q: 0.443665, mean_eps: 0.000000
 4028/5000: episode: 144, duration: 0.386s, episode steps:  32, steps per second:  83, episode reward: 40.188, mean reward:  1.256 [-2.263, 32.241], mean action: 5.062 [0.000, 14.000],  loss: 0.020771, mae: 0.451361, mean_q: 0.443385, mean_eps: 0.000000
 4063/5000: episode: 145, duration: 0.393s, episode steps:  35, steps per second:  89, episode reward: 46.376, mean reward:  1.325 [-0.470, 32.183], mean action: 2.143 [0.000, 14.000],  loss: 0.019416, mae: 0.441752, mean_q: 0.427914, mean_eps: 0.000000
 4093/5000: episode: 146, duration: 0.341s, episode steps:  30, steps per second:  88, episode reward: 44.097, mean reward:  1.470 [-2.264, 32.671], mean action: 3.233 [0.000, 14.000],  loss: 0.020905, mae: 0.442584, mean_q: 0.457991, mean_eps: 0.000000
 4128/5000: episode: 147, duration: 0.405s, episode steps:  35, steps per second:  86, episode reward: 41.939, mean reward:  1.198 [-2.478, 32.220], mean action: 1.457 [0.000, 9.000],  loss: 0.018716, mae: 0.437414, mean_q: 0.441657, mean_eps: 0.000000
 4157/5000: episode: 148, duration: 0.326s, episode steps:  29, steps per second:  89, episode reward: 38.567, mean reward:  1.330 [-2.374, 32.200], mean action: 3.793 [0.000, 14.000],  loss: 0.023065, mae: 0.464318, mean_q: 0.437352, mean_eps: 0.000000
 4172/5000: episode: 149, duration: 0.180s, episode steps:  15, steps per second:  83, episode reward: 44.825, mean reward:  2.988 [-2.510, 32.300], mean action: 3.133 [0.000, 14.000],  loss: 0.024910, mae: 0.472803, mean_q: 0.466968, mean_eps: 0.000000
 4189/5000: episode: 150, duration: 0.240s, episode steps:  17, steps per second:  71, episode reward: 41.901, mean reward:  2.465 [-2.163, 33.000], mean action: 3.529 [0.000, 9.000],  loss: 0.014995, mae: 0.421442, mean_q: 0.492670, mean_eps: 0.000000
 4222/5000: episode: 151, duration: 0.387s, episode steps:  33, steps per second:  85, episode reward: 38.203, mean reward:  1.158 [-2.204, 32.610], mean action: 4.455 [0.000, 19.000],  loss: 0.020818, mae: 0.440124, mean_q: 0.534629, mean_eps: 0.000000
 4247/5000: episode: 152, duration: 0.296s, episode steps:  25, steps per second:  84, episode reward: 43.600, mean reward:  1.744 [-2.072, 32.420], mean action: 3.680 [0.000, 20.000],  loss: 0.023239, mae: 0.446243, mean_q: 0.508285, mean_eps: 0.000000
 4271/5000: episode: 153, duration: 0.275s, episode steps:  24, steps per second:  87, episode reward: 41.376, mean reward:  1.724 [-3.000, 32.210], mean action: 2.500 [0.000, 9.000],  loss: 0.021166, mae: 0.447451, mean_q: 0.445520, mean_eps: 0.000000
 4305/5000: episode: 154, duration: 0.390s, episode steps:  34, steps per second:  87, episode reward: 41.435, mean reward:  1.219 [-2.399, 32.130], mean action: 2.588 [0.000, 12.000],  loss: 0.020742, mae: 0.444941, mean_q: 0.453227, mean_eps: 0.000000
 4329/5000: episode: 155, duration: 0.304s, episode steps:  24, steps per second:  79, episode reward: 35.583, mean reward:  1.483 [-3.000, 32.010], mean action: 5.000 [0.000, 21.000],  loss: 0.017126, mae: 0.435086, mean_q: 0.425033, mean_eps: 0.000000
 4356/5000: episode: 156, duration: 0.324s, episode steps:  27, steps per second:  83, episode reward: 38.546, mean reward:  1.428 [-2.375, 31.566], mean action: 2.667 [0.000, 19.000],  loss: 0.025113, mae: 0.465904, mean_q: 0.472991, mean_eps: 0.000000
 4394/5000: episode: 157, duration: 0.447s, episode steps:  38, steps per second:  85, episode reward: 38.113, mean reward:  1.003 [-2.338, 32.430], mean action: 5.579 [0.000, 19.000],  loss: 0.020045, mae: 0.433536, mean_q: 0.526449, mean_eps: 0.000000
 4416/5000: episode: 158, duration: 0.373s, episode steps:  22, steps per second:  59, episode reward: -32.240, mean reward: -1.465 [-32.520,  2.734], mean action: 4.182 [0.000, 12.000],  loss: 0.016851, mae: 0.423037, mean_q: 0.460017, mean_eps: 0.000000
 4433/5000: episode: 159, duration: 0.214s, episode steps:  17, steps per second:  80, episode reward: 47.296, mean reward:  2.782 [-0.060, 32.450], mean action: 2.882 [1.000, 21.000],  loss: 0.018267, mae: 0.423319, mean_q: 0.452811, mean_eps: 0.000000
 4457/5000: episode: 160, duration: 0.272s, episode steps:  24, steps per second:  88, episode reward: 41.753, mean reward:  1.740 [-2.586, 32.090], mean action: 3.667 [0.000, 14.000],  loss: 0.018855, mae: 0.414078, mean_q: 0.477746, mean_eps: 0.000000
 4503/5000: episode: 161, duration: 0.579s, episode steps:  46, steps per second:  79, episode reward: 40.611, mean reward:  0.883 [-2.129, 33.000], mean action: 2.870 [0.000, 19.000],  loss: 0.019742, mae: 0.418389, mean_q: 0.499344, mean_eps: 0.000000
 4525/5000: episode: 162, duration: 0.255s, episode steps:  22, steps per second:  86, episode reward: 41.193, mean reward:  1.872 [-2.791, 32.140], mean action: 3.955 [1.000, 19.000],  loss: 0.021451, mae: 0.439000, mean_q: 0.459334, mean_eps: 0.000000
 4548/5000: episode: 163, duration: 0.270s, episode steps:  23, steps per second:  85, episode reward: 41.574, mean reward:  1.808 [-2.800, 32.170], mean action: 2.609 [0.000, 19.000],  loss: 0.021692, mae: 0.442782, mean_q: 0.452189, mean_eps: 0.000000
 4568/5000: episode: 164, duration: 0.241s, episode steps:  20, steps per second:  83, episode reward: 44.752, mean reward:  2.238 [-2.231, 32.490], mean action: 1.550 [0.000, 15.000],  loss: 0.019025, mae: 0.433693, mean_q: 0.472186, mean_eps: 0.000000
 4594/5000: episode: 165, duration: 0.307s, episode steps:  26, steps per second:  85, episode reward: 39.000, mean reward:  1.500 [-2.840, 32.320], mean action: 3.038 [0.000, 15.000],  loss: 0.020857, mae: 0.440772, mean_q: 0.476507, mean_eps: 0.000000
 4628/5000: episode: 166, duration: 0.390s, episode steps:  34, steps per second:  87, episode reward: 40.305, mean reward:  1.185 [-2.471, 32.038], mean action: 7.941 [0.000, 21.000],  loss: 0.017615, mae: 0.426160, mean_q: 0.385340, mean_eps: 0.000000
 4645/5000: episode: 167, duration: 0.206s, episode steps:  17, steps per second:  82, episode reward: 39.000, mean reward:  2.294 [-2.519, 30.086], mean action: 3.000 [0.000, 15.000],  loss: 0.019436, mae: 0.453882, mean_q: 0.367596, mean_eps: 0.000000
 4670/5000: episode: 168, duration: 0.292s, episode steps:  25, steps per second:  86, episode reward: 38.614, mean reward:  1.545 [-3.000, 32.430], mean action: 4.040 [0.000, 15.000],  loss: 0.019028, mae: 0.434763, mean_q: 0.415248, mean_eps: 0.000000
 4689/5000: episode: 169, duration: 0.224s, episode steps:  19, steps per second:  85, episode reward: 44.549, mean reward:  2.345 [-2.423, 31.775], mean action: 2.105 [0.000, 11.000],  loss: 0.018497, mae: 0.428188, mean_q: 0.405379, mean_eps: 0.000000
 4712/5000: episode: 170, duration: 0.266s, episode steps:  23, steps per second:  87, episode reward: 47.342, mean reward:  2.058 [-0.252, 32.905], mean action: 1.348 [0.000, 3.000],  loss: 0.016251, mae: 0.423004, mean_q: 0.422351, mean_eps: 0.000000
 4726/5000: episode: 171, duration: 0.257s, episode steps:  14, steps per second:  55, episode reward: 44.261, mean reward:  3.162 [-2.101, 32.040], mean action: 3.286 [0.000, 11.000],  loss: 0.022958, mae: 0.450318, mean_q: 0.456479, mean_eps: 0.000000
 4751/5000: episode: 172, duration: 0.352s, episode steps:  25, steps per second:  71, episode reward: 38.468, mean reward:  1.539 [-2.664, 31.907], mean action: 3.000 [0.000, 19.000],  loss: 0.020276, mae: 0.443770, mean_q: 0.405522, mean_eps: 0.000000
 4782/5000: episode: 173, duration: 0.411s, episode steps:  31, steps per second:  75, episode reward: 40.238, mean reward:  1.298 [-2.459, 31.978], mean action: 4.097 [0.000, 16.000],  loss: 0.020259, mae: 0.446699, mean_q: 0.419890, mean_eps: 0.000000
 4812/5000: episode: 174, duration: 0.338s, episode steps:  30, steps per second:  89, episode reward: 37.989, mean reward:  1.266 [-3.000, 32.440], mean action: 7.133 [0.000, 20.000],  loss: 0.021847, mae: 0.454064, mean_q: 0.398613, mean_eps: 0.000000
 4831/5000: episode: 175, duration: 0.220s, episode steps:  19, steps per second:  86, episode reward: 45.000, mean reward:  2.368 [-2.195, 32.070], mean action: 4.579 [0.000, 20.000],  loss: 0.021468, mae: 0.448464, mean_q: 0.428557, mean_eps: 0.000000
 4854/5000: episode: 176, duration: 0.269s, episode steps:  23, steps per second:  85, episode reward: 41.185, mean reward:  1.791 [-2.716, 32.180], mean action: 4.522 [0.000, 20.000],  loss: 0.022889, mae: 0.444964, mean_q: 0.507145, mean_eps: 0.000000
 4885/5000: episode: 177, duration: 0.345s, episode steps:  31, steps per second:  90, episode reward: 38.845, mean reward:  1.253 [-3.000, 32.030], mean action: 3.097 [0.000, 19.000],  loss: 0.020917, mae: 0.439969, mean_q: 0.503922, mean_eps: 0.000000
 4911/5000: episode: 178, duration: 0.297s, episode steps:  26, steps per second:  88, episode reward: 41.876, mean reward:  1.611 [-2.568, 32.810], mean action: 2.385 [0.000, 14.000],  loss: 0.019279, mae: 0.442248, mean_q: 0.475184, mean_eps: 0.000000
 4938/5000: episode: 179, duration: 0.311s, episode steps:  27, steps per second:  87, episode reward: 35.577, mean reward:  1.318 [-3.000, 32.260], mean action: 5.704 [0.000, 14.000],  loss: 0.017873, mae: 0.435910, mean_q: 0.409354, mean_eps: 0.000000
 4972/5000: episode: 180, duration: 0.376s, episode steps:  34, steps per second:  90, episode reward: 35.609, mean reward:  1.047 [-3.000, 31.914], mean action: 3.529 [0.000, 21.000],  loss: 0.022380, mae: 0.444378, mean_q: 0.472913, mean_eps: 0.000000
 4992/5000: episode: 181, duration: 0.231s, episode steps:  20, steps per second:  87, episode reward: 38.148, mean reward:  1.907 [-2.529, 31.821], mean action: 5.800 [0.000, 19.000],  loss: 0.020235, mae: 0.441750, mean_q: 0.463249, mean_eps: 0.000000
done, took 56.368 seconds
DQN Evaluation: 1706 victories out of 2027 episodes
Training for 5000 steps ...
   20/5000: episode: 1, duration: 0.140s, episode steps:  20, steps per second: 142, episode reward: 41.295, mean reward:  2.065 [-2.460, 32.300], mean action: 2.750 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   44/5000: episode: 2, duration: 0.157s, episode steps:  24, steps per second: 153, episode reward: 33.000, mean reward:  1.375 [-2.544, 32.020], mean action: 5.083 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   65/5000: episode: 3, duration: 0.143s, episode steps:  21, steps per second: 147, episode reward: 40.568, mean reward:  1.932 [-2.260, 31.726], mean action: 3.333 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   94/5000: episode: 4, duration: 0.187s, episode steps:  29, steps per second: 155, episode reward: -33.000, mean reward: -1.138 [-32.222,  2.698], mean action: 6.690 [0.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  122/5000: episode: 5, duration: 0.181s, episode steps:  28, steps per second: 155, episode reward: 32.686, mean reward:  1.167 [-2.493, 32.366], mean action: 8.429 [1.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  150/5000: episode: 6, duration: 0.171s, episode steps:  28, steps per second: 163, episode reward: 33.000, mean reward:  1.179 [-2.495, 32.170], mean action: 3.250 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  168/5000: episode: 7, duration: 0.120s, episode steps:  18, steps per second: 150, episode reward: 38.676, mean reward:  2.149 [-2.523, 32.250], mean action: 1.889 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  195/5000: episode: 8, duration: 0.168s, episode steps:  27, steps per second: 160, episode reward: -36.000, mean reward: -1.333 [-32.007,  2.232], mean action: 9.000 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  215/5000: episode: 9, duration: 0.132s, episode steps:  20, steps per second: 151, episode reward: 40.832, mean reward:  2.042 [-2.196, 32.134], mean action: 2.950 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  240/5000: episode: 10, duration: 0.165s, episode steps:  25, steps per second: 151, episode reward: 38.487, mean reward:  1.539 [-2.493, 32.002], mean action: 4.400 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  262/5000: episode: 11, duration: 0.150s, episode steps:  22, steps per second: 147, episode reward: 38.571, mean reward:  1.753 [-3.000, 31.977], mean action: 2.727 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  285/5000: episode: 12, duration: 0.152s, episode steps:  23, steps per second: 152, episode reward: 30.000, mean reward:  1.304 [-3.000, 30.198], mean action: 7.043 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  307/5000: episode: 13, duration: 0.139s, episode steps:  22, steps per second: 158, episode reward: -35.190, mean reward: -1.600 [-32.220,  2.410], mean action: 5.909 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  329/5000: episode: 14, duration: 0.139s, episode steps:  22, steps per second: 158, episode reward: 41.296, mean reward:  1.877 [-3.000, 32.400], mean action: 3.636 [1.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  358/5000: episode: 15, duration: 0.181s, episode steps:  29, steps per second: 160, episode reward: 40.593, mean reward:  1.400 [-2.484, 32.310], mean action: 3.276 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  383/5000: episode: 16, duration: 0.155s, episode steps:  25, steps per second: 161, episode reward: -35.540, mean reward: -1.422 [-32.415,  3.000], mean action: 3.400 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  407/5000: episode: 17, duration: 0.157s, episode steps:  24, steps per second: 153, episode reward: 37.761, mean reward:  1.573 [-2.276, 32.545], mean action: 3.542 [1.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  426/5000: episode: 18, duration: 0.126s, episode steps:  19, steps per second: 151, episode reward: 36.000, mean reward:  1.895 [-3.000, 32.370], mean action: 4.684 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  447/5000: episode: 19, duration: 0.144s, episode steps:  21, steps per second: 146, episode reward: 35.756, mean reward:  1.703 [-3.000, 32.125], mean action: 6.619 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  473/5000: episode: 20, duration: 0.157s, episode steps:  26, steps per second: 165, episode reward: 32.890, mean reward:  1.265 [-2.570, 32.000], mean action: 4.692 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  510/5000: episode: 21, duration: 0.229s, episode steps:  37, steps per second: 161, episode reward: 32.065, mean reward:  0.867 [-3.000, 32.041], mean action: 8.838 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  540/5000: episode: 22, duration: 0.212s, episode steps:  30, steps per second: 142, episode reward: -32.010, mean reward: -1.067 [-32.081,  2.910], mean action: 8.133 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  556/5000: episode: 23, duration: 0.109s, episode steps:  16, steps per second: 147, episode reward: 35.802, mean reward:  2.238 [-2.756, 32.072], mean action: 4.500 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  593/5000: episode: 24, duration: 0.225s, episode steps:  37, steps per second: 165, episode reward: 38.125, mean reward:  1.030 [-3.000, 32.070], mean action: 2.892 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  622/5000: episode: 25, duration: 0.170s, episode steps:  29, steps per second: 171, episode reward: -41.080, mean reward: -1.417 [-32.014,  2.059], mean action: 8.828 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  660/5000: episode: 26, duration: 0.230s, episode steps:  38, steps per second: 165, episode reward: 38.139, mean reward:  1.004 [-2.426, 32.340], mean action: 1.816 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  693/5000: episode: 27, duration: 0.195s, episode steps:  33, steps per second: 169, episode reward: -35.840, mean reward: -1.086 [-32.111,  2.818], mean action: 6.606 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  722/5000: episode: 28, duration: 0.186s, episode steps:  29, steps per second: 156, episode reward: -32.790, mean reward: -1.131 [-32.088,  2.510], mean action: 6.241 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  741/5000: episode: 29, duration: 0.122s, episode steps:  19, steps per second: 156, episode reward: 35.541, mean reward:  1.871 [-2.334, 32.041], mean action: 4.684 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  758/5000: episode: 30, duration: 0.105s, episode steps:  17, steps per second: 161, episode reward: -37.650, mean reward: -2.215 [-33.000,  2.341], mean action: 6.353 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  778/5000: episode: 31, duration: 0.125s, episode steps:  20, steps per second: 160, episode reward: -32.740, mean reward: -1.637 [-32.098,  3.000], mean action: 3.900 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  810/5000: episode: 32, duration: 0.197s, episode steps:  32, steps per second: 163, episode reward: 38.743, mean reward:  1.211 [-2.559, 32.593], mean action: 2.656 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  834/5000: episode: 33, duration: 0.150s, episode steps:  24, steps per second: 160, episode reward: -32.440, mean reward: -1.352 [-31.995,  2.350], mean action: 5.375 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  861/5000: episode: 34, duration: 0.163s, episode steps:  27, steps per second: 166, episode reward: -32.600, mean reward: -1.207 [-31.966,  2.560], mean action: 7.148 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  878/5000: episode: 35, duration: 0.111s, episode steps:  17, steps per second: 153, episode reward: 41.230, mean reward:  2.425 [-2.692, 32.181], mean action: 2.235 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  905/5000: episode: 36, duration: 0.169s, episode steps:  27, steps per second: 160, episode reward: -33.000, mean reward: -1.222 [-32.424,  2.420], mean action: 8.000 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  954/5000: episode: 37, duration: 0.276s, episode steps:  49, steps per second: 178, episode reward: 40.978, mean reward:  0.836 [-2.438, 32.161], mean action: 2.347 [1.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  976/5000: episode: 38, duration: 0.141s, episode steps:  22, steps per second: 156, episode reward: 39.873, mean reward:  1.812 [-2.262, 32.204], mean action: 6.409 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1009/5000: episode: 39, duration: 0.250s, episode steps:  33, steps per second: 132, episode reward: 35.515, mean reward:  1.076 [-3.000, 32.120], mean action: 3.667 [0.000, 15.000],  loss: 0.024797, mae: 0.460695, mean_q: 0.511225, mean_eps: 0.000000
 1056/5000: episode: 40, duration: 0.520s, episode steps:  47, steps per second:  90, episode reward: -32.390, mean reward: -0.689 [-32.175,  2.574], mean action: 5.170 [0.000, 19.000],  loss: 0.020032, mae: 0.453148, mean_q: 0.420889, mean_eps: 0.000000
 1073/5000: episode: 41, duration: 0.202s, episode steps:  17, steps per second:  84, episode reward: 38.415, mean reward:  2.260 [-2.427, 32.140], mean action: 4.412 [0.000, 13.000],  loss: 0.017291, mae: 0.444350, mean_q: 0.426733, mean_eps: 0.000000
 1095/5000: episode: 42, duration: 0.249s, episode steps:  22, steps per second:  88, episode reward: -36.000, mean reward: -1.636 [-32.548,  2.210], mean action: 4.318 [0.000, 17.000],  loss: 0.018344, mae: 0.440645, mean_q: 0.424312, mean_eps: 0.000000
 1117/5000: episode: 43, duration: 0.255s, episode steps:  22, steps per second:  86, episode reward: 34.882, mean reward:  1.586 [-3.000, 32.220], mean action: 5.773 [0.000, 14.000],  loss: 0.023883, mae: 0.477627, mean_q: 0.412390, mean_eps: 0.000000
 1139/5000: episode: 44, duration: 0.250s, episode steps:  22, steps per second:  88, episode reward: 35.606, mean reward:  1.618 [-2.517, 33.000], mean action: 4.182 [0.000, 15.000],  loss: 0.027925, mae: 0.498869, mean_q: 0.388726, mean_eps: 0.000000
 1167/5000: episode: 45, duration: 0.346s, episode steps:  28, steps per second:  81, episode reward: 38.109, mean reward:  1.361 [-2.424, 32.275], mean action: 7.214 [1.000, 14.000],  loss: 0.018589, mae: 0.449960, mean_q: 0.442543, mean_eps: 0.000000
 1187/5000: episode: 46, duration: 0.234s, episode steps:  20, steps per second:  85, episode reward: 41.038, mean reward:  2.052 [-2.514, 31.691], mean action: 5.100 [0.000, 20.000],  loss: 0.018728, mae: 0.443939, mean_q: 0.496146, mean_eps: 0.000000
 1245/5000: episode: 47, duration: 0.640s, episode steps:  58, steps per second:  91, episode reward: 41.466, mean reward:  0.715 [-2.169, 32.590], mean action: 2.500 [0.000, 21.000],  loss: 0.018692, mae: 0.451811, mean_q: 0.455265, mean_eps: 0.000000
 1263/5000: episode: 48, duration: 0.212s, episode steps:  18, steps per second:  85, episode reward: 41.928, mean reward:  2.329 [-2.675, 32.350], mean action: 4.333 [0.000, 15.000],  loss: 0.021927, mae: 0.450191, mean_q: 0.507139, mean_eps: 0.000000
 1283/5000: episode: 49, duration: 0.236s, episode steps:  20, steps per second:  85, episode reward: 38.243, mean reward:  1.912 [-2.381, 31.888], mean action: 7.100 [1.000, 21.000],  loss: 0.019783, mae: 0.449830, mean_q: 0.484375, mean_eps: 0.000000
 1299/5000: episode: 50, duration: 0.195s, episode steps:  16, steps per second:  82, episode reward: 41.373, mean reward:  2.586 [-2.065, 33.000], mean action: 4.000 [0.000, 15.000],  loss: 0.020482, mae: 0.448037, mean_q: 0.490702, mean_eps: 0.000000
 1313/5000: episode: 51, duration: 0.170s, episode steps:  14, steps per second:  82, episode reward: 45.000, mean reward:  3.214 [-2.574, 32.780], mean action: 3.357 [1.000, 12.000],  loss: 0.020216, mae: 0.446513, mean_q: 0.506017, mean_eps: 0.000000
 1335/5000: episode: 52, duration: 0.256s, episode steps:  22, steps per second:  86, episode reward: 41.370, mean reward:  1.880 [-2.128, 32.450], mean action: 4.955 [0.000, 19.000],  loss: 0.014902, mae: 0.424901, mean_q: 0.438221, mean_eps: 0.000000
 1359/5000: episode: 53, duration: 0.274s, episode steps:  24, steps per second:  87, episode reward: 34.940, mean reward:  1.456 [-2.915, 32.321], mean action: 7.833 [0.000, 20.000],  loss: 0.018665, mae: 0.436847, mean_q: 0.468821, mean_eps: 0.000000
 1380/5000: episode: 54, duration: 0.246s, episode steps:  21, steps per second:  85, episode reward: 38.454, mean reward:  1.831 [-2.374, 32.490], mean action: 4.000 [0.000, 20.000],  loss: 0.019575, mae: 0.451795, mean_q: 0.469176, mean_eps: 0.000000
 1399/5000: episode: 55, duration: 0.221s, episode steps:  19, steps per second:  86, episode reward: 38.069, mean reward:  2.004 [-2.552, 33.000], mean action: 3.474 [0.000, 12.000],  loss: 0.019539, mae: 0.449611, mean_q: 0.473086, mean_eps: 0.000000
 1416/5000: episode: 56, duration: 0.201s, episode steps:  17, steps per second:  85, episode reward: 41.382, mean reward:  2.434 [-2.685, 32.170], mean action: 2.765 [1.000, 11.000],  loss: 0.021586, mae: 0.456640, mean_q: 0.457824, mean_eps: 0.000000
 1447/5000: episode: 57, duration: 0.346s, episode steps:  31, steps per second:  90, episode reward: 33.000, mean reward:  1.065 [-2.416, 33.000], mean action: 3.677 [0.000, 11.000],  loss: 0.019237, mae: 0.448997, mean_q: 0.439157, mean_eps: 0.000000
 1473/5000: episode: 58, duration: 0.297s, episode steps:  26, steps per second:  88, episode reward: 32.843, mean reward:  1.263 [-3.000, 32.043], mean action: 3.269 [0.000, 12.000],  loss: 0.017560, mae: 0.443777, mean_q: 0.444513, mean_eps: 0.000000
 1493/5000: episode: 59, duration: 0.231s, episode steps:  20, steps per second:  86, episode reward: 44.822, mean reward:  2.241 [-2.057, 32.235], mean action: 2.500 [1.000, 11.000],  loss: 0.019779, mae: 0.444888, mean_q: 0.450507, mean_eps: 0.000000
 1518/5000: episode: 60, duration: 0.282s, episode steps:  25, steps per second:  89, episode reward: 37.871, mean reward:  1.515 [-3.000, 32.082], mean action: 8.040 [0.000, 14.000],  loss: 0.021064, mae: 0.451926, mean_q: 0.447052, mean_eps: 0.000000
 1539/5000: episode: 61, duration: 0.251s, episode steps:  21, steps per second:  84, episode reward: 38.775, mean reward:  1.846 [-2.430, 32.232], mean action: 2.095 [0.000, 15.000],  loss: 0.018060, mae: 0.434978, mean_q: 0.458088, mean_eps: 0.000000
 1570/5000: episode: 62, duration: 0.369s, episode steps:  31, steps per second:  84, episode reward: 32.298, mean reward:  1.042 [-2.900, 31.928], mean action: 3.419 [0.000, 14.000],  loss: 0.016749, mae: 0.424071, mean_q: 0.493550, mean_eps: 0.000000
 1587/5000: episode: 63, duration: 0.202s, episode steps:  17, steps per second:  84, episode reward: 44.232, mean reward:  2.602 [-2.036, 32.370], mean action: 3.824 [0.000, 14.000],  loss: 0.020487, mae: 0.448370, mean_q: 0.428676, mean_eps: 0.000000
 1609/5000: episode: 64, duration: 0.255s, episode steps:  22, steps per second:  86, episode reward: 38.048, mean reward:  1.729 [-2.302, 32.860], mean action: 4.318 [0.000, 20.000],  loss: 0.018378, mae: 0.440491, mean_q: 0.404066, mean_eps: 0.000000
 1627/5000: episode: 65, duration: 0.210s, episode steps:  18, steps per second:  86, episode reward: 41.456, mean reward:  2.303 [-2.273, 32.790], mean action: 2.111 [0.000, 11.000],  loss: 0.022358, mae: 0.455099, mean_q: 0.390382, mean_eps: 0.000000
 1648/5000: episode: 66, duration: 0.248s, episode steps:  21, steps per second:  85, episode reward: 34.876, mean reward:  1.661 [-3.000, 32.190], mean action: 4.667 [0.000, 14.000],  loss: 0.019947, mae: 0.445181, mean_q: 0.432533, mean_eps: 0.000000
 1665/5000: episode: 67, duration: 0.200s, episode steps:  17, steps per second:  85, episode reward: 38.512, mean reward:  2.265 [-3.000, 32.172], mean action: 3.765 [0.000, 15.000],  loss: 0.016475, mae: 0.411627, mean_q: 0.486378, mean_eps: 0.000000
 1686/5000: episode: 68, duration: 0.242s, episode steps:  21, steps per second:  87, episode reward: 41.629, mean reward:  1.982 [-2.149, 32.450], mean action: 2.333 [0.000, 15.000],  loss: 0.020580, mae: 0.434901, mean_q: 0.477103, mean_eps: 0.000000
 1715/5000: episode: 69, duration: 0.335s, episode steps:  29, steps per second:  87, episode reward: 32.946, mean reward:  1.136 [-2.699, 32.366], mean action: 4.345 [0.000, 18.000],  loss: 0.019278, mae: 0.435735, mean_q: 0.389698, mean_eps: 0.000000
 1739/5000: episode: 70, duration: 0.274s, episode steps:  24, steps per second:  87, episode reward: -38.280, mean reward: -1.595 [-32.183,  3.000], mean action: 8.167 [0.000, 17.000],  loss: 0.017896, mae: 0.429111, mean_q: 0.440103, mean_eps: 0.000000
 1768/5000: episode: 71, duration: 0.336s, episode steps:  29, steps per second:  86, episode reward: 36.000, mean reward:  1.241 [-2.457, 32.370], mean action: 5.931 [0.000, 18.000],  loss: 0.019259, mae: 0.428016, mean_q: 0.456756, mean_eps: 0.000000
 1802/5000: episode: 72, duration: 0.488s, episode steps:  34, steps per second:  70, episode reward: 38.484, mean reward:  1.132 [-2.520, 32.544], mean action: 4.971 [0.000, 9.000],  loss: 0.018564, mae: 0.432782, mean_q: 0.447106, mean_eps: 0.000000
 1818/5000: episode: 73, duration: 0.233s, episode steps:  16, steps per second:  69, episode reward: 41.816, mean reward:  2.613 [-2.375, 32.416], mean action: 4.188 [0.000, 14.000],  loss: 0.018820, mae: 0.433439, mean_q: 0.458613, mean_eps: 0.000000
 1848/5000: episode: 74, duration: 0.339s, episode steps:  30, steps per second:  89, episode reward: 35.757, mean reward:  1.192 [-2.602, 32.390], mean action: 2.400 [0.000, 11.000],  loss: 0.014415, mae: 0.409734, mean_q: 0.478940, mean_eps: 0.000000
 1875/5000: episode: 75, duration: 0.308s, episode steps:  27, steps per second:  88, episode reward: 38.139, mean reward:  1.413 [-2.702, 32.109], mean action: 3.926 [0.000, 14.000],  loss: 0.019395, mae: 0.431982, mean_q: 0.438464, mean_eps: 0.000000
 1903/5000: episode: 76, duration: 0.320s, episode steps:  28, steps per second:  88, episode reward: 32.130, mean reward:  1.147 [-3.000, 32.130], mean action: 4.000 [0.000, 9.000],  loss: 0.019421, mae: 0.435647, mean_q: 0.427157, mean_eps: 0.000000
 1922/5000: episode: 77, duration: 0.223s, episode steps:  19, steps per second:  85, episode reward: 38.097, mean reward:  2.005 [-2.787, 31.297], mean action: 4.895 [0.000, 19.000],  loss: 0.018740, mae: 0.440292, mean_q: 0.443205, mean_eps: 0.000000
 1935/5000: episode: 78, duration: 0.159s, episode steps:  13, steps per second:  82, episode reward: 41.800, mean reward:  3.215 [-2.900, 33.000], mean action: 5.154 [1.000, 19.000],  loss: 0.016293, mae: 0.441318, mean_q: 0.393016, mean_eps: 0.000000
 1954/5000: episode: 79, duration: 0.228s, episode steps:  19, steps per second:  83, episode reward: 39.000, mean reward:  2.053 [-2.529, 32.330], mean action: 5.526 [0.000, 14.000],  loss: 0.021126, mae: 0.458793, mean_q: 0.429868, mean_eps: 0.000000
 2004/5000: episode: 80, duration: 0.540s, episode steps:  50, steps per second:  93, episode reward: -32.440, mean reward: -0.649 [-32.013,  2.536], mean action: 2.020 [0.000, 19.000],  loss: 0.022384, mae: 0.450145, mean_q: 0.488778, mean_eps: 0.000000
 2022/5000: episode: 81, duration: 0.208s, episode steps:  18, steps per second:  86, episode reward: 38.016, mean reward:  2.112 [-3.000, 32.571], mean action: 4.167 [0.000, 15.000],  loss: 0.018731, mae: 0.440598, mean_q: 0.470226, mean_eps: 0.000000
 2043/5000: episode: 82, duration: 0.254s, episode steps:  21, steps per second:  83, episode reward: 35.498, mean reward:  1.690 [-2.670, 32.230], mean action: 3.286 [0.000, 15.000],  loss: 0.019986, mae: 0.450163, mean_q: 0.445416, mean_eps: 0.000000
 2055/5000: episode: 83, duration: 0.149s, episode steps:  12, steps per second:  80, episode reward: 43.773, mean reward:  3.648 [-2.140, 32.811], mean action: 2.500 [0.000, 12.000],  loss: 0.018660, mae: 0.438303, mean_q: 0.487774, mean_eps: 0.000000
 2090/5000: episode: 84, duration: 0.389s, episode steps:  35, steps per second:  90, episode reward: 35.558, mean reward:  1.016 [-2.300, 32.110], mean action: 5.743 [0.000, 21.000],  loss: 0.020236, mae: 0.442725, mean_q: 0.472098, mean_eps: 0.000000
 2112/5000: episode: 85, duration: 0.256s, episode steps:  22, steps per second:  86, episode reward: 35.977, mean reward:  1.635 [-3.000, 32.040], mean action: 4.091 [0.000, 14.000],  loss: 0.022064, mae: 0.450707, mean_q: 0.412058, mean_eps: 0.000000
 2137/5000: episode: 86, duration: 0.696s, episode steps:  25, steps per second:  36, episode reward: 38.405, mean reward:  1.536 [-2.466, 32.040], mean action: 5.240 [0.000, 14.000],  loss: 0.020397, mae: 0.443960, mean_q: 0.402967, mean_eps: 0.000000
 2156/5000: episode: 87, duration: 0.218s, episode steps:  19, steps per second:  87, episode reward: 38.638, mean reward:  2.034 [-2.129, 32.350], mean action: 2.789 [0.000, 11.000],  loss: 0.020279, mae: 0.436487, mean_q: 0.438747, mean_eps: 0.000000
 2180/5000: episode: 88, duration: 0.276s, episode steps:  24, steps per second:  87, episode reward: 35.366, mean reward:  1.474 [-2.539, 32.320], mean action: 4.917 [1.000, 11.000],  loss: 0.017418, mae: 0.421366, mean_q: 0.440990, mean_eps: 0.000000
 2200/5000: episode: 89, duration: 0.699s, episode steps:  20, steps per second:  29, episode reward: -35.380, mean reward: -1.769 [-32.101,  2.947], mean action: 6.700 [1.000, 15.000],  loss: 0.022282, mae: 0.453394, mean_q: 0.407107, mean_eps: 0.000000
 2217/5000: episode: 90, duration: 0.220s, episode steps:  17, steps per second:  77, episode reward: 35.258, mean reward:  2.074 [-2.671, 32.111], mean action: 3.235 [0.000, 11.000],  loss: 0.025673, mae: 0.471815, mean_q: 0.447146, mean_eps: 0.000000
 2241/5000: episode: 91, duration: 0.286s, episode steps:  24, steps per second:  84, episode reward: -32.430, mean reward: -1.351 [-32.032,  3.000], mean action: 3.500 [0.000, 11.000],  loss: 0.021979, mae: 0.456256, mean_q: 0.472646, mean_eps: 0.000000
 2274/5000: episode: 92, duration: 0.572s, episode steps:  33, steps per second:  58, episode reward: 32.347, mean reward:  0.980 [-2.529, 32.050], mean action: 7.333 [0.000, 21.000],  loss: 0.020695, mae: 0.452824, mean_q: 0.441373, mean_eps: 0.000000
 2296/5000: episode: 93, duration: 0.273s, episode steps:  22, steps per second:  81, episode reward: 32.502, mean reward:  1.477 [-3.000, 32.150], mean action: 3.773 [0.000, 12.000],  loss: 0.017149, mae: 0.424637, mean_q: 0.497572, mean_eps: 0.000000
 2321/5000: episode: 94, duration: 0.302s, episode steps:  25, steps per second:  83, episode reward: -35.630, mean reward: -1.425 [-32.308,  2.537], mean action: 2.880 [0.000, 11.000],  loss: 0.022302, mae: 0.442105, mean_q: 0.518351, mean_eps: 0.000000
 2344/5000: episode: 95, duration: 0.294s, episode steps:  23, steps per second:  78, episode reward: 35.391, mean reward:  1.539 [-2.402, 31.813], mean action: 3.826 [0.000, 12.000],  loss: 0.017914, mae: 0.437170, mean_q: 0.455713, mean_eps: 0.000000
 2378/5000: episode: 96, duration: 0.402s, episode steps:  34, steps per second:  85, episode reward: -32.450, mean reward: -0.954 [-33.000,  2.632], mean action: 5.147 [0.000, 15.000],  loss: 0.021820, mae: 0.451820, mean_q: 0.441037, mean_eps: 0.000000
 2408/5000: episode: 97, duration: 0.368s, episode steps:  30, steps per second:  82, episode reward: 32.667, mean reward:  1.089 [-2.288, 32.027], mean action: 2.800 [0.000, 9.000],  loss: 0.019251, mae: 0.437062, mean_q: 0.445264, mean_eps: 0.000000
 2452/5000: episode: 98, duration: 0.503s, episode steps:  44, steps per second:  87, episode reward: -38.910, mean reward: -0.884 [-33.000,  2.390], mean action: 3.773 [0.000, 15.000],  loss: 0.020190, mae: 0.435932, mean_q: 0.488613, mean_eps: 0.000000
 2472/5000: episode: 99, duration: 0.382s, episode steps:  20, steps per second:  52, episode reward: 41.280, mean reward:  2.064 [-2.165, 31.841], mean action: 2.050 [0.000, 9.000],  loss: 0.025199, mae: 0.469490, mean_q: 0.462058, mean_eps: 0.000000
 2492/5000: episode: 100, duration: 0.250s, episode steps:  20, steps per second:  80, episode reward: 37.878, mean reward:  1.894 [-2.939, 31.949], mean action: 3.800 [0.000, 14.000],  loss: 0.019219, mae: 0.432281, mean_q: 0.476928, mean_eps: 0.000000
 2519/5000: episode: 101, duration: 0.633s, episode steps:  27, steps per second:  43, episode reward: 32.239, mean reward:  1.194 [-3.000, 32.340], mean action: 3.741 [0.000, 12.000],  loss: 0.018874, mae: 0.434751, mean_q: 0.469676, mean_eps: 0.000000
 2545/5000: episode: 102, duration: 0.761s, episode steps:  26, steps per second:  34, episode reward: -35.640, mean reward: -1.371 [-32.380,  2.570], mean action: 5.731 [0.000, 15.000],  loss: 0.019251, mae: 0.440432, mean_q: 0.426738, mean_eps: 0.000000
 2566/5000: episode: 103, duration: 0.322s, episode steps:  21, steps per second:  65, episode reward: -32.670, mean reward: -1.556 [-33.000,  2.676], mean action: 6.905 [0.000, 15.000],  loss: 0.019323, mae: 0.425665, mean_q: 0.527590, mean_eps: 0.000000
 2591/5000: episode: 104, duration: 0.547s, episode steps:  25, steps per second:  46, episode reward: 37.550, mean reward:  1.502 [-2.191, 32.260], mean action: 4.400 [0.000, 15.000],  loss: 0.019447, mae: 0.422991, mean_q: 0.493607, mean_eps: 0.000000
 2614/5000: episode: 105, duration: 0.453s, episode steps:  23, steps per second:  51, episode reward: 36.000, mean reward:  1.565 [-2.685, 32.400], mean action: 6.348 [0.000, 15.000],  loss: 0.021838, mae: 0.443810, mean_q: 0.445115, mean_eps: 0.000000
 2637/5000: episode: 106, duration: 0.278s, episode steps:  23, steps per second:  83, episode reward: 32.362, mean reward:  1.407 [-3.000, 32.350], mean action: 6.087 [0.000, 19.000],  loss: 0.017644, mae: 0.432661, mean_q: 0.402698, mean_eps: 0.000000
 2682/5000: episode: 107, duration: 0.637s, episode steps:  45, steps per second:  71, episode reward: -35.480, mean reward: -0.788 [-32.066,  2.150], mean action: 4.644 [0.000, 14.000],  loss: 0.018558, mae: 0.423987, mean_q: 0.460699, mean_eps: 0.000000
 2704/5000: episode: 108, duration: 0.504s, episode steps:  22, steps per second:  44, episode reward: 38.433, mean reward:  1.747 [-2.339, 32.379], mean action: 3.364 [0.000, 12.000],  loss: 0.024261, mae: 0.450805, mean_q: 0.424654, mean_eps: 0.000000
 2731/5000: episode: 109, duration: 0.473s, episode steps:  27, steps per second:  57, episode reward: 38.951, mean reward:  1.443 [-2.335, 33.000], mean action: 2.852 [0.000, 9.000],  loss: 0.019489, mae: 0.423281, mean_q: 0.442718, mean_eps: 0.000000
 2754/5000: episode: 110, duration: 0.314s, episode steps:  23, steps per second:  73, episode reward: 39.000, mean reward:  1.696 [-2.498, 32.350], mean action: 4.087 [0.000, 14.000],  loss: 0.020847, mae: 0.426886, mean_q: 0.457599, mean_eps: 0.000000
 2781/5000: episode: 111, duration: 0.426s, episode steps:  27, steps per second:  63, episode reward: 35.084, mean reward:  1.299 [-2.903, 32.595], mean action: 3.630 [0.000, 19.000],  loss: 0.017751, mae: 0.411925, mean_q: 0.443789, mean_eps: 0.000000
 2815/5000: episode: 112, duration: 0.853s, episode steps:  34, steps per second:  40, episode reward: -35.060, mean reward: -1.031 [-31.969,  2.340], mean action: 6.912 [0.000, 15.000],  loss: 0.018891, mae: 0.424187, mean_q: 0.453231, mean_eps: 0.000000
 2835/5000: episode: 113, duration: 0.268s, episode steps:  20, steps per second:  75, episode reward: -35.660, mean reward: -1.783 [-32.353,  2.900], mean action: 5.400 [0.000, 16.000],  loss: 0.018313, mae: 0.434088, mean_q: 0.462051, mean_eps: 0.000000
 2898/5000: episode: 114, duration: 0.767s, episode steps:  63, steps per second:  82, episode reward: 33.000, mean reward:  0.524 [-2.940, 32.100], mean action: 5.302 [0.000, 15.000],  loss: 0.018673, mae: 0.443340, mean_q: 0.443341, mean_eps: 0.000000
 2916/5000: episode: 115, duration: 0.238s, episode steps:  18, steps per second:  76, episode reward: -35.130, mean reward: -1.952 [-31.714,  2.090], mean action: 7.778 [0.000, 17.000],  loss: 0.017479, mae: 0.434454, mean_q: 0.418362, mean_eps: 0.000000
 2934/5000: episode: 116, duration: 0.225s, episode steps:  18, steps per second:  80, episode reward: 40.904, mean reward:  2.272 [-2.288, 32.100], mean action: 3.556 [0.000, 12.000],  loss: 0.019491, mae: 0.450217, mean_q: 0.417230, mean_eps: 0.000000
 2962/5000: episode: 117, duration: 0.481s, episode steps:  28, steps per second:  58, episode reward: 35.258, mean reward:  1.259 [-2.184, 31.808], mean action: 6.679 [0.000, 14.000],  loss: 0.019707, mae: 0.450541, mean_q: 0.431230, mean_eps: 0.000000
 2979/5000: episode: 118, duration: 0.224s, episode steps:  17, steps per second:  76, episode reward: 39.000, mean reward:  2.294 [-2.169, 30.254], mean action: 2.471 [0.000, 12.000],  loss: 0.020022, mae: 0.446049, mean_q: 0.472821, mean_eps: 0.000000
 3009/5000: episode: 119, duration: 0.346s, episode steps:  30, steps per second:  87, episode reward: -32.610, mean reward: -1.087 [-32.610,  3.000], mean action: 3.967 [0.000, 11.000],  loss: 0.019720, mae: 0.452659, mean_q: 0.467698, mean_eps: 0.000000
 3029/5000: episode: 120, duration: 0.251s, episode steps:  20, steps per second:  80, episode reward: 40.843, mean reward:  2.042 [-3.000, 32.120], mean action: 4.950 [0.000, 20.000],  loss: 0.024562, mae: 0.475563, mean_q: 0.449099, mean_eps: 0.000000
 3053/5000: episode: 121, duration: 0.312s, episode steps:  24, steps per second:  77, episode reward: 41.758, mean reward:  1.740 [-2.428, 32.250], mean action: 3.625 [0.000, 15.000],  loss: 0.018047, mae: 0.437454, mean_q: 0.433416, mean_eps: 0.000000
 3073/5000: episode: 122, duration: 0.240s, episode steps:  20, steps per second:  83, episode reward: 40.647, mean reward:  2.032 [-2.354, 32.080], mean action: 4.900 [0.000, 20.000],  loss: 0.018131, mae: 0.429948, mean_q: 0.460413, mean_eps: 0.000000
 3112/5000: episode: 123, duration: 0.477s, episode steps:  39, steps per second:  82, episode reward: 38.647, mean reward:  0.991 [-2.371, 32.647], mean action: 1.256 [0.000, 12.000],  loss: 0.019809, mae: 0.438877, mean_q: 0.473166, mean_eps: 0.000000
 3122/5000: episode: 124, duration: 0.135s, episode steps:  10, steps per second:  74, episode reward: 44.459, mean reward:  4.446 [-2.317, 32.080], mean action: 3.800 [0.000, 14.000],  loss: 0.020910, mae: 0.444787, mean_q: 0.490958, mean_eps: 0.000000
 3140/5000: episode: 125, duration: 0.230s, episode steps:  18, steps per second:  78, episode reward: 35.055, mean reward:  1.947 [-3.000, 31.965], mean action: 3.056 [0.000, 12.000],  loss: 0.022164, mae: 0.445343, mean_q: 0.520801, mean_eps: 0.000000
 3159/5000: episode: 126, duration: 0.240s, episode steps:  19, steps per second:  79, episode reward: 38.870, mean reward:  2.046 [-2.635, 32.120], mean action: 4.947 [0.000, 20.000],  loss: 0.016654, mae: 0.416305, mean_q: 0.495738, mean_eps: 0.000000
 3183/5000: episode: 127, duration: 0.294s, episode steps:  24, steps per second:  82, episode reward: 42.000, mean reward:  1.750 [-2.800, 32.320], mean action: 2.833 [0.000, 15.000],  loss: 0.022774, mae: 0.456250, mean_q: 0.442762, mean_eps: 0.000000
 3211/5000: episode: 128, duration: 0.370s, episode steps:  28, steps per second:  76, episode reward: 38.405, mean reward:  1.372 [-2.293, 32.330], mean action: 4.357 [0.000, 15.000],  loss: 0.018991, mae: 0.443133, mean_q: 0.467317, mean_eps: 0.000000
 3230/5000: episode: 129, duration: 0.232s, episode steps:  19, steps per second:  82, episode reward: -35.410, mean reward: -1.864 [-33.000,  3.059], mean action: 6.000 [0.000, 14.000],  loss: 0.022374, mae: 0.458561, mean_q: 0.447266, mean_eps: 0.000000
 3271/5000: episode: 130, duration: 0.466s, episode steps:  41, steps per second:  88, episode reward: -32.700, mean reward: -0.798 [-32.502,  2.650], mean action: 4.268 [0.000, 14.000],  loss: 0.017520, mae: 0.438982, mean_q: 0.466426, mean_eps: 0.000000
 3287/5000: episode: 131, duration: 0.202s, episode steps:  16, steps per second:  79, episode reward: 40.219, mean reward:  2.514 [-2.416, 32.081], mean action: 3.312 [0.000, 9.000],  loss: 0.020435, mae: 0.450273, mean_q: 0.540959, mean_eps: 0.000000
 3352/5000: episode: 132, duration: 0.735s, episode steps:  65, steps per second:  88, episode reward: 44.690, mean reward:  0.688 [-2.637, 33.000], mean action: 0.908 [0.000, 12.000],  loss: 0.019622, mae: 0.452439, mean_q: 0.476593, mean_eps: 0.000000
 3377/5000: episode: 133, duration: 0.295s, episode steps:  25, steps per second:  85, episode reward: 38.460, mean reward:  1.538 [-2.363, 32.123], mean action: 4.360 [0.000, 14.000],  loss: 0.019927, mae: 0.450930, mean_q: 0.487014, mean_eps: 0.000000
 3404/5000: episode: 134, duration: 0.308s, episode steps:  27, steps per second:  88, episode reward: 35.021, mean reward:  1.297 [-2.701, 32.380], mean action: 2.333 [0.000, 9.000],  loss: 0.016945, mae: 0.428621, mean_q: 0.503942, mean_eps: 0.000000
 3425/5000: episode: 135, duration: 0.256s, episode steps:  21, steps per second:  82, episode reward: 38.125, mean reward:  1.815 [-2.753, 32.130], mean action: 6.429 [0.000, 19.000],  loss: 0.019509, mae: 0.443971, mean_q: 0.487046, mean_eps: 0.000000
 3451/5000: episode: 136, duration: 0.308s, episode steps:  26, steps per second:  84, episode reward: -32.530, mean reward: -1.251 [-32.280,  2.970], mean action: 4.000 [0.000, 15.000],  loss: 0.018184, mae: 0.441588, mean_q: 0.418181, mean_eps: 0.000000
 3464/5000: episode: 137, duration: 0.168s, episode steps:  13, steps per second:  77, episode reward: 41.643, mean reward:  3.203 [-2.285, 32.633], mean action: 4.000 [0.000, 14.000],  loss: 0.025011, mae: 0.476112, mean_q: 0.408397, mean_eps: 0.000000
 3490/5000: episode: 138, duration: 0.298s, episode steps:  26, steps per second:  87, episode reward: -32.330, mean reward: -1.243 [-31.910,  2.266], mean action: 4.423 [0.000, 14.000],  loss: 0.020883, mae: 0.456696, mean_q: 0.465795, mean_eps: 0.000000
 3512/5000: episode: 139, duration: 0.259s, episode steps:  22, steps per second:  85, episode reward: -32.050, mean reward: -1.457 [-32.506,  2.419], mean action: 4.545 [0.000, 12.000],  loss: 0.022732, mae: 0.470562, mean_q: 0.450333, mean_eps: 0.000000
 3528/5000: episode: 140, duration: 0.200s, episode steps:  16, steps per second:  80, episode reward: 38.001, mean reward:  2.375 [-2.661, 32.161], mean action: 5.812 [0.000, 14.000],  loss: 0.020454, mae: 0.454637, mean_q: 0.496101, mean_eps: 0.000000
 3559/5000: episode: 141, duration: 0.362s, episode steps:  31, steps per second:  86, episode reward: 35.202, mean reward:  1.136 [-2.326, 32.758], mean action: 7.871 [0.000, 21.000],  loss: 0.016543, mae: 0.436141, mean_q: 0.502588, mean_eps: 0.000000
 3582/5000: episode: 142, duration: 0.269s, episode steps:  23, steps per second:  86, episode reward: -36.000, mean reward: -1.565 [-32.224,  2.840], mean action: 8.478 [0.000, 19.000],  loss: 0.020706, mae: 0.462648, mean_q: 0.484443, mean_eps: 0.000000
 3608/5000: episode: 143, duration: 0.298s, episode steps:  26, steps per second:  87, episode reward: 34.897, mean reward:  1.342 [-2.493, 32.393], mean action: 4.308 [0.000, 15.000],  loss: 0.019111, mae: 0.457118, mean_q: 0.453083, mean_eps: 0.000000
 3633/5000: episode: 144, duration: 0.295s, episode steps:  25, steps per second:  85, episode reward: 37.851, mean reward:  1.514 [-2.313, 31.823], mean action: 4.760 [0.000, 19.000],  loss: 0.016781, mae: 0.439397, mean_q: 0.442301, mean_eps: 0.000000
 3658/5000: episode: 145, duration: 0.290s, episode steps:  25, steps per second:  86, episode reward: -32.240, mean reward: -1.290 [-32.063,  2.623], mean action: 5.680 [0.000, 21.000],  loss: 0.022366, mae: 0.460433, mean_q: 0.439525, mean_eps: 0.000000
 3681/5000: episode: 146, duration: 0.272s, episode steps:  23, steps per second:  85, episode reward: 41.712, mean reward:  1.814 [-2.412, 32.008], mean action: 5.087 [0.000, 21.000],  loss: 0.018847, mae: 0.446164, mean_q: 0.448608, mean_eps: 0.000000
 3703/5000: episode: 147, duration: 0.275s, episode steps:  22, steps per second:  80, episode reward: -32.040, mean reward: -1.456 [-31.676,  2.970], mean action: 7.682 [0.000, 20.000],  loss: 0.019905, mae: 0.450829, mean_q: 0.422957, mean_eps: 0.000000
 3724/5000: episode: 148, duration: 0.251s, episode steps:  21, steps per second:  84, episode reward: 35.186, mean reward:  1.676 [-3.000, 32.230], mean action: 9.905 [0.000, 21.000],  loss: 0.021745, mae: 0.458759, mean_q: 0.403827, mean_eps: 0.000000
 3741/5000: episode: 149, duration: 0.202s, episode steps:  17, steps per second:  84, episode reward: 38.230, mean reward:  2.249 [-2.222, 31.990], mean action: 2.471 [0.000, 12.000],  loss: 0.018627, mae: 0.448488, mean_q: 0.420136, mean_eps: 0.000000
 3761/5000: episode: 150, duration: 0.242s, episode steps:  20, steps per second:  83, episode reward: 38.019, mean reward:  1.901 [-2.903, 32.140], mean action: 4.500 [0.000, 21.000],  loss: 0.020582, mae: 0.458011, mean_q: 0.413275, mean_eps: 0.000000
 3775/5000: episode: 151, duration: 0.169s, episode steps:  14, steps per second:  83, episode reward: 39.000, mean reward:  2.786 [-3.000, 33.000], mean action: 4.214 [0.000, 14.000],  loss: 0.014740, mae: 0.427565, mean_q: 0.426708, mean_eps: 0.000000
 3792/5000: episode: 152, duration: 0.219s, episode steps:  17, steps per second:  78, episode reward: 37.323, mean reward:  2.195 [-2.384, 32.040], mean action: 4.000 [0.000, 14.000],  loss: 0.018609, mae: 0.442928, mean_q: 0.472198, mean_eps: 0.000000
 3812/5000: episode: 153, duration: 0.303s, episode steps:  20, steps per second:  66, episode reward: 38.892, mean reward:  1.945 [-2.297, 32.082], mean action: 3.300 [0.000, 15.000],  loss: 0.022686, mae: 0.460496, mean_q: 0.508162, mean_eps: 0.000000
 3828/5000: episode: 154, duration: 0.294s, episode steps:  16, steps per second:  54, episode reward: -35.290, mean reward: -2.206 [-32.380,  2.953], mean action: 5.375 [0.000, 14.000],  loss: 0.019345, mae: 0.443574, mean_q: 0.493909, mean_eps: 0.000000
 3854/5000: episode: 155, duration: 0.300s, episode steps:  26, steps per second:  87, episode reward: -35.160, mean reward: -1.352 [-32.240,  2.432], mean action: 4.231 [0.000, 12.000],  loss: 0.020273, mae: 0.460830, mean_q: 0.386636, mean_eps: 0.000000
 3875/5000: episode: 156, duration: 0.255s, episode steps:  21, steps per second:  82, episode reward: 40.929, mean reward:  1.949 [-2.206, 32.165], mean action: 4.333 [0.000, 19.000],  loss: 0.018494, mae: 0.450346, mean_q: 0.435204, mean_eps: 0.000000
 3897/5000: episode: 157, duration: 0.250s, episode steps:  22, steps per second:  88, episode reward: 38.271, mean reward:  1.740 [-2.474, 32.450], mean action: 7.364 [0.000, 19.000],  loss: 0.020601, mae: 0.445212, mean_q: 0.463259, mean_eps: 0.000000
 3915/5000: episode: 158, duration: 0.218s, episode steps:  18, steps per second:  83, episode reward: -33.000, mean reward: -1.833 [-29.824,  3.000], mean action: 5.611 [0.000, 19.000],  loss: 0.019542, mae: 0.434422, mean_q: 0.406923, mean_eps: 0.000000
 3936/5000: episode: 159, duration: 0.247s, episode steps:  21, steps per second:  85, episode reward: 33.000, mean reward:  1.571 [-3.000, 32.190], mean action: 5.238 [0.000, 19.000],  loss: 0.022455, mae: 0.455206, mean_q: 0.418144, mean_eps: 0.000000
 3951/5000: episode: 160, duration: 0.188s, episode steps:  15, steps per second:  80, episode reward: 41.288, mean reward:  2.752 [-2.330, 31.843], mean action: 3.400 [0.000, 19.000],  loss: 0.023160, mae: 0.458100, mean_q: 0.430905, mean_eps: 0.000000
 3971/5000: episode: 161, duration: 0.241s, episode steps:  20, steps per second:  83, episode reward: 38.559, mean reward:  1.928 [-2.702, 30.153], mean action: 5.350 [1.000, 19.000],  loss: 0.017522, mae: 0.435775, mean_q: 0.423734, mean_eps: 0.000000
 3997/5000: episode: 162, duration: 0.305s, episode steps:  26, steps per second:  85, episode reward: -38.480, mean reward: -1.480 [-32.193,  3.000], mean action: 9.154 [0.000, 19.000],  loss: 0.017804, mae: 0.440256, mean_q: 0.392405, mean_eps: 0.000000
 4018/5000: episode: 163, duration: 0.239s, episode steps:  21, steps per second:  88, episode reward: -35.470, mean reward: -1.689 [-32.059,  2.500], mean action: 4.667 [0.000, 19.000],  loss: 0.018663, mae: 0.438574, mean_q: 0.362139, mean_eps: 0.000000
 4040/5000: episode: 164, duration: 0.262s, episode steps:  22, steps per second:  84, episode reward: 32.920, mean reward:  1.496 [-3.000, 32.030], mean action: 5.773 [0.000, 19.000],  loss: 0.017126, mae: 0.419241, mean_q: 0.412756, mean_eps: 0.000000
 4060/5000: episode: 165, duration: 0.233s, episode steps:  20, steps per second:  86, episode reward: 35.726, mean reward:  1.786 [-2.396, 30.940], mean action: 7.300 [0.000, 21.000],  loss: 0.018053, mae: 0.429414, mean_q: 0.423982, mean_eps: 0.000000
 4082/5000: episode: 166, duration: 0.266s, episode steps:  22, steps per second:  83, episode reward: 38.604, mean reward:  1.755 [-2.352, 31.684], mean action: 2.409 [0.000, 19.000],  loss: 0.017674, mae: 0.437306, mean_q: 0.426057, mean_eps: 0.000000
 4103/5000: episode: 167, duration: 0.238s, episode steps:  21, steps per second:  88, episode reward: 36.000, mean reward:  1.714 [-2.265, 32.340], mean action: 5.190 [3.000, 19.000],  loss: 0.017901, mae: 0.444927, mean_q: 0.483556, mean_eps: 0.000000
 4117/5000: episode: 168, duration: 0.173s, episode steps:  14, steps per second:  81, episode reward: 41.096, mean reward:  2.935 [-2.715, 32.220], mean action: 3.214 [0.000, 11.000],  loss: 0.023492, mae: 0.461455, mean_q: 0.460289, mean_eps: 0.000000
 4140/5000: episode: 169, duration: 0.273s, episode steps:  23, steps per second:  84, episode reward: 38.215, mean reward:  1.662 [-2.185, 31.976], mean action: 3.739 [0.000, 11.000],  loss: 0.017751, mae: 0.436276, mean_q: 0.460562, mean_eps: 0.000000
 4165/5000: episode: 170, duration: 0.294s, episode steps:  25, steps per second:  85, episode reward: -35.610, mean reward: -1.424 [-32.234,  2.405], mean action: 4.400 [0.000, 18.000],  loss: 0.021721, mae: 0.454842, mean_q: 0.479564, mean_eps: 0.000000
 4185/5000: episode: 171, duration: 0.229s, episode steps:  20, steps per second:  87, episode reward: 36.000, mean reward:  1.800 [-3.000, 32.170], mean action: 3.500 [1.000, 11.000],  loss: 0.016497, mae: 0.421576, mean_q: 0.490742, mean_eps: 0.000000
 4209/5000: episode: 172, duration: 0.288s, episode steps:  24, steps per second:  83, episode reward: 38.255, mean reward:  1.594 [-2.519, 32.650], mean action: 5.458 [0.000, 18.000],  loss: 0.016799, mae: 0.418491, mean_q: 0.474114, mean_eps: 0.000000
 4219/5000: episode: 173, duration: 0.136s, episode steps:  10, steps per second:  73, episode reward: 47.133, mean reward:  4.713 [ 0.038, 32.530], mean action: 3.500 [0.000, 14.000],  loss: 0.014069, mae: 0.414989, mean_q: 0.443398, mean_eps: 0.000000
 4237/5000: episode: 174, duration: 0.221s, episode steps:  18, steps per second:  82, episode reward: 35.062, mean reward:  1.948 [-3.000, 32.780], mean action: 4.167 [0.000, 20.000],  loss: 0.021368, mae: 0.446388, mean_q: 0.406844, mean_eps: 0.000000
 4253/5000: episode: 175, duration: 0.202s, episode steps:  16, steps per second:  79, episode reward: 36.000, mean reward:  2.250 [-3.000, 30.445], mean action: 5.188 [0.000, 14.000],  loss: 0.019406, mae: 0.441916, mean_q: 0.413614, mean_eps: 0.000000
 4263/5000: episode: 176, duration: 0.123s, episode steps:  10, steps per second:  81, episode reward: 44.315, mean reward:  4.431 [-2.171, 32.801], mean action: 1.400 [0.000, 11.000],  loss: 0.022913, mae: 0.452923, mean_q: 0.460053, mean_eps: 0.000000
 4286/5000: episode: 177, duration: 0.271s, episode steps:  23, steps per second:  85, episode reward: -35.810, mean reward: -1.557 [-31.923,  2.440], mean action: 7.522 [0.000, 19.000],  loss: 0.018090, mae: 0.447719, mean_q: 0.424910, mean_eps: 0.000000
 4315/5000: episode: 178, duration: 0.337s, episode steps:  29, steps per second:  86, episode reward: 35.346, mean reward:  1.219 [-2.381, 31.999], mean action: 7.586 [0.000, 19.000],  loss: 0.018066, mae: 0.441386, mean_q: 0.418105, mean_eps: 0.000000
 4330/5000: episode: 179, duration: 0.200s, episode steps:  15, steps per second:  75, episode reward: 44.320, mean reward:  2.955 [-2.333, 32.791], mean action: 4.733 [1.000, 11.000],  loss: 0.016724, mae: 0.439052, mean_q: 0.453338, mean_eps: 0.000000
 4355/5000: episode: 180, duration: 0.304s, episode steps:  25, steps per second:  82, episode reward: 35.086, mean reward:  1.403 [-2.314, 32.233], mean action: 3.320 [0.000, 12.000],  loss: 0.019997, mae: 0.444511, mean_q: 0.495995, mean_eps: 0.000000
 4377/5000: episode: 181, duration: 0.272s, episode steps:  22, steps per second:  81, episode reward: 32.407, mean reward:  1.473 [-2.378, 31.497], mean action: 3.500 [0.000, 12.000],  loss: 0.016157, mae: 0.418950, mean_q: 0.546075, mean_eps: 0.000000
 4403/5000: episode: 182, duration: 0.324s, episode steps:  26, steps per second:  80, episode reward: 33.000, mean reward:  1.269 [-3.000, 29.909], mean action: 4.385 [0.000, 14.000],  loss: 0.017693, mae: 0.434611, mean_q: 0.486908, mean_eps: 0.000000
 4413/5000: episode: 183, duration: 0.184s, episode steps:  10, steps per second:  54, episode reward: 43.760, mean reward:  4.376 [-2.119, 32.900], mean action: 5.100 [0.000, 14.000],  loss: 0.025427, mae: 0.472522, mean_q: 0.449844, mean_eps: 0.000000
 4442/5000: episode: 184, duration: 0.375s, episode steps:  29, steps per second:  77, episode reward: -36.000, mean reward: -1.241 [-32.138,  2.320], mean action: 8.759 [0.000, 20.000],  loss: 0.021072, mae: 0.462005, mean_q: 0.434889, mean_eps: 0.000000
 4478/5000: episode: 185, duration: 0.414s, episode steps:  36, steps per second:  87, episode reward: -38.050, mean reward: -1.057 [-32.218,  2.120], mean action: 8.556 [0.000, 15.000],  loss: 0.021215, mae: 0.456493, mean_q: 0.429949, mean_eps: 0.000000
 4504/5000: episode: 186, duration: 0.335s, episode steps:  26, steps per second:  78, episode reward: 39.000, mean reward:  1.500 [-2.078, 32.700], mean action: 2.000 [0.000, 9.000],  loss: 0.024966, mae: 0.467322, mean_q: 0.484779, mean_eps: 0.000000
 4523/5000: episode: 187, duration: 0.284s, episode steps:  19, steps per second:  67, episode reward: 37.694, mean reward:  1.984 [-2.553, 31.817], mean action: 3.895 [0.000, 14.000],  loss: 0.022091, mae: 0.467214, mean_q: 0.511647, mean_eps: 0.000000
 4546/5000: episode: 188, duration: 0.314s, episode steps:  23, steps per second:  73, episode reward: 34.495, mean reward:  1.500 [-2.456, 33.000], mean action: 5.217 [0.000, 14.000],  loss: 0.019120, mae: 0.453822, mean_q: 0.479322, mean_eps: 0.000000
 4565/5000: episode: 189, duration: 0.273s, episode steps:  19, steps per second:  70, episode reward: 38.077, mean reward:  2.004 [-2.219, 32.070], mean action: 4.789 [0.000, 14.000],  loss: 0.020599, mae: 0.455440, mean_q: 0.482194, mean_eps: 0.000000
 4583/5000: episode: 190, duration: 0.246s, episode steps:  18, steps per second:  73, episode reward: 41.242, mean reward:  2.291 [-2.202, 32.160], mean action: 2.944 [0.000, 11.000],  loss: 0.018812, mae: 0.447470, mean_q: 0.416767, mean_eps: 0.000000
 4619/5000: episode: 191, duration: 0.433s, episode steps:  36, steps per second:  83, episode reward: 32.178, mean reward:  0.894 [-3.000, 32.060], mean action: 3.556 [0.000, 19.000],  loss: 0.021961, mae: 0.449862, mean_q: 0.464743, mean_eps: 0.000000
 4639/5000: episode: 192, duration: 0.262s, episode steps:  20, steps per second:  76, episode reward: 40.469, mean reward:  2.023 [-2.183, 31.642], mean action: 3.850 [0.000, 15.000],  loss: 0.018056, mae: 0.436179, mean_q: 0.460060, mean_eps: 0.000000
 4666/5000: episode: 193, duration: 0.338s, episode steps:  27, steps per second:  80, episode reward: 37.712, mean reward:  1.397 [-2.244, 32.460], mean action: 6.481 [0.000, 20.000],  loss: 0.020387, mae: 0.450207, mean_q: 0.447240, mean_eps: 0.000000
 4713/5000: episode: 194, duration: 0.560s, episode steps:  47, steps per second:  84, episode reward: -40.970, mean reward: -0.872 [-32.041,  2.232], mean action: 5.447 [0.000, 20.000],  loss: 0.017609, mae: 0.429357, mean_q: 0.461483, mean_eps: 0.000000
 4749/5000: episode: 195, duration: 0.430s, episode steps:  36, steps per second:  84, episode reward: -33.000, mean reward: -0.917 [-33.000,  3.000], mean action: 3.611 [0.000, 14.000],  loss: 0.021563, mae: 0.449221, mean_q: 0.471965, mean_eps: 0.000000
 4768/5000: episode: 196, duration: 0.239s, episode steps:  19, steps per second:  79, episode reward: 39.000, mean reward:  2.053 [-2.379, 33.000], mean action: 2.789 [0.000, 14.000],  loss: 0.020731, mae: 0.439197, mean_q: 0.548246, mean_eps: 0.000000
 4852/5000: episode: 197, duration: 0.942s, episode steps:  84, steps per second:  89, episode reward: -35.120, mean reward: -0.418 [-32.411,  2.704], mean action: 12.714 [3.000, 20.000],  loss: 0.019467, mae: 0.448514, mean_q: 0.500418, mean_eps: 0.000000
 4882/5000: episode: 198, duration: 0.347s, episode steps:  30, steps per second:  86, episode reward: 35.960, mean reward:  1.199 [-2.312, 32.080], mean action: 5.067 [0.000, 20.000],  loss: 0.022422, mae: 0.464527, mean_q: 0.441604, mean_eps: 0.000000
 4919/5000: episode: 199, duration: 0.456s, episode steps:  37, steps per second:  81, episode reward: 38.548, mean reward:  1.042 [-2.235, 32.003], mean action: 2.514 [0.000, 14.000],  loss: 0.018642, mae: 0.439199, mean_q: 0.496504, mean_eps: 0.000000
 4954/5000: episode: 200, duration: 0.433s, episode steps:  35, steps per second:  81, episode reward: -32.800, mean reward: -0.937 [-32.161,  2.191], mean action: 7.314 [1.000, 21.000],  loss: 0.018578, mae: 0.442888, mean_q: 0.489389, mean_eps: 0.000000
 4973/5000: episode: 201, duration: 0.252s, episode steps:  19, steps per second:  75, episode reward: -35.130, mean reward: -1.849 [-32.740,  2.902], mean action: 5.000 [0.000, 15.000],  loss: 0.021087, mae: 0.443811, mean_q: 0.481081, mean_eps: 0.000000
 4986/5000: episode: 202, duration: 0.173s, episode steps:  13, steps per second:  75, episode reward: 41.632, mean reward:  3.202 [-3.000, 32.632], mean action: 3.923 [0.000, 15.000],  loss: 0.016331, mae: 0.425019, mean_q: 0.497715, mean_eps: 0.000000
done, took 58.270 seconds
DQN Evaluation: 1857 victories out of 2230 episodes
Training for 5000 steps ...
   28/5000: episode: 1, duration: 0.201s, episode steps:  28, steps per second: 139, episode reward: 35.806, mean reward:  1.279 [-2.425, 31.993], mean action: 2.536 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   73/5000: episode: 2, duration: 0.299s, episode steps:  45, steps per second: 150, episode reward: 43.727, mean reward:  0.972 [-2.258, 32.090], mean action: 4.489 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   98/5000: episode: 3, duration: 0.161s, episode steps:  25, steps per second: 155, episode reward: 44.029, mean reward:  1.761 [-2.795, 31.099], mean action: 0.800 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  129/5000: episode: 4, duration: 0.220s, episode steps:  31, steps per second: 141, episode reward: 38.459, mean reward:  1.241 [-3.000, 32.320], mean action: 3.194 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  157/5000: episode: 5, duration: 0.192s, episode steps:  28, steps per second: 146, episode reward: 33.000, mean reward:  1.179 [-2.925, 32.470], mean action: 5.500 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  182/5000: episode: 6, duration: 0.163s, episode steps:  25, steps per second: 153, episode reward: 41.152, mean reward:  1.646 [-2.257, 32.070], mean action: 2.840 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  219/5000: episode: 7, duration: 0.248s, episode steps:  37, steps per second: 149, episode reward: 33.000, mean reward:  0.892 [-2.964, 32.750], mean action: 7.405 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  238/5000: episode: 8, duration: 0.132s, episode steps:  19, steps per second: 143, episode reward: 45.766, mean reward:  2.409 [-0.538, 31.838], mean action: 2.211 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  265/5000: episode: 9, duration: 0.179s, episode steps:  27, steps per second: 151, episode reward: 35.875, mean reward:  1.329 [-2.456, 32.430], mean action: 4.333 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  298/5000: episode: 10, duration: 0.211s, episode steps:  33, steps per second: 156, episode reward: 38.500, mean reward:  1.167 [-3.000, 32.160], mean action: 4.727 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  334/5000: episode: 11, duration: 0.260s, episode steps:  36, steps per second: 139, episode reward: 34.953, mean reward:  0.971 [-2.975, 32.210], mean action: 3.972 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  381/5000: episode: 12, duration: 0.295s, episode steps:  47, steps per second: 159, episode reward: 35.354, mean reward:  0.752 [-3.000, 32.190], mean action: 4.298 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  399/5000: episode: 13, duration: 0.122s, episode steps:  18, steps per second: 147, episode reward: 44.938, mean reward:  2.497 [-2.327, 32.340], mean action: 3.667 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  422/5000: episode: 14, duration: 0.154s, episode steps:  23, steps per second: 150, episode reward: 38.137, mean reward:  1.658 [-3.000, 31.796], mean action: 3.565 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  460/5000: episode: 15, duration: 0.255s, episode steps:  38, steps per second: 149, episode reward: 41.290, mean reward:  1.087 [-2.105, 32.152], mean action: 2.500 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  491/5000: episode: 16, duration: 0.198s, episode steps:  31, steps per second: 157, episode reward: 40.520, mean reward:  1.307 [-2.150, 32.010], mean action: 2.323 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  519/5000: episode: 17, duration: 0.194s, episode steps:  28, steps per second: 145, episode reward: 34.519, mean reward:  1.233 [-2.255, 31.458], mean action: 5.857 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  561/5000: episode: 18, duration: 0.255s, episode steps:  42, steps per second: 164, episode reward: 32.808, mean reward:  0.781 [-3.000, 30.402], mean action: 8.071 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  602/5000: episode: 19, duration: 0.252s, episode steps:  41, steps per second: 163, episode reward: 41.102, mean reward:  1.002 [-2.143, 32.380], mean action: 3.390 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  630/5000: episode: 20, duration: 0.187s, episode steps:  28, steps per second: 150, episode reward: 35.184, mean reward:  1.257 [-2.695, 31.568], mean action: 4.393 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  665/5000: episode: 21, duration: 0.244s, episode steps:  35, steps per second: 143, episode reward: 39.995, mean reward:  1.143 [-2.095, 32.653], mean action: 5.029 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  690/5000: episode: 22, duration: 0.153s, episode steps:  25, steps per second: 163, episode reward: 38.576, mean reward:  1.543 [-2.555, 32.030], mean action: 4.360 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  706/5000: episode: 23, duration: 0.115s, episode steps:  16, steps per second: 139, episode reward: 43.958, mean reward:  2.747 [-2.523, 32.040], mean action: 3.250 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  732/5000: episode: 24, duration: 0.316s, episode steps:  26, steps per second:  82, episode reward: 44.168, mean reward:  1.699 [-2.182, 32.123], mean action: 2.385 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  762/5000: episode: 25, duration: 0.225s, episode steps:  30, steps per second: 133, episode reward: 44.277, mean reward:  1.476 [-2.326, 32.250], mean action: 3.100 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  783/5000: episode: 26, duration: 0.153s, episode steps:  21, steps per second: 137, episode reward: 42.000, mean reward:  2.000 [-2.234, 32.310], mean action: 3.762 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  804/5000: episode: 27, duration: 0.136s, episode steps:  21, steps per second: 155, episode reward: 36.000, mean reward:  1.714 [-3.000, 32.190], mean action: 3.952 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  823/5000: episode: 28, duration: 0.302s, episode steps:  19, steps per second:  63, episode reward: 44.273, mean reward:  2.330 [-2.062, 32.771], mean action: 4.684 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  845/5000: episode: 29, duration: 0.157s, episode steps:  22, steps per second: 140, episode reward: 47.332, mean reward:  2.151 [-0.587, 32.070], mean action: 0.591 [0.000, 6.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  875/5000: episode: 30, duration: 0.184s, episode steps:  30, steps per second: 163, episode reward: 41.420, mean reward:  1.381 [-2.683, 31.963], mean action: 5.600 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  899/5000: episode: 31, duration: 0.199s, episode steps:  24, steps per second: 121, episode reward: 41.805, mean reward:  1.742 [-2.359, 32.183], mean action: 1.708 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  929/5000: episode: 32, duration: 0.199s, episode steps:  30, steps per second: 151, episode reward: 39.000, mean reward:  1.300 [-3.000, 32.400], mean action: 4.233 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  984/5000: episode: 33, duration: 0.337s, episode steps:  55, steps per second: 163, episode reward: -34.710, mean reward: -0.631 [-32.097,  2.903], mean action: 8.109 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1001/5000: episode: 34, duration: 0.158s, episode steps:  17, steps per second: 108, episode reward: 42.000, mean reward:  2.471 [-2.492, 32.010], mean action: 2.941 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1028/5000: episode: 35, duration: 0.352s, episode steps:  27, steps per second:  77, episode reward: 38.437, mean reward:  1.424 [-2.440, 32.460], mean action: 4.444 [0.000, 19.000],  loss: 0.018255, mae: 0.433563, mean_q: 0.453036, mean_eps: 0.000000
 1053/5000: episode: 36, duration: 0.289s, episode steps:  25, steps per second:  86, episode reward: 40.579, mean reward:  1.623 [-2.833, 32.293], mean action: 6.360 [1.000, 19.000],  loss: 0.016914, mae: 0.425927, mean_q: 0.424677, mean_eps: 0.000000
 1076/5000: episode: 37, duration: 0.289s, episode steps:  23, steps per second:  80, episode reward: 44.904, mean reward:  1.952 [-2.039, 32.134], mean action: 2.087 [0.000, 19.000],  loss: 0.020926, mae: 0.442421, mean_q: 0.462473, mean_eps: 0.000000
 1117/5000: episode: 38, duration: 0.464s, episode steps:  41, steps per second:  88, episode reward: 41.131, mean reward:  1.003 [-2.791, 31.922], mean action: 6.927 [0.000, 20.000],  loss: 0.019245, mae: 0.431128, mean_q: 0.468437, mean_eps: 0.000000
 1135/5000: episode: 39, duration: 0.221s, episode steps:  18, steps per second:  82, episode reward: 41.310, mean reward:  2.295 [-3.000, 32.470], mean action: 5.111 [0.000, 19.000],  loss: 0.018336, mae: 0.423305, mean_q: 0.509453, mean_eps: 0.000000
 1164/5000: episode: 40, duration: 0.336s, episode steps:  29, steps per second:  86, episode reward: 39.763, mean reward:  1.371 [-2.681, 32.150], mean action: 4.586 [0.000, 20.000],  loss: 0.020779, mae: 0.445876, mean_q: 0.437400, mean_eps: 0.000000
 1195/5000: episode: 41, duration: 0.367s, episode steps:  31, steps per second:  85, episode reward: -35.020, mean reward: -1.130 [-32.129,  2.810], mean action: 8.903 [0.000, 20.000],  loss: 0.021235, mae: 0.441505, mean_q: 0.457555, mean_eps: 0.000000
 1215/5000: episode: 42, duration: 0.253s, episode steps:  20, steps per second:  79, episode reward: 43.595, mean reward:  2.180 [-2.045, 31.952], mean action: 2.600 [0.000, 9.000],  loss: 0.017570, mae: 0.430387, mean_q: 0.444486, mean_eps: 0.000000
 1269/5000: episode: 43, duration: 0.634s, episode steps:  54, steps per second:  85, episode reward: 44.281, mean reward:  0.820 [-2.035, 32.530], mean action: 3.037 [0.000, 13.000],  loss: 0.018812, mae: 0.425681, mean_q: 0.463345, mean_eps: 0.000000
 1319/5000: episode: 44, duration: 0.612s, episode steps:  50, steps per second:  82, episode reward: 47.932, mean reward:  0.959 [-0.437, 32.282], mean action: 1.220 [0.000, 8.000],  loss: 0.021614, mae: 0.448209, mean_q: 0.513742, mean_eps: 0.000000
 1346/5000: episode: 45, duration: 0.327s, episode steps:  27, steps per second:  83, episode reward: 35.215, mean reward:  1.304 [-2.887, 32.240], mean action: 9.852 [0.000, 20.000],  loss: 0.021713, mae: 0.450628, mean_q: 0.480857, mean_eps: 0.000000
 1362/5000: episode: 46, duration: 0.213s, episode steps:  16, steps per second:  75, episode reward: 44.845, mean reward:  2.803 [-2.078, 32.217], mean action: 4.000 [0.000, 19.000],  loss: 0.023256, mae: 0.472258, mean_q: 0.432608, mean_eps: 0.000000
 1400/5000: episode: 47, duration: 0.482s, episode steps:  38, steps per second:  79, episode reward: 37.639, mean reward:  0.990 [-2.912, 32.530], mean action: 4.553 [0.000, 14.000],  loss: 0.020633, mae: 0.453295, mean_q: 0.427412, mean_eps: 0.000000
 1415/5000: episode: 48, duration: 0.213s, episode steps:  15, steps per second:  70, episode reward: 47.225, mean reward:  3.148 [ 0.000, 32.560], mean action: 4.933 [0.000, 19.000],  loss: 0.026088, mae: 0.478715, mean_q: 0.497425, mean_eps: 0.000000
 1435/5000: episode: 49, duration: 0.285s, episode steps:  20, steps per second:  70, episode reward: 44.458, mean reward:  2.223 [-2.402, 32.118], mean action: 2.600 [1.000, 19.000],  loss: 0.015829, mae: 0.437197, mean_q: 0.470419, mean_eps: 0.000000
 1491/5000: episode: 50, duration: 0.689s, episode steps:  56, steps per second:  81, episode reward: 38.111, mean reward:  0.681 [-3.000, 31.981], mean action: 3.554 [0.000, 20.000],  loss: 0.018498, mae: 0.444992, mean_q: 0.462524, mean_eps: 0.000000
 1515/5000: episode: 51, duration: 0.286s, episode steps:  24, steps per second:  84, episode reward: 41.833, mean reward:  1.743 [-2.238, 32.050], mean action: 5.042 [0.000, 15.000],  loss: 0.019765, mae: 0.448455, mean_q: 0.484825, mean_eps: 0.000000
 1539/5000: episode: 52, duration: 0.299s, episode steps:  24, steps per second:  80, episode reward: 38.653, mean reward:  1.611 [-3.000, 32.340], mean action: 3.833 [0.000, 15.000],  loss: 0.017239, mae: 0.431742, mean_q: 0.511497, mean_eps: 0.000000
 1562/5000: episode: 53, duration: 0.268s, episode steps:  23, steps per second:  86, episode reward: 40.918, mean reward:  1.779 [-2.321, 32.004], mean action: 3.652 [0.000, 13.000],  loss: 0.017910, mae: 0.429125, mean_q: 0.470539, mean_eps: 0.000000
 1597/5000: episode: 54, duration: 0.397s, episode steps:  35, steps per second:  88, episode reward: 41.655, mean reward:  1.190 [-3.000, 32.190], mean action: 4.571 [0.000, 21.000],  loss: 0.018968, mae: 0.438711, mean_q: 0.435240, mean_eps: 0.000000
 1617/5000: episode: 55, duration: 0.245s, episode steps:  20, steps per second:  82, episode reward: 47.211, mean reward:  2.361 [-0.178, 32.610], mean action: 3.100 [0.000, 12.000],  loss: 0.020317, mae: 0.454401, mean_q: 0.435620, mean_eps: 0.000000
 1636/5000: episode: 56, duration: 0.228s, episode steps:  19, steps per second:  83, episode reward: 41.755, mean reward:  2.198 [-2.320, 32.475], mean action: 3.737 [1.000, 15.000],  loss: 0.020549, mae: 0.454166, mean_q: 0.448146, mean_eps: 0.000000
 1666/5000: episode: 57, duration: 0.347s, episode steps:  30, steps per second:  87, episode reward: 32.412, mean reward:  1.080 [-3.000, 31.909], mean action: 7.067 [0.000, 15.000],  loss: 0.018710, mae: 0.447695, mean_q: 0.465676, mean_eps: 0.000000
 1682/5000: episode: 58, duration: 0.195s, episode steps:  16, steps per second:  82, episode reward: 45.000, mean reward:  2.812 [-2.752, 33.000], mean action: 4.125 [0.000, 16.000],  loss: 0.016274, mae: 0.445360, mean_q: 0.403044, mean_eps: 0.000000
 1726/5000: episode: 59, duration: 0.499s, episode steps:  44, steps per second:  88, episode reward: -32.670, mean reward: -0.743 [-32.256,  2.422], mean action: 6.273 [0.000, 14.000],  loss: 0.019772, mae: 0.445231, mean_q: 0.443628, mean_eps: 0.000000
 1751/5000: episode: 60, duration: 0.307s, episode steps:  25, steps per second:  82, episode reward: 35.197, mean reward:  1.408 [-2.814, 32.130], mean action: 2.920 [0.000, 19.000],  loss: 0.018557, mae: 0.434018, mean_q: 0.510454, mean_eps: 0.000000
 1778/5000: episode: 61, duration: 0.317s, episode steps:  27, steps per second:  85, episode reward: 38.579, mean reward:  1.429 [-2.544, 32.120], mean action: 3.778 [0.000, 15.000],  loss: 0.018183, mae: 0.437444, mean_q: 0.452682, mean_eps: 0.000000
 1800/5000: episode: 62, duration: 0.264s, episode steps:  22, steps per second:  83, episode reward: 41.203, mean reward:  1.873 [-2.417, 32.040], mean action: 4.136 [0.000, 19.000],  loss: 0.021530, mae: 0.451090, mean_q: 0.470018, mean_eps: 0.000000
 1859/5000: episode: 63, duration: 0.657s, episode steps:  59, steps per second:  90, episode reward: -35.040, mean reward: -0.594 [-32.804,  2.904], mean action: 7.119 [0.000, 17.000],  loss: 0.018642, mae: 0.438651, mean_q: 0.447967, mean_eps: 0.000000
 1885/5000: episode: 64, duration: 0.324s, episode steps:  26, steps per second:  80, episode reward: 41.405, mean reward:  1.592 [-3.000, 32.275], mean action: 1.231 [0.000, 12.000],  loss: 0.019444, mae: 0.441600, mean_q: 0.467741, mean_eps: 0.000000
 1905/5000: episode: 65, duration: 0.247s, episode steps:  20, steps per second:  81, episode reward: 41.788, mean reward:  2.089 [-2.239, 32.160], mean action: 1.900 [0.000, 12.000],  loss: 0.014595, mae: 0.419604, mean_q: 0.427344, mean_eps: 0.000000
 1922/5000: episode: 66, duration: 0.218s, episode steps:  17, steps per second:  78, episode reward: 42.000, mean reward:  2.471 [-2.504, 32.020], mean action: 2.647 [0.000, 12.000],  loss: 0.022541, mae: 0.449354, mean_q: 0.476492, mean_eps: 0.000000
 1947/5000: episode: 67, duration: 1.101s, episode steps:  25, steps per second:  23, episode reward: 44.677, mean reward:  1.787 [-2.261, 32.451], mean action: 0.560 [0.000, 12.000],  loss: 0.019759, mae: 0.438276, mean_q: 0.483722, mean_eps: 0.000000
 1984/5000: episode: 68, duration: 0.681s, episode steps:  37, steps per second:  54, episode reward: 38.398, mean reward:  1.038 [-2.738, 32.901], mean action: 1.865 [0.000, 12.000],  loss: 0.019200, mae: 0.439593, mean_q: 0.477597, mean_eps: 0.000000
 2008/5000: episode: 69, duration: 0.470s, episode steps:  24, steps per second:  51, episode reward: 41.669, mean reward:  1.736 [-3.000, 31.990], mean action: 2.458 [0.000, 12.000],  loss: 0.020310, mae: 0.444183, mean_q: 0.453215, mean_eps: 0.000000
 2032/5000: episode: 70, duration: 0.451s, episode steps:  24, steps per second:  53, episode reward: 38.526, mean reward:  1.605 [-2.269, 30.375], mean action: 3.542 [0.000, 20.000],  loss: 0.017149, mae: 0.426970, mean_q: 0.452227, mean_eps: 0.000000
 2066/5000: episode: 71, duration: 0.480s, episode steps:  34, steps per second:  71, episode reward: 38.587, mean reward:  1.135 [-3.000, 32.460], mean action: 2.588 [0.000, 12.000],  loss: 0.023164, mae: 0.445922, mean_q: 0.516142, mean_eps: 0.000000
 2084/5000: episode: 72, duration: 0.229s, episode steps:  18, steps per second:  79, episode reward: 44.288, mean reward:  2.460 [-2.179, 32.090], mean action: 3.389 [1.000, 14.000],  loss: 0.020245, mae: 0.445081, mean_q: 0.513464, mean_eps: 0.000000
 2113/5000: episode: 73, duration: 0.447s, episode steps:  29, steps per second:  65, episode reward: 38.938, mean reward:  1.343 [-2.746, 32.420], mean action: 5.276 [0.000, 20.000],  loss: 0.020582, mae: 0.448665, mean_q: 0.531926, mean_eps: 0.000000
 2136/5000: episode: 74, duration: 0.388s, episode steps:  23, steps per second:  59, episode reward: 41.037, mean reward:  1.784 [-2.803, 31.884], mean action: 5.174 [1.000, 20.000],  loss: 0.023821, mae: 0.465823, mean_q: 0.476790, mean_eps: 0.000000
 2155/5000: episode: 75, duration: 0.634s, episode steps:  19, steps per second:  30, episode reward: 44.640, mean reward:  2.349 [-2.031, 32.670], mean action: 5.053 [0.000, 14.000],  loss: 0.020184, mae: 0.442425, mean_q: 0.454665, mean_eps: 0.000000
 2179/5000: episode: 76, duration: 0.820s, episode steps:  24, steps per second:  29, episode reward: 35.779, mean reward:  1.491 [-3.000, 32.249], mean action: 7.042 [1.000, 20.000],  loss: 0.021209, mae: 0.439473, mean_q: 0.469302, mean_eps: 0.000000
 2213/5000: episode: 77, duration: 1.392s, episode steps:  34, steps per second:  24, episode reward: 41.954, mean reward:  1.234 [-2.462, 32.120], mean action: 2.765 [0.000, 11.000],  loss: 0.019280, mae: 0.431539, mean_q: 0.485736, mean_eps: 0.000000
 2235/5000: episode: 78, duration: 0.551s, episode steps:  22, steps per second:  40, episode reward: 36.000, mean reward:  1.636 [-3.000, 32.050], mean action: 5.045 [0.000, 19.000],  loss: 0.017120, mae: 0.429497, mean_q: 0.431385, mean_eps: 0.000000
 2265/5000: episode: 79, duration: 0.889s, episode steps:  30, steps per second:  34, episode reward: 41.190, mean reward:  1.373 [-2.752, 29.947], mean action: 2.133 [0.000, 9.000],  loss: 0.020579, mae: 0.450446, mean_q: 0.442515, mean_eps: 0.000000
 2289/5000: episode: 80, duration: 1.122s, episode steps:  24, steps per second:  21, episode reward: 35.023, mean reward:  1.459 [-3.000, 31.493], mean action: 2.125 [0.000, 11.000],  loss: 0.026389, mae: 0.473416, mean_q: 0.468536, mean_eps: 0.000000
 2327/5000: episode: 81, duration: 1.279s, episode steps:  38, steps per second:  30, episode reward: 32.035, mean reward:  0.843 [-3.000, 31.781], mean action: 6.026 [0.000, 14.000],  loss: 0.022885, mae: 0.454144, mean_q: 0.480560, mean_eps: 0.000000
 2349/5000: episode: 82, duration: 0.836s, episode steps:  22, steps per second:  26, episode reward: 38.712, mean reward:  1.760 [-2.759, 32.384], mean action: 4.591 [0.000, 15.000],  loss: 0.019815, mae: 0.445568, mean_q: 0.464147, mean_eps: 0.000000
 2430/5000: episode: 83, duration: 2.649s, episode steps:  81, steps per second:  31, episode reward: -32.880, mean reward: -0.406 [-32.325,  2.399], mean action: 10.049 [0.000, 14.000],  loss: 0.020328, mae: 0.436638, mean_q: 0.488744, mean_eps: 0.000000
 2458/5000: episode: 84, duration: 0.337s, episode steps:  28, steps per second:  83, episode reward: 41.686, mean reward:  1.489 [-2.844, 32.256], mean action: 4.821 [1.000, 14.000],  loss: 0.019030, mae: 0.429025, mean_q: 0.431988, mean_eps: 0.000000
 2477/5000: episode: 85, duration: 0.234s, episode steps:  19, steps per second:  81, episode reward: 44.430, mean reward:  2.338 [-2.833, 32.091], mean action: 3.579 [1.000, 14.000],  loss: 0.018416, mae: 0.431212, mean_q: 0.484791, mean_eps: 0.000000
 2508/5000: episode: 86, duration: 1.012s, episode steps:  31, steps per second:  31, episode reward: 39.000, mean reward:  1.258 [-2.457, 32.070], mean action: 3.419 [0.000, 20.000],  loss: 0.015825, mae: 0.427777, mean_q: 0.478477, mean_eps: 0.000000
 2529/5000: episode: 87, duration: 0.426s, episode steps:  21, steps per second:  49, episode reward: 35.616, mean reward:  1.696 [-2.838, 31.716], mean action: 4.762 [0.000, 14.000],  loss: 0.021934, mae: 0.461849, mean_q: 0.431020, mean_eps: 0.000000
 2569/5000: episode: 88, duration: 0.465s, episode steps:  40, steps per second:  86, episode reward: 40.869, mean reward:  1.022 [-2.535, 32.260], mean action: 3.600 [0.000, 12.000],  loss: 0.019783, mae: 0.462430, mean_q: 0.419056, mean_eps: 0.000000
 2600/5000: episode: 89, duration: 0.439s, episode steps:  31, steps per second:  71, episode reward: 38.350, mean reward:  1.237 [-2.875, 32.420], mean action: 2.968 [0.000, 12.000],  loss: 0.022276, mae: 0.456311, mean_q: 0.466110, mean_eps: 0.000000
 2619/5000: episode: 90, duration: 0.240s, episode steps:  19, steps per second:  79, episode reward: 44.708, mean reward:  2.353 [-2.059, 32.383], mean action: 2.105 [0.000, 12.000],  loss: 0.014033, mae: 0.410474, mean_q: 0.458333, mean_eps: 0.000000
 2634/5000: episode: 91, duration: 0.187s, episode steps:  15, steps per second:  80, episode reward: 47.146, mean reward:  3.143 [-0.134, 32.061], mean action: 1.600 [0.000, 14.000],  loss: 0.023217, mae: 0.452662, mean_q: 0.491302, mean_eps: 0.000000
 2674/5000: episode: 92, duration: 0.444s, episode steps:  40, steps per second:  90, episode reward: 37.791, mean reward:  0.945 [-3.000, 31.450], mean action: 5.550 [0.000, 21.000],  loss: 0.019513, mae: 0.431948, mean_q: 0.482323, mean_eps: 0.000000
 2713/5000: episode: 93, duration: 1.086s, episode steps:  39, steps per second:  36, episode reward: 35.684, mean reward:  0.915 [-3.000, 32.350], mean action: 4.744 [0.000, 19.000],  loss: 0.020491, mae: 0.444444, mean_q: 0.435810, mean_eps: 0.000000
 2746/5000: episode: 94, duration: 1.005s, episode steps:  33, steps per second:  33, episode reward: 38.129, mean reward:  1.155 [-2.198, 32.450], mean action: 6.091 [0.000, 21.000],  loss: 0.017960, mae: 0.426766, mean_q: 0.456350, mean_eps: 0.000000
 2776/5000: episode: 95, duration: 0.940s, episode steps:  30, steps per second:  32, episode reward: 36.000, mean reward:  1.200 [-2.428, 32.180], mean action: 2.600 [0.000, 14.000],  loss: 0.020811, mae: 0.432991, mean_q: 0.516716, mean_eps: 0.000000
 2807/5000: episode: 96, duration: 0.794s, episode steps:  31, steps per second:  39, episode reward: 42.805, mean reward:  1.381 [-2.148, 32.080], mean action: 2.903 [0.000, 14.000],  loss: 0.019155, mae: 0.420727, mean_q: 0.506499, mean_eps: 0.000000
 2833/5000: episode: 97, duration: 0.440s, episode steps:  26, steps per second:  59, episode reward: 37.584, mean reward:  1.446 [-2.964, 31.795], mean action: 5.038 [0.000, 15.000],  loss: 0.016545, mae: 0.423822, mean_q: 0.440097, mean_eps: 0.000000
 2865/5000: episode: 98, duration: 0.892s, episode steps:  32, steps per second:  36, episode reward: -32.600, mean reward: -1.019 [-32.321,  3.000], mean action: 5.156 [0.000, 14.000],  loss: 0.015766, mae: 0.412443, mean_q: 0.449688, mean_eps: 0.000000
 2899/5000: episode: 99, duration: 0.590s, episode steps:  34, steps per second:  58, episode reward: 45.000, mean reward:  1.324 [-2.017, 32.620], mean action: 3.765 [1.000, 13.000],  loss: 0.020149, mae: 0.439647, mean_q: 0.454097, mean_eps: 0.000000
 2924/5000: episode: 100, duration: 0.408s, episode steps:  25, steps per second:  61, episode reward: 42.000, mean reward:  1.680 [-2.249, 32.070], mean action: 2.920 [0.000, 19.000],  loss: 0.020704, mae: 0.444604, mean_q: 0.430477, mean_eps: 0.000000
 2946/5000: episode: 101, duration: 0.284s, episode steps:  22, steps per second:  78, episode reward: 37.601, mean reward:  1.709 [-3.000, 31.732], mean action: 3.091 [0.000, 12.000],  loss: 0.018977, mae: 0.440762, mean_q: 0.457504, mean_eps: 0.000000
 2971/5000: episode: 102, duration: 0.310s, episode steps:  25, steps per second:  81, episode reward: 41.264, mean reward:  1.651 [-2.588, 32.028], mean action: 4.720 [0.000, 15.000],  loss: 0.019750, mae: 0.435162, mean_q: 0.441706, mean_eps: 0.000000
 3014/5000: episode: 103, duration: 0.596s, episode steps:  43, steps per second:  72, episode reward: 35.748, mean reward:  0.831 [-2.229, 31.909], mean action: 2.674 [0.000, 19.000],  loss: 0.019978, mae: 0.438568, mean_q: 0.516852, mean_eps: 0.000000
 3073/5000: episode: 104, duration: 0.668s, episode steps:  59, steps per second:  88, episode reward: 44.908, mean reward:  0.761 [-2.126, 32.680], mean action: 3.881 [0.000, 20.000],  loss: 0.021143, mae: 0.456990, mean_q: 0.448196, mean_eps: 0.000000
 3095/5000: episode: 105, duration: 0.268s, episode steps:  22, steps per second:  82, episode reward: 45.000, mean reward:  2.045 [-2.186, 32.280], mean action: 4.364 [1.000, 19.000],  loss: 0.021446, mae: 0.453452, mean_q: 0.394105, mean_eps: 0.000000
 3141/5000: episode: 106, duration: 0.528s, episode steps:  46, steps per second:  87, episode reward: 38.068, mean reward:  0.828 [-2.673, 32.620], mean action: 6.957 [0.000, 19.000],  loss: 0.020469, mae: 0.435034, mean_q: 0.497559, mean_eps: 0.000000
 3157/5000: episode: 107, duration: 0.198s, episode steps:  16, steps per second:  81, episode reward: 47.022, mean reward:  2.939 [-0.342, 32.240], mean action: 3.562 [0.000, 14.000],  loss: 0.020947, mae: 0.459454, mean_q: 0.475400, mean_eps: 0.000000
 3186/5000: episode: 108, duration: 0.336s, episode steps:  29, steps per second:  86, episode reward: 44.195, mean reward:  1.524 [-2.051, 32.160], mean action: 3.759 [0.000, 16.000],  loss: 0.020781, mae: 0.462004, mean_q: 0.412619, mean_eps: 0.000000
 3215/5000: episode: 109, duration: 0.336s, episode steps:  29, steps per second:  86, episode reward: 38.625, mean reward:  1.332 [-2.806, 32.400], mean action: 3.862 [0.000, 15.000],  loss: 0.019019, mae: 0.459234, mean_q: 0.415929, mean_eps: 0.000000
 3236/5000: episode: 110, duration: 0.253s, episode steps:  21, steps per second:  83, episode reward: 38.020, mean reward:  1.810 [-3.000, 32.040], mean action: 3.143 [0.000, 12.000],  loss: 0.020036, mae: 0.460637, mean_q: 0.432039, mean_eps: 0.000000
 3268/5000: episode: 111, duration: 0.371s, episode steps:  32, steps per second:  86, episode reward: 44.080, mean reward:  1.377 [-2.295, 32.530], mean action: 4.625 [0.000, 14.000],  loss: 0.019947, mae: 0.466149, mean_q: 0.468628, mean_eps: 0.000000
 3305/5000: episode: 112, duration: 0.463s, episode steps:  37, steps per second:  80, episode reward: 32.753, mean reward:  0.885 [-3.000, 32.820], mean action: 3.676 [0.000, 19.000],  loss: 0.022422, mae: 0.460912, mean_q: 0.509587, mean_eps: 0.000000
 3343/5000: episode: 113, duration: 0.865s, episode steps:  38, steps per second:  44, episode reward: 39.000, mean reward:  1.026 [-2.219, 32.370], mean action: 5.789 [0.000, 20.000],  loss: 0.024658, mae: 0.467101, mean_q: 0.521315, mean_eps: 0.000000
 3381/5000: episode: 114, duration: 0.504s, episode steps:  38, steps per second:  75, episode reward: 38.172, mean reward:  1.005 [-3.000, 32.625], mean action: 5.184 [0.000, 20.000],  loss: 0.016978, mae: 0.431837, mean_q: 0.494101, mean_eps: 0.000000
 3420/5000: episode: 115, duration: 0.543s, episode steps:  39, steps per second:  72, episode reward: 43.792, mean reward:  1.123 [-2.317, 32.320], mean action: 3.000 [0.000, 19.000],  loss: 0.020258, mae: 0.442865, mean_q: 0.496200, mean_eps: 0.000000
 3444/5000: episode: 116, duration: 0.767s, episode steps:  24, steps per second:  31, episode reward: 44.768, mean reward:  1.865 [-2.257, 32.020], mean action: 2.250 [0.000, 19.000],  loss: 0.019725, mae: 0.446274, mean_q: 0.503733, mean_eps: 0.000000
 3472/5000: episode: 117, duration: 0.407s, episode steps:  28, steps per second:  69, episode reward: -32.100, mean reward: -1.146 [-32.319,  3.000], mean action: 7.607 [0.000, 19.000],  loss: 0.024538, mae: 0.465827, mean_q: 0.486990, mean_eps: 0.000000
 3490/5000: episode: 118, duration: 0.230s, episode steps:  18, steps per second:  78, episode reward: 44.878, mean reward:  2.493 [-2.343, 33.000], mean action: 1.222 [0.000, 12.000],  loss: 0.020619, mae: 0.442939, mean_q: 0.465663, mean_eps: 0.000000
 3509/5000: episode: 119, duration: 0.236s, episode steps:  19, steps per second:  81, episode reward: 41.760, mean reward:  2.198 [-3.000, 32.220], mean action: 4.421 [0.000, 14.000],  loss: 0.018853, mae: 0.439727, mean_q: 0.477917, mean_eps: 0.000000
 3535/5000: episode: 120, duration: 0.312s, episode steps:  26, steps per second:  83, episode reward: 41.725, mean reward:  1.605 [-2.144, 32.129], mean action: 3.615 [0.000, 14.000],  loss: 0.020696, mae: 0.452006, mean_q: 0.445905, mean_eps: 0.000000
 3564/5000: episode: 121, duration: 0.366s, episode steps:  29, steps per second:  79, episode reward: 32.541, mean reward:  1.122 [-3.000, 32.150], mean action: 3.828 [0.000, 19.000],  loss: 0.019993, mae: 0.446459, mean_q: 0.416960, mean_eps: 0.000000
 3606/5000: episode: 122, duration: 0.712s, episode steps:  42, steps per second:  59, episode reward: 40.032, mean reward:  0.953 [-2.234, 31.891], mean action: 3.071 [0.000, 19.000],  loss: 0.017139, mae: 0.424787, mean_q: 0.427299, mean_eps: 0.000000
 3626/5000: episode: 123, duration: 0.382s, episode steps:  20, steps per second:  52, episode reward: 39.000, mean reward:  1.950 [-2.657, 32.160], mean action: 2.250 [0.000, 12.000],  loss: 0.018124, mae: 0.429822, mean_q: 0.455145, mean_eps: 0.000000
 3655/5000: episode: 124, duration: 0.334s, episode steps:  29, steps per second:  87, episode reward: 41.553, mean reward:  1.433 [-2.362, 32.122], mean action: 2.103 [0.000, 14.000],  loss: 0.020141, mae: 0.442817, mean_q: 0.441716, mean_eps: 0.000000
 3690/5000: episode: 125, duration: 0.478s, episode steps:  35, steps per second:  73, episode reward: 38.363, mean reward:  1.096 [-2.300, 32.098], mean action: 4.229 [0.000, 19.000],  loss: 0.018641, mae: 0.433574, mean_q: 0.490711, mean_eps: 0.000000
 3714/5000: episode: 126, duration: 0.395s, episode steps:  24, steps per second:  61, episode reward: 44.514, mean reward:  1.855 [-2.357, 32.060], mean action: 2.417 [0.000, 9.000],  loss: 0.020277, mae: 0.436235, mean_q: 0.469816, mean_eps: 0.000000
 3736/5000: episode: 127, duration: 0.277s, episode steps:  22, steps per second:  79, episode reward: 43.807, mean reward:  1.991 [-2.225, 32.451], mean action: 4.364 [0.000, 14.000],  loss: 0.022070, mae: 0.447090, mean_q: 0.449740, mean_eps: 0.000000
 3774/5000: episode: 128, duration: 0.463s, episode steps:  38, steps per second:  82, episode reward: 41.752, mean reward:  1.099 [-2.505, 32.240], mean action: 3.553 [0.000, 19.000],  loss: 0.018024, mae: 0.431410, mean_q: 0.471419, mean_eps: 0.000000
 3806/5000: episode: 129, duration: 0.605s, episode steps:  32, steps per second:  53, episode reward: 38.450, mean reward:  1.202 [-3.000, 32.570], mean action: 5.750 [0.000, 20.000],  loss: 0.020502, mae: 0.444267, mean_q: 0.464862, mean_eps: 0.000000
 3830/5000: episode: 130, duration: 0.310s, episode steps:  24, steps per second:  77, episode reward: 41.576, mean reward:  1.732 [-2.839, 32.760], mean action: 3.250 [0.000, 14.000],  loss: 0.014330, mae: 0.414452, mean_q: 0.479728, mean_eps: 0.000000
 3867/5000: episode: 131, duration: 0.448s, episode steps:  37, steps per second:  83, episode reward: 41.262, mean reward:  1.115 [-2.762, 32.150], mean action: 1.459 [0.000, 12.000],  loss: 0.020943, mae: 0.447503, mean_q: 0.473588, mean_eps: 0.000000
 3906/5000: episode: 132, duration: 0.445s, episode steps:  39, steps per second:  88, episode reward: -34.930, mean reward: -0.896 [-32.105,  2.590], mean action: 4.000 [0.000, 14.000],  loss: 0.018965, mae: 0.439257, mean_q: 0.463715, mean_eps: 0.000000
 3929/5000: episode: 133, duration: 0.266s, episode steps:  23, steps per second:  87, episode reward: 44.478, mean reward:  1.934 [-2.103, 32.760], mean action: 3.913 [0.000, 15.000],  loss: 0.019718, mae: 0.442734, mean_q: 0.465417, mean_eps: 0.000000
 3956/5000: episode: 134, duration: 0.340s, episode steps:  27, steps per second:  79, episode reward: 40.653, mean reward:  1.506 [-3.000, 32.980], mean action: 4.889 [0.000, 15.000],  loss: 0.018215, mae: 0.441657, mean_q: 0.500642, mean_eps: 0.000000
 3985/5000: episode: 135, duration: 0.346s, episode steps:  29, steps per second:  84, episode reward: 39.634, mean reward:  1.367 [-3.000, 32.430], mean action: 5.483 [0.000, 18.000],  loss: 0.018228, mae: 0.443191, mean_q: 0.451054, mean_eps: 0.000000
 4018/5000: episode: 136, duration: 0.401s, episode steps:  33, steps per second:  82, episode reward: 41.669, mean reward:  1.263 [-2.232, 32.690], mean action: 3.939 [1.000, 20.000],  loss: 0.020228, mae: 0.442180, mean_q: 0.506827, mean_eps: 0.000000
 4043/5000: episode: 137, duration: 0.295s, episode steps:  25, steps per second:  85, episode reward: 37.698, mean reward:  1.508 [-2.612, 32.052], mean action: 4.440 [0.000, 19.000],  loss: 0.019180, mae: 0.450691, mean_q: 0.410540, mean_eps: 0.000000
 4060/5000: episode: 138, duration: 0.211s, episode steps:  17, steps per second:  80, episode reward: 43.924, mean reward:  2.584 [-2.228, 32.470], mean action: 3.471 [0.000, 14.000],  loss: 0.024155, mae: 0.483243, mean_q: 0.371402, mean_eps: 0.000000
 4092/5000: episode: 139, duration: 0.438s, episode steps:  32, steps per second:  73, episode reward: 38.167, mean reward:  1.193 [-3.000, 32.120], mean action: 3.625 [0.000, 19.000],  loss: 0.019804, mae: 0.451367, mean_q: 0.455467, mean_eps: 0.000000
 4120/5000: episode: 140, duration: 0.337s, episode steps:  28, steps per second:  83, episode reward: 44.107, mean reward:  1.575 [-3.000, 31.911], mean action: 2.250 [1.000, 19.000],  loss: 0.018010, mae: 0.440653, mean_q: 0.519615, mean_eps: 0.000000
 4144/5000: episode: 141, duration: 0.283s, episode steps:  24, steps per second:  85, episode reward: 38.624, mean reward:  1.609 [-3.000, 32.172], mean action: 3.125 [0.000, 12.000],  loss: 0.022082, mae: 0.452347, mean_q: 0.489112, mean_eps: 0.000000
 4171/5000: episode: 142, duration: 0.324s, episode steps:  27, steps per second:  83, episode reward: 42.000, mean reward:  1.556 [-2.906, 32.250], mean action: 4.444 [0.000, 14.000],  loss: 0.021913, mae: 0.457697, mean_q: 0.472313, mean_eps: 0.000000
 4204/5000: episode: 143, duration: 0.393s, episode steps:  33, steps per second:  84, episode reward: 38.550, mean reward:  1.168 [-2.469, 32.595], mean action: 1.727 [0.000, 12.000],  loss: 0.019421, mae: 0.443378, mean_q: 0.435962, mean_eps: 0.000000
 4228/5000: episode: 144, duration: 0.284s, episode steps:  24, steps per second:  85, episode reward: 44.705, mean reward:  1.863 [-2.322, 32.495], mean action: 0.625 [0.000, 9.000],  loss: 0.023496, mae: 0.469040, mean_q: 0.402368, mean_eps: 0.000000
 4266/5000: episode: 145, duration: 0.468s, episode steps:  38, steps per second:  81, episode reward: 44.700, mean reward:  1.176 [-2.016, 32.041], mean action: 3.842 [0.000, 14.000],  loss: 0.021838, mae: 0.456876, mean_q: 0.452175, mean_eps: 0.000000
 4293/5000: episode: 146, duration: 0.314s, episode steps:  27, steps per second:  86, episode reward: 44.665, mean reward:  1.654 [-2.477, 32.075], mean action: 1.185 [0.000, 11.000],  loss: 0.018460, mae: 0.439016, mean_q: 0.484374, mean_eps: 0.000000
 4321/5000: episode: 147, duration: 0.330s, episode steps:  28, steps per second:  85, episode reward: 38.566, mean reward:  1.377 [-2.256, 32.030], mean action: 4.179 [0.000, 14.000],  loss: 0.017999, mae: 0.437991, mean_q: 0.467240, mean_eps: 0.000000
 4341/5000: episode: 148, duration: 0.244s, episode steps:  20, steps per second:  82, episode reward: 41.580, mean reward:  2.079 [-2.253, 32.200], mean action: 4.100 [1.000, 14.000],  loss: 0.016901, mae: 0.428492, mean_q: 0.465685, mean_eps: 0.000000
 4397/5000: episode: 149, duration: 0.623s, episode steps:  56, steps per second:  90, episode reward: -32.150, mean reward: -0.574 [-32.614,  2.227], mean action: 3.679 [0.000, 14.000],  loss: 0.018011, mae: 0.433664, mean_q: 0.479541, mean_eps: 0.000000
 4441/5000: episode: 150, duration: 0.515s, episode steps:  44, steps per second:  85, episode reward: 41.607, mean reward:  0.946 [-2.417, 32.250], mean action: 3.386 [0.000, 14.000],  loss: 0.022073, mae: 0.449783, mean_q: 0.443874, mean_eps: 0.000000
 4466/5000: episode: 151, duration: 0.296s, episode steps:  25, steps per second:  85, episode reward: 40.947, mean reward:  1.638 [-2.092, 29.658], mean action: 4.600 [0.000, 14.000],  loss: 0.021828, mae: 0.452836, mean_q: 0.503978, mean_eps: 0.000000
 4486/5000: episode: 152, duration: 0.235s, episode steps:  20, steps per second:  85, episode reward: 38.646, mean reward:  1.932 [-3.000, 32.162], mean action: 4.850 [0.000, 14.000],  loss: 0.018248, mae: 0.438402, mean_q: 0.433674, mean_eps: 0.000000
 4511/5000: episode: 153, duration: 0.300s, episode steps:  25, steps per second:  83, episode reward: 38.548, mean reward:  1.542 [-2.704, 32.240], mean action: 4.160 [0.000, 14.000],  loss: 0.021218, mae: 0.452951, mean_q: 0.454519, mean_eps: 0.000000
 4538/5000: episode: 154, duration: 0.315s, episode steps:  27, steps per second:  86, episode reward: 44.561, mean reward:  1.650 [-2.082, 32.872], mean action: 4.630 [0.000, 14.000],  loss: 0.017271, mae: 0.430434, mean_q: 0.484386, mean_eps: 0.000000
 4560/5000: episode: 155, duration: 0.262s, episode steps:  22, steps per second:  84, episode reward: 41.639, mean reward:  1.893 [-2.480, 31.840], mean action: 3.500 [0.000, 14.000],  loss: 0.020571, mae: 0.435303, mean_q: 0.487526, mean_eps: 0.000000
 4584/5000: episode: 156, duration: 0.276s, episode steps:  24, steps per second:  87, episode reward: 41.330, mean reward:  1.722 [-2.653, 32.350], mean action: 2.333 [0.000, 12.000],  loss: 0.017439, mae: 0.428686, mean_q: 0.497256, mean_eps: 0.000000
 4603/5000: episode: 157, duration: 0.232s, episode steps:  19, steps per second:  82, episode reward: 44.316, mean reward:  2.332 [-2.594, 32.130], mean action: 3.105 [0.000, 14.000],  loss: 0.019742, mae: 0.438695, mean_q: 0.514469, mean_eps: 0.000000
 4627/5000: episode: 158, duration: 0.365s, episode steps:  24, steps per second:  66, episode reward: 44.783, mean reward:  1.866 [-2.474, 32.433], mean action: 3.083 [0.000, 19.000],  loss: 0.023234, mae: 0.456549, mean_q: 0.504962, mean_eps: 0.000000
 4646/5000: episode: 159, duration: 0.229s, episode steps:  19, steps per second:  83, episode reward: 38.389, mean reward:  2.020 [-2.562, 32.116], mean action: 6.000 [0.000, 19.000],  loss: 0.025986, mae: 0.473437, mean_q: 0.482328, mean_eps: 0.000000
 4704/5000: episode: 160, duration: 0.666s, episode steps:  58, steps per second:  87, episode reward: 32.527, mean reward:  0.561 [-3.000, 32.070], mean action: 5.759 [0.000, 18.000],  loss: 0.020807, mae: 0.453102, mean_q: 0.441818, mean_eps: 0.000000
 4760/5000: episode: 161, duration: 0.642s, episode steps:  56, steps per second:  87, episode reward: -32.200, mean reward: -0.575 [-31.863,  2.432], mean action: 4.446 [0.000, 19.000],  loss: 0.018285, mae: 0.438419, mean_q: 0.421549, mean_eps: 0.000000
 4804/5000: episode: 162, duration: 0.519s, episode steps:  44, steps per second:  85, episode reward: 41.180, mean reward:  0.936 [-2.127, 32.382], mean action: 3.250 [0.000, 14.000],  loss: 0.019594, mae: 0.440973, mean_q: 0.460440, mean_eps: 0.000000
 4837/5000: episode: 163, duration: 0.389s, episode steps:  33, steps per second:  85, episode reward: 41.596, mean reward:  1.260 [-2.266, 32.290], mean action: 3.303 [0.000, 14.000],  loss: 0.019179, mae: 0.435698, mean_q: 0.484289, mean_eps: 0.000000
 4863/5000: episode: 164, duration: 0.312s, episode steps:  26, steps per second:  83, episode reward: 41.232, mean reward:  1.586 [-3.000, 30.473], mean action: 4.808 [1.000, 9.000],  loss: 0.021051, mae: 0.448304, mean_q: 0.443964, mean_eps: 0.000000
 4879/5000: episode: 165, duration: 0.400s, episode steps:  16, steps per second:  40, episode reward: 42.000, mean reward:  2.625 [-3.000, 32.080], mean action: 2.250 [0.000, 9.000],  loss: 0.021065, mae: 0.447386, mean_q: 0.495429, mean_eps: 0.000000
 4912/5000: episode: 166, duration: 0.391s, episode steps:  33, steps per second:  84, episode reward: 38.392, mean reward:  1.163 [-2.687, 32.381], mean action: 5.303 [0.000, 15.000],  loss: 0.018517, mae: 0.429746, mean_q: 0.472330, mean_eps: 0.000000
 4948/5000: episode: 167, duration: 0.405s, episode steps:  36, steps per second:  89, episode reward: 35.725, mean reward:  0.992 [-2.972, 32.150], mean action: 9.389 [0.000, 20.000],  loss: 0.023122, mae: 0.460920, mean_q: 0.460906, mean_eps: 0.000000
 4969/5000: episode: 168, duration: 0.248s, episode steps:  21, steps per second:  85, episode reward: 42.000, mean reward:  2.000 [-3.000, 32.150], mean action: 4.762 [0.000, 15.000],  loss: 0.019330, mae: 0.456138, mean_q: 0.414897, mean_eps: 0.000000
done, took 69.578 seconds
DQN Evaluation: 2016 victories out of 2399 episodes
Training for 5000 steps ...
   19/5000: episode: 1, duration: 0.139s, episode steps:  19, steps per second: 137, episode reward: 38.654, mean reward:  2.034 [-2.576, 32.478], mean action: 5.000 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   75/5000: episode: 2, duration: 0.347s, episode steps:  56, steps per second: 161, episode reward: 38.501, mean reward:  0.688 [-2.156, 31.988], mean action: 3.804 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   91/5000: episode: 3, duration: 0.112s, episode steps:  16, steps per second: 143, episode reward: 41.153, mean reward:  2.572 [-2.562, 32.347], mean action: 4.875 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  126/5000: episode: 4, duration: 0.219s, episode steps:  35, steps per second: 160, episode reward: -32.550, mean reward: -0.930 [-32.953,  2.904], mean action: 3.971 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  150/5000: episode: 5, duration: 0.159s, episode steps:  24, steps per second: 151, episode reward: 37.820, mean reward:  1.576 [-3.000, 31.760], mean action: 2.708 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  161/5000: episode: 6, duration: 0.086s, episode steps:  11, steps per second: 128, episode reward: 44.749, mean reward:  4.068 [-2.190, 32.360], mean action: 4.636 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  188/5000: episode: 7, duration: 0.167s, episode steps:  27, steps per second: 161, episode reward: -33.000, mean reward: -1.222 [-32.186,  2.590], mean action: 8.370 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  212/5000: episode: 8, duration: 0.163s, episode steps:  24, steps per second: 147, episode reward: 35.794, mean reward:  1.491 [-2.504, 32.120], mean action: 3.292 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  248/5000: episode: 9, duration: 0.244s, episode steps:  36, steps per second: 147, episode reward: 32.688, mean reward:  0.908 [-2.801, 32.608], mean action: 7.917 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  264/5000: episode: 10, duration: 0.134s, episode steps:  16, steps per second: 119, episode reward: 41.649, mean reward:  2.603 [-2.413, 32.154], mean action: 3.438 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  280/5000: episode: 11, duration: 0.108s, episode steps:  16, steps per second: 148, episode reward: 45.000, mean reward:  2.813 [-2.484, 32.200], mean action: 2.312 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  306/5000: episode: 12, duration: 0.175s, episode steps:  26, steps per second: 149, episode reward: 32.737, mean reward:  1.259 [-3.000, 30.871], mean action: 5.692 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  331/5000: episode: 13, duration: 0.158s, episode steps:  25, steps per second: 158, episode reward: -34.840, mean reward: -1.394 [-32.421,  2.860], mean action: 6.720 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  358/5000: episode: 14, duration: 0.169s, episode steps:  27, steps per second: 160, episode reward: 34.702, mean reward:  1.285 [-2.517, 31.961], mean action: 8.148 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  384/5000: episode: 15, duration: 0.169s, episode steps:  26, steps per second: 154, episode reward: 35.072, mean reward:  1.349 [-2.485, 32.803], mean action: 3.462 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  402/5000: episode: 16, duration: 0.116s, episode steps:  18, steps per second: 155, episode reward: -35.630, mean reward: -1.979 [-32.197,  3.000], mean action: 4.611 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  410/5000: episode: 17, duration: 0.071s, episode steps:   8, steps per second: 113, episode reward: 47.479, mean reward:  5.935 [ 0.063, 33.000], mean action: 0.875 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  455/5000: episode: 18, duration: 0.270s, episode steps:  45, steps per second: 167, episode reward: -32.610, mean reward: -0.725 [-32.147,  3.000], mean action: 11.222 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  479/5000: episode: 19, duration: 0.159s, episode steps:  24, steps per second: 151, episode reward: 39.807, mean reward:  1.659 [-2.206, 31.898], mean action: 7.875 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  505/5000: episode: 20, duration: 0.165s, episode steps:  26, steps per second: 157, episode reward: 33.000, mean reward:  1.269 [-2.279, 30.039], mean action: 3.192 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  527/5000: episode: 21, duration: 0.143s, episode steps:  22, steps per second: 153, episode reward: 32.251, mean reward:  1.466 [-2.418, 32.230], mean action: 3.682 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  555/5000: episode: 22, duration: 0.172s, episode steps:  28, steps per second: 163, episode reward: -32.910, mean reward: -1.175 [-32.463,  2.310], mean action: 5.857 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  577/5000: episode: 23, duration: 0.145s, episode steps:  22, steps per second: 152, episode reward: 37.456, mean reward:  1.703 [-2.397, 32.180], mean action: 3.364 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  604/5000: episode: 24, duration: 0.176s, episode steps:  27, steps per second: 154, episode reward: 39.000, mean reward:  1.444 [-2.462, 32.070], mean action: 2.704 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  626/5000: episode: 25, duration: 0.150s, episode steps:  22, steps per second: 147, episode reward: 41.449, mean reward:  1.884 [-2.263, 32.510], mean action: 2.182 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  645/5000: episode: 26, duration: 0.123s, episode steps:  19, steps per second: 154, episode reward: 38.900, mean reward:  2.047 [-2.465, 32.750], mean action: 4.158 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  669/5000: episode: 27, duration: 0.151s, episode steps:  24, steps per second: 159, episode reward: 35.328, mean reward:  1.472 [-3.000, 31.880], mean action: 5.458 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  696/5000: episode: 28, duration: 0.173s, episode steps:  27, steps per second: 156, episode reward: 32.108, mean reward:  1.189 [-3.000, 31.950], mean action: 5.481 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  728/5000: episode: 29, duration: 0.250s, episode steps:  32, steps per second: 128, episode reward: 35.352, mean reward:  1.105 [-2.337, 32.283], mean action: 5.125 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  746/5000: episode: 30, duration: 0.123s, episode steps:  18, steps per second: 146, episode reward: 41.397, mean reward:  2.300 [-2.309, 32.494], mean action: 5.944 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  759/5000: episode: 31, duration: 0.100s, episode steps:  13, steps per second: 130, episode reward: 44.049, mean reward:  3.388 [-2.577, 32.163], mean action: 3.308 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  772/5000: episode: 32, duration: 0.093s, episode steps:  13, steps per second: 140, episode reward: 41.698, mean reward:  3.208 [-2.082, 32.760], mean action: 3.077 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  785/5000: episode: 33, duration: 0.092s, episode steps:  13, steps per second: 141, episode reward: 41.024, mean reward:  3.156 [-2.578, 32.951], mean action: 4.538 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  806/5000: episode: 34, duration: 0.139s, episode steps:  21, steps per second: 152, episode reward: 38.944, mean reward:  1.854 [-2.192, 32.200], mean action: 4.810 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  829/5000: episode: 35, duration: 0.155s, episode steps:  23, steps per second: 148, episode reward: 36.000, mean reward:  1.565 [-2.273, 29.574], mean action: 2.870 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  848/5000: episode: 36, duration: 0.134s, episode steps:  19, steps per second: 142, episode reward: 41.487, mean reward:  2.184 [-2.086, 32.019], mean action: 2.526 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  867/5000: episode: 37, duration: 0.326s, episode steps:  19, steps per second:  58, episode reward: 38.416, mean reward:  2.022 [-3.000, 32.039], mean action: 5.684 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  898/5000: episode: 38, duration: 0.337s, episode steps:  31, steps per second:  92, episode reward: 35.609, mean reward:  1.149 [-2.675, 32.440], mean action: 9.581 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  922/5000: episode: 39, duration: 0.170s, episode steps:  24, steps per second: 141, episode reward: 35.180, mean reward:  1.466 [-2.901, 33.000], mean action: 4.333 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  946/5000: episode: 40, duration: 0.171s, episode steps:  24, steps per second: 140, episode reward: -32.380, mean reward: -1.349 [-31.897,  2.633], mean action: 5.917 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  969/5000: episode: 41, duration: 0.169s, episode steps:  23, steps per second: 136, episode reward: 32.066, mean reward:  1.394 [-3.000, 32.063], mean action: 8.870 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  997/5000: episode: 42, duration: 0.174s, episode steps:  28, steps per second: 161, episode reward: 33.000, mean reward:  1.179 [-2.902, 33.000], mean action: 3.857 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1026/5000: episode: 43, duration: 0.321s, episode steps:  29, steps per second:  90, episode reward: -32.900, mean reward: -1.134 [-32.166,  2.176], mean action: 6.483 [0.000, 19.000],  loss: 0.019205, mae: 0.436281, mean_q: 0.477060, mean_eps: 0.000000
 1050/5000: episode: 44, duration: 0.290s, episode steps:  24, steps per second:  83, episode reward: 38.727, mean reward:  1.614 [-2.617, 32.626], mean action: 3.667 [0.000, 19.000],  loss: 0.020279, mae: 0.449705, mean_q: 0.452020, mean_eps: 0.000000
 1065/5000: episode: 45, duration: 0.203s, episode steps:  15, steps per second:  74, episode reward: 41.125, mean reward:  2.742 [-2.320, 32.110], mean action: 5.267 [0.000, 19.000],  loss: 0.024821, mae: 0.465704, mean_q: 0.410479, mean_eps: 0.000000
 1091/5000: episode: 46, duration: 0.302s, episode steps:  26, steps per second:  86, episode reward: -41.120, mean reward: -1.582 [-32.560,  2.340], mean action: 7.731 [0.000, 20.000],  loss: 0.018852, mae: 0.424988, mean_q: 0.505409, mean_eps: 0.000000
 1115/5000: episode: 47, duration: 0.274s, episode steps:  24, steps per second:  88, episode reward: -32.210, mean reward: -1.342 [-32.658,  3.000], mean action: 7.667 [0.000, 19.000],  loss: 0.017823, mae: 0.424938, mean_q: 0.470633, mean_eps: 0.000000
 1140/5000: episode: 48, duration: 0.300s, episode steps:  25, steps per second:  83, episode reward: 32.114, mean reward:  1.285 [-2.902, 32.379], mean action: 5.520 [0.000, 19.000],  loss: 0.020861, mae: 0.437603, mean_q: 0.483573, mean_eps: 0.000000
 1169/5000: episode: 49, duration: 0.355s, episode steps:  29, steps per second:  82, episode reward: 32.170, mean reward:  1.109 [-3.000, 32.200], mean action: 7.310 [0.000, 21.000],  loss: 0.018414, mae: 0.438908, mean_q: 0.465468, mean_eps: 0.000000
 1193/5000: episode: 50, duration: 0.277s, episode steps:  24, steps per second:  87, episode reward: -32.950, mean reward: -1.373 [-32.466,  3.061], mean action: 9.833 [0.000, 21.000],  loss: 0.018918, mae: 0.448905, mean_q: 0.439702, mean_eps: 0.000000
 1208/5000: episode: 51, duration: 0.188s, episode steps:  15, steps per second:  80, episode reward: 44.234, mean reward:  2.949 [-2.492, 33.000], mean action: 3.200 [0.000, 19.000],  loss: 0.019880, mae: 0.444434, mean_q: 0.470253, mean_eps: 0.000000
 1235/5000: episode: 52, duration: 0.319s, episode steps:  27, steps per second:  85, episode reward: 32.862, mean reward:  1.217 [-2.510, 32.402], mean action: 3.111 [0.000, 15.000],  loss: 0.020010, mae: 0.442237, mean_q: 0.450044, mean_eps: 0.000000
 1258/5000: episode: 53, duration: 0.278s, episode steps:  23, steps per second:  83, episode reward: 38.570, mean reward:  1.677 [-2.124, 32.260], mean action: 3.217 [0.000, 12.000],  loss: 0.018697, mae: 0.441416, mean_q: 0.428102, mean_eps: 0.000000
 1287/5000: episode: 54, duration: 0.335s, episode steps:  29, steps per second:  87, episode reward: 38.004, mean reward:  1.310 [-2.853, 32.220], mean action: 5.138 [0.000, 15.000],  loss: 0.018951, mae: 0.447903, mean_q: 0.438019, mean_eps: 0.000000
 1309/5000: episode: 55, duration: 0.265s, episode steps:  22, steps per second:  83, episode reward: 35.066, mean reward:  1.594 [-2.509, 32.123], mean action: 4.773 [0.000, 12.000],  loss: 0.018617, mae: 0.452051, mean_q: 0.449424, mean_eps: 0.000000
 1321/5000: episode: 56, duration: 0.153s, episode steps:  12, steps per second:  78, episode reward: 44.855, mean reward:  3.738 [-2.155, 32.515], mean action: 3.500 [0.000, 12.000],  loss: 0.022652, mae: 0.466645, mean_q: 0.443012, mean_eps: 0.000000
 1351/5000: episode: 57, duration: 0.341s, episode steps:  30, steps per second:  88, episode reward: -35.610, mean reward: -1.187 [-31.684,  3.000], mean action: 7.033 [0.000, 14.000],  loss: 0.020561, mae: 0.451735, mean_q: 0.487513, mean_eps: 0.000000
 1378/5000: episode: 58, duration: 0.315s, episode steps:  27, steps per second:  86, episode reward: -38.170, mean reward: -1.414 [-31.907,  2.540], mean action: 6.111 [0.000, 15.000],  loss: 0.019293, mae: 0.445420, mean_q: 0.422250, mean_eps: 0.000000
 1400/5000: episode: 59, duration: 0.281s, episode steps:  22, steps per second:  78, episode reward: 35.855, mean reward:  1.630 [-2.176, 32.325], mean action: 3.091 [0.000, 11.000],  loss: 0.022687, mae: 0.461262, mean_q: 0.404471, mean_eps: 0.000000
 1436/5000: episode: 60, duration: 0.420s, episode steps:  36, steps per second:  86, episode reward: -32.110, mean reward: -0.892 [-31.747,  2.349], mean action: 4.944 [0.000, 19.000],  loss: 0.020501, mae: 0.445897, mean_q: 0.495881, mean_eps: 0.000000
 1457/5000: episode: 61, duration: 0.248s, episode steps:  21, steps per second:  85, episode reward: -32.910, mean reward: -1.567 [-32.481,  2.388], mean action: 5.286 [0.000, 19.000],  loss: 0.023519, mae: 0.453341, mean_q: 0.506512, mean_eps: 0.000000
 1474/5000: episode: 62, duration: 0.245s, episode steps:  17, steps per second:  69, episode reward: 35.571, mean reward:  2.092 [-3.000, 32.120], mean action: 4.176 [0.000, 15.000],  loss: 0.015688, mae: 0.421087, mean_q: 0.458565, mean_eps: 0.000000
 1502/5000: episode: 63, duration: 0.448s, episode steps:  28, steps per second:  62, episode reward: 38.014, mean reward:  1.358 [-2.161, 32.310], mean action: 5.536 [0.000, 15.000],  loss: 0.018852, mae: 0.452287, mean_q: 0.402334, mean_eps: 0.000000
 1523/5000: episode: 64, duration: 0.253s, episode steps:  21, steps per second:  83, episode reward: 32.541, mean reward:  1.550 [-3.000, 32.063], mean action: 4.190 [0.000, 10.000],  loss: 0.019459, mae: 0.455844, mean_q: 0.446103, mean_eps: 0.000000
 1545/5000: episode: 65, duration: 0.261s, episode steps:  22, steps per second:  84, episode reward: 33.000, mean reward:  1.500 [-2.609, 32.370], mean action: 4.682 [0.000, 19.000],  loss: 0.016035, mae: 0.416623, mean_q: 0.540880, mean_eps: 0.000000
 1571/5000: episode: 66, duration: 0.313s, episode steps:  26, steps per second:  83, episode reward: 38.205, mean reward:  1.469 [-2.399, 32.763], mean action: 4.500 [0.000, 19.000],  loss: 0.017787, mae: 0.424651, mean_q: 0.506492, mean_eps: 0.000000
 1603/5000: episode: 67, duration: 0.359s, episode steps:  32, steps per second:  89, episode reward: -39.000, mean reward: -1.219 [-33.000,  2.168], mean action: 2.594 [0.000, 12.000],  loss: 0.017786, mae: 0.422694, mean_q: 0.466503, mean_eps: 0.000000
 1628/5000: episode: 68, duration: 0.294s, episode steps:  25, steps per second:  85, episode reward: -35.320, mean reward: -1.413 [-32.410,  2.400], mean action: 3.800 [1.000, 17.000],  loss: 0.019549, mae: 0.436797, mean_q: 0.408059, mean_eps: 0.000000
 1647/5000: episode: 69, duration: 0.272s, episode steps:  19, steps per second:  70, episode reward: 38.067, mean reward:  2.004 [-2.580, 32.020], mean action: 4.158 [0.000, 13.000],  loss: 0.024658, mae: 0.459281, mean_q: 0.405180, mean_eps: 0.000000
 1672/5000: episode: 70, duration: 0.316s, episode steps:  25, steps per second:  79, episode reward: -32.250, mean reward: -1.290 [-32.250,  2.260], mean action: 3.280 [0.000, 15.000],  loss: 0.018186, mae: 0.437343, mean_q: 0.397224, mean_eps: 0.000000
 1694/5000: episode: 71, duration: 0.265s, episode steps:  22, steps per second:  83, episode reward: 33.000, mean reward:  1.500 [-2.562, 30.038], mean action: 3.227 [0.000, 15.000],  loss: 0.019891, mae: 0.438827, mean_q: 0.457575, mean_eps: 0.000000
 1723/5000: episode: 72, duration: 0.351s, episode steps:  29, steps per second:  83, episode reward: 35.041, mean reward:  1.208 [-2.309, 32.005], mean action: 4.241 [0.000, 15.000],  loss: 0.021786, mae: 0.449420, mean_q: 0.468544, mean_eps: 0.000000
 1748/5000: episode: 73, duration: 0.296s, episode steps:  25, steps per second:  85, episode reward: 35.726, mean reward:  1.429 [-3.000, 30.006], mean action: 2.600 [0.000, 9.000],  loss: 0.015980, mae: 0.426405, mean_q: 0.438347, mean_eps: 0.000000
 1769/5000: episode: 74, duration: 0.258s, episode steps:  21, steps per second:  81, episode reward: 35.145, mean reward:  1.674 [-3.000, 33.000], mean action: 5.429 [0.000, 20.000],  loss: 0.020716, mae: 0.448636, mean_q: 0.397569, mean_eps: 0.000000
 1785/5000: episode: 75, duration: 0.203s, episode steps:  16, steps per second:  79, episode reward: 41.903, mean reward:  2.619 [-2.222, 32.383], mean action: 3.688 [1.000, 19.000],  loss: 0.017064, mae: 0.426900, mean_q: 0.418172, mean_eps: 0.000000
 1808/5000: episode: 76, duration: 0.284s, episode steps:  23, steps per second:  81, episode reward: 37.395, mean reward:  1.626 [-2.268, 32.903], mean action: 5.609 [0.000, 19.000],  loss: 0.019061, mae: 0.432159, mean_q: 0.461876, mean_eps: 0.000000
 1839/5000: episode: 77, duration: 0.362s, episode steps:  31, steps per second:  86, episode reward: 32.084, mean reward:  1.035 [-2.459, 32.150], mean action: 9.194 [0.000, 19.000],  loss: 0.019572, mae: 0.439407, mean_q: 0.419684, mean_eps: 0.000000
 1870/5000: episode: 78, duration: 0.385s, episode steps:  31, steps per second:  80, episode reward: 32.457, mean reward:  1.047 [-2.417, 32.190], mean action: 4.452 [0.000, 15.000],  loss: 0.019519, mae: 0.431972, mean_q: 0.469232, mean_eps: 0.000000
 1891/5000: episode: 79, duration: 0.410s, episode steps:  21, steps per second:  51, episode reward: 38.065, mean reward:  1.813 [-2.475, 32.278], mean action: 5.524 [0.000, 14.000],  loss: 0.021029, mae: 0.437422, mean_q: 0.496086, mean_eps: 0.000000
 1905/5000: episode: 80, duration: 0.238s, episode steps:  14, steps per second:  59, episode reward: 41.023, mean reward:  2.930 [-3.000, 33.000], mean action: 2.071 [0.000, 11.000],  loss: 0.022576, mae: 0.456018, mean_q: 0.482875, mean_eps: 0.000000
 1939/5000: episode: 81, duration: 0.411s, episode steps:  34, steps per second:  83, episode reward: -39.000, mean reward: -1.147 [-32.953,  2.393], mean action: 8.147 [0.000, 14.000],  loss: 0.019300, mae: 0.437926, mean_q: 0.483227, mean_eps: 0.000000
 1966/5000: episode: 82, duration: 0.324s, episode steps:  27, steps per second:  83, episode reward: 38.635, mean reward:  1.431 [-2.878, 32.120], mean action: 4.074 [0.000, 14.000],  loss: 0.021979, mae: 0.448047, mean_q: 0.512961, mean_eps: 0.000000
 2017/5000: episode: 83, duration: 0.623s, episode steps:  51, steps per second:  82, episode reward: -32.910, mean reward: -0.645 [-32.283,  2.420], mean action: 3.667 [0.000, 19.000],  loss: 0.020117, mae: 0.452579, mean_q: 0.439875, mean_eps: 0.000000
 2036/5000: episode: 84, duration: 0.242s, episode steps:  19, steps per second:  78, episode reward: 35.625, mean reward:  1.875 [-2.718, 33.000], mean action: 4.000 [0.000, 19.000],  loss: 0.018747, mae: 0.457145, mean_q: 0.465137, mean_eps: 0.000000
 2062/5000: episode: 85, duration: 0.322s, episode steps:  26, steps per second:  81, episode reward: -35.530, mean reward: -1.367 [-30.290,  2.653], mean action: 5.500 [0.000, 17.000],  loss: 0.018918, mae: 0.451840, mean_q: 0.424226, mean_eps: 0.000000
 2090/5000: episode: 86, duration: 0.340s, episode steps:  28, steps per second:  82, episode reward: -33.000, mean reward: -1.179 [-32.271,  2.269], mean action: 5.286 [0.000, 14.000],  loss: 0.019416, mae: 0.454632, mean_q: 0.438831, mean_eps: 0.000000
 2112/5000: episode: 87, duration: 0.296s, episode steps:  22, steps per second:  74, episode reward: -38.460, mean reward: -1.748 [-33.000,  2.151], mean action: 5.273 [1.000, 14.000],  loss: 0.023324, mae: 0.476265, mean_q: 0.446402, mean_eps: 0.000000
 2132/5000: episode: 88, duration: 0.257s, episode steps:  20, steps per second:  78, episode reward: 38.938, mean reward:  1.947 [-2.340, 32.348], mean action: 2.700 [0.000, 12.000],  loss: 0.019396, mae: 0.452173, mean_q: 0.472271, mean_eps: 0.000000
 2160/5000: episode: 89, duration: 0.368s, episode steps:  28, steps per second:  76, episode reward: 32.426, mean reward:  1.158 [-3.000, 32.090], mean action: 6.643 [0.000, 19.000],  loss: 0.018688, mae: 0.439402, mean_q: 0.481970, mean_eps: 0.000000
 2188/5000: episode: 90, duration: 0.359s, episode steps:  28, steps per second:  78, episode reward: 35.764, mean reward:  1.277 [-2.641, 32.290], mean action: 4.429 [0.000, 18.000],  loss: 0.018186, mae: 0.441537, mean_q: 0.404904, mean_eps: 0.000000
 2204/5000: episode: 91, duration: 0.201s, episode steps:  16, steps per second:  80, episode reward: 42.000, mean reward:  2.625 [-2.270, 32.100], mean action: 4.875 [1.000, 19.000],  loss: 0.015883, mae: 0.428035, mean_q: 0.409651, mean_eps: 0.000000
 2228/5000: episode: 92, duration: 0.283s, episode steps:  24, steps per second:  85, episode reward: -32.930, mean reward: -1.372 [-32.930,  2.675], mean action: 8.250 [0.000, 21.000],  loss: 0.014629, mae: 0.414932, mean_q: 0.453714, mean_eps: 0.000000
 2256/5000: episode: 93, duration: 0.345s, episode steps:  28, steps per second:  81, episode reward: -35.540, mean reward: -1.269 [-32.068,  3.061], mean action: 7.179 [0.000, 20.000],  loss: 0.020655, mae: 0.443883, mean_q: 0.458014, mean_eps: 0.000000
 2279/5000: episode: 94, duration: 0.289s, episode steps:  23, steps per second:  80, episode reward: 35.811, mean reward:  1.557 [-3.000, 32.250], mean action: 4.478 [0.000, 15.000],  loss: 0.021052, mae: 0.458792, mean_q: 0.398956, mean_eps: 0.000000
 2294/5000: episode: 95, duration: 0.194s, episode steps:  15, steps per second:  77, episode reward: 38.301, mean reward:  2.553 [-3.000, 33.000], mean action: 3.067 [0.000, 9.000],  loss: 0.026012, mae: 0.476337, mean_q: 0.406014, mean_eps: 0.000000
 2316/5000: episode: 96, duration: 0.300s, episode steps:  22, steps per second:  73, episode reward: 36.000, mean reward:  1.636 [-2.618, 33.000], mean action: 3.682 [0.000, 19.000],  loss: 0.016124, mae: 0.431908, mean_q: 0.418415, mean_eps: 0.000000
 2328/5000: episode: 97, duration: 0.166s, episode steps:  12, steps per second:  72, episode reward: 43.762, mean reward:  3.647 [-2.118, 33.000], mean action: 4.917 [0.000, 19.000],  loss: 0.018995, mae: 0.454062, mean_q: 0.408242, mean_eps: 0.000000
 2375/5000: episode: 98, duration: 0.554s, episode steps:  47, steps per second:  85, episode reward: 32.354, mean reward:  0.688 [-2.250, 32.090], mean action: 7.872 [0.000, 15.000],  loss: 0.019112, mae: 0.439995, mean_q: 0.444711, mean_eps: 0.000000
 2403/5000: episode: 99, duration: 0.335s, episode steps:  28, steps per second:  84, episode reward: 32.413, mean reward:  1.158 [-2.167, 32.413], mean action: 5.393 [0.000, 19.000],  loss: 0.020350, mae: 0.444245, mean_q: 0.438505, mean_eps: 0.000000
 2430/5000: episode: 100, duration: 0.328s, episode steps:  27, steps per second:  82, episode reward: -32.140, mean reward: -1.190 [-32.327,  2.380], mean action: 8.407 [0.000, 19.000],  loss: 0.019674, mae: 0.451616, mean_q: 0.424473, mean_eps: 0.000000
 2456/5000: episode: 101, duration: 0.314s, episode steps:  26, steps per second:  83, episode reward: -32.300, mean reward: -1.242 [-32.233,  2.371], mean action: 3.423 [0.000, 19.000],  loss: 0.019039, mae: 0.459047, mean_q: 0.391239, mean_eps: 0.000000
 2476/5000: episode: 102, duration: 0.243s, episode steps:  20, steps per second:  82, episode reward: 43.232, mean reward:  2.162 [-2.063, 32.514], mean action: 5.850 [0.000, 20.000],  loss: 0.021240, mae: 0.460498, mean_q: 0.400351, mean_eps: 0.000000
 2515/5000: episode: 103, duration: 0.469s, episode steps:  39, steps per second:  83, episode reward: -35.020, mean reward: -0.898 [-32.082,  2.585], mean action: 7.846 [0.000, 18.000],  loss: 0.019808, mae: 0.442110, mean_q: 0.471264, mean_eps: 0.000000
 2533/5000: episode: 104, duration: 0.219s, episode steps:  18, steps per second:  82, episode reward: 36.000, mean reward:  2.000 [-3.000, 33.000], mean action: 6.389 [0.000, 15.000],  loss: 0.023646, mae: 0.451316, mean_q: 0.459595, mean_eps: 0.000000
 2563/5000: episode: 105, duration: 0.344s, episode steps:  30, steps per second:  87, episode reward: -35.790, mean reward: -1.193 [-32.239,  2.060], mean action: 6.433 [0.000, 15.000],  loss: 0.022849, mae: 0.459423, mean_q: 0.460423, mean_eps: 0.000000
 2597/5000: episode: 106, duration: 0.400s, episode steps:  34, steps per second:  85, episode reward: 35.271, mean reward:  1.037 [-2.374, 32.280], mean action: 4.029 [0.000, 15.000],  loss: 0.018627, mae: 0.436560, mean_q: 0.449071, mean_eps: 0.000000
 2627/5000: episode: 107, duration: 0.346s, episode steps:  30, steps per second:  87, episode reward: -32.710, mean reward: -1.090 [-32.362,  2.510], mean action: 10.633 [0.000, 21.000],  loss: 0.021983, mae: 0.449641, mean_q: 0.477855, mean_eps: 0.000000
 2658/5000: episode: 108, duration: 0.351s, episode steps:  31, steps per second:  88, episode reward: 35.679, mean reward:  1.151 [-3.000, 32.129], mean action: 3.258 [0.000, 14.000],  loss: 0.023816, mae: 0.460992, mean_q: 0.464641, mean_eps: 0.000000
 2680/5000: episode: 109, duration: 0.266s, episode steps:  22, steps per second:  83, episode reward: 38.287, mean reward:  1.740 [-2.317, 31.921], mean action: 4.000 [1.000, 12.000],  loss: 0.021889, mae: 0.450882, mean_q: 0.483512, mean_eps: 0.000000
 2701/5000: episode: 110, duration: 0.263s, episode steps:  21, steps per second:  80, episode reward: 35.778, mean reward:  1.704 [-2.628, 32.048], mean action: 4.952 [0.000, 14.000],  loss: 0.019603, mae: 0.437397, mean_q: 0.529347, mean_eps: 0.000000
 2733/5000: episode: 111, duration: 0.373s, episode steps:  32, steps per second:  86, episode reward: 32.397, mean reward:  1.012 [-2.908, 32.007], mean action: 7.406 [0.000, 14.000],  loss: 0.016808, mae: 0.428407, mean_q: 0.458514, mean_eps: 0.000000
 2750/5000: episode: 112, duration: 0.217s, episode steps:  17, steps per second:  78, episode reward: 41.096, mean reward:  2.417 [-2.208, 31.731], mean action: 2.941 [0.000, 11.000],  loss: 0.017550, mae: 0.432618, mean_q: 0.425604, mean_eps: 0.000000
 2765/5000: episode: 113, duration: 0.187s, episode steps:  15, steps per second:  80, episode reward: 39.000, mean reward:  2.600 [-2.359, 32.160], mean action: 3.400 [0.000, 14.000],  loss: 0.015884, mae: 0.425074, mean_q: 0.417122, mean_eps: 0.000000
 2796/5000: episode: 114, duration: 0.361s, episode steps:  31, steps per second:  86, episode reward: -36.000, mean reward: -1.161 [-32.304,  2.304], mean action: 10.968 [0.000, 16.000],  loss: 0.019239, mae: 0.438555, mean_q: 0.448693, mean_eps: 0.000000
 2817/5000: episode: 115, duration: 0.244s, episode steps:  21, steps per second:  86, episode reward: 38.444, mean reward:  1.831 [-2.998, 33.000], mean action: 5.762 [0.000, 14.000],  loss: 0.020972, mae: 0.437273, mean_q: 0.460644, mean_eps: 0.000000
 2839/5000: episode: 116, duration: 0.265s, episode steps:  22, steps per second:  83, episode reward: -32.190, mean reward: -1.463 [-32.422,  3.000], mean action: 6.364 [0.000, 20.000],  loss: 0.020176, mae: 0.435471, mean_q: 0.460729, mean_eps: 0.000000
 2855/5000: episode: 117, duration: 0.194s, episode steps:  16, steps per second:  83, episode reward: 37.957, mean reward:  2.372 [-3.000, 32.316], mean action: 3.438 [0.000, 9.000],  loss: 0.017989, mae: 0.429732, mean_q: 0.405181, mean_eps: 0.000000
 2876/5000: episode: 118, duration: 0.250s, episode steps:  21, steps per second:  84, episode reward: 32.494, mean reward:  1.547 [-3.000, 32.749], mean action: 5.619 [0.000, 17.000],  loss: 0.020359, mae: 0.449123, mean_q: 0.450296, mean_eps: 0.000000
 2909/5000: episode: 119, duration: 0.415s, episode steps:  33, steps per second:  79, episode reward: -32.060, mean reward: -0.972 [-32.326,  2.500], mean action: 3.091 [0.000, 12.000],  loss: 0.019883, mae: 0.448749, mean_q: 0.440958, mean_eps: 0.000000
 2937/5000: episode: 120, duration: 0.329s, episode steps:  28, steps per second:  85, episode reward: -35.710, mean reward: -1.275 [-32.172,  2.532], mean action: 8.036 [0.000, 18.000],  loss: 0.019368, mae: 0.435223, mean_q: 0.418530, mean_eps: 0.000000
 2961/5000: episode: 121, duration: 0.298s, episode steps:  24, steps per second:  81, episode reward: 38.293, mean reward:  1.596 [-2.721, 31.313], mean action: 6.708 [0.000, 20.000],  loss: 0.022283, mae: 0.448055, mean_q: 0.490727, mean_eps: 0.000000
 2989/5000: episode: 122, duration: 0.334s, episode steps:  28, steps per second:  84, episode reward: -32.480, mean reward: -1.160 [-31.903,  2.773], mean action: 4.964 [0.000, 18.000],  loss: 0.022491, mae: 0.455591, mean_q: 0.537373, mean_eps: 0.000000
 3013/5000: episode: 123, duration: 0.297s, episode steps:  24, steps per second:  81, episode reward: -39.000, mean reward: -1.625 [-32.290,  2.421], mean action: 6.167 [0.000, 20.000],  loss: 0.017384, mae: 0.431295, mean_q: 0.519691, mean_eps: 0.000000
 3035/5000: episode: 124, duration: 0.276s, episode steps:  22, steps per second:  80, episode reward: 38.872, mean reward:  1.767 [-2.593, 32.172], mean action: 3.500 [0.000, 15.000],  loss: 0.020691, mae: 0.445159, mean_q: 0.519118, mean_eps: 0.000000
 3057/5000: episode: 125, duration: 0.271s, episode steps:  22, steps per second:  81, episode reward: -32.550, mean reward: -1.480 [-32.777,  2.903], mean action: 6.955 [0.000, 19.000],  loss: 0.020133, mae: 0.432660, mean_q: 0.450338, mean_eps: 0.000000
 3078/5000: episode: 126, duration: 0.263s, episode steps:  21, steps per second:  80, episode reward: 41.099, mean reward:  1.957 [-2.409, 33.000], mean action: 6.905 [0.000, 21.000],  loss: 0.024919, mae: 0.453886, mean_q: 0.498225, mean_eps: 0.000000
 3094/5000: episode: 127, duration: 0.196s, episode steps:  16, steps per second:  82, episode reward: 35.901, mean reward:  2.244 [-2.722, 32.061], mean action: 5.375 [0.000, 15.000],  loss: 0.017971, mae: 0.428233, mean_q: 0.506157, mean_eps: 0.000000
 3148/5000: episode: 128, duration: 0.634s, episode steps:  54, steps per second:  85, episode reward: 35.042, mean reward:  0.649 [-3.000, 32.080], mean action: 4.037 [0.000, 19.000],  loss: 0.019817, mae: 0.436153, mean_q: 0.459315, mean_eps: 0.000000
 3173/5000: episode: 129, duration: 0.312s, episode steps:  25, steps per second:  80, episode reward: 38.307, mean reward:  1.532 [-2.193, 31.608], mean action: 5.040 [1.000, 19.000],  loss: 0.017332, mae: 0.421486, mean_q: 0.466940, mean_eps: 0.000000
 3195/5000: episode: 130, duration: 0.264s, episode steps:  22, steps per second:  83, episode reward: 35.648, mean reward:  1.620 [-2.244, 32.755], mean action: 4.727 [0.000, 19.000],  loss: 0.017906, mae: 0.424436, mean_q: 0.408956, mean_eps: 0.000000
 3234/5000: episode: 131, duration: 0.501s, episode steps:  39, steps per second:  78, episode reward: 38.681, mean reward:  0.992 [-2.563, 32.041], mean action: 4.564 [0.000, 19.000],  loss: 0.018243, mae: 0.423729, mean_q: 0.381175, mean_eps: 0.000000
 3258/5000: episode: 132, duration: 0.308s, episode steps:  24, steps per second:  78, episode reward: 35.163, mean reward:  1.465 [-2.526, 31.816], mean action: 5.000 [0.000, 19.000],  loss: 0.018003, mae: 0.430228, mean_q: 0.417041, mean_eps: 0.000000
 3302/5000: episode: 133, duration: 0.556s, episode steps:  44, steps per second:  79, episode reward: 38.303, mean reward:  0.871 [-2.696, 32.006], mean action: 2.659 [0.000, 19.000],  loss: 0.020528, mae: 0.444430, mean_q: 0.422662, mean_eps: 0.000000
 3325/5000: episode: 134, duration: 0.276s, episode steps:  23, steps per second:  83, episode reward: -38.460, mean reward: -1.672 [-32.286,  2.010], mean action: 4.783 [0.000, 19.000],  loss: 0.019818, mae: 0.441661, mean_q: 0.437271, mean_eps: 0.000000
 3334/5000: episode: 135, duration: 0.124s, episode steps:   9, steps per second:  73, episode reward: 47.273, mean reward:  5.253 [ 0.332, 32.053], mean action: 4.222 [3.000, 14.000],  loss: 0.025133, mae: 0.469116, mean_q: 0.402307, mean_eps: 0.000000
 3372/5000: episode: 136, duration: 0.462s, episode steps:  38, steps per second:  82, episode reward: -35.800, mean reward: -0.942 [-32.485,  2.480], mean action: 5.158 [0.000, 19.000],  loss: 0.020613, mae: 0.449923, mean_q: 0.419269, mean_eps: 0.000000
 3390/5000: episode: 137, duration: 0.265s, episode steps:  18, steps per second:  68, episode reward: 38.038, mean reward:  2.113 [-3.000, 31.882], mean action: 3.444 [0.000, 19.000],  loss: 0.019987, mae: 0.447229, mean_q: 0.427212, mean_eps: 0.000000
 3424/5000: episode: 138, duration: 0.402s, episode steps:  34, steps per second:  85, episode reward: -33.000, mean reward: -0.971 [-30.110,  2.500], mean action: 6.735 [0.000, 14.000],  loss: 0.020709, mae: 0.448767, mean_q: 0.471141, mean_eps: 0.000000
 3452/5000: episode: 139, duration: 0.322s, episode steps:  28, steps per second:  87, episode reward: -38.880, mean reward: -1.389 [-32.389,  2.630], mean action: 8.571 [0.000, 15.000],  loss: 0.022386, mae: 0.454440, mean_q: 0.415198, mean_eps: 0.000000
 3466/5000: episode: 140, duration: 0.173s, episode steps:  14, steps per second:  81, episode reward: 41.441, mean reward:  2.960 [-3.000, 32.341], mean action: 3.286 [1.000, 9.000],  loss: 0.026105, mae: 0.465243, mean_q: 0.456982, mean_eps: 0.000000
 3496/5000: episode: 141, duration: 0.363s, episode steps:  30, steps per second:  83, episode reward: 40.796, mean reward:  1.360 [-2.150, 32.470], mean action: 5.633 [0.000, 15.000],  loss: 0.019984, mae: 0.434759, mean_q: 0.437850, mean_eps: 0.000000
 3526/5000: episode: 142, duration: 0.355s, episode steps:  30, steps per second:  85, episode reward: -36.000, mean reward: -1.200 [-32.243,  2.660], mean action: 6.500 [0.000, 21.000],  loss: 0.018698, mae: 0.429091, mean_q: 0.424176, mean_eps: 0.000000
 3549/5000: episode: 143, duration: 0.270s, episode steps:  23, steps per second:  85, episode reward: 38.025, mean reward:  1.653 [-2.413, 32.310], mean action: 5.087 [0.000, 20.000],  loss: 0.020614, mae: 0.443023, mean_q: 0.412324, mean_eps: 0.000000
 3573/5000: episode: 144, duration: 0.303s, episode steps:  24, steps per second:  79, episode reward: 35.689, mean reward:  1.487 [-2.928, 31.891], mean action: 6.042 [0.000, 19.000],  loss: 0.017566, mae: 0.429082, mean_q: 0.396528, mean_eps: 0.000000
 3586/5000: episode: 145, duration: 0.204s, episode steps:  13, steps per second:  64, episode reward: 41.248, mean reward:  3.173 [-2.620, 32.697], mean action: 4.769 [0.000, 19.000],  loss: 0.022051, mae: 0.447034, mean_q: 0.455639, mean_eps: 0.000000
 3603/5000: episode: 146, duration: 0.225s, episode steps:  17, steps per second:  76, episode reward: 38.695, mean reward:  2.276 [-2.361, 31.905], mean action: 3.941 [1.000, 19.000],  loss: 0.022927, mae: 0.455523, mean_q: 0.530925, mean_eps: 0.000000
 3619/5000: episode: 147, duration: 0.202s, episode steps:  16, steps per second:  79, episode reward: 43.456, mean reward:  2.716 [-2.112, 32.020], mean action: 2.188 [0.000, 11.000],  loss: 0.022266, mae: 0.451043, mean_q: 0.558951, mean_eps: 0.000000
 3633/5000: episode: 148, duration: 0.171s, episode steps:  14, steps per second:  82, episode reward: 41.876, mean reward:  2.991 [-2.097, 32.650], mean action: 3.429 [1.000, 11.000],  loss: 0.026050, mae: 0.470497, mean_q: 0.492118, mean_eps: 0.000000
 3659/5000: episode: 149, duration: 0.306s, episode steps:  26, steps per second:  85, episode reward: 32.126, mean reward:  1.236 [-2.648, 32.714], mean action: 4.423 [0.000, 14.000],  loss: 0.020846, mae: 0.441971, mean_q: 0.483747, mean_eps: 0.000000
 3686/5000: episode: 150, duration: 0.311s, episode steps:  27, steps per second:  87, episode reward: -35.190, mean reward: -1.303 [-32.159,  2.565], mean action: 6.556 [0.000, 14.000],  loss: 0.020294, mae: 0.435567, mean_q: 0.501585, mean_eps: 0.000000
 3708/5000: episode: 151, duration: 0.264s, episode steps:  22, steps per second:  83, episode reward: -32.940, mean reward: -1.497 [-33.000,  2.200], mean action: 5.364 [0.000, 14.000],  loss: 0.018145, mae: 0.431402, mean_q: 0.503439, mean_eps: 0.000000
 3750/5000: episode: 152, duration: 0.488s, episode steps:  42, steps per second:  86, episode reward: 38.719, mean reward:  0.922 [-2.152, 30.342], mean action: 3.548 [0.000, 13.000],  loss: 0.016944, mae: 0.431732, mean_q: 0.484787, mean_eps: 0.000000
 3788/5000: episode: 153, duration: 0.432s, episode steps:  38, steps per second:  88, episode reward: -32.620, mean reward: -0.858 [-32.331,  3.000], mean action: 6.342 [0.000, 11.000],  loss: 0.022558, mae: 0.451423, mean_q: 0.466114, mean_eps: 0.000000
 3808/5000: episode: 154, duration: 0.241s, episode steps:  20, steps per second:  83, episode reward: 38.429, mean reward:  1.921 [-3.000, 32.334], mean action: 4.850 [0.000, 21.000],  loss: 0.016668, mae: 0.422401, mean_q: 0.493290, mean_eps: 0.000000
 3821/5000: episode: 155, duration: 0.178s, episode steps:  13, steps per second:  73, episode reward: 44.232, mean reward:  3.402 [-2.215, 32.537], mean action: 3.692 [0.000, 19.000],  loss: 0.016924, mae: 0.415923, mean_q: 0.494879, mean_eps: 0.000000
 3846/5000: episode: 156, duration: 0.323s, episode steps:  25, steps per second:  77, episode reward: 32.528, mean reward:  1.301 [-2.468, 32.038], mean action: 6.040 [0.000, 19.000],  loss: 0.020657, mae: 0.437243, mean_q: 0.469662, mean_eps: 0.000000
 3863/5000: episode: 157, duration: 0.329s, episode steps:  17, steps per second:  52, episode reward: 38.557, mean reward:  2.268 [-3.000, 30.445], mean action: 8.059 [0.000, 20.000],  loss: 0.018528, mae: 0.419973, mean_q: 0.485187, mean_eps: 0.000000
 3885/5000: episode: 158, duration: 0.311s, episode steps:  22, steps per second:  71, episode reward: 35.778, mean reward:  1.626 [-2.647, 32.190], mean action: 3.591 [0.000, 12.000],  loss: 0.022802, mae: 0.450277, mean_q: 0.478805, mean_eps: 0.000000
 3905/5000: episode: 159, duration: 0.344s, episode steps:  20, steps per second:  58, episode reward: 41.518, mean reward:  2.076 [-2.096, 32.190], mean action: 4.600 [0.000, 19.000],  loss: 0.014796, mae: 0.416025, mean_q: 0.456111, mean_eps: 0.000000
 3923/5000: episode: 160, duration: 0.249s, episode steps:  18, steps per second:  72, episode reward: 35.523, mean reward:  1.973 [-3.000, 32.904], mean action: 8.222 [0.000, 15.000],  loss: 0.017551, mae: 0.429111, mean_q: 0.433397, mean_eps: 0.000000
 3965/5000: episode: 161, duration: 0.496s, episode steps:  42, steps per second:  85, episode reward: -35.780, mean reward: -0.852 [-32.440,  3.000], mean action: 5.333 [0.000, 17.000],  loss: 0.023235, mae: 0.458145, mean_q: 0.466176, mean_eps: 0.000000
 4001/5000: episode: 162, duration: 0.472s, episode steps:  36, steps per second:  76, episode reward: 41.310, mean reward:  1.148 [-2.144, 32.500], mean action: 2.417 [0.000, 14.000],  loss: 0.022031, mae: 0.450689, mean_q: 0.524679, mean_eps: 0.000000
 4023/5000: episode: 163, duration: 0.282s, episode steps:  22, steps per second:  78, episode reward: 35.490, mean reward:  1.613 [-2.492, 32.210], mean action: 3.682 [0.000, 15.000],  loss: 0.015374, mae: 0.401233, mean_q: 0.497984, mean_eps: 0.000000
 4038/5000: episode: 164, duration: 0.202s, episode steps:  15, steps per second:  74, episode reward: 38.469, mean reward:  2.565 [-3.000, 32.568], mean action: 4.800 [0.000, 21.000],  loss: 0.022385, mae: 0.437894, mean_q: 0.474634, mean_eps: 0.000000
 4061/5000: episode: 165, duration: 0.298s, episode steps:  23, steps per second:  77, episode reward: 35.335, mean reward:  1.536 [-2.900, 32.288], mean action: 8.783 [0.000, 20.000],  loss: 0.016704, mae: 0.415152, mean_q: 0.450649, mean_eps: 0.000000
 4089/5000: episode: 166, duration: 0.344s, episode steps:  28, steps per second:  81, episode reward: -32.780, mean reward: -1.171 [-31.974,  2.480], mean action: 4.321 [0.000, 13.000],  loss: 0.018246, mae: 0.441655, mean_q: 0.411891, mean_eps: 0.000000
 4116/5000: episode: 167, duration: 0.346s, episode steps:  27, steps per second:  78, episode reward: 35.069, mean reward:  1.299 [-2.473, 32.230], mean action: 3.000 [0.000, 12.000],  loss: 0.024379, mae: 0.472168, mean_q: 0.400478, mean_eps: 0.000000
 4140/5000: episode: 168, duration: 0.324s, episode steps:  24, steps per second:  74, episode reward: 32.808, mean reward:  1.367 [-2.430, 29.875], mean action: 5.583 [1.000, 15.000],  loss: 0.023407, mae: 0.458593, mean_q: 0.455629, mean_eps: 0.000000
 4172/5000: episode: 169, duration: 0.399s, episode steps:  32, steps per second:  80, episode reward: 38.421, mean reward:  1.201 [-2.465, 32.331], mean action: 4.344 [0.000, 14.000],  loss: 0.021253, mae: 0.443394, mean_q: 0.451492, mean_eps: 0.000000
 4197/5000: episode: 170, duration: 0.315s, episode steps:  25, steps per second:  79, episode reward: 35.259, mean reward:  1.410 [-3.000, 32.489], mean action: 3.600 [0.000, 12.000],  loss: 0.021453, mae: 0.443400, mean_q: 0.480738, mean_eps: 0.000000
 4211/5000: episode: 171, duration: 0.178s, episode steps:  14, steps per second:  78, episode reward: -44.430, mean reward: -3.174 [-33.000,  1.712], mean action: 4.786 [0.000, 16.000],  loss: 0.021703, mae: 0.441677, mean_q: 0.437452, mean_eps: 0.000000
 4239/5000: episode: 172, duration: 0.336s, episode steps:  28, steps per second:  83, episode reward: 32.708, mean reward:  1.168 [-2.903, 32.322], mean action: 6.107 [0.000, 15.000],  loss: 0.019034, mae: 0.438757, mean_q: 0.393888, mean_eps: 0.000000
 4257/5000: episode: 173, duration: 0.219s, episode steps:  18, steps per second:  82, episode reward: 41.238, mean reward:  2.291 [-2.327, 32.737], mean action: 4.611 [0.000, 14.000],  loss: 0.019160, mae: 0.431268, mean_q: 0.416878, mean_eps: 0.000000
 4280/5000: episode: 174, duration: 0.279s, episode steps:  23, steps per second:  83, episode reward: 35.255, mean reward:  1.533 [-2.446, 32.530], mean action: 3.435 [0.000, 11.000],  loss: 0.020620, mae: 0.427571, mean_q: 0.466384, mean_eps: 0.000000
 4303/5000: episode: 175, duration: 0.294s, episode steps:  23, steps per second:  78, episode reward: 32.461, mean reward:  1.411 [-3.000, 32.032], mean action: 5.217 [0.000, 19.000],  loss: 0.022380, mae: 0.443528, mean_q: 0.437485, mean_eps: 0.000000
 4326/5000: episode: 176, duration: 0.296s, episode steps:  23, steps per second:  78, episode reward: 36.000, mean reward:  1.565 [-2.903, 32.100], mean action: 5.565 [0.000, 21.000],  loss: 0.022464, mae: 0.440648, mean_q: 0.420202, mean_eps: 0.000000
 4348/5000: episode: 177, duration: 0.329s, episode steps:  22, steps per second:  67, episode reward: -35.150, mean reward: -1.598 [-32.435,  2.690], mean action: 5.409 [0.000, 14.000],  loss: 0.018726, mae: 0.426240, mean_q: 0.453147, mean_eps: 0.000000
 4368/5000: episode: 178, duration: 0.267s, episode steps:  20, steps per second:  75, episode reward: 32.162, mean reward:  1.608 [-3.000, 32.951], mean action: 5.800 [0.000, 16.000],  loss: 0.017262, mae: 0.425650, mean_q: 0.432313, mean_eps: 0.000000
 4392/5000: episode: 179, duration: 0.295s, episode steps:  24, steps per second:  81, episode reward: 36.000, mean reward:  1.500 [-2.732, 33.000], mean action: 4.292 [0.000, 19.000],  loss: 0.020512, mae: 0.432100, mean_q: 0.466970, mean_eps: 0.000000
 4413/5000: episode: 180, duration: 0.258s, episode steps:  21, steps per second:  81, episode reward: -38.370, mean reward: -1.827 [-32.153,  2.480], mean action: 6.810 [0.000, 15.000],  loss: 0.019763, mae: 0.426327, mean_q: 0.477515, mean_eps: 0.000000
 4425/5000: episode: 181, duration: 0.160s, episode steps:  12, steps per second:  75, episode reward: 44.128, mean reward:  3.677 [-2.169, 32.313], mean action: 3.833 [1.000, 14.000],  loss: 0.020214, mae: 0.429817, mean_q: 0.481318, mean_eps: 0.000000
 4447/5000: episode: 182, duration: 0.262s, episode steps:  22, steps per second:  84, episode reward: 35.517, mean reward:  1.614 [-2.429, 32.063], mean action: 3.409 [0.000, 19.000],  loss: 0.018016, mae: 0.424779, mean_q: 0.443040, mean_eps: 0.000000
 4464/5000: episode: 183, duration: 0.210s, episode steps:  17, steps per second:  81, episode reward: 41.023, mean reward:  2.413 [-2.395, 33.000], mean action: 7.000 [3.000, 15.000],  loss: 0.022947, mae: 0.441205, mean_q: 0.442600, mean_eps: 0.000000
 4493/5000: episode: 184, duration: 0.339s, episode steps:  29, steps per second:  85, episode reward: -33.000, mean reward: -1.138 [-32.017,  3.000], mean action: 6.966 [0.000, 20.000],  loss: 0.019599, mae: 0.423489, mean_q: 0.435649, mean_eps: 0.000000
 4518/5000: episode: 185, duration: 0.297s, episode steps:  25, steps per second:  84, episode reward: 32.475, mean reward:  1.299 [-2.820, 32.205], mean action: 4.920 [0.000, 18.000],  loss: 0.022655, mae: 0.442462, mean_q: 0.461551, mean_eps: 0.000000
 4547/5000: episode: 186, duration: 0.340s, episode steps:  29, steps per second:  85, episode reward: 32.900, mean reward:  1.134 [-2.681, 32.150], mean action: 3.966 [0.000, 14.000],  loss: 0.019759, mae: 0.430202, mean_q: 0.433096, mean_eps: 0.000000
 4574/5000: episode: 187, duration: 0.332s, episode steps:  27, steps per second:  81, episode reward: -35.710, mean reward: -1.323 [-32.169,  3.000], mean action: 6.593 [0.000, 14.000],  loss: 0.020325, mae: 0.434606, mean_q: 0.451289, mean_eps: 0.000000
 4594/5000: episode: 188, duration: 0.277s, episode steps:  20, steps per second:  72, episode reward: 36.000, mean reward:  1.800 [-3.000, 32.040], mean action: 4.750 [0.000, 19.000],  loss: 0.021396, mae: 0.431722, mean_q: 0.461854, mean_eps: 0.000000
 4622/5000: episode: 189, duration: 0.338s, episode steps:  28, steps per second:  83, episode reward: -35.290, mean reward: -1.260 [-32.302,  3.000], mean action: 7.000 [0.000, 19.000],  loss: 0.021419, mae: 0.445656, mean_q: 0.458541, mean_eps: 0.000000
 4644/5000: episode: 190, duration: 0.273s, episode steps:  22, steps per second:  81, episode reward: 33.000, mean reward:  1.500 [-3.000, 32.120], mean action: 4.091 [0.000, 15.000],  loss: 0.019985, mae: 0.448687, mean_q: 0.419891, mean_eps: 0.000000
 4668/5000: episode: 191, duration: 0.301s, episode steps:  24, steps per second:  80, episode reward: 36.000, mean reward:  1.500 [-3.000, 32.160], mean action: 3.625 [0.000, 12.000],  loss: 0.020602, mae: 0.453226, mean_q: 0.427297, mean_eps: 0.000000
 4687/5000: episode: 192, duration: 0.243s, episode steps:  19, steps per second:  78, episode reward: 38.284, mean reward:  2.015 [-3.000, 32.300], mean action: 4.842 [0.000, 14.000],  loss: 0.018757, mae: 0.436662, mean_q: 0.472028, mean_eps: 0.000000
 4709/5000: episode: 193, duration: 0.299s, episode steps:  22, steps per second:  74, episode reward: 35.569, mean reward:  1.617 [-2.643, 31.759], mean action: 2.955 [0.000, 12.000],  loss: 0.022534, mae: 0.459018, mean_q: 0.484006, mean_eps: 0.000000
 4733/5000: episode: 194, duration: 0.314s, episode steps:  24, steps per second:  76, episode reward: 32.569, mean reward:  1.357 [-3.000, 32.240], mean action: 6.667 [0.000, 14.000],  loss: 0.016965, mae: 0.433055, mean_q: 0.415058, mean_eps: 0.000000
 4761/5000: episode: 195, duration: 0.350s, episode steps:  28, steps per second:  80, episode reward: 38.772, mean reward:  1.385 [-2.656, 32.340], mean action: 6.786 [0.000, 20.000],  loss: 0.018283, mae: 0.449303, mean_q: 0.405456, mean_eps: 0.000000
 4780/5000: episode: 196, duration: 0.276s, episode steps:  19, steps per second:  69, episode reward: 39.000, mean reward:  2.053 [-2.313, 32.440], mean action: 3.263 [0.000, 12.000],  loss: 0.018823, mae: 0.446760, mean_q: 0.453569, mean_eps: 0.000000
 4806/5000: episode: 197, duration: 0.486s, episode steps:  26, steps per second:  53, episode reward: 32.372, mean reward:  1.245 [-3.000, 32.319], mean action: 8.077 [0.000, 20.000],  loss: 0.021579, mae: 0.448080, mean_q: 0.460928, mean_eps: 0.000000
 4840/5000: episode: 198, duration: 0.474s, episode steps:  34, steps per second:  72, episode reward: 32.180, mean reward:  0.946 [-3.000, 32.110], mean action: 8.529 [0.000, 20.000],  loss: 0.018628, mae: 0.437952, mean_q: 0.448326, mean_eps: 0.000000
 4866/5000: episode: 199, duration: 0.344s, episode steps:  26, steps per second:  76, episode reward: 38.201, mean reward:  1.469 [-2.246, 32.040], mean action: 3.577 [0.000, 11.000],  loss: 0.022077, mae: 0.447373, mean_q: 0.477292, mean_eps: 0.000000
 4885/5000: episode: 200, duration: 0.273s, episode steps:  19, steps per second:  70, episode reward: 38.407, mean reward:  2.021 [-2.634, 32.030], mean action: 4.000 [0.000, 14.000],  loss: 0.018414, mae: 0.434962, mean_q: 0.471740, mean_eps: 0.000000
 4913/5000: episode: 201, duration: 0.369s, episode steps:  28, steps per second:  76, episode reward: 32.229, mean reward:  1.151 [-2.623, 31.996], mean action: 4.571 [0.000, 14.000],  loss: 0.023003, mae: 0.449623, mean_q: 0.441612, mean_eps: 0.000000
 4929/5000: episode: 202, duration: 0.235s, episode steps:  16, steps per second:  68, episode reward: 41.523, mean reward:  2.595 [-2.278, 32.080], mean action: 5.625 [0.000, 15.000],  loss: 0.018856, mae: 0.442357, mean_q: 0.406854, mean_eps: 0.000000
 4969/5000: episode: 203, duration: 0.519s, episode steps:  40, steps per second:  77, episode reward: 35.500, mean reward:  0.887 [-2.314, 31.724], mean action: 3.450 [0.000, 14.000],  loss: 0.020056, mae: 0.445519, mean_q: 0.433701, mean_eps: 0.000000
 4986/5000: episode: 204, duration: 0.244s, episode steps:  17, steps per second:  70, episode reward: -32.460, mean reward: -1.909 [-33.000,  3.002], mean action: 6.647 [0.000, 14.000],  loss: 0.021231, mae: 0.454324, mean_q: 0.432840, mean_eps: 0.000000
done, took 57.316 seconds
DQN Evaluation: 2166 victories out of 2604 episodes
Training for 5000 steps ...
   28/5000: episode: 1, duration: 0.188s, episode steps:  28, steps per second: 149, episode reward: 44.084, mean reward:  1.574 [-2.127, 32.110], mean action: 6.000 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   51/5000: episode: 2, duration: 0.150s, episode steps:  23, steps per second: 153, episode reward: 38.194, mean reward:  1.661 [-2.688, 32.090], mean action: 4.478 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   83/5000: episode: 3, duration: 0.201s, episode steps:  32, steps per second: 159, episode reward: 40.991, mean reward:  1.281 [-2.574, 32.180], mean action: 2.906 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  118/5000: episode: 4, duration: 0.220s, episode steps:  35, steps per second: 159, episode reward: 45.000, mean reward:  1.286 [-2.125, 34.830], mean action: 5.171 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  134/5000: episode: 5, duration: 0.110s, episode steps:  16, steps per second: 145, episode reward: 44.480, mean reward:  2.780 [-2.154, 32.070], mean action: 1.188 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  158/5000: episode: 6, duration: 0.156s, episode steps:  24, steps per second: 154, episode reward: 43.547, mean reward:  1.814 [-2.080, 31.642], mean action: 4.750 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  189/5000: episode: 7, duration: 0.209s, episode steps:  31, steps per second: 148, episode reward: 40.364, mean reward:  1.302 [-3.000, 32.523], mean action: 4.871 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  230/5000: episode: 8, duration: 0.258s, episode steps:  41, steps per second: 159, episode reward: 47.291, mean reward:  1.153 [-0.435, 32.090], mean action: 1.537 [0.000, 8.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  256/5000: episode: 9, duration: 0.169s, episode steps:  26, steps per second: 154, episode reward: 38.241, mean reward:  1.471 [-2.212, 32.032], mean action: 4.615 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  292/5000: episode: 10, duration: 0.224s, episode steps:  36, steps per second: 161, episode reward: 41.569, mean reward:  1.155 [-3.000, 32.060], mean action: 6.917 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  310/5000: episode: 11, duration: 0.120s, episode steps:  18, steps per second: 150, episode reward: 42.000, mean reward:  2.333 [-2.529, 32.640], mean action: 2.778 [1.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  327/5000: episode: 12, duration: 0.124s, episode steps:  17, steps per second: 137, episode reward: 42.000, mean reward:  2.471 [-2.661, 32.710], mean action: 4.059 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  368/5000: episode: 13, duration: 0.253s, episode steps:  41, steps per second: 162, episode reward: 43.325, mean reward:  1.057 [-2.172, 32.163], mean action: 3.463 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  399/5000: episode: 14, duration: 0.191s, episode steps:  31, steps per second: 162, episode reward: 33.000, mean reward:  1.065 [-2.993, 32.690], mean action: 4.968 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  454/5000: episode: 15, duration: 0.311s, episode steps:  55, steps per second: 177, episode reward: -32.420, mean reward: -0.589 [-32.002,  2.821], mean action: 3.327 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  482/5000: episode: 16, duration: 0.178s, episode steps:  28, steps per second: 157, episode reward: 44.606, mean reward:  1.593 [-2.754, 32.490], mean action: 4.357 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  504/5000: episode: 17, duration: 0.146s, episode steps:  22, steps per second: 151, episode reward: 44.166, mean reward:  2.008 [-2.194, 31.796], mean action: 4.273 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  531/5000: episode: 18, duration: 0.169s, episode steps:  27, steps per second: 160, episode reward: 38.467, mean reward:  1.425 [-2.073, 32.750], mean action: 4.889 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  561/5000: episode: 19, duration: 0.185s, episode steps:  30, steps per second: 162, episode reward: 35.951, mean reward:  1.198 [-3.000, 32.290], mean action: 5.033 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  582/5000: episode: 20, duration: 0.139s, episode steps:  21, steps per second: 151, episode reward: 47.361, mean reward:  2.255 [-0.184, 31.864], mean action: 1.381 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  606/5000: episode: 21, duration: 0.154s, episode steps:  24, steps per second: 156, episode reward: 43.950, mean reward:  1.831 [-2.126, 32.130], mean action: 3.708 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  631/5000: episode: 22, duration: 0.156s, episode steps:  25, steps per second: 161, episode reward: 34.990, mean reward:  1.400 [-3.000, 32.530], mean action: 6.640 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  654/5000: episode: 23, duration: 0.146s, episode steps:  23, steps per second: 158, episode reward: 43.903, mean reward:  1.909 [-2.488, 31.998], mean action: 2.957 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  674/5000: episode: 24, duration: 0.134s, episode steps:  20, steps per second: 150, episode reward: 41.405, mean reward:  2.070 [-2.378, 32.303], mean action: 3.600 [0.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  707/5000: episode: 25, duration: 0.200s, episode steps:  33, steps per second: 165, episode reward: 39.000, mean reward:  1.182 [-2.258, 32.070], mean action: 3.576 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  727/5000: episode: 26, duration: 0.123s, episode steps:  20, steps per second: 163, episode reward: 44.150, mean reward:  2.207 [-2.015, 32.084], mean action: 2.450 [1.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  756/5000: episode: 27, duration: 0.397s, episode steps:  29, steps per second:  73, episode reward: 40.968, mean reward:  1.413 [-2.502, 32.070], mean action: 2.621 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  776/5000: episode: 28, duration: 0.298s, episode steps:  20, steps per second:  67, episode reward: 39.000, mean reward:  1.950 [-3.000, 32.080], mean action: 3.550 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  808/5000: episode: 29, duration: 0.246s, episode steps:  32, steps per second: 130, episode reward: 38.099, mean reward:  1.191 [-3.000, 32.130], mean action: 4.219 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  828/5000: episode: 30, duration: 0.145s, episode steps:  20, steps per second: 138, episode reward: 42.000, mean reward:  2.100 [-2.216, 32.100], mean action: 4.650 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  871/5000: episode: 31, duration: 0.309s, episode steps:  43, steps per second: 139, episode reward: 38.778, mean reward:  0.902 [-2.169, 32.793], mean action: 3.023 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  888/5000: episode: 32, duration: 0.172s, episode steps:  17, steps per second:  99, episode reward: 44.167, mean reward:  2.598 [-2.096, 31.984], mean action: 2.765 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  901/5000: episode: 33, duration: 0.139s, episode steps:  13, steps per second:  93, episode reward: 47.706, mean reward:  3.670 [-0.060, 32.306], mean action: 3.231 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  920/5000: episode: 34, duration: 0.151s, episode steps:  19, steps per second: 126, episode reward: 40.919, mean reward:  2.154 [-2.288, 31.835], mean action: 4.421 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  967/5000: episode: 35, duration: 0.368s, episode steps:  47, steps per second: 128, episode reward: 35.639, mean reward:  0.758 [-2.460, 32.380], mean action: 4.106 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  986/5000: episode: 36, duration: 0.145s, episode steps:  19, steps per second: 131, episode reward: 43.605, mean reward:  2.295 [-2.241, 32.150], mean action: 3.368 [1.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1008/5000: episode: 37, duration: 0.191s, episode steps:  22, steps per second: 115, episode reward: 41.083, mean reward:  1.867 [-2.295, 32.210], mean action: 5.136 [1.000, 14.000],  loss: 0.024840, mae: 0.461885, mean_q: 0.465104, mean_eps: 0.000000
 1034/5000: episode: 38, duration: 0.320s, episode steps:  26, steps per second:  81, episode reward: 42.000, mean reward:  1.615 [-3.000, 32.090], mean action: 4.462 [0.000, 14.000],  loss: 0.020017, mae: 0.440851, mean_q: 0.461588, mean_eps: 0.000000
 1073/5000: episode: 39, duration: 0.452s, episode steps:  39, steps per second:  86, episode reward: 41.385, mean reward:  1.061 [-2.488, 32.140], mean action: 3.077 [0.000, 14.000],  loss: 0.020731, mae: 0.437665, mean_q: 0.430482, mean_eps: 0.000000
 1106/5000: episode: 40, duration: 0.435s, episode steps:  33, steps per second:  76, episode reward: 38.089, mean reward:  1.154 [-2.224, 32.072], mean action: 3.697 [0.000, 15.000],  loss: 0.018100, mae: 0.425067, mean_q: 0.466524, mean_eps: 0.000000
 1123/5000: episode: 41, duration: 0.334s, episode steps:  17, steps per second:  51, episode reward: 44.681, mean reward:  2.628 [-2.225, 32.380], mean action: 2.647 [0.000, 19.000],  loss: 0.020060, mae: 0.433684, mean_q: 0.477919, mean_eps: 0.000000
 1137/5000: episode: 42, duration: 0.176s, episode steps:  14, steps per second:  80, episode reward: 44.933, mean reward:  3.210 [-2.005, 32.773], mean action: 1.571 [0.000, 9.000],  loss: 0.016658, mae: 0.414199, mean_q: 0.462023, mean_eps: 0.000000
 1206/5000: episode: 43, duration: 0.833s, episode steps:  69, steps per second:  83, episode reward: 32.913, mean reward:  0.477 [-2.866, 32.170], mean action: 7.681 [0.000, 21.000],  loss: 0.019106, mae: 0.439781, mean_q: 0.465542, mean_eps: 0.000000
 1229/5000: episode: 44, duration: 0.272s, episode steps:  23, steps per second:  84, episode reward: 41.056, mean reward:  1.785 [-2.197, 32.142], mean action: 2.783 [0.000, 12.000],  loss: 0.020572, mae: 0.453962, mean_q: 0.410965, mean_eps: 0.000000
 1278/5000: episode: 45, duration: 0.547s, episode steps:  49, steps per second:  90, episode reward: -32.140, mean reward: -0.656 [-32.018,  2.240], mean action: 5.918 [0.000, 21.000],  loss: 0.019743, mae: 0.433884, mean_q: 0.483222, mean_eps: 0.000000
 1302/5000: episode: 46, duration: 0.541s, episode steps:  24, steps per second:  44, episode reward: 42.000, mean reward:  1.750 [-2.563, 32.200], mean action: 2.208 [0.000, 13.000],  loss: 0.019249, mae: 0.438992, mean_q: 0.459158, mean_eps: 0.000000
 1335/5000: episode: 47, duration: 0.567s, episode steps:  33, steps per second:  58, episode reward: 43.633, mean reward:  1.322 [-2.659, 32.240], mean action: 5.636 [0.000, 20.000],  loss: 0.016582, mae: 0.421277, mean_q: 0.423416, mean_eps: 0.000000
 1365/5000: episode: 48, duration: 0.501s, episode steps:  30, steps per second:  60, episode reward: 41.416, mean reward:  1.381 [-2.371, 32.280], mean action: 3.800 [1.000, 19.000],  loss: 0.021231, mae: 0.446802, mean_q: 0.416301, mean_eps: 0.000000
 1392/5000: episode: 49, duration: 0.402s, episode steps:  27, steps per second:  67, episode reward: 41.019, mean reward:  1.519 [-2.332, 32.270], mean action: 2.704 [0.000, 13.000],  loss: 0.021237, mae: 0.440468, mean_q: 0.402649, mean_eps: 0.000000
 1428/5000: episode: 50, duration: 0.422s, episode steps:  36, steps per second:  85, episode reward: 37.055, mean reward:  1.029 [-2.910, 32.715], mean action: 6.667 [0.000, 19.000],  loss: 0.019338, mae: 0.429049, mean_q: 0.446960, mean_eps: 0.000000
 1443/5000: episode: 51, duration: 0.222s, episode steps:  15, steps per second:  67, episode reward: 41.938, mean reward:  2.796 [-3.000, 32.300], mean action: 6.933 [0.000, 19.000],  loss: 0.021789, mae: 0.444352, mean_q: 0.457663, mean_eps: 0.000000
 1479/5000: episode: 52, duration: 0.473s, episode steps:  36, steps per second:  76, episode reward: 46.770, mean reward:  1.299 [-0.490, 31.964], mean action: 3.472 [0.000, 19.000],  loss: 0.020810, mae: 0.438682, mean_q: 0.467534, mean_eps: 0.000000
 1502/5000: episode: 53, duration: 0.286s, episode steps:  23, steps per second:  80, episode reward: 41.630, mean reward:  1.810 [-2.547, 32.100], mean action: 3.261 [1.000, 19.000],  loss: 0.024590, mae: 0.466256, mean_q: 0.485414, mean_eps: 0.000000
 1528/5000: episode: 54, duration: 0.385s, episode steps:  26, steps per second:  68, episode reward: 38.806, mean reward:  1.493 [-2.294, 32.160], mean action: 3.000 [0.000, 19.000],  loss: 0.018626, mae: 0.444577, mean_q: 0.466266, mean_eps: 0.000000
 1561/5000: episode: 55, duration: 0.423s, episode steps:  33, steps per second:  78, episode reward: 41.267, mean reward:  1.251 [-2.328, 31.848], mean action: 5.364 [0.000, 19.000],  loss: 0.018658, mae: 0.438667, mean_q: 0.439041, mean_eps: 0.000000
 1593/5000: episode: 56, duration: 0.394s, episode steps:  32, steps per second:  81, episode reward: 35.925, mean reward:  1.123 [-2.875, 31.975], mean action: 2.438 [0.000, 12.000],  loss: 0.019788, mae: 0.433524, mean_q: 0.442569, mean_eps: 0.000000
 1616/5000: episode: 57, duration: 0.293s, episode steps:  23, steps per second:  79, episode reward: 41.347, mean reward:  1.798 [-2.489, 32.120], mean action: 4.565 [0.000, 15.000],  loss: 0.020089, mae: 0.448306, mean_q: 0.448640, mean_eps: 0.000000
 1640/5000: episode: 58, duration: 0.311s, episode steps:  24, steps per second:  77, episode reward: 41.520, mean reward:  1.730 [-2.368, 32.140], mean action: 2.875 [0.000, 15.000],  loss: 0.019256, mae: 0.446233, mean_q: 0.492556, mean_eps: 0.000000
 1677/5000: episode: 59, duration: 0.452s, episode steps:  37, steps per second:  82, episode reward: 32.347, mean reward:  0.874 [-3.000, 31.686], mean action: 5.541 [0.000, 20.000],  loss: 0.018557, mae: 0.447304, mean_q: 0.413378, mean_eps: 0.000000
 1705/5000: episode: 60, duration: 0.398s, episode steps:  28, steps per second:  70, episode reward: 40.520, mean reward:  1.447 [-2.447, 32.067], mean action: 4.143 [0.000, 19.000],  loss: 0.018907, mae: 0.446425, mean_q: 0.406409, mean_eps: 0.000000
 1737/5000: episode: 61, duration: 0.392s, episode steps:  32, steps per second:  82, episode reward: 38.003, mean reward:  1.188 [-2.940, 32.010], mean action: 4.594 [0.000, 21.000],  loss: 0.019624, mae: 0.450350, mean_q: 0.392737, mean_eps: 0.000000
 1759/5000: episode: 62, duration: 0.303s, episode steps:  22, steps per second:  73, episode reward: 41.059, mean reward:  1.866 [-2.152, 32.043], mean action: 3.045 [0.000, 14.000],  loss: 0.018516, mae: 0.445702, mean_q: 0.425226, mean_eps: 0.000000
 1778/5000: episode: 63, duration: 0.270s, episode steps:  19, steps per second:  70, episode reward: 41.806, mean reward:  2.200 [-2.309, 32.590], mean action: 4.211 [0.000, 20.000],  loss: 0.021195, mae: 0.443709, mean_q: 0.475703, mean_eps: 0.000000
 1806/5000: episode: 64, duration: 0.336s, episode steps:  28, steps per second:  83, episode reward: 41.373, mean reward:  1.478 [-2.179, 32.144], mean action: 2.500 [0.000, 14.000],  loss: 0.017409, mae: 0.429705, mean_q: 0.458863, mean_eps: 0.000000
 1864/5000: episode: 65, duration: 0.687s, episode steps:  58, steps per second:  84, episode reward: 34.859, mean reward:  0.601 [-2.796, 32.290], mean action: 2.552 [0.000, 20.000],  loss: 0.019111, mae: 0.442309, mean_q: 0.452749, mean_eps: 0.000000
 1901/5000: episode: 66, duration: 0.491s, episode steps:  37, steps per second:  75, episode reward: 36.884, mean reward:  0.997 [-2.593, 32.380], mean action: 5.946 [0.000, 20.000],  loss: 0.020423, mae: 0.443767, mean_q: 0.490897, mean_eps: 0.000000
 1931/5000: episode: 67, duration: 0.363s, episode steps:  30, steps per second:  83, episode reward: 44.052, mean reward:  1.468 [-3.000, 31.866], mean action: 4.867 [1.000, 14.000],  loss: 0.019949, mae: 0.441772, mean_q: 0.499620, mean_eps: 0.000000
 1955/5000: episode: 68, duration: 0.287s, episode steps:  24, steps per second:  84, episode reward: 41.597, mean reward:  1.733 [-2.191, 32.051], mean action: 6.042 [0.000, 20.000],  loss: 0.017779, mae: 0.435575, mean_q: 0.432387, mean_eps: 0.000000
 1989/5000: episode: 69, duration: 0.404s, episode steps:  34, steps per second:  84, episode reward: 33.000, mean reward:  0.971 [-3.000, 32.010], mean action: 4.706 [0.000, 15.000],  loss: 0.019933, mae: 0.455872, mean_q: 0.432555, mean_eps: 0.000000
 2009/5000: episode: 70, duration: 0.297s, episode steps:  20, steps per second:  67, episode reward: 44.847, mean reward:  2.242 [-2.345, 32.560], mean action: 3.900 [0.000, 14.000],  loss: 0.017648, mae: 0.436024, mean_q: 0.473906, mean_eps: 0.000000
 2054/5000: episode: 71, duration: 0.547s, episode steps:  45, steps per second:  82, episode reward: 35.286, mean reward:  0.784 [-2.481, 32.230], mean action: 2.978 [0.000, 16.000],  loss: 0.020699, mae: 0.450147, mean_q: 0.475050, mean_eps: 0.000000
 2076/5000: episode: 72, duration: 0.275s, episode steps:  22, steps per second:  80, episode reward: 47.600, mean reward:  2.164 [-0.128, 31.610], mean action: 2.955 [1.000, 16.000],  loss: 0.026907, mae: 0.483055, mean_q: 0.437106, mean_eps: 0.000000
 2097/5000: episode: 73, duration: 0.254s, episode steps:  21, steps per second:  83, episode reward: 41.270, mean reward:  1.965 [-2.614, 32.120], mean action: 3.000 [0.000, 12.000],  loss: 0.019385, mae: 0.444331, mean_q: 0.422577, mean_eps: 0.000000
 2135/5000: episode: 74, duration: 0.438s, episode steps:  38, steps per second:  87, episode reward: 40.316, mean reward:  1.061 [-2.326, 32.220], mean action: 7.316 [0.000, 20.000],  loss: 0.019466, mae: 0.443628, mean_q: 0.424745, mean_eps: 0.000000
 2160/5000: episode: 75, duration: 0.294s, episode steps:  25, steps per second:  85, episode reward: 41.159, mean reward:  1.646 [-2.447, 31.591], mean action: 3.280 [0.000, 19.000],  loss: 0.018240, mae: 0.433011, mean_q: 0.463001, mean_eps: 0.000000
 2202/5000: episode: 76, duration: 0.493s, episode steps:  42, steps per second:  85, episode reward: 43.334, mean reward:  1.032 [-2.473, 31.878], mean action: 4.381 [0.000, 14.000],  loss: 0.019234, mae: 0.430163, mean_q: 0.506188, mean_eps: 0.000000
 2247/5000: episode: 77, duration: 1.599s, episode steps:  45, steps per second:  28, episode reward: 35.128, mean reward:  0.781 [-2.939, 31.915], mean action: 5.511 [0.000, 20.000],  loss: 0.018291, mae: 0.441780, mean_q: 0.400282, mean_eps: 0.000000
 2276/5000: episode: 78, duration: 0.466s, episode steps:  29, steps per second:  62, episode reward: 41.708, mean reward:  1.438 [-3.000, 32.050], mean action: 4.552 [0.000, 14.000],  loss: 0.018140, mae: 0.441287, mean_q: 0.482775, mean_eps: 0.000000
 2312/5000: episode: 79, duration: 0.473s, episode steps:  36, steps per second:  76, episode reward: 41.809, mean reward:  1.161 [-2.411, 32.160], mean action: 2.389 [0.000, 9.000],  loss: 0.019126, mae: 0.455751, mean_q: 0.424732, mean_eps: 0.000000
 2342/5000: episode: 80, duration: 0.378s, episode steps:  30, steps per second:  79, episode reward: 33.000, mean reward:  1.100 [-3.000, 29.050], mean action: 4.967 [1.000, 14.000],  loss: 0.022283, mae: 0.463568, mean_q: 0.435741, mean_eps: 0.000000
 2368/5000: episode: 81, duration: 0.316s, episode steps:  26, steps per second:  82, episode reward: 45.000, mean reward:  1.731 [-2.165, 32.140], mean action: 1.962 [0.000, 19.000],  loss: 0.018300, mae: 0.438858, mean_q: 0.470090, mean_eps: 0.000000
 2395/5000: episode: 82, duration: 0.359s, episode steps:  27, steps per second:  75, episode reward: 46.776, mean reward:  1.732 [-0.232, 32.270], mean action: 2.000 [0.000, 14.000],  loss: 0.018844, mae: 0.448718, mean_q: 0.463117, mean_eps: 0.000000
 2419/5000: episode: 83, duration: 0.302s, episode steps:  24, steps per second:  80, episode reward: 44.603, mean reward:  1.858 [-2.367, 32.750], mean action: 4.208 [0.000, 19.000],  loss: 0.019271, mae: 0.444544, mean_q: 0.467078, mean_eps: 0.000000
 2436/5000: episode: 84, duration: 0.214s, episode steps:  17, steps per second:  79, episode reward: 41.112, mean reward:  2.418 [-3.000, 32.090], mean action: 4.000 [0.000, 19.000],  loss: 0.022953, mae: 0.458548, mean_q: 0.477993, mean_eps: 0.000000
 2455/5000: episode: 85, duration: 0.233s, episode steps:  19, steps per second:  82, episode reward: 43.873, mean reward:  2.309 [-2.563, 32.270], mean action: 2.737 [0.000, 19.000],  loss: 0.020083, mae: 0.449153, mean_q: 0.483244, mean_eps: 0.000000
 2492/5000: episode: 86, duration: 0.471s, episode steps:  37, steps per second:  79, episode reward: 38.001, mean reward:  1.027 [-2.444, 32.300], mean action: 3.432 [0.000, 20.000],  loss: 0.020950, mae: 0.449425, mean_q: 0.498817, mean_eps: 0.000000
 2533/5000: episode: 87, duration: 0.524s, episode steps:  41, steps per second:  78, episode reward: 42.818, mean reward:  1.044 [-2.020, 33.000], mean action: 3.049 [0.000, 14.000],  loss: 0.019772, mae: 0.435883, mean_q: 0.530688, mean_eps: 0.000000
 2565/5000: episode: 88, duration: 0.388s, episode steps:  32, steps per second:  82, episode reward: 42.000, mean reward:  1.312 [-2.560, 32.150], mean action: 2.906 [1.000, 14.000],  loss: 0.017472, mae: 0.432270, mean_q: 0.460203, mean_eps: 0.000000
 2587/5000: episode: 89, duration: 0.287s, episode steps:  22, steps per second:  77, episode reward: 41.638, mean reward:  1.893 [-2.349, 32.168], mean action: 1.455 [0.000, 11.000],  loss: 0.020870, mae: 0.442904, mean_q: 0.451495, mean_eps: 0.000000
 2608/5000: episode: 90, duration: 0.256s, episode steps:  21, steps per second:  82, episode reward: 38.414, mean reward:  1.829 [-2.904, 32.445], mean action: 4.286 [0.000, 14.000],  loss: 0.021327, mae: 0.447135, mean_q: 0.519338, mean_eps: 0.000000
 2647/5000: episode: 91, duration: 0.460s, episode steps:  39, steps per second:  85, episode reward: 32.811, mean reward:  0.841 [-3.000, 31.952], mean action: 4.205 [0.000, 14.000],  loss: 0.023102, mae: 0.450438, mean_q: 0.485866, mean_eps: 0.000000
 2677/5000: episode: 92, duration: 0.355s, episode steps:  30, steps per second:  84, episode reward: 36.000, mean reward:  1.200 [-2.676, 32.690], mean action: 3.667 [0.000, 14.000],  loss: 0.020234, mae: 0.441061, mean_q: 0.460247, mean_eps: 0.000000
 2697/5000: episode: 93, duration: 0.251s, episode steps:  20, steps per second:  80, episode reward: 40.938, mean reward:  2.047 [-3.000, 31.859], mean action: 2.950 [0.000, 15.000],  loss: 0.015950, mae: 0.424576, mean_q: 0.459498, mean_eps: 0.000000
 2725/5000: episode: 94, duration: 0.346s, episode steps:  28, steps per second:  81, episode reward: 40.690, mean reward:  1.453 [-3.000, 32.434], mean action: 5.321 [0.000, 20.000],  loss: 0.021421, mae: 0.458022, mean_q: 0.446409, mean_eps: 0.000000
 2745/5000: episode: 95, duration: 0.250s, episode steps:  20, steps per second:  80, episode reward: 44.561, mean reward:  2.228 [-2.143, 32.904], mean action: 3.600 [0.000, 15.000],  loss: 0.016759, mae: 0.436048, mean_q: 0.399210, mean_eps: 0.000000
 2762/5000: episode: 96, duration: 0.247s, episode steps:  17, steps per second:  69, episode reward: 40.797, mean reward:  2.400 [-2.459, 32.362], mean action: 4.647 [0.000, 12.000],  loss: 0.016709, mae: 0.440144, mean_q: 0.369082, mean_eps: 0.000000
 2794/5000: episode: 97, duration: 0.426s, episode steps:  32, steps per second:  75, episode reward: 35.554, mean reward:  1.111 [-2.854, 31.774], mean action: 4.469 [0.000, 14.000],  loss: 0.021255, mae: 0.463348, mean_q: 0.375167, mean_eps: 0.000000
 2869/5000: episode: 98, duration: 1.025s, episode steps:  75, steps per second:  73, episode reward: -32.760, mean reward: -0.437 [-32.329,  2.443], mean action: 10.613 [0.000, 21.000],  loss: 0.021303, mae: 0.454813, mean_q: 0.492889, mean_eps: 0.000000
 2885/5000: episode: 99, duration: 0.239s, episode steps:  16, steps per second:  67, episode reward: 44.474, mean reward:  2.780 [-2.675, 32.230], mean action: 2.188 [0.000, 19.000],  loss: 0.022976, mae: 0.458924, mean_q: 0.470440, mean_eps: 0.000000
 2900/5000: episode: 100, duration: 0.193s, episode steps:  15, steps per second:  78, episode reward: 40.936, mean reward:  2.729 [-2.508, 32.229], mean action: 3.200 [0.000, 14.000],  loss: 0.017622, mae: 0.441203, mean_q: 0.415759, mean_eps: 0.000000
 2929/5000: episode: 101, duration: 0.366s, episode steps:  29, steps per second:  79, episode reward: 42.985, mean reward:  1.482 [-2.033, 32.160], mean action: 2.897 [0.000, 14.000],  loss: 0.021721, mae: 0.460660, mean_q: 0.422338, mean_eps: 0.000000
 2950/5000: episode: 102, duration: 0.269s, episode steps:  21, steps per second:  78, episode reward: 45.000, mean reward:  2.143 [-2.060, 32.340], mean action: 2.857 [1.000, 19.000],  loss: 0.022743, mae: 0.459906, mean_q: 0.438534, mean_eps: 0.000000
 2976/5000: episode: 103, duration: 0.351s, episode steps:  26, steps per second:  74, episode reward: 36.000, mean reward:  1.385 [-2.257, 32.130], mean action: 2.808 [0.000, 12.000],  loss: 0.023054, mae: 0.459292, mean_q: 0.479889, mean_eps: 0.000000
 3012/5000: episode: 104, duration: 0.441s, episode steps:  36, steps per second:  82, episode reward: 38.809, mean reward:  1.078 [-2.659, 32.060], mean action: 2.333 [0.000, 14.000],  loss: 0.021901, mae: 0.437604, mean_q: 0.561609, mean_eps: 0.000000
 3030/5000: episode: 105, duration: 0.228s, episode steps:  18, steps per second:  79, episode reward: 47.019, mean reward:  2.612 [-0.348, 31.888], mean action: 1.722 [0.000, 14.000],  loss: 0.019616, mae: 0.429634, mean_q: 0.564465, mean_eps: 0.000000
 3056/5000: episode: 106, duration: 0.332s, episode steps:  26, steps per second:  78, episode reward: 41.498, mean reward:  1.596 [-3.000, 32.250], mean action: 6.462 [0.000, 20.000],  loss: 0.019243, mae: 0.439075, mean_q: 0.471132, mean_eps: 0.000000
 3086/5000: episode: 107, duration: 0.369s, episode steps:  30, steps per second:  81, episode reward: 43.713, mean reward:  1.457 [-2.469, 32.319], mean action: 2.433 [0.000, 13.000],  loss: 0.017423, mae: 0.434109, mean_q: 0.482265, mean_eps: 0.000000
 3122/5000: episode: 108, duration: 0.466s, episode steps:  36, steps per second:  77, episode reward: 40.850, mean reward:  1.135 [-2.970, 32.160], mean action: 4.833 [0.000, 14.000],  loss: 0.020286, mae: 0.435617, mean_q: 0.509946, mean_eps: 0.000000
 3151/5000: episode: 109, duration: 0.349s, episode steps:  29, steps per second:  83, episode reward: 41.264, mean reward:  1.423 [-2.592, 31.811], mean action: 6.690 [0.000, 14.000],  loss: 0.018287, mae: 0.424830, mean_q: 0.475125, mean_eps: 0.000000
 3170/5000: episode: 110, duration: 0.240s, episode steps:  19, steps per second:  79, episode reward: 41.129, mean reward:  2.165 [-2.234, 32.230], mean action: 4.579 [0.000, 14.000],  loss: 0.017394, mae: 0.427224, mean_q: 0.485802, mean_eps: 0.000000
 3206/5000: episode: 111, duration: 0.452s, episode steps:  36, steps per second:  80, episode reward: 41.628, mean reward:  1.156 [-2.481, 32.080], mean action: 4.028 [0.000, 14.000],  loss: 0.019249, mae: 0.429905, mean_q: 0.507532, mean_eps: 0.000000
 3229/5000: episode: 112, duration: 0.385s, episode steps:  23, steps per second:  60, episode reward: 41.414, mean reward:  1.801 [-2.975, 32.170], mean action: 4.913 [0.000, 14.000],  loss: 0.017546, mae: 0.429499, mean_q: 0.463710, mean_eps: 0.000000
 3246/5000: episode: 113, duration: 0.224s, episode steps:  17, steps per second:  76, episode reward: 47.853, mean reward:  2.815 [-0.658, 32.070], mean action: 1.824 [1.000, 3.000],  loss: 0.021878, mae: 0.453009, mean_q: 0.414496, mean_eps: 0.000000
 3276/5000: episode: 114, duration: 0.364s, episode steps:  30, steps per second:  82, episode reward: 38.015, mean reward:  1.267 [-2.442, 32.061], mean action: 5.800 [0.000, 18.000],  loss: 0.018244, mae: 0.440814, mean_q: 0.442543, mean_eps: 0.000000
 3305/5000: episode: 115, duration: 0.374s, episode steps:  29, steps per second:  77, episode reward: 36.000, mean reward:  1.241 [-2.370, 32.420], mean action: 3.241 [0.000, 14.000],  loss: 0.021547, mae: 0.456137, mean_q: 0.394307, mean_eps: 0.000000
 3336/5000: episode: 116, duration: 0.499s, episode steps:  31, steps per second:  62, episode reward: 35.682, mean reward:  1.151 [-2.541, 32.030], mean action: 4.839 [0.000, 14.000],  loss: 0.024799, mae: 0.461531, mean_q: 0.464428, mean_eps: 0.000000
 3365/5000: episode: 117, duration: 0.360s, episode steps:  29, steps per second:  80, episode reward: 35.464, mean reward:  1.223 [-3.000, 31.801], mean action: 4.310 [0.000, 19.000],  loss: 0.018973, mae: 0.436363, mean_q: 0.507518, mean_eps: 0.000000
 3396/5000: episode: 118, duration: 0.380s, episode steps:  31, steps per second:  82, episode reward: 43.497, mean reward:  1.403 [-2.706, 32.311], mean action: 2.677 [0.000, 15.000],  loss: 0.021935, mae: 0.453154, mean_q: 0.560733, mean_eps: 0.000000
 3429/5000: episode: 119, duration: 0.428s, episode steps:  33, steps per second:  77, episode reward: 35.323, mean reward:  1.070 [-2.330, 32.500], mean action: 4.758 [0.000, 15.000],  loss: 0.019382, mae: 0.443226, mean_q: 0.566963, mean_eps: 0.000000
 3460/5000: episode: 120, duration: 0.370s, episode steps:  31, steps per second:  84, episode reward: 38.709, mean reward:  1.249 [-3.000, 32.193], mean action: 2.968 [0.000, 15.000],  loss: 0.018990, mae: 0.450358, mean_q: 0.492083, mean_eps: 0.000000
 3488/5000: episode: 121, duration: 0.337s, episode steps:  28, steps per second:  83, episode reward: 43.433, mean reward:  1.551 [-2.112, 32.170], mean action: 4.286 [1.000, 20.000],  loss: 0.021928, mae: 0.464887, mean_q: 0.459973, mean_eps: 0.000000
 3538/5000: episode: 122, duration: 0.575s, episode steps:  50, steps per second:  87, episode reward: 47.784, mean reward:  0.956 [-0.500, 32.020], mean action: 2.960 [1.000, 15.000],  loss: 0.020637, mae: 0.462226, mean_q: 0.457658, mean_eps: 0.000000
 3567/5000: episode: 123, duration: 0.346s, episode steps:  29, steps per second:  84, episode reward: 42.000, mean reward:  1.448 [-2.325, 32.220], mean action: 3.103 [0.000, 19.000],  loss: 0.021663, mae: 0.461081, mean_q: 0.478861, mean_eps: 0.000000
 3587/5000: episode: 124, duration: 0.259s, episode steps:  20, steps per second:  77, episode reward: 44.267, mean reward:  2.213 [-2.415, 32.290], mean action: 2.700 [0.000, 11.000],  loss: 0.021142, mae: 0.458600, mean_q: 0.526222, mean_eps: 0.000000
 3616/5000: episode: 125, duration: 0.352s, episode steps:  29, steps per second:  82, episode reward: 35.611, mean reward:  1.228 [-2.953, 32.320], mean action: 6.207 [0.000, 21.000],  loss: 0.019550, mae: 0.446464, mean_q: 0.483508, mean_eps: 0.000000
 3646/5000: episode: 126, duration: 0.365s, episode steps:  30, steps per second:  82, episode reward: 38.640, mean reward:  1.288 [-3.000, 32.130], mean action: 6.033 [0.000, 21.000],  loss: 0.019520, mae: 0.446957, mean_q: 0.426483, mean_eps: 0.000000
 3678/5000: episode: 127, duration: 0.430s, episode steps:  32, steps per second:  75, episode reward: 43.396, mean reward:  1.356 [-2.218, 31.714], mean action: 3.031 [0.000, 14.000],  loss: 0.019619, mae: 0.441780, mean_q: 0.452732, mean_eps: 0.000000
 3704/5000: episode: 128, duration: 0.323s, episode steps:  26, steps per second:  81, episode reward: 44.725, mean reward:  1.720 [-2.463, 32.233], mean action: 2.923 [1.000, 9.000],  loss: 0.021208, mae: 0.461237, mean_q: 0.448420, mean_eps: 0.000000
 3730/5000: episode: 129, duration: 0.336s, episode steps:  26, steps per second:  77, episode reward: 44.464, mean reward:  1.710 [-2.118, 31.725], mean action: 3.692 [0.000, 14.000],  loss: 0.020431, mae: 0.442787, mean_q: 0.503547, mean_eps: 0.000000
 3750/5000: episode: 130, duration: 0.240s, episode steps:  20, steps per second:  83, episode reward: 41.447, mean reward:  2.072 [-3.000, 32.840], mean action: 2.800 [0.000, 20.000],  loss: 0.021163, mae: 0.440084, mean_q: 0.543807, mean_eps: 0.000000
 3783/5000: episode: 131, duration: 0.398s, episode steps:  33, steps per second:  83, episode reward: 38.054, mean reward:  1.153 [-2.998, 32.190], mean action: 4.697 [1.000, 14.000],  loss: 0.018832, mae: 0.436786, mean_q: 0.509476, mean_eps: 0.000000
 3810/5000: episode: 132, duration: 0.351s, episode steps:  27, steps per second:  77, episode reward: 44.557, mean reward:  1.650 [-3.000, 32.260], mean action: 3.185 [0.000, 15.000],  loss: 0.019567, mae: 0.439604, mean_q: 0.476118, mean_eps: 0.000000
 3819/5000: episode: 133, duration: 0.143s, episode steps:   9, steps per second:  63, episode reward: 47.046, mean reward:  5.227 [ 0.279, 32.338], mean action: 4.111 [1.000, 14.000],  loss: 0.019765, mae: 0.448697, mean_q: 0.468588, mean_eps: 0.000000
 3844/5000: episode: 134, duration: 0.322s, episode steps:  25, steps per second:  78, episode reward: 41.612, mean reward:  1.664 [-2.394, 32.053], mean action: 2.440 [0.000, 12.000],  loss: 0.017973, mae: 0.440328, mean_q: 0.474526, mean_eps: 0.000000
 3877/5000: episode: 135, duration: 0.416s, episode steps:  33, steps per second:  79, episode reward: 42.518, mean reward:  1.288 [-2.000, 31.929], mean action: 4.818 [0.000, 16.000],  loss: 0.022034, mae: 0.461578, mean_q: 0.441567, mean_eps: 0.000000
 3921/5000: episode: 136, duration: 0.556s, episode steps:  44, steps per second:  79, episode reward: 44.809, mean reward:  1.018 [-2.765, 32.200], mean action: 1.227 [0.000, 12.000],  loss: 0.018113, mae: 0.425424, mean_q: 0.480788, mean_eps: 0.000000
 3969/5000: episode: 137, duration: 0.640s, episode steps:  48, steps per second:  75, episode reward: 40.271, mean reward:  0.839 [-2.488, 32.130], mean action: 6.458 [0.000, 20.000],  loss: 0.016864, mae: 0.423862, mean_q: 0.449723, mean_eps: 0.000000
 3994/5000: episode: 138, duration: 0.305s, episode steps:  25, steps per second:  82, episode reward: 38.594, mean reward:  1.544 [-2.562, 31.644], mean action: 4.080 [0.000, 20.000],  loss: 0.023915, mae: 0.458170, mean_q: 0.466153, mean_eps: 0.000000
 4026/5000: episode: 139, duration: 0.392s, episode steps:  32, steps per second:  82, episode reward: 44.500, mean reward:  1.391 [-2.112, 32.020], mean action: 2.719 [0.000, 9.000],  loss: 0.016679, mae: 0.444417, mean_q: 0.410088, mean_eps: 0.000000
 4056/5000: episode: 140, duration: 0.389s, episode steps:  30, steps per second:  77, episode reward: -32.310, mean reward: -1.077 [-32.103,  2.641], mean action: 3.167 [0.000, 19.000],  loss: 0.020247, mae: 0.465809, mean_q: 0.420336, mean_eps: 0.000000
 4102/5000: episode: 141, duration: 0.598s, episode steps:  46, steps per second:  77, episode reward: 41.450, mean reward:  0.901 [-2.188, 32.550], mean action: 2.217 [0.000, 9.000],  loss: 0.020064, mae: 0.445328, mean_q: 0.478054, mean_eps: 0.000000
 4126/5000: episode: 142, duration: 0.314s, episode steps:  24, steps per second:  76, episode reward: 42.000, mean reward:  1.750 [-2.245, 32.460], mean action: 3.333 [0.000, 11.000],  loss: 0.023459, mae: 0.463197, mean_q: 0.460097, mean_eps: 0.000000
 4150/5000: episode: 143, duration: 0.328s, episode steps:  24, steps per second:  73, episode reward: 41.408, mean reward:  1.725 [-3.000, 32.276], mean action: 2.000 [0.000, 11.000],  loss: 0.021298, mae: 0.464776, mean_q: 0.476412, mean_eps: 0.000000
 4188/5000: episode: 144, duration: 0.465s, episode steps:  38, steps per second:  82, episode reward: 38.961, mean reward:  1.025 [-3.000, 32.381], mean action: 4.684 [1.000, 12.000],  loss: 0.020723, mae: 0.455150, mean_q: 0.480809, mean_eps: 0.000000
 4213/5000: episode: 145, duration: 0.319s, episode steps:  25, steps per second:  78, episode reward: 43.380, mean reward:  1.735 [-2.558, 32.285], mean action: 3.720 [1.000, 15.000],  loss: 0.020053, mae: 0.456289, mean_q: 0.429151, mean_eps: 0.000000
 4244/5000: episode: 146, duration: 0.381s, episode steps:  31, steps per second:  81, episode reward: 41.421, mean reward:  1.336 [-2.369, 32.063], mean action: 4.032 [1.000, 15.000],  loss: 0.018548, mae: 0.450133, mean_q: 0.396751, mean_eps: 0.000000
 4311/5000: episode: 147, duration: 0.812s, episode steps:  67, steps per second:  82, episode reward: 46.355, mean reward:  0.692 [-0.448, 32.300], mean action: 2.463 [0.000, 20.000],  loss: 0.019934, mae: 0.448420, mean_q: 0.483760, mean_eps: 0.000000
 4328/5000: episode: 148, duration: 0.221s, episode steps:  17, steps per second:  77, episode reward: 42.000, mean reward:  2.471 [-2.151, 33.000], mean action: 2.824 [0.000, 12.000],  loss: 0.018816, mae: 0.447054, mean_q: 0.500951, mean_eps: 0.000000
 4347/5000: episode: 149, duration: 0.245s, episode steps:  19, steps per second:  78, episode reward: 44.216, mean reward:  2.327 [-2.540, 32.360], mean action: 2.684 [0.000, 12.000],  loss: 0.019469, mae: 0.445646, mean_q: 0.518720, mean_eps: 0.000000
 4363/5000: episode: 150, duration: 0.201s, episode steps:  16, steps per second:  80, episode reward: 41.188, mean reward:  2.574 [-3.000, 32.930], mean action: 3.688 [0.000, 19.000],  loss: 0.020130, mae: 0.456455, mean_q: 0.518552, mean_eps: 0.000000
 4379/5000: episode: 151, duration: 0.207s, episode steps:  16, steps per second:  77, episode reward: 44.712, mean reward:  2.795 [-2.203, 31.966], mean action: 2.688 [1.000, 19.000],  loss: 0.023117, mae: 0.471692, mean_q: 0.428803, mean_eps: 0.000000
 4398/5000: episode: 152, duration: 0.236s, episode steps:  19, steps per second:  80, episode reward: 41.355, mean reward:  2.177 [-2.903, 31.585], mean action: 4.632 [1.000, 15.000],  loss: 0.020420, mae: 0.456948, mean_q: 0.441752, mean_eps: 0.000000
 4419/5000: episode: 153, duration: 0.271s, episode steps:  21, steps per second:  77, episode reward: 44.085, mean reward:  2.099 [-2.495, 32.599], mean action: 2.762 [0.000, 14.000],  loss: 0.019587, mae: 0.438785, mean_q: 0.558508, mean_eps: 0.000000
 4445/5000: episode: 154, duration: 0.314s, episode steps:  26, steps per second:  83, episode reward: 35.637, mean reward:  1.371 [-3.000, 32.207], mean action: 4.923 [0.000, 18.000],  loss: 0.018636, mae: 0.444144, mean_q: 0.511117, mean_eps: 0.000000
 4485/5000: episode: 155, duration: 0.470s, episode steps:  40, steps per second:  85, episode reward: 36.000, mean reward:  0.900 [-2.444, 32.290], mean action: 5.975 [0.000, 20.000],  loss: 0.019790, mae: 0.447308, mean_q: 0.523567, mean_eps: 0.000000
 4500/5000: episode: 156, duration: 0.388s, episode steps:  15, steps per second:  39, episode reward: 44.558, mean reward:  2.971 [-2.104, 31.969], mean action: 3.267 [0.000, 14.000],  loss: 0.024895, mae: 0.459244, mean_q: 0.539904, mean_eps: 0.000000
 4530/5000: episode: 157, duration: 0.379s, episode steps:  30, steps per second:  79, episode reward: -34.930, mean reward: -1.164 [-32.219,  2.759], mean action: 7.633 [0.000, 21.000],  loss: 0.019001, mae: 0.433655, mean_q: 0.521723, mean_eps: 0.000000
 4561/5000: episode: 158, duration: 0.372s, episode steps:  31, steps per second:  83, episode reward: 38.646, mean reward:  1.247 [-2.920, 32.520], mean action: 5.871 [0.000, 20.000],  loss: 0.019084, mae: 0.443580, mean_q: 0.477903, mean_eps: 0.000000
 4580/5000: episode: 159, duration: 0.241s, episode steps:  19, steps per second:  79, episode reward: 40.191, mean reward:  2.115 [-3.000, 32.200], mean action: 5.158 [0.000, 14.000],  loss: 0.017916, mae: 0.432163, mean_q: 0.484301, mean_eps: 0.000000
 4605/5000: episode: 160, duration: 0.318s, episode steps:  25, steps per second:  79, episode reward: 39.000, mean reward:  1.560 [-2.646, 32.020], mean action: 4.600 [0.000, 19.000],  loss: 0.021439, mae: 0.440865, mean_q: 0.497136, mean_eps: 0.000000
 4645/5000: episode: 161, duration: 0.509s, episode steps:  40, steps per second:  79, episode reward: 40.675, mean reward:  1.017 [-3.000, 31.953], mean action: 5.400 [0.000, 21.000],  loss: 0.017659, mae: 0.431690, mean_q: 0.455289, mean_eps: 0.000000
 4672/5000: episode: 162, duration: 0.323s, episode steps:  27, steps per second:  84, episode reward: 38.608, mean reward:  1.430 [-3.000, 31.895], mean action: 3.889 [0.000, 19.000],  loss: 0.017969, mae: 0.423373, mean_q: 0.483869, mean_eps: 0.000000
 4707/5000: episode: 163, duration: 0.408s, episode steps:  35, steps per second:  86, episode reward: 39.000, mean reward:  1.114 [-2.406, 32.110], mean action: 3.543 [0.000, 19.000],  loss: 0.018535, mae: 0.432037, mean_q: 0.504160, mean_eps: 0.000000
 4724/5000: episode: 164, duration: 0.212s, episode steps:  17, steps per second:  80, episode reward: 45.000, mean reward:  2.647 [-2.098, 32.460], mean action: 2.529 [0.000, 19.000],  loss: 0.021570, mae: 0.452411, mean_q: 0.430627, mean_eps: 0.000000
 4748/5000: episode: 165, duration: 0.292s, episode steps:  24, steps per second:  82, episode reward: 38.904, mean reward:  1.621 [-2.909, 32.134], mean action: 6.667 [0.000, 20.000],  loss: 0.019201, mae: 0.440585, mean_q: 0.474378, mean_eps: 0.000000
 4792/5000: episode: 166, duration: 0.505s, episode steps:  44, steps per second:  87, episode reward: 36.000, mean reward:  0.818 [-2.794, 32.150], mean action: 4.955 [1.000, 19.000],  loss: 0.018650, mae: 0.434314, mean_q: 0.467690, mean_eps: 0.000000
 4819/5000: episode: 167, duration: 0.332s, episode steps:  27, steps per second:  81, episode reward: 41.321, mean reward:  1.530 [-2.369, 32.040], mean action: 3.741 [0.000, 20.000],  loss: 0.017377, mae: 0.428693, mean_q: 0.447692, mean_eps: 0.000000
 4839/5000: episode: 168, duration: 0.252s, episode steps:  20, steps per second:  80, episode reward: 44.393, mean reward:  2.220 [-2.518, 32.910], mean action: 2.550 [0.000, 12.000],  loss: 0.019504, mae: 0.443594, mean_q: 0.452653, mean_eps: 0.000000
 4867/5000: episode: 169, duration: 0.354s, episode steps:  28, steps per second:  79, episode reward: 38.752, mean reward:  1.384 [-2.474, 32.190], mean action: 3.143 [0.000, 19.000],  loss: 0.020827, mae: 0.443638, mean_q: 0.468194, mean_eps: 0.000000
 4893/5000: episode: 170, duration: 0.323s, episode steps:  26, steps per second:  81, episode reward: -32.830, mean reward: -1.263 [-32.126,  3.000], mean action: 10.500 [0.000, 19.000],  loss: 0.022707, mae: 0.448681, mean_q: 0.498149, mean_eps: 0.000000
 4904/5000: episode: 171, duration: 0.156s, episode steps:  11, steps per second:  70, episode reward: 46.537, mean reward:  4.231 [-0.132, 33.000], mean action: 1.455 [0.000, 12.000],  loss: 0.026272, mae: 0.471740, mean_q: 0.499538, mean_eps: 0.000000
 4939/5000: episode: 172, duration: 0.422s, episode steps:  35, steps per second:  83, episode reward: 42.000, mean reward:  1.200 [-2.064, 32.120], mean action: 4.286 [1.000, 15.000],  loss: 0.016978, mae: 0.425597, mean_q: 0.426208, mean_eps: 0.000000
 4978/5000: episode: 173, duration: 0.448s, episode steps:  39, steps per second:  87, episode reward: 32.610, mean reward:  0.836 [-3.000, 32.280], mean action: 4.821 [0.000, 14.000],  loss: 0.020044, mae: 0.436925, mean_q: 0.454358, mean_eps: 0.000000
done, took 59.486 seconds
DQN Evaluation: 2333 victories out of 2778 episodes
Training for 5000 steps ...
   26/5000: episode: 1, duration: 0.207s, episode steps:  26, steps per second: 126, episode reward: 40.664, mean reward:  1.564 [-2.438, 31.945], mean action: 3.962 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   57/5000: episode: 2, duration: 0.231s, episode steps:  31, steps per second: 134, episode reward: -32.660, mean reward: -1.054 [-31.748,  2.220], mean action: 4.000 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   79/5000: episode: 3, duration: 0.166s, episode steps:  22, steps per second: 132, episode reward: -38.690, mean reward: -1.759 [-33.000,  2.350], mean action: 4.045 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  104/5000: episode: 4, duration: 0.186s, episode steps:  25, steps per second: 134, episode reward: 36.000, mean reward:  1.440 [-2.258, 33.000], mean action: 3.680 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  130/5000: episode: 5, duration: 0.174s, episode steps:  26, steps per second: 149, episode reward: -35.050, mean reward: -1.348 [-31.592,  2.421], mean action: 7.308 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  148/5000: episode: 6, duration: 0.128s, episode steps:  18, steps per second: 141, episode reward: -33.000, mean reward: -1.833 [-30.251,  2.580], mean action: 4.667 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  168/5000: episode: 7, duration: 0.138s, episode steps:  20, steps per second: 145, episode reward: 38.704, mean reward:  1.935 [-2.160, 32.280], mean action: 5.950 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  187/5000: episode: 8, duration: 0.130s, episode steps:  19, steps per second: 146, episode reward: 38.065, mean reward:  2.003 [-3.000, 32.389], mean action: 4.789 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  241/5000: episode: 9, duration: 0.333s, episode steps:  54, steps per second: 162, episode reward: -35.050, mean reward: -0.649 [-31.843,  2.643], mean action: 9.889 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  259/5000: episode: 10, duration: 0.130s, episode steps:  18, steps per second: 138, episode reward: 35.724, mean reward:  1.985 [-3.000, 32.724], mean action: 3.889 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  281/5000: episode: 11, duration: 0.153s, episode steps:  22, steps per second: 144, episode reward: -38.510, mean reward: -1.750 [-32.385,  2.490], mean action: 7.545 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  302/5000: episode: 12, duration: 0.150s, episode steps:  21, steps per second: 140, episode reward: 35.396, mean reward:  1.686 [-2.694, 32.746], mean action: 3.381 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  331/5000: episode: 13, duration: 0.190s, episode steps:  29, steps per second: 153, episode reward: -38.410, mean reward: -1.324 [-32.860,  2.391], mean action: 11.655 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  355/5000: episode: 14, duration: 0.167s, episode steps:  24, steps per second: 144, episode reward: 31.000, mean reward:  1.292 [-3.000, 32.130], mean action: 8.167 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  377/5000: episode: 15, duration: 0.170s, episode steps:  22, steps per second: 130, episode reward: 38.031, mean reward:  1.729 [-3.000, 32.220], mean action: 8.091 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  402/5000: episode: 16, duration: 0.172s, episode steps:  25, steps per second: 145, episode reward: 41.109, mean reward:  1.644 [-3.000, 33.000], mean action: 3.880 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  431/5000: episode: 17, duration: 0.192s, episode steps:  29, steps per second: 151, episode reward: 32.095, mean reward:  1.107 [-2.675, 31.603], mean action: 6.276 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  447/5000: episode: 18, duration: 0.127s, episode steps:  16, steps per second: 126, episode reward: 41.262, mean reward:  2.579 [-2.525, 33.000], mean action: 4.625 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  471/5000: episode: 19, duration: 0.167s, episode steps:  24, steps per second: 143, episode reward: 34.370, mean reward:  1.432 [-3.000, 32.245], mean action: 7.625 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  493/5000: episode: 20, duration: 0.149s, episode steps:  22, steps per second: 147, episode reward: 30.000, mean reward:  1.364 [-3.000, 30.943], mean action: 5.000 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  521/5000: episode: 21, duration: 0.179s, episode steps:  28, steps per second: 156, episode reward: 44.627, mean reward:  1.594 [-2.521, 32.176], mean action: 4.750 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  560/5000: episode: 22, duration: 0.268s, episode steps:  39, steps per second: 146, episode reward: -32.100, mean reward: -0.823 [-32.535,  2.815], mean action: 5.513 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  581/5000: episode: 23, duration: 0.145s, episode steps:  21, steps per second: 145, episode reward: 42.000, mean reward:  2.000 [-2.386, 30.341], mean action: 2.333 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  602/5000: episode: 24, duration: 0.144s, episode steps:  21, steps per second: 146, episode reward: 38.068, mean reward:  1.813 [-3.000, 32.220], mean action: 4.286 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  629/5000: episode: 25, duration: 0.178s, episode steps:  27, steps per second: 151, episode reward: 33.000, mean reward:  1.222 [-2.434, 32.800], mean action: 6.963 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  643/5000: episode: 26, duration: 0.102s, episode steps:  14, steps per second: 137, episode reward: 41.622, mean reward:  2.973 [-2.076, 32.220], mean action: 2.429 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  662/5000: episode: 27, duration: 0.133s, episode steps:  19, steps per second: 143, episode reward: 41.738, mean reward:  2.197 [-2.445, 32.458], mean action: 6.316 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  680/5000: episode: 28, duration: 0.136s, episode steps:  18, steps per second: 132, episode reward: 38.177, mean reward:  2.121 [-2.114, 32.260], mean action: 3.722 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  713/5000: episode: 29, duration: 0.217s, episode steps:  33, steps per second: 152, episode reward: 36.000, mean reward:  1.091 [-2.472, 32.060], mean action: 7.182 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  725/5000: episode: 30, duration: 0.097s, episode steps:  12, steps per second: 123, episode reward: 44.079, mean reward:  3.673 [-2.467, 32.569], mean action: 2.500 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  743/5000: episode: 31, duration: 0.123s, episode steps:  18, steps per second: 146, episode reward: 38.366, mean reward:  2.131 [-2.571, 32.080], mean action: 6.944 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  753/5000: episode: 32, duration: 0.090s, episode steps:  10, steps per second: 112, episode reward: 44.685, mean reward:  4.468 [-2.423, 33.000], mean action: 5.100 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  804/5000: episode: 33, duration: 0.316s, episode steps:  51, steps per second: 161, episode reward: -33.000, mean reward: -0.647 [-30.453,  2.162], mean action: 7.824 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  831/5000: episode: 34, duration: 0.195s, episode steps:  27, steps per second: 138, episode reward: 33.000, mean reward:  1.222 [-2.610, 32.650], mean action: 6.074 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  853/5000: episode: 35, duration: 0.142s, episode steps:  22, steps per second: 155, episode reward: 33.000, mean reward:  1.500 [-2.279, 30.670], mean action: 7.045 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  884/5000: episode: 36, duration: 0.214s, episode steps:  31, steps per second: 145, episode reward: 35.098, mean reward:  1.132 [-2.273, 31.449], mean action: 4.484 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  907/5000: episode: 37, duration: 0.162s, episode steps:  23, steps per second: 142, episode reward: 40.997, mean reward:  1.782 [-2.261, 32.242], mean action: 5.652 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  949/5000: episode: 38, duration: 0.304s, episode steps:  42, steps per second: 138, episode reward: -32.790, mean reward: -0.781 [-33.000,  2.650], mean action: 3.595 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  986/5000: episode: 39, duration: 0.234s, episode steps:  37, steps per second: 158, episode reward: 37.219, mean reward:  1.006 [-2.215, 32.120], mean action: 2.459 [0.000, 10.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1015/5000: episode: 40, duration: 0.279s, episode steps:  29, steps per second: 104, episode reward: -32.530, mean reward: -1.122 [-31.887,  2.835], mean action: 3.172 [0.000, 19.000],  loss: 0.021910, mae: 0.436875, mean_q: 0.511256, mean_eps: 0.000000
 1035/5000: episode: 41, duration: 0.266s, episode steps:  20, steps per second:  75, episode reward: 38.105, mean reward:  1.905 [-2.197, 31.445], mean action: 2.850 [0.000, 19.000],  loss: 0.016579, mae: 0.423924, mean_q: 0.431739, mean_eps: 0.000000
 1109/5000: episode: 42, duration: 0.912s, episode steps:  74, steps per second:  81, episode reward: 32.319, mean reward:  0.437 [-2.335, 32.127], mean action: 4.716 [0.000, 21.000],  loss: 0.021814, mae: 0.444595, mean_q: 0.458219, mean_eps: 0.000000
 1134/5000: episode: 43, duration: 0.316s, episode steps:  25, steps per second:  79, episode reward: 38.899, mean reward:  1.556 [-2.294, 32.490], mean action: 9.000 [0.000, 21.000],  loss: 0.017344, mae: 0.420978, mean_q: 0.526335, mean_eps: 0.000000
 1158/5000: episode: 44, duration: 0.309s, episode steps:  24, steps per second:  78, episode reward: 32.619, mean reward:  1.359 [-2.487, 32.716], mean action: 4.208 [0.000, 19.000],  loss: 0.021291, mae: 0.439984, mean_q: 0.516153, mean_eps: 0.000000
 1182/5000: episode: 45, duration: 0.291s, episode steps:  24, steps per second:  82, episode reward: 35.339, mean reward:  1.472 [-2.373, 32.420], mean action: 9.167 [0.000, 21.000],  loss: 0.019141, mae: 0.434320, mean_q: 0.530902, mean_eps: 0.000000
 1201/5000: episode: 46, duration: 0.237s, episode steps:  19, steps per second:  80, episode reward: 35.024, mean reward:  1.843 [-2.644, 32.110], mean action: 4.737 [0.000, 15.000],  loss: 0.015503, mae: 0.418548, mean_q: 0.467725, mean_eps: 0.000000
 1214/5000: episode: 47, duration: 0.166s, episode steps:  13, steps per second:  78, episode reward: 41.904, mean reward:  3.223 [-2.232, 32.584], mean action: 2.769 [0.000, 15.000],  loss: 0.018436, mae: 0.433960, mean_q: 0.580741, mean_eps: 0.000000
 1230/5000: episode: 48, duration: 0.193s, episode steps:  16, steps per second:  83, episode reward: -38.910, mean reward: -2.432 [-32.328,  2.618], mean action: 6.062 [0.000, 15.000],  loss: 0.018401, mae: 0.438438, mean_q: 0.527154, mean_eps: 0.000000
 1252/5000: episode: 49, duration: 0.273s, episode steps:  22, steps per second:  81, episode reward: -35.430, mean reward: -1.610 [-33.000,  2.555], mean action: 6.636 [0.000, 14.000],  loss: 0.018851, mae: 0.431147, mean_q: 0.463110, mean_eps: 0.000000
 1287/5000: episode: 50, duration: 0.407s, episode steps:  35, steps per second:  86, episode reward: 35.450, mean reward:  1.013 [-2.540, 32.530], mean action: 8.143 [0.000, 21.000],  loss: 0.021521, mae: 0.454621, mean_q: 0.468289, mean_eps: 0.000000
 1309/5000: episode: 51, duration: 0.260s, episode steps:  22, steps per second:  85, episode reward: 32.476, mean reward:  1.476 [-2.491, 33.000], mean action: 3.273 [0.000, 12.000],  loss: 0.017301, mae: 0.440480, mean_q: 0.417261, mean_eps: 0.000000
 1324/5000: episode: 52, duration: 0.188s, episode steps:  15, steps per second:  80, episode reward: 42.000, mean reward:  2.800 [-2.316, 32.360], mean action: 3.067 [0.000, 12.000],  loss: 0.027632, mae: 0.481596, mean_q: 0.427037, mean_eps: 0.000000
 1345/5000: episode: 53, duration: 0.259s, episode steps:  21, steps per second:  81, episode reward: -35.240, mean reward: -1.678 [-32.318,  2.879], mean action: 3.333 [0.000, 12.000],  loss: 0.019266, mae: 0.435693, mean_q: 0.429723, mean_eps: 0.000000
 1373/5000: episode: 54, duration: 0.336s, episode steps:  28, steps per second:  83, episode reward: 36.000, mean reward:  1.286 [-2.535, 32.620], mean action: 3.500 [0.000, 14.000],  loss: 0.018813, mae: 0.435008, mean_q: 0.439902, mean_eps: 0.000000
 1389/5000: episode: 55, duration: 0.199s, episode steps:  16, steps per second:  80, episode reward: 35.901, mean reward:  2.244 [-2.900, 32.901], mean action: 4.750 [0.000, 12.000],  loss: 0.021629, mae: 0.443109, mean_q: 0.443496, mean_eps: 0.000000
 1416/5000: episode: 56, duration: 0.327s, episode steps:  27, steps per second:  83, episode reward: 35.331, mean reward:  1.309 [-2.463, 32.435], mean action: 4.185 [0.000, 15.000],  loss: 0.017666, mae: 0.421595, mean_q: 0.456263, mean_eps: 0.000000
 1443/5000: episode: 57, duration: 0.317s, episode steps:  27, steps per second:  85, episode reward: 32.867, mean reward:  1.217 [-2.551, 32.197], mean action: 7.185 [0.000, 19.000],  loss: 0.019906, mae: 0.434523, mean_q: 0.431798, mean_eps: 0.000000
 1469/5000: episode: 58, duration: 0.313s, episode steps:  26, steps per second:  83, episode reward: 35.874, mean reward:  1.380 [-2.488, 32.874], mean action: 5.808 [0.000, 20.000],  loss: 0.021953, mae: 0.439457, mean_q: 0.512379, mean_eps: 0.000000
 1494/5000: episode: 59, duration: 0.306s, episode steps:  25, steps per second:  82, episode reward: 38.264, mean reward:  1.531 [-2.185, 31.770], mean action: 3.080 [0.000, 9.000],  loss: 0.019273, mae: 0.430117, mean_q: 0.489986, mean_eps: 0.000000
 1528/5000: episode: 60, duration: 0.404s, episode steps:  34, steps per second:  84, episode reward: -32.580, mean reward: -0.958 [-32.251,  2.810], mean action: 5.529 [0.000, 21.000],  loss: 0.021708, mae: 0.439080, mean_q: 0.429834, mean_eps: 0.000000
 1556/5000: episode: 61, duration: 0.342s, episode steps:  28, steps per second:  82, episode reward: 32.539, mean reward:  1.162 [-3.000, 33.000], mean action: 7.321 [0.000, 21.000],  loss: 0.021882, mae: 0.435777, mean_q: 0.471269, mean_eps: 0.000000
 1579/5000: episode: 62, duration: 0.282s, episode steps:  23, steps per second:  82, episode reward: 41.798, mean reward:  1.817 [-2.161, 32.290], mean action: 3.435 [0.000, 19.000],  loss: 0.020636, mae: 0.433278, mean_q: 0.456916, mean_eps: 0.000000
 1617/5000: episode: 63, duration: 0.507s, episode steps:  38, steps per second:  75, episode reward: 35.319, mean reward:  0.929 [-2.310, 32.329], mean action: 9.868 [0.000, 19.000],  loss: 0.021155, mae: 0.432736, mean_q: 0.489865, mean_eps: 0.000000
 1642/5000: episode: 64, duration: 0.403s, episode steps:  25, steps per second:  62, episode reward: 38.254, mean reward:  1.530 [-2.224, 32.260], mean action: 3.520 [0.000, 19.000],  loss: 0.020786, mae: 0.430447, mean_q: 0.485996, mean_eps: 0.000000
 1658/5000: episode: 65, duration: 0.391s, episode steps:  16, steps per second:  41, episode reward: 44.043, mean reward:  2.753 [-2.275, 32.138], mean action: 2.375 [0.000, 19.000],  loss: 0.022726, mae: 0.436049, mean_q: 0.518399, mean_eps: 0.000000
 1683/5000: episode: 66, duration: 0.460s, episode steps:  25, steps per second:  54, episode reward: 35.636, mean reward:  1.425 [-2.481, 32.898], mean action: 4.720 [0.000, 15.000],  loss: 0.017781, mae: 0.420948, mean_q: 0.510826, mean_eps: 0.000000
 1703/5000: episode: 67, duration: 0.533s, episode steps:  20, steps per second:  38, episode reward: 38.216, mean reward:  1.911 [-2.399, 32.126], mean action: 3.450 [0.000, 15.000],  loss: 0.020507, mae: 0.437993, mean_q: 0.474999, mean_eps: 0.000000
 1724/5000: episode: 68, duration: 0.339s, episode steps:  21, steps per second:  62, episode reward: 35.811, mean reward:  1.705 [-3.000, 31.984], mean action: 5.381 [0.000, 15.000],  loss: 0.021289, mae: 0.448611, mean_q: 0.453736, mean_eps: 0.000000
 1740/5000: episode: 69, duration: 0.202s, episode steps:  16, steps per second:  79, episode reward: 35.387, mean reward:  2.212 [-3.000, 32.133], mean action: 6.812 [0.000, 15.000],  loss: 0.021134, mae: 0.437064, mean_q: 0.470652, mean_eps: 0.000000
 1758/5000: episode: 70, duration: 0.227s, episode steps:  18, steps per second:  79, episode reward: 38.702, mean reward:  2.150 [-2.408, 32.242], mean action: 5.278 [0.000, 15.000],  loss: 0.023674, mae: 0.452501, mean_q: 0.448271, mean_eps: 0.000000
 1775/5000: episode: 71, duration: 0.341s, episode steps:  17, steps per second:  50, episode reward: 41.164, mean reward:  2.421 [-2.482, 33.000], mean action: 4.235 [0.000, 18.000],  loss: 0.021727, mae: 0.457221, mean_q: 0.406856, mean_eps: 0.000000
 1793/5000: episode: 72, duration: 0.361s, episode steps:  18, steps per second:  50, episode reward: 41.950, mean reward:  2.331 [-2.394, 32.190], mean action: 3.111 [0.000, 15.000],  loss: 0.020840, mae: 0.445249, mean_q: 0.421705, mean_eps: 0.000000
 1825/5000: episode: 73, duration: 0.392s, episode steps:  32, steps per second:  82, episode reward: -35.630, mean reward: -1.113 [-32.740,  2.420], mean action: 3.781 [0.000, 12.000],  loss: 0.019446, mae: 0.431926, mean_q: 0.436201, mean_eps: 0.000000
 1870/5000: episode: 74, duration: 0.546s, episode steps:  45, steps per second:  82, episode reward: 40.410, mean reward:  0.898 [-3.000, 31.858], mean action: 3.933 [0.000, 13.000],  loss: 0.019404, mae: 0.429890, mean_q: 0.453745, mean_eps: 0.000000
 1899/5000: episode: 75, duration: 0.350s, episode steps:  29, steps per second:  83, episode reward: -32.040, mean reward: -1.105 [-32.282,  3.000], mean action: 5.552 [0.000, 19.000],  loss: 0.017543, mae: 0.424462, mean_q: 0.422489, mean_eps: 0.000000
 1922/5000: episode: 76, duration: 0.274s, episode steps:  23, steps per second:  84, episode reward: 38.051, mean reward:  1.654 [-2.613, 32.350], mean action: 3.174 [0.000, 12.000],  loss: 0.019916, mae: 0.442494, mean_q: 0.445694, mean_eps: 0.000000
 1945/5000: episode: 77, duration: 0.285s, episode steps:  23, steps per second:  81, episode reward: 38.092, mean reward:  1.656 [-2.660, 32.380], mean action: 4.043 [0.000, 19.000],  loss: 0.020394, mae: 0.445686, mean_q: 0.453164, mean_eps: 0.000000
 1962/5000: episode: 78, duration: 0.220s, episode steps:  17, steps per second:  77, episode reward: 39.000, mean reward:  2.294 [-2.191, 30.907], mean action: 4.588 [0.000, 19.000],  loss: 0.021338, mae: 0.445608, mean_q: 0.440743, mean_eps: 0.000000
 1987/5000: episode: 79, duration: 0.309s, episode steps:  25, steps per second:  81, episode reward: -38.170, mean reward: -1.527 [-33.000,  2.540], mean action: 4.800 [0.000, 19.000],  loss: 0.015240, mae: 0.427018, mean_q: 0.465747, mean_eps: 0.000000
 2010/5000: episode: 80, duration: 0.279s, episode steps:  23, steps per second:  82, episode reward: 35.410, mean reward:  1.540 [-3.000, 32.445], mean action: 5.043 [0.000, 19.000],  loss: 0.022005, mae: 0.457390, mean_q: 0.443217, mean_eps: 0.000000
 2031/5000: episode: 81, duration: 0.256s, episode steps:  21, steps per second:  82, episode reward: 36.000, mean reward:  1.714 [-2.523, 32.300], mean action: 3.714 [0.000, 15.000],  loss: 0.017423, mae: 0.432530, mean_q: 0.401422, mean_eps: 0.000000
 2051/5000: episode: 82, duration: 0.253s, episode steps:  20, steps per second:  79, episode reward: 38.592, mean reward:  1.930 [-2.127, 32.430], mean action: 3.450 [0.000, 12.000],  loss: 0.017538, mae: 0.435771, mean_q: 0.423150, mean_eps: 0.000000
 2088/5000: episode: 83, duration: 0.433s, episode steps:  37, steps per second:  85, episode reward: 44.142, mean reward:  1.193 [-2.262, 32.553], mean action: 1.676 [0.000, 12.000],  loss: 0.017343, mae: 0.429712, mean_q: 0.434552, mean_eps: 0.000000
 2110/5000: episode: 84, duration: 0.258s, episode steps:  22, steps per second:  85, episode reward: 38.346, mean reward:  1.743 [-2.810, 32.230], mean action: 3.273 [0.000, 12.000],  loss: 0.019866, mae: 0.434848, mean_q: 0.462351, mean_eps: 0.000000
 2135/5000: episode: 85, duration: 0.341s, episode steps:  25, steps per second:  73, episode reward: -32.170, mean reward: -1.287 [-31.442,  2.347], mean action: 5.640 [0.000, 17.000],  loss: 0.021924, mae: 0.453701, mean_q: 0.413136, mean_eps: 0.000000
 2150/5000: episode: 86, duration: 0.242s, episode steps:  15, steps per second:  62, episode reward: -41.550, mean reward: -2.770 [-33.000,  2.040], mean action: 6.067 [0.000, 19.000],  loss: 0.019298, mae: 0.454970, mean_q: 0.395838, mean_eps: 0.000000
 2168/5000: episode: 87, duration: 0.343s, episode steps:  18, steps per second:  52, episode reward: 42.000, mean reward:  2.333 [-2.329, 33.000], mean action: 2.278 [0.000, 11.000],  loss: 0.018908, mae: 0.441270, mean_q: 0.414197, mean_eps: 0.000000
 2193/5000: episode: 88, duration: 0.484s, episode steps:  25, steps per second:  52, episode reward: 36.911, mean reward:  1.476 [-2.561, 32.180], mean action: 3.320 [0.000, 11.000],  loss: 0.022862, mae: 0.448014, mean_q: 0.515058, mean_eps: 0.000000
 2221/5000: episode: 89, duration: 0.523s, episode steps:  28, steps per second:  54, episode reward: 41.243, mean reward:  1.473 [-2.277, 32.070], mean action: 5.500 [1.000, 15.000],  loss: 0.017204, mae: 0.421340, mean_q: 0.482490, mean_eps: 0.000000
 2246/5000: episode: 90, duration: 0.516s, episode steps:  25, steps per second:  48, episode reward: 32.115, mean reward:  1.285 [-3.000, 32.060], mean action: 4.120 [0.000, 19.000],  loss: 0.018701, mae: 0.427805, mean_q: 0.444230, mean_eps: 0.000000
 2268/5000: episode: 91, duration: 0.342s, episode steps:  22, steps per second:  64, episode reward: 35.309, mean reward:  1.605 [-2.259, 32.164], mean action: 4.136 [0.000, 15.000],  loss: 0.023218, mae: 0.459964, mean_q: 0.460240, mean_eps: 0.000000
 2295/5000: episode: 92, duration: 0.648s, episode steps:  27, steps per second:  42, episode reward: -36.000, mean reward: -1.333 [-32.553,  2.693], mean action: 4.407 [0.000, 17.000],  loss: 0.017896, mae: 0.433523, mean_q: 0.489886, mean_eps: 0.000000
 2322/5000: episode: 93, duration: 0.326s, episode steps:  27, steps per second:  83, episode reward: 32.237, mean reward:  1.194 [-2.578, 32.160], mean action: 4.148 [0.000, 19.000],  loss: 0.017237, mae: 0.426606, mean_q: 0.519077, mean_eps: 0.000000
 2335/5000: episode: 94, duration: 0.171s, episode steps:  13, steps per second:  76, episode reward: 41.495, mean reward:  3.192 [-3.000, 33.000], mean action: 2.538 [0.000, 11.000],  loss: 0.018327, mae: 0.427701, mean_q: 0.498986, mean_eps: 0.000000
 2345/5000: episode: 95, duration: 0.131s, episode steps:  10, steps per second:  76, episode reward: 44.133, mean reward:  4.413 [-2.083, 31.638], mean action: 4.200 [0.000, 14.000],  loss: 0.019374, mae: 0.447873, mean_q: 0.453219, mean_eps: 0.000000
 2367/5000: episode: 96, duration: 0.261s, episode steps:  22, steps per second:  84, episode reward: 35.344, mean reward:  1.607 [-3.000, 31.991], mean action: 2.682 [0.000, 11.000],  loss: 0.020428, mae: 0.457956, mean_q: 0.456546, mean_eps: 0.000000
 2392/5000: episode: 97, duration: 0.298s, episode steps:  25, steps per second:  84, episode reward: 38.178, mean reward:  1.527 [-2.737, 32.053], mean action: 2.160 [0.000, 11.000],  loss: 0.019153, mae: 0.447772, mean_q: 0.446918, mean_eps: 0.000000
 2450/5000: episode: 98, duration: 0.684s, episode steps:  58, steps per second:  85, episode reward: 35.240, mean reward:  0.608 [-2.366, 32.240], mean action: 2.379 [0.000, 20.000],  loss: 0.019641, mae: 0.441747, mean_q: 0.463879, mean_eps: 0.000000
 2473/5000: episode: 99, duration: 0.310s, episode steps:  23, steps per second:  74, episode reward: 32.115, mean reward:  1.396 [-2.998, 32.510], mean action: 4.696 [0.000, 14.000],  loss: 0.016295, mae: 0.430096, mean_q: 0.415090, mean_eps: 0.000000
 2496/5000: episode: 100, duration: 0.289s, episode steps:  23, steps per second:  79, episode reward: 35.374, mean reward:  1.538 [-2.260, 31.952], mean action: 2.826 [0.000, 19.000],  loss: 0.020184, mae: 0.440357, mean_q: 0.450571, mean_eps: 0.000000
 2520/5000: episode: 101, duration: 0.293s, episode steps:  24, steps per second:  82, episode reward: 34.086, mean reward:  1.420 [-3.000, 31.981], mean action: 11.667 [0.000, 21.000],  loss: 0.022167, mae: 0.460303, mean_q: 0.431894, mean_eps: 0.000000
 2546/5000: episode: 102, duration: 0.313s, episode steps:  26, steps per second:  83, episode reward: 34.298, mean reward:  1.319 [-3.000, 31.863], mean action: 6.923 [0.000, 19.000],  loss: 0.016127, mae: 0.430513, mean_q: 0.476808, mean_eps: 0.000000
 2568/5000: episode: 103, duration: 0.283s, episode steps:  22, steps per second:  78, episode reward: 38.274, mean reward:  1.740 [-2.518, 31.933], mean action: 3.773 [0.000, 19.000],  loss: 0.016802, mae: 0.427992, mean_q: 0.476106, mean_eps: 0.000000
 2586/5000: episode: 104, duration: 0.222s, episode steps:  18, steps per second:  81, episode reward: 38.227, mean reward:  2.124 [-2.157, 32.598], mean action: 5.000 [0.000, 19.000],  loss: 0.022666, mae: 0.467584, mean_q: 0.539251, mean_eps: 0.000000
 2635/5000: episode: 105, duration: 0.757s, episode steps:  49, steps per second:  65, episode reward: 32.211, mean reward:  0.657 [-3.000, 32.120], mean action: 11.408 [0.000, 20.000],  loss: 0.018122, mae: 0.429515, mean_q: 0.491443, mean_eps: 0.000000
 2651/5000: episode: 106, duration: 0.198s, episode steps:  16, steps per second:  81, episode reward: 39.000, mean reward:  2.438 [-3.000, 33.000], mean action: 5.000 [0.000, 19.000],  loss: 0.015310, mae: 0.418304, mean_q: 0.449552, mean_eps: 0.000000
 2675/5000: episode: 107, duration: 0.293s, episode steps:  24, steps per second:  82, episode reward: 38.638, mean reward:  1.610 [-2.207, 32.170], mean action: 5.000 [3.000, 19.000],  loss: 0.023803, mae: 0.466081, mean_q: 0.450273, mean_eps: 0.000000
 2691/5000: episode: 108, duration: 0.236s, episode steps:  16, steps per second:  68, episode reward: 44.340, mean reward:  2.771 [-2.145, 32.350], mean action: 3.438 [0.000, 19.000],  loss: 0.018346, mae: 0.442246, mean_q: 0.418234, mean_eps: 0.000000
 2712/5000: episode: 109, duration: 0.290s, episode steps:  21, steps per second:  72, episode reward: 38.420, mean reward:  1.830 [-2.454, 32.200], mean action: 4.238 [0.000, 19.000],  loss: 0.017685, mae: 0.440353, mean_q: 0.415237, mean_eps: 0.000000
 2755/5000: episode: 110, duration: 0.819s, episode steps:  43, steps per second:  52, episode reward: -35.820, mean reward: -0.833 [-32.459,  2.715], mean action: 2.512 [0.000, 18.000],  loss: 0.021888, mae: 0.458808, mean_q: 0.471468, mean_eps: 0.000000
 2776/5000: episode: 111, duration: 0.266s, episode steps:  21, steps per second:  79, episode reward: -35.130, mean reward: -1.673 [-32.127,  2.901], mean action: 4.381 [0.000, 19.000],  loss: 0.019431, mae: 0.440470, mean_q: 0.490237, mean_eps: 0.000000
 2804/5000: episode: 112, duration: 0.338s, episode steps:  28, steps per second:  83, episode reward: -32.200, mean reward: -1.150 [-32.058,  2.630], mean action: 5.643 [0.000, 20.000],  loss: 0.020433, mae: 0.444753, mean_q: 0.493605, mean_eps: 0.000000
 2838/5000: episode: 113, duration: 0.396s, episode steps:  34, steps per second:  86, episode reward: 32.266, mean reward:  0.949 [-3.000, 32.270], mean action: 4.147 [0.000, 19.000],  loss: 0.023370, mae: 0.452428, mean_q: 0.502320, mean_eps: 0.000000
 2853/5000: episode: 114, duration: 0.200s, episode steps:  15, steps per second:  75, episode reward: 41.319, mean reward:  2.755 [-3.000, 32.011], mean action: 2.733 [0.000, 9.000],  loss: 0.024730, mae: 0.456950, mean_q: 0.550171, mean_eps: 0.000000
 2881/5000: episode: 115, duration: 0.416s, episode steps:  28, steps per second:  67, episode reward: 37.400, mean reward:  1.336 [-2.560, 32.346], mean action: 4.250 [0.000, 21.000],  loss: 0.024218, mae: 0.461682, mean_q: 0.508304, mean_eps: 0.000000
 2900/5000: episode: 116, duration: 0.233s, episode steps:  19, steps per second:  81, episode reward: 32.814, mean reward:  1.727 [-3.000, 32.024], mean action: 6.000 [0.000, 19.000],  loss: 0.020922, mae: 0.446656, mean_q: 0.463254, mean_eps: 0.000000
 2920/5000: episode: 117, duration: 0.244s, episode steps:  20, steps per second:  82, episode reward: -32.480, mean reward: -1.624 [-31.805,  2.710], mean action: 6.150 [0.000, 19.000],  loss: 0.019502, mae: 0.441518, mean_q: 0.501552, mean_eps: 0.000000
 2942/5000: episode: 118, duration: 0.264s, episode steps:  22, steps per second:  83, episode reward: -32.840, mean reward: -1.493 [-32.253,  2.432], mean action: 8.318 [0.000, 17.000],  loss: 0.023295, mae: 0.451044, mean_q: 0.475094, mean_eps: 0.000000
 2958/5000: episode: 119, duration: 0.205s, episode steps:  16, steps per second:  78, episode reward: 41.519, mean reward:  2.595 [-2.294, 32.400], mean action: 4.188 [0.000, 12.000],  loss: 0.018273, mae: 0.425587, mean_q: 0.450456, mean_eps: 0.000000
 2984/5000: episode: 120, duration: 0.318s, episode steps:  26, steps per second:  82, episode reward: 35.580, mean reward:  1.368 [-2.515, 32.796], mean action: 2.923 [0.000, 12.000],  loss: 0.017494, mae: 0.420509, mean_q: 0.484373, mean_eps: 0.000000
 3010/5000: episode: 121, duration: 0.312s, episode steps:  26, steps per second:  83, episode reward: 32.804, mean reward:  1.262 [-2.656, 32.902], mean action: 3.962 [0.000, 9.000],  loss: 0.019551, mae: 0.433808, mean_q: 0.472189, mean_eps: 0.000000
 3033/5000: episode: 122, duration: 0.270s, episode steps:  23, steps per second:  85, episode reward: 33.000, mean reward:  1.435 [-2.416, 30.060], mean action: 2.478 [0.000, 9.000],  loss: 0.018483, mae: 0.436066, mean_q: 0.479898, mean_eps: 0.000000
 3061/5000: episode: 123, duration: 0.342s, episode steps:  28, steps per second:  82, episode reward: 35.208, mean reward:  1.257 [-2.395, 31.648], mean action: 4.250 [0.000, 15.000],  loss: 0.019830, mae: 0.434056, mean_q: 0.464194, mean_eps: 0.000000
 3083/5000: episode: 124, duration: 0.255s, episode steps:  22, steps per second:  86, episode reward: -42.000, mean reward: -1.909 [-33.000,  2.190], mean action: 6.409 [0.000, 15.000],  loss: 0.020919, mae: 0.435702, mean_q: 0.511868, mean_eps: 0.000000
 3108/5000: episode: 125, duration: 0.293s, episode steps:  25, steps per second:  85, episode reward: -32.310, mean reward: -1.292 [-32.118,  2.386], mean action: 6.280 [0.000, 18.000],  loss: 0.019597, mae: 0.427152, mean_q: 0.545652, mean_eps: 0.000000
 3136/5000: episode: 126, duration: 0.334s, episode steps:  28, steps per second:  84, episode reward: 32.614, mean reward:  1.165 [-2.388, 32.394], mean action: 4.500 [0.000, 12.000],  loss: 0.022135, mae: 0.451382, mean_q: 0.496756, mean_eps: 0.000000
 3162/5000: episode: 127, duration: 0.304s, episode steps:  26, steps per second:  86, episode reward: -35.610, mean reward: -1.370 [-31.681,  2.324], mean action: 8.231 [0.000, 15.000],  loss: 0.019913, mae: 0.454088, mean_q: 0.427156, mean_eps: 0.000000
 3180/5000: episode: 128, duration: 0.232s, episode steps:  18, steps per second:  78, episode reward: 38.399, mean reward:  2.133 [-3.000, 32.230], mean action: 3.278 [0.000, 14.000],  loss: 0.021190, mae: 0.460354, mean_q: 0.416262, mean_eps: 0.000000
 3202/5000: episode: 129, duration: 0.271s, episode steps:  22, steps per second:  81, episode reward: 35.775, mean reward:  1.626 [-2.552, 32.380], mean action: 4.773 [0.000, 20.000],  loss: 0.021834, mae: 0.457572, mean_q: 0.469793, mean_eps: 0.000000
 3224/5000: episode: 130, duration: 0.265s, episode steps:  22, steps per second:  83, episode reward: -32.910, mean reward: -1.496 [-32.727,  2.709], mean action: 5.409 [0.000, 14.000],  loss: 0.018223, mae: 0.438223, mean_q: 0.507495, mean_eps: 0.000000
 3242/5000: episode: 131, duration: 0.219s, episode steps:  18, steps per second:  82, episode reward: 40.904, mean reward:  2.272 [-2.273, 32.125], mean action: 3.111 [0.000, 9.000],  loss: 0.022416, mae: 0.459529, mean_q: 0.404368, mean_eps: 0.000000
 3269/5000: episode: 132, duration: 0.325s, episode steps:  27, steps per second:  83, episode reward: 32.570, mean reward:  1.206 [-3.000, 32.180], mean action: 5.815 [0.000, 19.000],  loss: 0.019660, mae: 0.440707, mean_q: 0.425850, mean_eps: 0.000000
 3297/5000: episode: 133, duration: 0.335s, episode steps:  28, steps per second:  84, episode reward: -37.110, mean reward: -1.325 [-31.922,  2.240], mean action: 7.464 [0.000, 15.000],  loss: 0.018404, mae: 0.435434, mean_q: 0.454797, mean_eps: 0.000000
 3311/5000: episode: 134, duration: 0.177s, episode steps:  14, steps per second:  79, episode reward: 44.556, mean reward:  3.183 [-2.355, 33.000], mean action: 5.286 [1.000, 19.000],  loss: 0.020603, mae: 0.447227, mean_q: 0.400948, mean_eps: 0.000000
 3326/5000: episode: 135, duration: 0.271s, episode steps:  15, steps per second:  55, episode reward: 40.935, mean reward:  2.729 [-2.490, 32.197], mean action: 5.467 [0.000, 19.000],  loss: 0.019496, mae: 0.447201, mean_q: 0.401611, mean_eps: 0.000000
 3345/5000: episode: 136, duration: 0.236s, episode steps:  19, steps per second:  80, episode reward: 38.530, mean reward:  2.028 [-2.326, 32.083], mean action: 4.421 [0.000, 19.000],  loss: 0.019568, mae: 0.441982, mean_q: 0.418054, mean_eps: 0.000000
 3362/5000: episode: 137, duration: 0.216s, episode steps:  17, steps per second:  79, episode reward: 43.107, mean reward:  2.536 [-2.229, 32.340], mean action: 3.471 [0.000, 11.000],  loss: 0.018315, mae: 0.437027, mean_q: 0.392734, mean_eps: 0.000000
 3408/5000: episode: 138, duration: 0.532s, episode steps:  46, steps per second:  86, episode reward: -35.180, mean reward: -0.765 [-32.210,  2.570], mean action: 5.935 [0.000, 19.000],  loss: 0.019729, mae: 0.449729, mean_q: 0.383853, mean_eps: 0.000000
 3435/5000: episode: 139, duration: 0.320s, episode steps:  27, steps per second:  84, episode reward: -32.630, mean reward: -1.209 [-32.235,  2.452], mean action: 4.963 [0.000, 19.000],  loss: 0.015609, mae: 0.421212, mean_q: 0.400711, mean_eps: 0.000000
 3503/5000: episode: 140, duration: 0.769s, episode steps:  68, steps per second:  88, episode reward: 33.000, mean reward:  0.485 [-2.450, 32.550], mean action: 12.662 [0.000, 17.000],  loss: 0.020196, mae: 0.433156, mean_q: 0.443265, mean_eps: 0.000000
 3526/5000: episode: 141, duration: 0.274s, episode steps:  23, steps per second:  84, episode reward: 36.000, mean reward:  1.565 [-3.000, 32.570], mean action: 5.304 [0.000, 19.000],  loss: 0.021920, mae: 0.427094, mean_q: 0.543018, mean_eps: 0.000000
 3550/5000: episode: 142, duration: 0.284s, episode steps:  24, steps per second:  84, episode reward: 32.812, mean reward:  1.367 [-2.297, 32.170], mean action: 4.542 [0.000, 19.000],  loss: 0.024510, mae: 0.455472, mean_q: 0.524452, mean_eps: 0.000000
 3571/5000: episode: 143, duration: 0.251s, episode steps:  21, steps per second:  84, episode reward: 34.663, mean reward:  1.651 [-2.805, 32.653], mean action: 6.000 [0.000, 19.000],  loss: 0.017769, mae: 0.422762, mean_q: 0.549530, mean_eps: 0.000000
 3600/5000: episode: 144, duration: 0.342s, episode steps:  29, steps per second:  85, episode reward: 35.296, mean reward:  1.217 [-3.000, 32.562], mean action: 3.586 [0.000, 19.000],  loss: 0.019181, mae: 0.432050, mean_q: 0.526366, mean_eps: 0.000000
 3622/5000: episode: 145, duration: 0.262s, episode steps:  22, steps per second:  84, episode reward: 32.250, mean reward:  1.466 [-2.562, 32.463], mean action: 5.182 [0.000, 19.000],  loss: 0.015411, mae: 0.416628, mean_q: 0.481337, mean_eps: 0.000000
 3649/5000: episode: 146, duration: 0.325s, episode steps:  27, steps per second:  83, episode reward: 35.834, mean reward:  1.327 [-2.284, 32.050], mean action: 7.296 [0.000, 20.000],  loss: 0.022920, mae: 0.456497, mean_q: 0.432538, mean_eps: 0.000000
 3669/5000: episode: 147, duration: 0.241s, episode steps:  20, steps per second:  83, episode reward: 35.534, mean reward:  1.777 [-2.339, 32.906], mean action: 3.850 [0.000, 20.000],  loss: 0.021309, mae: 0.445014, mean_q: 0.462812, mean_eps: 0.000000
 3698/5000: episode: 148, duration: 0.346s, episode steps:  29, steps per second:  84, episode reward: 34.970, mean reward:  1.206 [-3.000, 32.603], mean action: 7.483 [0.000, 19.000],  loss: 0.017746, mae: 0.423858, mean_q: 0.464106, mean_eps: 0.000000
 3713/5000: episode: 149, duration: 0.188s, episode steps:  15, steps per second:  80, episode reward: 39.000, mean reward:  2.600 [-2.453, 30.221], mean action: 2.667 [0.000, 11.000],  loss: 0.019876, mae: 0.441074, mean_q: 0.420442, mean_eps: 0.000000
 3734/5000: episode: 150, duration: 0.253s, episode steps:  21, steps per second:  83, episode reward: 38.202, mean reward:  1.819 [-3.000, 32.070], mean action: 7.048 [0.000, 20.000],  loss: 0.017749, mae: 0.427216, mean_q: 0.421497, mean_eps: 0.000000
 3765/5000: episode: 151, duration: 0.366s, episode steps:  31, steps per second:  85, episode reward: -35.840, mean reward: -1.156 [-32.117,  2.381], mean action: 8.161 [0.000, 15.000],  loss: 0.019258, mae: 0.426125, mean_q: 0.414233, mean_eps: 0.000000
 3776/5000: episode: 152, duration: 0.142s, episode steps:  11, steps per second:  78, episode reward: 44.296, mean reward:  4.027 [-2.021, 31.794], mean action: 3.364 [0.000, 14.000],  loss: 0.016985, mae: 0.411094, mean_q: 0.439102, mean_eps: 0.000000
 3804/5000: episode: 153, duration: 0.329s, episode steps:  28, steps per second:  85, episode reward: -32.720, mean reward: -1.169 [-32.245,  3.000], mean action: 6.821 [0.000, 19.000],  loss: 0.017805, mae: 0.420400, mean_q: 0.475981, mean_eps: 0.000000
 3831/5000: episode: 154, duration: 0.340s, episode steps:  27, steps per second:  79, episode reward: -32.440, mean reward: -1.201 [-32.156,  2.778], mean action: 8.852 [0.000, 20.000],  loss: 0.022221, mae: 0.441486, mean_q: 0.474218, mean_eps: 0.000000
 3857/5000: episode: 155, duration: 0.323s, episode steps:  26, steps per second:  80, episode reward: 40.660, mean reward:  1.564 [-2.173, 32.450], mean action: 6.077 [1.000, 17.000],  loss: 0.022444, mae: 0.451869, mean_q: 0.435351, mean_eps: 0.000000
 3888/5000: episode: 156, duration: 0.363s, episode steps:  31, steps per second:  85, episode reward: 38.181, mean reward:  1.232 [-2.484, 32.410], mean action: 8.871 [1.000, 20.000],  loss: 0.019740, mae: 0.430365, mean_q: 0.424747, mean_eps: 0.000000
 3905/5000: episode: 157, duration: 0.214s, episode steps:  17, steps per second:  79, episode reward: 41.030, mean reward:  2.414 [-2.801, 32.903], mean action: 4.765 [0.000, 20.000],  loss: 0.018107, mae: 0.418931, mean_q: 0.515124, mean_eps: 0.000000
 3924/5000: episode: 158, duration: 0.231s, episode steps:  19, steps per second:  82, episode reward: 40.887, mean reward:  2.152 [-3.000, 32.215], mean action: 4.579 [0.000, 20.000],  loss: 0.020717, mae: 0.428423, mean_q: 0.494518, mean_eps: 0.000000
 3951/5000: episode: 159, duration: 0.319s, episode steps:  27, steps per second:  85, episode reward: -38.600, mean reward: -1.430 [-32.284,  2.140], mean action: 4.778 [0.000, 15.000],  loss: 0.020176, mae: 0.433260, mean_q: 0.476116, mean_eps: 0.000000
 3972/5000: episode: 160, duration: 0.260s, episode steps:  21, steps per second:  81, episode reward: 40.699, mean reward:  1.938 [-2.793, 31.787], mean action: 4.429 [0.000, 11.000],  loss: 0.019461, mae: 0.428351, mean_q: 0.479698, mean_eps: 0.000000
 3985/5000: episode: 161, duration: 0.171s, episode steps:  13, steps per second:  76, episode reward: 42.000, mean reward:  3.231 [-2.673, 32.340], mean action: 3.385 [0.000, 14.000],  loss: 0.017566, mae: 0.423853, mean_q: 0.436255, mean_eps: 0.000000
 4006/5000: episode: 162, duration: 0.261s, episode steps:  21, steps per second:  81, episode reward: -32.670, mean reward: -1.556 [-32.084,  2.104], mean action: 4.952 [0.000, 14.000],  loss: 0.018266, mae: 0.425364, mean_q: 0.449758, mean_eps: 0.000000
 4034/5000: episode: 163, duration: 0.338s, episode steps:  28, steps per second:  83, episode reward: 32.749, mean reward:  1.170 [-2.387, 32.530], mean action: 6.750 [0.000, 14.000],  loss: 0.022370, mae: 0.444071, mean_q: 0.425726, mean_eps: 0.000000
 4055/5000: episode: 164, duration: 0.246s, episode steps:  21, steps per second:  85, episode reward: -38.930, mean reward: -1.854 [-32.814,  2.340], mean action: 8.905 [1.000, 16.000],  loss: 0.023361, mae: 0.454387, mean_q: 0.447870, mean_eps: 0.000000
 4069/5000: episode: 165, duration: 0.172s, episode steps:  14, steps per second:  81, episode reward: 39.000, mean reward:  2.786 [-3.000, 33.000], mean action: 2.643 [0.000, 9.000],  loss: 0.017658, mae: 0.427697, mean_q: 0.505537, mean_eps: 0.000000
 4103/5000: episode: 166, duration: 0.398s, episode steps:  34, steps per second:  85, episode reward: 34.417, mean reward:  1.012 [-2.511, 32.070], mean action: 11.529 [0.000, 21.000],  loss: 0.021290, mae: 0.443369, mean_q: 0.494216, mean_eps: 0.000000
 4130/5000: episode: 167, duration: 0.335s, episode steps:  27, steps per second:  81, episode reward: 34.755, mean reward:  1.287 [-2.309, 32.404], mean action: 8.148 [1.000, 20.000],  loss: 0.018946, mae: 0.439191, mean_q: 0.452715, mean_eps: 0.000000
 4159/5000: episode: 168, duration: 0.367s, episode steps:  29, steps per second:  79, episode reward: 39.000, mean reward:  1.345 [-2.869, 32.300], mean action: 3.931 [1.000, 20.000],  loss: 0.016936, mae: 0.427290, mean_q: 0.467334, mean_eps: 0.000000
 4177/5000: episode: 169, duration: 0.224s, episode steps:  18, steps per second:  81, episode reward: 38.328, mean reward:  2.129 [-2.479, 32.350], mean action: 7.111 [1.000, 19.000],  loss: 0.020594, mae: 0.431528, mean_q: 0.435132, mean_eps: 0.000000
 4207/5000: episode: 170, duration: 0.371s, episode steps:  30, steps per second:  81, episode reward: 35.612, mean reward:  1.187 [-2.815, 32.151], mean action: 3.833 [0.000, 14.000],  loss: 0.020202, mae: 0.437470, mean_q: 0.450663, mean_eps: 0.000000
 4236/5000: episode: 171, duration: 0.361s, episode steps:  29, steps per second:  80, episode reward: 41.364, mean reward:  1.426 [-2.224, 32.120], mean action: 3.724 [0.000, 19.000],  loss: 0.019619, mae: 0.433603, mean_q: 0.464787, mean_eps: 0.000000
 4274/5000: episode: 172, duration: 0.452s, episode steps:  38, steps per second:  84, episode reward: -35.140, mean reward: -0.925 [-32.188,  3.000], mean action: 7.553 [0.000, 21.000],  loss: 0.018731, mae: 0.426270, mean_q: 0.439503, mean_eps: 0.000000
 4317/5000: episode: 173, duration: 0.499s, episode steps:  43, steps per second:  86, episode reward: 32.058, mean reward:  0.746 [-2.581, 31.858], mean action: 4.767 [0.000, 19.000],  loss: 0.019299, mae: 0.420196, mean_q: 0.430488, mean_eps: 0.000000
 4344/5000: episode: 174, duration: 0.311s, episode steps:  27, steps per second:  87, episode reward: -38.410, mean reward: -1.423 [-31.863,  2.169], mean action: 6.259 [0.000, 15.000],  loss: 0.022583, mae: 0.440709, mean_q: 0.443176, mean_eps: 0.000000
 4364/5000: episode: 175, duration: 0.243s, episode steps:  20, steps per second:  82, episode reward: 38.902, mean reward:  1.945 [-2.410, 32.342], mean action: 4.050 [0.000, 19.000],  loss: 0.017342, mae: 0.429089, mean_q: 0.414874, mean_eps: 0.000000
 4389/5000: episode: 176, duration: 0.301s, episode steps:  25, steps per second:  83, episode reward: 35.551, mean reward:  1.422 [-2.410, 33.000], mean action: 3.760 [0.000, 19.000],  loss: 0.019454, mae: 0.440705, mean_q: 0.414602, mean_eps: 0.000000
 4409/5000: episode: 177, duration: 0.243s, episode steps:  20, steps per second:  82, episode reward: 35.326, mean reward:  1.766 [-3.000, 32.392], mean action: 2.550 [0.000, 12.000],  loss: 0.018886, mae: 0.422411, mean_q: 0.450608, mean_eps: 0.000000
 4433/5000: episode: 178, duration: 0.284s, episode steps:  24, steps per second:  84, episode reward: 38.079, mean reward:  1.587 [-3.000, 32.194], mean action: 4.500 [0.000, 20.000],  loss: 0.021219, mae: 0.431061, mean_q: 0.435551, mean_eps: 0.000000
 4458/5000: episode: 179, duration: 0.308s, episode steps:  25, steps per second:  81, episode reward: -32.940, mean reward: -1.318 [-33.000,  2.670], mean action: 3.680 [0.000, 20.000],  loss: 0.021163, mae: 0.438496, mean_q: 0.416885, mean_eps: 0.000000
 4495/5000: episode: 180, duration: 0.440s, episode steps:  37, steps per second:  84, episode reward: 38.374, mean reward:  1.037 [-3.000, 32.131], mean action: 2.757 [1.000, 20.000],  loss: 0.019825, mae: 0.433812, mean_q: 0.418400, mean_eps: 0.000000
 4521/5000: episode: 181, duration: 0.312s, episode steps:  26, steps per second:  83, episode reward: 37.794, mean reward:  1.454 [-2.226, 31.815], mean action: 6.192 [0.000, 15.000],  loss: 0.021355, mae: 0.444156, mean_q: 0.375079, mean_eps: 0.000000
 4541/5000: episode: 182, duration: 0.246s, episode steps:  20, steps per second:  81, episode reward: 32.814, mean reward:  1.641 [-3.000, 32.490], mean action: 7.400 [0.000, 15.000],  loss: 0.025632, mae: 0.457709, mean_q: 0.406784, mean_eps: 0.000000
 4591/5000: episode: 183, duration: 0.573s, episode steps:  50, steps per second:  87, episode reward: 32.671, mean reward:  0.653 [-2.528, 32.759], mean action: 4.940 [0.000, 20.000],  loss: 0.020213, mae: 0.428736, mean_q: 0.498935, mean_eps: 0.000000
 4608/5000: episode: 184, duration: 0.210s, episode steps:  17, steps per second:  81, episode reward: 40.893, mean reward:  2.405 [-2.498, 32.020], mean action: 5.118 [0.000, 19.000],  loss: 0.017149, mae: 0.425890, mean_q: 0.511527, mean_eps: 0.000000
 4629/5000: episode: 185, duration: 0.253s, episode steps:  21, steps per second:  83, episode reward: 35.243, mean reward:  1.678 [-3.000, 32.061], mean action: 5.762 [0.000, 19.000],  loss: 0.017991, mae: 0.424194, mean_q: 0.476641, mean_eps: 0.000000
 4646/5000: episode: 186, duration: 0.207s, episode steps:  17, steps per second:  82, episode reward: 36.000, mean reward:  2.118 [-3.000, 32.380], mean action: 7.647 [0.000, 19.000],  loss: 0.018144, mae: 0.422803, mean_q: 0.484351, mean_eps: 0.000000
 4672/5000: episode: 187, duration: 0.316s, episode steps:  26, steps per second:  82, episode reward: 35.761, mean reward:  1.375 [-2.644, 32.050], mean action: 5.423 [0.000, 19.000],  loss: 0.020742, mae: 0.431372, mean_q: 0.508741, mean_eps: 0.000000
 4702/5000: episode: 188, duration: 0.361s, episode steps:  30, steps per second:  83, episode reward: 32.621, mean reward:  1.087 [-2.511, 31.841], mean action: 7.567 [0.000, 19.000],  loss: 0.023150, mae: 0.439895, mean_q: 0.503633, mean_eps: 0.000000
 4722/5000: episode: 189, duration: 0.241s, episode steps:  20, steps per second:  83, episode reward: -36.000, mean reward: -1.800 [-32.017,  2.380], mean action: 4.150 [0.000, 19.000],  loss: 0.018267, mae: 0.420133, mean_q: 0.491053, mean_eps: 0.000000
 4733/5000: episode: 190, duration: 0.147s, episode steps:  11, steps per second:  75, episode reward: 44.034, mean reward:  4.003 [-2.113, 32.007], mean action: 2.818 [0.000, 19.000],  loss: 0.026170, mae: 0.459639, mean_q: 0.468014, mean_eps: 0.000000
 4767/5000: episode: 191, duration: 0.412s, episode steps:  34, steps per second:  83, episode reward: -32.400, mean reward: -0.953 [-31.599,  2.410], mean action: 7.794 [0.000, 19.000],  loss: 0.021409, mae: 0.436787, mean_q: 0.506239, mean_eps: 0.000000
 4791/5000: episode: 192, duration: 0.304s, episode steps:  24, steps per second:  79, episode reward: 44.293, mean reward:  1.846 [-2.011, 31.958], mean action: 3.667 [3.000, 19.000],  loss: 0.019592, mae: 0.426413, mean_q: 0.491557, mean_eps: 0.000000
 4810/5000: episode: 193, duration: 0.239s, episode steps:  19, steps per second:  80, episode reward: 41.220, mean reward:  2.169 [-2.281, 32.030], mean action: 4.842 [0.000, 19.000],  loss: 0.018979, mae: 0.430644, mean_q: 0.448128, mean_eps: 0.000000
 4848/5000: episode: 194, duration: 0.621s, episode steps:  38, steps per second:  61, episode reward: -32.130, mean reward: -0.846 [-31.932,  2.340], mean action: 4.605 [0.000, 19.000],  loss: 0.019212, mae: 0.433908, mean_q: 0.430287, mean_eps: 0.000000
 4869/5000: episode: 195, duration: 0.268s, episode steps:  21, steps per second:  78, episode reward: 38.805, mean reward:  1.848 [-2.423, 33.000], mean action: 4.429 [0.000, 19.000],  loss: 0.016278, mae: 0.419677, mean_q: 0.437922, mean_eps: 0.000000
 4892/5000: episode: 196, duration: 0.283s, episode steps:  23, steps per second:  81, episode reward: 32.261, mean reward:  1.403 [-2.904, 32.330], mean action: 7.087 [0.000, 19.000],  loss: 0.016570, mae: 0.422777, mean_q: 0.449921, mean_eps: 0.000000
 4914/5000: episode: 197, duration: 0.264s, episode steps:  22, steps per second:  83, episode reward: 35.770, mean reward:  1.626 [-3.000, 32.580], mean action: 4.500 [0.000, 19.000],  loss: 0.023774, mae: 0.460883, mean_q: 0.440770, mean_eps: 0.000000
 4952/5000: episode: 198, duration: 0.473s, episode steps:  38, steps per second:  80, episode reward: 35.441, mean reward:  0.933 [-3.000, 32.040], mean action: 3.605 [0.000, 19.000],  loss: 0.021623, mae: 0.441199, mean_q: 0.522210, mean_eps: 0.000000
 4971/5000: episode: 199, duration: 0.232s, episode steps:  19, steps per second:  82, episode reward: 38.673, mean reward:  2.035 [-3.000, 32.320], mean action: 3.895 [0.000, 19.000],  loss: 0.018339, mae: 0.428645, mean_q: 0.484756, mean_eps: 0.000000
done, took 58.798 seconds
DQN Evaluation: 2487 victories out of 2978 episodes
Training for 5000 steps ...
   29/5000: episode: 1, duration: 0.195s, episode steps:  29, steps per second: 149, episode reward: 44.306, mean reward:  1.528 [-2.335, 31.991], mean action: 5.069 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   57/5000: episode: 2, duration: 0.186s, episode steps:  28, steps per second: 150, episode reward: 38.430, mean reward:  1.373 [-2.546, 32.393], mean action: 5.714 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   81/5000: episode: 3, duration: 0.175s, episode steps:  24, steps per second: 137, episode reward: 43.425, mean reward:  1.809 [-3.000, 32.030], mean action: 5.167 [1.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   96/5000: episode: 4, duration: 0.109s, episode steps:  15, steps per second: 138, episode reward: 44.160, mean reward:  2.944 [-2.038, 32.173], mean action: 4.200 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  116/5000: episode: 5, duration: 0.135s, episode steps:  20, steps per second: 148, episode reward: 44.892, mean reward:  2.245 [-2.005, 32.421], mean action: 4.650 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  141/5000: episode: 6, duration: 0.159s, episode steps:  25, steps per second: 157, episode reward: 39.000, mean reward:  1.560 [-2.702, 30.093], mean action: 4.520 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  162/5000: episode: 7, duration: 0.137s, episode steps:  21, steps per second: 153, episode reward: 41.650, mean reward:  1.983 [-3.000, 32.030], mean action: 3.381 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  204/5000: episode: 8, duration: 0.256s, episode steps:  42, steps per second: 164, episode reward: 38.144, mean reward:  0.908 [-2.504, 32.577], mean action: 7.476 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  227/5000: episode: 9, duration: 0.148s, episode steps:  23, steps per second: 155, episode reward: 41.221, mean reward:  1.792 [-2.681, 32.154], mean action: 5.913 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  265/5000: episode: 10, duration: 0.236s, episode steps:  38, steps per second: 161, episode reward: 38.393, mean reward:  1.010 [-2.940, 32.230], mean action: 5.395 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  318/5000: episode: 11, duration: 0.325s, episode steps:  53, steps per second: 163, episode reward: -37.050, mean reward: -0.699 [-32.314,  2.270], mean action: 10.113 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  336/5000: episode: 12, duration: 0.129s, episode steps:  18, steps per second: 140, episode reward: 41.129, mean reward:  2.285 [-2.328, 32.400], mean action: 4.500 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  355/5000: episode: 13, duration: 0.133s, episode steps:  19, steps per second: 143, episode reward: 41.314, mean reward:  2.174 [-3.000, 31.513], mean action: 2.789 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  376/5000: episode: 14, duration: 0.139s, episode steps:  21, steps per second: 151, episode reward: 38.133, mean reward:  1.816 [-2.904, 32.401], mean action: 6.286 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  403/5000: episode: 15, duration: 0.181s, episode steps:  27, steps per second: 149, episode reward: 41.433, mean reward:  1.535 [-2.296, 31.895], mean action: 3.185 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  443/5000: episode: 16, duration: 0.244s, episode steps:  40, steps per second: 164, episode reward: 44.817, mean reward:  1.120 [-2.134, 32.260], mean action: 2.675 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  473/5000: episode: 17, duration: 0.200s, episode steps:  30, steps per second: 150, episode reward: 47.192, mean reward:  1.573 [-0.355, 32.190], mean action: 2.267 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  502/5000: episode: 18, duration: 0.183s, episode steps:  29, steps per second: 159, episode reward: 38.421, mean reward:  1.325 [-3.000, 32.430], mean action: 4.897 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  522/5000: episode: 19, duration: 0.132s, episode steps:  20, steps per second: 151, episode reward: 43.620, mean reward:  2.181 [-2.263, 31.934], mean action: 4.150 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  559/5000: episode: 20, duration: 0.222s, episode steps:  37, steps per second: 167, episode reward: 32.108, mean reward:  0.868 [-2.723, 31.972], mean action: 4.838 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  593/5000: episode: 21, duration: 0.198s, episode steps:  34, steps per second: 171, episode reward: 41.790, mean reward:  1.229 [-2.366, 32.130], mean action: 8.000 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  637/5000: episode: 22, duration: 0.254s, episode steps:  44, steps per second: 173, episode reward: 33.000, mean reward:  0.750 [-3.000, 33.000], mean action: 3.727 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  671/5000: episode: 23, duration: 0.284s, episode steps:  34, steps per second: 120, episode reward: 36.000, mean reward:  1.059 [-2.831, 32.300], mean action: 5.059 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  688/5000: episode: 24, duration: 0.116s, episode steps:  17, steps per second: 146, episode reward: 41.900, mean reward:  2.465 [-2.674, 32.900], mean action: 4.765 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  726/5000: episode: 25, duration: 0.242s, episode steps:  38, steps per second: 157, episode reward: 40.542, mean reward:  1.067 [-2.300, 32.050], mean action: 6.105 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  754/5000: episode: 26, duration: 0.181s, episode steps:  28, steps per second: 155, episode reward: 36.000, mean reward:  1.286 [-2.880, 32.430], mean action: 6.107 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  772/5000: episode: 27, duration: 0.227s, episode steps:  18, steps per second:  79, episode reward: 41.337, mean reward:  2.297 [-2.552, 32.440], mean action: 3.889 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  788/5000: episode: 28, duration: 0.125s, episode steps:  16, steps per second: 128, episode reward: 44.307, mean reward:  2.769 [-2.475, 32.630], mean action: 3.000 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  806/5000: episode: 29, duration: 0.125s, episode steps:  18, steps per second: 144, episode reward: 41.075, mean reward:  2.282 [-3.000, 32.053], mean action: 5.833 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  825/5000: episode: 30, duration: 0.142s, episode steps:  19, steps per second: 134, episode reward: 44.546, mean reward:  2.345 [-1.978, 32.060], mean action: 3.421 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  843/5000: episode: 31, duration: 0.125s, episode steps:  18, steps per second: 144, episode reward: 41.383, mean reward:  2.299 [-2.350, 32.030], mean action: 5.722 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  866/5000: episode: 32, duration: 0.150s, episode steps:  23, steps per second: 154, episode reward: 41.287, mean reward:  1.795 [-2.903, 32.810], mean action: 5.478 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  900/5000: episode: 33, duration: 0.216s, episode steps:  34, steps per second: 158, episode reward: -34.620, mean reward: -1.018 [-32.374,  2.440], mean action: 3.294 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  924/5000: episode: 34, duration: 0.148s, episode steps:  24, steps per second: 162, episode reward: 40.960, mean reward:  1.707 [-3.000, 32.430], mean action: 5.792 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  950/5000: episode: 35, duration: 0.168s, episode steps:  26, steps per second: 155, episode reward: 44.523, mean reward:  1.712 [-2.046, 32.370], mean action: 3.923 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  993/5000: episode: 36, duration: 0.491s, episode steps:  43, steps per second:  88, episode reward: 41.028, mean reward:  0.954 [-2.806, 32.850], mean action: 4.233 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1017/5000: episode: 37, duration: 0.273s, episode steps:  24, steps per second:  88, episode reward: 43.846, mean reward:  1.827 [-2.450, 32.140], mean action: 4.792 [0.000, 19.000],  loss: 0.021533, mae: 0.441693, mean_q: 0.482772, mean_eps: 0.000000
 1036/5000: episode: 38, duration: 0.237s, episode steps:  19, steps per second:  80, episode reward: 44.667, mean reward:  2.351 [-2.452, 32.100], mean action: 1.895 [0.000, 11.000],  loss: 0.019100, mae: 0.439087, mean_q: 0.449478, mean_eps: 0.000000
 1066/5000: episode: 39, duration: 0.359s, episode steps:  30, steps per second:  84, episode reward: 38.228, mean reward:  1.274 [-2.463, 32.190], mean action: 5.133 [0.000, 20.000],  loss: 0.018996, mae: 0.431410, mean_q: 0.463792, mean_eps: 0.000000
 1088/5000: episode: 40, duration: 0.263s, episode steps:  22, steps per second:  84, episode reward: 44.304, mean reward:  2.014 [-2.308, 31.998], mean action: 3.045 [0.000, 14.000],  loss: 0.018161, mae: 0.423552, mean_q: 0.480497, mean_eps: 0.000000
 1124/5000: episode: 41, duration: 0.432s, episode steps:  36, steps per second:  83, episode reward: 38.264, mean reward:  1.063 [-2.304, 32.191], mean action: 3.111 [0.000, 15.000],  loss: 0.020444, mae: 0.431699, mean_q: 0.444657, mean_eps: 0.000000
 1164/5000: episode: 42, duration: 0.475s, episode steps:  40, steps per second:  84, episode reward: 37.493, mean reward:  0.937 [-3.000, 32.211], mean action: 4.950 [0.000, 20.000],  loss: 0.019752, mae: 0.422195, mean_q: 0.520826, mean_eps: 0.000000
 1187/5000: episode: 43, duration: 0.276s, episode steps:  23, steps per second:  83, episode reward: 41.556, mean reward:  1.807 [-2.298, 31.807], mean action: 3.348 [0.000, 20.000],  loss: 0.017452, mae: 0.417383, mean_q: 0.463798, mean_eps: 0.000000
 1226/5000: episode: 44, duration: 0.496s, episode steps:  39, steps per second:  79, episode reward: 38.002, mean reward:  0.974 [-2.685, 31.803], mean action: 3.872 [0.000, 14.000],  loss: 0.018657, mae: 0.417276, mean_q: 0.494569, mean_eps: 0.000000
 1245/5000: episode: 45, duration: 0.236s, episode steps:  19, steps per second:  80, episode reward: 43.565, mean reward:  2.293 [-2.592, 32.470], mean action: 3.842 [0.000, 20.000],  loss: 0.019203, mae: 0.426225, mean_q: 0.472203, mean_eps: 0.000000
 1268/5000: episode: 46, duration: 0.278s, episode steps:  23, steps per second:  83, episode reward: 39.000, mean reward:  1.696 [-2.817, 32.040], mean action: 4.304 [1.000, 13.000],  loss: 0.017752, mae: 0.420613, mean_q: 0.458408, mean_eps: 0.000000
 1322/5000: episode: 47, duration: 0.803s, episode steps:  54, steps per second:  67, episode reward: 32.484, mean reward:  0.602 [-3.000, 31.684], mean action: 6.796 [0.000, 15.000],  loss: 0.017941, mae: 0.419300, mean_q: 0.492361, mean_eps: 0.000000
 1350/5000: episode: 48, duration: 0.352s, episode steps:  28, steps per second:  79, episode reward: 44.234, mean reward:  1.580 [-2.083, 33.000], mean action: 1.893 [0.000, 12.000],  loss: 0.020992, mae: 0.425804, mean_q: 0.487041, mean_eps: 0.000000
 1389/5000: episode: 49, duration: 0.473s, episode steps:  39, steps per second:  83, episode reward: 37.948, mean reward:  0.973 [-2.694, 32.140], mean action: 4.282 [0.000, 15.000],  loss: 0.019601, mae: 0.425892, mean_q: 0.462444, mean_eps: 0.000000
 1409/5000: episode: 50, duration: 0.484s, episode steps:  20, steps per second:  41, episode reward: 42.000, mean reward:  2.100 [-2.285, 32.170], mean action: 1.700 [0.000, 12.000],  loss: 0.021689, mae: 0.436166, mean_q: 0.438247, mean_eps: 0.000000
 1429/5000: episode: 51, duration: 0.720s, episode steps:  20, steps per second:  28, episode reward: 41.704, mean reward:  2.085 [-2.027, 32.300], mean action: 3.100 [0.000, 15.000],  loss: 0.021081, mae: 0.424654, mean_q: 0.491949, mean_eps: 0.000000
 1451/5000: episode: 52, duration: 0.278s, episode steps:  22, steps per second:  79, episode reward: 41.012, mean reward:  1.864 [-2.107, 32.081], mean action: 2.045 [0.000, 12.000],  loss: 0.013841, mae: 0.392514, mean_q: 0.502389, mean_eps: 0.000000
 1489/5000: episode: 53, duration: 0.468s, episode steps:  38, steps per second:  81, episode reward: -32.840, mean reward: -0.864 [-32.213,  2.641], mean action: 6.605 [0.000, 14.000],  loss: 0.021444, mae: 0.431607, mean_q: 0.503723, mean_eps: 0.000000
 1509/5000: episode: 54, duration: 0.401s, episode steps:  20, steps per second:  50, episode reward: 38.172, mean reward:  1.909 [-3.000, 32.310], mean action: 4.300 [1.000, 13.000],  loss: 0.019363, mae: 0.427218, mean_q: 0.500184, mean_eps: 0.000000
 1533/5000: episode: 55, duration: 0.350s, episode steps:  24, steps per second:  69, episode reward: 38.062, mean reward:  1.586 [-3.000, 31.758], mean action: 4.167 [0.000, 14.000],  loss: 0.017830, mae: 0.418417, mean_q: 0.473215, mean_eps: 0.000000
 1566/5000: episode: 56, duration: 0.406s, episode steps:  33, steps per second:  81, episode reward: 41.827, mean reward:  1.267 [-2.901, 32.170], mean action: 5.485 [0.000, 19.000],  loss: 0.019532, mae: 0.441315, mean_q: 0.444318, mean_eps: 0.000000
 1585/5000: episode: 57, duration: 0.240s, episode steps:  19, steps per second:  79, episode reward: 41.690, mean reward:  2.194 [-2.084, 32.500], mean action: 2.842 [0.000, 12.000],  loss: 0.018118, mae: 0.423234, mean_q: 0.492415, mean_eps: 0.000000
 1606/5000: episode: 58, duration: 0.261s, episode steps:  21, steps per second:  81, episode reward: 44.065, mean reward:  2.098 [-2.005, 32.030], mean action: 1.762 [0.000, 12.000],  loss: 0.019699, mae: 0.423324, mean_q: 0.504894, mean_eps: 0.000000
 1633/5000: episode: 59, duration: 0.497s, episode steps:  27, steps per second:  54, episode reward: 38.938, mean reward:  1.442 [-3.000, 32.300], mean action: 4.481 [0.000, 20.000],  loss: 0.017437, mae: 0.413093, mean_q: 0.491647, mean_eps: 0.000000
 1645/5000: episode: 60, duration: 0.196s, episode steps:  12, steps per second:  61, episode reward: 47.201, mean reward:  3.933 [ 0.000, 32.480], mean action: 2.000 [0.000, 20.000],  loss: 0.020875, mae: 0.435529, mean_q: 0.501963, mean_eps: 0.000000
 1685/5000: episode: 61, duration: 0.506s, episode steps:  40, steps per second:  79, episode reward: 35.725, mean reward:  0.893 [-2.158, 32.615], mean action: 5.025 [0.000, 20.000],  loss: 0.019019, mae: 0.421220, mean_q: 0.501832, mean_eps: 0.000000
 1722/5000: episode: 62, duration: 0.658s, episode steps:  37, steps per second:  56, episode reward: 36.000, mean reward:  0.973 [-3.000, 29.380], mean action: 3.081 [0.000, 19.000],  loss: 0.021138, mae: 0.437907, mean_q: 0.491430, mean_eps: 0.000000
 1747/5000: episode: 63, duration: 0.303s, episode steps:  25, steps per second:  82, episode reward: 44.601, mean reward:  1.784 [-2.234, 32.060], mean action: 2.720 [0.000, 15.000],  loss: 0.014995, mae: 0.402370, mean_q: 0.520896, mean_eps: 0.000000
 1769/5000: episode: 64, duration: 0.267s, episode steps:  22, steps per second:  82, episode reward: 44.145, mean reward:  2.007 [-2.324, 32.390], mean action: 3.273 [0.000, 19.000],  loss: 0.019100, mae: 0.432205, mean_q: 0.461602, mean_eps: 0.000000
 1793/5000: episode: 65, duration: 0.297s, episode steps:  24, steps per second:  81, episode reward: 40.918, mean reward:  1.705 [-3.000, 32.484], mean action: 7.333 [0.000, 20.000],  loss: 0.022674, mae: 0.444943, mean_q: 0.433797, mean_eps: 0.000000
 1823/5000: episode: 66, duration: 0.369s, episode steps:  30, steps per second:  81, episode reward: 43.984, mean reward:  1.466 [-2.223, 32.250], mean action: 7.067 [0.000, 19.000],  loss: 0.021565, mae: 0.436943, mean_q: 0.396132, mean_eps: 0.000000
 1851/5000: episode: 67, duration: 0.335s, episode steps:  28, steps per second:  83, episode reward: 44.178, mean reward:  1.578 [-2.131, 32.090], mean action: 4.571 [0.000, 19.000],  loss: 0.020769, mae: 0.426473, mean_q: 0.449274, mean_eps: 0.000000
 1868/5000: episode: 68, duration: 0.212s, episode steps:  17, steps per second:  80, episode reward: 43.644, mean reward:  2.567 [-3.000, 32.160], mean action: 4.765 [0.000, 19.000],  loss: 0.021221, mae: 0.423730, mean_q: 0.477561, mean_eps: 0.000000
 1885/5000: episode: 69, duration: 0.211s, episode steps:  17, steps per second:  80, episode reward: 40.757, mean reward:  2.397 [-2.239, 32.193], mean action: 5.765 [1.000, 19.000],  loss: 0.022310, mae: 0.424954, mean_q: 0.446343, mean_eps: 0.000000
 1933/5000: episode: 70, duration: 0.546s, episode steps:  48, steps per second:  88, episode reward: 35.667, mean reward:  0.743 [-2.871, 32.480], mean action: 6.083 [0.000, 20.000],  loss: 0.023252, mae: 0.438239, mean_q: 0.444337, mean_eps: 0.000000
 1956/5000: episode: 71, duration: 0.284s, episode steps:  23, steps per second:  81, episode reward: 37.540, mean reward:  1.632 [-2.631, 32.120], mean action: 5.391 [0.000, 19.000],  loss: 0.019898, mae: 0.420700, mean_q: 0.488930, mean_eps: 0.000000
 1974/5000: episode: 72, duration: 0.222s, episode steps:  18, steps per second:  81, episode reward: 44.741, mean reward:  2.486 [-2.137, 32.111], mean action: 4.500 [3.000, 19.000],  loss: 0.019858, mae: 0.420929, mean_q: 0.473441, mean_eps: 0.000000
 1994/5000: episode: 73, duration: 0.244s, episode steps:  20, steps per second:  82, episode reward: 38.120, mean reward:  1.906 [-3.000, 31.719], mean action: 4.000 [0.000, 14.000],  loss: 0.019327, mae: 0.420197, mean_q: 0.476311, mean_eps: 0.000000
 2014/5000: episode: 74, duration: 0.237s, episode steps:  20, steps per second:  84, episode reward: 38.600, mean reward:  1.930 [-3.000, 32.330], mean action: 2.950 [0.000, 11.000],  loss: 0.020790, mae: 0.430352, mean_q: 0.442587, mean_eps: 0.000000
 2029/5000: episode: 75, duration: 0.188s, episode steps:  15, steps per second:  80, episode reward: 44.704, mean reward:  2.980 [-2.197, 32.310], mean action: 3.133 [0.000, 19.000],  loss: 0.020837, mae: 0.422154, mean_q: 0.454974, mean_eps: 0.000000
 2051/5000: episode: 76, duration: 0.266s, episode steps:  22, steps per second:  83, episode reward: 44.632, mean reward:  2.029 [-2.172, 32.257], mean action: 2.727 [0.000, 9.000],  loss: 0.018761, mae: 0.413135, mean_q: 0.518096, mean_eps: 0.000000
 2070/5000: episode: 77, duration: 0.236s, episode steps:  19, steps per second:  81, episode reward: 41.580, mean reward:  2.188 [-2.715, 32.480], mean action: 4.053 [0.000, 14.000],  loss: 0.019101, mae: 0.419504, mean_q: 0.516726, mean_eps: 0.000000
 2102/5000: episode: 78, duration: 0.379s, episode steps:  32, steps per second:  84, episode reward: 47.207, mean reward:  1.475 [-0.192, 32.190], mean action: 3.656 [0.000, 14.000],  loss: 0.019332, mae: 0.424129, mean_q: 0.520696, mean_eps: 0.000000
 2148/5000: episode: 79, duration: 0.550s, episode steps:  46, steps per second:  84, episode reward: 35.388, mean reward:  0.769 [-2.310, 31.939], mean action: 2.304 [0.000, 19.000],  loss: 0.020787, mae: 0.435878, mean_q: 0.456262, mean_eps: 0.000000
 2197/5000: episode: 80, duration: 0.570s, episode steps:  49, steps per second:  86, episode reward: -32.960, mean reward: -0.673 [-32.148,  2.564], mean action: 10.755 [0.000, 21.000],  loss: 0.019861, mae: 0.421011, mean_q: 0.437766, mean_eps: 0.000000
 2226/5000: episode: 81, duration: 0.339s, episode steps:  29, steps per second:  85, episode reward: 42.000, mean reward:  1.448 [-2.696, 32.150], mean action: 2.793 [0.000, 15.000],  loss: 0.021350, mae: 0.427190, mean_q: 0.489969, mean_eps: 0.000000
 2247/5000: episode: 82, duration: 0.256s, episode steps:  21, steps per second:  82, episode reward: 40.990, mean reward:  1.952 [-3.000, 32.321], mean action: 5.333 [0.000, 20.000],  loss: 0.020354, mae: 0.413842, mean_q: 0.485381, mean_eps: 0.000000
 2272/5000: episode: 83, duration: 0.298s, episode steps:  25, steps per second:  84, episode reward: 40.197, mean reward:  1.608 [-2.902, 32.242], mean action: 4.640 [0.000, 19.000],  loss: 0.022375, mae: 0.430680, mean_q: 0.476195, mean_eps: 0.000000
 2289/5000: episode: 84, duration: 0.214s, episode steps:  17, steps per second:  80, episode reward: 41.347, mean reward:  2.432 [-3.000, 32.141], mean action: 3.882 [0.000, 15.000],  loss: 0.024277, mae: 0.445504, mean_q: 0.497768, mean_eps: 0.000000
 2331/5000: episode: 85, duration: 0.538s, episode steps:  42, steps per second:  78, episode reward: 35.215, mean reward:  0.838 [-2.436, 31.688], mean action: 3.833 [0.000, 21.000],  loss: 0.020957, mae: 0.429073, mean_q: 0.494097, mean_eps: 0.000000
 2363/5000: episode: 86, duration: 0.390s, episode steps:  32, steps per second:  82, episode reward: 40.969, mean reward:  1.280 [-2.396, 32.330], mean action: 3.094 [0.000, 20.000],  loss: 0.016747, mae: 0.412600, mean_q: 0.433958, mean_eps: 0.000000
 2382/5000: episode: 87, duration: 0.238s, episode steps:  19, steps per second:  80, episode reward: 41.122, mean reward:  2.164 [-2.505, 31.798], mean action: 6.105 [0.000, 19.000],  loss: 0.026162, mae: 0.448847, mean_q: 0.482177, mean_eps: 0.000000
 2398/5000: episode: 88, duration: 0.202s, episode steps:  16, steps per second:  79, episode reward: 44.669, mean reward:  2.792 [-2.743, 32.770], mean action: 3.250 [1.000, 14.000],  loss: 0.018758, mae: 0.418341, mean_q: 0.466954, mean_eps: 0.000000
 2433/5000: episode: 89, duration: 0.415s, episode steps:  35, steps per second:  84, episode reward: 34.402, mean reward:  0.983 [-3.000, 32.560], mean action: 6.257 [0.000, 20.000],  loss: 0.019557, mae: 0.416900, mean_q: 0.525131, mean_eps: 0.000000
 2450/5000: episode: 90, duration: 0.390s, episode steps:  17, steps per second:  44, episode reward: 47.447, mean reward:  2.791 [-0.400, 32.470], mean action: 2.059 [0.000, 7.000],  loss: 0.017408, mae: 0.413662, mean_q: 0.476296, mean_eps: 0.000000
 2514/5000: episode: 91, duration: 0.865s, episode steps:  64, steps per second:  74, episode reward: -32.010, mean reward: -0.500 [-31.927,  2.572], mean action: 6.844 [0.000, 20.000],  loss: 0.021694, mae: 0.431796, mean_q: 0.473372, mean_eps: 0.000000
 2557/5000: episode: 92, duration: 0.541s, episode steps:  43, steps per second:  80, episode reward: 34.493, mean reward:  0.802 [-2.476, 31.959], mean action: 6.744 [0.000, 15.000],  loss: 0.018339, mae: 0.414752, mean_q: 0.481425, mean_eps: 0.000000
 2587/5000: episode: 93, duration: 0.375s, episode steps:  30, steps per second:  80, episode reward: 35.223, mean reward:  1.174 [-3.000, 32.240], mean action: 6.300 [0.000, 19.000],  loss: 0.022297, mae: 0.437502, mean_q: 0.456867, mean_eps: 0.000000
 2613/5000: episode: 94, duration: 0.314s, episode steps:  26, steps per second:  83, episode reward: 40.968, mean reward:  1.576 [-2.755, 32.070], mean action: 4.000 [0.000, 14.000],  loss: 0.018475, mae: 0.416662, mean_q: 0.439727, mean_eps: 0.000000
 2646/5000: episode: 95, duration: 0.394s, episode steps:  33, steps per second:  84, episode reward: 44.116, mean reward:  1.337 [-2.135, 32.433], mean action: 4.061 [1.000, 14.000],  loss: 0.018444, mae: 0.412998, mean_q: 0.473254, mean_eps: 0.000000
 2700/5000: episode: 96, duration: 0.609s, episode steps:  54, steps per second:  89, episode reward: 35.358, mean reward:  0.655 [-2.204, 32.043], mean action: 7.093 [0.000, 19.000],  loss: 0.021253, mae: 0.423863, mean_q: 0.482079, mean_eps: 0.000000
 2731/5000: episode: 97, duration: 0.363s, episode steps:  31, steps per second:  86, episode reward: 38.468, mean reward:  1.241 [-2.459, 31.784], mean action: 4.290 [0.000, 19.000],  loss: 0.021275, mae: 0.415793, mean_q: 0.511320, mean_eps: 0.000000
 2742/5000: episode: 98, duration: 0.151s, episode steps:  11, steps per second:  73, episode reward: 43.801, mean reward:  3.982 [-2.463, 32.779], mean action: 3.818 [0.000, 14.000],  loss: 0.011587, mae: 0.378840, mean_q: 0.484235, mean_eps: 0.000000
 2771/5000: episode: 99, duration: 0.337s, episode steps:  29, steps per second:  86, episode reward: 33.000, mean reward:  1.138 [-2.832, 29.901], mean action: 8.034 [0.000, 14.000],  loss: 0.022074, mae: 0.428647, mean_q: 0.497080, mean_eps: 0.000000
 2795/5000: episode: 100, duration: 0.291s, episode steps:  24, steps per second:  82, episode reward: 38.463, mean reward:  1.603 [-2.938, 32.220], mean action: 6.583 [0.000, 19.000],  loss: 0.020687, mae: 0.423144, mean_q: 0.454856, mean_eps: 0.000000
 2822/5000: episode: 101, duration: 0.326s, episode steps:  27, steps per second:  83, episode reward: 39.000, mean reward:  1.444 [-3.000, 30.131], mean action: 3.926 [0.000, 14.000],  loss: 0.018910, mae: 0.421172, mean_q: 0.432681, mean_eps: 0.000000
 2843/5000: episode: 102, duration: 0.258s, episode steps:  21, steps per second:  82, episode reward: 46.621, mean reward:  2.220 [-0.382, 31.934], mean action: 1.905 [0.000, 14.000],  loss: 0.021543, mae: 0.437612, mean_q: 0.403501, mean_eps: 0.000000
 2864/5000: episode: 103, duration: 0.259s, episode steps:  21, steps per second:  81, episode reward: 44.718, mean reward:  2.129 [-2.107, 32.040], mean action: 2.905 [0.000, 12.000],  loss: 0.017179, mae: 0.410558, mean_q: 0.447403, mean_eps: 0.000000
 2892/5000: episode: 104, duration: 0.336s, episode steps:  28, steps per second:  83, episode reward: 34.500, mean reward:  1.232 [-3.000, 33.000], mean action: 5.607 [0.000, 15.000],  loss: 0.018783, mae: 0.410669, mean_q: 0.454911, mean_eps: 0.000000
 2923/5000: episode: 105, duration: 0.371s, episode steps:  31, steps per second:  84, episode reward: 43.763, mean reward:  1.412 [-2.253, 32.010], mean action: 3.645 [0.000, 14.000],  loss: 0.019035, mae: 0.417957, mean_q: 0.480664, mean_eps: 0.000000
 2957/5000: episode: 106, duration: 0.400s, episode steps:  34, steps per second:  85, episode reward: 43.794, mean reward:  1.288 [-2.065, 32.200], mean action: 3.382 [0.000, 14.000],  loss: 0.019509, mae: 0.416356, mean_q: 0.444829, mean_eps: 0.000000
 2978/5000: episode: 107, duration: 0.255s, episode steps:  21, steps per second:  82, episode reward: 38.198, mean reward:  1.819 [-2.640, 32.180], mean action: 6.095 [0.000, 14.000],  loss: 0.020499, mae: 0.411559, mean_q: 0.473627, mean_eps: 0.000000
 2995/5000: episode: 108, duration: 0.212s, episode steps:  17, steps per second:  80, episode reward: 45.265, mean reward:  2.663 [-0.362, 31.697], mean action: 4.529 [0.000, 14.000],  loss: 0.018484, mae: 0.409869, mean_q: 0.437210, mean_eps: 0.000000
 3015/5000: episode: 109, duration: 0.254s, episode steps:  20, steps per second:  79, episode reward: 43.141, mean reward:  2.157 [-3.000, 31.359], mean action: 5.850 [2.000, 19.000],  loss: 0.020972, mae: 0.410752, mean_q: 0.506345, mean_eps: 0.000000
 3053/5000: episode: 110, duration: 0.455s, episode steps:  38, steps per second:  83, episode reward: 39.000, mean reward:  1.026 [-3.000, 32.100], mean action: 4.684 [0.000, 20.000],  loss: 0.018160, mae: 0.403880, mean_q: 0.532190, mean_eps: 0.000000
 3080/5000: episode: 111, duration: 0.322s, episode steps:  27, steps per second:  84, episode reward: 39.000, mean reward:  1.444 [-3.000, 32.390], mean action: 2.444 [0.000, 14.000],  loss: 0.021828, mae: 0.431884, mean_q: 0.464492, mean_eps: 0.000000
 3106/5000: episode: 112, duration: 0.310s, episode steps:  26, steps per second:  84, episode reward: 41.255, mean reward:  1.587 [-2.615, 32.010], mean action: 6.077 [0.000, 19.000],  loss: 0.022277, mae: 0.442283, mean_q: 0.454042, mean_eps: 0.000000
 3129/5000: episode: 113, duration: 0.282s, episode steps:  23, steps per second:  82, episode reward: 42.000, mean reward:  1.826 [-2.170, 32.060], mean action: 2.913 [0.000, 15.000],  loss: 0.018634, mae: 0.427875, mean_q: 0.510182, mean_eps: 0.000000
 3156/5000: episode: 114, duration: 0.324s, episode steps:  27, steps per second:  83, episode reward: 40.463, mean reward:  1.499 [-2.964, 32.339], mean action: 5.593 [3.000, 14.000],  loss: 0.019159, mae: 0.422582, mean_q: 0.453665, mean_eps: 0.000000
 3186/5000: episode: 115, duration: 0.359s, episode steps:  30, steps per second:  83, episode reward: 44.301, mean reward:  1.477 [-2.100, 32.020], mean action: 2.933 [0.000, 15.000],  loss: 0.017064, mae: 0.406699, mean_q: 0.465268, mean_eps: 0.000000
 3213/5000: episode: 116, duration: 0.467s, episode steps:  27, steps per second:  58, episode reward: 40.482, mean reward:  1.499 [-3.000, 32.230], mean action: 2.889 [0.000, 14.000],  loss: 0.020404, mae: 0.419745, mean_q: 0.478723, mean_eps: 0.000000
 3252/5000: episode: 117, duration: 0.818s, episode steps:  39, steps per second:  48, episode reward: 32.607, mean reward:  0.836 [-3.000, 32.018], mean action: 6.692 [0.000, 20.000],  loss: 0.019345, mae: 0.418926, mean_q: 0.462645, mean_eps: 0.000000
 3302/5000: episode: 118, duration: 0.740s, episode steps:  50, steps per second:  68, episode reward: 32.576, mean reward:  0.652 [-3.000, 32.090], mean action: 4.880 [0.000, 19.000],  loss: 0.018379, mae: 0.409413, mean_q: 0.500476, mean_eps: 0.000000
 3323/5000: episode: 119, duration: 0.266s, episode steps:  21, steps per second:  79, episode reward: 41.237, mean reward:  1.964 [-2.819, 32.148], mean action: 6.048 [0.000, 15.000],  loss: 0.021471, mae: 0.419553, mean_q: 0.462506, mean_eps: 0.000000
 3350/5000: episode: 120, duration: 0.406s, episode steps:  27, steps per second:  66, episode reward: 43.623, mean reward:  1.616 [-2.242, 31.704], mean action: 6.593 [0.000, 20.000],  loss: 0.019526, mae: 0.417499, mean_q: 0.469681, mean_eps: 0.000000
 3371/5000: episode: 121, duration: 0.272s, episode steps:  21, steps per second:  77, episode reward: 41.237, mean reward:  1.964 [-2.513, 32.053], mean action: 2.905 [0.000, 9.000],  loss: 0.016196, mae: 0.405414, mean_q: 0.482787, mean_eps: 0.000000
 3393/5000: episode: 122, duration: 0.271s, episode steps:  22, steps per second:  81, episode reward: 38.951, mean reward:  1.771 [-2.704, 32.200], mean action: 3.500 [0.000, 15.000],  loss: 0.022075, mae: 0.433214, mean_q: 0.484799, mean_eps: 0.000000
 3422/5000: episode: 123, duration: 0.357s, episode steps:  29, steps per second:  81, episode reward: 41.407, mean reward:  1.428 [-3.000, 32.350], mean action: 3.103 [0.000, 13.000],  loss: 0.021695, mae: 0.432032, mean_q: 0.441353, mean_eps: 0.000000
 3485/5000: episode: 124, duration: 0.724s, episode steps:  63, steps per second:  87, episode reward: -32.870, mean reward: -0.522 [-32.301,  2.300], mean action: 3.905 [0.000, 20.000],  loss: 0.020060, mae: 0.418575, mean_q: 0.492301, mean_eps: 0.000000
 3513/5000: episode: 125, duration: 0.354s, episode steps:  28, steps per second:  79, episode reward: 43.874, mean reward:  1.567 [-2.104, 32.450], mean action: 4.286 [0.000, 14.000],  loss: 0.019489, mae: 0.425974, mean_q: 0.438356, mean_eps: 0.000000
 3539/5000: episode: 126, duration: 0.334s, episode steps:  26, steps per second:  78, episode reward: 38.271, mean reward:  1.472 [-2.477, 32.010], mean action: 8.154 [0.000, 19.000],  loss: 0.016398, mae: 0.416810, mean_q: 0.379486, mean_eps: 0.000000
 3576/5000: episode: 127, duration: 0.437s, episode steps:  37, steps per second:  85, episode reward: 37.792, mean reward:  1.021 [-2.877, 32.414], mean action: 5.703 [0.000, 20.000],  loss: 0.019072, mae: 0.419045, mean_q: 0.381987, mean_eps: 0.000000
 3596/5000: episode: 128, duration: 0.272s, episode steps:  20, steps per second:  74, episode reward: 32.769, mean reward:  1.638 [-3.000, 32.249], mean action: 3.550 [0.000, 19.000],  loss: 0.014065, mae: 0.387844, mean_q: 0.415692, mean_eps: 0.000000
 3635/5000: episode: 129, duration: 0.513s, episode steps:  39, steps per second:  76, episode reward: 40.105, mean reward:  1.028 [-2.373, 32.100], mean action: 4.051 [0.000, 19.000],  loss: 0.018628, mae: 0.404805, mean_q: 0.506876, mean_eps: 0.000000
 3674/5000: episode: 130, duration: 0.484s, episode steps:  39, steps per second:  81, episode reward: 35.750, mean reward:  0.917 [-3.000, 32.460], mean action: 5.256 [0.000, 20.000],  loss: 0.023422, mae: 0.433032, mean_q: 0.479151, mean_eps: 0.000000
 3704/5000: episode: 131, duration: 0.358s, episode steps:  30, steps per second:  84, episode reward: 38.548, mean reward:  1.285 [-2.637, 32.610], mean action: 8.200 [0.000, 15.000],  loss: 0.019052, mae: 0.413806, mean_q: 0.475194, mean_eps: 0.000000
 3736/5000: episode: 132, duration: 0.378s, episode steps:  32, steps per second:  85, episode reward: 41.481, mean reward:  1.296 [-2.631, 31.981], mean action: 3.875 [0.000, 19.000],  loss: 0.020107, mae: 0.417157, mean_q: 0.511225, mean_eps: 0.000000
 3754/5000: episode: 133, duration: 0.226s, episode steps:  18, steps per second:  80, episode reward: 44.996, mean reward:  2.500 [-2.252, 32.400], mean action: 3.167 [0.000, 14.000],  loss: 0.016673, mae: 0.408258, mean_q: 0.426466, mean_eps: 0.000000
 3776/5000: episode: 134, duration: 0.273s, episode steps:  22, steps per second:  80, episode reward: 41.706, mean reward:  1.896 [-2.532, 32.090], mean action: 3.182 [0.000, 15.000],  loss: 0.018758, mae: 0.420243, mean_q: 0.463631, mean_eps: 0.000000
 3795/5000: episode: 135, duration: 0.253s, episode steps:  19, steps per second:  75, episode reward: 43.583, mean reward:  2.294 [-2.620, 32.520], mean action: 4.526 [0.000, 19.000],  loss: 0.021574, mae: 0.433527, mean_q: 0.459613, mean_eps: 0.000000
 3814/5000: episode: 136, duration: 0.252s, episode steps:  19, steps per second:  75, episode reward: 41.129, mean reward:  2.165 [-2.205, 31.588], mean action: 4.895 [0.000, 19.000],  loss: 0.018030, mae: 0.421881, mean_q: 0.416997, mean_eps: 0.000000
 3833/5000: episode: 137, duration: 0.235s, episode steps:  19, steps per second:  81, episode reward: 44.176, mean reward:  2.325 [-2.464, 32.200], mean action: 4.947 [1.000, 19.000],  loss: 0.018800, mae: 0.424967, mean_q: 0.413843, mean_eps: 0.000000
 3856/5000: episode: 138, duration: 0.290s, episode steps:  23, steps per second:  79, episode reward: 40.473, mean reward:  1.760 [-2.480, 32.500], mean action: 3.000 [0.000, 19.000],  loss: 0.021489, mae: 0.434640, mean_q: 0.442780, mean_eps: 0.000000
 3881/5000: episode: 139, duration: 0.301s, episode steps:  25, steps per second:  83, episode reward: 35.486, mean reward:  1.419 [-2.868, 31.974], mean action: 4.560 [0.000, 19.000],  loss: 0.020215, mae: 0.424912, mean_q: 0.482922, mean_eps: 0.000000
 3909/5000: episode: 140, duration: 0.339s, episode steps:  28, steps per second:  83, episode reward: 43.167, mean reward:  1.542 [-2.183, 32.230], mean action: 3.357 [1.000, 19.000],  loss: 0.017773, mae: 0.419674, mean_q: 0.463852, mean_eps: 0.000000
 3931/5000: episode: 141, duration: 0.267s, episode steps:  22, steps per second:  82, episode reward: 39.000, mean reward:  1.773 [-2.351, 32.290], mean action: 2.136 [0.000, 12.000],  loss: 0.018317, mae: 0.419950, mean_q: 0.496300, mean_eps: 0.000000
 3970/5000: episode: 142, duration: 0.461s, episode steps:  39, steps per second:  85, episode reward: 41.636, mean reward:  1.068 [-2.248, 32.600], mean action: 3.308 [0.000, 14.000],  loss: 0.020897, mae: 0.431424, mean_q: 0.434202, mean_eps: 0.000000
 4002/5000: episode: 143, duration: 0.388s, episode steps:  32, steps per second:  82, episode reward: 37.734, mean reward:  1.179 [-2.350, 33.877], mean action: 9.969 [0.000, 20.000],  loss: 0.018705, mae: 0.430741, mean_q: 0.421841, mean_eps: 0.000000
 4036/5000: episode: 144, duration: 0.406s, episode steps:  34, steps per second:  84, episode reward: 43.817, mean reward:  1.289 [-2.229, 32.085], mean action: 3.941 [0.000, 14.000],  loss: 0.022265, mae: 0.433460, mean_q: 0.477086, mean_eps: 0.000000
 4085/5000: episode: 145, duration: 0.580s, episode steps:  49, steps per second:  85, episode reward: -35.110, mean reward: -0.717 [-32.080,  2.230], mean action: 6.857 [0.000, 19.000],  loss: 0.019313, mae: 0.425627, mean_q: 0.428856, mean_eps: 0.000000
 4110/5000: episode: 146, duration: 0.306s, episode steps:  25, steps per second:  82, episode reward: 38.241, mean reward:  1.530 [-3.000, 31.552], mean action: 7.040 [0.000, 21.000],  loss: 0.021208, mae: 0.435547, mean_q: 0.458331, mean_eps: 0.000000
 4138/5000: episode: 147, duration: 0.337s, episode steps:  28, steps per second:  83, episode reward: 41.852, mean reward:  1.495 [-2.248, 32.350], mean action: 3.071 [0.000, 19.000],  loss: 0.023983, mae: 0.443560, mean_q: 0.454380, mean_eps: 0.000000
 4163/5000: episode: 148, duration: 0.301s, episode steps:  25, steps per second:  83, episode reward: 41.939, mean reward:  1.678 [-2.596, 30.473], mean action: 3.320 [0.000, 19.000],  loss: 0.020275, mae: 0.429055, mean_q: 0.438475, mean_eps: 0.000000
 4189/5000: episode: 149, duration: 0.314s, episode steps:  26, steps per second:  83, episode reward: 38.311, mean reward:  1.473 [-2.257, 31.891], mean action: 4.923 [0.000, 19.000],  loss: 0.020472, mae: 0.426439, mean_q: 0.483072, mean_eps: 0.000000
 4225/5000: episode: 150, duration: 0.448s, episode steps:  36, steps per second:  80, episode reward: 38.272, mean reward:  1.063 [-2.217, 32.002], mean action: 2.806 [0.000, 19.000],  loss: 0.023392, mae: 0.448525, mean_q: 0.400480, mean_eps: 0.000000
 4254/5000: episode: 151, duration: 0.359s, episode steps:  29, steps per second:  81, episode reward: 41.775, mean reward:  1.441 [-2.156, 32.070], mean action: 3.276 [1.000, 19.000],  loss: 0.017961, mae: 0.424047, mean_q: 0.435560, mean_eps: 0.000000
 4281/5000: episode: 152, duration: 0.326s, episode steps:  27, steps per second:  83, episode reward: 32.809, mean reward:  1.215 [-3.000, 32.424], mean action: 3.222 [0.000, 14.000],  loss: 0.020645, mae: 0.430060, mean_q: 0.494161, mean_eps: 0.000000
 4298/5000: episode: 153, duration: 0.216s, episode steps:  17, steps per second:  79, episode reward: 47.012, mean reward:  2.765 [-1.438, 32.740], mean action: 1.941 [1.000, 3.000],  loss: 0.024098, mae: 0.448955, mean_q: 0.439113, mean_eps: 0.000000
 4332/5000: episode: 154, duration: 0.410s, episode steps:  34, steps per second:  83, episode reward: 38.342, mean reward:  1.128 [-2.342, 32.166], mean action: 3.941 [0.000, 18.000],  loss: 0.019790, mae: 0.429637, mean_q: 0.446924, mean_eps: 0.000000
 4377/5000: episode: 155, duration: 0.533s, episode steps:  45, steps per second:  84, episode reward: 34.989, mean reward:  0.778 [-3.000, 31.970], mean action: 3.978 [0.000, 20.000],  loss: 0.018456, mae: 0.415563, mean_q: 0.490030, mean_eps: 0.000000
 4406/5000: episode: 156, duration: 0.368s, episode steps:  29, steps per second:  79, episode reward: 44.394, mean reward:  1.531 [-2.162, 32.090], mean action: 3.724 [3.000, 15.000],  loss: 0.017373, mae: 0.409665, mean_q: 0.493153, mean_eps: 0.000000
 4434/5000: episode: 157, duration: 0.342s, episode steps:  28, steps per second:  82, episode reward: 38.183, mean reward:  1.364 [-2.315, 32.250], mean action: 3.607 [0.000, 15.000],  loss: 0.018729, mae: 0.413958, mean_q: 0.520589, mean_eps: 0.000000
 4456/5000: episode: 158, duration: 0.274s, episode steps:  22, steps per second:  80, episode reward: 40.927, mean reward:  1.860 [-2.900, 32.026], mean action: 6.409 [0.000, 20.000],  loss: 0.018952, mae: 0.415035, mean_q: 0.515833, mean_eps: 0.000000
 4474/5000: episode: 159, duration: 0.227s, episode steps:  18, steps per second:  79, episode reward: 47.698, mean reward:  2.650 [-0.405, 32.680], mean action: 1.778 [1.000, 3.000],  loss: 0.023302, mae: 0.444757, mean_q: 0.499304, mean_eps: 0.000000
 4494/5000: episode: 160, duration: 0.254s, episode steps:  20, steps per second:  79, episode reward: 44.011, mean reward:  2.201 [-2.084, 32.065], mean action: 2.900 [1.000, 14.000],  loss: 0.021764, mae: 0.447736, mean_q: 0.444125, mean_eps: 0.000000
 4523/5000: episode: 161, duration: 0.511s, episode steps:  29, steps per second:  57, episode reward: 38.372, mean reward:  1.323 [-2.213, 31.933], mean action: 3.966 [0.000, 14.000],  loss: 0.018339, mae: 0.418698, mean_q: 0.441107, mean_eps: 0.000000
 4555/5000: episode: 162, duration: 0.398s, episode steps:  32, steps per second:  80, episode reward: 41.663, mean reward:  1.302 [-2.342, 32.220], mean action: 2.312 [0.000, 14.000],  loss: 0.018254, mae: 0.414616, mean_q: 0.457515, mean_eps: 0.000000
 4598/5000: episode: 163, duration: 0.497s, episode steps:  43, steps per second:  86, episode reward: -32.300, mean reward: -0.751 [-32.054,  2.610], mean action: 6.837 [0.000, 19.000],  loss: 0.020116, mae: 0.423172, mean_q: 0.492922, mean_eps: 0.000000
 4642/5000: episode: 164, duration: 0.516s, episode steps:  44, steps per second:  85, episode reward: 41.778, mean reward:  0.950 [-2.107, 32.271], mean action: 2.023 [0.000, 9.000],  loss: 0.018622, mae: 0.424776, mean_q: 0.493383, mean_eps: 0.000000
 4678/5000: episode: 165, duration: 0.430s, episode steps:  36, steps per second:  84, episode reward: 41.884, mean reward:  1.163 [-2.833, 32.211], mean action: 2.833 [0.000, 13.000],  loss: 0.019037, mae: 0.430781, mean_q: 0.464671, mean_eps: 0.000000
 4701/5000: episode: 166, duration: 0.291s, episode steps:  23, steps per second:  79, episode reward: 44.739, mean reward:  1.945 [-2.048, 32.197], mean action: 2.217 [0.000, 9.000],  loss: 0.023464, mae: 0.452597, mean_q: 0.545839, mean_eps: 0.000000
 4721/5000: episode: 167, duration: 0.254s, episode steps:  20, steps per second:  79, episode reward: 38.811, mean reward:  1.941 [-2.633, 32.051], mean action: 2.500 [0.000, 11.000],  loss: 0.022050, mae: 0.443464, mean_q: 0.520480, mean_eps: 0.000000
 4756/5000: episode: 168, duration: 0.411s, episode steps:  35, steps per second:  85, episode reward: 35.826, mean reward:  1.024 [-2.401, 32.260], mean action: 4.943 [0.000, 14.000],  loss: 0.017623, mae: 0.426493, mean_q: 0.485351, mean_eps: 0.000000
 4781/5000: episode: 169, duration: 0.325s, episode steps:  25, steps per second:  77, episode reward: 44.086, mean reward:  1.763 [-2.129, 32.170], mean action: 4.440 [0.000, 14.000],  loss: 0.018769, mae: 0.432590, mean_q: 0.478155, mean_eps: 0.000000
 4809/5000: episode: 170, duration: 0.344s, episode steps:  28, steps per second:  81, episode reward: 38.948, mean reward:  1.391 [-3.000, 32.940], mean action: 2.143 [0.000, 11.000],  loss: 0.021604, mae: 0.460138, mean_q: 0.420799, mean_eps: 0.000000
 4832/5000: episode: 171, duration: 0.287s, episode steps:  23, steps per second:  80, episode reward: 42.000, mean reward:  1.826 [-2.552, 32.610], mean action: 1.739 [0.000, 11.000],  loss: 0.020543, mae: 0.463955, mean_q: 0.425275, mean_eps: 0.000000
 4853/5000: episode: 172, duration: 0.256s, episode steps:  21, steps per second:  82, episode reward: 38.560, mean reward:  1.836 [-2.617, 33.000], mean action: 3.048 [0.000, 11.000],  loss: 0.021044, mae: 0.465602, mean_q: 0.421625, mean_eps: 0.000000
 4874/5000: episode: 173, duration: 0.272s, episode steps:  21, steps per second:  77, episode reward: 41.229, mean reward:  1.963 [-2.702, 32.480], mean action: 1.905 [0.000, 11.000],  loss: 0.020155, mae: 0.459594, mean_q: 0.431170, mean_eps: 0.000000
 4921/5000: episode: 174, duration: 0.581s, episode steps:  47, steps per second:  81, episode reward: 41.399, mean reward:  0.881 [-2.683, 32.940], mean action: 1.766 [0.000, 13.000],  loss: 0.021832, mae: 0.454714, mean_q: 0.511876, mean_eps: 0.000000
 4949/5000: episode: 175, duration: 0.337s, episode steps:  28, steps per second:  83, episode reward: 32.917, mean reward:  1.176 [-2.698, 32.291], mean action: 4.821 [0.000, 19.000],  loss: 0.021696, mae: 0.447155, mean_q: 0.517221, mean_eps: 0.000000
 4978/5000: episode: 176, duration: 0.340s, episode steps:  29, steps per second:  85, episode reward: -32.070, mean reward: -1.106 [-31.907,  2.180], mean action: 6.276 [0.000, 20.000],  loss: 0.020704, mae: 0.445482, mean_q: 0.454454, mean_eps: 0.000000
 4999/5000: episode: 177, duration: 0.259s, episode steps:  21, steps per second:  81, episode reward: 41.219, mean reward:  1.963 [-2.128, 32.140], mean action: 1.381 [0.000, 14.000],  loss: 0.019940, mae: 0.443923, mean_q: 0.455322, mean_eps: 0.000000
done, took 58.348 seconds
DQN Evaluation: 2655 victories out of 3156 episodes
Training for 5000 steps ...
   20/5000: episode: 1, duration: 0.134s, episode steps:  20, steps per second: 149, episode reward: 36.000, mean reward:  1.800 [-3.000, 32.180], mean action: 3.100 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   50/5000: episode: 2, duration: 0.192s, episode steps:  30, steps per second: 156, episode reward: 35.416, mean reward:  1.181 [-3.000, 32.416], mean action: 4.133 [1.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   84/5000: episode: 3, duration: 0.215s, episode steps:  34, steps per second: 158, episode reward: 32.938, mean reward:  0.969 [-2.877, 32.250], mean action: 3.147 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  107/5000: episode: 4, duration: 0.151s, episode steps:  23, steps per second: 152, episode reward: 32.903, mean reward:  1.431 [-2.767, 32.153], mean action: 5.522 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  135/5000: episode: 5, duration: 0.170s, episode steps:  28, steps per second: 165, episode reward: 32.833, mean reward:  1.173 [-2.869, 32.533], mean action: 2.821 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  158/5000: episode: 6, duration: 0.145s, episode steps:  23, steps per second: 159, episode reward: 32.751, mean reward:  1.424 [-2.560, 32.184], mean action: 5.565 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  184/5000: episode: 7, duration: 0.170s, episode steps:  26, steps per second: 153, episode reward: 38.268, mean reward:  1.472 [-2.630, 32.087], mean action: 4.962 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  210/5000: episode: 8, duration: 0.164s, episode steps:  26, steps per second: 159, episode reward: -35.650, mean reward: -1.371 [-32.177,  2.502], mean action: 5.308 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  230/5000: episode: 9, duration: 0.134s, episode steps:  20, steps per second: 149, episode reward: 38.250, mean reward:  1.913 [-2.136, 32.130], mean action: 2.550 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  261/5000: episode: 10, duration: 0.198s, episode steps:  31, steps per second: 157, episode reward: 32.294, mean reward:  1.042 [-2.379, 32.380], mean action: 3.065 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  280/5000: episode: 11, duration: 0.120s, episode steps:  19, steps per second: 158, episode reward: 35.289, mean reward:  1.857 [-3.000, 32.248], mean action: 5.000 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  312/5000: episode: 12, duration: 0.201s, episode steps:  32, steps per second: 159, episode reward: -32.240, mean reward: -1.008 [-31.648,  2.352], mean action: 8.312 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  331/5000: episode: 13, duration: 0.126s, episode steps:  19, steps per second: 151, episode reward: 36.000, mean reward:  1.895 [-3.000, 32.060], mean action: 3.263 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  346/5000: episode: 14, duration: 0.103s, episode steps:  15, steps per second: 146, episode reward: 41.433, mean reward:  2.762 [-2.299, 32.140], mean action: 3.333 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  371/5000: episode: 15, duration: 0.158s, episode steps:  25, steps per second: 159, episode reward: -32.270, mean reward: -1.291 [-31.896,  2.353], mean action: 4.720 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  395/5000: episode: 16, duration: 0.151s, episode steps:  24, steps per second: 159, episode reward: 33.000, mean reward:  1.375 [-3.000, 32.700], mean action: 2.833 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  423/5000: episode: 17, duration: 0.184s, episode steps:  28, steps per second: 152, episode reward: 34.807, mean reward:  1.243 [-2.533, 31.548], mean action: 5.179 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  445/5000: episode: 18, duration: 0.144s, episode steps:  22, steps per second: 153, episode reward: 35.302, mean reward:  1.605 [-2.753, 32.176], mean action: 4.545 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  472/5000: episode: 19, duration: 0.167s, episode steps:  27, steps per second: 162, episode reward: -35.220, mean reward: -1.304 [-32.091,  2.859], mean action: 8.593 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  505/5000: episode: 20, duration: 0.206s, episode steps:  33, steps per second: 160, episode reward: 32.786, mean reward:  0.994 [-2.754, 32.386], mean action: 3.879 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  533/5000: episode: 21, duration: 0.170s, episode steps:  28, steps per second: 165, episode reward: -38.620, mean reward: -1.379 [-32.661,  2.413], mean action: 3.964 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  548/5000: episode: 22, duration: 0.091s, episode steps:  15, steps per second: 165, episode reward: -45.000, mean reward: -3.000 [-33.000,  1.911], mean action: 4.000 [1.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  562/5000: episode: 23, duration: 0.095s, episode steps:  14, steps per second: 148, episode reward: 38.725, mean reward:  2.766 [-3.000, 33.000], mean action: 6.857 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  599/5000: episode: 24, duration: 0.218s, episode steps:  37, steps per second: 170, episode reward: -38.400, mean reward: -1.038 [-32.033,  2.280], mean action: 6.108 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  622/5000: episode: 25, duration: 0.151s, episode steps:  23, steps per second: 152, episode reward: -33.000, mean reward: -1.435 [-32.294,  2.223], mean action: 5.261 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  661/5000: episode: 26, duration: 0.237s, episode steps:  39, steps per second: 164, episode reward: 32.159, mean reward:  0.825 [-2.905, 32.480], mean action: 12.308 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  687/5000: episode: 27, duration: 0.157s, episode steps:  26, steps per second: 165, episode reward: -32.760, mean reward: -1.260 [-32.429,  2.627], mean action: 5.538 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  707/5000: episode: 28, duration: 0.128s, episode steps:  20, steps per second: 156, episode reward: -33.000, mean reward: -1.650 [-33.000,  2.744], mean action: 3.800 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  726/5000: episode: 29, duration: 0.124s, episode steps:  19, steps per second: 154, episode reward: 36.000, mean reward:  1.895 [-2.482, 32.250], mean action: 2.421 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  761/5000: episode: 30, duration: 0.208s, episode steps:  35, steps per second: 169, episode reward: -32.780, mean reward: -0.937 [-32.247,  2.940], mean action: 9.171 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  803/5000: episode: 31, duration: 0.251s, episode steps:  42, steps per second: 167, episode reward: 40.279, mean reward:  0.959 [-2.476, 32.290], mean action: 2.905 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  820/5000: episode: 32, duration: 0.122s, episode steps:  17, steps per second: 139, episode reward: 38.774, mean reward:  2.281 [-2.379, 32.774], mean action: 2.059 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  852/5000: episode: 33, duration: 0.192s, episode steps:  32, steps per second: 167, episode reward: -35.040, mean reward: -1.095 [-32.013,  2.490], mean action: 8.031 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  867/5000: episode: 34, duration: 0.108s, episode steps:  15, steps per second: 138, episode reward: 40.367, mean reward:  2.691 [-2.367, 32.250], mean action: 2.667 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  887/5000: episode: 35, duration: 4.192s, episode steps:  20, steps per second:   5, episode reward: 38.285, mean reward:  1.914 [-2.392, 32.010], mean action: 2.900 [1.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  904/5000: episode: 36, duration: 0.234s, episode steps:  17, steps per second:  73, episode reward: 39.000, mean reward:  2.294 [-3.000, 32.260], mean action: 2.059 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  929/5000: episode: 37, duration: 0.274s, episode steps:  25, steps per second:  91, episode reward: -35.160, mean reward: -1.406 [-33.000,  2.970], mean action: 3.880 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  958/5000: episode: 38, duration: 0.178s, episode steps:  29, steps per second: 163, episode reward: 34.845, mean reward:  1.202 [-3.000, 32.361], mean action: 6.724 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  978/5000: episode: 39, duration: 0.135s, episode steps:  20, steps per second: 148, episode reward: 41.680, mean reward:  2.084 [-2.431, 32.420], mean action: 1.900 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  993/5000: episode: 40, duration: 0.102s, episode steps:  15, steps per second: 148, episode reward: -36.000, mean reward: -2.400 [-30.420,  2.760], mean action: 4.067 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1014/5000: episode: 41, duration: 0.549s, episode steps:  21, steps per second:  38, episode reward: 37.254, mean reward:  1.774 [-3.000, 32.515], mean action: 6.905 [1.000, 19.000],  loss: 0.023709, mae: 0.468333, mean_q: 0.438736, mean_eps: 0.000000
 1034/5000: episode: 42, duration: 0.288s, episode steps:  20, steps per second:  69, episode reward: 39.000, mean reward:  1.950 [-2.263, 32.800], mean action: 3.350 [0.000, 11.000],  loss: 0.018098, mae: 0.432433, mean_q: 0.444349, mean_eps: 0.000000
 1056/5000: episode: 43, duration: 0.444s, episode steps:  22, steps per second:  50, episode reward: 35.255, mean reward:  1.602 [-3.000, 32.050], mean action: 3.136 [1.000, 12.000],  loss: 0.021370, mae: 0.434152, mean_q: 0.485037, mean_eps: 0.000000
 1079/5000: episode: 44, duration: 0.314s, episode steps:  23, steps per second:  73, episode reward: 38.599, mean reward:  1.678 [-2.602, 32.062], mean action: 3.304 [0.000, 12.000],  loss: 0.017005, mae: 0.413798, mean_q: 0.493801, mean_eps: 0.000000
 1105/5000: episode: 45, duration: 0.303s, episode steps:  26, steps per second:  86, episode reward: -33.000, mean reward: -1.269 [-30.000,  3.000], mean action: 2.769 [0.000, 12.000],  loss: 0.017076, mae: 0.410783, mean_q: 0.473817, mean_eps: 0.000000
 1129/5000: episode: 46, duration: 0.440s, episode steps:  24, steps per second:  55, episode reward: -32.500, mean reward: -1.354 [-32.960,  2.963], mean action: 6.292 [0.000, 18.000],  loss: 0.020363, mae: 0.423635, mean_q: 0.490010, mean_eps: 0.000000
 1154/5000: episode: 47, duration: 0.311s, episode steps:  25, steps per second:  80, episode reward: 35.733, mean reward:  1.429 [-3.000, 31.893], mean action: 6.680 [0.000, 20.000],  loss: 0.016463, mae: 0.411425, mean_q: 0.462916, mean_eps: 0.000000
 1177/5000: episode: 48, duration: 0.277s, episode steps:  23, steps per second:  83, episode reward: 37.970, mean reward:  1.651 [-2.280, 31.991], mean action: 6.217 [0.000, 20.000],  loss: 0.020478, mae: 0.426342, mean_q: 0.505302, mean_eps: 0.000000
 1199/5000: episode: 49, duration: 0.267s, episode steps:  22, steps per second:  82, episode reward: 38.839, mean reward:  1.765 [-2.614, 32.110], mean action: 4.545 [0.000, 14.000],  loss: 0.024031, mae: 0.451817, mean_q: 0.442567, mean_eps: 0.000000
 1222/5000: episode: 50, duration: 0.289s, episode steps:  23, steps per second:  79, episode reward: 35.188, mean reward:  1.530 [-2.733, 31.990], mean action: 4.435 [0.000, 14.000],  loss: 0.015898, mae: 0.417182, mean_q: 0.415000, mean_eps: 0.000000
 1248/5000: episode: 51, duration: 0.319s, episode steps:  26, steps per second:  81, episode reward: 35.498, mean reward:  1.365 [-2.444, 32.180], mean action: 4.192 [0.000, 21.000],  loss: 0.021923, mae: 0.433909, mean_q: 0.439982, mean_eps: 0.000000
 1261/5000: episode: 52, duration: 0.162s, episode steps:  13, steps per second:  80, episode reward: 38.046, mean reward:  2.927 [-2.939, 31.866], mean action: 4.154 [0.000, 16.000],  loss: 0.019690, mae: 0.423034, mean_q: 0.466469, mean_eps: 0.000000
 1283/5000: episode: 53, duration: 0.284s, episode steps:  22, steps per second:  78, episode reward: 40.674, mean reward:  1.849 [-2.124, 32.200], mean action: 3.318 [0.000, 16.000],  loss: 0.017846, mae: 0.419244, mean_q: 0.502237, mean_eps: 0.000000
 1302/5000: episode: 54, duration: 0.235s, episode steps:  19, steps per second:  81, episode reward: 38.961, mean reward:  2.051 [-2.276, 32.148], mean action: 3.368 [0.000, 14.000],  loss: 0.019773, mae: 0.426124, mean_q: 0.443248, mean_eps: 0.000000
 1322/5000: episode: 55, duration: 0.244s, episode steps:  20, steps per second:  82, episode reward: 35.837, mean reward:  1.792 [-3.000, 32.280], mean action: 4.400 [0.000, 16.000],  loss: 0.016801, mae: 0.416866, mean_q: 0.413605, mean_eps: 0.000000
 1348/5000: episode: 56, duration: 0.326s, episode steps:  26, steps per second:  80, episode reward: 35.247, mean reward:  1.356 [-3.000, 32.247], mean action: 3.923 [0.000, 19.000],  loss: 0.018397, mae: 0.424627, mean_q: 0.415170, mean_eps: 0.000000
 1365/5000: episode: 57, duration: 0.215s, episode steps:  17, steps per second:  79, episode reward: 38.373, mean reward:  2.257 [-2.247, 32.448], mean action: 6.706 [0.000, 16.000],  loss: 0.021215, mae: 0.432535, mean_q: 0.462489, mean_eps: 0.000000
 1400/5000: episode: 58, duration: 0.420s, episode steps:  35, steps per second:  83, episode reward: -32.040, mean reward: -0.915 [-32.036,  2.710], mean action: 7.629 [0.000, 21.000],  loss: 0.022887, mae: 0.436689, mean_q: 0.450824, mean_eps: 0.000000
 1419/5000: episode: 59, duration: 0.241s, episode steps:  19, steps per second:  79, episode reward: 35.516, mean reward:  1.869 [-3.000, 32.922], mean action: 5.895 [0.000, 16.000],  loss: 0.021802, mae: 0.425049, mean_q: 0.483251, mean_eps: 0.000000
 1447/5000: episode: 60, duration: 0.336s, episode steps:  28, steps per second:  83, episode reward: -33.000, mean reward: -1.179 [-32.502,  2.601], mean action: 8.036 [0.000, 19.000],  loss: 0.020840, mae: 0.432501, mean_q: 0.448653, mean_eps: 0.000000
 1467/5000: episode: 61, duration: 0.249s, episode steps:  20, steps per second:  80, episode reward: 41.559, mean reward:  2.078 [-2.416, 32.103], mean action: 2.850 [0.000, 11.000],  loss: 0.019890, mae: 0.426261, mean_q: 0.394591, mean_eps: 0.000000
 1498/5000: episode: 62, duration: 0.377s, episode steps:  31, steps per second:  82, episode reward: 35.288, mean reward:  1.138 [-3.000, 32.301], mean action: 2.710 [0.000, 12.000],  loss: 0.018299, mae: 0.411812, mean_q: 0.448788, mean_eps: 0.000000
 1524/5000: episode: 63, duration: 0.321s, episode steps:  26, steps per second:  81, episode reward: 35.429, mean reward:  1.363 [-2.455, 32.903], mean action: 5.500 [0.000, 19.000],  loss: 0.020478, mae: 0.422408, mean_q: 0.481354, mean_eps: 0.000000
 1541/5000: episode: 64, duration: 0.212s, episode steps:  17, steps per second:  80, episode reward: 38.200, mean reward:  2.247 [-2.862, 32.900], mean action: 6.647 [0.000, 21.000],  loss: 0.020936, mae: 0.426831, mean_q: 0.463295, mean_eps: 0.000000
 1552/5000: episode: 65, duration: 0.145s, episode steps:  11, steps per second:  76, episode reward: 46.883, mean reward:  4.262 [-0.058, 32.811], mean action: 4.091 [0.000, 13.000],  loss: 0.018932, mae: 0.416092, mean_q: 0.456475, mean_eps: 0.000000
 1575/5000: episode: 66, duration: 0.279s, episode steps:  23, steps per second:  82, episode reward: 32.058, mean reward:  1.394 [-3.000, 32.745], mean action: 8.391 [1.000, 19.000],  loss: 0.016731, mae: 0.411621, mean_q: 0.435738, mean_eps: 0.000000
 1592/5000: episode: 67, duration: 0.210s, episode steps:  17, steps per second:  81, episode reward: 35.155, mean reward:  2.068 [-3.000, 32.155], mean action: 5.824 [1.000, 16.000],  loss: 0.019644, mae: 0.429921, mean_q: 0.423512, mean_eps: 0.000000
 1611/5000: episode: 68, duration: 0.230s, episode steps:  19, steps per second:  82, episode reward: 36.000, mean reward:  1.895 [-2.703, 32.160], mean action: 7.421 [0.000, 21.000],  loss: 0.023835, mae: 0.454098, mean_q: 0.438931, mean_eps: 0.000000
 1635/5000: episode: 69, duration: 0.294s, episode steps:  24, steps per second:  82, episode reward: 38.036, mean reward:  1.585 [-2.176, 32.363], mean action: 3.583 [0.000, 14.000],  loss: 0.019627, mae: 0.436144, mean_q: 0.435892, mean_eps: 0.000000
 1664/5000: episode: 70, duration: 0.345s, episode steps:  29, steps per second:  84, episode reward: -33.000, mean reward: -1.138 [-32.587,  2.706], mean action: 5.966 [0.000, 14.000],  loss: 0.022837, mae: 0.443348, mean_q: 0.483373, mean_eps: 0.000000
 1694/5000: episode: 71, duration: 0.371s, episode steps:  30, steps per second:  81, episode reward: 37.128, mean reward:  1.238 [-2.141, 31.827], mean action: 2.800 [0.000, 11.000],  loss: 0.019572, mae: 0.427738, mean_q: 0.519564, mean_eps: 0.000000
 1727/5000: episode: 72, duration: 0.385s, episode steps:  33, steps per second:  86, episode reward: 32.551, mean reward:  0.986 [-3.000, 31.943], mean action: 7.242 [0.000, 18.000],  loss: 0.016166, mae: 0.412999, mean_q: 0.471095, mean_eps: 0.000000
 1744/5000: episode: 73, duration: 0.213s, episode steps:  17, steps per second:  80, episode reward: 38.904, mean reward:  2.288 [-2.227, 32.904], mean action: 4.412 [0.000, 12.000],  loss: 0.016949, mae: 0.415678, mean_q: 0.486760, mean_eps: 0.000000
 1764/5000: episode: 74, duration: 0.247s, episode steps:  20, steps per second:  81, episode reward: 35.493, mean reward:  1.775 [-2.707, 32.330], mean action: 5.400 [0.000, 15.000],  loss: 0.021942, mae: 0.436251, mean_q: 0.446732, mean_eps: 0.000000
 1781/5000: episode: 75, duration: 0.214s, episode steps:  17, steps per second:  80, episode reward: 41.701, mean reward:  2.453 [-2.267, 32.401], mean action: 3.765 [0.000, 15.000],  loss: 0.021861, mae: 0.432524, mean_q: 0.444852, mean_eps: 0.000000
 1807/5000: episode: 76, duration: 0.314s, episode steps:  26, steps per second:  83, episode reward: 35.716, mean reward:  1.374 [-2.275, 32.370], mean action: 5.538 [0.000, 19.000],  loss: 0.021561, mae: 0.435273, mean_q: 0.467657, mean_eps: 0.000000
 1827/5000: episode: 77, duration: 0.442s, episode steps:  20, steps per second:  45, episode reward: 32.130, mean reward:  1.606 [-3.000, 32.259], mean action: 6.150 [0.000, 19.000],  loss: 0.026139, mae: 0.458537, mean_q: 0.487268, mean_eps: 0.000000
 1850/5000: episode: 78, duration: 0.280s, episode steps:  23, steps per second:  82, episode reward: -32.710, mean reward: -1.422 [-32.134,  2.726], mean action: 5.174 [0.000, 19.000],  loss: 0.019702, mae: 0.431461, mean_q: 0.436830, mean_eps: 0.000000
 1866/5000: episode: 79, duration: 0.199s, episode steps:  16, steps per second:  80, episode reward: 39.000, mean reward:  2.438 [-2.470, 32.090], mean action: 4.312 [0.000, 19.000],  loss: 0.020023, mae: 0.441382, mean_q: 0.435021, mean_eps: 0.000000
 1882/5000: episode: 80, duration: 0.198s, episode steps:  16, steps per second:  81, episode reward: 40.858, mean reward:  2.554 [-2.325, 33.145], mean action: 6.688 [1.000, 19.000],  loss: 0.017510, mae: 0.420946, mean_q: 0.405159, mean_eps: 0.000000
 1909/5000: episode: 81, duration: 0.332s, episode steps:  27, steps per second:  81, episode reward: 35.188, mean reward:  1.303 [-2.435, 32.390], mean action: 10.889 [0.000, 20.000],  loss: 0.022608, mae: 0.444869, mean_q: 0.395599, mean_eps: 0.000000
 1933/5000: episode: 82, duration: 0.295s, episode steps:  24, steps per second:  81, episode reward: 38.844, mean reward:  1.618 [-2.911, 32.430], mean action: 8.333 [0.000, 19.000],  loss: 0.020134, mae: 0.434251, mean_q: 0.430641, mean_eps: 0.000000
 1957/5000: episode: 83, duration: 0.290s, episode steps:  24, steps per second:  83, episode reward: -32.280, mean reward: -1.345 [-31.897,  3.000], mean action: 7.542 [0.000, 19.000],  loss: 0.020198, mae: 0.438328, mean_q: 0.432123, mean_eps: 0.000000
 1984/5000: episode: 84, duration: 0.335s, episode steps:  27, steps per second:  81, episode reward: -32.100, mean reward: -1.189 [-32.027,  2.626], mean action: 7.148 [0.000, 19.000],  loss: 0.020872, mae: 0.441900, mean_q: 0.484292, mean_eps: 0.000000
 2002/5000: episode: 85, duration: 0.230s, episode steps:  18, steps per second:  78, episode reward: 34.845, mean reward:  1.936 [-3.000, 32.724], mean action: 5.778 [0.000, 19.000],  loss: 0.018736, mae: 0.431694, mean_q: 0.486786, mean_eps: 0.000000
 2032/5000: episode: 86, duration: 0.377s, episode steps:  30, steps per second:  80, episode reward: 32.645, mean reward:  1.088 [-2.350, 32.195], mean action: 7.400 [0.000, 15.000],  loss: 0.021884, mae: 0.449955, mean_q: 0.423738, mean_eps: 0.000000
 2059/5000: episode: 87, duration: 0.323s, episode steps:  27, steps per second:  83, episode reward: -36.000, mean reward: -1.333 [-32.221,  2.363], mean action: 1.741 [0.000, 9.000],  loss: 0.017851, mae: 0.439385, mean_q: 0.448643, mean_eps: 0.000000
 2110/5000: episode: 88, duration: 0.578s, episode steps:  51, steps per second:  88, episode reward: -41.390, mean reward: -0.812 [-31.839,  2.140], mean action: 3.235 [0.000, 15.000],  loss: 0.017535, mae: 0.434159, mean_q: 0.475684, mean_eps: 0.000000
 2172/5000: episode: 89, duration: 0.711s, episode steps:  62, steps per second:  87, episode reward: 32.021, mean reward:  0.516 [-3.000, 32.590], mean action: 10.452 [0.000, 21.000],  loss: 0.019216, mae: 0.438256, mean_q: 0.438071, mean_eps: 0.000000
 2202/5000: episode: 90, duration: 0.363s, episode steps:  30, steps per second:  83, episode reward: -33.000, mean reward: -1.100 [-32.395,  2.900], mean action: 9.133 [0.000, 18.000],  loss: 0.019790, mae: 0.438037, mean_q: 0.400536, mean_eps: 0.000000
 2236/5000: episode: 91, duration: 0.405s, episode steps:  34, steps per second:  84, episode reward: 43.593, mean reward:  1.282 [-2.423, 32.260], mean action: 2.618 [0.000, 20.000],  loss: 0.018479, mae: 0.431479, mean_q: 0.431446, mean_eps: 0.000000
 2259/5000: episode: 92, duration: 0.280s, episode steps:  23, steps per second:  82, episode reward: 42.000, mean reward:  1.826 [-2.422, 32.440], mean action: 2.652 [0.000, 11.000],  loss: 0.017016, mae: 0.429580, mean_q: 0.416276, mean_eps: 0.000000
 2283/5000: episode: 93, duration: 0.284s, episode steps:  24, steps per second:  84, episode reward: 35.073, mean reward:  1.461 [-2.998, 32.074], mean action: 6.083 [0.000, 20.000],  loss: 0.020720, mae: 0.451750, mean_q: 0.395268, mean_eps: 0.000000
 2361/5000: episode: 94, duration: 0.912s, episode steps:  78, steps per second:  86, episode reward: 33.000, mean reward:  0.423 [-2.288, 32.440], mean action: 11.013 [0.000, 21.000],  loss: 0.018763, mae: 0.429560, mean_q: 0.452290, mean_eps: 0.000000
 2381/5000: episode: 95, duration: 0.248s, episode steps:  20, steps per second:  81, episode reward: 35.454, mean reward:  1.773 [-2.552, 31.859], mean action: 6.400 [1.000, 14.000],  loss: 0.018335, mae: 0.424741, mean_q: 0.462449, mean_eps: 0.000000
 2407/5000: episode: 96, duration: 0.309s, episode steps:  26, steps per second:  84, episode reward: 35.025, mean reward:  1.347 [-2.379, 32.092], mean action: 4.885 [0.000, 13.000],  loss: 0.021701, mae: 0.440829, mean_q: 0.518824, mean_eps: 0.000000
 2493/5000: episode: 97, duration: 0.999s, episode steps:  86, steps per second:  86, episode reward: -35.940, mean reward: -0.418 [-33.000,  2.898], mean action: 11.058 [0.000, 21.000],  loss: 0.019666, mae: 0.426140, mean_q: 0.483346, mean_eps: 0.000000
 2517/5000: episode: 98, duration: 0.298s, episode steps:  24, steps per second:  80, episode reward: 38.485, mean reward:  1.604 [-2.500, 32.030], mean action: 5.500 [0.000, 20.000],  loss: 0.016076, mae: 0.403739, mean_q: 0.477200, mean_eps: 0.000000
 2542/5000: episode: 99, duration: 0.308s, episode steps:  25, steps per second:  81, episode reward: -32.140, mean reward: -1.286 [-32.410,  2.720], mean action: 3.800 [0.000, 14.000],  loss: 0.017127, mae: 0.408896, mean_q: 0.466706, mean_eps: 0.000000
 2559/5000: episode: 100, duration: 0.220s, episode steps:  17, steps per second:  77, episode reward: 40.962, mean reward:  2.410 [-2.361, 32.177], mean action: 5.412 [0.000, 14.000],  loss: 0.021091, mae: 0.428410, mean_q: 0.467339, mean_eps: 0.000000
 2587/5000: episode: 101, duration: 0.346s, episode steps:  28, steps per second:  81, episode reward: -36.000, mean reward: -1.286 [-32.947,  2.254], mean action: 6.679 [0.000, 16.000],  loss: 0.020955, mae: 0.436461, mean_q: 0.484765, mean_eps: 0.000000
 2603/5000: episode: 102, duration: 0.200s, episode steps:  16, steps per second:  80, episode reward: 40.691, mean reward:  2.543 [-3.000, 33.000], mean action: 7.062 [0.000, 20.000],  loss: 0.019895, mae: 0.429550, mean_q: 0.482803, mean_eps: 0.000000
 2619/5000: episode: 103, duration: 0.197s, episode steps:  16, steps per second:  81, episode reward: 41.904, mean reward:  2.619 [-2.198, 32.394], mean action: 2.812 [0.000, 9.000],  loss: 0.019028, mae: 0.432944, mean_q: 0.383858, mean_eps: 0.000000
 2645/5000: episode: 104, duration: 0.313s, episode steps:  26, steps per second:  83, episode reward: 35.903, mean reward:  1.381 [-2.773, 32.333], mean action: 4.269 [0.000, 14.000],  loss: 0.018204, mae: 0.432042, mean_q: 0.398922, mean_eps: 0.000000
 2664/5000: episode: 105, duration: 0.231s, episode steps:  19, steps per second:  82, episode reward: 41.285, mean reward:  2.173 [-2.167, 31.967], mean action: 5.579 [0.000, 14.000],  loss: 0.018051, mae: 0.423950, mean_q: 0.413207, mean_eps: 0.000000
 2695/5000: episode: 106, duration: 0.375s, episode steps:  31, steps per second:  83, episode reward: -32.450, mean reward: -1.047 [-32.057,  3.000], mean action: 2.677 [0.000, 13.000],  loss: 0.018416, mae: 0.416361, mean_q: 0.433294, mean_eps: 0.000000
 2717/5000: episode: 107, duration: 0.271s, episode steps:  22, steps per second:  81, episode reward: 41.527, mean reward:  1.888 [-2.206, 32.310], mean action: 4.818 [1.000, 14.000],  loss: 0.019752, mae: 0.428320, mean_q: 0.455243, mean_eps: 0.000000
 2739/5000: episode: 108, duration: 0.269s, episode steps:  22, steps per second:  82, episode reward: -32.160, mean reward: -1.462 [-31.990,  3.000], mean action: 7.364 [0.000, 21.000],  loss: 0.022875, mae: 0.448663, mean_q: 0.421448, mean_eps: 0.000000
 2760/5000: episode: 109, duration: 0.259s, episode steps:  21, steps per second:  81, episode reward: 41.375, mean reward:  1.970 [-2.395, 32.030], mean action: 3.000 [0.000, 16.000],  loss: 0.023911, mae: 0.456539, mean_q: 0.454308, mean_eps: 0.000000
 2778/5000: episode: 110, duration: 0.229s, episode steps:  18, steps per second:  79, episode reward: 38.281, mean reward:  2.127 [-2.300, 32.220], mean action: 2.278 [0.000, 11.000],  loss: 0.022431, mae: 0.447463, mean_q: 0.458489, mean_eps: 0.000000
 2802/5000: episode: 111, duration: 0.291s, episode steps:  24, steps per second:  83, episode reward: -35.120, mean reward: -1.463 [-32.650,  2.110], mean action: 6.208 [0.000, 14.000],  loss: 0.018309, mae: 0.430125, mean_q: 0.461130, mean_eps: 0.000000
 2841/5000: episode: 112, duration: 0.453s, episode steps:  39, steps per second:  86, episode reward: 38.255, mean reward:  0.981 [-2.217, 32.990], mean action: 5.308 [0.000, 15.000],  loss: 0.020112, mae: 0.441078, mean_q: 0.427428, mean_eps: 0.000000
 2871/5000: episode: 113, duration: 0.357s, episode steps:  30, steps per second:  84, episode reward: 35.714, mean reward:  1.190 [-2.342, 32.052], mean action: 7.400 [0.000, 19.000],  loss: 0.019946, mae: 0.439716, mean_q: 0.424372, mean_eps: 0.000000
 2893/5000: episode: 114, duration: 0.277s, episode steps:  22, steps per second:  80, episode reward: 35.540, mean reward:  1.615 [-3.000, 32.017], mean action: 4.545 [0.000, 15.000],  loss: 0.018586, mae: 0.436695, mean_q: 0.378142, mean_eps: 0.000000
 2912/5000: episode: 115, duration: 0.245s, episode steps:  19, steps per second:  77, episode reward: 38.149, mean reward:  2.008 [-2.289, 32.443], mean action: 5.421 [0.000, 14.000],  loss: 0.020458, mae: 0.444974, mean_q: 0.398283, mean_eps: 0.000000
 2945/5000: episode: 116, duration: 0.401s, episode steps:  33, steps per second:  82, episode reward: -32.810, mean reward: -0.994 [-32.157,  2.380], mean action: 10.818 [0.000, 21.000],  loss: 0.017298, mae: 0.418884, mean_q: 0.426681, mean_eps: 0.000000
 2964/5000: episode: 117, duration: 0.247s, episode steps:  19, steps per second:  77, episode reward: 35.646, mean reward:  1.876 [-2.340, 32.096], mean action: 2.632 [0.000, 9.000],  loss: 0.019567, mae: 0.425792, mean_q: 0.488401, mean_eps: 0.000000
 2977/5000: episode: 118, duration: 0.171s, episode steps:  13, steps per second:  76, episode reward: 42.000, mean reward:  3.231 [-2.651, 33.000], mean action: 3.846 [0.000, 14.000],  loss: 0.023259, mae: 0.444877, mean_q: 0.445922, mean_eps: 0.000000
 3002/5000: episode: 119, duration: 0.298s, episode steps:  25, steps per second:  84, episode reward: -35.210, mean reward: -1.408 [-32.074,  3.000], mean action: 9.280 [0.000, 20.000],  loss: 0.018175, mae: 0.424432, mean_q: 0.488461, mean_eps: 0.000000
 3025/5000: episode: 120, duration: 0.281s, episode steps:  23, steps per second:  82, episode reward: 33.000, mean reward:  1.435 [-2.873, 32.180], mean action: 6.565 [0.000, 20.000],  loss: 0.019544, mae: 0.434135, mean_q: 0.477680, mean_eps: 0.000000
 3069/5000: episode: 121, duration: 0.527s, episode steps:  44, steps per second:  83, episode reward: 40.918, mean reward:  0.930 [-2.465, 32.120], mean action: 3.500 [0.000, 19.000],  loss: 0.021099, mae: 0.433836, mean_q: 0.453834, mean_eps: 0.000000
 3089/5000: episode: 122, duration: 0.247s, episode steps:  20, steps per second:  81, episode reward: 38.332, mean reward:  1.917 [-2.668, 31.763], mean action: 6.400 [0.000, 19.000],  loss: 0.024398, mae: 0.453436, mean_q: 0.413266, mean_eps: 0.000000
 3107/5000: episode: 123, duration: 0.220s, episode steps:  18, steps per second:  82, episode reward: 38.257, mean reward:  2.125 [-2.402, 31.755], mean action: 6.222 [0.000, 16.000],  loss: 0.022120, mae: 0.437073, mean_q: 0.401935, mean_eps: 0.000000
 3137/5000: episode: 124, duration: 0.365s, episode steps:  30, steps per second:  82, episode reward: -35.280, mean reward: -1.176 [-32.223,  3.000], mean action: 7.533 [0.000, 16.000],  loss: 0.022183, mae: 0.442578, mean_q: 0.426686, mean_eps: 0.000000
 3164/5000: episode: 125, duration: 0.335s, episode steps:  27, steps per second:  81, episode reward: 30.000, mean reward:  1.111 [-3.000, 30.125], mean action: 7.815 [0.000, 15.000],  loss: 0.018646, mae: 0.427766, mean_q: 0.418989, mean_eps: 0.000000
 3208/5000: episode: 126, duration: 0.532s, episode steps:  44, steps per second:  83, episode reward: 35.460, mean reward:  0.806 [-2.418, 31.720], mean action: 2.818 [0.000, 12.000],  loss: 0.021055, mae: 0.434516, mean_q: 0.458303, mean_eps: 0.000000
 3234/5000: episode: 127, duration: 0.346s, episode steps:  26, steps per second:  75, episode reward: 32.765, mean reward:  1.260 [-2.552, 32.615], mean action: 3.923 [0.000, 16.000],  loss: 0.022253, mae: 0.436185, mean_q: 0.453701, mean_eps: 0.000000
 3252/5000: episode: 128, duration: 0.224s, episode steps:  18, steps per second:  80, episode reward: 42.000, mean reward:  2.333 [-2.664, 32.090], mean action: 6.778 [0.000, 14.000],  loss: 0.019757, mae: 0.436797, mean_q: 0.471599, mean_eps: 0.000000
 3281/5000: episode: 129, duration: 0.363s, episode steps:  29, steps per second:  80, episode reward: -32.010, mean reward: -1.104 [-31.140,  2.280], mean action: 7.621 [0.000, 21.000],  loss: 0.015872, mae: 0.426748, mean_q: 0.419156, mean_eps: 0.000000
 3302/5000: episode: 130, duration: 0.270s, episode steps:  21, steps per second:  78, episode reward: 39.000, mean reward:  1.857 [-2.395, 32.580], mean action: 2.905 [0.000, 12.000],  loss: 0.021963, mae: 0.460547, mean_q: 0.427403, mean_eps: 0.000000
 3323/5000: episode: 131, duration: 0.269s, episode steps:  21, steps per second:  78, episode reward: 35.500, mean reward:  1.690 [-3.000, 32.200], mean action: 4.333 [0.000, 19.000],  loss: 0.018014, mae: 0.431222, mean_q: 0.452216, mean_eps: 0.000000
 3340/5000: episode: 132, duration: 0.217s, episode steps:  17, steps per second:  78, episode reward: 41.220, mean reward:  2.425 [-2.416, 32.430], mean action: 4.882 [0.000, 19.000],  loss: 0.019655, mae: 0.431336, mean_q: 0.463305, mean_eps: 0.000000
 3378/5000: episode: 133, duration: 0.540s, episode steps:  38, steps per second:  70, episode reward: -32.810, mean reward: -0.863 [-32.249,  2.920], mean action: 10.289 [0.000, 19.000],  loss: 0.017583, mae: 0.429358, mean_q: 0.424602, mean_eps: 0.000000
 3420/5000: episode: 134, duration: 0.527s, episode steps:  42, steps per second:  80, episode reward: 32.756, mean reward:  0.780 [-2.594, 32.036], mean action: 7.333 [0.000, 16.000],  loss: 0.019388, mae: 0.428227, mean_q: 0.441747, mean_eps: 0.000000
 3485/5000: episode: 135, duration: 0.750s, episode steps:  65, steps per second:  87, episode reward: 36.000, mean reward:  0.554 [-2.093, 32.040], mean action: 3.308 [0.000, 16.000],  loss: 0.020575, mae: 0.439501, mean_q: 0.423018, mean_eps: 0.000000
 3505/5000: episode: 136, duration: 0.244s, episode steps:  20, steps per second:  82, episode reward: -32.600, mean reward: -1.630 [-32.353,  2.933], mean action: 8.150 [0.000, 21.000],  loss: 0.022681, mae: 0.443418, mean_q: 0.423803, mean_eps: 0.000000
 3551/5000: episode: 137, duration: 0.539s, episode steps:  46, steps per second:  85, episode reward: 38.900, mean reward:  0.846 [-3.000, 32.780], mean action: 4.674 [0.000, 21.000],  loss: 0.020344, mae: 0.426201, mean_q: 0.428926, mean_eps: 0.000000
 3579/5000: episode: 138, duration: 0.336s, episode steps:  28, steps per second:  83, episode reward: -33.000, mean reward: -1.179 [-32.020,  2.400], mean action: 6.607 [0.000, 21.000],  loss: 0.019867, mae: 0.422454, mean_q: 0.435175, mean_eps: 0.000000
 3602/5000: episode: 139, duration: 0.287s, episode steps:  23, steps per second:  80, episode reward: 34.803, mean reward:  1.513 [-2.338, 32.230], mean action: 4.609 [0.000, 14.000],  loss: 0.018759, mae: 0.420222, mean_q: 0.484271, mean_eps: 0.000000
 3623/5000: episode: 140, duration: 0.262s, episode steps:  21, steps per second:  80, episode reward: 38.883, mean reward:  1.852 [-2.327, 32.213], mean action: 6.333 [0.000, 19.000],  loss: 0.019992, mae: 0.439361, mean_q: 0.420941, mean_eps: 0.000000
 3650/5000: episode: 141, duration: 0.333s, episode steps:  27, steps per second:  81, episode reward: 37.413, mean reward:  1.386 [-2.333, 32.030], mean action: 4.148 [0.000, 19.000],  loss: 0.017050, mae: 0.424130, mean_q: 0.434584, mean_eps: 0.000000
 3675/5000: episode: 142, duration: 0.304s, episode steps:  25, steps per second:  82, episode reward: 32.496, mean reward:  1.300 [-2.393, 32.183], mean action: 7.360 [0.000, 21.000],  loss: 0.017363, mae: 0.425947, mean_q: 0.417423, mean_eps: 0.000000
 3696/5000: episode: 143, duration: 0.286s, episode steps:  21, steps per second:  73, episode reward: 38.784, mean reward:  1.847 [-2.685, 32.192], mean action: 5.095 [0.000, 16.000],  loss: 0.020630, mae: 0.441140, mean_q: 0.443934, mean_eps: 0.000000
 3713/5000: episode: 144, duration: 0.205s, episode steps:  17, steps per second:  83, episode reward: -42.000, mean reward: -2.471 [-33.000,  2.904], mean action: 7.765 [0.000, 16.000],  loss: 0.018673, mae: 0.428654, mean_q: 0.488176, mean_eps: 0.000000
 3739/5000: episode: 145, duration: 0.309s, episode steps:  26, steps per second:  84, episode reward: -35.770, mean reward: -1.376 [-32.019,  2.910], mean action: 6.731 [0.000, 16.000],  loss: 0.018878, mae: 0.439213, mean_q: 0.426951, mean_eps: 0.000000
 3765/5000: episode: 146, duration: 0.321s, episode steps:  26, steps per second:  81, episode reward: 35.647, mean reward:  1.371 [-2.429, 32.490], mean action: 4.346 [0.000, 15.000],  loss: 0.021547, mae: 0.450053, mean_q: 0.425775, mean_eps: 0.000000
 3787/5000: episode: 147, duration: 0.329s, episode steps:  22, steps per second:  67, episode reward: -35.480, mean reward: -1.613 [-32.037,  2.440], mean action: 3.773 [0.000, 15.000],  loss: 0.018685, mae: 0.431630, mean_q: 0.483071, mean_eps: 0.000000
 3816/5000: episode: 148, duration: 0.415s, episode steps:  29, steps per second:  70, episode reward: 35.606, mean reward:  1.228 [-3.000, 32.026], mean action: 2.241 [0.000, 12.000],  loss: 0.021056, mae: 0.441097, mean_q: 0.481867, mean_eps: 0.000000
 3837/5000: episode: 149, duration: 0.817s, episode steps:  21, steps per second:  26, episode reward: 35.019, mean reward:  1.668 [-3.000, 33.000], mean action: 4.905 [0.000, 16.000],  loss: 0.019331, mae: 0.433738, mean_q: 0.437969, mean_eps: 0.000000
 3861/5000: episode: 150, duration: 0.604s, episode steps:  24, steps per second:  40, episode reward: 37.824, mean reward:  1.576 [-2.350, 32.070], mean action: 4.625 [0.000, 21.000],  loss: 0.019317, mae: 0.437099, mean_q: 0.444941, mean_eps: 0.000000
 3881/5000: episode: 151, duration: 0.259s, episode steps:  20, steps per second:  77, episode reward: 38.131, mean reward:  1.907 [-2.389, 33.000], mean action: 5.800 [0.000, 20.000],  loss: 0.019081, mae: 0.448190, mean_q: 0.410747, mean_eps: 0.000000
 3909/5000: episode: 152, duration: 0.335s, episode steps:  28, steps per second:  84, episode reward: -32.910, mean reward: -1.175 [-31.940,  2.010], mean action: 4.286 [0.000, 15.000],  loss: 0.018976, mae: 0.440589, mean_q: 0.426744, mean_eps: 0.000000
 3940/5000: episode: 153, duration: 0.413s, episode steps:  31, steps per second:  75, episode reward: -32.050, mean reward: -1.034 [-32.674,  3.000], mean action: 4.161 [0.000, 19.000],  loss: 0.019185, mae: 0.444801, mean_q: 0.412537, mean_eps: 0.000000
 3960/5000: episode: 154, duration: 0.283s, episode steps:  20, steps per second:  71, episode reward: 37.387, mean reward:  1.869 [-3.000, 32.637], mean action: 5.850 [0.000, 14.000],  loss: 0.017824, mae: 0.435255, mean_q: 0.380957, mean_eps: 0.000000
 4009/5000: episode: 155, duration: 0.688s, episode steps:  49, steps per second:  71, episode reward: 38.708, mean reward:  0.790 [-2.289, 32.433], mean action: 3.816 [0.000, 13.000],  loss: 0.019553, mae: 0.443155, mean_q: 0.437237, mean_eps: 0.000000
 4038/5000: episode: 156, duration: 0.601s, episode steps:  29, steps per second:  48, episode reward: -35.110, mean reward: -1.211 [-32.200,  2.230], mean action: 9.103 [0.000, 14.000],  loss: 0.020196, mae: 0.448509, mean_q: 0.471372, mean_eps: 0.000000
 4057/5000: episode: 157, duration: 0.362s, episode steps:  19, steps per second:  52, episode reward: 41.219, mean reward:  2.169 [-2.444, 33.000], mean action: 4.737 [1.000, 20.000],  loss: 0.018862, mae: 0.446605, mean_q: 0.487792, mean_eps: 0.000000
 4075/5000: episode: 158, duration: 0.328s, episode steps:  18, steps per second:  55, episode reward: 38.062, mean reward:  2.115 [-2.410, 32.630], mean action: 4.389 [1.000, 12.000],  loss: 0.021013, mae: 0.459548, mean_q: 0.469642, mean_eps: 0.000000
 4103/5000: episode: 159, duration: 0.335s, episode steps:  28, steps per second:  84, episode reward: 32.514, mean reward:  1.161 [-3.000, 31.993], mean action: 8.071 [0.000, 21.000],  loss: 0.017846, mae: 0.435579, mean_q: 0.451751, mean_eps: 0.000000
 4121/5000: episode: 160, duration: 0.220s, episode steps:  18, steps per second:  82, episode reward: 38.904, mean reward:  2.161 [-2.470, 32.380], mean action: 3.111 [0.000, 12.000],  loss: 0.023021, mae: 0.456437, mean_q: 0.435747, mean_eps: 0.000000
 4150/5000: episode: 161, duration: 0.347s, episode steps:  29, steps per second:  84, episode reward: -32.290, mean reward: -1.113 [-32.443,  2.900], mean action: 5.276 [0.000, 15.000],  loss: 0.019874, mae: 0.437024, mean_q: 0.479396, mean_eps: 0.000000
 4189/5000: episode: 162, duration: 0.457s, episode steps:  39, steps per second:  85, episode reward: -35.170, mean reward: -0.902 [-32.327,  2.540], mean action: 5.718 [0.000, 21.000],  loss: 0.014638, mae: 0.424956, mean_q: 0.481383, mean_eps: 0.000000
 4218/5000: episode: 163, duration: 0.347s, episode steps:  29, steps per second:  84, episode reward: 38.154, mean reward:  1.316 [-2.316, 32.285], mean action: 4.379 [0.000, 13.000],  loss: 0.019229, mae: 0.451614, mean_q: 0.407197, mean_eps: 0.000000
 4240/5000: episode: 164, duration: 0.266s, episode steps:  22, steps per second:  83, episode reward: -35.660, mean reward: -1.621 [-32.320,  2.900], mean action: 3.636 [0.000, 14.000],  loss: 0.020850, mae: 0.457182, mean_q: 0.419487, mean_eps: 0.000000
 4272/5000: episode: 165, duration: 0.375s, episode steps:  32, steps per second:  85, episode reward: 38.622, mean reward:  1.207 [-2.378, 32.540], mean action: 3.406 [0.000, 14.000],  loss: 0.017659, mae: 0.434481, mean_q: 0.450723, mean_eps: 0.000000
 4295/5000: episode: 166, duration: 0.292s, episode steps:  23, steps per second:  79, episode reward: 37.988, mean reward:  1.652 [-3.000, 32.008], mean action: 5.217 [0.000, 15.000],  loss: 0.019360, mae: 0.448626, mean_q: 0.439934, mean_eps: 0.000000
 4314/5000: episode: 167, duration: 0.238s, episode steps:  19, steps per second:  80, episode reward: 32.575, mean reward:  1.714 [-3.000, 32.575], mean action: 5.000 [0.000, 19.000],  loss: 0.015229, mae: 0.422078, mean_q: 0.463830, mean_eps: 0.000000
 4348/5000: episode: 168, duration: 0.415s, episode steps:  34, steps per second:  82, episode reward: -32.360, mean reward: -0.952 [-32.237,  2.902], mean action: 9.412 [0.000, 21.000],  loss: 0.020763, mae: 0.444233, mean_q: 0.457814, mean_eps: 0.000000
 4367/5000: episode: 169, duration: 0.242s, episode steps:  19, steps per second:  79, episode reward: 43.336, mean reward:  2.281 [-2.229, 32.852], mean action: 4.000 [0.000, 14.000],  loss: 0.020971, mae: 0.442888, mean_q: 0.491146, mean_eps: 0.000000
 4387/5000: episode: 170, duration: 0.235s, episode steps:  20, steps per second:  85, episode reward: -41.580, mean reward: -2.079 [-32.350,  2.250], mean action: 4.850 [1.000, 12.000],  loss: 0.018552, mae: 0.430386, mean_q: 0.495896, mean_eps: 0.000000
 4405/5000: episode: 171, duration: 0.219s, episode steps:  18, steps per second:  82, episode reward: 36.000, mean reward:  2.000 [-2.545, 32.910], mean action: 3.500 [0.000, 12.000],  loss: 0.021456, mae: 0.439528, mean_q: 0.440781, mean_eps: 0.000000
 4434/5000: episode: 172, duration: 0.346s, episode steps:  29, steps per second:  84, episode reward: 35.033, mean reward:  1.208 [-2.646, 32.031], mean action: 7.862 [1.000, 18.000],  loss: 0.020460, mae: 0.437783, mean_q: 0.474713, mean_eps: 0.000000
 4471/5000: episode: 173, duration: 0.434s, episode steps:  37, steps per second:  85, episode reward: 32.706, mean reward:  0.884 [-2.623, 32.240], mean action: 5.054 [0.000, 19.000],  loss: 0.020354, mae: 0.428263, mean_q: 0.522937, mean_eps: 0.000000
 4499/5000: episode: 174, duration: 0.432s, episode steps:  28, steps per second:  65, episode reward: 37.708, mean reward:  1.347 [-2.401, 32.110], mean action: 8.929 [0.000, 21.000],  loss: 0.020629, mae: 0.436109, mean_q: 0.488528, mean_eps: 0.000000
 4523/5000: episode: 175, duration: 0.301s, episode steps:  24, steps per second:  80, episode reward: 32.311, mean reward:  1.346 [-3.000, 32.180], mean action: 3.042 [0.000, 12.000],  loss: 0.022382, mae: 0.446034, mean_q: 0.491824, mean_eps: 0.000000
 4549/5000: episode: 176, duration: 0.322s, episode steps:  26, steps per second:  81, episode reward: 32.177, mean reward:  1.238 [-2.479, 32.260], mean action: 7.654 [0.000, 14.000],  loss: 0.021514, mae: 0.442574, mean_q: 0.476998, mean_eps: 0.000000
 4572/5000: episode: 177, duration: 0.282s, episode steps:  23, steps per second:  82, episode reward: 32.227, mean reward:  1.401 [-2.838, 31.860], mean action: 4.913 [0.000, 19.000],  loss: 0.018117, mae: 0.427016, mean_q: 0.500704, mean_eps: 0.000000
 4600/5000: episode: 178, duration: 0.343s, episode steps:  28, steps per second:  82, episode reward: 35.623, mean reward:  1.272 [-2.439, 32.940], mean action: 4.929 [0.000, 19.000],  loss: 0.017618, mae: 0.428601, mean_q: 0.418570, mean_eps: 0.000000
 4629/5000: episode: 179, duration: 0.359s, episode steps:  29, steps per second:  81, episode reward: 32.357, mean reward:  1.116 [-2.455, 31.813], mean action: 4.207 [0.000, 14.000],  loss: 0.017961, mae: 0.430027, mean_q: 0.429190, mean_eps: 0.000000
 4644/5000: episode: 180, duration: 0.195s, episode steps:  15, steps per second:  77, episode reward: 41.669, mean reward:  2.778 [-2.383, 32.320], mean action: 7.200 [0.000, 19.000],  loss: 0.023749, mae: 0.450112, mean_q: 0.463477, mean_eps: 0.000000
 4674/5000: episode: 181, duration: 0.376s, episode steps:  30, steps per second:  80, episode reward: -32.380, mean reward: -1.079 [-32.433,  2.570], mean action: 5.100 [0.000, 19.000],  loss: 0.021219, mae: 0.435617, mean_q: 0.468090, mean_eps: 0.000000
 4688/5000: episode: 182, duration: 0.189s, episode steps:  14, steps per second:  74, episode reward: 44.107, mean reward:  3.151 [-1.801, 32.600], mean action: 6.929 [0.000, 19.000],  loss: 0.023217, mae: 0.451894, mean_q: 0.461710, mean_eps: 0.000000
 4710/5000: episode: 183, duration: 0.268s, episode steps:  22, steps per second:  82, episode reward: 32.854, mean reward:  1.493 [-2.563, 32.854], mean action: 5.227 [0.000, 19.000],  loss: 0.014269, mae: 0.417561, mean_q: 0.446131, mean_eps: 0.000000
 4743/5000: episode: 184, duration: 0.395s, episode steps:  33, steps per second:  84, episode reward: 40.021, mean reward:  1.213 [-2.361, 32.739], mean action: 4.273 [0.000, 19.000],  loss: 0.019560, mae: 0.444115, mean_q: 0.424111, mean_eps: 0.000000
 4767/5000: episode: 185, duration: 0.298s, episode steps:  24, steps per second:  81, episode reward: -32.630, mean reward: -1.360 [-31.664,  2.810], mean action: 9.042 [1.000, 19.000],  loss: 0.020819, mae: 0.448743, mean_q: 0.376290, mean_eps: 0.000000
 4788/5000: episode: 186, duration: 0.260s, episode steps:  21, steps per second:  81, episode reward: 33.000, mean reward:  1.571 [-3.000, 32.200], mean action: 5.952 [0.000, 19.000],  loss: 0.017322, mae: 0.438211, mean_q: 0.443999, mean_eps: 0.000000
 4817/5000: episode: 187, duration: 0.353s, episode steps:  29, steps per second:  82, episode reward: -35.770, mean reward: -1.233 [-31.799,  2.710], mean action: 7.724 [0.000, 14.000],  loss: 0.016824, mae: 0.430983, mean_q: 0.456286, mean_eps: 0.000000
 4833/5000: episode: 188, duration: 0.204s, episode steps:  16, steps per second:  78, episode reward: 38.278, mean reward:  2.392 [-2.408, 32.350], mean action: 3.438 [0.000, 12.000],  loss: 0.018144, mae: 0.446581, mean_q: 0.389053, mean_eps: 0.000000
 4854/5000: episode: 189, duration: 0.253s, episode steps:  21, steps per second:  83, episode reward: -38.600, mean reward: -1.838 [-32.162,  2.443], mean action: 6.667 [0.000, 15.000],  loss: 0.020100, mae: 0.454665, mean_q: 0.409065, mean_eps: 0.000000
 4878/5000: episode: 190, duration: 0.290s, episode steps:  24, steps per second:  83, episode reward: -41.640, mean reward: -1.735 [-32.204,  2.110], mean action: 13.458 [0.000, 20.000],  loss: 0.020712, mae: 0.459552, mean_q: 0.432361, mean_eps: 0.000000
 4899/5000: episode: 191, duration: 0.253s, episode steps:  21, steps per second:  83, episode reward: 35.700, mean reward:  1.700 [-3.000, 32.140], mean action: 5.000 [0.000, 15.000],  loss: 0.017684, mae: 0.442224, mean_q: 0.446507, mean_eps: 0.000000
 4922/5000: episode: 192, duration: 0.278s, episode steps:  23, steps per second:  83, episode reward: 36.000, mean reward:  1.565 [-2.386, 32.400], mean action: 4.696 [0.000, 15.000],  loss: 0.017913, mae: 0.435784, mean_q: 0.437467, mean_eps: 0.000000
 4939/5000: episode: 193, duration: 0.210s, episode steps:  17, steps per second:  81, episode reward: 38.617, mean reward:  2.272 [-2.579, 32.466], mean action: 4.118 [0.000, 19.000],  loss: 0.024233, mae: 0.460931, mean_q: 0.463192, mean_eps: 0.000000
 4964/5000: episode: 194, duration: 0.312s, episode steps:  25, steps per second:  80, episode reward: 35.583, mean reward:  1.423 [-2.924, 32.331], mean action: 2.560 [0.000, 9.000],  loss: 0.023364, mae: 0.467861, mean_q: 0.475654, mean_eps: 0.000000
 4982/5000: episode: 195, duration: 0.219s, episode steps:  18, steps per second:  82, episode reward: 35.853, mean reward:  1.992 [-3.000, 32.951], mean action: 4.167 [0.000, 14.000],  loss: 0.019400, mae: 0.446158, mean_q: 0.483209, mean_eps: 0.000000
done, took 62.371 seconds
DQN Evaluation: 2797 victories out of 3352 episodes
Training for 5000 steps ...
   31/5000: episode: 1, duration: 0.213s, episode steps:  31, steps per second: 145, episode reward: 41.004, mean reward:  1.323 [-3.000, 32.390], mean action: 3.452 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   51/5000: episode: 2, duration: 0.149s, episode steps:  20, steps per second: 134, episode reward: 47.620, mean reward:  2.381 [-0.458, 33.024], mean action: 3.050 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   73/5000: episode: 3, duration: 0.151s, episode steps:  22, steps per second: 146, episode reward: 41.240, mean reward:  1.875 [-2.889, 31.762], mean action: 4.091 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   99/5000: episode: 4, duration: 0.172s, episode steps:  26, steps per second: 151, episode reward: 44.931, mean reward:  1.728 [-2.310, 32.180], mean action: 4.346 [3.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  131/5000: episode: 5, duration: 0.213s, episode steps:  32, steps per second: 151, episode reward: 44.103, mean reward:  1.378 [-2.531, 31.821], mean action: 2.250 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  158/5000: episode: 6, duration: 0.256s, episode steps:  27, steps per second: 106, episode reward: 37.881, mean reward:  1.403 [-2.487, 32.042], mean action: 4.000 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  186/5000: episode: 7, duration: 0.174s, episode steps:  28, steps per second: 161, episode reward: 43.642, mean reward:  1.559 [-2.106, 32.130], mean action: 5.214 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  210/5000: episode: 8, duration: 0.168s, episode steps:  24, steps per second: 143, episode reward: 45.000, mean reward:  1.875 [-2.888, 32.110], mean action: 2.750 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  234/5000: episode: 9, duration: 0.165s, episode steps:  24, steps per second: 146, episode reward: 32.658, mean reward:  1.361 [-3.000, 32.110], mean action: 4.792 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  255/5000: episode: 10, duration: 0.145s, episode steps:  21, steps per second: 145, episode reward: 43.948, mean reward:  2.093 [-3.000, 31.809], mean action: 6.143 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  278/5000: episode: 11, duration: 0.150s, episode steps:  23, steps per second: 153, episode reward: 41.718, mean reward:  1.814 [-2.646, 32.322], mean action: 4.261 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  306/5000: episode: 12, duration: 0.177s, episode steps:  28, steps per second: 158, episode reward: 38.638, mean reward:  1.380 [-2.146, 31.948], mean action: 2.714 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  334/5000: episode: 13, duration: 0.179s, episode steps:  28, steps per second: 157, episode reward: 40.395, mean reward:  1.443 [-2.222, 32.016], mean action: 3.464 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  362/5000: episode: 14, duration: 0.176s, episode steps:  28, steps per second: 159, episode reward: 38.088, mean reward:  1.360 [-2.737, 32.050], mean action: 4.536 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  393/5000: episode: 15, duration: 0.197s, episode steps:  31, steps per second: 157, episode reward: 38.566, mean reward:  1.244 [-2.427, 32.227], mean action: 3.065 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  418/5000: episode: 16, duration: 0.167s, episode steps:  25, steps per second: 150, episode reward: 44.433, mean reward:  1.777 [-2.072, 32.140], mean action: 3.600 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  478/5000: episode: 17, duration: 0.345s, episode steps:  60, steps per second: 174, episode reward: -34.140, mean reward: -0.569 [-32.277,  3.000], mean action: 6.617 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  503/5000: episode: 18, duration: 0.157s, episode steps:  25, steps per second: 159, episode reward: 38.877, mean reward:  1.555 [-2.354, 32.370], mean action: 3.000 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  551/5000: episode: 19, duration: 0.287s, episode steps:  48, steps per second: 167, episode reward: 40.561, mean reward:  0.845 [-2.162, 32.211], mean action: 3.479 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  566/5000: episode: 20, duration: 0.135s, episode steps:  15, steps per second: 111, episode reward: 47.515, mean reward:  3.168 [-0.002, 32.440], mean action: 3.800 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  590/5000: episode: 21, duration: 0.163s, episode steps:  24, steps per second: 147, episode reward: 44.389, mean reward:  1.850 [-2.112, 32.173], mean action: 3.958 [2.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  615/5000: episode: 22, duration: 0.163s, episode steps:  25, steps per second: 153, episode reward: 37.473, mean reward:  1.499 [-3.000, 32.102], mean action: 2.760 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  645/5000: episode: 23, duration: 0.185s, episode steps:  30, steps per second: 163, episode reward: 39.000, mean reward:  1.300 [-2.592, 32.100], mean action: 1.967 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  681/5000: episode: 24, duration: 0.229s, episode steps:  36, steps per second: 157, episode reward: 40.533, mean reward:  1.126 [-2.465, 31.964], mean action: 2.194 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  711/5000: episode: 25, duration: 0.197s, episode steps:  30, steps per second: 152, episode reward: 43.821, mean reward:  1.461 [-2.233, 31.905], mean action: 4.867 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  740/5000: episode: 26, duration: 0.190s, episode steps:  29, steps per second: 152, episode reward: 41.902, mean reward:  1.445 [-2.266, 32.220], mean action: 2.586 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  762/5000: episode: 27, duration: 0.146s, episode steps:  22, steps per second: 151, episode reward: 44.651, mean reward:  2.030 [-2.429, 31.807], mean action: 3.000 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  783/5000: episode: 28, duration: 0.163s, episode steps:  21, steps per second: 129, episode reward: 41.359, mean reward:  1.969 [-3.000, 33.000], mean action: 2.762 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  803/5000: episode: 29, duration: 0.148s, episode steps:  20, steps per second: 136, episode reward: 42.000, mean reward:  2.100 [-2.285, 33.000], mean action: 3.600 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  849/5000: episode: 30, duration: 0.394s, episode steps:  46, steps per second: 117, episode reward: -32.500, mean reward: -0.707 [-32.070,  2.310], mean action: 8.891 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  873/5000: episode: 31, duration: 0.314s, episode steps:  24, steps per second:  76, episode reward: 38.053, mean reward:  1.586 [-2.606, 32.780], mean action: 8.208 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  928/5000: episode: 32, duration: 0.342s, episode steps:  55, steps per second: 161, episode reward: 38.559, mean reward:  0.701 [-2.163, 32.570], mean action: 2.982 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  954/5000: episode: 33, duration: 0.176s, episode steps:  26, steps per second: 147, episode reward: 35.574, mean reward:  1.368 [-2.841, 32.100], mean action: 5.308 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  974/5000: episode: 34, duration: 0.130s, episode steps:  20, steps per second: 154, episode reward: 45.000, mean reward:  2.250 [-2.476, 32.190], mean action: 2.900 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  993/5000: episode: 35, duration: 0.125s, episode steps:  19, steps per second: 152, episode reward: 38.743, mean reward:  2.039 [-2.305, 31.893], mean action: 2.842 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1017/5000: episode: 36, duration: 0.303s, episode steps:  24, steps per second:  79, episode reward: 41.016, mean reward:  1.709 [-2.356, 32.120], mean action: 3.875 [0.000, 16.000],  loss: 0.023948, mae: 0.470125, mean_q: 0.431171, mean_eps: 0.000000
 1045/5000: episode: 37, duration: 0.381s, episode steps:  28, steps per second:  73, episode reward: 40.277, mean reward:  1.438 [-3.000, 31.840], mean action: 4.286 [0.000, 20.000],  loss: 0.017264, mae: 0.430703, mean_q: 0.418348, mean_eps: 0.000000
 1071/5000: episode: 38, duration: 1.295s, episode steps:  26, steps per second:  20, episode reward: 38.938, mean reward:  1.498 [-2.753, 32.090], mean action: 2.538 [0.000, 12.000],  loss: 0.019069, mae: 0.437119, mean_q: 0.445335, mean_eps: 0.000000
 1097/5000: episode: 39, duration: 0.328s, episode steps:  26, steps per second:  79, episode reward: 43.803, mean reward:  1.685 [-3.000, 32.260], mean action: 4.115 [0.000, 16.000],  loss: 0.019614, mae: 0.439511, mean_q: 0.459719, mean_eps: 0.000000
 1129/5000: episode: 40, duration: 0.694s, episode steps:  32, steps per second:  46, episode reward: 35.298, mean reward:  1.103 [-2.876, 32.455], mean action: 5.375 [0.000, 16.000],  loss: 0.023664, mae: 0.448265, mean_q: 0.477633, mean_eps: 0.000000
 1157/5000: episode: 41, duration: 0.385s, episode steps:  28, steps per second:  73, episode reward: 38.678, mean reward:  1.381 [-3.000, 32.668], mean action: 4.321 [0.000, 19.000],  loss: 0.019662, mae: 0.422688, mean_q: 0.460355, mean_eps: 0.000000
 1180/5000: episode: 42, duration: 0.302s, episode steps:  23, steps per second:  76, episode reward: 44.680, mean reward:  1.943 [-2.111, 31.912], mean action: 3.130 [0.000, 19.000],  loss: 0.020895, mae: 0.428106, mean_q: 0.418085, mean_eps: 0.000000
 1208/5000: episode: 43, duration: 0.360s, episode steps:  28, steps per second:  78, episode reward: 41.064, mean reward:  1.467 [-2.921, 32.160], mean action: 6.607 [0.000, 20.000],  loss: 0.018903, mae: 0.428848, mean_q: 0.453123, mean_eps: 0.000000
 1238/5000: episode: 44, duration: 0.373s, episode steps:  30, steps per second:  80, episode reward: 44.171, mean reward:  1.472 [-2.194, 32.360], mean action: 2.400 [0.000, 11.000],  loss: 0.018625, mae: 0.424544, mean_q: 0.451395, mean_eps: 0.000000
 1262/5000: episode: 45, duration: 0.299s, episode steps:  24, steps per second:  80, episode reward: 38.224, mean reward:  1.593 [-2.579, 32.180], mean action: 4.583 [0.000, 14.000],  loss: 0.018657, mae: 0.415924, mean_q: 0.459930, mean_eps: 0.000000
 1285/5000: episode: 46, duration: 0.291s, episode steps:  23, steps per second:  79, episode reward: 41.421, mean reward:  1.801 [-2.575, 32.130], mean action: 5.174 [0.000, 19.000],  loss: 0.017770, mae: 0.419255, mean_q: 0.463986, mean_eps: 0.000000
 1321/5000: episode: 47, duration: 0.436s, episode steps:  36, steps per second:  83, episode reward: 40.786, mean reward:  1.133 [-3.000, 32.150], mean action: 4.639 [0.000, 19.000],  loss: 0.018815, mae: 0.425417, mean_q: 0.431320, mean_eps: 0.000000
 1363/5000: episode: 48, duration: 0.535s, episode steps:  42, steps per second:  79, episode reward: 33.000, mean reward:  0.786 [-3.000, 32.050], mean action: 8.071 [0.000, 21.000],  loss: 0.017970, mae: 0.432869, mean_q: 0.409996, mean_eps: 0.000000
 1405/5000: episode: 49, duration: 0.567s, episode steps:  42, steps per second:  74, episode reward: 35.537, mean reward:  0.846 [-3.000, 32.510], mean action: 4.310 [0.000, 19.000],  loss: 0.019832, mae: 0.439786, mean_q: 0.428812, mean_eps: 0.000000
 1435/5000: episode: 50, duration: 0.384s, episode steps:  30, steps per second:  78, episode reward: 46.767, mean reward:  1.559 [-0.210, 32.040], mean action: 2.733 [0.000, 14.000],  loss: 0.021068, mae: 0.430570, mean_q: 0.470887, mean_eps: 0.000000
 1462/5000: episode: 51, duration: 0.343s, episode steps:  27, steps per second:  79, episode reward: 32.370, mean reward:  1.199 [-3.000, 32.250], mean action: 5.926 [0.000, 19.000],  loss: 0.019968, mae: 0.436017, mean_q: 0.468653, mean_eps: 0.000000
 1485/5000: episode: 52, duration: 0.303s, episode steps:  23, steps per second:  76, episode reward: 43.615, mean reward:  1.896 [-2.000, 32.693], mean action: 5.087 [1.000, 19.000],  loss: 0.021638, mae: 0.452032, mean_q: 0.426432, mean_eps: 0.000000
 1518/5000: episode: 53, duration: 0.539s, episode steps:  33, steps per second:  61, episode reward: 33.000, mean reward:  1.000 [-2.819, 32.400], mean action: 2.515 [0.000, 16.000],  loss: 0.016653, mae: 0.420994, mean_q: 0.456547, mean_eps: 0.000000
 1532/5000: episode: 54, duration: 0.187s, episode steps:  14, steps per second:  75, episode reward: 44.043, mean reward:  3.146 [-2.126, 31.373], mean action: 4.286 [0.000, 14.000],  loss: 0.025151, mae: 0.454613, mean_q: 0.476576, mean_eps: 0.000000
 1545/5000: episode: 55, duration: 0.301s, episode steps:  13, steps per second:  43, episode reward: 44.281, mean reward:  3.406 [-2.319, 33.000], mean action: 3.615 [0.000, 14.000],  loss: 0.020659, mae: 0.430858, mean_q: 0.416696, mean_eps: 0.000000
 1586/5000: episode: 56, duration: 0.590s, episode steps:  41, steps per second:  69, episode reward: 37.404, mean reward:  0.912 [-2.869, 32.170], mean action: 4.317 [0.000, 15.000],  loss: 0.020173, mae: 0.424221, mean_q: 0.501364, mean_eps: 0.000000
 1634/5000: episode: 57, duration: 0.599s, episode steps:  48, steps per second:  80, episode reward: 39.000, mean reward:  0.812 [-3.000, 32.340], mean action: 3.688 [0.000, 19.000],  loss: 0.019429, mae: 0.418806, mean_q: 0.479625, mean_eps: 0.000000
 1702/5000: episode: 58, duration: 0.910s, episode steps:  68, steps per second:  75, episode reward: -32.410, mean reward: -0.477 [-32.234,  2.585], mean action: 8.059 [0.000, 19.000],  loss: 0.019806, mae: 0.418605, mean_q: 0.452558, mean_eps: 0.000000
 1741/5000: episode: 59, duration: 0.461s, episode steps:  39, steps per second:  85, episode reward: 41.784, mean reward:  1.071 [-2.197, 32.050], mean action: 3.385 [0.000, 20.000],  loss: 0.021466, mae: 0.433457, mean_q: 0.419815, mean_eps: 0.000000
 1768/5000: episode: 60, duration: 0.326s, episode steps:  27, steps per second:  83, episode reward: 44.130, mean reward:  1.634 [-2.282, 31.937], mean action: 2.593 [1.000, 16.000],  loss: 0.019055, mae: 0.415364, mean_q: 0.459378, mean_eps: 0.000000
 1788/5000: episode: 61, duration: 0.246s, episode steps:  20, steps per second:  81, episode reward: 41.502, mean reward:  2.075 [-2.190, 32.272], mean action: 2.650 [0.000, 16.000],  loss: 0.017978, mae: 0.413748, mean_q: 0.449509, mean_eps: 0.000000
 1835/5000: episode: 62, duration: 0.556s, episode steps:  47, steps per second:  85, episode reward: 32.753, mean reward:  0.697 [-2.598, 32.133], mean action: 10.596 [0.000, 21.000],  loss: 0.018202, mae: 0.425046, mean_q: 0.454940, mean_eps: 0.000000
 1860/5000: episode: 63, duration: 0.308s, episode steps:  25, steps per second:  81, episode reward: 41.502, mean reward:  1.660 [-2.670, 32.140], mean action: 1.640 [0.000, 12.000],  loss: 0.021531, mae: 0.435097, mean_q: 0.435515, mean_eps: 0.000000
 1901/5000: episode: 64, duration: 0.506s, episode steps:  41, steps per second:  81, episode reward: 41.743, mean reward:  1.018 [-2.651, 32.900], mean action: 2.098 [0.000, 21.000],  loss: 0.019887, mae: 0.420280, mean_q: 0.485084, mean_eps: 0.000000
 1937/5000: episode: 65, duration: 0.438s, episode steps:  36, steps per second:  82, episode reward: 37.316, mean reward:  1.037 [-2.633, 32.070], mean action: 4.306 [1.000, 16.000],  loss: 0.019462, mae: 0.421356, mean_q: 0.428464, mean_eps: 0.000000
 1966/5000: episode: 66, duration: 0.381s, episode steps:  29, steps per second:  76, episode reward: 36.000, mean reward:  1.241 [-2.510, 32.090], mean action: 3.069 [0.000, 16.000],  loss: 0.020070, mae: 0.419196, mean_q: 0.469050, mean_eps: 0.000000
 1986/5000: episode: 67, duration: 0.248s, episode steps:  20, steps per second:  81, episode reward: 38.040, mean reward:  1.902 [-2.709, 32.760], mean action: 3.750 [0.000, 16.000],  loss: 0.014960, mae: 0.390195, mean_q: 0.507453, mean_eps: 0.000000
 1997/5000: episode: 68, duration: 0.150s, episode steps:  11, steps per second:  73, episode reward: 47.498, mean reward:  4.318 [ 0.000, 33.000], mean action: 1.091 [0.000, 3.000],  loss: 0.021616, mae: 0.422649, mean_q: 0.513521, mean_eps: 0.000000
 2030/5000: episode: 69, duration: 0.520s, episode steps:  33, steps per second:  63, episode reward: 41.654, mean reward:  1.262 [-2.219, 32.400], mean action: 5.485 [0.000, 19.000],  loss: 0.020953, mae: 0.421136, mean_q: 0.488182, mean_eps: 0.000000
 2050/5000: episode: 70, duration: 0.275s, episode steps:  20, steps per second:  73, episode reward: 41.365, mean reward:  2.068 [-2.570, 32.150], mean action: 2.950 [0.000, 19.000],  loss: 0.019249, mae: 0.415082, mean_q: 0.474884, mean_eps: 0.000000
 2091/5000: episode: 71, duration: 0.521s, episode steps:  41, steps per second:  79, episode reward: 44.425, mean reward:  1.084 [-2.508, 32.290], mean action: 2.902 [0.000, 19.000],  loss: 0.020182, mae: 0.418003, mean_q: 0.506306, mean_eps: 0.000000
 2123/5000: episode: 72, duration: 0.385s, episode steps:  32, steps per second:  83, episode reward: 42.000, mean reward:  1.313 [-2.806, 32.480], mean action: 2.250 [0.000, 19.000],  loss: 0.019061, mae: 0.410458, mean_q: 0.485651, mean_eps: 0.000000
 2151/5000: episode: 73, duration: 0.369s, episode steps:  28, steps per second:  76, episode reward: 40.016, mean reward:  1.429 [-3.000, 32.110], mean action: 5.214 [0.000, 19.000],  loss: 0.019327, mae: 0.419320, mean_q: 0.436804, mean_eps: 0.000000
 2196/5000: episode: 74, duration: 0.583s, episode steps:  45, steps per second:  77, episode reward: 46.956, mean reward:  1.043 [-0.369, 32.180], mean action: 2.756 [0.000, 13.000],  loss: 0.018091, mae: 0.419788, mean_q: 0.474213, mean_eps: 0.000000
 2220/5000: episode: 75, duration: 0.303s, episode steps:  24, steps per second:  79, episode reward: 41.736, mean reward:  1.739 [-2.035, 29.932], mean action: 2.208 [1.000, 9.000],  loss: 0.022248, mae: 0.443425, mean_q: 0.439749, mean_eps: 0.000000
 2247/5000: episode: 76, duration: 0.331s, episode steps:  27, steps per second:  82, episode reward: 36.000, mean reward:  1.333 [-2.941, 32.950], mean action: 4.037 [0.000, 19.000],  loss: 0.019831, mae: 0.433463, mean_q: 0.434963, mean_eps: 0.000000
 2276/5000: episode: 77, duration: 0.486s, episode steps:  29, steps per second:  60, episode reward: 47.595, mean reward:  1.641 [-0.320, 32.030], mean action: 2.448 [0.000, 19.000],  loss: 0.019873, mae: 0.426536, mean_q: 0.453927, mean_eps: 0.000000
 2306/5000: episode: 78, duration: 0.389s, episode steps:  30, steps per second:  77, episode reward: 41.208, mean reward:  1.374 [-2.191, 32.320], mean action: 2.767 [0.000, 19.000],  loss: 0.018422, mae: 0.412569, mean_q: 0.473181, mean_eps: 0.000000
 2341/5000: episode: 79, duration: 0.509s, episode steps:  35, steps per second:  69, episode reward: 32.741, mean reward:  0.935 [-2.329, 32.140], mean action: 8.000 [0.000, 20.000],  loss: 0.018613, mae: 0.413621, mean_q: 0.458651, mean_eps: 0.000000
 2391/5000: episode: 80, duration: 0.591s, episode steps:  50, steps per second:  85, episode reward: -32.480, mean reward: -0.650 [-32.450,  2.730], mean action: 4.260 [0.000, 14.000],  loss: 0.021159, mae: 0.424784, mean_q: 0.471727, mean_eps: 0.000000
 2417/5000: episode: 81, duration: 0.324s, episode steps:  26, steps per second:  80, episode reward: 44.398, mean reward:  1.708 [-2.177, 32.163], mean action: 2.385 [0.000, 20.000],  loss: 0.015777, mae: 0.398855, mean_q: 0.473201, mean_eps: 0.000000
 2460/5000: episode: 82, duration: 0.505s, episode steps:  43, steps per second:  85, episode reward: 32.227, mean reward:  0.749 [-2.318, 31.672], mean action: 5.814 [0.000, 14.000],  loss: 0.016272, mae: 0.402463, mean_q: 0.469593, mean_eps: 0.000000
 2483/5000: episode: 83, duration: 0.333s, episode steps:  23, steps per second:  69, episode reward: 38.327, mean reward:  1.666 [-2.626, 31.617], mean action: 4.261 [1.000, 16.000],  loss: 0.015748, mae: 0.404903, mean_q: 0.465809, mean_eps: 0.000000
 2509/5000: episode: 84, duration: 0.367s, episode steps:  26, steps per second:  71, episode reward: 38.421, mean reward:  1.478 [-2.903, 32.310], mean action: 4.923 [0.000, 15.000],  loss: 0.022529, mae: 0.437001, mean_q: 0.451499, mean_eps: 0.000000
 2530/5000: episode: 85, duration: 0.302s, episode steps:  21, steps per second:  70, episode reward: 38.544, mean reward:  1.835 [-2.727, 32.110], mean action: 3.238 [0.000, 14.000],  loss: 0.021204, mae: 0.429737, mean_q: 0.410844, mean_eps: 0.000000
 2566/5000: episode: 86, duration: 0.431s, episode steps:  36, steps per second:  83, episode reward: 35.589, mean reward:  0.989 [-2.673, 29.774], mean action: 5.444 [0.000, 20.000],  loss: 0.019614, mae: 0.428424, mean_q: 0.433457, mean_eps: 0.000000
 2600/5000: episode: 87, duration: 0.405s, episode steps:  34, steps per second:  84, episode reward: 41.104, mean reward:  1.209 [-2.252, 31.578], mean action: 1.765 [0.000, 16.000],  loss: 0.022592, mae: 0.435352, mean_q: 0.426591, mean_eps: 0.000000
 2623/5000: episode: 88, duration: 0.280s, episode steps:  23, steps per second:  82, episode reward: 41.592, mean reward:  1.808 [-2.880, 31.971], mean action: 2.261 [0.000, 19.000],  loss: 0.018905, mae: 0.416544, mean_q: 0.437682, mean_eps: 0.000000
 2655/5000: episode: 89, duration: 0.408s, episode steps:  32, steps per second:  78, episode reward: 38.837, mean reward:  1.214 [-2.134, 32.320], mean action: 3.250 [0.000, 19.000],  loss: 0.019379, mae: 0.415197, mean_q: 0.427689, mean_eps: 0.000000
 2690/5000: episode: 90, duration: 0.421s, episode steps:  35, steps per second:  83, episode reward: 39.000, mean reward:  1.114 [-2.368, 32.120], mean action: 4.286 [0.000, 19.000],  loss: 0.019338, mae: 0.415974, mean_q: 0.464721, mean_eps: 0.000000
 2712/5000: episode: 91, duration: 0.268s, episode steps:  22, steps per second:  82, episode reward: 44.650, mean reward:  2.030 [-2.250, 32.220], mean action: 5.136 [2.000, 19.000],  loss: 0.017019, mae: 0.409503, mean_q: 0.463374, mean_eps: 0.000000
 2746/5000: episode: 92, duration: 0.508s, episode steps:  34, steps per second:  67, episode reward: 41.165, mean reward:  1.211 [-2.153, 32.289], mean action: 5.294 [0.000, 19.000],  loss: 0.021332, mae: 0.434291, mean_q: 0.422775, mean_eps: 0.000000
 2773/5000: episode: 93, duration: 0.555s, episode steps:  27, steps per second:  49, episode reward: 44.083, mean reward:  1.633 [-2.182, 31.920], mean action: 4.111 [0.000, 19.000],  loss: 0.020421, mae: 0.424241, mean_q: 0.432574, mean_eps: 0.000000
 2798/5000: episode: 94, duration: 0.327s, episode steps:  25, steps per second:  76, episode reward: 37.923, mean reward:  1.517 [-2.903, 32.013], mean action: 3.760 [0.000, 19.000],  loss: 0.019251, mae: 0.410459, mean_q: 0.407257, mean_eps: 0.000000
 2832/5000: episode: 95, duration: 0.412s, episode steps:  34, steps per second:  83, episode reward: -32.070, mean reward: -0.943 [-32.145,  2.900], mean action: 3.618 [0.000, 19.000],  loss: 0.024377, mae: 0.440825, mean_q: 0.446215, mean_eps: 0.000000
 2858/5000: episode: 96, duration: 0.338s, episode steps:  26, steps per second:  77, episode reward: 38.741, mean reward:  1.490 [-2.570, 31.841], mean action: 3.731 [0.000, 19.000],  loss: 0.021104, mae: 0.424840, mean_q: 0.491131, mean_eps: 0.000000
 2880/5000: episode: 97, duration: 0.293s, episode steps:  22, steps per second:  75, episode reward: 43.761, mean reward:  1.989 [-2.494, 32.690], mean action: 5.182 [0.000, 19.000],  loss: 0.018290, mae: 0.415308, mean_q: 0.460221, mean_eps: 0.000000
 2914/5000: episode: 98, duration: 0.420s, episode steps:  34, steps per second:  81, episode reward: 33.000, mean reward:  0.971 [-3.000, 32.220], mean action: 3.853 [0.000, 19.000],  loss: 0.018084, mae: 0.403673, mean_q: 0.482329, mean_eps: 0.000000
 2969/5000: episode: 99, duration: 0.643s, episode steps:  55, steps per second:  86, episode reward: 32.498, mean reward:  0.591 [-2.299, 32.310], mean action: 7.400 [0.000, 20.000],  loss: 0.019102, mae: 0.421969, mean_q: 0.457633, mean_eps: 0.000000
 2993/5000: episode: 100, duration: 0.310s, episode steps:  24, steps per second:  77, episode reward: 41.072, mean reward:  1.711 [-2.506, 32.430], mean action: 3.958 [0.000, 12.000],  loss: 0.020973, mae: 0.442667, mean_q: 0.449576, mean_eps: 0.000000
 3026/5000: episode: 101, duration: 0.405s, episode steps:  33, steps per second:  81, episode reward: 37.915, mean reward:  1.149 [-2.277, 32.330], mean action: 3.727 [0.000, 13.000],  loss: 0.021124, mae: 0.437908, mean_q: 0.445354, mean_eps: 0.000000
 3054/5000: episode: 102, duration: 0.358s, episode steps:  28, steps per second:  78, episode reward: 39.000, mean reward:  1.393 [-2.308, 32.140], mean action: 2.821 [0.000, 19.000],  loss: 0.018782, mae: 0.429949, mean_q: 0.449478, mean_eps: 0.000000
 3078/5000: episode: 103, duration: 0.305s, episode steps:  24, steps per second:  79, episode reward: 42.000, mean reward:  1.750 [-2.246, 33.000], mean action: 3.458 [0.000, 19.000],  loss: 0.021337, mae: 0.434724, mean_q: 0.466499, mean_eps: 0.000000
 3106/5000: episode: 104, duration: 0.349s, episode steps:  28, steps per second:  80, episode reward: 38.373, mean reward:  1.370 [-2.535, 32.190], mean action: 3.571 [0.000, 19.000],  loss: 0.019417, mae: 0.426027, mean_q: 0.501855, mean_eps: 0.000000
 3132/5000: episode: 105, duration: 0.317s, episode steps:  26, steps per second:  82, episode reward: 38.368, mean reward:  1.476 [-2.723, 32.510], mean action: 3.231 [0.000, 20.000],  loss: 0.019495, mae: 0.425278, mean_q: 0.466479, mean_eps: 0.000000
 3159/5000: episode: 106, duration: 0.347s, episode steps:  27, steps per second:  78, episode reward: 44.467, mean reward:  1.647 [-2.631, 31.788], mean action: 1.741 [0.000, 12.000],  loss: 0.022217, mae: 0.447972, mean_q: 0.422122, mean_eps: 0.000000
 3181/5000: episode: 107, duration: 0.278s, episode steps:  22, steps per second:  79, episode reward: 43.576, mean reward:  1.981 [-2.329, 32.317], mean action: 4.182 [1.000, 14.000],  loss: 0.019597, mae: 0.417154, mean_q: 0.452009, mean_eps: 0.000000
 3226/5000: episode: 108, duration: 0.554s, episode steps:  45, steps per second:  81, episode reward: -32.410, mean reward: -0.720 [-32.179,  2.632], mean action: 9.422 [0.000, 21.000],  loss: 0.019067, mae: 0.412950, mean_q: 0.425661, mean_eps: 0.000000
 3257/5000: episode: 109, duration: 0.386s, episode steps:  31, steps per second:  80, episode reward: 44.504, mean reward:  1.436 [-2.152, 31.943], mean action: 6.032 [0.000, 19.000],  loss: 0.017773, mae: 0.415022, mean_q: 0.393765, mean_eps: 0.000000
 3280/5000: episode: 110, duration: 0.297s, episode steps:  23, steps per second:  78, episode reward: 44.160, mean reward:  1.920 [-2.486, 32.160], mean action: 2.348 [0.000, 8.000],  loss: 0.016550, mae: 0.414842, mean_q: 0.427815, mean_eps: 0.000000
 3302/5000: episode: 111, duration: 0.272s, episode steps:  22, steps per second:  81, episode reward: 41.003, mean reward:  1.864 [-2.430, 32.070], mean action: 6.000 [0.000, 19.000],  loss: 0.020878, mae: 0.424399, mean_q: 0.432749, mean_eps: 0.000000
 3327/5000: episode: 112, duration: 0.318s, episode steps:  25, steps per second:  79, episode reward: 41.635, mean reward:  1.665 [-2.595, 32.145], mean action: 1.760 [0.000, 9.000],  loss: 0.020616, mae: 0.435787, mean_q: 0.420694, mean_eps: 0.000000
 3354/5000: episode: 113, duration: 0.344s, episode steps:  27, steps per second:  79, episode reward: 38.071, mean reward:  1.410 [-3.000, 31.592], mean action: 2.963 [1.000, 13.000],  loss: 0.019009, mae: 0.423711, mean_q: 0.448424, mean_eps: 0.000000
 3369/5000: episode: 114, duration: 0.196s, episode steps:  15, steps per second:  77, episode reward: 44.361, mean reward:  2.957 [-2.385, 32.393], mean action: 2.133 [0.000, 14.000],  loss: 0.022155, mae: 0.432267, mean_q: 0.445355, mean_eps: 0.000000
 3386/5000: episode: 115, duration: 0.252s, episode steps:  17, steps per second:  68, episode reward: 41.517, mean reward:  2.442 [-2.417, 32.140], mean action: 2.706 [0.000, 9.000],  loss: 0.016931, mae: 0.408062, mean_q: 0.506603, mean_eps: 0.000000
 3413/5000: episode: 116, duration: 0.378s, episode steps:  27, steps per second:  71, episode reward: 41.544, mean reward:  1.539 [-2.623, 32.125], mean action: 4.704 [0.000, 19.000],  loss: 0.020110, mae: 0.422338, mean_q: 0.487940, mean_eps: 0.000000
 3446/5000: episode: 117, duration: 0.398s, episode steps:  33, steps per second:  83, episode reward: 40.828, mean reward:  1.237 [-2.371, 31.833], mean action: 2.848 [0.000, 11.000],  loss: 0.022247, mae: 0.430870, mean_q: 0.497805, mean_eps: 0.000000
 3478/5000: episode: 118, duration: 0.651s, episode steps:  32, steps per second:  49, episode reward: 38.241, mean reward:  1.195 [-3.000, 32.170], mean action: 4.406 [0.000, 19.000],  loss: 0.022061, mae: 0.432729, mean_q: 0.458413, mean_eps: 0.000000
 3509/5000: episode: 119, duration: 0.580s, episode steps:  31, steps per second:  53, episode reward: 41.375, mean reward:  1.335 [-2.414, 32.790], mean action: 2.935 [0.000, 13.000],  loss: 0.020372, mae: 0.431965, mean_q: 0.433444, mean_eps: 0.000000
 3533/5000: episode: 120, duration: 1.490s, episode steps:  24, steps per second:  16, episode reward: -35.240, mean reward: -1.468 [-33.000,  2.510], mean action: 2.625 [0.000, 12.000],  loss: 0.021885, mae: 0.436387, mean_q: 0.435335, mean_eps: 0.000000
 3560/5000: episode: 121, duration: 0.441s, episode steps:  27, steps per second:  61, episode reward: 38.315, mean reward:  1.419 [-2.270, 32.050], mean action: 4.852 [0.000, 20.000],  loss: 0.016312, mae: 0.422481, mean_q: 0.504310, mean_eps: 0.000000
 3579/5000: episode: 122, duration: 0.291s, episode steps:  19, steps per second:  65, episode reward: 41.542, mean reward:  2.186 [-2.308, 32.634], mean action: 1.105 [0.000, 9.000],  loss: 0.020081, mae: 0.438556, mean_q: 0.488729, mean_eps: 0.000000
 3609/5000: episode: 123, duration: 0.379s, episode steps:  30, steps per second:  79, episode reward: 38.803, mean reward:  1.293 [-3.000, 31.991], mean action: 2.400 [0.000, 14.000],  loss: 0.019706, mae: 0.439484, mean_q: 0.456851, mean_eps: 0.000000
 3627/5000: episode: 124, duration: 0.245s, episode steps:  18, steps per second:  74, episode reward: 41.307, mean reward:  2.295 [-2.255, 32.210], mean action: 3.667 [0.000, 19.000],  loss: 0.024167, mae: 0.450544, mean_q: 0.453294, mean_eps: 0.000000
 3650/5000: episode: 125, duration: 0.288s, episode steps:  23, steps per second:  80, episode reward: 43.774, mean reward:  1.903 [-2.020, 31.867], mean action: 2.217 [0.000, 19.000],  loss: 0.019634, mae: 0.430606, mean_q: 0.473198, mean_eps: 0.000000
 3676/5000: episode: 126, duration: 0.351s, episode steps:  26, steps per second:  74, episode reward: 42.000, mean reward:  1.615 [-2.212, 32.300], mean action: 2.308 [0.000, 11.000],  loss: 0.018836, mae: 0.434468, mean_q: 0.450586, mean_eps: 0.000000
 3715/5000: episode: 127, duration: 0.466s, episode steps:  39, steps per second:  84, episode reward: -32.140, mean reward: -0.824 [-32.004,  2.630], mean action: 7.821 [0.000, 21.000],  loss: 0.019696, mae: 0.433108, mean_q: 0.428491, mean_eps: 0.000000
 3752/5000: episode: 128, duration: 0.457s, episode steps:  37, steps per second:  81, episode reward: 44.198, mean reward:  1.195 [-2.016, 32.080], mean action: 8.838 [0.000, 19.000],  loss: 0.020607, mae: 0.439072, mean_q: 0.434776, mean_eps: 0.000000
 3770/5000: episode: 129, duration: 0.237s, episode steps:  18, steps per second:  76, episode reward: 44.461, mean reward:  2.470 [-2.034, 31.782], mean action: 4.278 [0.000, 19.000],  loss: 0.017758, mae: 0.425375, mean_q: 0.442445, mean_eps: 0.000000
 3782/5000: episode: 130, duration: 0.161s, episode steps:  12, steps per second:  75, episode reward: 45.000, mean reward:  3.750 [-2.266, 32.100], mean action: 4.667 [1.000, 19.000],  loss: 0.017671, mae: 0.422718, mean_q: 0.439746, mean_eps: 0.000000
 3804/5000: episode: 131, duration: 0.277s, episode steps:  22, steps per second:  79, episode reward: 44.649, mean reward:  2.029 [-2.031, 32.300], mean action: 4.273 [0.000, 19.000],  loss: 0.018133, mae: 0.423232, mean_q: 0.428423, mean_eps: 0.000000
 3830/5000: episode: 132, duration: 0.544s, episode steps:  26, steps per second:  48, episode reward: 41.142, mean reward:  1.582 [-3.000, 32.080], mean action: 2.192 [0.000, 14.000],  loss: 0.016798, mae: 0.414048, mean_q: 0.465022, mean_eps: 0.000000
 3852/5000: episode: 133, duration: 0.779s, episode steps:  22, steps per second:  28, episode reward: 35.900, mean reward:  1.632 [-3.000, 32.240], mean action: 2.636 [0.000, 15.000],  loss: 0.019499, mae: 0.428290, mean_q: 0.409454, mean_eps: 0.000000
 3898/5000: episode: 134, duration: 0.993s, episode steps:  46, steps per second:  46, episode reward: 35.463, mean reward:  0.771 [-2.725, 32.045], mean action: 3.957 [0.000, 19.000],  loss: 0.022964, mae: 0.438471, mean_q: 0.438856, mean_eps: 0.000000
 3928/5000: episode: 135, duration: 0.550s, episode steps:  30, steps per second:  55, episode reward: 41.210, mean reward:  1.374 [-2.217, 32.450], mean action: 3.100 [0.000, 15.000],  loss: 0.019020, mae: 0.414917, mean_q: 0.448194, mean_eps: 0.000000
 3960/5000: episode: 136, duration: 0.484s, episode steps:  32, steps per second:  66, episode reward: 38.162, mean reward:  1.193 [-2.194, 32.140], mean action: 6.344 [0.000, 15.000],  loss: 0.020788, mae: 0.415884, mean_q: 0.458365, mean_eps: 0.000000
 3977/5000: episode: 137, duration: 0.460s, episode steps:  17, steps per second:  37, episode reward: 41.763, mean reward:  2.457 [-2.137, 32.180], mean action: 3.824 [0.000, 19.000],  loss: 0.020500, mae: 0.414736, mean_q: 0.461717, mean_eps: 0.000000
 4003/5000: episode: 138, duration: 0.985s, episode steps:  26, steps per second:  26, episode reward: 43.451, mean reward:  1.671 [-2.695, 32.050], mean action: 2.654 [1.000, 14.000],  loss: 0.023058, mae: 0.423595, mean_q: 0.497432, mean_eps: 0.000000
 4025/5000: episode: 139, duration: 0.560s, episode steps:  22, steps per second:  39, episode reward: 38.710, mean reward:  1.760 [-2.837, 32.280], mean action: 4.636 [0.000, 19.000],  loss: 0.017575, mae: 0.405215, mean_q: 0.488456, mean_eps: 0.000000
 4045/5000: episode: 140, duration: 0.395s, episode steps:  20, steps per second:  51, episode reward: 41.805, mean reward:  2.090 [-3.000, 32.680], mean action: 3.700 [0.000, 13.000],  loss: 0.020947, mae: 0.428439, mean_q: 0.476780, mean_eps: 0.000000
 4088/5000: episode: 141, duration: 0.788s, episode steps:  43, steps per second:  55, episode reward: 34.680, mean reward:  0.807 [-2.285, 32.440], mean action: 3.186 [0.000, 14.000],  loss: 0.019288, mae: 0.414808, mean_q: 0.468201, mean_eps: 0.000000
 4115/5000: episode: 142, duration: 0.482s, episode steps:  27, steps per second:  56, episode reward: 41.143, mean reward:  1.524 [-2.682, 32.290], mean action: 4.852 [0.000, 15.000],  loss: 0.018037, mae: 0.408566, mean_q: 0.488683, mean_eps: 0.000000
 4136/5000: episode: 143, duration: 0.371s, episode steps:  21, steps per second:  57, episode reward: 43.464, mean reward:  2.070 [-2.302, 32.370], mean action: 5.524 [0.000, 15.000],  loss: 0.019415, mae: 0.410700, mean_q: 0.449508, mean_eps: 0.000000
 4157/5000: episode: 144, duration: 0.483s, episode steps:  21, steps per second:  43, episode reward: 47.461, mean reward:  2.260 [ 0.000, 32.130], mean action: 3.476 [0.000, 13.000],  loss: 0.017532, mae: 0.405236, mean_q: 0.419634, mean_eps: 0.000000
 4196/5000: episode: 145, duration: 0.639s, episode steps:  39, steps per second:  61, episode reward: 37.050, mean reward:  0.950 [-2.409, 32.436], mean action: 6.795 [0.000, 20.000],  loss: 0.017986, mae: 0.411957, mean_q: 0.408691, mean_eps: 0.000000
 4217/5000: episode: 146, duration: 0.379s, episode steps:  21, steps per second:  55, episode reward: 38.534, mean reward:  1.835 [-2.232, 32.300], mean action: 4.238 [0.000, 15.000],  loss: 0.021554, mae: 0.430038, mean_q: 0.412137, mean_eps: 0.000000
 4240/5000: episode: 147, duration: 0.332s, episode steps:  23, steps per second:  69, episode reward: 41.575, mean reward:  1.808 [-2.071, 32.162], mean action: 4.043 [0.000, 16.000],  loss: 0.013840, mae: 0.389308, mean_q: 0.427440, mean_eps: 0.000000
 4274/5000: episode: 148, duration: 0.425s, episode steps:  34, steps per second:  80, episode reward: 41.876, mean reward:  1.232 [-2.941, 32.190], mean action: 2.588 [0.000, 16.000],  loss: 0.021067, mae: 0.429295, mean_q: 0.473814, mean_eps: 0.000000
 4308/5000: episode: 149, duration: 0.411s, episode steps:  34, steps per second:  83, episode reward: 38.420, mean reward:  1.130 [-2.403, 31.811], mean action: 1.941 [0.000, 12.000],  loss: 0.018973, mae: 0.417432, mean_q: 0.445939, mean_eps: 0.000000
 4322/5000: episode: 150, duration: 0.178s, episode steps:  14, steps per second:  78, episode reward: 44.541, mean reward:  3.182 [-2.210, 31.621], mean action: 2.429 [0.000, 19.000],  loss: 0.017999, mae: 0.428410, mean_q: 0.387498, mean_eps: 0.000000
 4355/5000: episode: 151, duration: 0.405s, episode steps:  33, steps per second:  81, episode reward: 41.340, mean reward:  1.253 [-2.249, 32.240], mean action: 3.455 [0.000, 19.000],  loss: 0.018499, mae: 0.427879, mean_q: 0.392926, mean_eps: 0.000000
 4399/5000: episode: 152, duration: 0.527s, episode steps:  44, steps per second:  83, episode reward: 32.856, mean reward:  0.747 [-3.000, 32.602], mean action: 3.227 [0.000, 16.000],  loss: 0.018078, mae: 0.417474, mean_q: 0.477131, mean_eps: 0.000000
 4430/5000: episode: 153, duration: 0.467s, episode steps:  31, steps per second:  66, episode reward: 33.000, mean reward:  1.065 [-3.000, 29.080], mean action: 8.516 [0.000, 20.000],  loss: 0.019228, mae: 0.437814, mean_q: 0.462926, mean_eps: 0.000000
 4450/5000: episode: 154, duration: 0.421s, episode steps:  20, steps per second:  47, episode reward: 47.153, mean reward:  2.358 [-0.130, 32.200], mean action: 2.600 [0.000, 13.000],  loss: 0.022188, mae: 0.443934, mean_q: 0.413761, mean_eps: 0.000000
 4480/5000: episode: 155, duration: 0.377s, episode steps:  30, steps per second:  80, episode reward: 41.337, mean reward:  1.378 [-2.301, 32.360], mean action: 3.100 [0.000, 19.000],  loss: 0.021163, mae: 0.439000, mean_q: 0.482456, mean_eps: 0.000000
 4512/5000: episode: 156, duration: 0.393s, episode steps:  32, steps per second:  81, episode reward: 41.130, mean reward:  1.285 [-2.190, 32.160], mean action: 3.750 [0.000, 19.000],  loss: 0.021154, mae: 0.432640, mean_q: 0.494539, mean_eps: 0.000000
 4540/5000: episode: 157, duration: 0.334s, episode steps:  28, steps per second:  84, episode reward: 41.527, mean reward:  1.483 [-2.575, 32.320], mean action: 4.571 [0.000, 20.000],  loss: 0.019225, mae: 0.418446, mean_q: 0.423566, mean_eps: 0.000000
 4562/5000: episode: 158, duration: 0.277s, episode steps:  22, steps per second:  79, episode reward: 44.169, mean reward:  2.008 [-2.458, 32.150], mean action: 1.909 [0.000, 16.000],  loss: 0.022323, mae: 0.427099, mean_q: 0.451799, mean_eps: 0.000000
 4588/5000: episode: 159, duration: 0.308s, episode steps:  26, steps per second:  84, episode reward: 38.118, mean reward:  1.466 [-3.000, 32.442], mean action: 7.346 [1.000, 19.000],  loss: 0.018429, mae: 0.415859, mean_q: 0.447753, mean_eps: 0.000000
 4615/5000: episode: 160, duration: 0.640s, episode steps:  27, steps per second:  42, episode reward: 36.000, mean reward:  1.333 [-3.000, 32.020], mean action: 7.593 [0.000, 21.000],  loss: 0.017693, mae: 0.412789, mean_q: 0.469020, mean_eps: 0.000000
 4638/5000: episode: 161, duration: 0.286s, episode steps:  23, steps per second:  80, episode reward: 44.587, mean reward:  1.939 [-2.354, 32.141], mean action: 2.870 [0.000, 14.000],  loss: 0.019470, mae: 0.415625, mean_q: 0.455878, mean_eps: 0.000000
 4652/5000: episode: 162, duration: 0.186s, episode steps:  14, steps per second:  75, episode reward: 44.323, mean reward:  3.166 [-2.293, 32.420], mean action: 4.929 [0.000, 13.000],  loss: 0.022159, mae: 0.431733, mean_q: 0.488013, mean_eps: 0.000000
 4682/5000: episode: 163, duration: 0.367s, episode steps:  30, steps per second:  82, episode reward: 41.443, mean reward:  1.381 [-2.636, 32.150], mean action: 1.900 [0.000, 16.000],  loss: 0.017862, mae: 0.416622, mean_q: 0.434058, mean_eps: 0.000000
 4708/5000: episode: 164, duration: 0.761s, episode steps:  26, steps per second:  34, episode reward: 44.844, mean reward:  1.725 [-2.069, 32.290], mean action: 3.115 [1.000, 16.000],  loss: 0.019190, mae: 0.418299, mean_q: 0.459667, mean_eps: 0.000000
 4749/5000: episode: 165, duration: 1.338s, episode steps:  41, steps per second:  31, episode reward: 37.441, mean reward:  0.913 [-2.751, 32.060], mean action: 6.098 [0.000, 20.000],  loss: 0.016353, mae: 0.398103, mean_q: 0.410741, mean_eps: 0.000000
 4771/5000: episode: 166, duration: 0.559s, episode steps:  22, steps per second:  39, episode reward: 44.752, mean reward:  2.034 [-2.142, 32.070], mean action: 3.091 [0.000, 19.000],  loss: 0.019685, mae: 0.417746, mean_q: 0.407938, mean_eps: 0.000000
 4806/5000: episode: 167, duration: 0.542s, episode steps:  35, steps per second:  65, episode reward: 35.240, mean reward:  1.007 [-2.697, 32.090], mean action: 4.657 [0.000, 20.000],  loss: 0.020176, mae: 0.425911, mean_q: 0.404964, mean_eps: 0.000000
 4835/5000: episode: 168, duration: 0.706s, episode steps:  29, steps per second:  41, episode reward: 39.000, mean reward:  1.345 [-3.000, 32.090], mean action: 1.897 [0.000, 15.000],  loss: 0.020938, mae: 0.421867, mean_q: 0.461876, mean_eps: 0.000000
 4857/5000: episode: 169, duration: 0.388s, episode steps:  22, steps per second:  57, episode reward: 47.107, mean reward:  2.141 [-0.042, 32.250], mean action: 1.364 [0.000, 3.000],  loss: 0.023656, mae: 0.438432, mean_q: 0.473005, mean_eps: 0.000000
 4878/5000: episode: 170, duration: 0.466s, episode steps:  21, steps per second:  45, episode reward: 36.000, mean reward:  1.714 [-2.802, 29.448], mean action: 3.190 [0.000, 15.000],  loss: 0.025570, mae: 0.447089, mean_q: 0.415308, mean_eps: 0.000000
 4893/5000: episode: 171, duration: 0.286s, episode steps:  15, steps per second:  52, episode reward: 43.901, mean reward:  2.927 [-2.146, 32.788], mean action: 3.933 [0.000, 14.000],  loss: 0.021725, mae: 0.427577, mean_q: 0.432967, mean_eps: 0.000000
 4966/5000: episode: 172, duration: 1.168s, episode steps:  73, steps per second:  63, episode reward: 32.424, mean reward:  0.444 [-3.000, 32.600], mean action: 4.274 [0.000, 19.000],  loss: 0.020363, mae: 0.422160, mean_q: 0.465748, mean_eps: 0.000000
done, took 68.346 seconds
DQN Evaluation: 2962 victories out of 3525 episodes
Training for 5000 steps ...
   25/5000: episode: 1, duration: 0.165s, episode steps:  25, steps per second: 151, episode reward: -32.820, mean reward: -1.313 [-32.142,  2.901], mean action: 10.160 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   48/5000: episode: 2, duration: 0.154s, episode steps:  23, steps per second: 149, episode reward: 35.580, mean reward:  1.547 [-2.393, 32.201], mean action: 3.870 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   81/5000: episode: 3, duration: 0.214s, episode steps:  33, steps per second: 154, episode reward: -32.770, mean reward: -0.993 [-32.245,  2.870], mean action: 7.030 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  103/5000: episode: 4, duration: 0.148s, episode steps:  22, steps per second: 149, episode reward: 38.528, mean reward:  1.751 [-3.000, 33.000], mean action: 5.227 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  125/5000: episode: 5, duration: 0.145s, episode steps:  22, steps per second: 151, episode reward: 36.000, mean reward:  1.636 [-3.000, 32.060], mean action: 11.864 [1.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  153/5000: episode: 6, duration: 0.166s, episode steps:  28, steps per second: 168, episode reward: 34.524, mean reward:  1.233 [-3.000, 32.837], mean action: 8.179 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  171/5000: episode: 7, duration: 0.124s, episode steps:  18, steps per second: 145, episode reward: 38.498, mean reward:  2.139 [-2.437, 32.220], mean action: 5.167 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  200/5000: episode: 8, duration: 0.182s, episode steps:  29, steps per second: 160, episode reward: 32.336, mean reward:  1.115 [-2.443, 32.060], mean action: 7.345 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  221/5000: episode: 9, duration: 0.139s, episode steps:  21, steps per second: 151, episode reward: 37.853, mean reward:  1.803 [-2.626, 32.250], mean action: 6.952 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  239/5000: episode: 10, duration: 0.126s, episode steps:  18, steps per second: 143, episode reward: 38.901, mean reward:  2.161 [-2.518, 33.000], mean action: 4.389 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  265/5000: episode: 11, duration: 0.175s, episode steps:  26, steps per second: 149, episode reward: -36.000, mean reward: -1.385 [-32.411,  2.777], mean action: 9.577 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  294/5000: episode: 12, duration: 0.190s, episode steps:  29, steps per second: 152, episode reward: -32.340, mean reward: -1.115 [-32.293,  3.000], mean action: 6.552 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  315/5000: episode: 13, duration: 0.143s, episode steps:  21, steps per second: 147, episode reward: -32.540, mean reward: -1.550 [-32.383,  3.000], mean action: 6.857 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  342/5000: episode: 14, duration: 0.166s, episode steps:  27, steps per second: 163, episode reward: 32.516, mean reward:  1.204 [-3.000, 32.460], mean action: 4.630 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  369/5000: episode: 15, duration: 0.170s, episode steps:  27, steps per second: 159, episode reward: 33.000, mean reward:  1.222 [-2.289, 32.540], mean action: 6.222 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  387/5000: episode: 16, duration: 0.125s, episode steps:  18, steps per second: 144, episode reward: -32.420, mean reward: -1.801 [-32.316,  2.621], mean action: 6.111 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  415/5000: episode: 17, duration: 0.171s, episode steps:  28, steps per second: 163, episode reward: 32.656, mean reward:  1.166 [-2.900, 32.200], mean action: 7.893 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  439/5000: episode: 18, duration: 0.145s, episode steps:  24, steps per second: 166, episode reward: -32.870, mean reward: -1.370 [-32.012,  3.000], mean action: 9.000 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  466/5000: episode: 19, duration: 0.185s, episode steps:  27, steps per second: 146, episode reward: 36.000, mean reward:  1.333 [-3.000, 32.340], mean action: 4.926 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  491/5000: episode: 20, duration: 0.162s, episode steps:  25, steps per second: 154, episode reward: 38.696, mean reward:  1.548 [-2.548, 32.106], mean action: 5.800 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  531/5000: episode: 21, duration: 0.236s, episode steps:  40, steps per second: 170, episode reward: -35.520, mean reward: -0.888 [-32.448,  2.569], mean action: 7.900 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  554/5000: episode: 22, duration: 0.196s, episode steps:  23, steps per second: 117, episode reward: -35.230, mean reward: -1.532 [-32.208,  2.560], mean action: 9.043 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  581/5000: episode: 23, duration: 0.180s, episode steps:  27, steps per second: 150, episode reward: 37.952, mean reward:  1.406 [-2.998, 32.079], mean action: 7.815 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  607/5000: episode: 24, duration: 0.165s, episode steps:  26, steps per second: 157, episode reward: 38.770, mean reward:  1.491 [-3.000, 33.000], mean action: 3.923 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  624/5000: episode: 25, duration: 0.114s, episode steps:  17, steps per second: 149, episode reward: 42.000, mean reward:  2.471 [-2.615, 32.350], mean action: 3.059 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  652/5000: episode: 26, duration: 0.180s, episode steps:  28, steps per second: 156, episode reward: 39.573, mean reward:  1.413 [-2.188, 32.433], mean action: 7.107 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  680/5000: episode: 27, duration: 0.189s, episode steps:  28, steps per second: 148, episode reward: -32.220, mean reward: -1.151 [-31.842,  2.440], mean action: 9.607 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  707/5000: episode: 28, duration: 0.170s, episode steps:  27, steps per second: 159, episode reward: -32.540, mean reward: -1.205 [-32.210,  2.482], mean action: 6.407 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  724/5000: episode: 29, duration: 0.116s, episode steps:  17, steps per second: 146, episode reward: 40.475, mean reward:  2.381 [-2.779, 31.647], mean action: 6.941 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  745/5000: episode: 30, duration: 0.145s, episode steps:  21, steps per second: 145, episode reward: -41.710, mean reward: -1.986 [-32.613,  2.413], mean action: 7.524 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  768/5000: episode: 31, duration: 0.150s, episode steps:  23, steps per second: 153, episode reward: 35.626, mean reward:  1.549 [-2.805, 32.810], mean action: 5.696 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  798/5000: episode: 32, duration: 0.196s, episode steps:  30, steps per second: 153, episode reward: 42.000, mean reward:  1.400 [-2.313, 32.260], mean action: 3.500 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  824/5000: episode: 33, duration: 0.197s, episode steps:  26, steps per second: 132, episode reward: 36.000, mean reward:  1.385 [-2.380, 32.220], mean action: 5.231 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  846/5000: episode: 34, duration: 0.159s, episode steps:  22, steps per second: 139, episode reward: 36.000, mean reward:  1.636 [-2.442, 30.160], mean action: 5.136 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  865/5000: episode: 35, duration: 0.132s, episode steps:  19, steps per second: 144, episode reward: 35.577, mean reward:  1.872 [-2.447, 33.000], mean action: 5.842 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  906/5000: episode: 36, duration: 0.261s, episode steps:  41, steps per second: 157, episode reward: 32.497, mean reward:  0.793 [-2.529, 31.945], mean action: 11.171 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  935/5000: episode: 37, duration: 0.194s, episode steps:  29, steps per second: 150, episode reward: 32.437, mean reward:  1.119 [-3.000, 31.857], mean action: 7.310 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  961/5000: episode: 38, duration: 0.160s, episode steps:  26, steps per second: 162, episode reward: -32.660, mean reward: -1.256 [-32.415,  3.000], mean action: 6.885 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1024/5000: episode: 39, duration: 0.517s, episode steps:  63, steps per second: 122, episode reward: 32.302, mean reward:  0.513 [-2.542, 32.210], mean action: 13.095 [0.000, 21.000],  loss: 0.018207, mae: 0.413185, mean_q: 0.413121, mean_eps: 0.000000
 1052/5000: episode: 40, duration: 0.351s, episode steps:  28, steps per second:  80, episode reward: -32.360, mean reward: -1.156 [-32.570,  2.447], mean action: 7.821 [0.000, 20.000],  loss: 0.017629, mae: 0.404896, mean_q: 0.454534, mean_eps: 0.000000
 1077/5000: episode: 41, duration: 0.313s, episode steps:  25, steps per second:  80, episode reward: 33.000, mean reward:  1.320 [-2.853, 32.320], mean action: 4.360 [0.000, 19.000],  loss: 0.020945, mae: 0.416694, mean_q: 0.476568, mean_eps: 0.000000
 1102/5000: episode: 42, duration: 0.307s, episode steps:  25, steps per second:  81, episode reward: 32.368, mean reward:  1.295 [-3.000, 32.300], mean action: 5.520 [0.000, 19.000],  loss: 0.020855, mae: 0.423080, mean_q: 0.458918, mean_eps: 0.000000
 1126/5000: episode: 43, duration: 0.297s, episode steps:  24, steps per second:  81, episode reward: 33.000, mean reward:  1.375 [-2.424, 32.480], mean action: 3.583 [0.000, 16.000],  loss: 0.024535, mae: 0.449985, mean_q: 0.389871, mean_eps: 0.000000
 1151/5000: episode: 44, duration: 0.306s, episode steps:  25, steps per second:  82, episode reward: 32.173, mean reward:  1.287 [-2.763, 31.827], mean action: 7.520 [0.000, 21.000],  loss: 0.021569, mae: 0.440730, mean_q: 0.396855, mean_eps: 0.000000
 1187/5000: episode: 45, duration: 0.435s, episode steps:  36, steps per second:  83, episode reward: -32.550, mean reward: -0.904 [-32.195,  2.562], mean action: 9.806 [0.000, 21.000],  loss: 0.018914, mae: 0.421699, mean_q: 0.465168, mean_eps: 0.000000
 1213/5000: episode: 46, duration: 0.321s, episode steps:  26, steps per second:  81, episode reward: 32.613, mean reward:  1.254 [-3.000, 32.350], mean action: 6.154 [0.000, 14.000],  loss: 0.018659, mae: 0.418614, mean_q: 0.507650, mean_eps: 0.000000
 1232/5000: episode: 47, duration: 0.246s, episode steps:  19, steps per second:  77, episode reward: 36.000, mean reward:  1.895 [-3.000, 32.030], mean action: 3.895 [0.000, 12.000],  loss: 0.021086, mae: 0.433921, mean_q: 0.490117, mean_eps: 0.000000
 1260/5000: episode: 48, duration: 0.347s, episode steps:  28, steps per second:  81, episode reward: 35.726, mean reward:  1.276 [-3.000, 32.150], mean action: 5.214 [0.000, 16.000],  loss: 0.017516, mae: 0.418501, mean_q: 0.425445, mean_eps: 0.000000
 1285/5000: episode: 49, duration: 0.311s, episode steps:  25, steps per second:  80, episode reward: 38.234, mean reward:  1.529 [-2.769, 31.980], mean action: 2.600 [0.000, 14.000],  loss: 0.017097, mae: 0.411000, mean_q: 0.469341, mean_eps: 0.000000
 1308/5000: episode: 50, duration: 0.300s, episode steps:  23, steps per second:  77, episode reward: 44.196, mean reward:  1.922 [-2.553, 32.224], mean action: 0.913 [0.000, 9.000],  loss: 0.019900, mae: 0.423136, mean_q: 0.432045, mean_eps: 0.000000
 1336/5000: episode: 51, duration: 0.342s, episode steps:  28, steps per second:  82, episode reward: 43.767, mean reward:  1.563 [-2.257, 32.348], mean action: 2.571 [0.000, 20.000],  loss: 0.020724, mae: 0.424346, mean_q: 0.427951, mean_eps: 0.000000
 1353/5000: episode: 52, duration: 0.271s, episode steps:  17, steps per second:  63, episode reward: 40.867, mean reward:  2.404 [-2.882, 33.000], mean action: 7.353 [0.000, 18.000],  loss: 0.013396, mae: 0.392310, mean_q: 0.413339, mean_eps: 0.000000
 1378/5000: episode: 53, duration: 0.336s, episode steps:  25, steps per second:  75, episode reward: -32.520, mean reward: -1.301 [-32.011,  2.390], mean action: 4.360 [0.000, 19.000],  loss: 0.023229, mae: 0.430653, mean_q: 0.454498, mean_eps: 0.000000
 1394/5000: episode: 54, duration: 0.236s, episode steps:  16, steps per second:  68, episode reward: 41.448, mean reward:  2.591 [-3.000, 31.987], mean action: 4.312 [0.000, 14.000],  loss: 0.020622, mae: 0.414536, mean_q: 0.494484, mean_eps: 0.000000
 1416/5000: episode: 55, duration: 0.292s, episode steps:  22, steps per second:  75, episode reward: 37.939, mean reward:  1.725 [-2.472, 32.193], mean action: 5.227 [0.000, 21.000],  loss: 0.019313, mae: 0.416270, mean_q: 0.500211, mean_eps: 0.000000
 1440/5000: episode: 56, duration: 0.300s, episode steps:  24, steps per second:  80, episode reward: 32.621, mean reward:  1.359 [-2.726, 32.420], mean action: 5.917 [0.000, 14.000],  loss: 0.018521, mae: 0.415232, mean_q: 0.477284, mean_eps: 0.000000
 1464/5000: episode: 57, duration: 0.309s, episode steps:  24, steps per second:  78, episode reward: -32.150, mean reward: -1.340 [-32.203,  2.840], mean action: 5.375 [0.000, 14.000],  loss: 0.023101, mae: 0.432640, mean_q: 0.479495, mean_eps: 0.000000
 1493/5000: episode: 58, duration: 0.431s, episode steps:  29, steps per second:  67, episode reward: 37.264, mean reward:  1.285 [-2.343, 31.636], mean action: 4.069 [0.000, 13.000],  loss: 0.020378, mae: 0.413939, mean_q: 0.502440, mean_eps: 0.000000
 1515/5000: episode: 59, duration: 0.327s, episode steps:  22, steps per second:  67, episode reward: -32.600, mean reward: -1.482 [-31.702,  2.903], mean action: 6.909 [0.000, 19.000],  loss: 0.020887, mae: 0.418601, mean_q: 0.450926, mean_eps: 0.000000
 1542/5000: episode: 60, duration: 0.336s, episode steps:  27, steps per second:  80, episode reward: 32.480, mean reward:  1.203 [-2.628, 32.120], mean action: 8.704 [0.000, 21.000],  loss: 0.018940, mae: 0.409392, mean_q: 0.431935, mean_eps: 0.000000
 1557/5000: episode: 61, duration: 0.193s, episode steps:  15, steps per second:  78, episode reward: 41.054, mean reward:  2.737 [-2.384, 31.942], mean action: 1.333 [0.000, 9.000],  loss: 0.021881, mae: 0.431852, mean_q: 0.404667, mean_eps: 0.000000
 1578/5000: episode: 62, duration: 0.259s, episode steps:  21, steps per second:  81, episode reward: 33.000, mean reward:  1.571 [-2.605, 32.680], mean action: 3.571 [0.000, 19.000],  loss: 0.021313, mae: 0.424207, mean_q: 0.388434, mean_eps: 0.000000
 1599/5000: episode: 63, duration: 0.258s, episode steps:  21, steps per second:  82, episode reward: 35.165, mean reward:  1.675 [-3.000, 32.320], mean action: 7.333 [0.000, 16.000],  loss: 0.019784, mae: 0.418113, mean_q: 0.413187, mean_eps: 0.000000
 1636/5000: episode: 64, duration: 0.701s, episode steps:  37, steps per second:  53, episode reward: 35.559, mean reward:  0.961 [-2.455, 31.992], mean action: 3.081 [0.000, 16.000],  loss: 0.018490, mae: 0.407617, mean_q: 0.452452, mean_eps: 0.000000
 1661/5000: episode: 65, duration: 0.369s, episode steps:  25, steps per second:  68, episode reward: 35.121, mean reward:  1.405 [-2.572, 32.143], mean action: 8.560 [0.000, 16.000],  loss: 0.017805, mae: 0.401124, mean_q: 0.466954, mean_eps: 0.000000
 1689/5000: episode: 66, duration: 0.358s, episode steps:  28, steps per second:  78, episode reward: -33.000, mean reward: -1.179 [-32.622,  2.805], mean action: 3.821 [0.000, 14.000],  loss: 0.019297, mae: 0.414618, mean_q: 0.435141, mean_eps: 0.000000
 1718/5000: episode: 67, duration: 0.358s, episode steps:  29, steps per second:  81, episode reward: 37.384, mean reward:  1.289 [-2.286, 32.120], mean action: 6.621 [0.000, 20.000],  loss: 0.020250, mae: 0.421577, mean_q: 0.455224, mean_eps: 0.000000
 1734/5000: episode: 68, duration: 0.207s, episode steps:  16, steps per second:  77, episode reward: 38.420, mean reward:  2.401 [-3.000, 32.609], mean action: 5.188 [0.000, 15.000],  loss: 0.022215, mae: 0.429975, mean_q: 0.432796, mean_eps: 0.000000
 1761/5000: episode: 69, duration: 0.327s, episode steps:  27, steps per second:  83, episode reward: 35.423, mean reward:  1.312 [-3.000, 32.420], mean action: 4.926 [0.000, 20.000],  loss: 0.024039, mae: 0.437551, mean_q: 0.446014, mean_eps: 0.000000
 1788/5000: episode: 70, duration: 0.426s, episode steps:  27, steps per second:  63, episode reward: 41.130, mean reward:  1.523 [-2.091, 32.140], mean action: 5.407 [0.000, 19.000],  loss: 0.017571, mae: 0.415943, mean_q: 0.457010, mean_eps: 0.000000
 1805/5000: episode: 71, duration: 0.357s, episode steps:  17, steps per second:  48, episode reward: 38.902, mean reward:  2.288 [-2.699, 32.232], mean action: 4.706 [0.000, 19.000],  loss: 0.020262, mae: 0.423145, mean_q: 0.477694, mean_eps: 0.000000
 1822/5000: episode: 72, duration: 0.363s, episode steps:  17, steps per second:  47, episode reward: 37.382, mean reward:  2.199 [-3.000, 32.791], mean action: 8.941 [0.000, 19.000],  loss: 0.022728, mae: 0.430363, mean_q: 0.485220, mean_eps: 0.000000
 1844/5000: episode: 73, duration: 0.313s, episode steps:  22, steps per second:  70, episode reward: 35.575, mean reward:  1.617 [-2.800, 32.180], mean action: 4.909 [0.000, 19.000],  loss: 0.017935, mae: 0.405293, mean_q: 0.539006, mean_eps: 0.000000
 1882/5000: episode: 74, duration: 0.457s, episode steps:  38, steps per second:  83, episode reward: 32.186, mean reward:  0.847 [-2.283, 32.106], mean action: 5.105 [0.000, 19.000],  loss: 0.022570, mae: 0.429589, mean_q: 0.512282, mean_eps: 0.000000
 1905/5000: episode: 75, duration: 0.278s, episode steps:  23, steps per second:  83, episode reward: 36.000, mean reward:  1.565 [-2.806, 32.320], mean action: 6.130 [0.000, 19.000],  loss: 0.021377, mae: 0.427296, mean_q: 0.479094, mean_eps: 0.000000
 1937/5000: episode: 76, duration: 0.380s, episode steps:  32, steps per second:  84, episode reward: 32.604, mean reward:  1.019 [-3.000, 32.400], mean action: 10.250 [0.000, 20.000],  loss: 0.022412, mae: 0.434410, mean_q: 0.474324, mean_eps: 0.000000
 1964/5000: episode: 77, duration: 0.360s, episode steps:  27, steps per second:  75, episode reward: 32.697, mean reward:  1.211 [-2.901, 31.827], mean action: 4.556 [0.000, 20.000],  loss: 0.019190, mae: 0.413045, mean_q: 0.461915, mean_eps: 0.000000
 1989/5000: episode: 78, duration: 0.472s, episode steps:  25, steps per second:  53, episode reward: 38.066, mean reward:  1.523 [-2.538, 32.244], mean action: 3.680 [0.000, 21.000],  loss: 0.023031, mae: 0.439754, mean_q: 0.410776, mean_eps: 0.000000
 2011/5000: episode: 79, duration: 0.703s, episode steps:  22, steps per second:  31, episode reward: 35.600, mean reward:  1.618 [-2.496, 32.329], mean action: 4.364 [0.000, 21.000],  loss: 0.019831, mae: 0.418001, mean_q: 0.426681, mean_eps: 0.000000
 2035/5000: episode: 80, duration: 0.835s, episode steps:  24, steps per second:  29, episode reward: -35.830, mean reward: -1.493 [-32.302,  2.458], mean action: 7.375 [0.000, 20.000],  loss: 0.019657, mae: 0.425297, mean_q: 0.443703, mean_eps: 0.000000
 2060/5000: episode: 81, duration: 0.727s, episode steps:  25, steps per second:  34, episode reward: -38.150, mean reward: -1.526 [-32.454,  2.480], mean action: 5.880 [0.000, 16.000],  loss: 0.019347, mae: 0.412923, mean_q: 0.433197, mean_eps: 0.000000
 2088/5000: episode: 82, duration: 0.415s, episode steps:  28, steps per second:  67, episode reward: 35.709, mean reward:  1.275 [-3.000, 31.889], mean action: 4.321 [0.000, 19.000],  loss: 0.020423, mae: 0.423346, mean_q: 0.478781, mean_eps: 0.000000
 2112/5000: episode: 83, duration: 0.702s, episode steps:  24, steps per second:  34, episode reward: 35.189, mean reward:  1.466 [-2.573, 32.409], mean action: 5.750 [0.000, 19.000],  loss: 0.018356, mae: 0.413271, mean_q: 0.445944, mean_eps: 0.000000
 2140/5000: episode: 84, duration: 0.582s, episode steps:  28, steps per second:  48, episode reward: 35.223, mean reward:  1.258 [-2.385, 32.730], mean action: 5.000 [0.000, 20.000],  loss: 0.017183, mae: 0.418838, mean_q: 0.430080, mean_eps: 0.000000
 2166/5000: episode: 85, duration: 0.420s, episode steps:  26, steps per second:  62, episode reward: 38.316, mean reward:  1.474 [-2.372, 32.220], mean action: 5.615 [0.000, 21.000],  loss: 0.017620, mae: 0.418308, mean_q: 0.445480, mean_eps: 0.000000
 2213/5000: episode: 86, duration: 0.637s, episode steps:  47, steps per second:  74, episode reward: 34.984, mean reward:  0.744 [-3.000, 31.970], mean action: 12.340 [0.000, 20.000],  loss: 0.019953, mae: 0.421885, mean_q: 0.460463, mean_eps: 0.000000
 2231/5000: episode: 87, duration: 0.224s, episode steps:  18, steps per second:  80, episode reward: 37.569, mean reward:  2.087 [-2.513, 33.000], mean action: 6.278 [0.000, 20.000],  loss: 0.021276, mae: 0.432325, mean_q: 0.457767, mean_eps: 0.000000
 2252/5000: episode: 88, duration: 0.282s, episode steps:  21, steps per second:  75, episode reward: 35.719, mean reward:  1.701 [-2.662, 32.719], mean action: 2.905 [0.000, 16.000],  loss: 0.020371, mae: 0.417438, mean_q: 0.498863, mean_eps: 0.000000
 2362/5000: episode: 89, duration: 1.786s, episode steps: 110, steps per second:  62, episode reward: -42.000, mean reward: -0.382 [-32.136,  2.215], mean action: 12.564 [0.000, 21.000],  loss: 0.021499, mae: 0.421646, mean_q: 0.497926, mean_eps: 0.000000
 2396/5000: episode: 90, duration: 0.544s, episode steps:  34, steps per second:  62, episode reward: 38.541, mean reward:  1.134 [-3.000, 32.220], mean action: 5.529 [0.000, 20.000],  loss: 0.018384, mae: 0.407767, mean_q: 0.449630, mean_eps: 0.000000
 2448/5000: episode: 91, duration: 0.933s, episode steps:  52, steps per second:  56, episode reward: -32.430, mean reward: -0.624 [-32.021,  2.620], mean action: 9.923 [1.000, 16.000],  loss: 0.020209, mae: 0.418747, mean_q: 0.458391, mean_eps: 0.000000
 2470/5000: episode: 92, duration: 0.303s, episode steps:  22, steps per second:  73, episode reward: -36.000, mean reward: -1.636 [-32.647,  2.471], mean action: 7.136 [0.000, 19.000],  loss: 0.021612, mae: 0.422271, mean_q: 0.468646, mean_eps: 0.000000
 2499/5000: episode: 93, duration: 0.473s, episode steps:  29, steps per second:  61, episode reward: 36.000, mean reward:  1.241 [-2.636, 32.130], mean action: 3.759 [1.000, 16.000],  loss: 0.020156, mae: 0.411902, mean_q: 0.476153, mean_eps: 0.000000
 2532/5000: episode: 94, duration: 0.621s, episode steps:  33, steps per second:  53, episode reward: -32.320, mean reward: -0.979 [-31.837,  2.460], mean action: 4.636 [0.000, 16.000],  loss: 0.019891, mae: 0.420534, mean_q: 0.493150, mean_eps: 0.000000
 2561/5000: episode: 95, duration: 0.567s, episode steps:  29, steps per second:  51, episode reward: 35.801, mean reward:  1.235 [-2.441, 32.060], mean action: 4.379 [0.000, 20.000],  loss: 0.021283, mae: 0.431571, mean_q: 0.515892, mean_eps: 0.000000
 2584/5000: episode: 96, duration: 0.299s, episode steps:  23, steps per second:  77, episode reward: 38.568, mean reward:  1.677 [-2.231, 32.042], mean action: 4.870 [0.000, 16.000],  loss: 0.018254, mae: 0.430366, mean_q: 0.428175, mean_eps: 0.000000
 2605/5000: episode: 97, duration: 0.274s, episode steps:  21, steps per second:  77, episode reward: 40.795, mean reward:  1.943 [-2.079, 32.300], mean action: 2.905 [0.000, 16.000],  loss: 0.019936, mae: 0.439594, mean_q: 0.425082, mean_eps: 0.000000
 2641/5000: episode: 98, duration: 0.627s, episode steps:  36, steps per second:  57, episode reward: -32.680, mean reward: -0.908 [-32.095,  2.440], mean action: 8.778 [0.000, 20.000],  loss: 0.019469, mae: 0.455336, mean_q: 0.347960, mean_eps: 0.000000
 2672/5000: episode: 99, duration: 0.480s, episode steps:  31, steps per second:  65, episode reward: -33.000, mean reward: -1.065 [-32.673,  3.000], mean action: 7.194 [0.000, 17.000],  loss: 0.017626, mae: 0.419216, mean_q: 0.425487, mean_eps: 0.000000
 2729/5000: episode: 100, duration: 0.874s, episode steps:  57, steps per second:  65, episode reward: 32.888, mean reward:  0.577 [-3.000, 32.208], mean action: 10.825 [0.000, 17.000],  loss: 0.018111, mae: 0.416808, mean_q: 0.429685, mean_eps: 0.000000
 2750/5000: episode: 101, duration: 0.452s, episode steps:  21, steps per second:  46, episode reward: 35.462, mean reward:  1.689 [-2.901, 32.111], mean action: 5.857 [0.000, 19.000],  loss: 0.023129, mae: 0.422357, mean_q: 0.453979, mean_eps: 0.000000
 2767/5000: episode: 102, duration: 0.395s, episode steps:  17, steps per second:  43, episode reward: 41.293, mean reward:  2.429 [-2.615, 32.334], mean action: 3.588 [0.000, 19.000],  loss: 0.020916, mae: 0.403031, mean_q: 0.472880, mean_eps: 0.000000
 2781/5000: episode: 103, duration: 0.216s, episode steps:  14, steps per second:  65, episode reward: 46.330, mean reward:  3.309 [-0.748, 32.798], mean action: 4.857 [0.000, 14.000],  loss: 0.020009, mae: 0.403785, mean_q: 0.506548, mean_eps: 0.000000
 2811/5000: episode: 104, duration: 0.398s, episode steps:  30, steps per second:  75, episode reward: 35.720, mean reward:  1.191 [-2.344, 32.720], mean action: 5.867 [0.000, 19.000],  loss: 0.019573, mae: 0.412251, mean_q: 0.460404, mean_eps: 0.000000
 2836/5000: episode: 105, duration: 0.328s, episode steps:  25, steps per second:  76, episode reward: -32.510, mean reward: -1.300 [-32.261,  3.000], mean action: 4.600 [0.000, 19.000],  loss: 0.021117, mae: 0.421921, mean_q: 0.432131, mean_eps: 0.000000
 2861/5000: episode: 106, duration: 0.470s, episode steps:  25, steps per second:  53, episode reward: 32.707, mean reward:  1.308 [-2.306, 32.167], mean action: 4.400 [0.000, 19.000],  loss: 0.020229, mae: 0.409665, mean_q: 0.508577, mean_eps: 0.000000
 2907/5000: episode: 107, duration: 0.670s, episode steps:  46, steps per second:  69, episode reward: -32.890, mean reward: -0.715 [-32.362,  2.481], mean action: 5.478 [0.000, 20.000],  loss: 0.019619, mae: 0.411114, mean_q: 0.475594, mean_eps: 0.000000
 2942/5000: episode: 108, duration: 0.462s, episode steps:  35, steps per second:  76, episode reward: -32.280, mean reward: -0.922 [-32.064,  2.251], mean action: 7.229 [0.000, 19.000],  loss: 0.020203, mae: 0.416237, mean_q: 0.479667, mean_eps: 0.000000
 2977/5000: episode: 109, duration: 0.615s, episode steps:  35, steps per second:  57, episode reward: -35.530, mean reward: -1.015 [-32.281,  2.400], mean action: 6.600 [0.000, 17.000],  loss: 0.017123, mae: 0.409296, mean_q: 0.504502, mean_eps: 0.000000
 3008/5000: episode: 110, duration: 0.422s, episode steps:  31, steps per second:  74, episode reward: 37.667, mean reward:  1.215 [-2.682, 32.745], mean action: 5.355 [0.000, 21.000],  loss: 0.019137, mae: 0.418126, mean_q: 0.475224, mean_eps: 0.000000
 3029/5000: episode: 111, duration: 0.292s, episode steps:  21, steps per second:  72, episode reward: 36.000, mean reward:  1.714 [-2.494, 30.931], mean action: 3.667 [0.000, 16.000],  loss: 0.016409, mae: 0.407051, mean_q: 0.426148, mean_eps: 0.000000
 3059/5000: episode: 112, duration: 0.408s, episode steps:  30, steps per second:  74, episode reward: 32.048, mean reward:  1.068 [-2.522, 32.090], mean action: 3.567 [0.000, 19.000],  loss: 0.022239, mae: 0.431010, mean_q: 0.449132, mean_eps: 0.000000
 3081/5000: episode: 113, duration: 0.491s, episode steps:  22, steps per second:  45, episode reward: 37.657, mean reward:  1.712 [-2.697, 32.516], mean action: 6.591 [0.000, 21.000],  loss: 0.018091, mae: 0.407928, mean_q: 0.459303, mean_eps: 0.000000
 3104/5000: episode: 114, duration: 0.507s, episode steps:  23, steps per second:  45, episode reward: 35.343, mean reward:  1.537 [-2.590, 31.927], mean action: 2.913 [0.000, 11.000],  loss: 0.020850, mae: 0.413022, mean_q: 0.483093, mean_eps: 0.000000
 3126/5000: episode: 115, duration: 0.338s, episode steps:  22, steps per second:  65, episode reward: 32.295, mean reward:  1.468 [-3.000, 32.344], mean action: 5.818 [0.000, 19.000],  loss: 0.017986, mae: 0.402384, mean_q: 0.432324, mean_eps: 0.000000
 3155/5000: episode: 116, duration: 0.558s, episode steps:  29, steps per second:  52, episode reward: 36.000, mean reward:  1.241 [-2.903, 32.660], mean action: 3.483 [0.000, 21.000],  loss: 0.020026, mae: 0.415134, mean_q: 0.427465, mean_eps: 0.000000
 3179/5000: episode: 117, duration: 0.533s, episode steps:  24, steps per second:  45, episode reward: 35.875, mean reward:  1.495 [-2.854, 32.155], mean action: 2.667 [0.000, 11.000],  loss: 0.019046, mae: 0.410700, mean_q: 0.466886, mean_eps: 0.000000
 3205/5000: episode: 118, duration: 0.518s, episode steps:  26, steps per second:  50, episode reward: -32.160, mean reward: -1.237 [-32.063,  2.250], mean action: 2.731 [0.000, 12.000],  loss: 0.018111, mae: 0.421126, mean_q: 0.424656, mean_eps: 0.000000
 3224/5000: episode: 119, duration: 0.274s, episode steps:  19, steps per second:  69, episode reward: 38.375, mean reward:  2.020 [-3.000, 32.880], mean action: 4.842 [0.000, 15.000],  loss: 0.018372, mae: 0.421612, mean_q: 0.417450, mean_eps: 0.000000
 3252/5000: episode: 120, duration: 0.391s, episode steps:  28, steps per second:  72, episode reward: 36.000, mean reward:  1.286 [-2.327, 32.100], mean action: 5.500 [0.000, 20.000],  loss: 0.019460, mae: 0.427089, mean_q: 0.434219, mean_eps: 0.000000
 3273/5000: episode: 121, duration: 0.360s, episode steps:  21, steps per second:  58, episode reward: 41.060, mean reward:  1.955 [-2.272, 32.532], mean action: 3.571 [0.000, 19.000],  loss: 0.018508, mae: 0.424462, mean_q: 0.440863, mean_eps: 0.000000
 3291/5000: episode: 122, duration: 0.460s, episode steps:  18, steps per second:  39, episode reward: 38.429, mean reward:  2.135 [-3.000, 32.667], mean action: 4.389 [0.000, 19.000],  loss: 0.018934, mae: 0.423080, mean_q: 0.438630, mean_eps: 0.000000
 3306/5000: episode: 123, duration: 0.316s, episode steps:  15, steps per second:  47, episode reward: 36.000, mean reward:  2.400 [-3.000, 32.410], mean action: 4.267 [0.000, 19.000],  loss: 0.019681, mae: 0.421563, mean_q: 0.465052, mean_eps: 0.000000
 3339/5000: episode: 124, duration: 1.262s, episode steps:  33, steps per second:  26, episode reward: 32.518, mean reward:  0.985 [-2.507, 32.877], mean action: 2.606 [0.000, 19.000],  loss: 0.021853, mae: 0.435889, mean_q: 0.470087, mean_eps: 0.000000
 3372/5000: episode: 125, duration: 1.301s, episode steps:  33, steps per second:  25, episode reward: -38.520, mean reward: -1.167 [-32.198,  2.250], mean action: 6.242 [0.000, 19.000],  loss: 0.016126, mae: 0.422120, mean_q: 0.411744, mean_eps: 0.000000
 3389/5000: episode: 126, duration: 0.378s, episode steps:  17, steps per second:  45, episode reward: 38.713, mean reward:  2.277 [-2.903, 32.240], mean action: 7.529 [0.000, 19.000],  loss: 0.018540, mae: 0.429945, mean_q: 0.390516, mean_eps: 0.000000
 3409/5000: episode: 127, duration: 0.391s, episode steps:  20, steps per second:  51, episode reward: -33.000, mean reward: -1.650 [-33.000,  2.630], mean action: 6.800 [0.000, 21.000],  loss: 0.019510, mae: 0.426905, mean_q: 0.432634, mean_eps: 0.000000
 3431/5000: episode: 128, duration: 0.362s, episode steps:  22, steps per second:  61, episode reward: 35.642, mean reward:  1.620 [-3.000, 32.280], mean action: 4.773 [0.000, 19.000],  loss: 0.015477, mae: 0.406185, mean_q: 0.452998, mean_eps: 0.000000
 3458/5000: episode: 129, duration: 0.383s, episode steps:  27, steps per second:  70, episode reward: -33.000, mean reward: -1.222 [-32.475,  2.309], mean action: 6.296 [0.000, 19.000],  loss: 0.020748, mae: 0.429527, mean_q: 0.433107, mean_eps: 0.000000
 3476/5000: episode: 130, duration: 0.517s, episode steps:  18, steps per second:  35, episode reward: -41.680, mean reward: -2.316 [-32.372,  2.274], mean action: 8.444 [0.000, 16.000],  loss: 0.023459, mae: 0.439574, mean_q: 0.505556, mean_eps: 0.000000
 3495/5000: episode: 131, duration: 0.264s, episode steps:  19, steps per second:  72, episode reward: 44.105, mean reward:  2.321 [-2.399, 32.049], mean action: 2.895 [1.000, 19.000],  loss: 0.022819, mae: 0.437392, mean_q: 0.530544, mean_eps: 0.000000
 3518/5000: episode: 132, duration: 0.329s, episode steps:  23, steps per second:  70, episode reward: 34.390, mean reward:  1.495 [-3.000, 32.170], mean action: 7.087 [0.000, 19.000],  loss: 0.015627, mae: 0.408179, mean_q: 0.508378, mean_eps: 0.000000
 3541/5000: episode: 133, duration: 0.336s, episode steps:  23, steps per second:  69, episode reward: 36.000, mean reward:  1.565 [-3.000, 33.000], mean action: 7.217 [0.000, 19.000],  loss: 0.023098, mae: 0.435971, mean_q: 0.465929, mean_eps: 0.000000
 3563/5000: episode: 134, duration: 0.273s, episode steps:  22, steps per second:  80, episode reward: -32.440, mean reward: -1.475 [-32.680,  2.709], mean action: 5.182 [0.000, 19.000],  loss: 0.020972, mae: 0.425648, mean_q: 0.515735, mean_eps: 0.000000
 3590/5000: episode: 135, duration: 0.355s, episode steps:  27, steps per second:  76, episode reward: 30.000, mean reward:  1.111 [-3.000, 30.649], mean action: 3.889 [0.000, 19.000],  loss: 0.020276, mae: 0.423758, mean_q: 0.530121, mean_eps: 0.000000
 3624/5000: episode: 136, duration: 0.422s, episode steps:  34, steps per second:  81, episode reward: -35.370, mean reward: -1.040 [-32.452,  2.762], mean action: 5.588 [0.000, 19.000],  loss: 0.022866, mae: 0.441307, mean_q: 0.517167, mean_eps: 0.000000
 3642/5000: episode: 137, duration: 0.228s, episode steps:  18, steps per second:  79, episode reward: 38.484, mean reward:  2.138 [-2.507, 32.120], mean action: 6.278 [1.000, 14.000],  loss: 0.017474, mae: 0.420335, mean_q: 0.506351, mean_eps: 0.000000
 3666/5000: episode: 138, duration: 0.299s, episode steps:  24, steps per second:  80, episode reward: 34.650, mean reward:  1.444 [-2.903, 32.010], mean action: 4.708 [0.000, 15.000],  loss: 0.019564, mae: 0.434957, mean_q: 0.459988, mean_eps: 0.000000
 3690/5000: episode: 139, duration: 0.290s, episode steps:  24, steps per second:  83, episode reward: -41.470, mean reward: -1.728 [-32.226,  2.482], mean action: 9.250 [0.000, 15.000],  loss: 0.018860, mae: 0.432063, mean_q: 0.410258, mean_eps: 0.000000
 3712/5000: episode: 140, duration: 0.282s, episode steps:  22, steps per second:  78, episode reward: 39.000, mean reward:  1.773 [-2.298, 32.910], mean action: 3.545 [0.000, 12.000],  loss: 0.019744, mae: 0.441392, mean_q: 0.406846, mean_eps: 0.000000
 3746/5000: episode: 141, duration: 0.403s, episode steps:  34, steps per second:  84, episode reward: 32.871, mean reward:  0.967 [-2.443, 32.134], mean action: 6.882 [0.000, 21.000],  loss: 0.021072, mae: 0.439777, mean_q: 0.429765, mean_eps: 0.000000
 3762/5000: episode: 142, duration: 0.201s, episode steps:  16, steps per second:  80, episode reward: 34.771, mean reward:  2.173 [-3.000, 31.468], mean action: 5.562 [0.000, 14.000],  loss: 0.017905, mae: 0.411957, mean_q: 0.456677, mean_eps: 0.000000
 3785/5000: episode: 143, duration: 0.288s, episode steps:  23, steps per second:  80, episode reward: 38.168, mean reward:  1.659 [-2.476, 32.440], mean action: 4.391 [0.000, 14.000],  loss: 0.018790, mae: 0.416249, mean_q: 0.483943, mean_eps: 0.000000
 3805/5000: episode: 144, duration: 0.242s, episode steps:  20, steps per second:  83, episode reward: -35.700, mean reward: -1.785 [-32.132,  2.583], mean action: 8.450 [0.000, 21.000],  loss: 0.020273, mae: 0.426212, mean_q: 0.481141, mean_eps: 0.000000
 3831/5000: episode: 145, duration: 0.309s, episode steps:  26, steps per second:  84, episode reward: -38.490, mean reward: -1.480 [-32.048,  2.721], mean action: 4.615 [0.000, 16.000],  loss: 0.017930, mae: 0.416027, mean_q: 0.476015, mean_eps: 0.000000
 3847/5000: episode: 146, duration: 0.208s, episode steps:  16, steps per second:  77, episode reward: 43.638, mean reward:  2.727 [-2.047, 33.000], mean action: 3.875 [0.000, 19.000],  loss: 0.019953, mae: 0.430220, mean_q: 0.476369, mean_eps: 0.000000
 3877/5000: episode: 147, duration: 0.369s, episode steps:  30, steps per second:  81, episode reward: 32.573, mean reward:  1.086 [-3.000, 32.979], mean action: 5.467 [0.000, 19.000],  loss: 0.020002, mae: 0.435603, mean_q: 0.488965, mean_eps: 0.000000
 3902/5000: episode: 148, duration: 0.308s, episode steps:  25, steps per second:  81, episode reward: 32.142, mean reward:  1.286 [-3.000, 32.001], mean action: 6.600 [0.000, 21.000],  loss: 0.021039, mae: 0.444851, mean_q: 0.479202, mean_eps: 0.000000
 3930/5000: episode: 149, duration: 0.392s, episode steps:  28, steps per second:  71, episode reward: 35.710, mean reward:  1.275 [-3.000, 33.000], mean action: 7.929 [0.000, 21.000],  loss: 0.019432, mae: 0.433184, mean_q: 0.478687, mean_eps: 0.000000
 3961/5000: episode: 150, duration: 0.373s, episode steps:  31, steps per second:  83, episode reward: -33.000, mean reward: -1.065 [-32.166,  2.149], mean action: 5.516 [0.000, 16.000],  loss: 0.017023, mae: 0.413450, mean_q: 0.476698, mean_eps: 0.000000
 3990/5000: episode: 151, duration: 0.350s, episode steps:  29, steps per second:  83, episode reward: 32.271, mean reward:  1.113 [-2.507, 32.130], mean action: 4.931 [1.000, 14.000],  loss: 0.020358, mae: 0.435591, mean_q: 0.477462, mean_eps: 0.000000
 4012/5000: episode: 152, duration: 0.268s, episode steps:  22, steps per second:  82, episode reward: 33.000, mean reward:  1.500 [-3.000, 32.500], mean action: 4.818 [0.000, 15.000],  loss: 0.018257, mae: 0.427543, mean_q: 0.450064, mean_eps: 0.000000
 4032/5000: episode: 153, duration: 0.255s, episode steps:  20, steps per second:  78, episode reward: 41.725, mean reward:  2.086 [-2.561, 32.149], mean action: 4.000 [0.000, 12.000],  loss: 0.021243, mae: 0.430985, mean_q: 0.485511, mean_eps: 0.000000
 4067/5000: episode: 154, duration: 0.419s, episode steps:  35, steps per second:  83, episode reward: 38.727, mean reward:  1.106 [-2.443, 32.051], mean action: 2.857 [0.000, 15.000],  loss: 0.019617, mae: 0.427351, mean_q: 0.494117, mean_eps: 0.000000
 4090/5000: episode: 155, duration: 0.287s, episode steps:  23, steps per second:  80, episode reward: -35.700, mean reward: -1.552 [-32.242,  2.837], mean action: 5.609 [0.000, 15.000],  loss: 0.018823, mae: 0.421078, mean_q: 0.496667, mean_eps: 0.000000
 4113/5000: episode: 156, duration: 0.286s, episode steps:  23, steps per second:  80, episode reward: 38.581, mean reward:  1.677 [-2.647, 32.216], mean action: 4.261 [0.000, 12.000],  loss: 0.020305, mae: 0.436255, mean_q: 0.465508, mean_eps: 0.000000
 4134/5000: episode: 157, duration: 0.258s, episode steps:  21, steps per second:  81, episode reward: 38.191, mean reward:  1.819 [-3.000, 31.794], mean action: 2.905 [0.000, 11.000],  loss: 0.019273, mae: 0.424543, mean_q: 0.492199, mean_eps: 0.000000
 4149/5000: episode: 158, duration: 0.194s, episode steps:  15, steps per second:  77, episode reward: 40.740, mean reward:  2.716 [-3.000, 32.350], mean action: 2.867 [0.000, 12.000],  loss: 0.018097, mae: 0.424842, mean_q: 0.453927, mean_eps: 0.000000
 4172/5000: episode: 159, duration: 0.419s, episode steps:  23, steps per second:  55, episode reward: 38.259, mean reward:  1.663 [-2.758, 32.259], mean action: 3.174 [1.000, 12.000],  loss: 0.019997, mae: 0.430437, mean_q: 0.486386, mean_eps: 0.000000
 4193/5000: episode: 160, duration: 0.347s, episode steps:  21, steps per second:  61, episode reward: 37.043, mean reward:  1.764 [-3.000, 31.887], mean action: 6.286 [0.000, 21.000],  loss: 0.018112, mae: 0.415610, mean_q: 0.468186, mean_eps: 0.000000
 4229/5000: episode: 161, duration: 0.465s, episode steps:  36, steps per second:  77, episode reward: 33.000, mean reward:  0.917 [-2.456, 30.084], mean action: 4.139 [0.000, 15.000],  loss: 0.020079, mae: 0.424220, mean_q: 0.459761, mean_eps: 0.000000
 4255/5000: episode: 162, duration: 0.320s, episode steps:  26, steps per second:  81, episode reward: 33.000, mean reward:  1.269 [-3.000, 32.280], mean action: 3.962 [0.000, 13.000],  loss: 0.015138, mae: 0.405691, mean_q: 0.425309, mean_eps: 0.000000
 4276/5000: episode: 163, duration: 0.260s, episode steps:  21, steps per second:  81, episode reward: 32.903, mean reward:  1.567 [-3.000, 32.063], mean action: 5.190 [0.000, 20.000],  loss: 0.017822, mae: 0.411216, mean_q: 0.476120, mean_eps: 0.000000
 4297/5000: episode: 164, duration: 0.258s, episode steps:  21, steps per second:  81, episode reward: -32.840, mean reward: -1.564 [-32.199,  3.000], mean action: 6.048 [0.000, 17.000],  loss: 0.022670, mae: 0.431819, mean_q: 0.469472, mean_eps: 0.000000
 4321/5000: episode: 165, duration: 0.299s, episode steps:  24, steps per second:  80, episode reward: 35.546, mean reward:  1.481 [-2.502, 33.000], mean action: 5.292 [0.000, 20.000],  loss: 0.021788, mae: 0.426360, mean_q: 0.502988, mean_eps: 0.000000
 4359/5000: episode: 166, duration: 0.458s, episode steps:  38, steps per second:  83, episode reward: 38.431, mean reward:  1.011 [-2.339, 32.170], mean action: 5.105 [0.000, 21.000],  loss: 0.021018, mae: 0.419271, mean_q: 0.513987, mean_eps: 0.000000
 4377/5000: episode: 167, duration: 0.226s, episode steps:  18, steps per second:  80, episode reward: 38.054, mean reward:  2.114 [-2.500, 32.141], mean action: 4.444 [0.000, 16.000],  loss: 0.018152, mae: 0.398218, mean_q: 0.490262, mean_eps: 0.000000
 4393/5000: episode: 168, duration: 0.207s, episode steps:  16, steps per second:  77, episode reward: 38.547, mean reward:  2.409 [-2.341, 32.831], mean action: 3.562 [0.000, 16.000],  loss: 0.019365, mae: 0.416707, mean_q: 0.502122, mean_eps: 0.000000
 4415/5000: episode: 169, duration: 0.273s, episode steps:  22, steps per second:  80, episode reward: 38.474, mean reward:  1.749 [-2.248, 32.380], mean action: 8.318 [0.000, 21.000],  loss: 0.020295, mae: 0.428185, mean_q: 0.458093, mean_eps: 0.000000
 4435/5000: episode: 170, duration: 0.247s, episode steps:  20, steps per second:  81, episode reward: 37.656, mean reward:  1.883 [-3.000, 31.649], mean action: 5.150 [0.000, 16.000],  loss: 0.019548, mae: 0.424114, mean_q: 0.500249, mean_eps: 0.000000
 4458/5000: episode: 171, duration: 0.278s, episode steps:  23, steps per second:  83, episode reward: 33.000, mean reward:  1.435 [-3.000, 32.230], mean action: 4.000 [0.000, 16.000],  loss: 0.020834, mae: 0.430363, mean_q: 0.553089, mean_eps: 0.000000
 4484/5000: episode: 172, duration: 0.312s, episode steps:  26, steps per second:  83, episode reward: -35.310, mean reward: -1.358 [-32.293,  2.433], mean action: 4.962 [0.000, 16.000],  loss: 0.016772, mae: 0.405398, mean_q: 0.571360, mean_eps: 0.000000
 4501/5000: episode: 173, duration: 0.214s, episode steps:  17, steps per second:  80, episode reward: 38.857, mean reward:  2.286 [-2.383, 32.250], mean action: 4.706 [0.000, 16.000],  loss: 0.022491, mae: 0.435535, mean_q: 0.538486, mean_eps: 0.000000
 4523/5000: episode: 174, duration: 0.272s, episode steps:  22, steps per second:  81, episode reward: 38.189, mean reward:  1.736 [-3.000, 32.031], mean action: 2.818 [0.000, 16.000],  loss: 0.017053, mae: 0.408325, mean_q: 0.489470, mean_eps: 0.000000
 4557/5000: episode: 175, duration: 0.413s, episode steps:  34, steps per second:  82, episode reward: 35.029, mean reward:  1.030 [-2.521, 31.840], mean action: 6.324 [0.000, 16.000],  loss: 0.018469, mae: 0.418508, mean_q: 0.484133, mean_eps: 0.000000
 4575/5000: episode: 176, duration: 0.227s, episode steps:  18, steps per second:  79, episode reward: 38.441, mean reward:  2.136 [-2.790, 32.090], mean action: 4.833 [0.000, 21.000],  loss: 0.017821, mae: 0.425107, mean_q: 0.458498, mean_eps: 0.000000
 4604/5000: episode: 177, duration: 0.347s, episode steps:  29, steps per second:  84, episode reward: 32.749, mean reward:  1.129 [-2.405, 32.438], mean action: 4.828 [0.000, 19.000],  loss: 0.023242, mae: 0.449198, mean_q: 0.489063, mean_eps: 0.000000
 4628/5000: episode: 178, duration: 0.292s, episode steps:  24, steps per second:  82, episode reward: 36.000, mean reward:  1.500 [-3.000, 32.250], mean action: 4.292 [0.000, 14.000],  loss: 0.016069, mae: 0.413606, mean_q: 0.521140, mean_eps: 0.000000
 4644/5000: episode: 179, duration: 0.206s, episode steps:  16, steps per second:  78, episode reward: 41.176, mean reward:  2.574 [-2.315, 32.949], mean action: 3.375 [0.000, 11.000],  loss: 0.020450, mae: 0.432850, mean_q: 0.500564, mean_eps: 0.000000
 4678/5000: episode: 180, duration: 0.405s, episode steps:  34, steps per second:  84, episode reward: -32.540, mean reward: -0.957 [-31.548,  2.520], mean action: 5.382 [0.000, 19.000],  loss: 0.019898, mae: 0.432697, mean_q: 0.498346, mean_eps: 0.000000
 4698/5000: episode: 181, duration: 0.252s, episode steps:  20, steps per second:  79, episode reward: 35.158, mean reward:  1.758 [-3.000, 31.924], mean action: 4.700 [1.000, 19.000],  loss: 0.015884, mae: 0.408177, mean_q: 0.525119, mean_eps: 0.000000
 4717/5000: episode: 182, duration: 0.247s, episode steps:  19, steps per second:  77, episode reward: 32.692, mean reward:  1.721 [-3.000, 32.352], mean action: 4.632 [0.000, 19.000],  loss: 0.017286, mae: 0.415317, mean_q: 0.493911, mean_eps: 0.000000
 4748/5000: episode: 183, duration: 0.373s, episode steps:  31, steps per second:  83, episode reward: -33.000, mean reward: -1.065 [-32.262,  2.420], mean action: 5.226 [0.000, 21.000],  loss: 0.021674, mae: 0.440795, mean_q: 0.454808, mean_eps: 0.000000
 4772/5000: episode: 184, duration: 0.289s, episode steps:  24, steps per second:  83, episode reward: -35.330, mean reward: -1.472 [-32.266,  2.997], mean action: 5.083 [0.000, 14.000],  loss: 0.019934, mae: 0.433948, mean_q: 0.432200, mean_eps: 0.000000
 4791/5000: episode: 185, duration: 0.234s, episode steps:  19, steps per second:  81, episode reward: 39.000, mean reward:  2.053 [-2.525, 33.000], mean action: 3.579 [0.000, 11.000],  loss: 0.020641, mae: 0.436042, mean_q: 0.458672, mean_eps: 0.000000
 4808/5000: episode: 186, duration: 0.215s, episode steps:  17, steps per second:  79, episode reward: 41.353, mean reward:  2.433 [-2.734, 31.860], mean action: 2.000 [0.000, 11.000],  loss: 0.018048, mae: 0.423721, mean_q: 0.440770, mean_eps: 0.000000
 4823/5000: episode: 187, duration: 0.192s, episode steps:  15, steps per second:  78, episode reward: 44.227, mean reward:  2.948 [-2.105, 32.410], mean action: 3.467 [0.000, 12.000],  loss: 0.021780, mae: 0.448662, mean_q: 0.452589, mean_eps: 0.000000
 4838/5000: episode: 188, duration: 0.194s, episode steps:  15, steps per second:  77, episode reward: 41.531, mean reward:  2.769 [-2.298, 31.901], mean action: 3.067 [0.000, 19.000],  loss: 0.022107, mae: 0.448106, mean_q: 0.461342, mean_eps: 0.000000
 4874/5000: episode: 189, duration: 0.428s, episode steps:  36, steps per second:  84, episode reward: -32.350, mean reward: -0.899 [-32.093,  2.613], mean action: 9.556 [1.000, 21.000],  loss: 0.022887, mae: 0.440267, mean_q: 0.522444, mean_eps: 0.000000
 4909/5000: episode: 190, duration: 0.442s, episode steps:  35, steps per second:  79, episode reward: 32.313, mean reward:  0.923 [-2.429, 32.734], mean action: 5.114 [0.000, 15.000],  loss: 0.023565, mae: 0.442600, mean_q: 0.458730, mean_eps: 0.000000
 4925/5000: episode: 191, duration: 0.204s, episode steps:  16, steps per second:  78, episode reward: 43.503, mean reward:  2.719 [-2.143, 33.000], mean action: 4.500 [0.000, 13.000],  loss: 0.023866, mae: 0.447712, mean_q: 0.454675, mean_eps: 0.000000
 4947/5000: episode: 192, duration: 0.268s, episode steps:  22, steps per second:  82, episode reward: -36.000, mean reward: -1.636 [-32.211,  2.231], mean action: 4.773 [0.000, 15.000],  loss: 0.018237, mae: 0.417246, mean_q: 0.458159, mean_eps: 0.000000
 4963/5000: episode: 193, duration: 0.205s, episode steps:  16, steps per second:  78, episode reward: 41.539, mean reward:  2.596 [-2.236, 33.000], mean action: 4.000 [0.000, 15.000],  loss: 0.019592, mae: 0.429750, mean_q: 0.475124, mean_eps: 0.000000
 4985/5000: episode: 194, duration: 0.269s, episode steps:  22, steps per second:  82, episode reward: 38.654, mean reward:  1.757 [-2.581, 32.280], mean action: 3.591 [0.000, 12.000],  loss: 0.018524, mae: 0.415537, mean_q: 0.490861, mean_eps: 0.000000
done, took 67.107 seconds
DQN Evaluation: 3106 victories out of 3720 episodes
Training for 5000 steps ...
   33/5000: episode: 1, duration: 0.221s, episode steps:  33, steps per second: 149, episode reward: 35.568, mean reward:  1.078 [-3.000, 32.350], mean action: 4.697 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   86/5000: episode: 2, duration: 0.323s, episode steps:  53, steps per second: 164, episode reward: 32.564, mean reward:  0.614 [-2.805, 32.540], mean action: 3.132 [0.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  100/5000: episode: 3, duration: 0.115s, episode steps:  14, steps per second: 121, episode reward: 47.012, mean reward:  3.358 [-0.145, 33.000], mean action: 2.786 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  118/5000: episode: 4, duration: 0.145s, episode steps:  18, steps per second: 124, episode reward: 47.220, mean reward:  2.623 [-0.060, 32.280], mean action: 3.389 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  139/5000: episode: 5, duration: 0.147s, episode steps:  21, steps per second: 143, episode reward: 44.008, mean reward:  2.096 [-2.218, 32.170], mean action: 1.714 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  175/5000: episode: 6, duration: 0.224s, episode steps:  36, steps per second: 161, episode reward: 40.962, mean reward:  1.138 [-2.286, 32.610], mean action: 3.500 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  208/5000: episode: 7, duration: 0.202s, episode steps:  33, steps per second: 163, episode reward: 38.056, mean reward:  1.153 [-2.093, 31.780], mean action: 3.152 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  241/5000: episode: 8, duration: 0.209s, episode steps:  33, steps per second: 158, episode reward: 41.808, mean reward:  1.267 [-2.255, 32.210], mean action: 2.000 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  293/5000: episode: 9, duration: 0.310s, episode steps:  52, steps per second: 168, episode reward: -32.120, mean reward: -0.618 [-32.046,  2.560], mean action: 6.596 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  312/5000: episode: 10, duration: 0.128s, episode steps:  19, steps per second: 148, episode reward: 44.551, mean reward:  2.345 [-2.074, 32.550], mean action: 2.684 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  342/5000: episode: 11, duration: 0.198s, episode steps:  30, steps per second: 151, episode reward: 43.847, mean reward:  1.462 [-2.593, 32.100], mean action: 2.867 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  379/5000: episode: 12, duration: 0.223s, episode steps:  37, steps per second: 166, episode reward: -32.700, mean reward: -0.884 [-32.474,  2.890], mean action: 6.243 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  411/5000: episode: 13, duration: 0.188s, episode steps:  32, steps per second: 170, episode reward: -32.420, mean reward: -1.013 [-29.636,  2.680], mean action: 4.562 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  436/5000: episode: 14, duration: 0.157s, episode steps:  25, steps per second: 159, episode reward: 37.757, mean reward:  1.510 [-3.000, 32.038], mean action: 4.840 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  456/5000: episode: 15, duration: 0.134s, episode steps:  20, steps per second: 149, episode reward: 38.297, mean reward:  1.915 [-2.756, 31.717], mean action: 2.450 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  475/5000: episode: 16, duration: 0.124s, episode steps:  19, steps per second: 153, episode reward: 42.000, mean reward:  2.211 [-2.615, 32.800], mean action: 3.158 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  502/5000: episode: 17, duration: 0.170s, episode steps:  27, steps per second: 158, episode reward: 41.528, mean reward:  1.538 [-2.123, 31.718], mean action: 3.074 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  558/5000: episode: 18, duration: 0.347s, episode steps:  56, steps per second: 161, episode reward: -32.970, mean reward: -0.589 [-31.982,  2.328], mean action: 3.536 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  574/5000: episode: 19, duration: 0.136s, episode steps:  16, steps per second: 118, episode reward: 44.314, mean reward:  2.770 [-2.160, 32.030], mean action: 3.312 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  612/5000: episode: 20, duration: 0.278s, episode steps:  38, steps per second: 137, episode reward: 43.416, mean reward:  1.143 [-2.039, 32.238], mean action: 2.105 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  636/5000: episode: 21, duration: 0.157s, episode steps:  24, steps per second: 153, episode reward: 41.713, mean reward:  1.738 [-2.529, 32.520], mean action: 1.792 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  679/5000: episode: 22, duration: 0.302s, episode steps:  43, steps per second: 143, episode reward: 36.000, mean reward:  0.837 [-2.939, 32.630], mean action: 2.442 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  712/5000: episode: 23, duration: 0.202s, episode steps:  33, steps per second: 163, episode reward: 39.000, mean reward:  1.182 [-3.000, 29.239], mean action: 5.182 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  755/5000: episode: 24, duration: 0.266s, episode steps:  43, steps per second: 162, episode reward: -32.110, mean reward: -0.747 [-32.067,  2.409], mean action: 3.907 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  786/5000: episode: 25, duration: 0.191s, episode steps:  31, steps per second: 162, episode reward: 37.980, mean reward:  1.225 [-2.632, 31.986], mean action: 3.516 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  805/5000: episode: 26, duration: 0.131s, episode steps:  19, steps per second: 145, episode reward: 43.489, mean reward:  2.289 [-3.000, 32.210], mean action: 2.263 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  819/5000: episode: 27, duration: 0.100s, episode steps:  14, steps per second: 140, episode reward: 44.269, mean reward:  3.162 [-2.493, 32.137], mean action: 2.929 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  843/5000: episode: 28, duration: 0.154s, episode steps:  24, steps per second: 155, episode reward: 38.817, mean reward:  1.617 [-2.401, 33.061], mean action: 2.208 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  871/5000: episode: 29, duration: 0.180s, episode steps:  28, steps per second: 156, episode reward: 38.142, mean reward:  1.362 [-3.000, 32.090], mean action: 4.286 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  901/5000: episode: 30, duration: 0.195s, episode steps:  30, steps per second: 154, episode reward: 41.097, mean reward:  1.370 [-2.703, 31.602], mean action: 3.000 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  928/5000: episode: 31, duration: 0.169s, episode steps:  27, steps per second: 160, episode reward: 44.175, mean reward:  1.636 [-2.378, 32.020], mean action: 1.889 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  958/5000: episode: 32, duration: 0.188s, episode steps:  30, steps per second: 159, episode reward: 38.329, mean reward:  1.278 [-2.167, 32.080], mean action: 2.233 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  973/5000: episode: 33, duration: 0.104s, episode steps:  15, steps per second: 144, episode reward: 45.000, mean reward:  3.000 [-2.003, 33.000], mean action: 2.133 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  999/5000: episode: 34, duration: 0.167s, episode steps:  26, steps per second: 156, episode reward: 43.370, mean reward:  1.668 [-2.321, 32.820], mean action: 2.615 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1038/5000: episode: 35, duration: 0.463s, episode steps:  39, steps per second:  84, episode reward: 38.507, mean reward:  0.987 [-3.000, 32.210], mean action: 2.564 [0.000, 10.000],  loss: 0.020293, mae: 0.426314, mean_q: 0.477292, mean_eps: 0.000000
 1057/5000: episode: 36, duration: 0.239s, episode steps:  19, steps per second:  79, episode reward: 43.389, mean reward:  2.284 [-2.000, 31.258], mean action: 2.895 [0.000, 12.000],  loss: 0.017314, mae: 0.426101, mean_q: 0.473621, mean_eps: 0.000000
 1094/5000: episode: 37, duration: 0.548s, episode steps:  37, steps per second:  67, episode reward: 32.008, mean reward:  0.865 [-3.000, 33.000], mean action: 4.297 [0.000, 17.000],  loss: 0.018987, mae: 0.427387, mean_q: 0.474350, mean_eps: 0.000000
 1121/5000: episode: 38, duration: 0.338s, episode steps:  27, steps per second:  80, episode reward: 35.027, mean reward:  1.297 [-2.297, 31.474], mean action: 4.778 [0.000, 16.000],  loss: 0.020741, mae: 0.433972, mean_q: 0.411667, mean_eps: 0.000000
 1141/5000: episode: 39, duration: 0.319s, episode steps:  20, steps per second:  63, episode reward: 44.262, mean reward:  2.213 [-2.301, 31.941], mean action: 2.250 [1.000, 12.000],  loss: 0.019539, mae: 0.429732, mean_q: 0.411834, mean_eps: 0.000000
 1171/5000: episode: 40, duration: 0.361s, episode steps:  30, steps per second:  83, episode reward: 32.939, mean reward:  1.098 [-2.628, 29.216], mean action: 6.267 [0.000, 21.000],  loss: 0.019076, mae: 0.427886, mean_q: 0.431083, mean_eps: 0.000000
 1195/5000: episode: 41, duration: 0.294s, episode steps:  24, steps per second:  81, episode reward: 41.182, mean reward:  1.716 [-2.636, 32.140], mean action: 4.042 [1.000, 14.000],  loss: 0.016999, mae: 0.415293, mean_q: 0.428373, mean_eps: 0.000000
 1224/5000: episode: 42, duration: 0.360s, episode steps:  29, steps per second:  81, episode reward: 38.647, mean reward:  1.333 [-3.000, 32.190], mean action: 4.069 [0.000, 15.000],  loss: 0.019596, mae: 0.423175, mean_q: 0.457405, mean_eps: 0.000000
 1245/5000: episode: 43, duration: 0.262s, episode steps:  21, steps per second:  80, episode reward: 40.940, mean reward:  1.950 [-3.000, 32.070], mean action: 5.286 [0.000, 14.000],  loss: 0.022280, mae: 0.434426, mean_q: 0.474789, mean_eps: 0.000000
 1291/5000: episode: 44, duration: 0.539s, episode steps:  46, steps per second:  85, episode reward: 35.259, mean reward:  0.767 [-3.000, 32.880], mean action: 5.130 [0.000, 19.000],  loss: 0.019741, mae: 0.422004, mean_q: 0.523895, mean_eps: 0.000000
 1310/5000: episode: 45, duration: 0.315s, episode steps:  19, steps per second:  60, episode reward: 40.391, mean reward:  2.126 [-2.662, 32.840], mean action: 5.579 [0.000, 19.000],  loss: 0.025101, mae: 0.456127, mean_q: 0.480788, mean_eps: 0.000000
 1339/5000: episode: 46, duration: 0.482s, episode steps:  29, steps per second:  60, episode reward: 37.800, mean reward:  1.303 [-3.000, 31.677], mean action: 3.138 [0.000, 15.000],  loss: 0.022697, mae: 0.451160, mean_q: 0.483579, mean_eps: 0.000000
 1360/5000: episode: 47, duration: 0.304s, episode steps:  21, steps per second:  69, episode reward: 44.834, mean reward:  2.135 [-2.164, 32.130], mean action: 3.381 [0.000, 19.000],  loss: 0.023620, mae: 0.452507, mean_q: 0.488172, mean_eps: 0.000000
 1414/5000: episode: 48, duration: 0.709s, episode steps:  54, steps per second:  76, episode reward: 38.927, mean reward:  0.721 [-2.355, 32.537], mean action: 2.500 [0.000, 13.000],  loss: 0.021719, mae: 0.434847, mean_q: 0.526027, mean_eps: 0.000000
 1441/5000: episode: 49, duration: 0.331s, episode steps:  27, steps per second:  82, episode reward: 41.381, mean reward:  1.533 [-2.725, 32.020], mean action: 5.333 [0.000, 14.000],  loss: 0.020092, mae: 0.426657, mean_q: 0.510875, mean_eps: 0.000000
 1493/5000: episode: 50, duration: 0.618s, episode steps:  52, steps per second:  84, episode reward: 35.505, mean reward:  0.683 [-2.093, 32.300], mean action: 2.596 [0.000, 14.000],  loss: 0.018229, mae: 0.419433, mean_q: 0.472312, mean_eps: 0.000000
 1514/5000: episode: 51, duration: 0.260s, episode steps:  21, steps per second:  81, episode reward: 41.780, mean reward:  1.990 [-2.905, 32.342], mean action: 4.190 [0.000, 14.000],  loss: 0.018242, mae: 0.428843, mean_q: 0.440359, mean_eps: 0.000000
 1560/5000: episode: 52, duration: 0.550s, episode steps:  46, steps per second:  84, episode reward: 37.530, mean reward:  0.816 [-2.485, 31.881], mean action: 6.543 [0.000, 14.000],  loss: 0.020701, mae: 0.427748, mean_q: 0.481129, mean_eps: 0.000000
 1584/5000: episode: 53, duration: 0.303s, episode steps:  24, steps per second:  79, episode reward: 38.731, mean reward:  1.614 [-2.370, 32.200], mean action: 2.417 [0.000, 12.000],  loss: 0.015907, mae: 0.410002, mean_q: 0.495643, mean_eps: 0.000000
 1618/5000: episode: 54, duration: 0.413s, episode steps:  34, steps per second:  82, episode reward: 41.550, mean reward:  1.222 [-2.198, 31.865], mean action: 3.706 [0.000, 14.000],  loss: 0.018677, mae: 0.423812, mean_q: 0.477743, mean_eps: 0.000000
 1638/5000: episode: 55, duration: 0.257s, episode steps:  20, steps per second:  78, episode reward: 38.559, mean reward:  1.928 [-2.651, 32.023], mean action: 3.950 [0.000, 12.000],  loss: 0.018062, mae: 0.430160, mean_q: 0.432091, mean_eps: 0.000000
 1658/5000: episode: 56, duration: 0.254s, episode steps:  20, steps per second:  79, episode reward: 43.490, mean reward:  2.175 [-2.099, 32.120], mean action: 5.700 [2.000, 20.000],  loss: 0.021931, mae: 0.443693, mean_q: 0.421368, mean_eps: 0.000000
 1683/5000: episode: 57, duration: 0.308s, episode steps:  25, steps per second:  81, episode reward: 40.331, mean reward:  1.613 [-2.266, 31.973], mean action: 5.200 [0.000, 15.000],  loss: 0.022063, mae: 0.445994, mean_q: 0.452917, mean_eps: 0.000000
 1703/5000: episode: 58, duration: 0.248s, episode steps:  20, steps per second:  81, episode reward: 41.791, mean reward:  2.090 [-2.535, 32.090], mean action: 2.150 [0.000, 11.000],  loss: 0.022389, mae: 0.448865, mean_q: 0.447293, mean_eps: 0.000000
 1730/5000: episode: 59, duration: 0.322s, episode steps:  27, steps per second:  84, episode reward: -37.910, mean reward: -1.404 [-32.404,  2.526], mean action: 6.000 [0.000, 15.000],  loss: 0.019940, mae: 0.433359, mean_q: 0.484523, mean_eps: 0.000000
 1762/5000: episode: 60, duration: 0.380s, episode steps:  32, steps per second:  84, episode reward: 35.882, mean reward:  1.121 [-2.241, 32.052], mean action: 6.938 [1.000, 19.000],  loss: 0.021516, mae: 0.439489, mean_q: 0.448261, mean_eps: 0.000000
 1796/5000: episode: 61, duration: 0.417s, episode steps:  34, steps per second:  81, episode reward: 42.957, mean reward:  1.263 [-2.313, 31.705], mean action: 2.500 [0.000, 12.000],  loss: 0.020966, mae: 0.434343, mean_q: 0.419373, mean_eps: 0.000000
 1817/5000: episode: 62, duration: 0.269s, episode steps:  21, steps per second:  78, episode reward: 40.318, mean reward:  1.920 [-3.000, 32.157], mean action: 6.143 [0.000, 14.000],  loss: 0.021021, mae: 0.440290, mean_q: 0.373709, mean_eps: 0.000000
 1841/5000: episode: 63, duration: 0.323s, episode steps:  24, steps per second:  74, episode reward: 41.050, mean reward:  1.710 [-3.000, 32.040], mean action: 1.792 [0.000, 9.000],  loss: 0.013184, mae: 0.397872, mean_q: 0.402418, mean_eps: 0.000000
 1863/5000: episode: 64, duration: 0.279s, episode steps:  22, steps per second:  79, episode reward: 44.581, mean reward:  2.026 [-2.173, 32.130], mean action: 2.273 [0.000, 12.000],  loss: 0.023619, mae: 0.443740, mean_q: 0.470794, mean_eps: 0.000000
 1878/5000: episode: 65, duration: 0.191s, episode steps:  15, steps per second:  79, episode reward: 39.000, mean reward:  2.600 [-2.819, 32.040], mean action: 4.267 [0.000, 19.000],  loss: 0.021045, mae: 0.425012, mean_q: 0.429633, mean_eps: 0.000000
 1920/5000: episode: 66, duration: 0.542s, episode steps:  42, steps per second:  77, episode reward: 41.412, mean reward:  0.986 [-2.500, 29.763], mean action: 3.048 [0.000, 13.000],  loss: 0.021163, mae: 0.428760, mean_q: 0.439205, mean_eps: 0.000000
 1952/5000: episode: 67, duration: 0.387s, episode steps:  32, steps per second:  83, episode reward: 41.078, mean reward:  1.284 [-2.282, 32.318], mean action: 5.969 [0.000, 21.000],  loss: 0.018270, mae: 0.408244, mean_q: 0.484762, mean_eps: 0.000000
 1997/5000: episode: 68, duration: 0.553s, episode steps:  45, steps per second:  81, episode reward: 33.000, mean reward:  0.733 [-3.000, 32.340], mean action: 5.400 [0.000, 19.000],  loss: 0.018661, mae: 0.412424, mean_q: 0.453430, mean_eps: 0.000000
 2031/5000: episode: 69, duration: 0.412s, episode steps:  34, steps per second:  83, episode reward: 41.351, mean reward:  1.216 [-2.940, 32.037], mean action: 5.353 [0.000, 20.000],  loss: 0.023167, mae: 0.441549, mean_q: 0.416229, mean_eps: 0.000000
 2048/5000: episode: 70, duration: 0.220s, episode steps:  17, steps per second:  77, episode reward: 39.000, mean reward:  2.294 [-2.873, 32.510], mean action: 3.941 [0.000, 11.000],  loss: 0.021010, mae: 0.428369, mean_q: 0.466831, mean_eps: 0.000000
 2074/5000: episode: 71, duration: 0.320s, episode steps:  26, steps per second:  81, episode reward: 39.000, mean reward:  1.500 [-2.349, 32.180], mean action: 3.115 [0.000, 14.000],  loss: 0.018320, mae: 0.419813, mean_q: 0.492646, mean_eps: 0.000000
 2094/5000: episode: 72, duration: 0.246s, episode steps:  20, steps per second:  81, episode reward: 41.654, mean reward:  2.083 [-3.000, 32.250], mean action: 3.200 [0.000, 14.000],  loss: 0.016580, mae: 0.419441, mean_q: 0.481884, mean_eps: 0.000000
 2109/5000: episode: 73, duration: 0.194s, episode steps:  15, steps per second:  77, episode reward: 44.803, mean reward:  2.987 [-2.069, 32.901], mean action: 1.533 [0.000, 11.000],  loss: 0.021594, mae: 0.428996, mean_q: 0.474104, mean_eps: 0.000000
 2145/5000: episode: 74, duration: 0.444s, episode steps:  36, steps per second:  81, episode reward: 34.904, mean reward:  0.970 [-2.380, 33.000], mean action: 4.333 [0.000, 20.000],  loss: 0.021043, mae: 0.425815, mean_q: 0.479869, mean_eps: 0.000000
 2171/5000: episode: 75, duration: 0.316s, episode steps:  26, steps per second:  82, episode reward: 41.598, mean reward:  1.600 [-2.658, 32.100], mean action: 3.346 [0.000, 15.000],  loss: 0.018434, mae: 0.414027, mean_q: 0.526849, mean_eps: 0.000000
 2213/5000: episode: 76, duration: 0.498s, episode steps:  42, steps per second:  84, episode reward: 43.827, mean reward:  1.044 [-2.370, 32.290], mean action: 3.762 [0.000, 15.000],  loss: 0.020520, mae: 0.414653, mean_q: 0.535009, mean_eps: 0.000000
 2231/5000: episode: 77, duration: 0.230s, episode steps:  18, steps per second:  78, episode reward: 40.882, mean reward:  2.271 [-2.521, 31.837], mean action: 2.722 [0.000, 12.000],  loss: 0.020947, mae: 0.414629, mean_q: 0.572610, mean_eps: 0.000000
 2273/5000: episode: 78, duration: 0.499s, episode steps:  42, steps per second:  84, episode reward: 32.744, mean reward:  0.780 [-2.264, 32.630], mean action: 5.476 [0.000, 19.000],  loss: 0.020858, mae: 0.419048, mean_q: 0.490815, mean_eps: 0.000000
 2295/5000: episode: 79, duration: 0.273s, episode steps:  22, steps per second:  81, episode reward: 42.000, mean reward:  1.909 [-2.113, 32.180], mean action: 3.636 [0.000, 19.000],  loss: 0.019173, mae: 0.415163, mean_q: 0.479107, mean_eps: 0.000000
 2313/5000: episode: 80, duration: 0.232s, episode steps:  18, steps per second:  78, episode reward: 41.118, mean reward:  2.284 [-2.171, 32.714], mean action: 2.278 [0.000, 11.000],  loss: 0.022328, mae: 0.439051, mean_q: 0.446640, mean_eps: 0.000000
 2342/5000: episode: 81, duration: 0.350s, episode steps:  29, steps per second:  83, episode reward: 42.000, mean reward:  1.448 [-2.108, 32.810], mean action: 2.552 [0.000, 11.000],  loss: 0.017609, mae: 0.421334, mean_q: 0.386443, mean_eps: 0.000000
 2365/5000: episode: 82, duration: 0.284s, episode steps:  23, steps per second:  81, episode reward: 41.864, mean reward:  1.820 [-2.074, 32.140], mean action: 3.435 [0.000, 12.000],  loss: 0.019853, mae: 0.426818, mean_q: 0.412920, mean_eps: 0.000000
 2400/5000: episode: 83, duration: 0.417s, episode steps:  35, steps per second:  84, episode reward: 41.381, mean reward:  1.182 [-2.527, 32.150], mean action: 3.829 [0.000, 15.000],  loss: 0.019172, mae: 0.416793, mean_q: 0.431603, mean_eps: 0.000000
 2430/5000: episode: 84, duration: 0.359s, episode steps:  30, steps per second:  84, episode reward: 38.506, mean reward:  1.284 [-2.392, 32.220], mean action: 3.300 [0.000, 15.000],  loss: 0.020592, mae: 0.424684, mean_q: 0.425739, mean_eps: 0.000000
 2459/5000: episode: 85, duration: 0.354s, episode steps:  29, steps per second:  82, episode reward: 38.009, mean reward:  1.311 [-2.904, 32.070], mean action: 4.379 [0.000, 14.000],  loss: 0.017961, mae: 0.424153, mean_q: 0.444326, mean_eps: 0.000000
 2481/5000: episode: 86, duration: 0.280s, episode steps:  22, steps per second:  79, episode reward: 38.329, mean reward:  1.742 [-2.382, 31.429], mean action: 3.955 [0.000, 15.000],  loss: 0.016898, mae: 0.418629, mean_q: 0.486683, mean_eps: 0.000000
 2503/5000: episode: 87, duration: 0.275s, episode steps:  22, steps per second:  80, episode reward: 42.000, mean reward:  1.909 [-2.430, 32.230], mean action: 2.591 [0.000, 14.000],  loss: 0.025548, mae: 0.457228, mean_q: 0.534713, mean_eps: 0.000000
 2524/5000: episode: 88, duration: 0.257s, episode steps:  21, steps per second:  82, episode reward: 39.000, mean reward:  1.857 [-3.000, 32.380], mean action: 4.190 [0.000, 14.000],  loss: 0.024757, mae: 0.449246, mean_q: 0.507056, mean_eps: 0.000000
 2548/5000: episode: 89, duration: 0.295s, episode steps:  24, steps per second:  81, episode reward: 42.000, mean reward:  1.750 [-2.482, 32.340], mean action: 5.750 [0.000, 20.000],  loss: 0.021197, mae: 0.441801, mean_q: 0.445868, mean_eps: 0.000000
 2568/5000: episode: 90, duration: 0.264s, episode steps:  20, steps per second:  76, episode reward: 44.890, mean reward:  2.244 [-2.142, 32.256], mean action: 1.150 [0.000, 12.000],  loss: 0.016192, mae: 0.414422, mean_q: 0.447379, mean_eps: 0.000000
 2591/5000: episode: 91, duration: 0.290s, episode steps:  23, steps per second:  79, episode reward: 38.620, mean reward:  1.679 [-3.000, 31.956], mean action: 3.130 [1.000, 15.000],  loss: 0.023131, mae: 0.449726, mean_q: 0.478173, mean_eps: 0.000000
 2610/5000: episode: 92, duration: 0.240s, episode steps:  19, steps per second:  79, episode reward: 38.908, mean reward:  2.048 [-2.876, 32.210], mean action: 2.895 [0.000, 12.000],  loss: 0.018498, mae: 0.424925, mean_q: 0.446568, mean_eps: 0.000000
 2657/5000: episode: 93, duration: 0.567s, episode steps:  47, steps per second:  83, episode reward: 37.647, mean reward:  0.801 [-2.179, 32.465], mean action: 10.362 [0.000, 20.000],  loss: 0.020802, mae: 0.422502, mean_q: 0.486193, mean_eps: 0.000000
 2691/5000: episode: 94, duration: 0.412s, episode steps:  34, steps per second:  83, episode reward: 41.070, mean reward:  1.208 [-2.459, 32.220], mean action: 3.059 [0.000, 19.000],  loss: 0.016934, mae: 0.398512, mean_q: 0.512477, mean_eps: 0.000000
 2719/5000: episode: 95, duration: 0.353s, episode steps:  28, steps per second:  79, episode reward: 34.602, mean reward:  1.236 [-2.311, 32.010], mean action: 4.071 [0.000, 15.000],  loss: 0.019463, mae: 0.412805, mean_q: 0.486501, mean_eps: 0.000000
 2761/5000: episode: 96, duration: 0.499s, episode steps:  42, steps per second:  84, episode reward: 35.189, mean reward:  0.838 [-2.318, 32.370], mean action: 5.690 [0.000, 20.000],  loss: 0.021927, mae: 0.424741, mean_q: 0.495685, mean_eps: 0.000000
 2823/5000: episode: 97, duration: 0.730s, episode steps:  62, steps per second:  85, episode reward: 37.666, mean reward:  0.608 [-2.448, 31.920], mean action: 8.387 [0.000, 21.000],  loss: 0.022970, mae: 0.431833, mean_q: 0.461306, mean_eps: 0.000000
 2850/5000: episode: 98, duration: 0.330s, episode steps:  27, steps per second:  82, episode reward: 43.920, mean reward:  1.627 [-2.063, 32.210], mean action: 2.259 [0.000, 11.000],  loss: 0.018624, mae: 0.413470, mean_q: 0.462790, mean_eps: 0.000000
 2872/5000: episode: 99, duration: 0.268s, episode steps:  22, steps per second:  82, episode reward: 38.819, mean reward:  1.765 [-3.000, 32.320], mean action: 5.818 [0.000, 15.000],  loss: 0.020864, mae: 0.427828, mean_q: 0.447757, mean_eps: 0.000000
 2900/5000: episode: 100, duration: 0.341s, episode steps:  28, steps per second:  82, episode reward: 42.725, mean reward:  1.526 [-2.612, 31.982], mean action: 4.357 [0.000, 15.000],  loss: 0.022397, mae: 0.431952, mean_q: 0.461060, mean_eps: 0.000000
 2926/5000: episode: 101, duration: 0.318s, episode steps:  26, steps per second:  82, episode reward: 44.692, mean reward:  1.719 [-2.209, 32.030], mean action: 1.731 [0.000, 19.000],  loss: 0.018588, mae: 0.407078, mean_q: 0.466421, mean_eps: 0.000000
 2956/5000: episode: 102, duration: 0.364s, episode steps:  30, steps per second:  82, episode reward: 43.891, mean reward:  1.463 [-2.304, 32.220], mean action: 3.100 [0.000, 13.000],  loss: 0.021358, mae: 0.419713, mean_q: 0.503151, mean_eps: 0.000000
 2983/5000: episode: 103, duration: 0.333s, episode steps:  27, steps per second:  81, episode reward: 43.519, mean reward:  1.612 [-2.422, 33.000], mean action: 3.815 [0.000, 19.000],  loss: 0.019753, mae: 0.408389, mean_q: 0.537317, mean_eps: 0.000000
 3020/5000: episode: 104, duration: 0.445s, episode steps:  37, steps per second:  83, episode reward: 38.829, mean reward:  1.049 [-2.212, 31.863], mean action: 4.162 [0.000, 19.000],  loss: 0.018967, mae: 0.416454, mean_q: 0.427140, mean_eps: 0.000000
 3034/5000: episode: 105, duration: 0.182s, episode steps:  14, steps per second:  77, episode reward: 48.000, mean reward:  3.429 [-0.140, 33.000], mean action: 0.643 [0.000, 3.000],  loss: 0.018424, mae: 0.413392, mean_q: 0.437671, mean_eps: 0.000000
 3057/5000: episode: 106, duration: 0.284s, episode steps:  23, steps per second:  81, episode reward: 41.824, mean reward:  1.818 [-2.255, 31.894], mean action: 3.043 [0.000, 19.000],  loss: 0.018449, mae: 0.410371, mean_q: 0.456772, mean_eps: 0.000000
 3082/5000: episode: 107, duration: 0.307s, episode steps:  25, steps per second:  81, episode reward: 39.000, mean reward:  1.560 [-2.630, 32.110], mean action: 5.120 [0.000, 19.000],  loss: 0.021211, mae: 0.419845, mean_q: 0.458121, mean_eps: 0.000000
 3100/5000: episode: 108, duration: 0.234s, episode steps:  18, steps per second:  77, episode reward: 44.170, mean reward:  2.454 [-2.059, 32.080], mean action: 4.000 [0.000, 15.000],  loss: 0.018169, mae: 0.404349, mean_q: 0.467091, mean_eps: 0.000000
 3129/5000: episode: 109, duration: 0.369s, episode steps:  29, steps per second:  79, episode reward: 41.834, mean reward:  1.443 [-2.285, 31.948], mean action: 5.069 [0.000, 19.000],  loss: 0.020118, mae: 0.401719, mean_q: 0.489919, mean_eps: 0.000000
 3144/5000: episode: 110, duration: 0.191s, episode steps:  15, steps per second:  79, episode reward: 41.951, mean reward:  2.797 [-3.000, 32.161], mean action: 3.533 [0.000, 19.000],  loss: 0.022529, mae: 0.411476, mean_q: 0.490366, mean_eps: 0.000000
 3167/5000: episode: 111, duration: 0.283s, episode steps:  23, steps per second:  81, episode reward: 41.179, mean reward:  1.790 [-3.000, 32.080], mean action: 6.783 [0.000, 19.000],  loss: 0.023903, mae: 0.424678, mean_q: 0.480883, mean_eps: 0.000000
 3203/5000: episode: 112, duration: 0.435s, episode steps:  36, steps per second:  83, episode reward: 38.372, mean reward:  1.066 [-2.430, 32.500], mean action: 4.528 [0.000, 19.000],  loss: 0.017532, mae: 0.397789, mean_q: 0.533689, mean_eps: 0.000000
 3222/5000: episode: 113, duration: 0.238s, episode steps:  19, steps per second:  80, episode reward: 41.230, mean reward:  2.170 [-2.412, 31.850], mean action: 3.684 [0.000, 19.000],  loss: 0.020909, mae: 0.415195, mean_q: 0.525515, mean_eps: 0.000000
 3244/5000: episode: 114, duration: 0.273s, episode steps:  22, steps per second:  81, episode reward: 41.157, mean reward:  1.871 [-2.774, 32.271], mean action: 2.591 [0.000, 14.000],  loss: 0.018831, mae: 0.412373, mean_q: 0.493012, mean_eps: 0.000000
 3263/5000: episode: 115, duration: 0.281s, episode steps:  19, steps per second:  68, episode reward: 44.952, mean reward:  2.366 [-2.282, 32.140], mean action: 3.316 [0.000, 19.000],  loss: 0.019593, mae: 0.411950, mean_q: 0.457675, mean_eps: 0.000000
 3293/5000: episode: 116, duration: 0.460s, episode steps:  30, steps per second:  65, episode reward: 41.700, mean reward:  1.390 [-2.510, 32.160], mean action: 3.200 [0.000, 19.000],  loss: 0.018502, mae: 0.413189, mean_q: 0.460063, mean_eps: 0.000000
 3309/5000: episode: 117, duration: 0.203s, episode steps:  16, steps per second:  79, episode reward: 41.269, mean reward:  2.579 [-3.000, 32.120], mean action: 2.812 [0.000, 12.000],  loss: 0.016914, mae: 0.408792, mean_q: 0.441649, mean_eps: 0.000000
 3340/5000: episode: 118, duration: 0.375s, episode steps:  31, steps per second:  83, episode reward: 32.830, mean reward:  1.059 [-2.912, 32.450], mean action: 5.290 [0.000, 19.000],  loss: 0.023308, mae: 0.443516, mean_q: 0.419697, mean_eps: 0.000000
 3383/5000: episode: 119, duration: 0.512s, episode steps:  43, steps per second:  84, episode reward: -32.340, mean reward: -0.752 [-31.994,  2.199], mean action: 4.488 [0.000, 17.000],  loss: 0.017167, mae: 0.410638, mean_q: 0.469722, mean_eps: 0.000000
 3422/5000: episode: 120, duration: 0.479s, episode steps:  39, steps per second:  81, episode reward: 35.703, mean reward:  0.915 [-3.000, 32.486], mean action: 4.205 [0.000, 19.000],  loss: 0.022193, mae: 0.435013, mean_q: 0.436699, mean_eps: 0.000000
 3451/5000: episode: 121, duration: 0.359s, episode steps:  29, steps per second:  81, episode reward: 38.323, mean reward:  1.321 [-2.286, 29.635], mean action: 2.724 [0.000, 16.000],  loss: 0.020707, mae: 0.418013, mean_q: 0.482485, mean_eps: 0.000000
 3473/5000: episode: 122, duration: 0.303s, episode steps:  22, steps per second:  72, episode reward: 44.502, mean reward:  2.023 [-2.430, 32.151], mean action: 4.727 [0.000, 16.000],  loss: 0.020061, mae: 0.404565, mean_q: 0.493575, mean_eps: 0.000000
 3494/5000: episode: 123, duration: 0.261s, episode steps:  21, steps per second:  81, episode reward: 38.802, mean reward:  1.848 [-2.595, 32.531], mean action: 3.714 [0.000, 16.000],  loss: 0.018855, mae: 0.408742, mean_q: 0.521392, mean_eps: 0.000000
 3520/5000: episode: 124, duration: 0.320s, episode steps:  26, steps per second:  81, episode reward: 41.682, mean reward:  1.603 [-3.000, 32.090], mean action: 3.462 [0.000, 16.000],  loss: 0.018333, mae: 0.400852, mean_q: 0.473441, mean_eps: 0.000000
 3542/5000: episode: 125, duration: 0.273s, episode steps:  22, steps per second:  81, episode reward: 38.833, mean reward:  1.765 [-2.242, 32.103], mean action: 3.955 [0.000, 19.000],  loss: 0.017854, mae: 0.415180, mean_q: 0.451322, mean_eps: 0.000000
 3557/5000: episode: 126, duration: 0.194s, episode steps:  15, steps per second:  77, episode reward: 44.601, mean reward:  2.973 [-2.228, 32.210], mean action: 2.600 [0.000, 16.000],  loss: 0.021540, mae: 0.433855, mean_q: 0.479103, mean_eps: 0.000000
 3593/5000: episode: 127, duration: 0.427s, episode steps:  36, steps per second:  84, episode reward: 37.946, mean reward:  1.054 [-3.000, 32.130], mean action: 4.222 [0.000, 16.000],  loss: 0.019841, mae: 0.418796, mean_q: 0.487667, mean_eps: 0.000000
 3611/5000: episode: 128, duration: 0.234s, episode steps:  18, steps per second:  77, episode reward: 47.107, mean reward:  2.617 [-0.333, 31.969], mean action: 2.222 [1.000, 3.000],  loss: 0.017402, mae: 0.414163, mean_q: 0.468651, mean_eps: 0.000000
 3640/5000: episode: 129, duration: 0.463s, episode steps:  29, steps per second:  63, episode reward: 42.000, mean reward:  1.448 [-2.543, 32.290], mean action: 4.724 [0.000, 19.000],  loss: 0.021645, mae: 0.437451, mean_q: 0.441879, mean_eps: 0.000000
 3658/5000: episode: 130, duration: 0.308s, episode steps:  18, steps per second:  58, episode reward: 45.000, mean reward:  2.500 [-2.014, 32.380], mean action: 3.000 [0.000, 19.000],  loss: 0.017743, mae: 0.420662, mean_q: 0.448673, mean_eps: 0.000000
 3688/5000: episode: 131, duration: 0.372s, episode steps:  30, steps per second:  81, episode reward: 38.054, mean reward:  1.268 [-3.000, 31.789], mean action: 6.567 [0.000, 19.000],  loss: 0.018323, mae: 0.426826, mean_q: 0.447340, mean_eps: 0.000000
 3716/5000: episode: 132, duration: 0.345s, episode steps:  28, steps per second:  81, episode reward: 38.685, mean reward:  1.382 [-2.333, 32.124], mean action: 5.929 [1.000, 19.000],  loss: 0.021771, mae: 0.437766, mean_q: 0.499626, mean_eps: 0.000000
 3742/5000: episode: 133, duration: 0.332s, episode steps:  26, steps per second:  78, episode reward: 40.545, mean reward:  1.559 [-3.000, 31.850], mean action: 7.269 [0.000, 19.000],  loss: 0.018465, mae: 0.422840, mean_q: 0.476062, mean_eps: 0.000000
 3789/5000: episode: 134, duration: 0.560s, episode steps:  47, steps per second:  84, episode reward: 41.904, mean reward:  0.892 [-2.216, 32.164], mean action: 7.085 [0.000, 20.000],  loss: 0.019727, mae: 0.430917, mean_q: 0.431057, mean_eps: 0.000000
 3816/5000: episode: 135, duration: 0.340s, episode steps:  27, steps per second:  79, episode reward: 38.790, mean reward:  1.437 [-3.000, 32.350], mean action: 4.778 [0.000, 19.000],  loss: 0.019363, mae: 0.424153, mean_q: 0.485903, mean_eps: 0.000000
 3840/5000: episode: 136, duration: 0.295s, episode steps:  24, steps per second:  81, episode reward: 41.208, mean reward:  1.717 [-2.869, 32.393], mean action: 2.583 [0.000, 16.000],  loss: 0.020107, mae: 0.419953, mean_q: 0.478128, mean_eps: 0.000000
 3857/5000: episode: 137, duration: 0.221s, episode steps:  17, steps per second:  77, episode reward: 44.241, mean reward:  2.602 [-3.000, 32.152], mean action: 3.529 [0.000, 16.000],  loss: 0.018151, mae: 0.411614, mean_q: 0.493274, mean_eps: 0.000000
 3884/5000: episode: 138, duration: 0.331s, episode steps:  27, steps per second:  81, episode reward: 36.000, mean reward:  1.333 [-2.470, 32.580], mean action: 3.185 [0.000, 16.000],  loss: 0.021672, mae: 0.428459, mean_q: 0.471330, mean_eps: 0.000000
 3916/5000: episode: 139, duration: 0.389s, episode steps:  32, steps per second:  82, episode reward: 33.000, mean reward:  1.031 [-3.000, 29.663], mean action: 4.469 [0.000, 16.000],  loss: 0.021959, mae: 0.424928, mean_q: 0.477484, mean_eps: 0.000000
 3939/5000: episode: 140, duration: 0.283s, episode steps:  23, steps per second:  81, episode reward: 38.355, mean reward:  1.668 [-2.465, 31.555], mean action: 3.435 [0.000, 16.000],  loss: 0.023690, mae: 0.429926, mean_q: 0.507674, mean_eps: 0.000000
 3966/5000: episode: 141, duration: 0.325s, episode steps:  27, steps per second:  83, episode reward: 38.764, mean reward:  1.436 [-2.192, 32.264], mean action: 3.000 [0.000, 14.000],  loss: 0.023862, mae: 0.428309, mean_q: 0.516577, mean_eps: 0.000000
 3999/5000: episode: 142, duration: 0.396s, episode steps:  33, steps per second:  83, episode reward: 38.642, mean reward:  1.171 [-2.591, 32.510], mean action: 2.879 [0.000, 12.000],  loss: 0.020106, mae: 0.415872, mean_q: 0.497104, mean_eps: 0.000000
 4029/5000: episode: 143, duration: 0.363s, episode steps:  30, steps per second:  83, episode reward: 41.595, mean reward:  1.387 [-2.283, 32.320], mean action: 2.667 [0.000, 12.000],  loss: 0.019504, mae: 0.411291, mean_q: 0.524926, mean_eps: 0.000000
 4063/5000: episode: 144, duration: 0.421s, episode steps:  34, steps per second:  81, episode reward: -32.400, mean reward: -0.953 [-30.331,  2.488], mean action: 6.647 [0.000, 20.000],  loss: 0.016470, mae: 0.413428, mean_q: 0.430777, mean_eps: 0.000000
 4093/5000: episode: 145, duration: 0.360s, episode steps:  30, steps per second:  83, episode reward: 36.000, mean reward:  1.200 [-2.504, 32.180], mean action: 3.067 [0.000, 9.000],  loss: 0.018310, mae: 0.434652, mean_q: 0.363584, mean_eps: 0.000000
 4109/5000: episode: 146, duration: 0.208s, episode steps:  16, steps per second:  77, episode reward: 47.349, mean reward:  2.959 [-0.543, 32.380], mean action: 1.875 [1.000, 3.000],  loss: 0.015880, mae: 0.405248, mean_q: 0.432541, mean_eps: 0.000000
 4134/5000: episode: 147, duration: 0.309s, episode steps:  25, steps per second:  81, episode reward: 38.421, mean reward:  1.537 [-3.000, 32.060], mean action: 3.880 [0.000, 14.000],  loss: 0.020140, mae: 0.431485, mean_q: 0.403142, mean_eps: 0.000000
 4157/5000: episode: 148, duration: 0.285s, episode steps:  23, steps per second:  81, episode reward: 38.401, mean reward:  1.670 [-3.000, 32.006], mean action: 3.609 [1.000, 11.000],  loss: 0.019030, mae: 0.428482, mean_q: 0.369456, mean_eps: 0.000000
 4182/5000: episode: 149, duration: 0.310s, episode steps:  25, steps per second:  81, episode reward: 43.505, mean reward:  1.740 [-2.602, 32.090], mean action: 3.200 [0.000, 20.000],  loss: 0.021912, mae: 0.442056, mean_q: 0.420321, mean_eps: 0.000000
 4207/5000: episode: 150, duration: 0.310s, episode steps:  25, steps per second:  81, episode reward: 38.218, mean reward:  1.529 [-2.937, 32.450], mean action: 6.280 [0.000, 15.000],  loss: 0.022549, mae: 0.444069, mean_q: 0.411291, mean_eps: 0.000000
 4234/5000: episode: 151, duration: 0.331s, episode steps:  27, steps per second:  82, episode reward: 37.641, mean reward:  1.394 [-2.525, 31.326], mean action: 3.815 [0.000, 14.000],  loss: 0.020597, mae: 0.433068, mean_q: 0.449191, mean_eps: 0.000000
 4287/5000: episode: 152, duration: 0.633s, episode steps:  53, steps per second:  84, episode reward: 33.000, mean reward:  0.623 [-2.635, 32.050], mean action: 4.189 [0.000, 21.000],  loss: 0.018200, mae: 0.416569, mean_q: 0.450821, mean_eps: 0.000000
 4322/5000: episode: 153, duration: 0.426s, episode steps:  35, steps per second:  82, episode reward: 41.621, mean reward:  1.189 [-2.461, 33.000], mean action: 2.829 [0.000, 15.000],  loss: 0.020631, mae: 0.429569, mean_q: 0.479591, mean_eps: 0.000000
 4348/5000: episode: 154, duration: 0.320s, episode steps:  26, steps per second:  81, episode reward: 41.637, mean reward:  1.601 [-2.021, 31.967], mean action: 2.923 [0.000, 14.000],  loss: 0.021849, mae: 0.435620, mean_q: 0.501001, mean_eps: 0.000000
 4372/5000: episode: 155, duration: 0.304s, episode steps:  24, steps per second:  79, episode reward: 38.611, mean reward:  1.609 [-3.000, 32.240], mean action: 2.375 [0.000, 12.000],  loss: 0.021378, mae: 0.433453, mean_q: 0.486665, mean_eps: 0.000000
 4390/5000: episode: 156, duration: 0.239s, episode steps:  18, steps per second:  75, episode reward: 41.649, mean reward:  2.314 [-2.330, 31.819], mean action: 3.722 [0.000, 14.000],  loss: 0.023727, mae: 0.443131, mean_q: 0.453582, mean_eps: 0.000000
 4420/5000: episode: 157, duration: 0.379s, episode steps:  30, steps per second:  79, episode reward: 41.142, mean reward:  1.371 [-3.000, 31.987], mean action: 3.133 [0.000, 19.000],  loss: 0.020721, mae: 0.429289, mean_q: 0.418625, mean_eps: 0.000000
 4438/5000: episode: 158, duration: 0.234s, episode steps:  18, steps per second:  77, episode reward: 41.570, mean reward:  2.309 [-2.156, 34.360], mean action: 3.778 [0.000, 15.000],  loss: 0.020629, mae: 0.436213, mean_q: 0.447037, mean_eps: 0.000000
 4473/5000: episode: 159, duration: 0.424s, episode steps:  35, steps per second:  83, episode reward: 37.949, mean reward:  1.084 [-2.124, 32.160], mean action: 4.057 [0.000, 21.000],  loss: 0.019892, mae: 0.432626, mean_q: 0.427154, mean_eps: 0.000000
 4500/5000: episode: 160, duration: 0.336s, episode steps:  27, steps per second:  80, episode reward: 38.172, mean reward:  1.414 [-2.428, 32.437], mean action: 4.444 [0.000, 21.000],  loss: 0.018299, mae: 0.409322, mean_q: 0.453121, mean_eps: 0.000000
 4514/5000: episode: 161, duration: 0.178s, episode steps:  14, steps per second:  79, episode reward: 44.698, mean reward:  3.193 [-2.831, 32.092], mean action: 1.929 [0.000, 19.000],  loss: 0.017947, mae: 0.399284, mean_q: 0.451128, mean_eps: 0.000000
 4532/5000: episode: 162, duration: 0.222s, episode steps:  18, steps per second:  81, episode reward: 39.000, mean reward:  2.167 [-2.252, 32.390], mean action: 2.667 [0.000, 12.000],  loss: 0.023430, mae: 0.427876, mean_q: 0.457940, mean_eps: 0.000000
 4552/5000: episode: 163, duration: 0.254s, episode steps:  20, steps per second:  79, episode reward: 39.000, mean reward:  1.950 [-3.000, 29.944], mean action: 1.300 [0.000, 12.000],  loss: 0.017241, mae: 0.399114, mean_q: 0.511190, mean_eps: 0.000000
 4572/5000: episode: 164, duration: 0.245s, episode steps:  20, steps per second:  81, episode reward: 41.938, mean reward:  2.097 [-2.760, 32.290], mean action: 4.150 [0.000, 20.000],  loss: 0.023784, mae: 0.427511, mean_q: 0.491736, mean_eps: 0.000000
 4594/5000: episode: 165, duration: 0.268s, episode steps:  22, steps per second:  82, episode reward: 41.651, mean reward:  1.893 [-2.494, 32.181], mean action: 3.136 [0.000, 19.000],  loss: 0.018721, mae: 0.396502, mean_q: 0.487105, mean_eps: 0.000000
 4614/5000: episode: 166, duration: 0.259s, episode steps:  20, steps per second:  77, episode reward: 44.823, mean reward:  2.241 [-2.233, 32.230], mean action: 1.200 [0.000, 19.000],  loss: 0.016355, mae: 0.381216, mean_q: 0.441605, mean_eps: 0.000000
 4642/5000: episode: 167, duration: 0.341s, episode steps:  28, steps per second:  82, episode reward: 46.666, mean reward:  1.667 [-0.365, 32.310], mean action: 1.214 [0.000, 5.000],  loss: 0.019127, mae: 0.399943, mean_q: 0.482039, mean_eps: 0.000000
 4670/5000: episode: 168, duration: 0.341s, episode steps:  28, steps per second:  82, episode reward: 41.035, mean reward:  1.466 [-2.525, 32.295], mean action: 3.286 [0.000, 19.000],  loss: 0.017988, mae: 0.386205, mean_q: 0.461568, mean_eps: 0.000000
 4687/5000: episode: 169, duration: 0.221s, episode steps:  17, steps per second:  77, episode reward: 44.401, mean reward:  2.612 [-2.360, 32.150], mean action: 3.588 [0.000, 19.000],  loss: 0.022782, mae: 0.411807, mean_q: 0.467944, mean_eps: 0.000000
 4700/5000: episode: 170, duration: 0.169s, episode steps:  13, steps per second:  77, episode reward: 44.654, mean reward:  3.435 [-2.631, 33.000], mean action: 2.000 [1.000, 3.000],  loss: 0.019934, mae: 0.402265, mean_q: 0.480365, mean_eps: 0.000000
 4722/5000: episode: 171, duration: 0.278s, episode steps:  22, steps per second:  79, episode reward: 43.387, mean reward:  1.972 [-2.817, 31.918], mean action: 5.364 [0.000, 14.000],  loss: 0.022567, mae: 0.420453, mean_q: 0.464667, mean_eps: 0.000000
 4753/5000: episode: 172, duration: 0.368s, episode steps:  31, steps per second:  84, episode reward: 35.060, mean reward:  1.131 [-3.000, 32.146], mean action: 2.516 [0.000, 11.000],  loss: 0.016231, mae: 0.395916, mean_q: 0.477683, mean_eps: 0.000000
 4773/5000: episode: 173, duration: 0.246s, episode steps:  20, steps per second:  81, episode reward: 34.000, mean reward:  1.700 [-3.000, 30.157], mean action: 3.300 [0.000, 11.000],  loss: 0.017784, mae: 0.403136, mean_q: 0.538706, mean_eps: 0.000000
 4800/5000: episode: 174, duration: 0.341s, episode steps:  27, steps per second:  79, episode reward: 38.516, mean reward:  1.427 [-2.712, 32.040], mean action: 6.111 [0.000, 21.000],  loss: 0.018907, mae: 0.404364, mean_q: 0.540119, mean_eps: 0.000000
 4835/5000: episode: 175, duration: 0.432s, episode steps:  35, steps per second:  81, episode reward: 38.776, mean reward:  1.108 [-3.000, 32.210], mean action: 4.171 [0.000, 15.000],  loss: 0.020878, mae: 0.415315, mean_q: 0.525523, mean_eps: 0.000000
 4858/5000: episode: 176, duration: 0.356s, episode steps:  23, steps per second:  65, episode reward: 41.052, mean reward:  1.785 [-2.602, 31.793], mean action: 4.783 [0.000, 19.000],  loss: 0.017116, mae: 0.403186, mean_q: 0.500347, mean_eps: 0.000000
 4889/5000: episode: 177, duration: 0.378s, episode steps:  31, steps per second:  82, episode reward: 43.923, mean reward:  1.417 [-2.834, 32.350], mean action: 1.968 [0.000, 19.000],  loss: 0.019727, mae: 0.416393, mean_q: 0.530463, mean_eps: 0.000000
 4912/5000: episode: 178, duration: 0.288s, episode steps:  23, steps per second:  80, episode reward: 41.358, mean reward:  1.798 [-2.428, 31.992], mean action: 4.957 [0.000, 20.000],  loss: 0.020870, mae: 0.423788, mean_q: 0.476004, mean_eps: 0.000000
 4939/5000: episode: 179, duration: 0.331s, episode steps:  27, steps per second:  82, episode reward: 41.540, mean reward:  1.539 [-2.255, 32.006], mean action: 2.519 [0.000, 14.000],  loss: 0.020757, mae: 0.413381, mean_q: 0.488409, mean_eps: 0.000000
 4985/5000: episode: 180, duration: 0.542s, episode steps:  46, steps per second:  85, episode reward: 35.374, mean reward:  0.769 [-3.000, 32.290], mean action: 3.761 [0.000, 16.000],  loss: 0.019489, mae: 0.407845, mean_q: 0.451900, mean_eps: 0.000000
done, took 56.689 seconds
DQN Evaluation: 3279 victories out of 3901 episodes
Training for 5000 steps ...
   29/5000: episode: 1, duration: 0.221s, episode steps:  29, steps per second: 131, episode reward: 32.545, mean reward:  1.122 [-2.710, 32.523], mean action: 5.828 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   64/5000: episode: 2, duration: 0.223s, episode steps:  35, steps per second: 157, episode reward: 32.574, mean reward:  0.931 [-3.000, 32.070], mean action: 4.857 [1.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   83/5000: episode: 3, duration: 0.136s, episode steps:  19, steps per second: 140, episode reward: 44.435, mean reward:  2.339 [-2.241, 32.480], mean action: 0.947 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  103/5000: episode: 4, duration: 0.136s, episode steps:  20, steps per second: 147, episode reward: 38.562, mean reward:  1.928 [-2.529, 32.472], mean action: 3.650 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  132/5000: episode: 5, duration: 0.189s, episode steps:  29, steps per second: 153, episode reward: -35.140, mean reward: -1.212 [-32.188,  2.310], mean action: 8.000 [0.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  154/5000: episode: 6, duration: 0.142s, episode steps:  22, steps per second: 155, episode reward: 41.053, mean reward:  1.866 [-2.431, 32.604], mean action: 2.864 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  176/5000: episode: 7, duration: 0.140s, episode steps:  22, steps per second: 158, episode reward: 35.696, mean reward:  1.623 [-3.000, 32.820], mean action: 5.455 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  199/5000: episode: 8, duration: 0.146s, episode steps:  23, steps per second: 158, episode reward: 32.881, mean reward:  1.430 [-3.000, 32.591], mean action: 4.261 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  218/5000: episode: 9, duration: 0.122s, episode steps:  19, steps per second: 156, episode reward: 41.240, mean reward:  2.171 [-2.360, 32.260], mean action: 4.737 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  237/5000: episode: 10, duration: 0.128s, episode steps:  19, steps per second: 149, episode reward: 47.387, mean reward:  2.494 [-0.108, 32.410], mean action: 2.579 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  249/5000: episode: 11, duration: 0.090s, episode steps:  12, steps per second: 133, episode reward: 41.318, mean reward:  3.443 [-3.000, 32.910], mean action: 3.000 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  280/5000: episode: 12, duration: 0.190s, episode steps:  31, steps per second: 163, episode reward: 32.689, mean reward:  1.054 [-3.000, 32.689], mean action: 6.613 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  306/5000: episode: 13, duration: 0.164s, episode steps:  26, steps per second: 159, episode reward: 35.438, mean reward:  1.363 [-2.135, 32.900], mean action: 4.731 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  327/5000: episode: 14, duration: 0.143s, episode steps:  21, steps per second: 147, episode reward: 38.511, mean reward:  1.834 [-2.468, 32.784], mean action: 5.000 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  347/5000: episode: 15, duration: 0.124s, episode steps:  20, steps per second: 161, episode reward: 38.134, mean reward:  1.907 [-2.460, 31.893], mean action: 3.950 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  372/5000: episode: 16, duration: 0.158s, episode steps:  25, steps per second: 158, episode reward: 36.000, mean reward:  1.440 [-2.329, 32.290], mean action: 5.840 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  408/5000: episode: 17, duration: 0.218s, episode steps:  36, steps per second: 165, episode reward: 36.000, mean reward:  1.000 [-2.219, 32.160], mean action: 4.472 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  429/5000: episode: 18, duration: 0.137s, episode steps:  21, steps per second: 153, episode reward: 38.391, mean reward:  1.828 [-2.254, 32.300], mean action: 4.238 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  456/5000: episode: 19, duration: 0.166s, episode steps:  27, steps per second: 162, episode reward: 32.902, mean reward:  1.219 [-2.900, 32.812], mean action: 4.630 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  474/5000: episode: 20, duration: 0.122s, episode steps:  18, steps per second: 147, episode reward: 41.620, mean reward:  2.312 [-2.414, 33.000], mean action: 6.333 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  499/5000: episode: 21, duration: 0.158s, episode steps:  25, steps per second: 158, episode reward: 39.000, mean reward:  1.560 [-2.431, 32.650], mean action: 6.680 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  529/5000: episode: 22, duration: 0.182s, episode steps:  30, steps per second: 165, episode reward: -35.810, mean reward: -1.194 [-32.245,  2.175], mean action: 9.167 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  548/5000: episode: 23, duration: 0.127s, episode steps:  19, steps per second: 149, episode reward: 38.261, mean reward:  2.014 [-2.903, 32.300], mean action: 5.737 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  573/5000: episode: 24, duration: 0.158s, episode steps:  25, steps per second: 158, episode reward: -35.170, mean reward: -1.407 [-31.863,  2.540], mean action: 5.120 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  592/5000: episode: 25, duration: 0.130s, episode steps:  19, steps per second: 146, episode reward: 36.000, mean reward:  1.895 [-2.525, 32.450], mean action: 3.737 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  623/5000: episode: 26, duration: 0.193s, episode steps:  31, steps per second: 161, episode reward: 35.215, mean reward:  1.136 [-2.155, 31.979], mean action: 5.258 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  640/5000: episode: 27, duration: 0.119s, episode steps:  17, steps per second: 142, episode reward: 41.786, mean reward:  2.458 [-2.458, 32.081], mean action: 3.000 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  655/5000: episode: 28, duration: 0.114s, episode steps:  15, steps per second: 132, episode reward: 41.163, mean reward:  2.744 [-2.074, 32.350], mean action: 3.333 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  682/5000: episode: 29, duration: 0.172s, episode steps:  27, steps per second: 157, episode reward: 35.617, mean reward:  1.319 [-2.431, 32.254], mean action: 4.074 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  708/5000: episode: 30, duration: 0.173s, episode steps:  26, steps per second: 150, episode reward: 33.000, mean reward:  1.269 [-2.940, 33.000], mean action: 6.385 [0.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  719/5000: episode: 31, duration: 0.086s, episode steps:  11, steps per second: 129, episode reward: 44.652, mean reward:  4.059 [-2.009, 33.000], mean action: 3.000 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  745/5000: episode: 32, duration: 0.159s, episode steps:  26, steps per second: 164, episode reward: 35.269, mean reward:  1.356 [-2.903, 32.230], mean action: 7.077 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  754/5000: episode: 33, duration: 0.074s, episode steps:   9, steps per second: 122, episode reward: 45.000, mean reward:  5.000 [-2.191, 33.365], mean action: 3.667 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  777/5000: episode: 34, duration: 0.152s, episode steps:  23, steps per second: 152, episode reward: -32.110, mean reward: -1.396 [-31.782,  2.951], mean action: 4.826 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  803/5000: episode: 35, duration: 0.167s, episode steps:  26, steps per second: 156, episode reward: -32.600, mean reward: -1.254 [-32.199,  2.648], mean action: 4.538 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  828/5000: episode: 36, duration: 0.160s, episode steps:  25, steps per second: 156, episode reward: 35.300, mean reward:  1.412 [-2.340, 31.703], mean action: 2.920 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  856/5000: episode: 37, duration: 0.168s, episode steps:  28, steps per second: 167, episode reward: -32.260, mean reward: -1.152 [-32.260,  2.679], mean action: 8.536 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  874/5000: episode: 38, duration: 0.123s, episode steps:  18, steps per second: 146, episode reward: 41.246, mean reward:  2.291 [-2.675, 33.000], mean action: 3.611 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  898/5000: episode: 39, duration: 0.153s, episode steps:  24, steps per second: 157, episode reward: -32.010, mean reward: -1.334 [-32.052,  2.370], mean action: 6.958 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  916/5000: episode: 40, duration: 0.123s, episode steps:  18, steps per second: 146, episode reward: 35.311, mean reward:  1.962 [-3.000, 32.280], mean action: 3.556 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  936/5000: episode: 41, duration: 0.132s, episode steps:  20, steps per second: 152, episode reward: 35.333, mean reward:  1.767 [-2.685, 32.520], mean action: 3.450 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  952/5000: episode: 42, duration: 0.111s, episode steps:  16, steps per second: 145, episode reward: 41.940, mean reward:  2.621 [-2.851, 33.000], mean action: 2.188 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  977/5000: episode: 43, duration: 0.158s, episode steps:  25, steps per second: 159, episode reward: 33.000, mean reward:  1.320 [-3.000, 32.940], mean action: 4.080 [0.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1001/5000: episode: 44, duration: 0.164s, episode steps:  24, steps per second: 146, episode reward: 37.409, mean reward:  1.559 [-2.339, 31.966], mean action: 3.375 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1027/5000: episode: 45, duration: 0.322s, episode steps:  26, steps per second:  81, episode reward: 38.073, mean reward:  1.464 [-2.105, 33.000], mean action: 3.962 [0.000, 18.000],  loss: 0.023538, mae: 0.423934, mean_q: 0.469609, mean_eps: 0.000000
 1048/5000: episode: 46, duration: 0.260s, episode steps:  21, steps per second:  81, episode reward: 32.017, mean reward:  1.525 [-3.000, 32.030], mean action: 4.048 [0.000, 16.000],  loss: 0.019868, mae: 0.407425, mean_q: 0.516047, mean_eps: 0.000000
 1068/5000: episode: 47, duration: 0.251s, episode steps:  20, steps per second:  80, episode reward: 38.739, mean reward:  1.937 [-2.511, 32.459], mean action: 4.450 [0.000, 19.000],  loss: 0.018887, mae: 0.409002, mean_q: 0.496088, mean_eps: 0.000000
 1095/5000: episode: 48, duration: 0.331s, episode steps:  27, steps per second:  82, episode reward: 36.000, mean reward:  1.333 [-2.445, 32.090], mean action: 3.407 [0.000, 19.000],  loss: 0.019371, mae: 0.420581, mean_q: 0.422590, mean_eps: 0.000000
 1122/5000: episode: 49, duration: 0.333s, episode steps:  27, steps per second:  81, episode reward: -38.550, mean reward: -1.428 [-31.748,  2.540], mean action: 5.222 [0.000, 15.000],  loss: 0.021149, mae: 0.432992, mean_q: 0.415260, mean_eps: 0.000000
 1149/5000: episode: 50, duration: 0.323s, episode steps:  27, steps per second:  84, episode reward: -32.970, mean reward: -1.221 [-32.070,  2.620], mean action: 4.963 [0.000, 15.000],  loss: 0.019405, mae: 0.416209, mean_q: 0.425697, mean_eps: 0.000000
 1171/5000: episode: 51, duration: 0.274s, episode steps:  22, steps per second:  80, episode reward: 41.020, mean reward:  1.865 [-2.696, 32.360], mean action: 5.636 [0.000, 15.000],  loss: 0.017768, mae: 0.396694, mean_q: 0.465388, mean_eps: 0.000000
 1196/5000: episode: 52, duration: 0.304s, episode steps:  25, steps per second:  82, episode reward: -32.590, mean reward: -1.304 [-32.001,  2.210], mean action: 4.840 [0.000, 16.000],  loss: 0.019577, mae: 0.410169, mean_q: 0.462621, mean_eps: 0.000000
 1226/5000: episode: 53, duration: 0.366s, episode steps:  30, steps per second:  82, episode reward: 32.953, mean reward:  1.098 [-3.000, 33.000], mean action: 4.100 [0.000, 15.000],  loss: 0.022199, mae: 0.413460, mean_q: 0.547899, mean_eps: 0.000000
 1254/5000: episode: 54, duration: 0.343s, episode steps:  28, steps per second:  82, episode reward: 38.481, mean reward:  1.374 [-2.109, 31.944], mean action: 3.536 [0.000, 14.000],  loss: 0.022685, mae: 0.427806, mean_q: 0.516821, mean_eps: 0.000000
 1269/5000: episode: 55, duration: 0.198s, episode steps:  15, steps per second:  76, episode reward: 44.528, mean reward:  2.969 [-3.000, 32.348], mean action: 2.867 [0.000, 14.000],  loss: 0.019413, mae: 0.403908, mean_q: 0.566066, mean_eps: 0.000000
 1288/5000: episode: 56, duration: 0.240s, episode steps:  19, steps per second:  79, episode reward: 38.437, mean reward:  2.023 [-2.424, 32.240], mean action: 4.526 [0.000, 14.000],  loss: 0.021491, mae: 0.423024, mean_q: 0.507596, mean_eps: 0.000000
 1311/5000: episode: 57, duration: 0.292s, episode steps:  23, steps per second:  79, episode reward: 32.903, mean reward:  1.431 [-2.476, 32.323], mean action: 5.174 [0.000, 14.000],  loss: 0.019592, mae: 0.421173, mean_q: 0.442120, mean_eps: 0.000000
 1333/5000: episode: 58, duration: 0.272s, episode steps:  22, steps per second:  81, episode reward: -33.000, mean reward: -1.500 [-29.870,  2.360], mean action: 8.136 [0.000, 19.000],  loss: 0.019312, mae: 0.420948, mean_q: 0.474866, mean_eps: 0.000000
 1360/5000: episode: 59, duration: 0.337s, episode steps:  27, steps per second:  80, episode reward: -32.910, mean reward: -1.219 [-31.933,  2.560], mean action: 6.407 [0.000, 19.000],  loss: 0.019172, mae: 0.430741, mean_q: 0.418566, mean_eps: 0.000000
 1384/5000: episode: 60, duration: 0.336s, episode steps:  24, steps per second:  72, episode reward: 32.216, mean reward:  1.342 [-3.000, 32.240], mean action: 4.333 [0.000, 19.000],  loss: 0.018201, mae: 0.415142, mean_q: 0.451358, mean_eps: 0.000000
 1413/5000: episode: 61, duration: 0.547s, episode steps:  29, steps per second:  53, episode reward: 35.380, mean reward:  1.220 [-3.000, 32.180], mean action: 5.966 [0.000, 14.000],  loss: 0.021564, mae: 0.426476, mean_q: 0.478182, mean_eps: 0.000000
 1453/5000: episode: 62, duration: 0.478s, episode steps:  40, steps per second:  84, episode reward: -35.160, mean reward: -0.879 [-32.030,  2.970], mean action: 4.800 [0.000, 12.000],  loss: 0.022353, mae: 0.439862, mean_q: 0.475121, mean_eps: 0.000000
 1483/5000: episode: 63, duration: 0.404s, episode steps:  30, steps per second:  74, episode reward: 30.000, mean reward:  1.000 [-2.453, 30.246], mean action: 8.167 [0.000, 20.000],  loss: 0.020073, mae: 0.424598, mean_q: 0.491933, mean_eps: 0.000000
 1508/5000: episode: 64, duration: 0.306s, episode steps:  25, steps per second:  82, episode reward: 32.336, mean reward:  1.293 [-3.000, 31.706], mean action: 9.120 [1.000, 20.000],  loss: 0.023736, mae: 0.441715, mean_q: 0.472630, mean_eps: 0.000000
 1533/5000: episode: 65, duration: 0.305s, episode steps:  25, steps per second:  82, episode reward: 33.000, mean reward:  1.320 [-3.000, 32.200], mean action: 3.840 [0.000, 20.000],  loss: 0.020435, mae: 0.438246, mean_q: 0.476902, mean_eps: 0.000000
 1555/5000: episode: 66, duration: 0.266s, episode steps:  22, steps per second:  83, episode reward: -38.560, mean reward: -1.753 [-32.032,  2.470], mean action: 9.136 [0.000, 19.000],  loss: 0.016807, mae: 0.406194, mean_q: 0.444078, mean_eps: 0.000000
 1577/5000: episode: 67, duration: 0.279s, episode steps:  22, steps per second:  79, episode reward: 35.749, mean reward:  1.625 [-2.528, 32.530], mean action: 5.227 [0.000, 20.000],  loss: 0.018296, mae: 0.416598, mean_q: 0.432202, mean_eps: 0.000000
 1596/5000: episode: 68, duration: 0.235s, episode steps:  19, steps per second:  81, episode reward: -41.100, mean reward: -2.163 [-32.391,  2.080], mean action: 8.316 [1.000, 15.000],  loss: 0.016008, mae: 0.405172, mean_q: 0.419825, mean_eps: 0.000000
 1618/5000: episode: 69, duration: 0.279s, episode steps:  22, steps per second:  79, episode reward: 38.041, mean reward:  1.729 [-2.424, 32.299], mean action: 4.091 [0.000, 13.000],  loss: 0.022086, mae: 0.432232, mean_q: 0.431469, mean_eps: 0.000000
 1646/5000: episode: 70, duration: 0.352s, episode steps:  28, steps per second:  80, episode reward: 35.202, mean reward:  1.257 [-3.000, 32.110], mean action: 4.857 [0.000, 15.000],  loss: 0.022691, mae: 0.428763, mean_q: 0.445901, mean_eps: 0.000000
 1693/5000: episode: 71, duration: 0.558s, episode steps:  47, steps per second:  84, episode reward: 38.708, mean reward:  0.824 [-2.289, 32.880], mean action: 3.298 [0.000, 12.000],  loss: 0.017214, mae: 0.399883, mean_q: 0.510432, mean_eps: 0.000000
 1727/5000: episode: 72, duration: 0.408s, episode steps:  34, steps per second:  83, episode reward: 43.202, mean reward:  1.271 [-2.213, 32.510], mean action: 3.206 [0.000, 21.000],  loss: 0.018475, mae: 0.410664, mean_q: 0.515684, mean_eps: 0.000000
 1746/5000: episode: 73, duration: 0.255s, episode steps:  19, steps per second:  75, episode reward: -36.000, mean reward: -1.895 [-33.000,  2.530], mean action: 7.789 [0.000, 19.000],  loss: 0.020815, mae: 0.428949, mean_q: 0.442733, mean_eps: 0.000000
 1783/5000: episode: 74, duration: 0.444s, episode steps:  37, steps per second:  83, episode reward: -35.560, mean reward: -0.961 [-31.737,  3.000], mean action: 10.486 [0.000, 20.000],  loss: 0.016344, mae: 0.413912, mean_q: 0.393462, mean_eps: 0.000000
 1803/5000: episode: 75, duration: 0.251s, episode steps:  20, steps per second:  80, episode reward: 38.656, mean reward:  1.933 [-2.636, 31.956], mean action: 3.850 [0.000, 12.000],  loss: 0.022207, mae: 0.432101, mean_q: 0.408419, mean_eps: 0.000000
 1823/5000: episode: 76, duration: 0.253s, episode steps:  20, steps per second:  79, episode reward: 36.000, mean reward:  1.800 [-2.453, 32.870], mean action: 3.650 [0.000, 12.000],  loss: 0.019789, mae: 0.423828, mean_q: 0.443550, mean_eps: 0.000000
 1847/5000: episode: 77, duration: 0.298s, episode steps:  24, steps per second:  80, episode reward: 35.209, mean reward:  1.467 [-3.000, 32.110], mean action: 5.750 [0.000, 15.000],  loss: 0.021212, mae: 0.427906, mean_q: 0.460350, mean_eps: 0.000000
 1868/5000: episode: 78, duration: 0.257s, episode steps:  21, steps per second:  82, episode reward: 35.168, mean reward:  1.675 [-2.498, 32.240], mean action: 4.429 [0.000, 19.000],  loss: 0.017189, mae: 0.408591, mean_q: 0.460841, mean_eps: 0.000000
 1894/5000: episode: 79, duration: 0.317s, episode steps:  26, steps per second:  82, episode reward: -32.250, mean reward: -1.240 [-31.779,  2.900], mean action: 4.423 [0.000, 19.000],  loss: 0.023856, mae: 0.444049, mean_q: 0.461159, mean_eps: 0.000000
 1924/5000: episode: 80, duration: 0.366s, episode steps:  30, steps per second:  82, episode reward: 35.277, mean reward:  1.176 [-2.477, 31.577], mean action: 5.000 [0.000, 19.000],  loss: 0.018830, mae: 0.422549, mean_q: 0.421571, mean_eps: 0.000000
 1953/5000: episode: 81, duration: 0.353s, episode steps:  29, steps per second:  82, episode reward: 35.082, mean reward:  1.210 [-2.257, 32.120], mean action: 3.172 [0.000, 12.000],  loss: 0.018860, mae: 0.424456, mean_q: 0.459677, mean_eps: 0.000000
 1975/5000: episode: 82, duration: 0.330s, episode steps:  22, steps per second:  67, episode reward: 32.674, mean reward:  1.485 [-3.000, 32.570], mean action: 5.182 [0.000, 19.000],  loss: 0.020251, mae: 0.420509, mean_q: 0.513158, mean_eps: 0.000000
 1996/5000: episode: 83, duration: 0.264s, episode steps:  21, steps per second:  79, episode reward: 35.024, mean reward:  1.668 [-2.445, 32.174], mean action: 5.476 [0.000, 19.000],  loss: 0.019508, mae: 0.420806, mean_q: 0.446723, mean_eps: 0.000000
 2025/5000: episode: 84, duration: 0.370s, episode steps:  29, steps per second:  78, episode reward: -35.330, mean reward: -1.218 [-32.321,  2.490], mean action: 10.172 [1.000, 20.000],  loss: 0.020643, mae: 0.433523, mean_q: 0.394957, mean_eps: 0.000000
 2044/5000: episode: 85, duration: 0.242s, episode steps:  19, steps per second:  78, episode reward: 41.411, mean reward:  2.180 [-2.268, 32.470], mean action: 3.789 [0.000, 19.000],  loss: 0.019689, mae: 0.419928, mean_q: 0.437096, mean_eps: 0.000000
 2071/5000: episode: 86, duration: 0.343s, episode steps:  27, steps per second:  79, episode reward: -35.360, mean reward: -1.310 [-32.076,  2.264], mean action: 5.815 [0.000, 19.000],  loss: 0.020123, mae: 0.419337, mean_q: 0.443468, mean_eps: 0.000000
 2088/5000: episode: 87, duration: 0.219s, episode steps:  17, steps per second:  78, episode reward: 38.549, mean reward:  2.268 [-2.805, 33.000], mean action: 4.647 [0.000, 19.000],  loss: 0.021308, mae: 0.421866, mean_q: 0.428026, mean_eps: 0.000000
 2115/5000: episode: 88, duration: 0.330s, episode steps:  27, steps per second:  82, episode reward: 35.438, mean reward:  1.313 [-2.167, 31.988], mean action: 5.444 [0.000, 19.000],  loss: 0.019595, mae: 0.416170, mean_q: 0.426239, mean_eps: 0.000000
 2147/5000: episode: 89, duration: 0.389s, episode steps:  32, steps per second:  82, episode reward: -33.000, mean reward: -1.031 [-30.127,  2.300], mean action: 4.562 [0.000, 18.000],  loss: 0.018976, mae: 0.412004, mean_q: 0.388293, mean_eps: 0.000000
 2162/5000: episode: 90, duration: 0.255s, episode steps:  15, steps per second:  59, episode reward: 44.352, mean reward:  2.957 [-2.214, 33.000], mean action: 1.267 [0.000, 11.000],  loss: 0.017850, mae: 0.403836, mean_q: 0.441909, mean_eps: 0.000000
 2182/5000: episode: 91, duration: 0.251s, episode steps:  20, steps per second:  80, episode reward: 34.994, mean reward:  1.750 [-3.000, 32.149], mean action: 5.300 [0.000, 21.000],  loss: 0.019228, mae: 0.408059, mean_q: 0.477906, mean_eps: 0.000000
 2197/5000: episode: 92, duration: 0.194s, episode steps:  15, steps per second:  77, episode reward: 42.000, mean reward:  2.800 [-2.517, 33.000], mean action: 2.667 [0.000, 12.000],  loss: 0.020531, mae: 0.415509, mean_q: 0.396653, mean_eps: 0.000000
 2220/5000: episode: 93, duration: 0.279s, episode steps:  23, steps per second:  82, episode reward: -32.000, mean reward: -1.391 [-30.023,  2.904], mean action: 8.652 [0.000, 14.000],  loss: 0.020679, mae: 0.423064, mean_q: 0.417162, mean_eps: 0.000000
 2251/5000: episode: 94, duration: 0.378s, episode steps:  31, steps per second:  82, episode reward: 32.148, mean reward:  1.037 [-3.000, 31.980], mean action: 4.677 [0.000, 14.000],  loss: 0.018607, mae: 0.397506, mean_q: 0.492044, mean_eps: 0.000000
 2280/5000: episode: 95, duration: 0.369s, episode steps:  29, steps per second:  79, episode reward: 35.749, mean reward:  1.233 [-2.245, 32.040], mean action: 2.552 [0.000, 12.000],  loss: 0.021684, mae: 0.418336, mean_q: 0.494608, mean_eps: 0.000000
 2303/5000: episode: 96, duration: 0.287s, episode steps:  23, steps per second:  80, episode reward: -35.070, mean reward: -1.525 [-32.055,  3.000], mean action: 5.957 [0.000, 14.000],  loss: 0.021275, mae: 0.425229, mean_q: 0.483846, mean_eps: 0.000000
 2320/5000: episode: 97, duration: 0.218s, episode steps:  17, steps per second:  78, episode reward: 39.000, mean reward:  2.294 [-2.207, 30.634], mean action: 2.235 [0.000, 12.000],  loss: 0.024263, mae: 0.429832, mean_q: 0.479874, mean_eps: 0.000000
 2342/5000: episode: 98, duration: 0.271s, episode steps:  22, steps per second:  81, episode reward: 32.904, mean reward:  1.496 [-2.382, 32.334], mean action: 3.318 [0.000, 12.000],  loss: 0.020749, mae: 0.421220, mean_q: 0.461924, mean_eps: 0.000000
 2376/5000: episode: 99, duration: 0.416s, episode steps:  34, steps per second:  82, episode reward: 33.000, mean reward:  0.971 [-2.647, 32.110], mean action: 4.059 [0.000, 12.000],  loss: 0.020324, mae: 0.415402, mean_q: 0.470864, mean_eps: 0.000000
 2415/5000: episode: 100, duration: 0.454s, episode steps:  39, steps per second:  86, episode reward: -32.010, mean reward: -0.821 [-31.913,  2.813], mean action: 2.718 [0.000, 14.000],  loss: 0.019532, mae: 0.410252, mean_q: 0.490627, mean_eps: 0.000000
 2444/5000: episode: 101, duration: 0.353s, episode steps:  29, steps per second:  82, episode reward: 38.637, mean reward:  1.332 [-2.505, 31.966], mean action: 4.448 [0.000, 12.000],  loss: 0.017861, mae: 0.399895, mean_q: 0.464298, mean_eps: 0.000000
 2467/5000: episode: 102, duration: 0.290s, episode steps:  23, steps per second:  79, episode reward: 35.263, mean reward:  1.533 [-2.370, 31.483], mean action: 5.609 [0.000, 19.000],  loss: 0.020463, mae: 0.416805, mean_q: 0.480155, mean_eps: 0.000000
 2501/5000: episode: 103, duration: 0.406s, episode steps:  34, steps per second:  84, episode reward: 35.060, mean reward:  1.031 [-2.434, 32.204], mean action: 4.441 [0.000, 19.000],  loss: 0.019942, mae: 0.407623, mean_q: 0.450590, mean_eps: 0.000000
 2521/5000: episode: 104, duration: 0.247s, episode steps:  20, steps per second:  81, episode reward: 32.396, mean reward:  1.620 [-3.000, 32.020], mean action: 3.650 [0.000, 16.000],  loss: 0.016717, mae: 0.390841, mean_q: 0.468191, mean_eps: 0.000000
 2544/5000: episode: 105, duration: 0.289s, episode steps:  23, steps per second:  80, episode reward: 41.556, mean reward:  1.807 [-2.153, 31.937], mean action: 3.000 [1.000, 13.000],  loss: 0.018644, mae: 0.399138, mean_q: 0.465961, mean_eps: 0.000000
 2568/5000: episode: 106, duration: 0.304s, episode steps:  24, steps per second:  79, episode reward: 38.764, mean reward:  1.615 [-2.445, 33.000], mean action: 4.208 [0.000, 14.000],  loss: 0.018870, mae: 0.397599, mean_q: 0.491465, mean_eps: 0.000000
 2590/5000: episode: 107, duration: 0.275s, episode steps:  22, steps per second:  80, episode reward: 36.000, mean reward:  1.636 [-2.236, 32.240], mean action: 2.909 [0.000, 15.000],  loss: 0.017795, mae: 0.395528, mean_q: 0.473651, mean_eps: 0.000000
 2615/5000: episode: 108, duration: 0.319s, episode steps:  25, steps per second:  78, episode reward: 37.891, mean reward:  1.516 [-2.787, 32.160], mean action: 3.520 [0.000, 14.000],  loss: 0.018145, mae: 0.404382, mean_q: 0.429288, mean_eps: 0.000000
 2641/5000: episode: 109, duration: 0.324s, episode steps:  26, steps per second:  80, episode reward: -41.910, mean reward: -1.612 [-32.085,  2.161], mean action: 9.000 [0.000, 16.000],  loss: 0.019884, mae: 0.408851, mean_q: 0.422819, mean_eps: 0.000000
 2662/5000: episode: 110, duration: 0.271s, episode steps:  21, steps per second:  77, episode reward: 43.743, mean reward:  2.083 [-2.261, 32.100], mean action: 2.952 [0.000, 18.000],  loss: 0.017874, mae: 0.401062, mean_q: 0.472627, mean_eps: 0.000000
 2683/5000: episode: 111, duration: 0.271s, episode steps:  21, steps per second:  77, episode reward: 35.596, mean reward:  1.695 [-3.000, 32.280], mean action: 6.524 [0.000, 19.000],  loss: 0.015857, mae: 0.397808, mean_q: 0.451746, mean_eps: 0.000000
 2702/5000: episode: 112, duration: 0.247s, episode steps:  19, steps per second:  77, episode reward: 41.860, mean reward:  2.203 [-2.246, 32.410], mean action: 2.579 [0.000, 11.000],  loss: 0.023758, mae: 0.425291, mean_q: 0.491948, mean_eps: 0.000000
 2729/5000: episode: 113, duration: 0.332s, episode steps:  27, steps per second:  81, episode reward: 32.295, mean reward:  1.196 [-2.288, 32.170], mean action: 2.704 [0.000, 11.000],  loss: 0.019755, mae: 0.405722, mean_q: 0.482261, mean_eps: 0.000000
 2751/5000: episode: 114, duration: 0.274s, episode steps:  22, steps per second:  80, episode reward: 35.086, mean reward:  1.595 [-3.000, 32.811], mean action: 5.818 [0.000, 15.000],  loss: 0.022146, mae: 0.412521, mean_q: 0.471842, mean_eps: 0.000000
 2770/5000: episode: 115, duration: 0.236s, episode steps:  19, steps per second:  80, episode reward: -33.000, mean reward: -1.737 [-32.286,  2.903], mean action: 7.684 [0.000, 15.000],  loss: 0.018743, mae: 0.395310, mean_q: 0.501151, mean_eps: 0.000000
 2790/5000: episode: 116, duration: 0.249s, episode steps:  20, steps per second:  80, episode reward: -36.000, mean reward: -1.800 [-30.437,  2.099], mean action: 7.400 [0.000, 20.000],  loss: 0.020283, mae: 0.405603, mean_q: 0.480897, mean_eps: 0.000000
 2812/5000: episode: 117, duration: 0.267s, episode steps:  22, steps per second:  82, episode reward: 36.000, mean reward:  1.636 [-2.427, 32.610], mean action: 4.500 [0.000, 16.000],  loss: 0.020931, mae: 0.405567, mean_q: 0.434551, mean_eps: 0.000000
 2835/5000: episode: 118, duration: 0.281s, episode steps:  23, steps per second:  82, episode reward: -35.130, mean reward: -1.527 [-32.430,  2.764], mean action: 4.000 [0.000, 16.000],  loss: 0.019456, mae: 0.402725, mean_q: 0.454013, mean_eps: 0.000000
 2857/5000: episode: 119, duration: 0.262s, episode steps:  22, steps per second:  84, episode reward: -41.120, mean reward: -1.869 [-32.207,  2.570], mean action: 6.136 [0.000, 16.000],  loss: 0.020479, mae: 0.416763, mean_q: 0.418176, mean_eps: 0.000000
 2875/5000: episode: 120, duration: 0.234s, episode steps:  18, steps per second:  77, episode reward: 39.000, mean reward:  2.167 [-2.885, 32.380], mean action: 3.167 [0.000, 16.000],  loss: 0.018935, mae: 0.411330, mean_q: 0.435191, mean_eps: 0.000000
 2902/5000: episode: 121, duration: 0.327s, episode steps:  27, steps per second:  83, episode reward: 32.634, mean reward:  1.209 [-2.677, 32.070], mean action: 8.963 [0.000, 20.000],  loss: 0.017235, mae: 0.400676, mean_q: 0.403493, mean_eps: 0.000000
 2932/5000: episode: 122, duration: 0.365s, episode steps:  30, steps per second:  82, episode reward: 34.798, mean reward:  1.160 [-2.859, 32.040], mean action: 6.367 [0.000, 19.000],  loss: 0.024000, mae: 0.440629, mean_q: 0.413819, mean_eps: 0.000000
 2942/5000: episode: 123, duration: 0.139s, episode steps:  10, steps per second:  72, episode reward: 44.052, mean reward:  4.405 [-3.000, 33.000], mean action: 4.400 [0.000, 16.000],  loss: 0.017360, mae: 0.404124, mean_q: 0.462454, mean_eps: 0.000000
 2978/5000: episode: 124, duration: 0.436s, episode steps:  36, steps per second:  82, episode reward: 41.440, mean reward:  1.151 [-2.214, 32.163], mean action: 3.389 [0.000, 16.000],  loss: 0.020522, mae: 0.417596, mean_q: 0.439799, mean_eps: 0.000000
 3003/5000: episode: 125, duration: 0.306s, episode steps:  25, steps per second:  82, episode reward: 30.000, mean reward:  1.200 [-3.000, 30.537], mean action: 3.160 [0.000, 16.000],  loss: 0.017948, mae: 0.398917, mean_q: 0.434864, mean_eps: 0.000000
 3036/5000: episode: 126, duration: 0.408s, episode steps:  33, steps per second:  81, episode reward: 32.525, mean reward:  0.986 [-2.751, 32.211], mean action: 6.667 [0.000, 18.000],  loss: 0.018342, mae: 0.404207, mean_q: 0.474199, mean_eps: 0.000000
 3063/5000: episode: 127, duration: 0.328s, episode steps:  27, steps per second:  82, episode reward: 33.000, mean reward:  1.222 [-2.762, 32.475], mean action: 4.074 [0.000, 12.000],  loss: 0.017015, mae: 0.390434, mean_q: 0.492774, mean_eps: 0.000000
 3097/5000: episode: 128, duration: 0.417s, episode steps:  34, steps per second:  82, episode reward: -32.950, mean reward: -0.969 [-32.084,  2.649], mean action: 6.206 [0.000, 21.000],  loss: 0.019573, mae: 0.408199, mean_q: 0.434553, mean_eps: 0.000000
 3122/5000: episode: 129, duration: 0.332s, episode steps:  25, steps per second:  75, episode reward: 34.643, mean reward:  1.386 [-2.624, 32.514], mean action: 5.680 [0.000, 19.000],  loss: 0.017161, mae: 0.400342, mean_q: 0.428446, mean_eps: 0.000000
 3161/5000: episode: 130, duration: 0.451s, episode steps:  39, steps per second:  87, episode reward: -44.290, mean reward: -1.136 [-31.764,  2.731], mean action: 5.436 [0.000, 15.000],  loss: 0.018801, mae: 0.412961, mean_q: 0.466154, mean_eps: 0.000000
 3186/5000: episode: 131, duration: 0.311s, episode steps:  25, steps per second:  80, episode reward: 38.418, mean reward:  1.537 [-2.621, 32.046], mean action: 3.680 [0.000, 11.000],  loss: 0.017211, mae: 0.411289, mean_q: 0.455907, mean_eps: 0.000000
 3213/5000: episode: 132, duration: 0.330s, episode steps:  27, steps per second:  82, episode reward: 32.550, mean reward:  1.206 [-2.584, 32.079], mean action: 5.815 [0.000, 20.000],  loss: 0.018895, mae: 0.417215, mean_q: 0.447436, mean_eps: 0.000000
 3231/5000: episode: 133, duration: 0.227s, episode steps:  18, steps per second:  79, episode reward: 39.000, mean reward:  2.167 [-2.364, 32.080], mean action: 3.278 [0.000, 19.000],  loss: 0.018207, mae: 0.412610, mean_q: 0.436386, mean_eps: 0.000000
 3253/5000: episode: 134, duration: 0.269s, episode steps:  22, steps per second:  82, episode reward: 38.401, mean reward:  1.745 [-2.397, 32.311], mean action: 4.773 [0.000, 14.000],  loss: 0.020363, mae: 0.422352, mean_q: 0.449284, mean_eps: 0.000000
 3275/5000: episode: 135, duration: 0.312s, episode steps:  22, steps per second:  70, episode reward: 38.921, mean reward:  1.769 [-2.633, 32.140], mean action: 3.091 [0.000, 11.000],  loss: 0.020521, mae: 0.428511, mean_q: 0.435888, mean_eps: 0.000000
 3302/5000: episode: 136, duration: 0.333s, episode steps:  27, steps per second:  81, episode reward: 32.466, mean reward:  1.202 [-2.489, 32.206], mean action: 6.148 [0.000, 20.000],  loss: 0.017639, mae: 0.411119, mean_q: 0.452789, mean_eps: 0.000000
 3319/5000: episode: 137, duration: 0.303s, episode steps:  17, steps per second:  56, episode reward: 41.805, mean reward:  2.459 [-2.754, 32.320], mean action: 2.882 [0.000, 19.000],  loss: 0.017620, mae: 0.407634, mean_q: 0.426977, mean_eps: 0.000000
 3337/5000: episode: 138, duration: 0.234s, episode steps:  18, steps per second:  77, episode reward: 37.321, mean reward:  2.073 [-2.609, 32.903], mean action: 6.000 [0.000, 20.000],  loss: 0.018319, mae: 0.414024, mean_q: 0.405411, mean_eps: 0.000000
 3359/5000: episode: 139, duration: 0.276s, episode steps:  22, steps per second:  80, episode reward: -35.430, mean reward: -1.610 [-32.103,  2.620], mean action: 3.864 [0.000, 16.000],  loss: 0.017403, mae: 0.407540, mean_q: 0.448839, mean_eps: 0.000000
 3380/5000: episode: 140, duration: 0.269s, episode steps:  21, steps per second:  78, episode reward: 41.025, mean reward:  1.954 [-2.190, 32.380], mean action: 2.190 [0.000, 12.000],  loss: 0.017250, mae: 0.397835, mean_q: 0.511201, mean_eps: 0.000000
 3403/5000: episode: 141, duration: 0.313s, episode steps:  23, steps per second:  73, episode reward: 38.831, mean reward:  1.688 [-2.163, 32.217], mean action: 4.087 [0.000, 15.000],  loss: 0.017261, mae: 0.405749, mean_q: 0.474867, mean_eps: 0.000000
 3426/5000: episode: 142, duration: 0.292s, episode steps:  23, steps per second:  79, episode reward: 41.290, mean reward:  1.795 [-2.398, 32.122], mean action: 3.783 [0.000, 17.000],  loss: 0.026769, mae: 0.445894, mean_q: 0.446110, mean_eps: 0.000000
 3439/5000: episode: 143, duration: 0.177s, episode steps:  13, steps per second:  73, episode reward: 44.493, mean reward:  3.423 [-2.080, 32.260], mean action: 4.154 [1.000, 14.000],  loss: 0.024883, mae: 0.443661, mean_q: 0.436663, mean_eps: 0.000000
 3465/5000: episode: 144, duration: 0.318s, episode steps:  26, steps per second:  82, episode reward: 32.021, mean reward:  1.232 [-2.483, 32.020], mean action: 5.808 [0.000, 14.000],  loss: 0.022761, mae: 0.426601, mean_q: 0.456796, mean_eps: 0.000000
 3484/5000: episode: 145, duration: 0.239s, episode steps:  19, steps per second:  80, episode reward: 36.000, mean reward:  1.895 [-3.000, 32.620], mean action: 4.368 [0.000, 12.000],  loss: 0.022247, mae: 0.424846, mean_q: 0.439487, mean_eps: 0.000000
 3514/5000: episode: 146, duration: 0.367s, episode steps:  30, steps per second:  82, episode reward: -35.580, mean reward: -1.186 [-32.269,  2.310], mean action: 7.133 [0.000, 14.000],  loss: 0.016749, mae: 0.404871, mean_q: 0.456001, mean_eps: 0.000000
 3540/5000: episode: 147, duration: 0.370s, episode steps:  26, steps per second:  70, episode reward: 37.879, mean reward:  1.457 [-2.325, 32.400], mean action: 2.038 [0.000, 12.000],  loss: 0.020659, mae: 0.430691, mean_q: 0.419916, mean_eps: 0.000000
 3558/5000: episode: 148, duration: 0.376s, episode steps:  18, steps per second:  48, episode reward: -41.830, mean reward: -2.324 [-32.900,  2.481], mean action: 6.333 [0.000, 14.000],  loss: 0.017022, mae: 0.415691, mean_q: 0.455318, mean_eps: 0.000000
 3604/5000: episode: 149, duration: 0.560s, episode steps:  46, steps per second:  82, episode reward: 35.256, mean reward:  0.766 [-2.386, 32.270], mean action: 7.543 [0.000, 21.000],  loss: 0.017605, mae: 0.414131, mean_q: 0.450179, mean_eps: 0.000000
 3623/5000: episode: 150, duration: 0.240s, episode steps:  19, steps per second:  79, episode reward: 39.000, mean reward:  2.053 [-2.296, 32.270], mean action: 3.158 [0.000, 16.000],  loss: 0.025410, mae: 0.441765, mean_q: 0.450425, mean_eps: 0.000000
 3643/5000: episode: 151, duration: 0.273s, episode steps:  20, steps per second:  73, episode reward: 38.405, mean reward:  1.920 [-2.352, 31.938], mean action: 5.400 [0.000, 16.000],  loss: 0.017705, mae: 0.403770, mean_q: 0.418021, mean_eps: 0.000000
 3674/5000: episode: 152, duration: 0.380s, episode steps:  31, steps per second:  82, episode reward: -32.650, mean reward: -1.053 [-32.200,  2.410], mean action: 7.484 [0.000, 16.000],  loss: 0.022294, mae: 0.430432, mean_q: 0.366418, mean_eps: 0.000000
 3713/5000: episode: 153, duration: 0.474s, episode steps:  39, steps per second:  82, episode reward: 33.000, mean reward:  0.846 [-2.542, 32.430], mean action: 11.487 [0.000, 19.000],  loss: 0.021060, mae: 0.415235, mean_q: 0.443902, mean_eps: 0.000000
 3735/5000: episode: 154, duration: 0.272s, episode steps:  22, steps per second:  81, episode reward: -32.200, mean reward: -1.464 [-31.546,  3.000], mean action: 4.682 [0.000, 16.000],  loss: 0.024340, mae: 0.426554, mean_q: 0.420752, mean_eps: 0.000000
 3762/5000: episode: 155, duration: 0.337s, episode steps:  27, steps per second:  80, episode reward: 37.722, mean reward:  1.397 [-3.000, 31.900], mean action: 3.037 [0.000, 12.000],  loss: 0.018812, mae: 0.393896, mean_q: 0.483498, mean_eps: 0.000000
 3782/5000: episode: 156, duration: 0.252s, episode steps:  20, steps per second:  79, episode reward: 32.773, mean reward:  1.639 [-3.000, 30.513], mean action: 4.350 [0.000, 14.000],  loss: 0.020923, mae: 0.404933, mean_q: 0.504904, mean_eps: 0.000000
 3822/5000: episode: 157, duration: 0.485s, episode steps:  40, steps per second:  82, episode reward: -33.000, mean reward: -0.825 [-32.067,  2.803], mean action: 3.700 [0.000, 17.000],  loss: 0.018375, mae: 0.401367, mean_q: 0.506147, mean_eps: 0.000000
 3846/5000: episode: 158, duration: 0.296s, episode steps:  24, steps per second:  81, episode reward: 35.034, mean reward:  1.460 [-2.686, 32.283], mean action: 4.792 [0.000, 14.000],  loss: 0.017787, mae: 0.401535, mean_q: 0.486476, mean_eps: 0.000000
 3867/5000: episode: 159, duration: 0.264s, episode steps:  21, steps per second:  79, episode reward: 35.904, mean reward:  1.710 [-2.586, 32.104], mean action: 3.905 [1.000, 12.000],  loss: 0.015973, mae: 0.384007, mean_q: 0.488342, mean_eps: 0.000000
 3882/5000: episode: 160, duration: 0.198s, episode steps:  15, steps per second:  76, episode reward: 41.108, mean reward:  2.741 [-2.901, 32.600], mean action: 4.600 [0.000, 19.000],  loss: 0.020666, mae: 0.413833, mean_q: 0.468800, mean_eps: 0.000000
 3907/5000: episode: 161, duration: 0.307s, episode steps:  25, steps per second:  81, episode reward: -32.910, mean reward: -1.316 [-32.616,  2.952], mean action: 5.960 [0.000, 19.000],  loss: 0.020868, mae: 0.409843, mean_q: 0.430273, mean_eps: 0.000000
 3925/5000: episode: 162, duration: 0.213s, episode steps:  18, steps per second:  84, episode reward: -44.660, mean reward: -2.481 [-33.000,  1.923], mean action: 8.333 [0.000, 19.000],  loss: 0.019369, mae: 0.415831, mean_q: 0.432054, mean_eps: 0.000000
 3964/5000: episode: 163, duration: 0.471s, episode steps:  39, steps per second:  83, episode reward: -32.400, mean reward: -0.831 [-32.065,  3.061], mean action: 5.410 [0.000, 21.000],  loss: 0.021916, mae: 0.420756, mean_q: 0.488453, mean_eps: 0.000000
 3996/5000: episode: 164, duration: 0.391s, episode steps:  32, steps per second:  82, episode reward: 33.000, mean reward:  1.031 [-2.858, 32.710], mean action: 6.062 [0.000, 21.000],  loss: 0.018740, mae: 0.410100, mean_q: 0.463955, mean_eps: 0.000000
 4013/5000: episode: 165, duration: 0.218s, episode steps:  17, steps per second:  78, episode reward: 44.483, mean reward:  2.617 [-2.255, 32.230], mean action: 1.706 [0.000, 12.000],  loss: 0.022047, mae: 0.419479, mean_q: 0.464388, mean_eps: 0.000000
 4030/5000: episode: 166, duration: 0.217s, episode steps:  17, steps per second:  78, episode reward: 44.410, mean reward:  2.612 [-2.276, 31.962], mean action: 5.000 [0.000, 21.000],  loss: 0.021947, mae: 0.418326, mean_q: 0.460578, mean_eps: 0.000000
 4051/5000: episode: 167, duration: 0.262s, episode steps:  21, steps per second:  80, episode reward: 38.077, mean reward:  1.813 [-2.707, 33.000], mean action: 5.571 [0.000, 14.000],  loss: 0.021214, mae: 0.420734, mean_q: 0.505332, mean_eps: 0.000000
 4071/5000: episode: 168, duration: 0.249s, episode steps:  20, steps per second:  80, episode reward: 37.702, mean reward:  1.885 [-2.342, 32.170], mean action: 3.400 [0.000, 12.000],  loss: 0.024129, mae: 0.427779, mean_q: 0.480817, mean_eps: 0.000000
 4089/5000: episode: 169, duration: 0.221s, episode steps:  18, steps per second:  81, episode reward: -36.000, mean reward: -2.000 [-32.384,  2.330], mean action: 4.500 [0.000, 12.000],  loss: 0.022318, mae: 0.425644, mean_q: 0.543140, mean_eps: 0.000000
 4111/5000: episode: 170, duration: 0.279s, episode steps:  22, steps per second:  79, episode reward: 39.000, mean reward:  1.773 [-2.255, 32.730], mean action: 8.091 [0.000, 21.000],  loss: 0.020224, mae: 0.411856, mean_q: 0.489356, mean_eps: 0.000000
 4131/5000: episode: 171, duration: 0.255s, episode steps:  20, steps per second:  78, episode reward: 35.684, mean reward:  1.784 [-3.000, 32.684], mean action: 3.700 [0.000, 12.000],  loss: 0.020617, mae: 0.415167, mean_q: 0.477482, mean_eps: 0.000000
 4224/5000: episode: 172, duration: 1.072s, episode steps:  93, steps per second:  87, episode reward: 38.522, mean reward:  0.414 [-2.653, 31.964], mean action: 3.011 [0.000, 20.000],  loss: 0.019733, mae: 0.417362, mean_q: 0.439048, mean_eps: 0.000000
 4250/5000: episode: 173, duration: 0.323s, episode steps:  26, steps per second:  81, episode reward: -32.890, mean reward: -1.265 [-32.136,  2.400], mean action: 6.615 [0.000, 15.000],  loss: 0.020563, mae: 0.409340, mean_q: 0.452360, mean_eps: 0.000000
 4263/5000: episode: 174, duration: 0.174s, episode steps:  13, steps per second:  75, episode reward: 44.028, mean reward:  3.387 [-2.455, 32.140], mean action: 3.615 [0.000, 14.000],  loss: 0.020586, mae: 0.412740, mean_q: 0.521707, mean_eps: 0.000000
 4284/5000: episode: 175, duration: 0.264s, episode steps:  21, steps per second:  80, episode reward: 35.103, mean reward:  1.672 [-3.000, 32.260], mean action: 1.952 [0.000, 9.000],  loss: 0.016312, mae: 0.387052, mean_q: 0.501243, mean_eps: 0.000000
 4345/5000: episode: 176, duration: 0.724s, episode steps:  61, steps per second:  84, episode reward: 32.772, mean reward:  0.537 [-3.000, 32.470], mean action: 3.148 [0.000, 16.000],  loss: 0.019893, mae: 0.410667, mean_q: 0.489260, mean_eps: 0.000000
 4369/5000: episode: 177, duration: 0.293s, episode steps:  24, steps per second:  82, episode reward: 32.588, mean reward:  1.358 [-3.000, 32.138], mean action: 4.667 [0.000, 16.000],  loss: 0.021686, mae: 0.417517, mean_q: 0.569692, mean_eps: 0.000000
 4389/5000: episode: 178, duration: 0.260s, episode steps:  20, steps per second:  77, episode reward: 37.650, mean reward:  1.882 [-2.394, 31.434], mean action: 8.350 [0.000, 21.000],  loss: 0.020400, mae: 0.416951, mean_q: 0.477184, mean_eps: 0.000000
 4422/5000: episode: 179, duration: 0.434s, episode steps:  33, steps per second:  76, episode reward: 32.156, mean reward:  0.974 [-2.319, 32.530], mean action: 9.909 [0.000, 18.000],  loss: 0.019547, mae: 0.410501, mean_q: 0.469819, mean_eps: 0.000000
 4445/5000: episode: 180, duration: 0.293s, episode steps:  23, steps per second:  78, episode reward: 35.439, mean reward:  1.541 [-3.000, 32.112], mean action: 4.087 [0.000, 14.000],  loss: 0.018715, mae: 0.398939, mean_q: 0.482903, mean_eps: 0.000000
 4475/5000: episode: 181, duration: 0.371s, episode steps:  30, steps per second:  81, episode reward: -35.800, mean reward: -1.193 [-31.966,  2.160], mean action: 8.633 [0.000, 17.000],  loss: 0.020821, mae: 0.416595, mean_q: 0.442662, mean_eps: 0.000000
 4496/5000: episode: 182, duration: 0.262s, episode steps:  21, steps per second:  80, episode reward: 32.566, mean reward:  1.551 [-2.716, 32.566], mean action: 6.000 [1.000, 15.000],  loss: 0.018093, mae: 0.399139, mean_q: 0.446578, mean_eps: 0.000000
 4527/5000: episode: 183, duration: 0.377s, episode steps:  31, steps per second:  82, episode reward: 32.601, mean reward:  1.052 [-2.502, 32.088], mean action: 8.290 [0.000, 16.000],  loss: 0.020897, mae: 0.416175, mean_q: 0.448695, mean_eps: 0.000000
 4550/5000: episode: 184, duration: 0.283s, episode steps:  23, steps per second:  81, episode reward: -36.000, mean reward: -1.565 [-32.046,  3.000], mean action: 5.261 [0.000, 16.000],  loss: 0.019202, mae: 0.406299, mean_q: 0.466559, mean_eps: 0.000000
 4576/5000: episode: 185, duration: 0.320s, episode steps:  26, steps per second:  81, episode reward: 32.085, mean reward:  1.234 [-3.000, 31.805], mean action: 3.885 [0.000, 12.000],  loss: 0.021876, mae: 0.423041, mean_q: 0.446316, mean_eps: 0.000000
 4601/5000: episode: 186, duration: 0.308s, episode steps:  25, steps per second:  81, episode reward: 35.118, mean reward:  1.405 [-2.571, 31.723], mean action: 4.480 [0.000, 14.000],  loss: 0.019901, mae: 0.408343, mean_q: 0.436535, mean_eps: 0.000000
 4621/5000: episode: 187, duration: 0.251s, episode steps:  20, steps per second:  80, episode reward: 41.876, mean reward:  2.094 [-2.718, 33.000], mean action: 2.950 [0.000, 14.000],  loss: 0.022452, mae: 0.418414, mean_q: 0.457540, mean_eps: 0.000000
 4644/5000: episode: 188, duration: 0.275s, episode steps:  23, steps per second:  84, episode reward: -38.350, mean reward: -1.667 [-33.000,  2.220], mean action: 7.000 [0.000, 14.000],  loss: 0.019071, mae: 0.398977, mean_q: 0.410823, mean_eps: 0.000000
 4660/5000: episode: 189, duration: 0.218s, episode steps:  16, steps per second:  73, episode reward: 38.149, mean reward:  2.384 [-2.335, 32.290], mean action: 4.125 [0.000, 12.000],  loss: 0.021889, mae: 0.417208, mean_q: 0.451058, mean_eps: 0.000000
 4686/5000: episode: 190, duration: 0.314s, episode steps:  26, steps per second:  83, episode reward: -35.350, mean reward: -1.360 [-32.118,  2.320], mean action: 3.885 [0.000, 13.000],  loss: 0.018556, mae: 0.399935, mean_q: 0.425626, mean_eps: 0.000000
 4706/5000: episode: 191, duration: 0.247s, episode steps:  20, steps per second:  81, episode reward: 35.631, mean reward:  1.782 [-3.000, 32.180], mean action: 7.400 [0.000, 16.000],  loss: 0.019880, mae: 0.403934, mean_q: 0.423810, mean_eps: 0.000000
 4725/5000: episode: 192, duration: 0.233s, episode steps:  19, steps per second:  82, episode reward: 35.358, mean reward:  1.861 [-2.337, 32.220], mean action: 4.684 [0.000, 16.000],  loss: 0.022396, mae: 0.418323, mean_q: 0.436971, mean_eps: 0.000000
 4766/5000: episode: 193, duration: 0.494s, episode steps:  41, steps per second:  83, episode reward: 38.649, mean reward:  0.943 [-2.731, 32.110], mean action: 2.585 [0.000, 16.000],  loss: 0.019890, mae: 0.411947, mean_q: 0.424351, mean_eps: 0.000000
 4782/5000: episode: 194, duration: 0.203s, episode steps:  16, steps per second:  79, episode reward: 35.633, mean reward:  2.227 [-3.000, 32.280], mean action: 7.250 [0.000, 16.000],  loss: 0.021856, mae: 0.412937, mean_q: 0.435470, mean_eps: 0.000000
 4806/5000: episode: 195, duration: 0.310s, episode steps:  24, steps per second:  77, episode reward: 32.861, mean reward:  1.369 [-2.471, 32.330], mean action: 6.333 [0.000, 16.000],  loss: 0.019046, mae: 0.404270, mean_q: 0.450223, mean_eps: 0.000000
 4832/5000: episode: 196, duration: 0.325s, episode steps:  26, steps per second:  80, episode reward: -32.740, mean reward: -1.259 [-32.263,  2.235], mean action: 5.846 [0.000, 16.000],  loss: 0.021317, mae: 0.413348, mean_q: 0.469609, mean_eps: 0.000000
 4854/5000: episode: 197, duration: 0.277s, episode steps:  22, steps per second:  80, episode reward: 36.000, mean reward:  1.636 [-3.000, 32.170], mean action: 3.455 [0.000, 16.000],  loss: 0.016865, mae: 0.385762, mean_q: 0.480754, mean_eps: 0.000000
 4881/5000: episode: 198, duration: 0.331s, episode steps:  27, steps per second:  82, episode reward: -35.400, mean reward: -1.311 [-32.134,  2.260], mean action: 8.037 [0.000, 16.000],  loss: 0.019503, mae: 0.403252, mean_q: 0.417757, mean_eps: 0.000000
 4903/5000: episode: 199, duration: 0.282s, episode steps:  22, steps per second:  78, episode reward: 37.434, mean reward:  1.702 [-2.617, 33.000], mean action: 4.591 [0.000, 21.000],  loss: 0.016358, mae: 0.384168, mean_q: 0.435660, mean_eps: 0.000000
 4927/5000: episode: 200, duration: 0.301s, episode steps:  24, steps per second:  80, episode reward: -38.420, mean reward: -1.601 [-32.003,  2.073], mean action: 7.167 [0.000, 17.000],  loss: 0.020113, mae: 0.404824, mean_q: 0.418679, mean_eps: 0.000000
 4973/5000: episode: 201, duration: 0.544s, episode steps:  46, steps per second:  84, episode reward: -38.580, mean reward: -0.839 [-31.986,  2.700], mean action: 7.761 [1.000, 16.000],  loss: 0.019744, mae: 0.401889, mean_q: 0.459295, mean_eps: 0.000000
done, took 56.882 seconds
DQN Evaluation: 3430 victories out of 4103 episodes
Training for 5000 steps ...
   22/5000: episode: 1, duration: 0.155s, episode steps:  22, steps per second: 142, episode reward: 41.314, mean reward:  1.878 [-3.000, 32.093], mean action: 3.682 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   52/5000: episode: 2, duration: 0.193s, episode steps:  30, steps per second: 156, episode reward: 44.274, mean reward:  1.476 [-3.000, 32.154], mean action: 5.100 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   78/5000: episode: 3, duration: 0.176s, episode steps:  26, steps per second: 148, episode reward: 41.575, mean reward:  1.599 [-2.120, 32.070], mean action: 3.692 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  107/5000: episode: 4, duration: 0.188s, episode steps:  29, steps per second: 154, episode reward: 43.929, mean reward:  1.515 [-2.391, 32.174], mean action: 3.103 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  141/5000: episode: 5, duration: 0.218s, episode steps:  34, steps per second: 156, episode reward: 46.324, mean reward:  1.362 [-0.486, 31.792], mean action: 8.441 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  170/5000: episode: 6, duration: 0.191s, episode steps:  29, steps per second: 152, episode reward: 35.153, mean reward:  1.212 [-2.597, 31.283], mean action: 3.069 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  204/5000: episode: 7, duration: 0.214s, episode steps:  34, steps per second: 159, episode reward: 35.126, mean reward:  1.033 [-3.000, 31.863], mean action: 6.206 [0.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  229/5000: episode: 8, duration: 0.159s, episode steps:  25, steps per second: 157, episode reward: 39.000, mean reward:  1.560 [-2.802, 32.090], mean action: 4.400 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  261/5000: episode: 9, duration: 0.194s, episode steps:  32, steps per second: 165, episode reward: 38.126, mean reward:  1.191 [-2.938, 32.053], mean action: 4.906 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  284/5000: episode: 10, duration: 0.142s, episode steps:  23, steps per second: 162, episode reward: 44.050, mean reward:  1.915 [-2.100, 33.000], mean action: 6.261 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  308/5000: episode: 11, duration: 0.156s, episode steps:  24, steps per second: 154, episode reward: 44.656, mean reward:  1.861 [-2.126, 31.875], mean action: 2.500 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  336/5000: episode: 12, duration: 0.174s, episode steps:  28, steps per second: 161, episode reward: 38.312, mean reward:  1.368 [-3.000, 32.066], mean action: 5.679 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  370/5000: episode: 13, duration: 0.207s, episode steps:  34, steps per second: 164, episode reward: 38.500, mean reward:  1.132 [-2.411, 32.193], mean action: 3.353 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  405/5000: episode: 14, duration: 0.223s, episode steps:  35, steps per second: 157, episode reward: 40.738, mean reward:  1.164 [-2.063, 29.897], mean action: 3.771 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  433/5000: episode: 15, duration: 0.175s, episode steps:  28, steps per second: 160, episode reward: 41.382, mean reward:  1.478 [-2.597, 32.110], mean action: 5.500 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  498/5000: episode: 16, duration: 0.424s, episode steps:  65, steps per second: 153, episode reward: 37.221, mean reward:  0.573 [-2.794, 32.800], mean action: 7.215 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  534/5000: episode: 17, duration: 0.213s, episode steps:  36, steps per second: 169, episode reward: 41.292, mean reward:  1.147 [-2.463, 32.230], mean action: 2.889 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  553/5000: episode: 18, duration: 0.147s, episode steps:  19, steps per second: 129, episode reward: 44.300, mean reward:  2.332 [-2.026, 32.100], mean action: 2.316 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  576/5000: episode: 19, duration: 0.150s, episode steps:  23, steps per second: 153, episode reward: 41.643, mean reward:  1.811 [-3.000, 32.340], mean action: 4.043 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  592/5000: episode: 20, duration: 0.110s, episode steps:  16, steps per second: 146, episode reward: 42.000, mean reward:  2.625 [-2.658, 32.300], mean action: 3.375 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  624/5000: episode: 21, duration: 0.199s, episode steps:  32, steps per second: 161, episode reward: 40.603, mean reward:  1.269 [-2.901, 32.190], mean action: 7.031 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  664/5000: episode: 22, duration: 0.246s, episode steps:  40, steps per second: 163, episode reward: 42.858, mean reward:  1.071 [-2.381, 32.090], mean action: 7.225 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  733/5000: episode: 23, duration: 0.399s, episode steps:  69, steps per second: 173, episode reward: 47.739, mean reward:  0.692 [-0.729, 32.903], mean action: 1.841 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  764/5000: episode: 24, duration: 0.190s, episode steps:  31, steps per second: 163, episode reward: 41.903, mean reward:  1.352 [-2.152, 32.350], mean action: 2.871 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  792/5000: episode: 25, duration: 0.179s, episode steps:  28, steps per second: 157, episode reward: 44.901, mean reward:  1.604 [-2.196, 32.020], mean action: 3.786 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  823/5000: episode: 26, duration: 0.187s, episode steps:  31, steps per second: 166, episode reward: 35.707, mean reward:  1.152 [-3.000, 32.102], mean action: 6.226 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  841/5000: episode: 27, duration: 0.128s, episode steps:  18, steps per second: 141, episode reward: 46.431, mean reward:  2.579 [-0.490, 32.350], mean action: 1.722 [0.000, 8.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  869/5000: episode: 28, duration: 0.178s, episode steps:  28, steps per second: 157, episode reward: 41.123, mean reward:  1.469 [-2.408, 32.090], mean action: 3.714 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  893/5000: episode: 29, duration: 0.156s, episode steps:  24, steps per second: 154, episode reward: 41.060, mean reward:  1.711 [-3.000, 32.249], mean action: 3.167 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  926/5000: episode: 30, duration: 0.199s, episode steps:  33, steps per second: 165, episode reward: 38.404, mean reward:  1.164 [-2.811, 32.230], mean action: 3.727 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  949/5000: episode: 31, duration: 0.147s, episode steps:  23, steps per second: 156, episode reward: 38.361, mean reward:  1.668 [-2.598, 31.981], mean action: 3.304 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  975/5000: episode: 32, duration: 0.159s, episode steps:  26, steps per second: 163, episode reward: 39.000, mean reward:  1.500 [-3.000, 32.030], mean action: 5.231 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1004/5000: episode: 33, duration: 0.204s, episode steps:  29, steps per second: 142, episode reward: 35.841, mean reward:  1.236 [-2.762, 32.080], mean action: 6.172 [0.000, 19.000],  loss: 0.021857, mae: 0.424037, mean_q: 0.489517, mean_eps: 0.000000
 1021/5000: episode: 34, duration: 0.219s, episode steps:  17, steps per second:  78, episode reward: 38.606, mean reward:  2.271 [-3.000, 31.848], mean action: 3.588 [0.000, 12.000],  loss: 0.019337, mae: 0.396638, mean_q: 0.460541, mean_eps: 0.000000
 1055/5000: episode: 35, duration: 0.416s, episode steps:  34, steps per second:  82, episode reward: 38.378, mean reward:  1.129 [-2.252, 32.889], mean action: 2.735 [0.000, 14.000],  loss: 0.016596, mae: 0.377173, mean_q: 0.451457, mean_eps: 0.000000
 1085/5000: episode: 36, duration: 0.368s, episode steps:  30, steps per second:  82, episode reward: 46.281, mean reward:  1.543 [-0.271, 31.991], mean action: 4.767 [0.000, 14.000],  loss: 0.020096, mae: 0.395045, mean_q: 0.478661, mean_eps: 0.000000
 1127/5000: episode: 37, duration: 0.513s, episode steps:  42, steps per second:  82, episode reward: 38.429, mean reward:  0.915 [-3.000, 32.287], mean action: 4.143 [0.000, 19.000],  loss: 0.019753, mae: 0.391040, mean_q: 0.469149, mean_eps: 0.000000
 1150/5000: episode: 38, duration: 0.293s, episode steps:  23, steps per second:  78, episode reward: 38.875, mean reward:  1.690 [-2.528, 32.680], mean action: 3.348 [0.000, 15.000],  loss: 0.018972, mae: 0.391105, mean_q: 0.479968, mean_eps: 0.000000
 1174/5000: episode: 39, duration: 0.295s, episode steps:  24, steps per second:  81, episode reward: 42.000, mean reward:  1.750 [-3.000, 32.500], mean action: 1.542 [0.000, 15.000],  loss: 0.020509, mae: 0.398661, mean_q: 0.488318, mean_eps: 0.000000
 1201/5000: episode: 40, duration: 0.330s, episode steps:  27, steps per second:  82, episode reward: 42.000, mean reward:  1.556 [-3.000, 32.140], mean action: 2.963 [0.000, 15.000],  loss: 0.019812, mae: 0.388604, mean_q: 0.466883, mean_eps: 0.000000
 1227/5000: episode: 41, duration: 0.325s, episode steps:  26, steps per second:  80, episode reward: 38.528, mean reward:  1.482 [-2.902, 32.180], mean action: 4.731 [0.000, 14.000],  loss: 0.016088, mae: 0.368329, mean_q: 0.483958, mean_eps: 0.000000
 1254/5000: episode: 42, duration: 0.330s, episode steps:  27, steps per second:  82, episode reward: 38.660, mean reward:  1.432 [-2.384, 32.270], mean action: 6.222 [0.000, 16.000],  loss: 0.019696, mae: 0.398431, mean_q: 0.459253, mean_eps: 0.000000
 1307/5000: episode: 43, duration: 0.843s, episode steps:  53, steps per second:  63, episode reward: -32.470, mean reward: -0.613 [-32.736,  2.304], mean action: 7.660 [0.000, 19.000],  loss: 0.021534, mae: 0.413776, mean_q: 0.418205, mean_eps: 0.000000
 1350/5000: episode: 44, duration: 0.513s, episode steps:  43, steps per second:  84, episode reward: 39.000, mean reward:  0.907 [-2.847, 32.300], mean action: 4.326 [0.000, 19.000],  loss: 0.018478, mae: 0.396868, mean_q: 0.560651, mean_eps: 0.000000
 1378/5000: episode: 45, duration: 0.355s, episode steps:  28, steps per second:  79, episode reward: 43.107, mean reward:  1.540 [-2.077, 32.280], mean action: 4.607 [0.000, 19.000],  loss: 0.018101, mae: 0.397303, mean_q: 0.492567, mean_eps: 0.000000
 1400/5000: episode: 46, duration: 0.279s, episode steps:  22, steps per second:  79, episode reward: 38.231, mean reward:  1.738 [-2.283, 31.910], mean action: 5.364 [0.000, 19.000],  loss: 0.016416, mae: 0.383271, mean_q: 0.481343, mean_eps: 0.000000
 1434/5000: episode: 47, duration: 0.418s, episode steps:  34, steps per second:  81, episode reward: 32.852, mean reward:  0.966 [-2.649, 32.472], mean action: 3.853 [0.000, 19.000],  loss: 0.019113, mae: 0.398911, mean_q: 0.444046, mean_eps: 0.000000
 1464/5000: episode: 48, duration: 0.380s, episode steps:  30, steps per second:  79, episode reward: 38.867, mean reward:  1.296 [-2.603, 32.190], mean action: 3.700 [0.000, 14.000],  loss: 0.019170, mae: 0.394856, mean_q: 0.473524, mean_eps: 0.000000
 1493/5000: episode: 49, duration: 0.359s, episode steps:  29, steps per second:  81, episode reward: 38.657, mean reward:  1.333 [-3.000, 32.220], mean action: 3.931 [0.000, 15.000],  loss: 0.021164, mae: 0.399401, mean_q: 0.500468, mean_eps: 0.000000
 1545/5000: episode: 50, duration: 0.625s, episode steps:  52, steps per second:  83, episode reward: 35.163, mean reward:  0.676 [-2.305, 32.180], mean action: 7.115 [0.000, 20.000],  loss: 0.020606, mae: 0.401574, mean_q: 0.478924, mean_eps: 0.000000
 1580/5000: episode: 51, duration: 0.432s, episode steps:  35, steps per second:  81, episode reward: 41.581, mean reward:  1.188 [-2.293, 32.143], mean action: 3.171 [0.000, 14.000],  loss: 0.021992, mae: 0.414177, mean_q: 0.454814, mean_eps: 0.000000
 1600/5000: episode: 52, duration: 0.258s, episode steps:  20, steps per second:  77, episode reward: 38.769, mean reward:  1.938 [-3.000, 32.127], mean action: 2.650 [0.000, 12.000],  loss: 0.017809, mae: 0.390573, mean_q: 0.497629, mean_eps: 0.000000
 1623/5000: episode: 53, duration: 0.294s, episode steps:  23, steps per second:  78, episode reward: 34.042, mean reward:  1.480 [-3.000, 31.561], mean action: 4.826 [0.000, 14.000],  loss: 0.022367, mae: 0.404180, mean_q: 0.516844, mean_eps: 0.000000
 1651/5000: episode: 54, duration: 0.346s, episode steps:  28, steps per second:  81, episode reward: 38.903, mean reward:  1.389 [-2.421, 32.213], mean action: 1.893 [0.000, 12.000],  loss: 0.022424, mae: 0.411853, mean_q: 0.477973, mean_eps: 0.000000
 1679/5000: episode: 55, duration: 0.341s, episode steps:  28, steps per second:  82, episode reward: 38.668, mean reward:  1.381 [-2.148, 31.838], mean action: 3.714 [0.000, 14.000],  loss: 0.023331, mae: 0.411001, mean_q: 0.454934, mean_eps: 0.000000
 1698/5000: episode: 56, duration: 0.244s, episode steps:  19, steps per second:  78, episode reward: 41.163, mean reward:  2.166 [-2.533, 32.160], mean action: 3.105 [0.000, 13.000],  loss: 0.019481, mae: 0.397674, mean_q: 0.438291, mean_eps: 0.000000
 1732/5000: episode: 57, duration: 0.420s, episode steps:  34, steps per second:  81, episode reward: 39.000, mean reward:  1.147 [-2.458, 32.280], mean action: 3.029 [0.000, 20.000],  loss: 0.018466, mae: 0.393962, mean_q: 0.450757, mean_eps: 0.000000
 1762/5000: episode: 58, duration: 0.367s, episode steps:  30, steps per second:  82, episode reward: 41.742, mean reward:  1.391 [-2.401, 32.170], mean action: 2.233 [0.000, 14.000],  loss: 0.019805, mae: 0.410327, mean_q: 0.441755, mean_eps: 0.000000
 1778/5000: episode: 59, duration: 0.208s, episode steps:  16, steps per second:  77, episode reward: 41.120, mean reward:  2.570 [-2.564, 32.306], mean action: 5.688 [1.000, 14.000],  loss: 0.022078, mae: 0.408895, mean_q: 0.483854, mean_eps: 0.000000
 1798/5000: episode: 60, duration: 0.270s, episode steps:  20, steps per second:  74, episode reward: 44.309, mean reward:  2.215 [-2.071, 33.000], mean action: 2.200 [1.000, 9.000],  loss: 0.024630, mae: 0.415588, mean_q: 0.493944, mean_eps: 0.000000
 1819/5000: episode: 61, duration: 0.268s, episode steps:  21, steps per second:  78, episode reward: 44.557, mean reward:  2.122 [-2.260, 31.697], mean action: 2.429 [0.000, 14.000],  loss: 0.020817, mae: 0.408578, mean_q: 0.515884, mean_eps: 0.000000
 1896/5000: episode: 62, duration: 0.891s, episode steps:  77, steps per second:  86, episode reward: 32.056, mean reward:  0.416 [-3.000, 32.050], mean action: 6.104 [0.000, 21.000],  loss: 0.020303, mae: 0.412242, mean_q: 0.461260, mean_eps: 0.000000
 1940/5000: episode: 63, duration: 0.533s, episode steps:  44, steps per second:  83, episode reward: 38.255, mean reward:  0.869 [-2.725, 32.283], mean action: 4.000 [0.000, 16.000],  loss: 0.020674, mae: 0.419852, mean_q: 0.452643, mean_eps: 0.000000
 1955/5000: episode: 64, duration: 0.200s, episode steps:  15, steps per second:  75, episode reward: 47.890, mean reward:  3.193 [ 0.116, 32.390], mean action: 0.000 [0.000, 0.000],  loss: 0.021489, mae: 0.404852, mean_q: 0.577299, mean_eps: 0.000000
 1981/5000: episode: 65, duration: 0.318s, episode steps:  26, steps per second:  82, episode reward: 36.000, mean reward:  1.385 [-2.624, 32.160], mean action: 9.500 [0.000, 20.000],  loss: 0.020190, mae: 0.399865, mean_q: 0.531925, mean_eps: 0.000000
 2009/5000: episode: 66, duration: 0.353s, episode steps:  28, steps per second:  79, episode reward: 35.203, mean reward:  1.257 [-2.753, 32.750], mean action: 4.143 [0.000, 19.000],  loss: 0.017423, mae: 0.387329, mean_q: 0.539045, mean_eps: 0.000000
 2023/5000: episode: 67, duration: 0.185s, episode steps:  14, steps per second:  76, episode reward: 41.569, mean reward:  2.969 [-3.000, 32.070], mean action: 4.143 [0.000, 19.000],  loss: 0.015756, mae: 0.383346, mean_q: 0.490500, mean_eps: 0.000000
 2055/5000: episode: 68, duration: 0.394s, episode steps:  32, steps per second:  81, episode reward: 40.812, mean reward:  1.275 [-2.329, 31.568], mean action: 3.312 [0.000, 19.000],  loss: 0.018182, mae: 0.395734, mean_q: 0.446046, mean_eps: 0.000000
 2075/5000: episode: 69, duration: 0.255s, episode steps:  20, steps per second:  78, episode reward: 38.687, mean reward:  1.934 [-3.000, 32.180], mean action: 3.550 [0.000, 18.000],  loss: 0.023361, mae: 0.421781, mean_q: 0.457391, mean_eps: 0.000000
 2121/5000: episode: 70, duration: 0.566s, episode steps:  46, steps per second:  81, episode reward: 43.849, mean reward:  0.953 [-2.201, 32.134], mean action: 2.087 [0.000, 19.000],  loss: 0.018794, mae: 0.386435, mean_q: 0.438792, mean_eps: 0.000000
 2147/5000: episode: 71, duration: 0.328s, episode steps:  26, steps per second:  79, episode reward: 36.000, mean reward:  1.385 [-2.546, 30.004], mean action: 2.808 [0.000, 15.000],  loss: 0.021220, mae: 0.394790, mean_q: 0.476471, mean_eps: 0.000000
 2173/5000: episode: 72, duration: 0.325s, episode steps:  26, steps per second:  80, episode reward: 38.100, mean reward:  1.465 [-2.809, 32.040], mean action: 3.846 [0.000, 15.000],  loss: 0.020312, mae: 0.393108, mean_q: 0.477867, mean_eps: 0.000000
 2198/5000: episode: 73, duration: 0.308s, episode steps:  25, steps per second:  81, episode reward: 33.000, mean reward:  1.320 [-3.000, 32.230], mean action: 5.640 [0.000, 15.000],  loss: 0.020075, mae: 0.393798, mean_q: 0.417042, mean_eps: 0.000000
 2217/5000: episode: 74, duration: 0.237s, episode steps:  19, steps per second:  80, episode reward: 41.381, mean reward:  2.178 [-2.639, 31.824], mean action: 2.579 [0.000, 12.000],  loss: 0.015651, mae: 0.371384, mean_q: 0.455768, mean_eps: 0.000000
 2252/5000: episode: 75, duration: 0.434s, episode steps:  35, steps per second:  81, episode reward: 43.579, mean reward:  1.245 [-2.038, 32.311], mean action: 3.086 [0.000, 14.000],  loss: 0.019102, mae: 0.380574, mean_q: 0.495431, mean_eps: 0.000000
 2273/5000: episode: 76, duration: 0.262s, episode steps:  21, steps per second:  80, episode reward: 39.000, mean reward:  1.857 [-3.000, 32.330], mean action: 4.000 [0.000, 14.000],  loss: 0.021818, mae: 0.392604, mean_q: 0.535508, mean_eps: 0.000000
 2308/5000: episode: 77, duration: 0.430s, episode steps:  35, steps per second:  81, episode reward: 46.175, mean reward:  1.319 [-0.319, 32.163], mean action: 1.171 [0.000, 13.000],  loss: 0.017558, mae: 0.379738, mean_q: 0.467933, mean_eps: 0.000000
 2330/5000: episode: 78, duration: 0.277s, episode steps:  22, steps per second:  79, episode reward: 39.000, mean reward:  1.773 [-2.369, 32.360], mean action: 5.364 [1.000, 20.000],  loss: 0.018421, mae: 0.379174, mean_q: 0.506449, mean_eps: 0.000000
 2349/5000: episode: 79, duration: 0.239s, episode steps:  19, steps per second:  79, episode reward: 41.896, mean reward:  2.205 [-3.000, 32.020], mean action: 3.737 [1.000, 19.000],  loss: 0.019267, mae: 0.388551, mean_q: 0.476016, mean_eps: 0.000000
 2369/5000: episode: 80, duration: 0.251s, episode steps:  20, steps per second:  80, episode reward: 39.000, mean reward:  1.950 [-2.438, 32.300], mean action: 3.600 [0.000, 19.000],  loss: 0.024092, mae: 0.409558, mean_q: 0.549180, mean_eps: 0.000000
 2392/5000: episode: 81, duration: 0.286s, episode steps:  23, steps per second:  81, episode reward: 44.190, mean reward:  1.921 [-2.247, 32.070], mean action: 1.696 [0.000, 12.000],  loss: 0.020859, mae: 0.391168, mean_q: 0.558730, mean_eps: 0.000000
 2449/5000: episode: 82, duration: 0.684s, episode steps:  57, steps per second:  83, episode reward: -33.000, mean reward: -0.579 [-32.023,  3.000], mean action: 6.649 [0.000, 18.000],  loss: 0.019226, mae: 0.384719, mean_q: 0.500518, mean_eps: 0.000000
 2476/5000: episode: 83, duration: 0.333s, episode steps:  27, steps per second:  81, episode reward: 38.670, mean reward:  1.432 [-2.435, 32.220], mean action: 5.111 [0.000, 19.000],  loss: 0.020147, mae: 0.387954, mean_q: 0.467114, mean_eps: 0.000000
 2508/5000: episode: 84, duration: 0.401s, episode steps:  32, steps per second:  80, episode reward: 39.000, mean reward:  1.219 [-2.239, 29.306], mean action: 3.438 [0.000, 20.000],  loss: 0.017409, mae: 0.377777, mean_q: 0.487338, mean_eps: 0.000000
 2538/5000: episode: 85, duration: 0.364s, episode steps:  30, steps per second:  82, episode reward: 32.626, mean reward:  1.088 [-3.000, 32.180], mean action: 2.867 [0.000, 14.000],  loss: 0.020480, mae: 0.399594, mean_q: 0.437111, mean_eps: 0.000000
 2561/5000: episode: 86, duration: 0.292s, episode steps:  23, steps per second:  79, episode reward: 41.749, mean reward:  1.815 [-2.431, 32.850], mean action: 2.391 [0.000, 9.000],  loss: 0.018106, mae: 0.389534, mean_q: 0.449369, mean_eps: 0.000000
 2582/5000: episode: 87, duration: 0.271s, episode steps:  21, steps per second:  78, episode reward: 47.131, mean reward:  2.244 [-0.118, 31.971], mean action: 0.714 [0.000, 3.000],  loss: 0.016650, mae: 0.369986, mean_q: 0.491287, mean_eps: 0.000000
 2614/5000: episode: 88, duration: 0.391s, episode steps:  32, steps per second:  82, episode reward: 43.470, mean reward:  1.358 [-2.664, 32.150], mean action: 4.281 [0.000, 15.000],  loss: 0.020902, mae: 0.393026, mean_q: 0.410668, mean_eps: 0.000000
 2651/5000: episode: 89, duration: 0.440s, episode steps:  37, steps per second:  84, episode reward: -32.210, mean reward: -0.871 [-32.315,  2.680], mean action: 4.027 [0.000, 15.000],  loss: 0.018883, mae: 0.384073, mean_q: 0.462467, mean_eps: 0.000000
 2671/5000: episode: 90, duration: 0.261s, episode steps:  20, steps per second:  77, episode reward: 46.351, mean reward:  2.318 [-0.498, 32.417], mean action: 3.300 [0.000, 9.000],  loss: 0.018517, mae: 0.382185, mean_q: 0.492784, mean_eps: 0.000000
 2707/5000: episode: 91, duration: 0.443s, episode steps:  36, steps per second:  81, episode reward: 44.011, mean reward:  1.223 [-2.237, 32.194], mean action: 2.500 [0.000, 14.000],  loss: 0.018152, mae: 0.387670, mean_q: 0.476981, mean_eps: 0.000000
 2734/5000: episode: 92, duration: 0.334s, episode steps:  27, steps per second:  81, episode reward: 44.516, mean reward:  1.649 [-2.846, 31.951], mean action: 2.741 [0.000, 19.000],  loss: 0.021093, mae: 0.403212, mean_q: 0.440876, mean_eps: 0.000000
 2765/5000: episode: 93, duration: 0.379s, episode steps:  31, steps per second:  82, episode reward: 38.647, mean reward:  1.247 [-2.516, 31.927], mean action: 3.258 [0.000, 16.000],  loss: 0.020492, mae: 0.405299, mean_q: 0.470731, mean_eps: 0.000000
 2808/5000: episode: 94, duration: 0.517s, episode steps:  43, steps per second:  83, episode reward: 38.385, mean reward:  0.893 [-2.592, 32.380], mean action: 4.814 [0.000, 19.000],  loss: 0.021843, mae: 0.413530, mean_q: 0.449166, mean_eps: 0.000000
 2827/5000: episode: 95, duration: 0.241s, episode steps:  19, steps per second:  79, episode reward: 44.637, mean reward:  2.349 [-2.031, 32.300], mean action: 2.789 [0.000, 19.000],  loss: 0.018402, mae: 0.403166, mean_q: 0.456391, mean_eps: 0.000000
 2856/5000: episode: 96, duration: 0.360s, episode steps:  29, steps per second:  81, episode reward: 38.153, mean reward:  1.316 [-2.339, 32.901], mean action: 4.897 [0.000, 21.000],  loss: 0.020367, mae: 0.406460, mean_q: 0.432167, mean_eps: 0.000000
 2880/5000: episode: 97, duration: 0.297s, episode steps:  24, steps per second:  81, episode reward: 38.320, mean reward:  1.597 [-2.097, 32.152], mean action: 4.042 [0.000, 14.000],  loss: 0.017248, mae: 0.383272, mean_q: 0.494922, mean_eps: 0.000000
 2905/5000: episode: 98, duration: 0.319s, episode steps:  25, steps per second:  78, episode reward: 41.065, mean reward:  1.643 [-2.877, 32.080], mean action: 4.080 [0.000, 20.000],  loss: 0.023119, mae: 0.411782, mean_q: 0.445436, mean_eps: 0.000000
 2923/5000: episode: 99, duration: 0.237s, episode steps:  18, steps per second:  76, episode reward: 44.213, mean reward:  2.456 [-2.089, 31.949], mean action: 3.667 [0.000, 14.000],  loss: 0.017294, mae: 0.382842, mean_q: 0.484663, mean_eps: 0.000000
 2960/5000: episode: 100, duration: 0.449s, episode steps:  37, steps per second:  82, episode reward: 37.993, mean reward:  1.027 [-3.000, 31.951], mean action: 2.216 [0.000, 11.000],  loss: 0.023231, mae: 0.415203, mean_q: 0.458554, mean_eps: 0.000000
 2992/5000: episode: 101, duration: 0.390s, episode steps:  32, steps per second:  82, episode reward: 34.175, mean reward:  1.068 [-3.000, 31.871], mean action: 6.156 [0.000, 14.000],  loss: 0.017278, mae: 0.398172, mean_q: 0.434676, mean_eps: 0.000000
 3021/5000: episode: 102, duration: 0.357s, episode steps:  29, steps per second:  81, episode reward: 41.373, mean reward:  1.427 [-3.000, 32.330], mean action: 1.172 [0.000, 11.000],  loss: 0.019830, mae: 0.403068, mean_q: 0.480676, mean_eps: 0.000000
 3050/5000: episode: 103, duration: 0.466s, episode steps:  29, steps per second:  62, episode reward: 34.897, mean reward:  1.203 [-3.000, 31.819], mean action: 6.379 [0.000, 19.000],  loss: 0.020117, mae: 0.410467, mean_q: 0.458578, mean_eps: 0.000000
 3076/5000: episode: 104, duration: 0.453s, episode steps:  26, steps per second:  57, episode reward: -37.190, mean reward: -1.430 [-31.713,  2.712], mean action: 6.654 [0.000, 14.000],  loss: 0.020152, mae: 0.409382, mean_q: 0.434543, mean_eps: 0.000000
 3111/5000: episode: 105, duration: 0.428s, episode steps:  35, steps per second:  82, episode reward: 40.465, mean reward:  1.156 [-2.615, 32.490], mean action: 3.543 [1.000, 14.000],  loss: 0.021734, mae: 0.408762, mean_q: 0.471933, mean_eps: 0.000000
 3131/5000: episode: 106, duration: 0.254s, episode steps:  20, steps per second:  79, episode reward: 41.540, mean reward:  2.077 [-2.418, 32.190], mean action: 3.800 [0.000, 14.000],  loss: 0.024350, mae: 0.421089, mean_q: 0.488305, mean_eps: 0.000000
 3159/5000: episode: 107, duration: 0.361s, episode steps:  28, steps per second:  78, episode reward: 40.447, mean reward:  1.445 [-2.032, 32.180], mean action: 5.179 [0.000, 20.000],  loss: 0.017722, mae: 0.398125, mean_q: 0.464293, mean_eps: 0.000000
 3185/5000: episode: 108, duration: 0.325s, episode steps:  26, steps per second:  80, episode reward: 41.882, mean reward:  1.611 [-2.314, 32.200], mean action: 3.885 [0.000, 20.000],  loss: 0.025848, mae: 0.436038, mean_q: 0.449090, mean_eps: 0.000000
 3216/5000: episode: 109, duration: 0.381s, episode steps:  31, steps per second:  81, episode reward: 38.006, mean reward:  1.226 [-3.000, 32.183], mean action: 5.419 [0.000, 21.000],  loss: 0.021689, mae: 0.420808, mean_q: 0.476489, mean_eps: 0.000000
 3237/5000: episode: 110, duration: 0.262s, episode steps:  21, steps per second:  80, episode reward: 38.870, mean reward:  1.851 [-2.256, 32.210], mean action: 5.048 [0.000, 19.000],  loss: 0.021038, mae: 0.417015, mean_q: 0.450644, mean_eps: 0.000000
 3276/5000: episode: 111, duration: 0.475s, episode steps:  39, steps per second:  82, episode reward: 44.546, mean reward:  1.142 [-2.217, 31.993], mean action: 2.026 [1.000, 19.000],  loss: 0.019682, mae: 0.414237, mean_q: 0.457761, mean_eps: 0.000000
 3300/5000: episode: 112, duration: 0.301s, episode steps:  24, steps per second:  80, episode reward: 44.941, mean reward:  1.873 [-2.282, 32.280], mean action: 4.792 [0.000, 20.000],  loss: 0.021071, mae: 0.412616, mean_q: 0.417675, mean_eps: 0.000000
 3321/5000: episode: 113, duration: 0.263s, episode steps:  21, steps per second:  80, episode reward: 41.241, mean reward:  1.964 [-3.000, 32.127], mean action: 1.429 [0.000, 15.000],  loss: 0.023099, mae: 0.433531, mean_q: 0.401908, mean_eps: 0.000000
 3344/5000: episode: 114, duration: 0.294s, episode steps:  23, steps per second:  78, episode reward: 41.635, mean reward:  1.810 [-2.694, 32.610], mean action: 3.348 [0.000, 14.000],  loss: 0.020148, mae: 0.419168, mean_q: 0.451378, mean_eps: 0.000000
 3391/5000: episode: 115, duration: 0.569s, episode steps:  47, steps per second:  83, episode reward: 33.000, mean reward:  0.702 [-3.000, 32.170], mean action: 2.660 [0.000, 16.000],  loss: 0.021717, mae: 0.414709, mean_q: 0.485336, mean_eps: 0.000000
 3413/5000: episode: 116, duration: 0.274s, episode steps:  22, steps per second:  80, episode reward: 41.875, mean reward:  1.903 [-2.349, 32.560], mean action: 3.000 [0.000, 19.000],  loss: 0.015733, mae: 0.374690, mean_q: 0.499757, mean_eps: 0.000000
 3468/5000: episode: 117, duration: 0.659s, episode steps:  55, steps per second:  83, episode reward: 38.687, mean reward:  0.703 [-2.526, 32.050], mean action: 5.382 [0.000, 20.000],  loss: 0.020898, mae: 0.401304, mean_q: 0.467114, mean_eps: 0.000000
 3489/5000: episode: 118, duration: 0.262s, episode steps:  21, steps per second:  80, episode reward: 38.233, mean reward:  1.821 [-2.443, 32.250], mean action: 5.857 [0.000, 19.000],  loss: 0.015787, mae: 0.379246, mean_q: 0.436909, mean_eps: 0.000000
 3517/5000: episode: 119, duration: 0.350s, episode steps:  28, steps per second:  80, episode reward: 44.718, mean reward:  1.597 [-2.668, 32.340], mean action: 2.357 [0.000, 9.000],  loss: 0.021529, mae: 0.408676, mean_q: 0.457997, mean_eps: 0.000000
 3552/5000: episode: 120, duration: 0.430s, episode steps:  35, steps per second:  81, episode reward: 35.500, mean reward:  1.014 [-3.000, 32.110], mean action: 3.629 [0.000, 15.000],  loss: 0.022973, mae: 0.416610, mean_q: 0.510902, mean_eps: 0.000000
 3577/5000: episode: 121, duration: 0.318s, episode steps:  25, steps per second:  79, episode reward: 44.535, mean reward:  1.781 [-2.547, 32.050], mean action: 1.640 [0.000, 15.000],  loss: 0.021003, mae: 0.409012, mean_q: 0.533034, mean_eps: 0.000000
 3600/5000: episode: 122, duration: 0.291s, episode steps:  23, steps per second:  79, episode reward: 40.829, mean reward:  1.775 [-2.560, 32.164], mean action: 4.261 [0.000, 15.000],  loss: 0.023408, mae: 0.435456, mean_q: 0.477686, mean_eps: 0.000000
 3632/5000: episode: 123, duration: 0.394s, episode steps:  32, steps per second:  81, episode reward: 35.364, mean reward:  1.105 [-2.303, 31.771], mean action: 3.656 [0.000, 15.000],  loss: 0.020585, mae: 0.417846, mean_q: 0.480813, mean_eps: 0.000000
 3661/5000: episode: 124, duration: 0.455s, episode steps:  29, steps per second:  64, episode reward: 46.297, mean reward:  1.596 [-0.625, 31.671], mean action: 4.207 [0.000, 20.000],  loss: 0.019058, mae: 0.408811, mean_q: 0.439350, mean_eps: 0.000000
 3690/5000: episode: 125, duration: 0.360s, episode steps:  29, steps per second:  80, episode reward: 40.981, mean reward:  1.413 [-2.486, 32.593], mean action: 2.931 [0.000, 15.000],  loss: 0.019030, mae: 0.407163, mean_q: 0.486202, mean_eps: 0.000000
 3754/5000: episode: 126, duration: 0.800s, episode steps:  64, steps per second:  80, episode reward: 38.801, mean reward:  0.606 [-2.543, 32.310], mean action: 3.625 [0.000, 20.000],  loss: 0.021633, mae: 0.420334, mean_q: 0.466640, mean_eps: 0.000000
 3770/5000: episode: 127, duration: 0.212s, episode steps:  16, steps per second:  76, episode reward: 41.628, mean reward:  2.602 [-2.356, 32.568], mean action: 3.000 [0.000, 15.000],  loss: 0.018067, mae: 0.397726, mean_q: 0.446495, mean_eps: 0.000000
 3793/5000: episode: 128, duration: 0.293s, episode steps:  23, steps per second:  78, episode reward: 41.024, mean reward:  1.784 [-2.220, 32.190], mean action: 3.087 [0.000, 12.000],  loss: 0.019411, mae: 0.402551, mean_q: 0.431673, mean_eps: 0.000000
 3809/5000: episode: 129, duration: 0.211s, episode steps:  16, steps per second:  76, episode reward: 42.000, mean reward:  2.625 [-2.715, 32.140], mean action: 3.125 [0.000, 12.000],  loss: 0.018491, mae: 0.410339, mean_q: 0.382749, mean_eps: 0.000000
 3836/5000: episode: 130, duration: 0.335s, episode steps:  27, steps per second:  81, episode reward: 38.832, mean reward:  1.438 [-2.625, 32.152], mean action: 3.741 [0.000, 15.000],  loss: 0.020455, mae: 0.417820, mean_q: 0.405070, mean_eps: 0.000000
 3869/5000: episode: 131, duration: 0.416s, episode steps:  33, steps per second:  79, episode reward: 47.310, mean reward:  1.434 [-0.779, 32.062], mean action: 1.303 [1.000, 4.000],  loss: 0.018930, mae: 0.410207, mean_q: 0.416043, mean_eps: 0.000000
 3908/5000: episode: 132, duration: 0.472s, episode steps:  39, steps per second:  83, episode reward: -35.300, mean reward: -0.905 [-32.480,  2.720], mean action: 9.410 [1.000, 15.000],  loss: 0.024736, mae: 0.434434, mean_q: 0.458325, mean_eps: 0.000000
 3937/5000: episode: 133, duration: 0.366s, episode steps:  29, steps per second:  79, episode reward: 43.998, mean reward:  1.517 [-2.215, 31.922], mean action: 2.655 [0.000, 12.000],  loss: 0.020816, mae: 0.414088, mean_q: 0.476800, mean_eps: 0.000000
 3960/5000: episode: 134, duration: 0.296s, episode steps:  23, steps per second:  78, episode reward: 44.317, mean reward:  1.927 [-2.007, 32.670], mean action: 4.783 [0.000, 19.000],  loss: 0.016183, mae: 0.387286, mean_q: 0.502858, mean_eps: 0.000000
 3979/5000: episode: 135, duration: 0.252s, episode steps:  19, steps per second:  76, episode reward: 41.105, mean reward:  2.163 [-2.619, 32.120], mean action: 3.053 [0.000, 19.000],  loss: 0.013999, mae: 0.379262, mean_q: 0.526424, mean_eps: 0.000000
 3999/5000: episode: 136, duration: 0.384s, episode steps:  20, steps per second:  52, episode reward: 44.564, mean reward:  2.228 [-2.081, 32.453], mean action: 2.900 [0.000, 19.000],  loss: 0.022034, mae: 0.415444, mean_q: 0.497802, mean_eps: 0.000000
 4014/5000: episode: 137, duration: 0.195s, episode steps:  15, steps per second:  77, episode reward: 41.901, mean reward:  2.793 [-3.000, 32.391], mean action: 5.667 [1.000, 19.000],  loss: 0.018005, mae: 0.399068, mean_q: 0.446823, mean_eps: 0.000000
 4050/5000: episode: 138, duration: 0.443s, episode steps:  36, steps per second:  81, episode reward: 44.858, mean reward:  1.246 [-2.103, 32.190], mean action: 1.750 [0.000, 12.000],  loss: 0.019808, mae: 0.404646, mean_q: 0.497470, mean_eps: 0.000000
 4095/5000: episode: 139, duration: 0.539s, episode steps:  45, steps per second:  83, episode reward: 39.975, mean reward:  0.888 [-2.819, 32.040], mean action: 5.244 [0.000, 15.000],  loss: 0.018702, mae: 0.401928, mean_q: 0.485144, mean_eps: 0.000000
 4113/5000: episode: 140, duration: 0.229s, episode steps:  18, steps per second:  79, episode reward: 38.694, mean reward:  2.150 [-2.299, 32.274], mean action: 4.111 [0.000, 16.000],  loss: 0.020706, mae: 0.416850, mean_q: 0.487910, mean_eps: 0.000000
 4184/5000: episode: 141, duration: 0.853s, episode steps:  71, steps per second:  83, episode reward: 37.892, mean reward:  0.534 [-3.000, 31.569], mean action: 2.775 [0.000, 15.000],  loss: 0.018505, mae: 0.402052, mean_q: 0.448430, mean_eps: 0.000000
 4199/5000: episode: 142, duration: 0.202s, episode steps:  15, steps per second:  74, episode reward: 44.258, mean reward:  2.951 [-2.294, 32.500], mean action: 4.400 [0.000, 15.000],  loss: 0.020702, mae: 0.411046, mean_q: 0.437844, mean_eps: 0.000000
 4231/5000: episode: 143, duration: 0.398s, episode steps:  32, steps per second:  80, episode reward: 40.826, mean reward:  1.276 [-2.491, 32.020], mean action: 1.594 [0.000, 9.000],  loss: 0.023587, mae: 0.420160, mean_q: 0.437095, mean_eps: 0.000000
 4260/5000: episode: 144, duration: 0.362s, episode steps:  29, steps per second:  80, episode reward: 41.660, mean reward:  1.437 [-2.389, 32.120], mean action: 4.345 [0.000, 17.000],  loss: 0.019836, mae: 0.409329, mean_q: 0.406727, mean_eps: 0.000000
 4289/5000: episode: 145, duration: 0.356s, episode steps:  29, steps per second:  82, episode reward: 44.657, mean reward:  1.540 [-2.024, 32.110], mean action: 2.138 [0.000, 12.000],  loss: 0.021169, mae: 0.409266, mean_q: 0.441049, mean_eps: 0.000000
 4319/5000: episode: 146, duration: 0.377s, episode steps:  30, steps per second:  80, episode reward: 41.104, mean reward:  1.370 [-2.328, 32.070], mean action: 2.867 [0.000, 14.000],  loss: 0.019485, mae: 0.399396, mean_q: 0.518281, mean_eps: 0.000000
 4353/5000: episode: 147, duration: 0.410s, episode steps:  34, steps per second:  83, episode reward: 41.384, mean reward:  1.217 [-2.739, 32.080], mean action: 3.735 [0.000, 19.000],  loss: 0.020802, mae: 0.416258, mean_q: 0.444129, mean_eps: 0.000000
 4368/5000: episode: 148, duration: 0.212s, episode steps:  15, steps per second:  71, episode reward: 44.685, mean reward:  2.979 [-2.398, 33.000], mean action: 3.733 [0.000, 19.000],  loss: 0.026129, mae: 0.442505, mean_q: 0.508177, mean_eps: 0.000000
 4402/5000: episode: 149, duration: 0.429s, episode steps:  34, steps per second:  79, episode reward: 32.480, mean reward:  0.955 [-2.807, 32.150], mean action: 5.735 [0.000, 19.000],  loss: 0.021004, mae: 0.413387, mean_q: 0.514505, mean_eps: 0.000000
 4433/5000: episode: 150, duration: 2.067s, episode steps:  31, steps per second:  15, episode reward: 34.893, mean reward:  1.126 [-3.000, 32.080], mean action: 1.452 [0.000, 11.000],  loss: 0.025614, mae: 0.434698, mean_q: 0.480981, mean_eps: 0.000000
 4480/5000: episode: 151, duration: 0.593s, episode steps:  47, steps per second:  79, episode reward: 38.741, mean reward:  0.824 [-3.000, 32.020], mean action: 3.489 [0.000, 21.000],  loss: 0.019352, mae: 0.412697, mean_q: 0.446691, mean_eps: 0.000000
 4499/5000: episode: 152, duration: 0.271s, episode steps:  19, steps per second:  70, episode reward: 41.903, mean reward:  2.205 [-2.190, 32.113], mean action: 4.632 [0.000, 14.000],  loss: 0.020546, mae: 0.411647, mean_q: 0.435947, mean_eps: 0.000000
 4526/5000: episode: 153, duration: 0.347s, episode steps:  27, steps per second:  78, episode reward: 41.111, mean reward:  1.523 [-2.138, 32.553], mean action: 3.852 [0.000, 21.000],  loss: 0.020050, mae: 0.407655, mean_q: 0.470387, mean_eps: 0.000000
 4576/5000: episode: 154, duration: 0.664s, episode steps:  50, steps per second:  75, episode reward: 37.586, mean reward:  0.752 [-3.000, 32.140], mean action: 5.440 [0.000, 21.000],  loss: 0.020567, mae: 0.411505, mean_q: 0.467141, mean_eps: 0.000000
 4604/5000: episode: 155, duration: 0.344s, episode steps:  28, steps per second:  82, episode reward: -34.940, mean reward: -1.248 [-32.004,  2.940], mean action: 8.179 [0.000, 20.000],  loss: 0.021182, mae: 0.411157, mean_q: 0.488537, mean_eps: 0.000000
 4626/5000: episode: 156, duration: 0.281s, episode steps:  22, steps per second:  78, episode reward: 38.555, mean reward:  1.753 [-2.914, 32.045], mean action: 4.000 [0.000, 19.000],  loss: 0.024146, mae: 0.417927, mean_q: 0.488757, mean_eps: 0.000000
 4668/5000: episode: 157, duration: 0.561s, episode steps:  42, steps per second:  75, episode reward: 35.380, mean reward:  0.842 [-3.000, 32.210], mean action: 5.357 [0.000, 19.000],  loss: 0.019981, mae: 0.410089, mean_q: 0.458290, mean_eps: 0.000000
 4696/5000: episode: 158, duration: 0.349s, episode steps:  28, steps per second:  80, episode reward: 47.048, mean reward:  1.680 [-0.662, 32.030], mean action: 1.679 [0.000, 3.000],  loss: 0.019030, mae: 0.399996, mean_q: 0.474819, mean_eps: 0.000000
 4726/5000: episode: 159, duration: 0.373s, episode steps:  30, steps per second:  80, episode reward: 41.603, mean reward:  1.387 [-2.353, 32.160], mean action: 2.400 [0.000, 19.000],  loss: 0.021895, mae: 0.414453, mean_q: 0.460256, mean_eps: 0.000000
 4759/5000: episode: 160, duration: 0.402s, episode steps:  33, steps per second:  82, episode reward: 37.808, mean reward:  1.146 [-2.553, 32.421], mean action: 4.273 [0.000, 20.000],  loss: 0.016365, mae: 0.383271, mean_q: 0.471670, mean_eps: 0.000000
 4777/5000: episode: 161, duration: 0.230s, episode steps:  18, steps per second:  78, episode reward: 42.000, mean reward:  2.333 [-2.746, 32.450], mean action: 2.389 [0.000, 15.000],  loss: 0.023096, mae: 0.422067, mean_q: 0.540323, mean_eps: 0.000000
 4789/5000: episode: 162, duration: 0.162s, episode steps:  12, steps per second:  74, episode reward: 42.000, mean reward:  3.500 [-2.016, 30.290], mean action: 3.833 [0.000, 19.000],  loss: 0.022186, mae: 0.421943, mean_q: 0.446468, mean_eps: 0.000000
 4814/5000: episode: 163, duration: 0.320s, episode steps:  25, steps per second:  78, episode reward: 35.577, mean reward:  1.423 [-2.269, 31.725], mean action: 4.160 [0.000, 15.000],  loss: 0.017620, mae: 0.405969, mean_q: 0.426374, mean_eps: 0.000000
 4853/5000: episode: 164, duration: 0.485s, episode steps:  39, steps per second:  80, episode reward: 41.427, mean reward:  1.062 [-2.119, 32.023], mean action: 3.179 [0.000, 13.000],  loss: 0.020040, mae: 0.412189, mean_q: 0.464757, mean_eps: 0.000000
 4878/5000: episode: 165, duration: 0.315s, episode steps:  25, steps per second:  79, episode reward: 41.455, mean reward:  1.658 [-3.000, 32.010], mean action: 3.480 [1.000, 9.000],  loss: 0.018595, mae: 0.401166, mean_q: 0.503746, mean_eps: 0.000000
 4902/5000: episode: 166, duration: 0.302s, episode steps:  24, steps per second:  79, episode reward: 42.000, mean reward:  1.750 [-3.000, 32.440], mean action: 2.625 [0.000, 16.000],  loss: 0.017502, mae: 0.403206, mean_q: 0.465210, mean_eps: 0.000000
 4947/5000: episode: 167, duration: 0.542s, episode steps:  45, steps per second:  83, episode reward: 38.056, mean reward:  0.846 [-3.000, 31.984], mean action: 3.956 [0.000, 20.000],  loss: 0.021561, mae: 0.412784, mean_q: 0.490481, mean_eps: 0.000000
 4963/5000: episode: 168, duration: 0.208s, episode steps:  16, steps per second:  77, episode reward: 41.685, mean reward:  2.605 [-3.000, 33.000], mean action: 2.500 [0.000, 16.000],  loss: 0.019361, mae: 0.401830, mean_q: 0.485194, mean_eps: 0.000000
 4986/5000: episode: 169, duration: 0.292s, episode steps:  23, steps per second:  79, episode reward: 41.202, mean reward:  1.791 [-2.132, 32.130], mean action: 4.478 [0.000, 16.000],  loss: 0.024854, mae: 0.425283, mean_q: 0.455191, mean_eps: 0.000000
done, took 58.563 seconds
DQN Evaluation: 3594 victories out of 4273 episodes
Training for 5000 steps ...
   27/5000: episode: 1, duration: 0.231s, episode steps:  27, steps per second: 117, episode reward: -35.350, mean reward: -1.309 [-32.155,  2.471], mean action: 5.667 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   69/5000: episode: 2, duration: 0.304s, episode steps:  42, steps per second: 138, episode reward: 35.575, mean reward:  0.847 [-2.875, 32.460], mean action: 3.810 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   93/5000: episode: 3, duration: 0.166s, episode steps:  24, steps per second: 145, episode reward: 36.000, mean reward:  1.500 [-2.683, 32.130], mean action: 3.083 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  110/5000: episode: 4, duration: 0.127s, episode steps:  17, steps per second: 133, episode reward: 38.355, mean reward:  2.256 [-2.522, 32.355], mean action: 2.588 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  127/5000: episode: 5, duration: 0.114s, episode steps:  17, steps per second: 149, episode reward: 38.470, mean reward:  2.263 [-2.554, 32.400], mean action: 3.588 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  145/5000: episode: 6, duration: 0.153s, episode steps:  18, steps per second: 118, episode reward: 38.502, mean reward:  2.139 [-3.000, 33.000], mean action: 2.944 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  166/5000: episode: 7, duration: 0.258s, episode steps:  21, steps per second:  82, episode reward: 35.045, mean reward:  1.669 [-2.640, 31.600], mean action: 3.857 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  183/5000: episode: 8, duration: 0.150s, episode steps:  17, steps per second: 113, episode reward: 38.094, mean reward:  2.241 [-3.000, 33.000], mean action: 2.176 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  208/5000: episode: 9, duration: 0.179s, episode steps:  25, steps per second: 140, episode reward: 41.722, mean reward:  1.669 [-2.146, 32.029], mean action: 3.280 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  238/5000: episode: 10, duration: 0.205s, episode steps:  30, steps per second: 146, episode reward: 32.493, mean reward:  1.083 [-2.381, 32.493], mean action: 5.400 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  264/5000: episode: 11, duration: 0.176s, episode steps:  26, steps per second: 148, episode reward: 32.327, mean reward:  1.243 [-2.939, 31.758], mean action: 5.231 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  280/5000: episode: 12, duration: 0.131s, episode steps:  16, steps per second: 122, episode reward: 45.000, mean reward:  2.812 [-2.320, 32.210], mean action: 2.125 [1.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  304/5000: episode: 13, duration: 0.168s, episode steps:  24, steps per second: 143, episode reward: 38.133, mean reward:  1.589 [-2.486, 32.123], mean action: 1.958 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  338/5000: episode: 14, duration: 0.219s, episode steps:  34, steps per second: 155, episode reward: -35.640, mean reward: -1.048 [-31.883,  2.506], mean action: 6.647 [0.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  376/5000: episode: 15, duration: 0.223s, episode steps:  38, steps per second: 171, episode reward: 35.252, mean reward:  0.928 [-2.279, 32.280], mean action: 1.605 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  399/5000: episode: 16, duration: 0.147s, episode steps:  23, steps per second: 157, episode reward: 35.430, mean reward:  1.540 [-3.000, 32.460], mean action: 4.870 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  412/5000: episode: 17, duration: 0.100s, episode steps:  13, steps per second: 130, episode reward: 41.890, mean reward:  3.222 [-2.333, 32.050], mean action: 3.769 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  431/5000: episode: 18, duration: 0.131s, episode steps:  19, steps per second: 145, episode reward: 39.000, mean reward:  2.053 [-2.365, 32.020], mean action: 2.421 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  455/5000: episode: 19, duration: 0.155s, episode steps:  24, steps per second: 155, episode reward: -36.000, mean reward: -1.500 [-32.105,  3.000], mean action: 6.333 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  474/5000: episode: 20, duration: 0.136s, episode steps:  19, steps per second: 140, episode reward: 35.194, mean reward:  1.852 [-3.000, 32.781], mean action: 2.632 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  499/5000: episode: 21, duration: 0.167s, episode steps:  25, steps per second: 150, episode reward: 32.267, mean reward:  1.291 [-3.000, 32.570], mean action: 5.080 [0.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  524/5000: episode: 22, duration: 0.158s, episode steps:  25, steps per second: 158, episode reward: -35.630, mean reward: -1.425 [-32.541,  2.211], mean action: 9.000 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  548/5000: episode: 23, duration: 0.152s, episode steps:  24, steps per second: 158, episode reward: 35.326, mean reward:  1.472 [-2.330, 32.360], mean action: 6.042 [1.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  589/5000: episode: 24, duration: 0.246s, episode steps:  41, steps per second: 167, episode reward: 34.750, mean reward:  0.848 [-2.219, 32.001], mean action: 5.976 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  609/5000: episode: 25, duration: 0.136s, episode steps:  20, steps per second: 147, episode reward: 39.000, mean reward:  1.950 [-2.562, 33.000], mean action: 3.750 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  630/5000: episode: 26, duration: 0.139s, episode steps:  21, steps per second: 151, episode reward: 38.817, mean reward:  1.848 [-2.420, 32.130], mean action: 2.810 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  654/5000: episode: 27, duration: 0.159s, episode steps:  24, steps per second: 151, episode reward: 32.566, mean reward:  1.357 [-2.456, 31.901], mean action: 5.958 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  677/5000: episode: 28, duration: 0.166s, episode steps:  23, steps per second: 138, episode reward: 35.028, mean reward:  1.523 [-3.000, 32.003], mean action: 4.391 [2.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  704/5000: episode: 29, duration: 0.175s, episode steps:  27, steps per second: 154, episode reward: -35.670, mean reward: -1.321 [-32.084,  2.321], mean action: 5.852 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  748/5000: episode: 30, duration: 0.252s, episode steps:  44, steps per second: 175, episode reward: -39.000, mean reward: -0.886 [-32.018,  2.150], mean action: 10.818 [1.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  769/5000: episode: 31, duration: 0.156s, episode steps:  21, steps per second: 135, episode reward: 36.000, mean reward:  1.714 [-2.211, 32.130], mean action: 3.429 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  786/5000: episode: 32, duration: 0.118s, episode steps:  17, steps per second: 144, episode reward: 36.000, mean reward:  2.118 [-3.000, 30.889], mean action: 2.765 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  805/5000: episode: 33, duration: 0.497s, episode steps:  19, steps per second:  38, episode reward: 38.903, mean reward:  2.048 [-2.485, 32.443], mean action: 2.316 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  837/5000: episode: 34, duration: 0.754s, episode steps:  32, steps per second:  42, episode reward: -32.100, mean reward: -1.003 [-32.349,  2.491], mean action: 5.188 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  867/5000: episode: 35, duration: 0.210s, episode steps:  30, steps per second: 143, episode reward: 32.599, mean reward:  1.087 [-2.676, 32.700], mean action: 7.167 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  888/5000: episode: 36, duration: 0.145s, episode steps:  21, steps per second: 145, episode reward: 36.000, mean reward:  1.714 [-2.712, 33.000], mean action: 4.190 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  911/5000: episode: 37, duration: 0.147s, episode steps:  23, steps per second: 157, episode reward: 39.000, mean reward:  1.696 [-2.444, 32.130], mean action: 2.087 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  928/5000: episode: 38, duration: 0.123s, episode steps:  17, steps per second: 138, episode reward: 44.566, mean reward:  2.622 [-2.125, 31.736], mean action: 2.118 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  938/5000: episode: 39, duration: 0.085s, episode steps:  10, steps per second: 118, episode reward: 44.659, mean reward:  4.466 [-2.063, 33.000], mean action: 3.300 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  961/5000: episode: 40, duration: 0.157s, episode steps:  23, steps per second: 146, episode reward: 33.000, mean reward:  1.435 [-2.691, 32.260], mean action: 7.565 [1.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  985/5000: episode: 41, duration: 0.156s, episode steps:  24, steps per second: 153, episode reward: -32.760, mean reward: -1.365 [-31.850,  3.062], mean action: 4.833 [1.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1013/5000: episode: 42, duration: 0.265s, episode steps:  28, steps per second: 106, episode reward: 41.388, mean reward:  1.478 [-2.917, 33.000], mean action: 3.536 [0.000, 14.000],  loss: 0.018532, mae: 0.388184, mean_q: 0.479673, mean_eps: 0.000000
 1035/5000: episode: 43, duration: 0.382s, episode steps:  22, steps per second:  58, episode reward: -35.090, mean reward: -1.595 [-29.993,  2.160], mean action: 4.773 [0.000, 17.000],  loss: 0.020227, mae: 0.402922, mean_q: 0.478049, mean_eps: 0.000000
 1065/5000: episode: 44, duration: 0.433s, episode steps:  30, steps per second:  69, episode reward: 37.742, mean reward:  1.258 [-2.529, 31.920], mean action: 3.767 [0.000, 14.000],  loss: 0.017904, mae: 0.391685, mean_q: 0.508390, mean_eps: 0.000000
 1077/5000: episode: 45, duration: 0.181s, episode steps:  12, steps per second:  66, episode reward: 41.152, mean reward:  3.429 [-2.247, 32.062], mean action: 3.583 [1.000, 12.000],  loss: 0.022202, mae: 0.406232, mean_q: 0.499688, mean_eps: 0.000000
 1110/5000: episode: 46, duration: 0.460s, episode steps:  33, steps per second:  72, episode reward: 32.164, mean reward:  0.975 [-2.581, 31.612], mean action: 3.333 [0.000, 21.000],  loss: 0.022081, mae: 0.402997, mean_q: 0.473152, mean_eps: 0.000000
 1148/5000: episode: 47, duration: 0.527s, episode steps:  38, steps per second:  72, episode reward: 32.033, mean reward:  0.843 [-3.000, 32.120], mean action: 8.632 [0.000, 21.000],  loss: 0.019291, mae: 0.392677, mean_q: 0.527979, mean_eps: 0.000000
 1165/5000: episode: 48, duration: 0.247s, episode steps:  17, steps per second:  69, episode reward: 42.000, mean reward:  2.471 [-2.408, 32.910], mean action: 3.294 [0.000, 19.000],  loss: 0.020562, mae: 0.399384, mean_q: 0.512146, mean_eps: 0.000000
 1181/5000: episode: 49, duration: 0.467s, episode steps:  16, steps per second:  34, episode reward: 38.732, mean reward:  2.421 [-2.514, 32.732], mean action: 5.625 [0.000, 19.000],  loss: 0.019530, mae: 0.402434, mean_q: 0.547373, mean_eps: 0.000000
 1221/5000: episode: 50, duration: 1.311s, episode steps:  40, steps per second:  31, episode reward: -32.910, mean reward: -0.823 [-32.193,  2.459], mean action: 10.350 [0.000, 19.000],  loss: 0.020643, mae: 0.414770, mean_q: 0.432515, mean_eps: 0.000000
 1244/5000: episode: 51, duration: 0.666s, episode steps:  23, steps per second:  35, episode reward: 33.000, mean reward:  1.435 [-3.000, 32.800], mean action: 4.826 [0.000, 18.000],  loss: 0.019000, mae: 0.405074, mean_q: 0.464921, mean_eps: 0.000000
 1273/5000: episode: 52, duration: 0.553s, episode steps:  29, steps per second:  52, episode reward: -33.000, mean reward: -1.138 [-32.064,  2.721], mean action: 9.690 [0.000, 21.000],  loss: 0.019105, mae: 0.401638, mean_q: 0.464784, mean_eps: 0.000000
 1298/5000: episode: 53, duration: 0.397s, episode steps:  25, steps per second:  63, episode reward: 32.877, mean reward:  1.315 [-3.000, 32.378], mean action: 8.440 [0.000, 20.000],  loss: 0.020112, mae: 0.396369, mean_q: 0.500957, mean_eps: 0.000000
 1318/5000: episode: 54, duration: 0.371s, episode steps:  20, steps per second:  54, episode reward: 34.720, mean reward:  1.736 [-3.000, 32.361], mean action: 3.650 [0.000, 12.000],  loss: 0.019466, mae: 0.394803, mean_q: 0.488448, mean_eps: 0.000000
 1341/5000: episode: 55, duration: 0.385s, episode steps:  23, steps per second:  60, episode reward: 32.483, mean reward:  1.412 [-2.386, 32.050], mean action: 5.478 [3.000, 12.000],  loss: 0.022540, mae: 0.420827, mean_q: 0.481643, mean_eps: 0.000000
 1366/5000: episode: 56, duration: 0.725s, episode steps:  25, steps per second:  34, episode reward: 34.830, mean reward:  1.393 [-2.478, 32.070], mean action: 5.440 [0.000, 14.000],  loss: 0.017330, mae: 0.393133, mean_q: 0.483602, mean_eps: 0.000000
 1388/5000: episode: 57, duration: 0.342s, episode steps:  22, steps per second:  64, episode reward: -32.320, mean reward: -1.469 [-32.410,  2.500], mean action: 4.636 [0.000, 14.000],  loss: 0.018588, mae: 0.396581, mean_q: 0.451689, mean_eps: 0.000000
 1412/5000: episode: 58, duration: 0.346s, episode steps:  24, steps per second:  69, episode reward: -32.350, mean reward: -1.348 [-32.720,  2.750], mean action: 7.792 [0.000, 21.000],  loss: 0.019519, mae: 0.409661, mean_q: 0.433106, mean_eps: 0.000000
 1442/5000: episode: 59, duration: 0.458s, episode steps:  30, steps per second:  65, episode reward: 35.689, mean reward:  1.190 [-3.000, 32.250], mean action: 5.500 [0.000, 20.000],  loss: 0.020332, mae: 0.410065, mean_q: 0.401356, mean_eps: 0.000000
 1464/5000: episode: 60, duration: 0.398s, episode steps:  22, steps per second:  55, episode reward: 32.009, mean reward:  1.455 [-3.000, 32.814], mean action: 4.364 [0.000, 17.000],  loss: 0.019516, mae: 0.394486, mean_q: 0.454394, mean_eps: 0.000000
 1485/5000: episode: 61, duration: 0.352s, episode steps:  21, steps per second:  60, episode reward: 38.667, mean reward:  1.841 [-2.318, 32.390], mean action: 3.524 [0.000, 20.000],  loss: 0.017825, mae: 0.384906, mean_q: 0.468156, mean_eps: 0.000000
 1506/5000: episode: 62, duration: 0.685s, episode steps:  21, steps per second:  31, episode reward: 38.875, mean reward:  1.851 [-2.745, 32.875], mean action: 2.238 [0.000, 9.000],  loss: 0.020060, mae: 0.396048, mean_q: 0.527857, mean_eps: 0.000000
 1521/5000: episode: 63, duration: 0.354s, episode steps:  15, steps per second:  42, episode reward: 41.162, mean reward:  2.744 [-2.431, 32.150], mean action: 2.133 [0.000, 9.000],  loss: 0.017913, mae: 0.385863, mean_q: 0.560435, mean_eps: 0.000000
 1549/5000: episode: 64, duration: 0.772s, episode steps:  28, steps per second:  36, episode reward: -32.110, mean reward: -1.147 [-32.406,  2.716], mean action: 6.143 [0.000, 20.000],  loss: 0.020184, mae: 0.403095, mean_q: 0.479208, mean_eps: 0.000000
 1572/5000: episode: 65, duration: 0.378s, episode steps:  23, steps per second:  61, episode reward: -33.000, mean reward: -1.435 [-33.000,  2.607], mean action: 2.696 [0.000, 19.000],  loss: 0.019707, mae: 0.400553, mean_q: 0.488103, mean_eps: 0.000000
 1591/5000: episode: 66, duration: 0.256s, episode steps:  19, steps per second:  74, episode reward: 46.930, mean reward:  2.470 [-0.337, 32.099], mean action: 3.947 [0.000, 20.000],  loss: 0.018822, mae: 0.406151, mean_q: 0.466177, mean_eps: 0.000000
 1617/5000: episode: 67, duration: 0.466s, episode steps:  26, steps per second:  56, episode reward: 38.089, mean reward:  1.465 [-2.301, 31.843], mean action: 3.962 [0.000, 20.000],  loss: 0.017016, mae: 0.382565, mean_q: 0.471681, mean_eps: 0.000000
 1628/5000: episode: 68, duration: 0.155s, episode steps:  11, steps per second:  71, episode reward: 44.201, mean reward:  4.018 [-2.305, 32.176], mean action: 4.909 [1.000, 20.000],  loss: 0.021641, mae: 0.405793, mean_q: 0.530744, mean_eps: 0.000000
 1644/5000: episode: 69, duration: 0.214s, episode steps:  16, steps per second:  75, episode reward: 40.457, mean reward:  2.529 [-2.863, 32.296], mean action: 5.562 [0.000, 19.000],  loss: 0.021804, mae: 0.408541, mean_q: 0.488910, mean_eps: 0.000000
 1671/5000: episode: 70, duration: 0.372s, episode steps:  27, steps per second:  73, episode reward: 38.401, mean reward:  1.422 [-3.000, 32.196], mean action: 4.778 [0.000, 15.000],  loss: 0.018913, mae: 0.398393, mean_q: 0.530436, mean_eps: 0.000000
 1699/5000: episode: 71, duration: 0.367s, episode steps:  28, steps per second:  76, episode reward: 32.140, mean reward:  1.148 [-2.603, 32.190], mean action: 5.786 [0.000, 16.000],  loss: 0.021004, mae: 0.409548, mean_q: 0.513637, mean_eps: 0.000000
 1722/5000: episode: 72, duration: 0.322s, episode steps:  23, steps per second:  71, episode reward: 39.000, mean reward:  1.696 [-2.345, 32.620], mean action: 2.826 [1.000, 9.000],  loss: 0.022836, mae: 0.418206, mean_q: 0.450562, mean_eps: 0.000000
 1748/5000: episode: 73, duration: 0.513s, episode steps:  26, steps per second:  51, episode reward: 38.482, mean reward:  1.480 [-3.000, 32.482], mean action: 3.192 [0.000, 16.000],  loss: 0.021583, mae: 0.411690, mean_q: 0.461462, mean_eps: 0.000000
 1771/5000: episode: 74, duration: 0.537s, episode steps:  23, steps per second:  43, episode reward: 38.782, mean reward:  1.686 [-2.277, 32.250], mean action: 3.217 [0.000, 12.000],  loss: 0.023749, mae: 0.428719, mean_q: 0.407098, mean_eps: 0.000000
 1794/5000: episode: 75, duration: 0.341s, episode steps:  23, steps per second:  67, episode reward: 33.000, mean reward:  1.435 [-2.708, 32.030], mean action: 4.391 [0.000, 14.000],  loss: 0.017637, mae: 0.395112, mean_q: 0.406481, mean_eps: 0.000000
 1818/5000: episode: 76, duration: 0.319s, episode steps:  24, steps per second:  75, episode reward: -32.910, mean reward: -1.371 [-32.572,  2.833], mean action: 3.917 [0.000, 16.000],  loss: 0.020372, mae: 0.407099, mean_q: 0.467484, mean_eps: 0.000000
 1844/5000: episode: 77, duration: 0.353s, episode steps:  26, steps per second:  74, episode reward: 32.674, mean reward:  1.257 [-2.475, 32.240], mean action: 3.731 [0.000, 16.000],  loss: 0.017759, mae: 0.382928, mean_q: 0.517007, mean_eps: 0.000000
 1864/5000: episode: 78, duration: 0.278s, episode steps:  20, steps per second:  72, episode reward: 41.321, mean reward:  2.066 [-2.092, 29.946], mean action: 2.000 [0.000, 9.000],  loss: 0.016292, mae: 0.383687, mean_q: 0.497902, mean_eps: 0.000000
 1882/5000: episode: 79, duration: 0.270s, episode steps:  18, steps per second:  67, episode reward: 39.000, mean reward:  2.167 [-2.415, 32.170], mean action: 3.444 [0.000, 11.000],  loss: 0.020981, mae: 0.409816, mean_q: 0.422235, mean_eps: 0.000000
 1905/5000: episode: 80, duration: 0.367s, episode steps:  23, steps per second:  63, episode reward: -32.560, mean reward: -1.416 [-32.560,  2.618], mean action: 5.478 [0.000, 20.000],  loss: 0.020514, mae: 0.416663, mean_q: 0.408583, mean_eps: 0.000000
 1931/5000: episode: 81, duration: 0.420s, episode steps:  26, steps per second:  62, episode reward: 32.148, mean reward:  1.236 [-3.000, 30.152], mean action: 2.962 [0.000, 12.000],  loss: 0.023070, mae: 0.418688, mean_q: 0.435167, mean_eps: 0.000000
 1952/5000: episode: 82, duration: 0.300s, episode steps:  21, steps per second:  70, episode reward: 35.792, mean reward:  1.704 [-3.000, 29.911], mean action: 5.429 [0.000, 14.000],  loss: 0.020559, mae: 0.405662, mean_q: 0.471039, mean_eps: 0.000000
 1971/5000: episode: 83, duration: 0.252s, episode steps:  19, steps per second:  76, episode reward: -41.400, mean reward: -2.179 [-32.447,  1.767], mean action: 6.368 [0.000, 14.000],  loss: 0.019047, mae: 0.403643, mean_q: 0.512653, mean_eps: 0.000000
 2013/5000: episode: 84, duration: 0.513s, episode steps:  42, steps per second:  82, episode reward: 34.668, mean reward:  0.825 [-3.000, 32.366], mean action: 4.405 [0.000, 14.000],  loss: 0.020583, mae: 0.415032, mean_q: 0.441229, mean_eps: 0.000000
 2032/5000: episode: 85, duration: 0.249s, episode steps:  19, steps per second:  76, episode reward: 32.024, mean reward:  1.685 [-3.000, 31.527], mean action: 3.947 [0.000, 12.000],  loss: 0.020578, mae: 0.415797, mean_q: 0.467297, mean_eps: 0.000000
 2051/5000: episode: 86, duration: 0.247s, episode steps:  19, steps per second:  77, episode reward: -41.050, mean reward: -2.161 [-32.349,  2.090], mean action: 3.895 [0.000, 12.000],  loss: 0.020005, mae: 0.406832, mean_q: 0.513407, mean_eps: 0.000000
 2080/5000: episode: 87, duration: 0.364s, episode steps:  29, steps per second:  80, episode reward: -32.100, mean reward: -1.107 [-32.169,  2.492], mean action: 5.690 [0.000, 15.000],  loss: 0.020700, mae: 0.412874, mean_q: 0.533708, mean_eps: 0.000000
 2107/5000: episode: 88, duration: 0.378s, episode steps:  27, steps per second:  71, episode reward: 35.531, mean reward:  1.316 [-2.541, 32.120], mean action: 4.889 [0.000, 14.000],  loss: 0.018146, mae: 0.400870, mean_q: 0.542286, mean_eps: 0.000000
 2125/5000: episode: 89, duration: 0.244s, episode steps:  18, steps per second:  74, episode reward: 44.617, mean reward:  2.479 [-2.228, 33.000], mean action: 4.111 [0.000, 19.000],  loss: 0.022208, mae: 0.428763, mean_q: 0.468569, mean_eps: 0.000000
 2146/5000: episode: 90, duration: 0.269s, episode steps:  21, steps per second:  78, episode reward: 38.606, mean reward:  1.838 [-3.000, 32.663], mean action: 4.238 [0.000, 19.000],  loss: 0.017675, mae: 0.410669, mean_q: 0.435790, mean_eps: 0.000000
 2169/5000: episode: 91, duration: 0.291s, episode steps:  23, steps per second:  79, episode reward: 32.616, mean reward:  1.418 [-3.000, 32.396], mean action: 5.696 [0.000, 19.000],  loss: 0.019592, mae: 0.410068, mean_q: 0.427641, mean_eps: 0.000000
 2190/5000: episode: 92, duration: 0.274s, episode steps:  21, steps per second:  77, episode reward: 44.049, mean reward:  2.098 [-2.469, 31.534], mean action: 3.095 [0.000, 9.000],  loss: 0.019240, mae: 0.398031, mean_q: 0.474547, mean_eps: 0.000000
 2226/5000: episode: 93, duration: 0.733s, episode steps:  36, steps per second:  49, episode reward: 40.818, mean reward:  1.134 [-2.127, 32.340], mean action: 3.639 [0.000, 15.000],  loss: 0.020682, mae: 0.401094, mean_q: 0.472949, mean_eps: 0.000000
 2244/5000: episode: 94, duration: 0.238s, episode steps:  18, steps per second:  75, episode reward: 35.094, mean reward:  1.950 [-3.000, 31.214], mean action: 4.778 [0.000, 19.000],  loss: 0.019602, mae: 0.407343, mean_q: 0.474115, mean_eps: 0.000000
 2264/5000: episode: 95, duration: 0.264s, episode steps:  20, steps per second:  76, episode reward: 38.900, mean reward:  1.945 [-2.176, 32.070], mean action: 2.250 [0.000, 12.000],  loss: 0.018019, mae: 0.403780, mean_q: 0.439298, mean_eps: 0.000000
 2293/5000: episode: 96, duration: 0.369s, episode steps:  29, steps per second:  79, episode reward: 35.401, mean reward:  1.221 [-2.802, 32.090], mean action: 5.448 [0.000, 21.000],  loss: 0.024271, mae: 0.432055, mean_q: 0.423818, mean_eps: 0.000000
 2314/5000: episode: 97, duration: 0.269s, episode steps:  21, steps per second:  78, episode reward: 32.091, mean reward:  1.528 [-3.000, 32.070], mean action: 6.286 [0.000, 19.000],  loss: 0.021467, mae: 0.407703, mean_q: 0.433499, mean_eps: 0.000000
 2331/5000: episode: 98, duration: 0.229s, episode steps:  17, steps per second:  74, episode reward: 41.796, mean reward:  2.459 [-2.238, 32.206], mean action: 3.294 [0.000, 19.000],  loss: 0.016568, mae: 0.389945, mean_q: 0.463835, mean_eps: 0.000000
 2361/5000: episode: 99, duration: 0.448s, episode steps:  30, steps per second:  67, episode reward: -32.330, mean reward: -1.078 [-31.620,  2.330], mean action: 8.200 [0.000, 20.000],  loss: 0.021754, mae: 0.421608, mean_q: 0.448952, mean_eps: 0.000000
 2387/5000: episode: 100, duration: 0.363s, episode steps:  26, steps per second:  72, episode reward: 35.187, mean reward:  1.353 [-2.367, 32.259], mean action: 4.115 [0.000, 12.000],  loss: 0.018920, mae: 0.406174, mean_q: 0.429238, mean_eps: 0.000000
 2405/5000: episode: 101, duration: 0.248s, episode steps:  18, steps per second:  73, episode reward: -35.430, mean reward: -1.968 [-32.029,  2.722], mean action: 8.444 [0.000, 14.000],  loss: 0.015886, mae: 0.380895, mean_q: 0.425922, mean_eps: 0.000000
 2428/5000: episode: 102, duration: 0.338s, episode steps:  23, steps per second:  68, episode reward: 38.569, mean reward:  1.677 [-2.533, 32.690], mean action: 2.174 [0.000, 11.000],  loss: 0.021432, mae: 0.415320, mean_q: 0.449312, mean_eps: 0.000000
 2446/5000: episode: 103, duration: 0.447s, episode steps:  18, steps per second:  40, episode reward: 38.694, mean reward:  2.150 [-3.000, 32.790], mean action: 3.278 [0.000, 19.000],  loss: 0.020736, mae: 0.416457, mean_q: 0.504969, mean_eps: 0.000000
 2468/5000: episode: 104, duration: 0.295s, episode steps:  22, steps per second:  74, episode reward: 41.807, mean reward:  1.900 [-2.214, 32.153], mean action: 2.091 [0.000, 19.000],  loss: 0.023983, mae: 0.429820, mean_q: 0.509645, mean_eps: 0.000000
 2493/5000: episode: 105, duration: 0.325s, episode steps:  25, steps per second:  77, episode reward: 35.171, mean reward:  1.407 [-2.412, 32.831], mean action: 5.800 [0.000, 19.000],  loss: 0.020964, mae: 0.416670, mean_q: 0.565505, mean_eps: 0.000000
 2516/5000: episode: 106, duration: 0.293s, episode steps:  23, steps per second:  78, episode reward: -38.590, mean reward: -1.678 [-32.145,  2.490], mean action: 3.696 [0.000, 19.000],  loss: 0.019189, mae: 0.415043, mean_q: 0.528786, mean_eps: 0.000000
 2539/5000: episode: 107, duration: 0.425s, episode steps:  23, steps per second:  54, episode reward: 32.754, mean reward:  1.424 [-3.000, 32.484], mean action: 6.174 [0.000, 19.000],  loss: 0.022266, mae: 0.431021, mean_q: 0.450502, mean_eps: 0.000000
 2560/5000: episode: 108, duration: 0.410s, episode steps:  21, steps per second:  51, episode reward: 36.000, mean reward:  1.714 [-3.000, 32.440], mean action: 3.762 [0.000, 15.000],  loss: 0.022948, mae: 0.431328, mean_q: 0.435339, mean_eps: 0.000000
 2579/5000: episode: 109, duration: 0.261s, episode steps:  19, steps per second:  73, episode reward: 38.061, mean reward:  2.003 [-2.611, 32.120], mean action: 5.158 [0.000, 20.000],  loss: 0.017499, mae: 0.402401, mean_q: 0.431194, mean_eps: 0.000000
 2602/5000: episode: 110, duration: 0.301s, episode steps:  23, steps per second:  76, episode reward: 32.274, mean reward:  1.403 [-3.000, 32.050], mean action: 4.870 [0.000, 15.000],  loss: 0.019880, mae: 0.415392, mean_q: 0.445460, mean_eps: 0.000000
 2625/5000: episode: 111, duration: 0.292s, episode steps:  23, steps per second:  79, episode reward: 36.000, mean reward:  1.565 [-2.599, 33.000], mean action: 4.435 [0.000, 12.000],  loss: 0.014910, mae: 0.387927, mean_q: 0.468914, mean_eps: 0.000000
 2650/5000: episode: 112, duration: 0.337s, episode steps:  25, steps per second:  74, episode reward: 35.532, mean reward:  1.421 [-2.630, 32.251], mean action: 5.400 [0.000, 21.000],  loss: 0.018685, mae: 0.401090, mean_q: 0.512936, mean_eps: 0.000000
 2671/5000: episode: 113, duration: 0.300s, episode steps:  21, steps per second:  70, episode reward: 38.806, mean reward:  1.848 [-2.227, 32.210], mean action: 2.905 [0.000, 16.000],  loss: 0.018134, mae: 0.404806, mean_q: 0.461410, mean_eps: 0.000000
 2697/5000: episode: 114, duration: 0.332s, episode steps:  26, steps per second:  78, episode reward: 34.999, mean reward:  1.346 [-2.390, 32.016], mean action: 3.538 [0.000, 19.000],  loss: 0.019736, mae: 0.415568, mean_q: 0.470468, mean_eps: 0.000000
 2716/5000: episode: 115, duration: 0.248s, episode steps:  19, steps per second:  77, episode reward: 40.742, mean reward:  2.144 [-2.097, 31.899], mean action: 4.947 [0.000, 19.000],  loss: 0.018054, mae: 0.395692, mean_q: 0.474131, mean_eps: 0.000000
 2731/5000: episode: 116, duration: 0.199s, episode steps:  15, steps per second:  75, episode reward: 44.172, mean reward:  2.945 [-2.137, 32.240], mean action: 3.600 [1.000, 19.000],  loss: 0.018360, mae: 0.398299, mean_q: 0.498150, mean_eps: 0.000000
 2748/5000: episode: 117, duration: 0.215s, episode steps:  17, steps per second:  79, episode reward: -41.710, mean reward: -2.454 [-32.242,  2.130], mean action: 9.765 [0.000, 19.000],  loss: 0.020180, mae: 0.411961, mean_q: 0.457999, mean_eps: 0.000000
 2770/5000: episode: 118, duration: 0.303s, episode steps:  22, steps per second:  73, episode reward: 35.037, mean reward:  1.593 [-2.202, 30.692], mean action: 5.455 [0.000, 16.000],  loss: 0.015410, mae: 0.379307, mean_q: 0.496485, mean_eps: 0.000000
 2792/5000: episode: 119, duration: 0.275s, episode steps:  22, steps per second:  80, episode reward: -38.270, mean reward: -1.740 [-32.270,  2.278], mean action: 4.727 [0.000, 15.000],  loss: 0.019873, mae: 0.397687, mean_q: 0.520248, mean_eps: 0.000000
 2808/5000: episode: 120, duration: 0.209s, episode steps:  16, steps per second:  76, episode reward: 41.904, mean reward:  2.619 [-2.310, 32.940], mean action: 3.500 [0.000, 11.000],  loss: 0.019711, mae: 0.397453, mean_q: 0.519868, mean_eps: 0.000000
 2833/5000: episode: 121, duration: 0.319s, episode steps:  25, steps per second:  78, episode reward: -32.880, mean reward: -1.315 [-32.052,  2.582], mean action: 4.680 [0.000, 15.000],  loss: 0.023332, mae: 0.419752, mean_q: 0.514320, mean_eps: 0.000000
 2869/5000: episode: 122, duration: 0.452s, episode steps:  36, steps per second:  80, episode reward: 35.868, mean reward:  0.996 [-2.265, 32.558], mean action: 4.667 [0.000, 20.000],  loss: 0.018397, mae: 0.399886, mean_q: 0.526297, mean_eps: 0.000000
 2879/5000: episode: 123, duration: 0.142s, episode steps:  10, steps per second:  71, episode reward: 47.390, mean reward:  4.739 [ 0.000, 32.270], mean action: 1.400 [0.000, 8.000],  loss: 0.019879, mae: 0.399457, mean_q: 0.551491, mean_eps: 0.000000
 2897/5000: episode: 124, duration: 0.243s, episode steps:  18, steps per second:  74, episode reward: 41.514, mean reward:  2.306 [-2.242, 32.270], mean action: 1.833 [0.000, 9.000],  loss: 0.015964, mae: 0.384148, mean_q: 0.548742, mean_eps: 0.000000
 2923/5000: episode: 125, duration: 0.334s, episode steps:  26, steps per second:  78, episode reward: 32.862, mean reward:  1.264 [-3.000, 32.362], mean action: 4.654 [0.000, 21.000],  loss: 0.022809, mae: 0.413738, mean_q: 0.528030, mean_eps: 0.000000
 2947/5000: episode: 126, duration: 0.302s, episode steps:  24, steps per second:  79, episode reward: 35.152, mean reward:  1.465 [-3.000, 32.033], mean action: 3.417 [0.000, 19.000],  loss: 0.020938, mae: 0.415710, mean_q: 0.493544, mean_eps: 0.000000
 2971/5000: episode: 127, duration: 0.317s, episode steps:  24, steps per second:  76, episode reward: 34.757, mean reward:  1.448 [-2.644, 31.701], mean action: 5.250 [0.000, 15.000],  loss: 0.025073, mae: 0.427877, mean_q: 0.453166, mean_eps: 0.000000
 3002/5000: episode: 128, duration: 0.400s, episode steps:  31, steps per second:  77, episode reward: -32.290, mean reward: -1.042 [-31.974,  2.403], mean action: 6.355 [0.000, 19.000],  loss: 0.019565, mae: 0.400584, mean_q: 0.429150, mean_eps: 0.000000
 3025/5000: episode: 129, duration: 0.296s, episode steps:  23, steps per second:  78, episode reward: 35.918, mean reward:  1.562 [-2.521, 32.098], mean action: 5.087 [0.000, 19.000],  loss: 0.019685, mae: 0.400886, mean_q: 0.433313, mean_eps: 0.000000
 3053/5000: episode: 130, duration: 0.350s, episode steps:  28, steps per second:  80, episode reward: -32.580, mean reward: -1.164 [-32.083,  3.000], mean action: 4.500 [0.000, 19.000],  loss: 0.018565, mae: 0.398999, mean_q: 0.437012, mean_eps: 0.000000
 3075/5000: episode: 131, duration: 0.288s, episode steps:  22, steps per second:  76, episode reward: 38.472, mean reward:  1.749 [-2.609, 32.310], mean action: 4.227 [0.000, 15.000],  loss: 0.020046, mae: 0.407624, mean_q: 0.425010, mean_eps: 0.000000
 3093/5000: episode: 132, duration: 0.233s, episode steps:  18, steps per second:  77, episode reward: 35.353, mean reward:  1.964 [-2.807, 31.753], mean action: 3.222 [0.000, 12.000],  loss: 0.019472, mae: 0.388965, mean_q: 0.510164, mean_eps: 0.000000
 3110/5000: episode: 133, duration: 0.221s, episode steps:  17, steps per second:  77, episode reward: 38.235, mean reward:  2.249 [-2.251, 32.065], mean action: 7.235 [0.000, 15.000],  loss: 0.019580, mae: 0.386977, mean_q: 0.539328, mean_eps: 0.000000
 3135/5000: episode: 134, duration: 0.317s, episode steps:  25, steps per second:  79, episode reward: 32.715, mean reward:  1.309 [-2.387, 32.425], mean action: 3.560 [0.000, 12.000],  loss: 0.021584, mae: 0.393570, mean_q: 0.557110, mean_eps: 0.000000
 3151/5000: episode: 135, duration: 0.215s, episode steps:  16, steps per second:  74, episode reward: 38.387, mean reward:  2.399 [-2.524, 32.099], mean action: 3.938 [0.000, 11.000],  loss: 0.019569, mae: 0.383849, mean_q: 0.517927, mean_eps: 0.000000
 3179/5000: episode: 136, duration: 0.367s, episode steps:  28, steps per second:  76, episode reward: 38.602, mean reward:  1.379 [-2.401, 31.981], mean action: 1.393 [0.000, 11.000],  loss: 0.017903, mae: 0.389880, mean_q: 0.434388, mean_eps: 0.000000
 3202/5000: episode: 137, duration: 0.297s, episode steps:  23, steps per second:  77, episode reward: 34.864, mean reward:  1.516 [-3.000, 31.939], mean action: 3.174 [0.000, 19.000],  loss: 0.021404, mae: 0.406477, mean_q: 0.461722, mean_eps: 0.000000
 3211/5000: episode: 138, duration: 0.139s, episode steps:   9, steps per second:  65, episode reward: 44.343, mean reward:  4.927 [-2.185, 32.260], mean action: 4.111 [1.000, 15.000],  loss: 0.022001, mae: 0.405693, mean_q: 0.500271, mean_eps: 0.000000
 3232/5000: episode: 139, duration: 0.266s, episode steps:  21, steps per second:  79, episode reward: 35.712, mean reward:  1.701 [-3.000, 32.150], mean action: 4.857 [0.000, 20.000],  loss: 0.020529, mae: 0.400372, mean_q: 0.504952, mean_eps: 0.000000
 3258/5000: episode: 140, duration: 0.350s, episode steps:  26, steps per second:  74, episode reward: -35.420, mean reward: -1.362 [-31.940,  2.714], mean action: 7.077 [0.000, 20.000],  loss: 0.022245, mae: 0.404823, mean_q: 0.510244, mean_eps: 0.000000
 3280/5000: episode: 141, duration: 0.280s, episode steps:  22, steps per second:  78, episode reward: -32.530, mean reward: -1.479 [-32.351,  3.000], mean action: 4.909 [0.000, 20.000],  loss: 0.019707, mae: 0.398380, mean_q: 0.490028, mean_eps: 0.000000
 3296/5000: episode: 142, duration: 0.232s, episode steps:  16, steps per second:  69, episode reward: 38.335, mean reward:  2.396 [-2.494, 33.000], mean action: 3.875 [0.000, 15.000],  loss: 0.026471, mae: 0.422142, mean_q: 0.481053, mean_eps: 0.000000
 3323/5000: episode: 143, duration: 0.589s, episode steps:  27, steps per second:  46, episode reward: -35.770, mean reward: -1.325 [-32.018,  2.696], mean action: 5.222 [0.000, 20.000],  loss: 0.023461, mae: 0.410016, mean_q: 0.488039, mean_eps: 0.000000
 3353/5000: episode: 144, duration: 0.449s, episode steps:  30, steps per second:  67, episode reward: 32.352, mean reward:  1.078 [-2.700, 32.800], mean action: 4.933 [0.000, 16.000],  loss: 0.022500, mae: 0.403622, mean_q: 0.472739, mean_eps: 0.000000
 3376/5000: episode: 145, duration: 0.289s, episode steps:  23, steps per second:  80, episode reward: 34.697, mean reward:  1.509 [-2.664, 33.000], mean action: 3.261 [0.000, 16.000],  loss: 0.017475, mae: 0.382983, mean_q: 0.533929, mean_eps: 0.000000
 3403/5000: episode: 146, duration: 0.377s, episode steps:  27, steps per second:  72, episode reward: 32.819, mean reward:  1.216 [-2.482, 32.039], mean action: 3.296 [0.000, 12.000],  loss: 0.022832, mae: 0.409282, mean_q: 0.496392, mean_eps: 0.000000
 3423/5000: episode: 147, duration: 0.278s, episode steps:  20, steps per second:  72, episode reward: 38.240, mean reward:  1.912 [-2.492, 32.100], mean action: 3.950 [0.000, 20.000],  loss: 0.025171, mae: 0.428724, mean_q: 0.475125, mean_eps: 0.000000
 3445/5000: episode: 148, duration: 0.288s, episode steps:  22, steps per second:  77, episode reward: -32.460, mean reward: -1.475 [-31.833,  2.840], mean action: 5.773 [0.000, 16.000],  loss: 0.024241, mae: 0.418910, mean_q: 0.495749, mean_eps: 0.000000
 3462/5000: episode: 149, duration: 0.223s, episode steps:  17, steps per second:  76, episode reward: 38.070, mean reward:  2.239 [-2.654, 31.937], mean action: 5.824 [1.000, 19.000],  loss: 0.024129, mae: 0.423074, mean_q: 0.500278, mean_eps: 0.000000
 3482/5000: episode: 150, duration: 0.261s, episode steps:  20, steps per second:  77, episode reward: 35.845, mean reward:  1.792 [-2.756, 33.000], mean action: 3.650 [0.000, 19.000],  loss: 0.016474, mae: 0.383930, mean_q: 0.522856, mean_eps: 0.000000
 3496/5000: episode: 151, duration: 0.208s, episode steps:  14, steps per second:  67, episode reward: 43.749, mean reward:  3.125 [-2.472, 31.991], mean action: 3.286 [0.000, 19.000],  loss: 0.023362, mae: 0.422948, mean_q: 0.512458, mean_eps: 0.000000
 3521/5000: episode: 152, duration: 0.492s, episode steps:  25, steps per second:  51, episode reward: 36.000, mean reward:  1.440 [-2.424, 32.680], mean action: 5.800 [0.000, 19.000],  loss: 0.021553, mae: 0.405713, mean_q: 0.478301, mean_eps: 0.000000
 3539/5000: episode: 153, duration: 0.245s, episode steps:  18, steps per second:  73, episode reward: 41.102, mean reward:  2.283 [-2.379, 32.510], mean action: 2.944 [0.000, 19.000],  loss: 0.022943, mae: 0.412660, mean_q: 0.459883, mean_eps: 0.000000
 3569/5000: episode: 154, duration: 0.517s, episode steps:  30, steps per second:  58, episode reward: 35.090, mean reward:  1.170 [-3.000, 32.375], mean action: 8.233 [0.000, 19.000],  loss: 0.020792, mae: 0.408297, mean_q: 0.403817, mean_eps: 0.000000
 3587/5000: episode: 155, duration: 0.286s, episode steps:  18, steps per second:  63, episode reward: 38.741, mean reward:  2.152 [-2.258, 32.270], mean action: 3.556 [0.000, 19.000],  loss: 0.019513, mae: 0.408502, mean_q: 0.415742, mean_eps: 0.000000
 3606/5000: episode: 156, duration: 0.252s, episode steps:  19, steps per second:  75, episode reward: 35.606, mean reward:  1.874 [-3.000, 32.606], mean action: 4.211 [0.000, 19.000],  loss: 0.023573, mae: 0.424932, mean_q: 0.467358, mean_eps: 0.000000
 3637/5000: episode: 157, duration: 0.483s, episode steps:  31, steps per second:  64, episode reward: 34.356, mean reward:  1.108 [-2.444, 32.190], mean action: 6.581 [0.000, 21.000],  loss: 0.021792, mae: 0.407571, mean_q: 0.490469, mean_eps: 0.000000
 3663/5000: episode: 158, duration: 0.850s, episode steps:  26, steps per second:  31, episode reward: 32.952, mean reward:  1.267 [-3.000, 32.152], mean action: 4.115 [0.000, 14.000],  loss: 0.015619, mae: 0.393752, mean_q: 0.441238, mean_eps: 0.000000
 3684/5000: episode: 159, duration: 0.334s, episode steps:  21, steps per second:  63, episode reward: 38.611, mean reward:  1.839 [-2.420, 33.000], mean action: 2.381 [0.000, 12.000],  loss: 0.017614, mae: 0.408914, mean_q: 0.428523, mean_eps: 0.000000
 3700/5000: episode: 160, duration: 0.317s, episode steps:  16, steps per second:  50, episode reward: 41.900, mean reward:  2.619 [-2.372, 32.530], mean action: 2.062 [0.000, 12.000],  loss: 0.019647, mae: 0.410807, mean_q: 0.476396, mean_eps: 0.000000
 3730/5000: episode: 161, duration: 0.803s, episode steps:  30, steps per second:  37, episode reward: 31.000, mean reward:  1.033 [-3.000, 32.340], mean action: 5.333 [0.000, 20.000],  loss: 0.019599, mae: 0.416630, mean_q: 0.431410, mean_eps: 0.000000
 3753/5000: episode: 162, duration: 0.497s, episode steps:  23, steps per second:  46, episode reward: 35.714, mean reward:  1.553 [-2.446, 32.540], mean action: 5.609 [0.000, 21.000],  loss: 0.019227, mae: 0.408629, mean_q: 0.491863, mean_eps: 0.000000
 3773/5000: episode: 163, duration: 0.285s, episode steps:  20, steps per second:  70, episode reward: -38.270, mean reward: -1.913 [-32.358,  2.299], mean action: 3.750 [0.000, 15.000],  loss: 0.018713, mae: 0.410967, mean_q: 0.453504, mean_eps: 0.000000
 3786/5000: episode: 164, duration: 0.202s, episode steps:  13, steps per second:  64, episode reward: 39.000, mean reward:  3.000 [-2.131, 30.934], mean action: 6.615 [0.000, 15.000],  loss: 0.021750, mae: 0.429902, mean_q: 0.441475, mean_eps: 0.000000
 3808/5000: episode: 165, duration: 0.318s, episode steps:  22, steps per second:  69, episode reward: 35.020, mean reward:  1.592 [-3.000, 32.260], mean action: 5.545 [0.000, 15.000],  loss: 0.021057, mae: 0.427784, mean_q: 0.451157, mean_eps: 0.000000
 3828/5000: episode: 166, duration: 0.286s, episode steps:  20, steps per second:  70, episode reward: 41.848, mean reward:  2.092 [-2.220, 32.260], mean action: 5.000 [0.000, 16.000],  loss: 0.019737, mae: 0.417841, mean_q: 0.460581, mean_eps: 0.000000
 3848/5000: episode: 167, duration: 0.281s, episode steps:  20, steps per second:  71, episode reward: 39.000, mean reward:  1.950 [-2.148, 32.110], mean action: 3.950 [0.000, 16.000],  loss: 0.021365, mae: 0.431870, mean_q: 0.419676, mean_eps: 0.000000
 3875/5000: episode: 168, duration: 0.670s, episode steps:  27, steps per second:  40, episode reward: 35.336, mean reward:  1.309 [-2.333, 32.590], mean action: 3.667 [0.000, 20.000],  loss: 0.017901, mae: 0.414026, mean_q: 0.460657, mean_eps: 0.000000
 3892/5000: episode: 169, duration: 0.295s, episode steps:  17, steps per second:  58, episode reward: 38.572, mean reward:  2.269 [-2.261, 31.916], mean action: 4.412 [0.000, 16.000],  loss: 0.018534, mae: 0.401126, mean_q: 0.484293, mean_eps: 0.000000
 3918/5000: episode: 170, duration: 0.363s, episode steps:  26, steps per second:  72, episode reward: -32.580, mean reward: -1.253 [-32.129,  2.430], mean action: 7.538 [1.000, 16.000],  loss: 0.018155, mae: 0.403948, mean_q: 0.442040, mean_eps: 0.000000
 3946/5000: episode: 171, duration: 0.389s, episode steps:  28, steps per second:  72, episode reward: 32.290, mean reward:  1.153 [-3.000, 32.045], mean action: 8.143 [0.000, 16.000],  loss: 0.022333, mae: 0.423644, mean_q: 0.406924, mean_eps: 0.000000
 3974/5000: episode: 172, duration: 0.393s, episode steps:  28, steps per second:  71, episode reward: 33.000, mean reward:  1.179 [-3.000, 32.650], mean action: 6.679 [0.000, 16.000],  loss: 0.018903, mae: 0.408981, mean_q: 0.431456, mean_eps: 0.000000
 3995/5000: episode: 173, duration: 0.291s, episode steps:  21, steps per second:  72, episode reward: -41.650, mean reward: -1.983 [-32.255,  2.300], mean action: 6.048 [0.000, 17.000],  loss: 0.019346, mae: 0.407479, mean_q: 0.478061, mean_eps: 0.000000
 4018/5000: episode: 174, duration: 0.331s, episode steps:  23, steps per second:  69, episode reward: 35.787, mean reward:  1.556 [-2.494, 32.082], mean action: 4.087 [0.000, 16.000],  loss: 0.023966, mae: 0.427062, mean_q: 0.495878, mean_eps: 0.000000
 4040/5000: episode: 175, duration: 0.354s, episode steps:  22, steps per second:  62, episode reward: 38.730, mean reward:  1.760 [-2.694, 29.952], mean action: 3.955 [0.000, 16.000],  loss: 0.020560, mae: 0.404124, mean_q: 0.539246, mean_eps: 0.000000
 4059/5000: episode: 176, duration: 0.544s, episode steps:  19, steps per second:  35, episode reward: 38.749, mean reward:  2.039 [-2.604, 32.075], mean action: 5.632 [0.000, 16.000],  loss: 0.023574, mae: 0.423170, mean_q: 0.504172, mean_eps: 0.000000
 4088/5000: episode: 177, duration: 0.900s, episode steps:  29, steps per second:  32, episode reward: 32.779, mean reward:  1.130 [-2.620, 32.073], mean action: 6.897 [0.000, 21.000],  loss: 0.022302, mae: 0.418440, mean_q: 0.442254, mean_eps: 0.000000
 4109/5000: episode: 178, duration: 0.397s, episode steps:  21, steps per second:  53, episode reward: 32.446, mean reward:  1.545 [-3.000, 32.190], mean action: 5.238 [0.000, 15.000],  loss: 0.018646, mae: 0.402748, mean_q: 0.446848, mean_eps: 0.000000
 4136/5000: episode: 179, duration: 0.349s, episode steps:  27, steps per second:  77, episode reward: 35.631, mean reward:  1.320 [-3.000, 31.641], mean action: 5.630 [0.000, 16.000],  loss: 0.021256, mae: 0.408360, mean_q: 0.452018, mean_eps: 0.000000
 4159/5000: episode: 180, duration: 0.485s, episode steps:  23, steps per second:  47, episode reward: 35.406, mean reward:  1.539 [-2.668, 32.903], mean action: 8.130 [0.000, 16.000],  loss: 0.020944, mae: 0.402453, mean_q: 0.474319, mean_eps: 0.000000
 4179/5000: episode: 181, duration: 0.283s, episode steps:  20, steps per second:  71, episode reward: 35.419, mean reward:  1.771 [-3.000, 32.610], mean action: 4.800 [0.000, 16.000],  loss: 0.018696, mae: 0.394684, mean_q: 0.416212, mean_eps: 0.000000
 4196/5000: episode: 182, duration: 0.230s, episode steps:  17, steps per second:  74, episode reward: 41.787, mean reward:  2.458 [-2.152, 30.048], mean action: 3.294 [0.000, 20.000],  loss: 0.016939, mae: 0.392280, mean_q: 0.413065, mean_eps: 0.000000
 4215/5000: episode: 183, duration: 0.247s, episode steps:  19, steps per second:  77, episode reward: 38.647, mean reward:  2.034 [-2.801, 33.000], mean action: 4.000 [0.000, 15.000],  loss: 0.023113, mae: 0.425212, mean_q: 0.414655, mean_eps: 0.000000
 4230/5000: episode: 184, duration: 0.213s, episode steps:  15, steps per second:  70, episode reward: 41.684, mean reward:  2.779 [-2.521, 32.114], mean action: 3.933 [1.000, 19.000],  loss: 0.022513, mae: 0.422038, mean_q: 0.425766, mean_eps: 0.000000
 4251/5000: episode: 185, duration: 0.262s, episode steps:  21, steps per second:  80, episode reward: -38.210, mean reward: -1.820 [-31.665,  2.360], mean action: 6.190 [0.000, 15.000],  loss: 0.021613, mae: 0.426557, mean_q: 0.434156, mean_eps: 0.000000
 4279/5000: episode: 186, duration: 0.472s, episode steps:  28, steps per second:  59, episode reward: 30.000, mean reward:  1.071 [-2.220, 29.100], mean action: 3.643 [0.000, 15.000],  loss: 0.020070, mae: 0.407232, mean_q: 0.504746, mean_eps: 0.000000
 4300/5000: episode: 187, duration: 0.348s, episode steps:  21, steps per second:  60, episode reward: 33.000, mean reward:  1.571 [-2.551, 32.910], mean action: 4.810 [0.000, 16.000],  loss: 0.017908, mae: 0.396090, mean_q: 0.490792, mean_eps: 0.000000
 4328/5000: episode: 188, duration: 0.391s, episode steps:  28, steps per second:  72, episode reward: 35.792, mean reward:  1.278 [-2.469, 32.060], mean action: 6.821 [0.000, 20.000],  loss: 0.021244, mae: 0.416243, mean_q: 0.484057, mean_eps: 0.000000
 4345/5000: episode: 189, duration: 0.237s, episode steps:  17, steps per second:  72, episode reward: 38.190, mean reward:  2.246 [-2.428, 32.680], mean action: 4.059 [0.000, 16.000],  loss: 0.020528, mae: 0.410918, mean_q: 0.497820, mean_eps: 0.000000
 4374/5000: episode: 190, duration: 0.372s, episode steps:  29, steps per second:  78, episode reward: 41.653, mean reward:  1.436 [-2.103, 32.490], mean action: 2.414 [0.000, 12.000],  loss: 0.023862, mae: 0.421021, mean_q: 0.490752, mean_eps: 0.000000
 4394/5000: episode: 191, duration: 0.272s, episode steps:  20, steps per second:  74, episode reward: 38.218, mean reward:  1.911 [-2.713, 32.901], mean action: 3.800 [0.000, 19.000],  loss: 0.017438, mae: 0.392007, mean_q: 0.448887, mean_eps: 0.000000
 4427/5000: episode: 192, duration: 0.416s, episode steps:  33, steps per second:  79, episode reward: 35.781, mean reward:  1.084 [-3.000, 31.991], mean action: 4.515 [0.000, 19.000],  loss: 0.021593, mae: 0.403405, mean_q: 0.446896, mean_eps: 0.000000
 4444/5000: episode: 193, duration: 0.234s, episode steps:  17, steps per second:  73, episode reward: 43.252, mean reward:  2.544 [-2.322, 32.200], mean action: 3.647 [0.000, 19.000],  loss: 0.021018, mae: 0.396729, mean_q: 0.434199, mean_eps: 0.000000
 4471/5000: episode: 194, duration: 0.348s, episode steps:  27, steps per second:  78, episode reward: -35.720, mean reward: -1.323 [-31.985,  2.167], mean action: 8.074 [0.000, 19.000],  loss: 0.022979, mae: 0.411057, mean_q: 0.432397, mean_eps: 0.000000
 4491/5000: episode: 195, duration: 0.272s, episode steps:  20, steps per second:  74, episode reward: 35.375, mean reward:  1.769 [-3.000, 32.111], mean action: 3.700 [0.000, 16.000],  loss: 0.014322, mae: 0.380258, mean_q: 0.432933, mean_eps: 0.000000
 4528/5000: episode: 196, duration: 0.463s, episode steps:  37, steps per second:  80, episode reward: 38.680, mean reward:  1.045 [-2.495, 32.190], mean action: 5.730 [0.000, 20.000],  loss: 0.019246, mae: 0.395778, mean_q: 0.461691, mean_eps: 0.000000
 4549/5000: episode: 197, duration: 0.272s, episode steps:  21, steps per second:  77, episode reward: 32.102, mean reward:  1.529 [-3.000, 32.245], mean action: 6.095 [0.000, 16.000],  loss: 0.016856, mae: 0.378676, mean_q: 0.527278, mean_eps: 0.000000
 4568/5000: episode: 198, duration: 0.241s, episode steps:  19, steps per second:  79, episode reward: -38.280, mean reward: -2.015 [-31.525,  2.061], mean action: 5.474 [0.000, 19.000],  loss: 0.017939, mae: 0.385472, mean_q: 0.515808, mean_eps: 0.000000
 4584/5000: episode: 199, duration: 0.211s, episode steps:  16, steps per second:  76, episode reward: 38.507, mean reward:  2.407 [-2.298, 32.417], mean action: 3.938 [0.000, 16.000],  loss: 0.015609, mae: 0.376875, mean_q: 0.510580, mean_eps: 0.000000
 4611/5000: episode: 200, duration: 0.344s, episode steps:  27, steps per second:  79, episode reward: -32.190, mean reward: -1.192 [-31.501,  2.413], mean action: 4.667 [0.000, 14.000],  loss: 0.022337, mae: 0.405401, mean_q: 0.478735, mean_eps: 0.000000
 4637/5000: episode: 201, duration: 0.345s, episode steps:  26, steps per second:  75, episode reward: 35.217, mean reward:  1.355 [-3.000, 32.190], mean action: 6.654 [0.000, 19.000],  loss: 0.019877, mae: 0.393488, mean_q: 0.487896, mean_eps: 0.000000
 4655/5000: episode: 202, duration: 0.233s, episode steps:  18, steps per second:  77, episode reward: 33.000, mean reward:  1.833 [-2.785, 33.000], mean action: 2.889 [0.000, 9.000],  loss: 0.023221, mae: 0.409998, mean_q: 0.501663, mean_eps: 0.000000
 4681/5000: episode: 203, duration: 0.638s, episode steps:  26, steps per second:  41, episode reward: 32.676, mean reward:  1.257 [-2.349, 32.320], mean action: 5.731 [0.000, 21.000],  loss: 0.019492, mae: 0.389619, mean_q: 0.450872, mean_eps: 0.000000
 4707/5000: episode: 204, duration: 0.897s, episode steps:  26, steps per second:  29, episode reward: 36.000, mean reward:  1.385 [-2.308, 32.110], mean action: 3.000 [0.000, 14.000],  loss: 0.021933, mae: 0.407273, mean_q: 0.452634, mean_eps: 0.000000
 4730/5000: episode: 205, duration: 0.355s, episode steps:  23, steps per second:  65, episode reward: 34.841, mean reward:  1.515 [-3.000, 31.566], mean action: 3.522 [0.000, 15.000],  loss: 0.023906, mae: 0.423466, mean_q: 0.446868, mean_eps: 0.000000
 4758/5000: episode: 206, duration: 0.899s, episode steps:  28, steps per second:  31, episode reward: 32.380, mean reward:  1.156 [-2.862, 31.935], mean action: 4.714 [0.000, 19.000],  loss: 0.021031, mae: 0.411205, mean_q: 0.441863, mean_eps: 0.000000
 4778/5000: episode: 207, duration: 0.337s, episode steps:  20, steps per second:  59, episode reward: -39.000, mean reward: -1.950 [-33.000,  2.804], mean action: 3.850 [0.000, 12.000],  loss: 0.017954, mae: 0.393965, mean_q: 0.390348, mean_eps: 0.000000
 4798/5000: episode: 208, duration: 0.508s, episode steps:  20, steps per second:  39, episode reward: 38.904, mean reward:  1.945 [-3.000, 32.302], mean action: 5.200 [1.000, 14.000],  loss: 0.019944, mae: 0.400617, mean_q: 0.410313, mean_eps: 0.000000
 4823/5000: episode: 209, duration: 0.470s, episode steps:  25, steps per second:  53, episode reward: -32.280, mean reward: -1.291 [-31.628,  2.401], mean action: 4.920 [0.000, 20.000],  loss: 0.025728, mae: 0.431843, mean_q: 0.426255, mean_eps: 0.000000
 4845/5000: episode: 210, duration: 0.675s, episode steps:  22, steps per second:  33, episode reward: 36.000, mean reward:  1.636 [-2.408, 32.860], mean action: 5.182 [0.000, 19.000],  loss: 0.017663, mae: 0.382923, mean_q: 0.490758, mean_eps: 0.000000
 4871/5000: episode: 211, duration: 0.638s, episode steps:  26, steps per second:  41, episode reward: 36.000, mean reward:  1.385 [-3.000, 32.100], mean action: 4.346 [0.000, 20.000],  loss: 0.018404, mae: 0.386037, mean_q: 0.481080, mean_eps: 0.000000
 4896/5000: episode: 212, duration: 0.660s, episode steps:  25, steps per second:  38, episode reward: -35.290, mean reward: -1.412 [-32.180,  2.060], mean action: 6.400 [0.000, 15.000],  loss: 0.023037, mae: 0.394354, mean_q: 0.554192, mean_eps: 0.000000
 4911/5000: episode: 213, duration: 0.432s, episode steps:  15, steps per second:  35, episode reward: 42.000, mean reward:  2.800 [-2.338, 32.200], mean action: 3.933 [2.000, 12.000],  loss: 0.019753, mae: 0.376197, mean_q: 0.564692, mean_eps: 0.000000
 4936/5000: episode: 214, duration: 0.485s, episode steps:  25, steps per second:  52, episode reward: -32.250, mean reward: -1.290 [-31.889,  2.711], mean action: 4.520 [0.000, 15.000],  loss: 0.021969, mae: 0.395464, mean_q: 0.519793, mean_eps: 0.000000
 4973/5000: episode: 215, duration: 0.607s, episode steps:  37, steps per second:  61, episode reward: -32.150, mean reward: -0.869 [-31.840,  2.965], mean action: 10.730 [0.000, 19.000],  loss: 0.020095, mae: 0.390881, mean_q: 0.482446, mean_eps: 0.000000
 4998/5000: episode: 216, duration: 0.362s, episode steps:  25, steps per second:  69, episode reward: 32.584, mean reward:  1.303 [-3.000, 33.000], mean action: 5.760 [0.000, 14.000],  loss: 0.021960, mae: 0.401633, mean_q: 0.464735, mean_eps: 0.000000
done, took 73.472 seconds
DQN Evaluation: 3766 victories out of 4490 episodes
Training for 5000 steps ...
   24/5000: episode: 1, duration: 0.220s, episode steps:  24, steps per second: 109, episode reward: 44.216, mean reward:  1.842 [-2.792, 32.372], mean action: 3.125 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   60/5000: episode: 2, duration: 0.292s, episode steps:  36, steps per second: 123, episode reward: 41.870, mean reward:  1.163 [-2.476, 32.020], mean action: 4.278 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   94/5000: episode: 3, duration: 0.289s, episode steps:  34, steps per second: 117, episode reward: 47.287, mean reward:  1.391 [-0.161, 32.050], mean action: 3.176 [1.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  149/5000: episode: 4, duration: 0.330s, episode steps:  55, steps per second: 167, episode reward: -32.620, mean reward: -0.593 [-31.856,  3.000], mean action: 10.055 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  165/5000: episode: 5, duration: 0.123s, episode steps:  16, steps per second: 130, episode reward: 44.202, mean reward:  2.763 [-3.000, 32.680], mean action: 1.812 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  191/5000: episode: 6, duration: 0.166s, episode steps:  26, steps per second: 156, episode reward: 39.000, mean reward:  1.500 [-2.762, 32.050], mean action: 4.077 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  211/5000: episode: 7, duration: 0.140s, episode steps:  20, steps per second: 143, episode reward: 42.000, mean reward:  2.100 [-2.281, 32.140], mean action: 3.600 [1.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  233/5000: episode: 8, duration: 0.145s, episode steps:  22, steps per second: 151, episode reward: 38.594, mean reward:  1.754 [-3.000, 32.254], mean action: 2.545 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  265/5000: episode: 9, duration: 0.204s, episode steps:  32, steps per second: 157, episode reward: 41.115, mean reward:  1.285 [-2.813, 32.340], mean action: 4.312 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  302/5000: episode: 10, duration: 0.226s, episode steps:  37, steps per second: 164, episode reward: 38.442, mean reward:  1.039 [-2.763, 32.430], mean action: 3.865 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  329/5000: episode: 11, duration: 0.177s, episode steps:  27, steps per second: 153, episode reward: 41.655, mean reward:  1.543 [-2.235, 32.233], mean action: 3.074 [1.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  353/5000: episode: 12, duration: 0.152s, episode steps:  24, steps per second: 158, episode reward: 38.796, mean reward:  1.616 [-2.338, 32.620], mean action: 3.375 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  392/5000: episode: 13, duration: 0.239s, episode steps:  39, steps per second: 163, episode reward: 38.432, mean reward:  0.985 [-2.496, 32.060], mean action: 2.872 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  437/5000: episode: 14, duration: 0.267s, episode steps:  45, steps per second: 168, episode reward: 32.774, mean reward:  0.728 [-3.000, 32.290], mean action: 7.400 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  470/5000: episode: 15, duration: 0.211s, episode steps:  33, steps per second: 156, episode reward: 38.814, mean reward:  1.176 [-3.000, 32.600], mean action: 2.576 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  511/5000: episode: 16, duration: 0.252s, episode steps:  41, steps per second: 163, episode reward: 36.949, mean reward:  0.901 [-2.520, 32.170], mean action: 5.122 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  555/5000: episode: 17, duration: 0.262s, episode steps:  44, steps per second: 168, episode reward: 35.647, mean reward:  0.810 [-2.314, 32.280], mean action: 5.818 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  577/5000: episode: 18, duration: 0.140s, episode steps:  22, steps per second: 157, episode reward: 38.719, mean reward:  1.760 [-2.938, 32.309], mean action: 3.682 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  594/5000: episode: 19, duration: 0.120s, episode steps:  17, steps per second: 142, episode reward: 47.307, mean reward:  2.783 [-0.133, 32.010], mean action: 3.471 [1.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  641/5000: episode: 20, duration: 0.283s, episode steps:  47, steps per second: 166, episode reward: -32.420, mean reward: -0.690 [-31.876,  2.449], mean action: 9.319 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  671/5000: episode: 21, duration: 0.230s, episode steps:  30, steps per second: 130, episode reward: 38.894, mean reward:  1.296 [-3.000, 32.190], mean action: 3.600 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  698/5000: episode: 22, duration: 0.398s, episode steps:  27, steps per second:  68, episode reward: 32.457, mean reward:  1.202 [-3.000, 31.909], mean action: 7.630 [1.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  717/5000: episode: 23, duration: 0.128s, episode steps:  19, steps per second: 149, episode reward: 41.065, mean reward:  2.161 [-2.600, 31.461], mean action: 2.579 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  743/5000: episode: 24, duration: 0.169s, episode steps:  26, steps per second: 154, episode reward: 41.606, mean reward:  1.600 [-2.424, 32.170], mean action: 2.846 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  769/5000: episode: 25, duration: 0.428s, episode steps:  26, steps per second:  61, episode reward: 37.664, mean reward:  1.449 [-2.338, 32.340], mean action: 4.769 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  813/5000: episode: 26, duration: 0.284s, episode steps:  44, steps per second: 155, episode reward: 38.804, mean reward:  0.882 [-2.493, 32.061], mean action: 3.659 [1.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  835/5000: episode: 27, duration: 0.144s, episode steps:  22, steps per second: 153, episode reward: 42.000, mean reward:  1.909 [-2.552, 29.627], mean action: 2.636 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  889/5000: episode: 28, duration: 0.338s, episode steps:  54, steps per second: 160, episode reward: -32.550, mean reward: -0.603 [-32.467,  2.491], mean action: 5.981 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  916/5000: episode: 29, duration: 0.178s, episode steps:  27, steps per second: 152, episode reward: 44.105, mean reward:  1.634 [-3.000, 32.370], mean action: 2.704 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  958/5000: episode: 30, duration: 0.254s, episode steps:  42, steps per second: 165, episode reward: -34.510, mean reward: -0.822 [-32.816,  2.470], mean action: 2.810 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  990/5000: episode: 31, duration: 0.204s, episode steps:  32, steps per second: 157, episode reward: 41.145, mean reward:  1.286 [-2.412, 32.902], mean action: 4.688 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1014/5000: episode: 32, duration: 0.249s, episode steps:  24, steps per second:  96, episode reward: 35.359, mean reward:  1.473 [-2.744, 32.170], mean action: 3.375 [0.000, 16.000],  loss: 0.022418, mae: 0.396053, mean_q: 0.530665, mean_eps: 0.000000
 1033/5000: episode: 33, duration: 0.267s, episode steps:  19, steps per second:  71, episode reward: 41.967, mean reward:  2.209 [-2.112, 32.320], mean action: 3.053 [0.000, 12.000],  loss: 0.019540, mae: 0.387097, mean_q: 0.490405, mean_eps: 0.000000
 1052/5000: episode: 34, duration: 0.260s, episode steps:  19, steps per second:  73, episode reward: 41.067, mean reward:  2.161 [-2.388, 32.001], mean action: 3.053 [0.000, 14.000],  loss: 0.020644, mae: 0.388733, mean_q: 0.525077, mean_eps: 0.000000
 1093/5000: episode: 35, duration: 0.507s, episode steps:  41, steps per second:  81, episode reward: 43.542, mean reward:  1.062 [-2.094, 32.901], mean action: 4.854 [0.000, 20.000],  loss: 0.017872, mae: 0.384022, mean_q: 0.498960, mean_eps: 0.000000
 1128/5000: episode: 36, duration: 0.428s, episode steps:  35, steps per second:  82, episode reward: -32.190, mean reward: -0.920 [-32.310,  2.670], mean action: 5.571 [0.000, 20.000],  loss: 0.021170, mae: 0.395524, mean_q: 0.495764, mean_eps: 0.000000
 1154/5000: episode: 37, duration: 0.334s, episode steps:  26, steps per second:  78, episode reward: 44.556, mean reward:  1.714 [-2.063, 31.866], mean action: 3.769 [0.000, 15.000],  loss: 0.020764, mae: 0.393470, mean_q: 0.553643, mean_eps: 0.000000
 1195/5000: episode: 38, duration: 0.498s, episode steps:  41, steps per second:  82, episode reward: 33.000, mean reward:  0.805 [-3.000, 32.300], mean action: 5.585 [0.000, 19.000],  loss: 0.019747, mae: 0.391490, mean_q: 0.581222, mean_eps: 0.000000
 1212/5000: episode: 39, duration: 0.223s, episode steps:  17, steps per second:  76, episode reward: 47.313, mean reward:  2.783 [-0.155, 32.190], mean action: 3.765 [0.000, 13.000],  loss: 0.017578, mae: 0.381584, mean_q: 0.599442, mean_eps: 0.000000
 1247/5000: episode: 40, duration: 0.454s, episode steps:  35, steps per second:  77, episode reward: 38.470, mean reward:  1.099 [-3.000, 32.230], mean action: 3.086 [0.000, 19.000],  loss: 0.017728, mae: 0.377402, mean_q: 0.531564, mean_eps: 0.000000
 1286/5000: episode: 41, duration: 0.473s, episode steps:  39, steps per second:  82, episode reward: 32.790, mean reward:  0.841 [-3.000, 32.070], mean action: 7.641 [0.000, 19.000],  loss: 0.023785, mae: 0.403978, mean_q: 0.514077, mean_eps: 0.000000
 1306/5000: episode: 42, duration: 0.258s, episode steps:  20, steps per second:  77, episode reward: 41.787, mean reward:  2.089 [-2.922, 32.210], mean action: 4.750 [1.000, 14.000],  loss: 0.019437, mae: 0.389562, mean_q: 0.519792, mean_eps: 0.000000
 1323/5000: episode: 43, duration: 0.223s, episode steps:  17, steps per second:  76, episode reward: 44.151, mean reward:  2.597 [-0.317, 29.934], mean action: 2.882 [0.000, 14.000],  loss: 0.025676, mae: 0.427461, mean_q: 0.493272, mean_eps: 0.000000
 1335/5000: episode: 44, duration: 0.157s, episode steps:  12, steps per second:  77, episode reward: 44.352, mean reward:  3.696 [-2.164, 31.572], mean action: 1.583 [0.000, 9.000],  loss: 0.019816, mae: 0.389993, mean_q: 0.482280, mean_eps: 0.000000
 1369/5000: episode: 45, duration: 0.442s, episode steps:  34, steps per second:  77, episode reward: 42.000, mean reward:  1.235 [-2.171, 32.370], mean action: 2.324 [0.000, 14.000],  loss: 0.020338, mae: 0.396835, mean_q: 0.510995, mean_eps: 0.000000
 1403/5000: episode: 46, duration: 0.423s, episode steps:  34, steps per second:  80, episode reward: 44.087, mean reward:  1.297 [-2.004, 32.241], mean action: 5.912 [0.000, 19.000],  loss: 0.022754, mae: 0.410328, mean_q: 0.519963, mean_eps: 0.000000
 1429/5000: episode: 47, duration: 0.369s, episode steps:  26, steps per second:  70, episode reward: 37.522, mean reward:  1.443 [-3.000, 32.610], mean action: 7.115 [0.000, 15.000],  loss: 0.023193, mae: 0.414428, mean_q: 0.515712, mean_eps: 0.000000
 1447/5000: episode: 48, duration: 0.373s, episode steps:  18, steps per second:  48, episode reward: 38.561, mean reward:  2.142 [-3.000, 32.157], mean action: 4.667 [1.000, 14.000],  loss: 0.020814, mae: 0.397390, mean_q: 0.497782, mean_eps: 0.000000
 1482/5000: episode: 49, duration: 0.459s, episode steps:  35, steps per second:  76, episode reward: 35.030, mean reward:  1.001 [-3.000, 31.276], mean action: 4.171 [0.000, 15.000],  loss: 0.019501, mae: 0.382795, mean_q: 0.486714, mean_eps: 0.000000
 1512/5000: episode: 50, duration: 0.387s, episode steps:  30, steps per second:  78, episode reward: 38.374, mean reward:  1.279 [-2.428, 29.150], mean action: 3.700 [0.000, 20.000],  loss: 0.023189, mae: 0.400338, mean_q: 0.451224, mean_eps: 0.000000
 1536/5000: episode: 51, duration: 0.317s, episode steps:  24, steps per second:  76, episode reward: 35.605, mean reward:  1.484 [-2.504, 32.020], mean action: 4.792 [0.000, 15.000],  loss: 0.018047, mae: 0.383806, mean_q: 0.484691, mean_eps: 0.000000
 1565/5000: episode: 52, duration: 0.401s, episode steps:  29, steps per second:  72, episode reward: 43.789, mean reward:  1.510 [-2.625, 32.130], mean action: 3.448 [0.000, 19.000],  loss: 0.022072, mae: 0.398184, mean_q: 0.491290, mean_eps: 0.000000
 1577/5000: episode: 53, duration: 0.164s, episode steps:  12, steps per second:  73, episode reward: 47.494, mean reward:  3.958 [ 0.000, 33.000], mean action: 0.250 [0.000, 3.000],  loss: 0.023298, mae: 0.404980, mean_q: 0.451363, mean_eps: 0.000000
 1604/5000: episode: 54, duration: 0.385s, episode steps:  27, steps per second:  70, episode reward: 35.903, mean reward:  1.330 [-2.464, 32.653], mean action: 3.963 [1.000, 19.000],  loss: 0.019088, mae: 0.390151, mean_q: 0.480329, mean_eps: 0.000000
 1625/5000: episode: 55, duration: 0.279s, episode steps:  21, steps per second:  75, episode reward: 38.353, mean reward:  1.826 [-3.000, 31.963], mean action: 4.905 [0.000, 19.000],  loss: 0.021753, mae: 0.409573, mean_q: 0.433919, mean_eps: 0.000000
 1647/5000: episode: 56, duration: 0.331s, episode steps:  22, steps per second:  66, episode reward: 41.196, mean reward:  1.873 [-2.268, 31.788], mean action: 5.227 [1.000, 19.000],  loss: 0.023095, mae: 0.418760, mean_q: 0.460087, mean_eps: 0.000000
 1682/5000: episode: 57, duration: 0.696s, episode steps:  35, steps per second:  50, episode reward: 34.886, mean reward:  0.997 [-3.000, 32.420], mean action: 6.257 [0.000, 20.000],  loss: 0.018443, mae: 0.403129, mean_q: 0.456344, mean_eps: 0.000000
 1733/5000: episode: 58, duration: 0.979s, episode steps:  51, steps per second:  52, episode reward: 32.836, mean reward:  0.644 [-3.000, 31.901], mean action: 8.961 [0.000, 19.000],  loss: 0.020802, mae: 0.416332, mean_q: 0.443065, mean_eps: 0.000000
 1752/5000: episode: 59, duration: 0.390s, episode steps:  19, steps per second:  49, episode reward: 44.506, mean reward:  2.342 [-3.000, 32.270], mean action: 2.947 [0.000, 19.000],  loss: 0.023530, mae: 0.424054, mean_q: 0.481714, mean_eps: 0.000000
 1784/5000: episode: 60, duration: 0.617s, episode steps:  32, steps per second:  52, episode reward: 40.671, mean reward:  1.271 [-2.539, 32.000], mean action: 3.562 [0.000, 14.000],  loss: 0.016741, mae: 0.401978, mean_q: 0.447421, mean_eps: 0.000000
 1804/5000: episode: 61, duration: 0.343s, episode steps:  20, steps per second:  58, episode reward: 47.572, mean reward:  2.379 [-0.094, 32.050], mean action: 2.150 [0.000, 12.000],  loss: 0.023038, mae: 0.433026, mean_q: 0.413708, mean_eps: 0.000000
 1832/5000: episode: 62, duration: 0.841s, episode steps:  28, steps per second:  33, episode reward: 41.558, mean reward:  1.484 [-2.915, 31.961], mean action: 4.464 [0.000, 16.000],  loss: 0.019825, mae: 0.417899, mean_q: 0.427129, mean_eps: 0.000000
 1870/5000: episode: 63, duration: 0.813s, episode steps:  38, steps per second:  47, episode reward: 44.330, mean reward:  1.167 [-2.237, 32.210], mean action: 3.421 [0.000, 20.000],  loss: 0.018690, mae: 0.411254, mean_q: 0.429794, mean_eps: 0.000000
 1897/5000: episode: 64, duration: 0.564s, episode steps:  27, steps per second:  48, episode reward: 38.505, mean reward:  1.426 [-2.500, 31.645], mean action: 4.741 [0.000, 20.000],  loss: 0.024049, mae: 0.426527, mean_q: 0.477968, mean_eps: 0.000000
 1912/5000: episode: 65, duration: 0.202s, episode steps:  15, steps per second:  74, episode reward: 41.673, mean reward:  2.778 [-2.813, 31.913], mean action: 4.067 [1.000, 16.000],  loss: 0.023796, mae: 0.418272, mean_q: 0.467695, mean_eps: 0.000000
 1927/5000: episode: 66, duration: 0.213s, episode steps:  15, steps per second:  71, episode reward: 47.510, mean reward:  3.167 [-0.444, 32.230], mean action: 1.333 [1.000, 2.000],  loss: 0.021482, mae: 0.414460, mean_q: 0.501279, mean_eps: 0.000000
 1942/5000: episode: 67, duration: 0.207s, episode steps:  15, steps per second:  72, episode reward: 41.651, mean reward:  2.777 [-2.708, 32.340], mean action: 3.533 [0.000, 21.000],  loss: 0.017112, mae: 0.382592, mean_q: 0.551586, mean_eps: 0.000000
 1964/5000: episode: 68, duration: 0.306s, episode steps:  22, steps per second:  72, episode reward: 41.285, mean reward:  1.877 [-3.000, 32.205], mean action: 3.773 [0.000, 16.000],  loss: 0.019226, mae: 0.391523, mean_q: 0.518919, mean_eps: 0.000000
 1979/5000: episode: 69, duration: 0.199s, episode steps:  15, steps per second:  76, episode reward: 47.289, mean reward:  3.153 [-0.320, 32.660], mean action: 2.467 [0.000, 13.000],  loss: 0.019611, mae: 0.402121, mean_q: 0.462787, mean_eps: 0.000000
 1991/5000: episode: 70, duration: 0.164s, episode steps:  12, steps per second:  73, episode reward: 44.463, mean reward:  3.705 [-2.511, 32.373], mean action: 3.083 [0.000, 14.000],  loss: 0.024244, mae: 0.428160, mean_q: 0.494723, mean_eps: 0.000000
 2019/5000: episode: 71, duration: 0.485s, episode steps:  28, steps per second:  58, episode reward: 38.158, mean reward:  1.363 [-2.533, 31.589], mean action: 1.893 [0.000, 11.000],  loss: 0.020653, mae: 0.410219, mean_q: 0.474081, mean_eps: 0.000000
 2037/5000: episode: 72, duration: 0.282s, episode steps:  18, steps per second:  64, episode reward: 42.000, mean reward:  2.333 [-2.488, 32.610], mean action: 3.333 [0.000, 12.000],  loss: 0.017049, mae: 0.390957, mean_q: 0.520372, mean_eps: 0.000000
 2057/5000: episode: 73, duration: 0.266s, episode steps:  20, steps per second:  75, episode reward: 44.312, mean reward:  2.216 [-2.317, 32.340], mean action: 2.600 [0.000, 14.000],  loss: 0.019974, mae: 0.403100, mean_q: 0.462663, mean_eps: 0.000000
 2075/5000: episode: 74, duration: 0.246s, episode steps:  18, steps per second:  73, episode reward: 44.100, mean reward:  2.450 [-2.223, 32.190], mean action: 3.389 [1.000, 16.000],  loss: 0.019015, mae: 0.400922, mean_q: 0.471106, mean_eps: 0.000000
 2098/5000: episode: 75, duration: 0.289s, episode steps:  23, steps per second:  80, episode reward: 38.681, mean reward:  1.682 [-2.586, 31.941], mean action: 2.913 [0.000, 15.000],  loss: 0.018805, mae: 0.408233, mean_q: 0.441565, mean_eps: 0.000000
 2121/5000: episode: 76, duration: 0.284s, episode steps:  23, steps per second:  81, episode reward: 41.903, mean reward:  1.822 [-2.222, 32.033], mean action: 3.174 [0.000, 16.000],  loss: 0.024748, mae: 0.441574, mean_q: 0.447630, mean_eps: 0.000000
 2146/5000: episode: 77, duration: 0.321s, episode steps:  25, steps per second:  78, episode reward: 43.907, mean reward:  1.756 [-2.239, 32.520], mean action: 3.640 [0.000, 16.000],  loss: 0.016953, mae: 0.402904, mean_q: 0.459674, mean_eps: 0.000000
 2183/5000: episode: 78, duration: 0.457s, episode steps:  37, steps per second:  81, episode reward: 41.150, mean reward:  1.112 [-2.255, 32.050], mean action: 3.054 [0.000, 16.000],  loss: 0.019755, mae: 0.405627, mean_q: 0.460249, mean_eps: 0.000000
 2220/5000: episode: 79, duration: 0.528s, episode steps:  37, steps per second:  70, episode reward: 38.903, mean reward:  1.051 [-2.362, 32.133], mean action: 3.676 [0.000, 16.000],  loss: 0.019632, mae: 0.403558, mean_q: 0.455262, mean_eps: 0.000000
 2247/5000: episode: 80, duration: 0.344s, episode steps:  27, steps per second:  79, episode reward: 41.752, mean reward:  1.546 [-2.546, 32.771], mean action: 2.963 [0.000, 16.000],  loss: 0.022913, mae: 0.427699, mean_q: 0.465801, mean_eps: 0.000000
 2273/5000: episode: 81, duration: 0.329s, episode steps:  26, steps per second:  79, episode reward: 41.615, mean reward:  1.601 [-2.253, 32.030], mean action: 2.808 [0.000, 16.000],  loss: 0.019795, mae: 0.401308, mean_q: 0.448481, mean_eps: 0.000000
 2300/5000: episode: 82, duration: 0.343s, episode steps:  27, steps per second:  79, episode reward: 35.118, mean reward:  1.301 [-2.998, 32.110], mean action: 4.333 [0.000, 14.000],  loss: 0.019049, mae: 0.409592, mean_q: 0.407546, mean_eps: 0.000000
 2328/5000: episode: 83, duration: 0.352s, episode steps:  28, steps per second:  80, episode reward: 35.229, mean reward:  1.258 [-3.000, 31.425], mean action: 3.250 [0.000, 15.000],  loss: 0.020781, mae: 0.413341, mean_q: 0.448519, mean_eps: 0.000000
 2364/5000: episode: 84, duration: 0.492s, episode steps:  36, steps per second:  73, episode reward: 41.225, mean reward:  1.145 [-2.712, 32.353], mean action: 3.028 [0.000, 14.000],  loss: 0.023392, mae: 0.432074, mean_q: 0.435912, mean_eps: 0.000000
 2383/5000: episode: 85, duration: 0.243s, episode steps:  19, steps per second:  78, episode reward: 38.736, mean reward:  2.039 [-2.802, 32.106], mean action: 3.579 [1.000, 9.000],  loss: 0.018699, mae: 0.404480, mean_q: 0.485768, mean_eps: 0.000000
 2410/5000: episode: 86, duration: 0.341s, episode steps:  27, steps per second:  79, episode reward: 36.000, mean reward:  1.333 [-2.901, 32.190], mean action: 3.000 [0.000, 15.000],  loss: 0.021757, mae: 0.405819, mean_q: 0.488236, mean_eps: 0.000000
 2433/5000: episode: 87, duration: 0.324s, episode steps:  23, steps per second:  71, episode reward: 44.006, mean reward:  1.913 [-2.128, 32.400], mean action: 2.609 [0.000, 12.000],  loss: 0.018596, mae: 0.385227, mean_q: 0.491425, mean_eps: 0.000000
 2462/5000: episode: 88, duration: 0.771s, episode steps:  29, steps per second:  38, episode reward: 44.398, mean reward:  1.531 [-2.187, 32.200], mean action: 1.759 [0.000, 16.000],  loss: 0.018373, mae: 0.386474, mean_q: 0.506121, mean_eps: 0.000000
 2482/5000: episode: 89, duration: 0.288s, episode steps:  20, steps per second:  69, episode reward: 41.313, mean reward:  2.066 [-2.591, 31.871], mean action: 3.950 [1.000, 21.000],  loss: 0.021066, mae: 0.404110, mean_q: 0.481272, mean_eps: 0.000000
 2516/5000: episode: 90, duration: 0.417s, episode steps:  34, steps per second:  82, episode reward: 41.676, mean reward:  1.226 [-2.701, 32.180], mean action: 3.853 [0.000, 20.000],  loss: 0.021460, mae: 0.401251, mean_q: 0.480559, mean_eps: 0.000000
 2540/5000: episode: 91, duration: 0.318s, episode steps:  24, steps per second:  75, episode reward: 38.572, mean reward:  1.607 [-2.618, 32.220], mean action: 3.458 [0.000, 16.000],  loss: 0.019433, mae: 0.392470, mean_q: 0.466526, mean_eps: 0.000000
 2567/5000: episode: 92, duration: 0.421s, episode steps:  27, steps per second:  64, episode reward: 38.745, mean reward:  1.435 [-2.591, 32.003], mean action: 2.333 [0.000, 12.000],  loss: 0.021801, mae: 0.394054, mean_q: 0.481282, mean_eps: 0.000000
 2589/5000: episode: 93, duration: 0.296s, episode steps:  22, steps per second:  74, episode reward: 42.000, mean reward:  1.909 [-2.757, 29.751], mean action: 2.818 [0.000, 16.000],  loss: 0.020064, mae: 0.387459, mean_q: 0.470375, mean_eps: 0.000000
 2620/5000: episode: 94, duration: 0.383s, episode steps:  31, steps per second:  81, episode reward: -40.100, mean reward: -1.294 [-32.194,  2.028], mean action: 6.968 [0.000, 16.000],  loss: 0.017615, mae: 0.377182, mean_q: 0.506334, mean_eps: 0.000000
 2649/5000: episode: 95, duration: 0.460s, episode steps:  29, steps per second:  63, episode reward: 33.000, mean reward:  1.138 [-3.000, 32.630], mean action: 4.586 [0.000, 19.000],  loss: 0.019441, mae: 0.386470, mean_q: 0.497052, mean_eps: 0.000000
 2665/5000: episode: 96, duration: 0.212s, episode steps:  16, steps per second:  75, episode reward: 41.435, mean reward:  2.590 [-2.757, 32.200], mean action: 4.938 [0.000, 16.000],  loss: 0.017900, mae: 0.380980, mean_q: 0.474127, mean_eps: 0.000000
 2696/5000: episode: 97, duration: 0.438s, episode steps:  31, steps per second:  71, episode reward: 41.750, mean reward:  1.347 [-2.296, 32.700], mean action: 5.097 [0.000, 20.000],  loss: 0.021837, mae: 0.400661, mean_q: 0.477718, mean_eps: 0.000000
 2718/5000: episode: 98, duration: 0.297s, episode steps:  22, steps per second:  74, episode reward: 40.849, mean reward:  1.857 [-2.148, 32.450], mean action: 6.273 [1.000, 14.000],  loss: 0.022169, mae: 0.411526, mean_q: 0.479186, mean_eps: 0.000000
 2739/5000: episode: 99, duration: 0.263s, episode steps:  21, steps per second:  80, episode reward: 35.851, mean reward:  1.707 [-2.900, 32.681], mean action: 4.429 [0.000, 19.000],  loss: 0.021341, mae: 0.413839, mean_q: 0.449433, mean_eps: 0.000000
 2767/5000: episode: 100, duration: 0.408s, episode steps:  28, steps per second:  69, episode reward: 35.883, mean reward:  1.282 [-3.000, 32.235], mean action: 4.393 [0.000, 16.000],  loss: 0.023919, mae: 0.429723, mean_q: 0.469350, mean_eps: 0.000000
 2795/5000: episode: 101, duration: 0.369s, episode steps:  28, steps per second:  76, episode reward: 44.875, mean reward:  1.603 [-2.268, 32.690], mean action: 3.071 [0.000, 21.000],  loss: 0.018944, mae: 0.389029, mean_q: 0.498267, mean_eps: 0.000000
 2841/5000: episode: 102, duration: 0.597s, episode steps:  46, steps per second:  77, episode reward: 34.914, mean reward:  0.759 [-2.352, 31.767], mean action: 4.804 [0.000, 21.000],  loss: 0.020303, mae: 0.391751, mean_q: 0.448577, mean_eps: 0.000000
 2867/5000: episode: 103, duration: 0.330s, episode steps:  26, steps per second:  79, episode reward: 43.847, mean reward:  1.686 [-2.248, 31.856], mean action: 5.308 [1.000, 19.000],  loss: 0.023416, mae: 0.421825, mean_q: 0.417067, mean_eps: 0.000000
 2900/5000: episode: 104, duration: 0.411s, episode steps:  33, steps per second:  80, episode reward: 44.791, mean reward:  1.357 [-2.292, 32.180], mean action: 3.364 [1.000, 19.000],  loss: 0.020543, mae: 0.398854, mean_q: 0.388468, mean_eps: 0.000000
 2924/5000: episode: 105, duration: 0.303s, episode steps:  24, steps per second:  79, episode reward: 41.221, mean reward:  1.718 [-3.000, 32.470], mean action: 4.083 [0.000, 14.000],  loss: 0.019605, mae: 0.387748, mean_q: 0.392351, mean_eps: 0.000000
 2948/5000: episode: 106, duration: 0.304s, episode steps:  24, steps per second:  79, episode reward: 41.001, mean reward:  1.708 [-2.693, 32.040], mean action: 6.375 [0.000, 15.000],  loss: 0.022296, mae: 0.394153, mean_q: 0.410348, mean_eps: 0.000000
 2969/5000: episode: 107, duration: 0.381s, episode steps:  21, steps per second:  55, episode reward: 44.453, mean reward:  2.117 [-2.163, 32.280], mean action: 2.857 [0.000, 19.000],  loss: 0.022016, mae: 0.384280, mean_q: 0.454500, mean_eps: 0.000000
 3003/5000: episode: 108, duration: 0.470s, episode steps:  34, steps per second:  72, episode reward: 40.939, mean reward:  1.204 [-3.000, 31.645], mean action: 2.235 [0.000, 9.000],  loss: 0.020314, mae: 0.380850, mean_q: 0.439714, mean_eps: 0.000000
 3040/5000: episode: 109, duration: 0.460s, episode steps:  37, steps per second:  80, episode reward: 38.970, mean reward:  1.053 [-2.375, 32.070], mean action: 2.973 [0.000, 14.000],  loss: 0.019083, mae: 0.377423, mean_q: 0.458197, mean_eps: 0.000000
 3067/5000: episode: 110, duration: 0.340s, episode steps:  27, steps per second:  79, episode reward: 42.000, mean reward:  1.556 [-2.091, 32.170], mean action: 2.444 [0.000, 12.000],  loss: 0.018835, mae: 0.371173, mean_q: 0.491954, mean_eps: 0.000000
 3103/5000: episode: 111, duration: 0.527s, episode steps:  36, steps per second:  68, episode reward: 35.837, mean reward:  0.995 [-3.000, 32.181], mean action: 4.500 [0.000, 21.000],  loss: 0.021650, mae: 0.386743, mean_q: 0.427250, mean_eps: 0.000000
 3127/5000: episode: 112, duration: 0.376s, episode steps:  24, steps per second:  64, episode reward: 44.289, mean reward:  1.845 [-2.240, 32.180], mean action: 3.333 [0.000, 13.000],  loss: 0.019890, mae: 0.379948, mean_q: 0.409701, mean_eps: 0.000000
 3156/5000: episode: 113, duration: 0.399s, episode steps:  29, steps per second:  73, episode reward: 41.904, mean reward:  1.445 [-2.271, 32.014], mean action: 2.621 [0.000, 13.000],  loss: 0.016310, mae: 0.364533, mean_q: 0.441787, mean_eps: 0.000000
 3171/5000: episode: 114, duration: 0.279s, episode steps:  15, steps per second:  54, episode reward: 44.806, mean reward:  2.987 [-2.009, 32.143], mean action: 1.467 [0.000, 12.000],  loss: 0.018868, mae: 0.377091, mean_q: 0.448217, mean_eps: 0.000000
 3196/5000: episode: 115, duration: 0.313s, episode steps:  25, steps per second:  80, episode reward: 38.582, mean reward:  1.543 [-3.000, 32.240], mean action: 3.400 [0.000, 16.000],  loss: 0.017048, mae: 0.371306, mean_q: 0.486965, mean_eps: 0.000000
 3224/5000: episode: 116, duration: 0.348s, episode steps:  28, steps per second:  80, episode reward: 43.781, mean reward:  1.564 [-2.481, 32.150], mean action: 3.786 [0.000, 19.000],  loss: 0.017844, mae: 0.377469, mean_q: 0.490359, mean_eps: 0.000000
 3252/5000: episode: 117, duration: 0.358s, episode steps:  28, steps per second:  78, episode reward: 38.076, mean reward:  1.360 [-3.000, 32.200], mean action: 3.143 [0.000, 19.000],  loss: 0.022640, mae: 0.403502, mean_q: 0.465697, mean_eps: 0.000000
 3266/5000: episode: 118, duration: 0.207s, episode steps:  14, steps per second:  68, episode reward: 44.310, mean reward:  3.165 [-2.074, 32.310], mean action: 5.143 [0.000, 19.000],  loss: 0.021559, mae: 0.396065, mean_q: 0.435634, mean_eps: 0.000000
 3286/5000: episode: 119, duration: 0.262s, episode steps:  20, steps per second:  76, episode reward: 44.691, mean reward:  2.235 [-2.145, 32.334], mean action: 3.450 [0.000, 19.000],  loss: 0.023598, mae: 0.412357, mean_q: 0.428349, mean_eps: 0.000000
 3313/5000: episode: 120, duration: 0.338s, episode steps:  27, steps per second:  80, episode reward: 38.383, mean reward:  1.422 [-2.516, 31.655], mean action: 3.704 [0.000, 19.000],  loss: 0.018640, mae: 0.384325, mean_q: 0.392949, mean_eps: 0.000000
 3352/5000: episode: 121, duration: 0.484s, episode steps:  39, steps per second:  81, episode reward: 32.122, mean reward:  0.824 [-3.000, 31.823], mean action: 6.692 [0.000, 19.000],  loss: 0.023214, mae: 0.400439, mean_q: 0.446859, mean_eps: 0.000000
 3399/5000: episode: 122, duration: 0.574s, episode steps:  47, steps per second:  82, episode reward: 32.767, mean reward:  0.697 [-2.325, 30.004], mean action: 6.553 [0.000, 21.000],  loss: 0.020874, mae: 0.380180, mean_q: 0.470659, mean_eps: 0.000000
 3420/5000: episode: 123, duration: 0.273s, episode steps:  21, steps per second:  77, episode reward: 44.035, mean reward:  2.097 [-2.442, 32.110], mean action: 3.762 [1.000, 14.000],  loss: 0.018546, mae: 0.375166, mean_q: 0.463761, mean_eps: 0.000000
 3445/5000: episode: 124, duration: 0.358s, episode steps:  25, steps per second:  70, episode reward: 43.390, mean reward:  1.736 [-2.048, 31.931], mean action: 5.280 [0.000, 20.000],  loss: 0.018133, mae: 0.373210, mean_q: 0.451533, mean_eps: 0.000000
 3475/5000: episode: 125, duration: 0.370s, episode steps:  30, steps per second:  81, episode reward: 38.335, mean reward:  1.278 [-3.000, 32.335], mean action: 2.000 [0.000, 12.000],  loss: 0.019123, mae: 0.375690, mean_q: 0.451776, mean_eps: 0.000000
 3517/5000: episode: 126, duration: 0.605s, episode steps:  42, steps per second:  69, episode reward: 43.706, mean reward:  1.041 [-2.095, 32.240], mean action: 3.476 [0.000, 19.000],  loss: 0.017686, mae: 0.371685, mean_q: 0.436791, mean_eps: 0.000000
 3550/5000: episode: 127, duration: 0.415s, episode steps:  33, steps per second:  80, episode reward: 41.327, mean reward:  1.252 [-2.937, 32.570], mean action: 3.455 [1.000, 15.000],  loss: 0.021943, mae: 0.392187, mean_q: 0.497694, mean_eps: 0.000000
 3578/5000: episode: 128, duration: 0.353s, episode steps:  28, steps per second:  79, episode reward: 41.198, mean reward:  1.471 [-2.517, 32.240], mean action: 2.071 [0.000, 16.000],  loss: 0.021247, mae: 0.388191, mean_q: 0.530993, mean_eps: 0.000000
 3608/5000: episode: 129, duration: 0.377s, episode steps:  30, steps per second:  80, episode reward: 36.000, mean reward:  1.200 [-2.707, 33.000], mean action: 3.967 [0.000, 16.000],  loss: 0.021746, mae: 0.396291, mean_q: 0.442118, mean_eps: 0.000000
 3640/5000: episode: 130, duration: 0.401s, episode steps:  32, steps per second:  80, episode reward: 32.728, mean reward:  1.023 [-2.431, 32.200], mean action: 4.875 [0.000, 20.000],  loss: 0.020052, mae: 0.381460, mean_q: 0.477476, mean_eps: 0.000000
 3673/5000: episode: 131, duration: 0.417s, episode steps:  33, steps per second:  79, episode reward: 44.492, mean reward:  1.348 [-2.176, 32.317], mean action: 3.333 [0.000, 15.000],  loss: 0.019548, mae: 0.382474, mean_q: 0.448489, mean_eps: 0.000000
 3702/5000: episode: 132, duration: 0.368s, episode steps:  29, steps per second:  79, episode reward: 44.262, mean reward:  1.526 [-2.456, 31.802], mean action: 4.034 [0.000, 15.000],  loss: 0.019801, mae: 0.391898, mean_q: 0.408501, mean_eps: 0.000000
 3726/5000: episode: 133, duration: 0.318s, episode steps:  24, steps per second:  75, episode reward: 38.080, mean reward:  1.587 [-2.270, 32.120], mean action: 4.917 [0.000, 16.000],  loss: 0.020014, mae: 0.393404, mean_q: 0.479947, mean_eps: 0.000000
 3764/5000: episode: 134, duration: 0.480s, episode steps:  38, steps per second:  79, episode reward: 40.760, mean reward:  1.073 [-3.000, 31.590], mean action: 3.605 [0.000, 20.000],  loss: 0.020440, mae: 0.399196, mean_q: 0.442660, mean_eps: 0.000000
 3790/5000: episode: 135, duration: 0.324s, episode steps:  26, steps per second:  80, episode reward: 42.000, mean reward:  1.615 [-2.462, 32.120], mean action: 3.462 [0.000, 13.000],  loss: 0.019963, mae: 0.388821, mean_q: 0.517248, mean_eps: 0.000000
 3812/5000: episode: 136, duration: 0.283s, episode steps:  22, steps per second:  78, episode reward: 39.000, mean reward:  1.773 [-2.247, 32.030], mean action: 1.591 [0.000, 9.000],  loss: 0.020456, mae: 0.394348, mean_q: 0.508395, mean_eps: 0.000000
 3843/5000: episode: 137, duration: 0.480s, episode steps:  31, steps per second:  65, episode reward: 41.205, mean reward:  1.329 [-2.468, 31.525], mean action: 3.677 [0.000, 14.000],  loss: 0.020210, mae: 0.395070, mean_q: 0.475940, mean_eps: 0.000000
 3866/5000: episode: 138, duration: 0.338s, episode steps:  23, steps per second:  68, episode reward: 38.720, mean reward:  1.683 [-2.532, 32.550], mean action: 4.957 [0.000, 14.000],  loss: 0.022915, mae: 0.413471, mean_q: 0.451362, mean_eps: 0.000000
 3888/5000: episode: 139, duration: 0.379s, episode steps:  22, steps per second:  58, episode reward: 37.059, mean reward:  1.685 [-3.000, 32.040], mean action: 4.273 [0.000, 19.000],  loss: 0.018940, mae: 0.395375, mean_q: 0.444672, mean_eps: 0.000000
 3921/5000: episode: 140, duration: 0.427s, episode steps:  33, steps per second:  77, episode reward: 39.704, mean reward:  1.203 [-2.818, 32.033], mean action: 3.303 [0.000, 19.000],  loss: 0.019136, mae: 0.386981, mean_q: 0.502187, mean_eps: 0.000000
 3944/5000: episode: 141, duration: 0.308s, episode steps:  23, steps per second:  75, episode reward: 47.035, mean reward:  2.045 [-0.175, 32.220], mean action: 2.391 [0.000, 17.000],  loss: 0.018085, mae: 0.384278, mean_q: 0.498322, mean_eps: 0.000000
 3967/5000: episode: 142, duration: 0.296s, episode steps:  23, steps per second:  78, episode reward: 41.109, mean reward:  1.787 [-2.275, 32.190], mean action: 2.565 [0.000, 9.000],  loss: 0.019098, mae: 0.391468, mean_q: 0.481255, mean_eps: 0.000000
 3996/5000: episode: 143, duration: 0.508s, episode steps:  29, steps per second:  57, episode reward: 37.619, mean reward:  1.297 [-3.000, 32.410], mean action: 5.621 [0.000, 19.000],  loss: 0.016775, mae: 0.372612, mean_q: 0.498895, mean_eps: 0.000000
 4010/5000: episode: 144, duration: 0.202s, episode steps:  14, steps per second:  69, episode reward: 44.542, mean reward:  3.182 [-2.041, 33.000], mean action: 2.000 [0.000, 19.000],  loss: 0.018354, mae: 0.378582, mean_q: 0.501804, mean_eps: 0.000000
 4043/5000: episode: 145, duration: 0.410s, episode steps:  33, steps per second:  80, episode reward: 32.458, mean reward:  0.984 [-3.000, 32.201], mean action: 4.485 [0.000, 19.000],  loss: 0.017427, mae: 0.370441, mean_q: 0.514928, mean_eps: 0.000000
 4090/5000: episode: 146, duration: 0.577s, episode steps:  47, steps per second:  81, episode reward: 32.904, mean reward:  0.700 [-2.325, 32.527], mean action: 3.723 [0.000, 20.000],  loss: 0.017623, mae: 0.366539, mean_q: 0.489131, mean_eps: 0.000000
 4117/5000: episode: 147, duration: 0.343s, episode steps:  27, steps per second:  79, episode reward: 42.000, mean reward:  1.556 [-3.000, 32.370], mean action: 2.704 [0.000, 19.000],  loss: 0.018151, mae: 0.379237, mean_q: 0.446529, mean_eps: 0.000000
 4145/5000: episode: 148, duration: 0.357s, episode steps:  28, steps per second:  78, episode reward: 42.000, mean reward:  1.500 [-3.000, 32.340], mean action: 2.679 [0.000, 12.000],  loss: 0.018967, mae: 0.388072, mean_q: 0.438420, mean_eps: 0.000000
 4167/5000: episode: 149, duration: 0.292s, episode steps:  22, steps per second:  75, episode reward: 41.131, mean reward:  1.870 [-2.739, 32.201], mean action: 1.682 [0.000, 12.000],  loss: 0.020318, mae: 0.394923, mean_q: 0.444931, mean_eps: 0.000000
 4208/5000: episode: 150, duration: 0.517s, episode steps:  41, steps per second:  79, episode reward: 35.379, mean reward:  0.863 [-2.672, 32.090], mean action: 2.171 [0.000, 19.000],  loss: 0.016956, mae: 0.387976, mean_q: 0.423304, mean_eps: 0.000000
 4233/5000: episode: 151, duration: 0.697s, episode steps:  25, steps per second:  36, episode reward: 38.564, mean reward:  1.543 [-3.000, 32.030], mean action: 4.080 [1.000, 16.000],  loss: 0.021991, mae: 0.403624, mean_q: 0.414078, mean_eps: 0.000000
 4256/5000: episode: 152, duration: 0.302s, episode steps:  23, steps per second:  76, episode reward: 47.652, mean reward:  2.072 [ 0.000, 32.040], mean action: 1.391 [0.000, 3.000],  loss: 0.015979, mae: 0.360622, mean_q: 0.521577, mean_eps: 0.000000
 4276/5000: episode: 153, duration: 0.268s, episode steps:  20, steps per second:  75, episode reward: 44.631, mean reward:  2.232 [-2.059, 32.080], mean action: 2.400 [1.000, 19.000],  loss: 0.019904, mae: 0.383531, mean_q: 0.500459, mean_eps: 0.000000
 4300/5000: episode: 154, duration: 0.319s, episode steps:  24, steps per second:  75, episode reward: 40.742, mean reward:  1.698 [-2.567, 32.910], mean action: 7.833 [0.000, 19.000],  loss: 0.021681, mae: 0.385023, mean_q: 0.499916, mean_eps: 0.000000
 4326/5000: episode: 155, duration: 0.345s, episode steps:  26, steps per second:  75, episode reward: 41.347, mean reward:  1.590 [-2.456, 32.240], mean action: 4.769 [0.000, 19.000],  loss: 0.018742, mae: 0.369879, mean_q: 0.454887, mean_eps: 0.000000
 4348/5000: episode: 156, duration: 0.291s, episode steps:  22, steps per second:  76, episode reward: 44.088, mean reward:  2.004 [-2.067, 31.887], mean action: 1.864 [0.000, 19.000],  loss: 0.022185, mae: 0.397466, mean_q: 0.431529, mean_eps: 0.000000
 4375/5000: episode: 157, duration: 0.469s, episode steps:  27, steps per second:  58, episode reward: 38.318, mean reward:  1.419 [-2.303, 31.428], mean action: 3.296 [0.000, 19.000],  loss: 0.018640, mae: 0.379704, mean_q: 0.400914, mean_eps: 0.000000
 4400/5000: episode: 158, duration: 0.517s, episode steps:  25, steps per second:  48, episode reward: 41.467, mean reward:  1.659 [-2.539, 32.490], mean action: 1.800 [0.000, 16.000],  loss: 0.022588, mae: 0.390657, mean_q: 0.463813, mean_eps: 0.000000
 4423/5000: episode: 159, duration: 0.307s, episode steps:  23, steps per second:  75, episode reward: 38.445, mean reward:  1.672 [-2.528, 32.600], mean action: 5.739 [0.000, 20.000],  loss: 0.017991, mae: 0.365030, mean_q: 0.470875, mean_eps: 0.000000
 4463/5000: episode: 160, duration: 0.513s, episode steps:  40, steps per second:  78, episode reward: 37.499, mean reward:  0.937 [-2.353, 31.739], mean action: 3.600 [0.000, 16.000],  loss: 0.021408, mae: 0.375234, mean_q: 0.509849, mean_eps: 0.000000
 4485/5000: episode: 161, duration: 0.296s, episode steps:  22, steps per second:  74, episode reward: 45.000, mean reward:  2.045 [-2.061, 32.110], mean action: 0.773 [0.000, 9.000],  loss: 0.017032, mae: 0.360619, mean_q: 0.482898, mean_eps: 0.000000
 4510/5000: episode: 162, duration: 0.405s, episode steps:  25, steps per second:  62, episode reward: 37.740, mean reward:  1.510 [-3.000, 32.230], mean action: 3.200 [0.000, 16.000],  loss: 0.020658, mae: 0.383543, mean_q: 0.451297, mean_eps: 0.000000
 4533/5000: episode: 163, duration: 0.292s, episode steps:  23, steps per second:  79, episode reward: 41.943, mean reward:  1.824 [-2.074, 32.393], mean action: 5.000 [0.000, 16.000],  loss: 0.023003, mae: 0.397807, mean_q: 0.469651, mean_eps: 0.000000
 4548/5000: episode: 164, duration: 0.205s, episode steps:  15, steps per second:  73, episode reward: 44.325, mean reward:  2.955 [-2.091, 31.525], mean action: 4.133 [1.000, 14.000],  loss: 0.014686, mae: 0.359601, mean_q: 0.480350, mean_eps: 0.000000
 4583/5000: episode: 165, duration: 0.436s, episode steps:  35, steps per second:  80, episode reward: 35.193, mean reward:  1.006 [-3.000, 32.164], mean action: 3.457 [0.000, 14.000],  loss: 0.019378, mae: 0.380467, mean_q: 0.491064, mean_eps: 0.000000
 4601/5000: episode: 166, duration: 0.233s, episode steps:  18, steps per second:  77, episode reward: 38.555, mean reward:  2.142 [-2.801, 32.530], mean action: 3.833 [0.000, 14.000],  loss: 0.016547, mae: 0.361297, mean_q: 0.496003, mean_eps: 0.000000
 4629/5000: episode: 167, duration: 0.359s, episode steps:  28, steps per second:  78, episode reward: 43.854, mean reward:  1.566 [-2.322, 32.320], mean action: 3.321 [0.000, 14.000],  loss: 0.018757, mae: 0.377106, mean_q: 0.487926, mean_eps: 0.000000
 4661/5000: episode: 168, duration: 0.411s, episode steps:  32, steps per second:  78, episode reward: 43.357, mean reward:  1.355 [-2.246, 32.300], mean action: 5.875 [0.000, 20.000],  loss: 0.018611, mae: 0.379595, mean_q: 0.458440, mean_eps: 0.000000
 4684/5000: episode: 169, duration: 0.296s, episode steps:  23, steps per second:  78, episode reward: 44.662, mean reward:  1.942 [-2.544, 32.089], mean action: 3.391 [3.000, 9.000],  loss: 0.017959, mae: 0.373076, mean_q: 0.482803, mean_eps: 0.000000
 4724/5000: episode: 170, duration: 0.488s, episode steps:  40, steps per second:  82, episode reward: 32.221, mean reward:  0.806 [-2.938, 32.193], mean action: 5.175 [0.000, 18.000],  loss: 0.021349, mae: 0.387704, mean_q: 0.462306, mean_eps: 0.000000
 4752/5000: episode: 171, duration: 0.424s, episode steps:  28, steps per second:  66, episode reward: 40.771, mean reward:  1.456 [-2.077, 32.140], mean action: 5.214 [0.000, 14.000],  loss: 0.017840, mae: 0.372544, mean_q: 0.482912, mean_eps: 0.000000
 4777/5000: episode: 172, duration: 0.325s, episode steps:  25, steps per second:  77, episode reward: 44.200, mean reward:  1.768 [-2.157, 32.060], mean action: 3.840 [1.000, 14.000],  loss: 0.020647, mae: 0.387918, mean_q: 0.466298, mean_eps: 0.000000
 4825/5000: episode: 173, duration: 0.603s, episode steps:  48, steps per second:  80, episode reward: 38.943, mean reward:  0.811 [-2.687, 32.663], mean action: 1.938 [0.000, 15.000],  loss: 0.020250, mae: 0.386246, mean_q: 0.486971, mean_eps: 0.000000
 4844/5000: episode: 174, duration: 0.249s, episode steps:  19, steps per second:  76, episode reward: 41.880, mean reward:  2.204 [-2.305, 33.000], mean action: 2.105 [0.000, 12.000],  loss: 0.018571, mae: 0.388381, mean_q: 0.465413, mean_eps: 0.000000
 4868/5000: episode: 175, duration: 0.305s, episode steps:  24, steps per second:  79, episode reward: 39.000, mean reward:  1.625 [-2.695, 32.640], mean action: 5.667 [0.000, 14.000],  loss: 0.019715, mae: 0.384707, mean_q: 0.432816, mean_eps: 0.000000
 4904/5000: episode: 176, duration: 0.453s, episode steps:  36, steps per second:  80, episode reward: 32.886, mean reward:  0.914 [-3.000, 32.161], mean action: 6.222 [1.000, 20.000],  loss: 0.020430, mae: 0.384362, mean_q: 0.486020, mean_eps: 0.000000
 4923/5000: episode: 177, duration: 0.248s, episode steps:  19, steps per second:  77, episode reward: 39.000, mean reward:  2.053 [-2.574, 32.360], mean action: 3.684 [0.000, 12.000],  loss: 0.020467, mae: 0.384538, mean_q: 0.483443, mean_eps: 0.000000
 4952/5000: episode: 178, duration: 0.401s, episode steps:  29, steps per second:  72, episode reward: 41.518, mean reward:  1.432 [-2.578, 32.100], mean action: 5.034 [0.000, 15.000],  loss: 0.019638, mae: 0.375361, mean_q: 0.522690, mean_eps: 0.000000
 4975/5000: episode: 179, duration: 0.296s, episode steps:  23, steps per second:  78, episode reward: 46.436, mean reward:  2.019 [-0.391, 32.230], mean action: 1.391 [0.000, 8.000],  loss: 0.017289, mae: 0.366899, mean_q: 0.516753, mean_eps: 0.000000
done, took 63.496 seconds
DQN Evaluation: 3940 victories out of 4670 episodes
Training for 5000 steps ...
   23/5000: episode: 1, duration: 0.217s, episode steps:  23, steps per second: 106, episode reward: -41.590, mean reward: -1.808 [-33.000,  2.235], mean action: 7.043 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   48/5000: episode: 2, duration: 0.177s, episode steps:  25, steps per second: 141, episode reward: 40.648, mean reward:  1.626 [-2.136, 32.664], mean action: 4.480 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   72/5000: episode: 3, duration: 0.169s, episode steps:  24, steps per second: 142, episode reward: 37.846, mean reward:  1.577 [-2.546, 32.100], mean action: 5.375 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   97/5000: episode: 4, duration: 0.159s, episode steps:  25, steps per second: 157, episode reward: 40.383, mean reward:  1.615 [-2.493, 32.120], mean action: 4.720 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  125/5000: episode: 5, duration: 0.182s, episode steps:  28, steps per second: 154, episode reward: 32.125, mean reward:  1.147 [-3.000, 32.010], mean action: 4.893 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  145/5000: episode: 6, duration: 0.142s, episode steps:  20, steps per second: 141, episode reward: 35.431, mean reward:  1.772 [-2.903, 31.988], mean action: 8.100 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  167/5000: episode: 7, duration: 0.152s, episode steps:  22, steps per second: 145, episode reward: 38.012, mean reward:  1.728 [-3.000, 32.280], mean action: 4.182 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  196/5000: episode: 8, duration: 0.191s, episode steps:  29, steps per second: 152, episode reward: 35.114, mean reward:  1.211 [-2.302, 32.224], mean action: 4.379 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  218/5000: episode: 9, duration: 0.148s, episode steps:  22, steps per second: 149, episode reward: 32.344, mean reward:  1.470 [-2.709, 32.633], mean action: 6.909 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  232/5000: episode: 10, duration: 0.108s, episode steps:  14, steps per second: 129, episode reward: 38.051, mean reward:  2.718 [-2.430, 32.480], mean action: 6.429 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  250/5000: episode: 11, duration: 0.122s, episode steps:  18, steps per second: 148, episode reward: 38.663, mean reward:  2.148 [-2.434, 32.033], mean action: 4.444 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  289/5000: episode: 12, duration: 0.245s, episode steps:  39, steps per second: 159, episode reward: -32.250, mean reward: -0.827 [-31.821,  2.162], mean action: 6.154 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  316/5000: episode: 13, duration: 0.173s, episode steps:  27, steps per second: 156, episode reward: -33.000, mean reward: -1.222 [-32.283,  3.000], mean action: 10.037 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  330/5000: episode: 14, duration: 0.097s, episode steps:  14, steps per second: 145, episode reward: 38.647, mean reward:  2.761 [-3.000, 32.157], mean action: 3.857 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  360/5000: episode: 15, duration: 0.179s, episode steps:  30, steps per second: 167, episode reward: -32.530, mean reward: -1.084 [-32.086,  2.742], mean action: 5.233 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  381/5000: episode: 16, duration: 0.132s, episode steps:  21, steps per second: 159, episode reward: -33.000, mean reward: -1.571 [-30.490,  3.000], mean action: 5.952 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  396/5000: episode: 17, duration: 0.102s, episode steps:  15, steps per second: 148, episode reward: 38.317, mean reward:  2.554 [-2.484, 32.460], mean action: 3.600 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  422/5000: episode: 18, duration: 0.164s, episode steps:  26, steps per second: 158, episode reward: 35.382, mean reward:  1.361 [-2.479, 32.130], mean action: 4.577 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  459/5000: episode: 19, duration: 0.225s, episode steps:  37, steps per second: 165, episode reward: 42.000, mean reward:  1.135 [-2.622, 32.660], mean action: 6.459 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  476/5000: episode: 20, duration: 0.119s, episode steps:  17, steps per second: 143, episode reward: 41.610, mean reward:  2.448 [-2.383, 32.320], mean action: 3.353 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  498/5000: episode: 21, duration: 0.151s, episode steps:  22, steps per second: 146, episode reward: 35.903, mean reward:  1.632 [-2.404, 32.433], mean action: 3.000 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  532/5000: episode: 22, duration: 0.217s, episode steps:  34, steps per second: 157, episode reward: 37.503, mean reward:  1.103 [-3.000, 32.140], mean action: 6.706 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  546/5000: episode: 23, duration: 0.100s, episode steps:  14, steps per second: 139, episode reward: 44.498, mean reward:  3.178 [-2.440, 33.000], mean action: 1.286 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  572/5000: episode: 24, duration: 0.166s, episode steps:  26, steps per second: 157, episode reward: -32.440, mean reward: -1.248 [-31.781,  3.000], mean action: 6.038 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  595/5000: episode: 25, duration: 0.145s, episode steps:  23, steps per second: 158, episode reward: 34.503, mean reward:  1.500 [-2.515, 32.272], mean action: 5.348 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  624/5000: episode: 26, duration: 0.181s, episode steps:  29, steps per second: 160, episode reward: 32.832, mean reward:  1.132 [-2.822, 32.352], mean action: 7.483 [1.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  653/5000: episode: 27, duration: 0.183s, episode steps:  29, steps per second: 159, episode reward: 32.607, mean reward:  1.124 [-3.000, 32.220], mean action: 3.414 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  677/5000: episode: 28, duration: 0.152s, episode steps:  24, steps per second: 157, episode reward: 32.474, mean reward:  1.353 [-2.902, 32.080], mean action: 3.792 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  702/5000: episode: 29, duration: 0.158s, episode steps:  25, steps per second: 158, episode reward: 32.782, mean reward:  1.311 [-3.000, 31.982], mean action: 6.760 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  729/5000: episode: 30, duration: 0.175s, episode steps:  27, steps per second: 155, episode reward: 35.625, mean reward:  1.319 [-2.428, 32.420], mean action: 4.481 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  757/5000: episode: 31, duration: 0.181s, episode steps:  28, steps per second: 155, episode reward: 33.000, mean reward:  1.179 [-2.468, 30.241], mean action: 4.143 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  775/5000: episode: 32, duration: 0.127s, episode steps:  18, steps per second: 142, episode reward: 38.798, mean reward:  2.155 [-2.456, 32.080], mean action: 3.222 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  800/5000: episode: 33, duration: 0.163s, episode steps:  25, steps per second: 153, episode reward: 41.148, mean reward:  1.646 [-2.290, 32.524], mean action: 4.080 [1.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  822/5000: episode: 34, duration: 0.141s, episode steps:  22, steps per second: 156, episode reward: -32.810, mean reward: -1.491 [-32.307,  2.735], mean action: 6.136 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  841/5000: episode: 35, duration: 0.132s, episode steps:  19, steps per second: 144, episode reward: 36.000, mean reward:  1.895 [-2.938, 32.610], mean action: 3.316 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  876/5000: episode: 36, duration: 0.214s, episode steps:  35, steps per second: 163, episode reward: 33.000, mean reward:  0.943 [-2.481, 33.000], mean action: 7.800 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  895/5000: episode: 37, duration: 0.138s, episode steps:  19, steps per second: 138, episode reward: 38.546, mean reward:  2.029 [-2.591, 33.000], mean action: 3.474 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  921/5000: episode: 38, duration: 0.161s, episode steps:  26, steps per second: 161, episode reward: 32.914, mean reward:  1.266 [-2.471, 32.154], mean action: 3.962 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  945/5000: episode: 39, duration: 0.154s, episode steps:  24, steps per second: 156, episode reward: -35.020, mean reward: -1.459 [-32.023,  2.351], mean action: 4.583 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  971/5000: episode: 40, duration: 0.174s, episode steps:  26, steps per second: 150, episode reward: 35.600, mean reward:  1.369 [-2.713, 31.850], mean action: 3.577 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  994/5000: episode: 41, duration: 0.148s, episode steps:  23, steps per second: 156, episode reward: -35.090, mean reward: -1.526 [-32.243,  2.814], mean action: 5.478 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1022/5000: episode: 42, duration: 0.319s, episode steps:  28, steps per second:  88, episode reward: 35.214, mean reward:  1.258 [-2.745, 32.070], mean action: 5.143 [0.000, 12.000],  loss: 0.020022, mae: 0.382900, mean_q: 0.499568, mean_eps: 0.000000
 1046/5000: episode: 43, duration: 0.372s, episode steps:  24, steps per second:  64, episode reward: 37.974, mean reward:  1.582 [-3.000, 32.209], mean action: 5.042 [1.000, 17.000],  loss: 0.018101, mae: 0.371178, mean_q: 0.532790, mean_eps: 0.000000
 1067/5000: episode: 44, duration: 0.450s, episode steps:  21, steps per second:  47, episode reward: 35.424, mean reward:  1.687 [-2.439, 32.430], mean action: 2.952 [0.000, 15.000],  loss: 0.018177, mae: 0.381971, mean_q: 0.469814, mean_eps: 0.000000
 1096/5000: episode: 45, duration: 0.374s, episode steps:  29, steps per second:  78, episode reward: -32.440, mean reward: -1.119 [-32.410,  2.860], mean action: 4.483 [0.000, 12.000],  loss: 0.021706, mae: 0.409119, mean_q: 0.433559, mean_eps: 0.000000
 1119/5000: episode: 46, duration: 0.292s, episode steps:  23, steps per second:  79, episode reward: 32.578, mean reward:  1.416 [-2.877, 32.080], mean action: 5.174 [0.000, 19.000],  loss: 0.016777, mae: 0.390390, mean_q: 0.413593, mean_eps: 0.000000
 1134/5000: episode: 47, duration: 0.200s, episode steps:  15, steps per second:  75, episode reward: 44.328, mean reward:  2.955 [-2.019, 33.000], mean action: 3.800 [1.000, 13.000],  loss: 0.017192, mae: 0.377627, mean_q: 0.471698, mean_eps: 0.000000
 1151/5000: episode: 48, duration: 0.221s, episode steps:  17, steps per second:  77, episode reward: 41.343, mean reward:  2.432 [-2.093, 32.570], mean action: 2.647 [0.000, 11.000],  loss: 0.016796, mae: 0.373268, mean_q: 0.447673, mean_eps: 0.000000
 1173/5000: episode: 49, duration: 0.283s, episode steps:  22, steps per second:  78, episode reward: 32.903, mean reward:  1.496 [-2.498, 32.283], mean action: 5.500 [0.000, 15.000],  loss: 0.016237, mae: 0.363061, mean_q: 0.436508, mean_eps: 0.000000
 1192/5000: episode: 50, duration: 0.252s, episode steps:  19, steps per second:  75, episode reward: 35.564, mean reward:  1.872 [-3.000, 32.271], mean action: 5.000 [0.000, 14.000],  loss: 0.016916, mae: 0.362878, mean_q: 0.523269, mean_eps: 0.000000
 1214/5000: episode: 51, duration: 0.273s, episode steps:  22, steps per second:  81, episode reward: -35.830, mean reward: -1.629 [-32.302,  2.540], mean action: 8.318 [0.000, 20.000],  loss: 0.021664, mae: 0.386140, mean_q: 0.518166, mean_eps: 0.000000
 1236/5000: episode: 52, duration: 0.282s, episode steps:  22, steps per second:  78, episode reward: 35.808, mean reward:  1.628 [-3.000, 32.260], mean action: 6.273 [1.000, 19.000],  loss: 0.020376, mae: 0.381756, mean_q: 0.491263, mean_eps: 0.000000
 1255/5000: episode: 53, duration: 0.253s, episode steps:  19, steps per second:  75, episode reward: 34.974, mean reward:  1.841 [-2.582, 32.090], mean action: 5.474 [0.000, 19.000],  loss: 0.022411, mae: 0.394305, mean_q: 0.451181, mean_eps: 0.000000
 1267/5000: episode: 54, duration: 0.171s, episode steps:  12, steps per second:  70, episode reward: 42.000, mean reward:  3.500 [-2.153, 32.550], mean action: 4.833 [0.000, 19.000],  loss: 0.016498, mae: 0.364324, mean_q: 0.491117, mean_eps: 0.000000
 1320/5000: episode: 55, duration: 0.647s, episode steps:  53, steps per second:  82, episode reward: 35.670, mean reward:  0.673 [-3.000, 32.210], mean action: 4.340 [0.000, 19.000],  loss: 0.018783, mae: 0.373557, mean_q: 0.469793, mean_eps: 0.000000
 1335/5000: episode: 56, duration: 0.199s, episode steps:  15, steps per second:  75, episode reward: 41.937, mean reward:  2.796 [-2.467, 32.230], mean action: 3.800 [0.000, 14.000],  loss: 0.020618, mae: 0.392105, mean_q: 0.497150, mean_eps: 0.000000
 1355/5000: episode: 57, duration: 0.273s, episode steps:  20, steps per second:  73, episode reward: 38.585, mean reward:  1.929 [-2.664, 32.220], mean action: 7.350 [0.000, 21.000],  loss: 0.018081, mae: 0.376687, mean_q: 0.475877, mean_eps: 0.000000
 1398/5000: episode: 58, duration: 0.524s, episode steps:  43, steps per second:  82, episode reward: -32.130, mean reward: -0.747 [-32.056,  2.641], mean action: 4.302 [0.000, 18.000],  loss: 0.021207, mae: 0.396988, mean_q: 0.510050, mean_eps: 0.000000
 1425/5000: episode: 59, duration: 0.340s, episode steps:  27, steps per second:  79, episode reward: -32.730, mean reward: -1.212 [-32.634,  2.400], mean action: 4.185 [0.000, 18.000],  loss: 0.020828, mae: 0.391274, mean_q: 0.499349, mean_eps: 0.000000
 1440/5000: episode: 60, duration: 0.205s, episode steps:  15, steps per second:  73, episode reward: 41.173, mean reward:  2.745 [-2.369, 32.702], mean action: 7.600 [0.000, 21.000],  loss: 0.020770, mae: 0.386667, mean_q: 0.481258, mean_eps: 0.000000
 1458/5000: episode: 61, duration: 0.234s, episode steps:  18, steps per second:  77, episode reward: 38.265, mean reward:  2.126 [-2.668, 31.395], mean action: 3.444 [0.000, 19.000],  loss: 0.017337, mae: 0.374974, mean_q: 0.471911, mean_eps: 0.000000
 1487/5000: episode: 62, duration: 0.405s, episode steps:  29, steps per second:  72, episode reward: -35.580, mean reward: -1.227 [-32.127,  2.300], mean action: 4.759 [0.000, 19.000],  loss: 0.018624, mae: 0.386095, mean_q: 0.489461, mean_eps: 0.000000
 1515/5000: episode: 63, duration: 0.355s, episode steps:  28, steps per second:  79, episode reward: -32.450, mean reward: -1.159 [-32.094,  2.763], mean action: 6.036 [0.000, 20.000],  loss: 0.020610, mae: 0.389603, mean_q: 0.528962, mean_eps: 0.000000
 1538/5000: episode: 64, duration: 0.292s, episode steps:  23, steps per second:  79, episode reward: 34.603, mean reward:  1.504 [-2.434, 32.756], mean action: 5.043 [0.000, 16.000],  loss: 0.022142, mae: 0.400912, mean_q: 0.517945, mean_eps: 0.000000
 1572/5000: episode: 65, duration: 0.425s, episode steps:  34, steps per second:  80, episode reward: 36.000, mean reward:  1.059 [-2.398, 29.929], mean action: 2.206 [0.000, 16.000],  loss: 0.024671, mae: 0.426212, mean_q: 0.450072, mean_eps: 0.000000
 1592/5000: episode: 66, duration: 0.255s, episode steps:  20, steps per second:  78, episode reward: 36.000, mean reward:  1.800 [-3.000, 32.150], mean action: 6.600 [0.000, 16.000],  loss: 0.022040, mae: 0.412469, mean_q: 0.482706, mean_eps: 0.000000
 1622/5000: episode: 67, duration: 0.379s, episode steps:  30, steps per second:  79, episode reward: -32.120, mean reward: -1.071 [-31.949,  2.910], mean action: 6.867 [0.000, 17.000],  loss: 0.020775, mae: 0.395174, mean_q: 0.488338, mean_eps: 0.000000
 1667/5000: episode: 68, duration: 0.578s, episode steps:  45, steps per second:  78, episode reward: 32.115, mean reward:  0.714 [-2.452, 31.993], mean action: 4.489 [0.000, 15.000],  loss: 0.022654, mae: 0.399838, mean_q: 0.479156, mean_eps: 0.000000
 1690/5000: episode: 69, duration: 0.316s, episode steps:  23, steps per second:  73, episode reward: 38.238, mean reward:  1.663 [-2.660, 32.400], mean action: 6.000 [0.000, 21.000],  loss: 0.022627, mae: 0.400037, mean_q: 0.499278, mean_eps: 0.000000
 1709/5000: episode: 70, duration: 0.254s, episode steps:  19, steps per second:  75, episode reward: 39.000, mean reward:  2.053 [-2.432, 32.550], mean action: 5.158 [1.000, 19.000],  loss: 0.020856, mae: 0.394959, mean_q: 0.492342, mean_eps: 0.000000
 1742/5000: episode: 71, duration: 0.420s, episode steps:  33, steps per second:  79, episode reward: -35.620, mean reward: -1.079 [-32.010,  2.460], mean action: 7.333 [0.000, 19.000],  loss: 0.018327, mae: 0.382730, mean_q: 0.436129, mean_eps: 0.000000
 1765/5000: episode: 72, duration: 0.299s, episode steps:  23, steps per second:  77, episode reward: -32.470, mean reward: -1.412 [-32.109,  3.000], mean action: 5.261 [0.000, 14.000],  loss: 0.018824, mae: 0.387957, mean_q: 0.439091, mean_eps: 0.000000
 1794/5000: episode: 73, duration: 0.452s, episode steps:  29, steps per second:  64, episode reward: -32.050, mean reward: -1.105 [-31.715,  2.171], mean action: 6.655 [0.000, 20.000],  loss: 0.019516, mae: 0.387684, mean_q: 0.438011, mean_eps: 0.000000
 1822/5000: episode: 74, duration: 0.778s, episode steps:  28, steps per second:  36, episode reward: -32.780, mean reward: -1.171 [-31.869,  3.000], mean action: 6.964 [0.000, 14.000],  loss: 0.018769, mae: 0.387751, mean_q: 0.451797, mean_eps: 0.000000
 1851/5000: episode: 75, duration: 0.390s, episode steps:  29, steps per second:  74, episode reward: 35.459, mean reward:  1.223 [-2.970, 32.386], mean action: 7.000 [0.000, 21.000],  loss: 0.019604, mae: 0.388532, mean_q: 0.508744, mean_eps: 0.000000
 1865/5000: episode: 76, duration: 0.189s, episode steps:  14, steps per second:  74, episode reward: 38.602, mean reward:  2.757 [-3.000, 32.602], mean action: 4.000 [0.000, 14.000],  loss: 0.022012, mae: 0.397723, mean_q: 0.517748, mean_eps: 0.000000
 1886/5000: episode: 77, duration: 0.272s, episode steps:  21, steps per second:  77, episode reward: 35.521, mean reward:  1.691 [-2.695, 32.360], mean action: 3.429 [0.000, 11.000],  loss: 0.020861, mae: 0.394427, mean_q: 0.452489, mean_eps: 0.000000
 1929/5000: episode: 78, duration: 0.583s, episode steps:  43, steps per second:  74, episode reward: -32.240, mean reward: -0.750 [-32.151,  2.400], mean action: 7.651 [0.000, 13.000],  loss: 0.020172, mae: 0.397849, mean_q: 0.479425, mean_eps: 0.000000
 1947/5000: episode: 79, duration: 0.235s, episode steps:  18, steps per second:  76, episode reward: 41.373, mean reward:  2.299 [-2.171, 32.373], mean action: 2.278 [0.000, 11.000],  loss: 0.020125, mae: 0.394413, mean_q: 0.489336, mean_eps: 0.000000
 1969/5000: episode: 80, duration: 0.303s, episode steps:  22, steps per second:  73, episode reward: 37.911, mean reward:  1.723 [-2.596, 32.010], mean action: 4.000 [0.000, 11.000],  loss: 0.020129, mae: 0.398485, mean_q: 0.447977, mean_eps: 0.000000
 1982/5000: episode: 81, duration: 0.180s, episode steps:  13, steps per second:  72, episode reward: 41.394, mean reward:  3.184 [-3.000, 32.090], mean action: 4.385 [0.000, 14.000],  loss: 0.025681, mae: 0.420254, mean_q: 0.499009, mean_eps: 0.000000
 2008/5000: episode: 82, duration: 0.599s, episode steps:  26, steps per second:  43, episode reward: 35.528, mean reward:  1.366 [-2.704, 32.070], mean action: 5.615 [0.000, 15.000],  loss: 0.019446, mae: 0.392181, mean_q: 0.490611, mean_eps: 0.000000
 2031/5000: episode: 83, duration: 0.381s, episode steps:  23, steps per second:  60, episode reward: -38.800, mean reward: -1.687 [-32.087,  2.280], mean action: 7.261 [0.000, 17.000],  loss: 0.021680, mae: 0.401942, mean_q: 0.522663, mean_eps: 0.000000
 2074/5000: episode: 84, duration: 0.535s, episode steps:  43, steps per second:  80, episode reward: -32.830, mean reward: -0.763 [-32.180,  2.342], mean action: 4.628 [0.000, 19.000],  loss: 0.021920, mae: 0.401455, mean_q: 0.492114, mean_eps: 0.000000
 2093/5000: episode: 85, duration: 0.289s, episode steps:  19, steps per second:  66, episode reward: 38.938, mean reward:  2.049 [-2.362, 32.438], mean action: 7.000 [1.000, 20.000],  loss: 0.016242, mae: 0.374781, mean_q: 0.478624, mean_eps: 0.000000
 2123/5000: episode: 86, duration: 0.454s, episode steps:  30, steps per second:  66, episode reward: -30.000, mean reward: -1.000 [-30.024,  2.884], mean action: 6.200 [0.000, 19.000],  loss: 0.020780, mae: 0.400157, mean_q: 0.532688, mean_eps: 0.000000
 2155/5000: episode: 87, duration: 0.648s, episode steps:  32, steps per second:  49, episode reward: 35.097, mean reward:  1.097 [-2.343, 32.283], mean action: 5.625 [0.000, 19.000],  loss: 0.019814, mae: 0.396615, mean_q: 0.460724, mean_eps: 0.000000
 2177/5000: episode: 88, duration: 0.383s, episode steps:  22, steps per second:  57, episode reward: 35.306, mean reward:  1.605 [-2.684, 31.919], mean action: 5.455 [0.000, 19.000],  loss: 0.019390, mae: 0.402575, mean_q: 0.406401, mean_eps: 0.000000
 2204/5000: episode: 89, duration: 0.422s, episode steps:  27, steps per second:  64, episode reward: 32.654, mean reward:  1.209 [-2.867, 31.994], mean action: 2.667 [0.000, 19.000],  loss: 0.018546, mae: 0.383847, mean_q: 0.470502, mean_eps: 0.000000
 2231/5000: episode: 90, duration: 0.339s, episode steps:  27, steps per second:  80, episode reward: -35.550, mean reward: -1.317 [-32.622,  2.500], mean action: 4.444 [0.000, 21.000],  loss: 0.019719, mae: 0.389099, mean_q: 0.457827, mean_eps: 0.000000
 2257/5000: episode: 91, duration: 0.329s, episode steps:  26, steps per second:  79, episode reward: 32.712, mean reward:  1.258 [-3.000, 32.319], mean action: 5.385 [1.000, 19.000],  loss: 0.017395, mae: 0.374558, mean_q: 0.461291, mean_eps: 0.000000
 2280/5000: episode: 92, duration: 0.438s, episode steps:  23, steps per second:  53, episode reward: 33.000, mean reward:  1.435 [-3.000, 32.090], mean action: 4.913 [0.000, 19.000],  loss: 0.017898, mae: 0.383477, mean_q: 0.431018, mean_eps: 0.000000
 2302/5000: episode: 93, duration: 0.335s, episode steps:  22, steps per second:  66, episode reward: 32.884, mean reward:  1.495 [-2.710, 32.884], mean action: 5.909 [0.000, 19.000],  loss: 0.017825, mae: 0.382537, mean_q: 0.422195, mean_eps: 0.000000
 2319/5000: episode: 94, duration: 0.790s, episode steps:  17, steps per second:  22, episode reward: 41.202, mean reward:  2.424 [-2.716, 32.870], mean action: 3.706 [0.000, 19.000],  loss: 0.019003, mae: 0.385092, mean_q: 0.428612, mean_eps: 0.000000
 2336/5000: episode: 95, duration: 0.392s, episode steps:  17, steps per second:  43, episode reward: 42.000, mean reward:  2.471 [-2.351, 33.000], mean action: 3.529 [0.000, 19.000],  loss: 0.017470, mae: 0.375326, mean_q: 0.419835, mean_eps: 0.000000
 2381/5000: episode: 96, duration: 0.805s, episode steps:  45, steps per second:  56, episode reward: 32.299, mean reward:  0.718 [-2.420, 32.280], mean action: 7.400 [0.000, 20.000],  loss: 0.018054, mae: 0.376941, mean_q: 0.438816, mean_eps: 0.000000
 2414/5000: episode: 97, duration: 0.564s, episode steps:  33, steps per second:  58, episode reward: 32.740, mean reward:  0.992 [-2.317, 31.910], mean action: 6.848 [0.000, 16.000],  loss: 0.019871, mae: 0.386365, mean_q: 0.499491, mean_eps: 0.000000
 2438/5000: episode: 98, duration: 0.785s, episode steps:  24, steps per second:  31, episode reward: 39.000, mean reward:  1.625 [-2.700, 32.050], mean action: 3.292 [0.000, 16.000],  loss: 0.019194, mae: 0.387363, mean_q: 0.484794, mean_eps: 0.000000
 2457/5000: episode: 99, duration: 0.293s, episode steps:  19, steps per second:  65, episode reward: 39.000, mean reward:  2.053 [-2.142, 32.630], mean action: 4.947 [0.000, 16.000],  loss: 0.022141, mae: 0.406514, mean_q: 0.486079, mean_eps: 0.000000
 2478/5000: episode: 100, duration: 0.316s, episode steps:  21, steps per second:  66, episode reward: 35.133, mean reward:  1.673 [-2.290, 32.293], mean action: 3.286 [0.000, 16.000],  loss: 0.015987, mae: 0.372352, mean_q: 0.524716, mean_eps: 0.000000
 2505/5000: episode: 101, duration: 0.380s, episode steps:  27, steps per second:  71, episode reward: 32.890, mean reward:  1.218 [-2.354, 32.600], mean action: 4.963 [0.000, 18.000],  loss: 0.020345, mae: 0.385100, mean_q: 0.496104, mean_eps: 0.000000
 2535/5000: episode: 102, duration: 0.452s, episode steps:  30, steps per second:  66, episode reward: -32.230, mean reward: -1.074 [-32.094,  2.654], mean action: 6.033 [0.000, 16.000],  loss: 0.018279, mae: 0.366282, mean_q: 0.474768, mean_eps: 0.000000
 2561/5000: episode: 103, duration: 0.543s, episode steps:  26, steps per second:  48, episode reward: 32.787, mean reward:  1.261 [-3.000, 31.857], mean action: 4.385 [0.000, 18.000],  loss: 0.017755, mae: 0.384869, mean_q: 0.428338, mean_eps: 0.000000
 2591/5000: episode: 104, duration: 0.453s, episode steps:  30, steps per second:  66, episode reward: 37.431, mean reward:  1.248 [-2.234, 32.091], mean action: 3.600 [0.000, 16.000],  loss: 0.019497, mae: 0.398868, mean_q: 0.482628, mean_eps: 0.000000
 2619/5000: episode: 105, duration: 0.405s, episode steps:  28, steps per second:  69, episode reward: -35.780, mean reward: -1.278 [-32.419,  2.380], mean action: 3.036 [0.000, 12.000],  loss: 0.022402, mae: 0.403362, mean_q: 0.474872, mean_eps: 0.000000
 2646/5000: episode: 106, duration: 0.354s, episode steps:  27, steps per second:  76, episode reward: -35.900, mean reward: -1.330 [-32.308,  2.210], mean action: 4.926 [0.000, 21.000],  loss: 0.022185, mae: 0.404381, mean_q: 0.471127, mean_eps: 0.000000
 2666/5000: episode: 107, duration: 0.264s, episode steps:  20, steps per second:  76, episode reward: 38.459, mean reward:  1.923 [-2.841, 32.040], mean action: 4.250 [0.000, 14.000],  loss: 0.018242, mae: 0.389650, mean_q: 0.436885, mean_eps: 0.000000
 2685/5000: episode: 108, duration: 0.246s, episode steps:  19, steps per second:  77, episode reward: 38.484, mean reward:  2.025 [-2.476, 32.900], mean action: 5.947 [0.000, 18.000],  loss: 0.020184, mae: 0.394679, mean_q: 0.417129, mean_eps: 0.000000
 2707/5000: episode: 109, duration: 0.285s, episode steps:  22, steps per second:  77, episode reward: 32.073, mean reward:  1.458 [-2.370, 32.140], mean action: 4.455 [0.000, 16.000],  loss: 0.018892, mae: 0.391065, mean_q: 0.383539, mean_eps: 0.000000
 2727/5000: episode: 110, duration: 0.260s, episode steps:  20, steps per second:  77, episode reward: 32.772, mean reward:  1.639 [-3.000, 32.410], mean action: 4.350 [0.000, 15.000],  loss: 0.022557, mae: 0.406019, mean_q: 0.415833, mean_eps: 0.000000
 2747/5000: episode: 111, duration: 0.268s, episode steps:  20, steps per second:  75, episode reward: 38.141, mean reward:  1.907 [-3.000, 32.162], mean action: 5.100 [0.000, 14.000],  loss: 0.022500, mae: 0.402071, mean_q: 0.437768, mean_eps: 0.000000
 2770/5000: episode: 112, duration: 0.319s, episode steps:  23, steps per second:  72, episode reward: 33.000, mean reward:  1.435 [-3.000, 33.000], mean action: 8.478 [0.000, 19.000],  loss: 0.021871, mae: 0.392869, mean_q: 0.448473, mean_eps: 0.000000
 2797/5000: episode: 113, duration: 0.350s, episode steps:  27, steps per second:  77, episode reward: 38.568, mean reward:  1.428 [-2.347, 32.299], mean action: 3.481 [0.000, 19.000],  loss: 0.020758, mae: 0.389182, mean_q: 0.489191, mean_eps: 0.000000
 2818/5000: episode: 114, duration: 0.279s, episode steps:  21, steps per second:  75, episode reward: 35.218, mean reward:  1.677 [-2.416, 32.090], mean action: 3.381 [0.000, 11.000],  loss: 0.024358, mae: 0.401132, mean_q: 0.479118, mean_eps: 0.000000
 2842/5000: episode: 115, duration: 0.303s, episode steps:  24, steps per second:  79, episode reward: -35.450, mean reward: -1.477 [-32.072,  2.720], mean action: 7.625 [0.000, 16.000],  loss: 0.019557, mae: 0.378121, mean_q: 0.441805, mean_eps: 0.000000
 2853/5000: episode: 116, duration: 0.156s, episode steps:  11, steps per second:  71, episode reward: 41.744, mean reward:  3.795 [-3.000, 32.204], mean action: 3.545 [0.000, 19.000],  loss: 0.017163, mae: 0.362078, mean_q: 0.489609, mean_eps: 0.000000
 2883/5000: episode: 117, duration: 0.376s, episode steps:  30, steps per second:  80, episode reward: 32.600, mean reward:  1.087 [-3.000, 31.910], mean action: 2.967 [0.000, 19.000],  loss: 0.020087, mae: 0.376879, mean_q: 0.510154, mean_eps: 0.000000
 2910/5000: episode: 118, duration: 0.345s, episode steps:  27, steps per second:  78, episode reward: 37.523, mean reward:  1.390 [-3.000, 32.060], mean action: 4.741 [0.000, 14.000],  loss: 0.020748, mae: 0.388027, mean_q: 0.426313, mean_eps: 0.000000
 2936/5000: episode: 119, duration: 0.329s, episode steps:  26, steps per second:  79, episode reward: -32.910, mean reward: -1.266 [-31.949,  2.911], mean action: 9.385 [0.000, 20.000],  loss: 0.017519, mae: 0.375669, mean_q: 0.430004, mean_eps: 0.000000
 2952/5000: episode: 120, duration: 0.220s, episode steps:  16, steps per second:  73, episode reward: 44.108, mean reward:  2.757 [-2.095, 32.150], mean action: 2.750 [0.000, 19.000],  loss: 0.015787, mae: 0.359631, mean_q: 0.492206, mean_eps: 0.000000
 2981/5000: episode: 121, duration: 0.367s, episode steps:  29, steps per second:  79, episode reward: 32.807, mean reward:  1.131 [-2.571, 32.240], mean action: 6.862 [0.000, 19.000],  loss: 0.023100, mae: 0.399888, mean_q: 0.473673, mean_eps: 0.000000
 3016/5000: episode: 122, duration: 0.460s, episode steps:  35, steps per second:  76, episode reward: 33.000, mean reward:  0.943 [-2.553, 32.162], mean action: 9.714 [1.000, 19.000],  loss: 0.018910, mae: 0.387476, mean_q: 0.435259, mean_eps: 0.000000
 3038/5000: episode: 123, duration: 0.281s, episode steps:  22, steps per second:  78, episode reward: 35.698, mean reward:  1.623 [-2.591, 32.035], mean action: 4.909 [0.000, 19.000],  loss: 0.020549, mae: 0.388098, mean_q: 0.510591, mean_eps: 0.000000
 3057/5000: episode: 124, duration: 0.248s, episode steps:  19, steps per second:  77, episode reward: 40.342, mean reward:  2.123 [-2.183, 32.174], mean action: 2.789 [0.000, 11.000],  loss: 0.018992, mae: 0.381960, mean_q: 0.481508, mean_eps: 0.000000
 3071/5000: episode: 125, duration: 0.192s, episode steps:  14, steps per second:  73, episode reward: 41.163, mean reward:  2.940 [-2.378, 33.000], mean action: 2.714 [0.000, 11.000],  loss: 0.022328, mae: 0.397631, mean_q: 0.514003, mean_eps: 0.000000
 3093/5000: episode: 126, duration: 0.350s, episode steps:  22, steps per second:  63, episode reward: 39.000, mean reward:  1.773 [-2.333, 32.260], mean action: 2.955 [0.000, 16.000],  loss: 0.023301, mae: 0.398422, mean_q: 0.511750, mean_eps: 0.000000
 3120/5000: episode: 127, duration: 0.448s, episode steps:  27, steps per second:  60, episode reward: 35.445, mean reward:  1.313 [-3.000, 31.505], mean action: 4.741 [0.000, 16.000],  loss: 0.018336, mae: 0.377602, mean_q: 0.489880, mean_eps: 0.000000
 3141/5000: episode: 128, duration: 0.307s, episode steps:  21, steps per second:  68, episode reward: -35.230, mean reward: -1.678 [-31.717,  2.160], mean action: 3.429 [0.000, 12.000],  loss: 0.019342, mae: 0.384942, mean_q: 0.497166, mean_eps: 0.000000
 3163/5000: episode: 129, duration: 0.309s, episode steps:  22, steps per second:  71, episode reward: 32.841, mean reward:  1.493 [-3.000, 32.350], mean action: 5.455 [0.000, 16.000],  loss: 0.023044, mae: 0.407838, mean_q: 0.529474, mean_eps: 0.000000
 3180/5000: episode: 130, duration: 0.251s, episode steps:  17, steps per second:  68, episode reward: 35.267, mean reward:  2.075 [-3.000, 32.514], mean action: 5.706 [0.000, 16.000],  loss: 0.019961, mae: 0.396477, mean_q: 0.486513, mean_eps: 0.000000
 3212/5000: episode: 131, duration: 0.427s, episode steps:  32, steps per second:  75, episode reward: 32.462, mean reward:  1.014 [-3.000, 31.789], mean action: 6.656 [0.000, 19.000],  loss: 0.020575, mae: 0.404493, mean_q: 0.456312, mean_eps: 0.000000
 3233/5000: episode: 132, duration: 0.275s, episode steps:  21, steps per second:  76, episode reward: 38.574, mean reward:  1.837 [-2.610, 32.120], mean action: 3.857 [0.000, 19.000],  loss: 0.022367, mae: 0.413682, mean_q: 0.429131, mean_eps: 0.000000
 3252/5000: episode: 133, duration: 0.267s, episode steps:  19, steps per second:  71, episode reward: 41.776, mean reward:  2.199 [-2.143, 32.120], mean action: 3.842 [0.000, 19.000],  loss: 0.021065, mae: 0.409579, mean_q: 0.436221, mean_eps: 0.000000
 3274/5000: episode: 134, duration: 0.442s, episode steps:  22, steps per second:  50, episode reward: 35.506, mean reward:  1.614 [-3.000, 32.280], mean action: 5.182 [0.000, 16.000],  loss: 0.020474, mae: 0.401136, mean_q: 0.450110, mean_eps: 0.000000
 3296/5000: episode: 135, duration: 0.462s, episode steps:  22, steps per second:  48, episode reward: 38.103, mean reward:  1.732 [-2.292, 31.890], mean action: 4.091 [0.000, 16.000],  loss: 0.025383, mae: 0.424514, mean_q: 0.485356, mean_eps: 0.000000
 3316/5000: episode: 136, duration: 0.297s, episode steps:  20, steps per second:  67, episode reward: -38.390, mean reward: -1.919 [-31.768,  2.783], mean action: 4.350 [0.000, 16.000],  loss: 0.015079, mae: 0.355772, mean_q: 0.496500, mean_eps: 0.000000
 3345/5000: episode: 137, duration: 0.412s, episode steps:  29, steps per second:  70, episode reward: 32.403, mean reward:  1.117 [-2.581, 32.090], mean action: 3.172 [0.000, 16.000],  loss: 0.023139, mae: 0.401618, mean_q: 0.483451, mean_eps: 0.000000
 3370/5000: episode: 138, duration: 0.316s, episode steps:  25, steps per second:  79, episode reward: -32.330, mean reward: -1.293 [-31.712,  2.535], mean action: 8.880 [0.000, 20.000],  loss: 0.020680, mae: 0.384059, mean_q: 0.473902, mean_eps: 0.000000
 3436/5000: episode: 139, duration: 1.493s, episode steps:  66, steps per second:  44, episode reward: -32.910, mean reward: -0.499 [-32.061,  2.910], mean action: 16.606 [0.000, 21.000],  loss: 0.020271, mae: 0.385637, mean_q: 0.491783, mean_eps: 0.000000
 3457/5000: episode: 140, duration: 0.316s, episode steps:  21, steps per second:  67, episode reward: -38.970, mean reward: -1.856 [-32.081,  2.420], mean action: 9.762 [1.000, 15.000],  loss: 0.020516, mae: 0.391621, mean_q: 0.434574, mean_eps: 0.000000
 3492/5000: episode: 141, duration: 0.457s, episode steps:  35, steps per second:  77, episode reward: -32.600, mean reward: -0.931 [-31.726,  2.340], mean action: 4.800 [0.000, 15.000],  loss: 0.020201, mae: 0.390213, mean_q: 0.452234, mean_eps: 0.000000
 3519/5000: episode: 142, duration: 0.402s, episode steps:  27, steps per second:  67, episode reward: -32.890, mean reward: -1.218 [-32.084,  2.390], mean action: 9.481 [1.000, 20.000],  loss: 0.021380, mae: 0.401923, mean_q: 0.421936, mean_eps: 0.000000
 3546/5000: episode: 143, duration: 0.343s, episode steps:  27, steps per second:  79, episode reward: 40.845, mean reward:  1.513 [-2.314, 32.098], mean action: 3.370 [0.000, 19.000],  loss: 0.020777, mae: 0.387537, mean_q: 0.508282, mean_eps: 0.000000
 3572/5000: episode: 144, duration: 0.336s, episode steps:  26, steps per second:  77, episode reward: 41.478, mean reward:  1.595 [-2.367, 32.028], mean action: 3.269 [0.000, 20.000],  loss: 0.020429, mae: 0.381428, mean_q: 0.550571, mean_eps: 0.000000
 3591/5000: episode: 145, duration: 0.256s, episode steps:  19, steps per second:  74, episode reward: 41.381, mean reward:  2.178 [-2.155, 32.387], mean action: 2.684 [0.000, 12.000],  loss: 0.019226, mae: 0.376931, mean_q: 0.580740, mean_eps: 0.000000
 3617/5000: episode: 146, duration: 0.339s, episode steps:  26, steps per second:  77, episode reward: 37.275, mean reward:  1.434 [-2.457, 32.273], mean action: 3.962 [0.000, 16.000],  loss: 0.023241, mae: 0.401766, mean_q: 0.497801, mean_eps: 0.000000
 3638/5000: episode: 147, duration: 0.268s, episode steps:  21, steps per second:  78, episode reward: -38.270, mean reward: -1.822 [-32.189,  2.290], mean action: 8.429 [0.000, 16.000],  loss: 0.021369, mae: 0.389383, mean_q: 0.513622, mean_eps: 0.000000
 3662/5000: episode: 148, duration: 0.316s, episode steps:  24, steps per second:  76, episode reward: 35.188, mean reward:  1.466 [-2.578, 32.241], mean action: 5.583 [1.000, 16.000],  loss: 0.019811, mae: 0.384196, mean_q: 0.512074, mean_eps: 0.000000
 3680/5000: episode: 149, duration: 0.237s, episode steps:  18, steps per second:  76, episode reward: 38.036, mean reward:  2.113 [-3.000, 33.000], mean action: 4.889 [1.000, 14.000],  loss: 0.020049, mae: 0.386215, mean_q: 0.494945, mean_eps: 0.000000
 3704/5000: episode: 150, duration: 0.314s, episode steps:  24, steps per second:  76, episode reward: -33.000, mean reward: -1.375 [-32.677,  2.470], mean action: 5.750 [0.000, 16.000],  loss: 0.018842, mae: 0.380209, mean_q: 0.505470, mean_eps: 0.000000
 3724/5000: episode: 151, duration: 0.268s, episode steps:  20, steps per second:  75, episode reward: 35.693, mean reward:  1.785 [-2.562, 32.076], mean action: 6.200 [0.000, 16.000],  loss: 0.020250, mae: 0.384141, mean_q: 0.519923, mean_eps: 0.000000
 3745/5000: episode: 152, duration: 0.273s, episode steps:  21, steps per second:  77, episode reward: 32.805, mean reward:  1.562 [-2.901, 31.963], mean action: 6.952 [0.000, 17.000],  loss: 0.024921, mae: 0.404762, mean_q: 0.519889, mean_eps: 0.000000
 3771/5000: episode: 153, duration: 0.682s, episode steps:  26, steps per second:  38, episode reward: -32.510, mean reward: -1.250 [-32.267,  3.000], mean action: 8.962 [0.000, 17.000],  loss: 0.019057, mae: 0.374327, mean_q: 0.527038, mean_eps: 0.000000
 3789/5000: episode: 154, duration: 0.249s, episode steps:  18, steps per second:  72, episode reward: 43.859, mean reward:  2.437 [-2.134, 33.000], mean action: 3.333 [0.000, 19.000],  loss: 0.021317, mae: 0.391838, mean_q: 0.528309, mean_eps: 0.000000
 3832/5000: episode: 155, duration: 0.546s, episode steps:  43, steps per second:  79, episode reward: 38.025, mean reward:  0.884 [-2.433, 32.140], mean action: 7.047 [0.000, 21.000],  loss: 0.017572, mae: 0.368919, mean_q: 0.533859, mean_eps: 0.000000
 3860/5000: episode: 156, duration: 0.356s, episode steps:  28, steps per second:  79, episode reward: -33.000, mean reward: -1.179 [-32.603,  2.805], mean action: 8.143 [0.000, 21.000],  loss: 0.019400, mae: 0.382809, mean_q: 0.523448, mean_eps: 0.000000
 3886/5000: episode: 157, duration: 0.327s, episode steps:  26, steps per second:  80, episode reward: -35.180, mean reward: -1.353 [-32.067,  2.543], mean action: 6.000 [0.000, 16.000],  loss: 0.017603, mae: 0.375337, mean_q: 0.512534, mean_eps: 0.000000
 3912/5000: episode: 158, duration: 0.423s, episode steps:  26, steps per second:  61, episode reward: -35.240, mean reward: -1.355 [-31.805,  2.430], mean action: 7.077 [0.000, 16.000],  loss: 0.017211, mae: 0.382631, mean_q: 0.452933, mean_eps: 0.000000
 3937/5000: episode: 159, duration: 0.393s, episode steps:  25, steps per second:  64, episode reward: -32.190, mean reward: -1.288 [-32.286,  2.453], mean action: 5.240 [0.000, 16.000],  loss: 0.016472, mae: 0.373395, mean_q: 0.501163, mean_eps: 0.000000
 3957/5000: episode: 160, duration: 0.257s, episode steps:  20, steps per second:  78, episode reward: 32.690, mean reward:  1.635 [-3.000, 32.170], mean action: 5.850 [0.000, 16.000],  loss: 0.020160, mae: 0.384618, mean_q: 0.466433, mean_eps: 0.000000
 3983/5000: episode: 161, duration: 0.327s, episode steps:  26, steps per second:  79, episode reward: 30.000, mean reward:  1.154 [-3.000, 30.371], mean action: 5.923 [0.000, 16.000],  loss: 0.022808, mae: 0.392806, mean_q: 0.399027, mean_eps: 0.000000
 4007/5000: episode: 162, duration: 0.309s, episode steps:  24, steps per second:  78, episode reward: 35.611, mean reward:  1.484 [-3.000, 32.600], mean action: 6.000 [0.000, 16.000],  loss: 0.015838, mae: 0.357771, mean_q: 0.445313, mean_eps: 0.000000
 4034/5000: episode: 163, duration: 0.344s, episode steps:  27, steps per second:  79, episode reward: -39.000, mean reward: -1.444 [-32.401,  2.400], mean action: 5.037 [0.000, 17.000],  loss: 0.015482, mae: 0.360700, mean_q: 0.447841, mean_eps: 0.000000
 4080/5000: episode: 164, duration: 0.656s, episode steps:  46, steps per second:  70, episode reward: -32.140, mean reward: -0.699 [-32.279,  2.630], mean action: 7.391 [0.000, 21.000],  loss: 0.021218, mae: 0.383477, mean_q: 0.450787, mean_eps: 0.000000
 4103/5000: episode: 165, duration: 0.300s, episode steps:  23, steps per second:  77, episode reward: 38.651, mean reward:  1.680 [-2.545, 32.261], mean action: 3.783 [0.000, 16.000],  loss: 0.021824, mae: 0.391276, mean_q: 0.460553, mean_eps: 0.000000
 4133/5000: episode: 166, duration: 0.413s, episode steps:  30, steps per second:  73, episode reward: -32.380, mean reward: -1.079 [-31.914,  2.351], mean action: 2.767 [0.000, 15.000],  loss: 0.019131, mae: 0.375116, mean_q: 0.464607, mean_eps: 0.000000
 4165/5000: episode: 167, duration: 0.606s, episode steps:  32, steps per second:  53, episode reward: -32.020, mean reward: -1.001 [-32.099,  2.380], mean action: 3.938 [0.000, 19.000],  loss: 0.018508, mae: 0.375238, mean_q: 0.464649, mean_eps: 0.000000
 4192/5000: episode: 168, duration: 0.557s, episode steps:  27, steps per second:  48, episode reward: 41.807, mean reward:  1.548 [-2.260, 32.052], mean action: 2.926 [0.000, 9.000],  loss: 0.018637, mae: 0.368206, mean_q: 0.488750, mean_eps: 0.000000
 4213/5000: episode: 169, duration: 0.294s, episode steps:  21, steps per second:  71, episode reward: 35.931, mean reward:  1.711 [-3.000, 32.191], mean action: 5.333 [0.000, 17.000],  loss: 0.019072, mae: 0.375251, mean_q: 0.500679, mean_eps: 0.000000
 4239/5000: episode: 170, duration: 0.339s, episode steps:  26, steps per second:  77, episode reward: 38.253, mean reward:  1.471 [-3.000, 31.703], mean action: 3.923 [0.000, 20.000],  loss: 0.021024, mae: 0.384798, mean_q: 0.478781, mean_eps: 0.000000
 4265/5000: episode: 171, duration: 0.338s, episode steps:  26, steps per second:  77, episode reward: 35.331, mean reward:  1.359 [-2.806, 33.000], mean action: 7.346 [0.000, 21.000],  loss: 0.018584, mae: 0.363087, mean_q: 0.521628, mean_eps: 0.000000
 4283/5000: episode: 172, duration: 0.241s, episode steps:  18, steps per second:  75, episode reward: 35.119, mean reward:  1.951 [-2.542, 31.681], mean action: 4.778 [0.000, 17.000],  loss: 0.022140, mae: 0.385518, mean_q: 0.623453, mean_eps: 0.000000
 4300/5000: episode: 173, duration: 0.232s, episode steps:  17, steps per second:  73, episode reward: 37.638, mean reward:  2.214 [-2.749, 31.653], mean action: 7.235 [0.000, 19.000],  loss: 0.020591, mae: 0.383738, mean_q: 0.628735, mean_eps: 0.000000
 4313/5000: episode: 174, duration: 0.222s, episode steps:  13, steps per second:  59, episode reward: 41.660, mean reward:  3.205 [-2.213, 32.570], mean action: 2.923 [0.000, 19.000],  loss: 0.014240, mae: 0.355859, mean_q: 0.558303, mean_eps: 0.000000
 4338/5000: episode: 175, duration: 0.318s, episode steps:  25, steps per second:  79, episode reward: 32.479, mean reward:  1.299 [-3.000, 32.530], mean action: 6.520 [0.000, 14.000],  loss: 0.019709, mae: 0.391163, mean_q: 0.501218, mean_eps: 0.000000
 4357/5000: episode: 176, duration: 0.252s, episode steps:  19, steps per second:  76, episode reward: 38.832, mean reward:  2.044 [-2.667, 32.412], mean action: 6.211 [0.000, 14.000],  loss: 0.022015, mae: 0.398321, mean_q: 0.465900, mean_eps: 0.000000
 4382/5000: episode: 177, duration: 0.324s, episode steps:  25, steps per second:  77, episode reward: 32.769, mean reward:  1.311 [-3.000, 32.070], mean action: 3.600 [0.000, 12.000],  loss: 0.016830, mae: 0.370697, mean_q: 0.478190, mean_eps: 0.000000
 4405/5000: episode: 178, duration: 0.307s, episode steps:  23, steps per second:  75, episode reward: 33.000, mean reward:  1.435 [-2.693, 32.060], mean action: 8.652 [0.000, 19.000],  loss: 0.018024, mae: 0.379698, mean_q: 0.466303, mean_eps: 0.000000
 4428/5000: episode: 179, duration: 0.307s, episode steps:  23, steps per second:  75, episode reward: 36.000, mean reward:  1.565 [-2.603, 32.320], mean action: 4.435 [0.000, 19.000],  loss: 0.023207, mae: 0.403327, mean_q: 0.436600, mean_eps: 0.000000
 4460/5000: episode: 180, duration: 0.441s, episode steps:  32, steps per second:  73, episode reward: 35.092, mean reward:  1.097 [-3.000, 31.747], mean action: 6.062 [0.000, 19.000],  loss: 0.021098, mae: 0.383626, mean_q: 0.499421, mean_eps: 0.000000
 4478/5000: episode: 181, duration: 0.235s, episode steps:  18, steps per second:  77, episode reward: 38.584, mean reward:  2.144 [-2.351, 31.934], mean action: 5.056 [0.000, 19.000],  loss: 0.021115, mae: 0.386089, mean_q: 0.509713, mean_eps: 0.000000
 4508/5000: episode: 182, duration: 0.388s, episode steps:  30, steps per second:  77, episode reward: 32.813, mean reward:  1.094 [-2.601, 32.813], mean action: 4.800 [1.000, 17.000],  loss: 0.020771, mae: 0.386102, mean_q: 0.429262, mean_eps: 0.000000
 4525/5000: episode: 183, duration: 0.225s, episode steps:  17, steps per second:  76, episode reward: 44.183, mean reward:  2.599 [-2.441, 32.110], mean action: 2.235 [1.000, 12.000],  loss: 0.017212, mae: 0.367682, mean_q: 0.441288, mean_eps: 0.000000
 4547/5000: episode: 184, duration: 0.287s, episode steps:  22, steps per second:  77, episode reward: 40.566, mean reward:  1.844 [-2.345, 31.911], mean action: 3.545 [1.000, 16.000],  loss: 0.020500, mae: 0.377117, mean_q: 0.484987, mean_eps: 0.000000
 4567/5000: episode: 185, duration: 0.266s, episode steps:  20, steps per second:  75, episode reward: 41.303, mean reward:  2.065 [-2.352, 32.190], mean action: 5.050 [0.000, 16.000],  loss: 0.018475, mae: 0.369789, mean_q: 0.480902, mean_eps: 0.000000
 4584/5000: episode: 186, duration: 0.222s, episode steps:  17, steps per second:  77, episode reward: 38.456, mean reward:  2.262 [-2.541, 32.456], mean action: 5.059 [0.000, 20.000],  loss: 0.019712, mae: 0.371853, mean_q: 0.445739, mean_eps: 0.000000
 4617/5000: episode: 187, duration: 0.416s, episode steps:  33, steps per second:  79, episode reward: -35.310, mean reward: -1.070 [-32.202,  2.620], mean action: 10.758 [0.000, 17.000],  loss: 0.019662, mae: 0.366215, mean_q: 0.441049, mean_eps: 0.000000
 4641/5000: episode: 188, duration: 0.307s, episode steps:  24, steps per second:  78, episode reward: 35.520, mean reward:  1.480 [-2.244, 32.320], mean action: 4.083 [0.000, 16.000],  loss: 0.017042, mae: 0.360739, mean_q: 0.473084, mean_eps: 0.000000
 4670/5000: episode: 189, duration: 0.370s, episode steps:  29, steps per second:  78, episode reward: 30.000, mean reward:  1.034 [-2.563, 30.047], mean action: 4.966 [0.000, 19.000],  loss: 0.022315, mae: 0.384890, mean_q: 0.485338, mean_eps: 0.000000
 4701/5000: episode: 190, duration: 0.386s, episode steps:  31, steps per second:  80, episode reward: 38.453, mean reward:  1.240 [-2.311, 32.250], mean action: 2.548 [0.000, 16.000],  loss: 0.021910, mae: 0.388825, mean_q: 0.464578, mean_eps: 0.000000
 4716/5000: episode: 191, duration: 0.317s, episode steps:  15, steps per second:  47, episode reward: 44.038, mean reward:  2.936 [-2.241, 32.030], mean action: 5.200 [1.000, 20.000],  loss: 0.018861, mae: 0.372358, mean_q: 0.453545, mean_eps: 0.000000
 4744/5000: episode: 192, duration: 0.545s, episode steps:  28, steps per second:  51, episode reward: 44.938, mean reward:  1.605 [-2.201, 32.039], mean action: 1.964 [1.000, 16.000],  loss: 0.021791, mae: 0.379627, mean_q: 0.468218, mean_eps: 0.000000
 4783/5000: episode: 193, duration: 0.500s, episode steps:  39, steps per second:  78, episode reward: 35.102, mean reward:  0.900 [-2.741, 32.628], mean action: 6.282 [0.000, 20.000],  loss: 0.022599, mae: 0.378726, mean_q: 0.476309, mean_eps: 0.000000
 4809/5000: episode: 194, duration: 0.327s, episode steps:  26, steps per second:  79, episode reward: -32.910, mean reward: -1.266 [-32.387,  3.031], mean action: 5.692 [0.000, 17.000],  loss: 0.023275, mae: 0.383242, mean_q: 0.500817, mean_eps: 0.000000
 4832/5000: episode: 195, duration: 0.293s, episode steps:  23, steps per second:  78, episode reward: 36.000, mean reward:  1.565 [-2.339, 33.000], mean action: 3.478 [0.000, 14.000],  loss: 0.017433, mae: 0.361005, mean_q: 0.484910, mean_eps: 0.000000
 4853/5000: episode: 196, duration: 0.290s, episode steps:  21, steps per second:  72, episode reward: 35.688, mean reward:  1.699 [-3.000, 32.335], mean action: 3.667 [0.000, 14.000],  loss: 0.019654, mae: 0.367642, mean_q: 0.500520, mean_eps: 0.000000
 4878/5000: episode: 197, duration: 0.343s, episode steps:  25, steps per second:  73, episode reward: -38.620, mean reward: -1.545 [-32.311,  2.321], mean action: 7.760 [0.000, 21.000],  loss: 0.021536, mae: 0.376803, mean_q: 0.488075, mean_eps: 0.000000
 4901/5000: episode: 198, duration: 0.292s, episode steps:  23, steps per second:  79, episode reward: 35.174, mean reward:  1.529 [-2.600, 31.961], mean action: 6.174 [0.000, 19.000],  loss: 0.015772, mae: 0.348801, mean_q: 0.484882, mean_eps: 0.000000
 4927/5000: episode: 199, duration: 0.330s, episode steps:  26, steps per second:  79, episode reward: -30.000, mean reward: -1.154 [-30.490,  2.579], mean action: 3.846 [0.000, 19.000],  loss: 0.018631, mae: 0.363362, mean_q: 0.519283, mean_eps: 0.000000
 4947/5000: episode: 200, duration: 0.394s, episode steps:  20, steps per second:  51, episode reward: 37.777, mean reward:  1.889 [-2.618, 32.670], mean action: 5.950 [0.000, 19.000],  loss: 0.017886, mae: 0.366750, mean_q: 0.552458, mean_eps: 0.000000
 4969/5000: episode: 201, duration: 0.329s, episode steps:  22, steps per second:  67, episode reward: 33.000, mean reward:  1.500 [-3.000, 33.000], mean action: 7.318 [1.000, 19.000],  loss: 0.016000, mae: 0.361531, mean_q: 0.506154, mean_eps: 0.000000
 4986/5000: episode: 202, duration: 0.232s, episode steps:  17, steps per second:  73, episode reward: 35.900, mean reward:  2.112 [-2.464, 32.610], mean action: 4.588 [0.000, 19.000],  loss: 0.018766, mae: 0.378340, mean_q: 0.481147, mean_eps: 0.000000
done, took 65.446 seconds
DQN Evaluation: 4091 victories out of 4873 episodes
Training for 5000 steps ...
   22/5000: episode: 1, duration: 0.160s, episode steps:  22, steps per second: 138, episode reward: 47.071, mean reward:  2.140 [-0.030, 32.370], mean action: 1.091 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   76/5000: episode: 2, duration: 0.346s, episode steps:  54, steps per second: 156, episode reward: 35.806, mean reward:  0.663 [-2.318, 31.983], mean action: 2.556 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   96/5000: episode: 3, duration: 0.139s, episode steps:  20, steps per second: 144, episode reward: 41.068, mean reward:  2.053 [-2.577, 32.500], mean action: 3.950 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  122/5000: episode: 4, duration: 0.170s, episode steps:  26, steps per second: 153, episode reward: 41.380, mean reward:  1.592 [-2.486, 32.190], mean action: 2.308 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  150/5000: episode: 5, duration: 0.179s, episode steps:  28, steps per second: 156, episode reward: 36.000, mean reward:  1.286 [-2.903, 30.073], mean action: 7.857 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  176/5000: episode: 6, duration: 0.166s, episode steps:  26, steps per second: 156, episode reward: -32.730, mean reward: -1.259 [-33.000,  2.421], mean action: 5.346 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  200/5000: episode: 7, duration: 0.160s, episode steps:  24, steps per second: 150, episode reward: 41.557, mean reward:  1.732 [-2.724, 32.110], mean action: 2.458 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  228/5000: episode: 8, duration: 0.182s, episode steps:  28, steps per second: 154, episode reward: 38.865, mean reward:  1.388 [-2.385, 32.115], mean action: 3.679 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  246/5000: episode: 9, duration: 0.124s, episode steps:  18, steps per second: 145, episode reward: 41.512, mean reward:  2.306 [-2.903, 32.630], mean action: 2.444 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  268/5000: episode: 10, duration: 0.151s, episode steps:  22, steps per second: 146, episode reward: 43.954, mean reward:  1.998 [-2.004, 32.240], mean action: 1.409 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  289/5000: episode: 11, duration: 0.142s, episode steps:  21, steps per second: 148, episode reward: 41.460, mean reward:  1.974 [-2.125, 31.623], mean action: 4.095 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  312/5000: episode: 12, duration: 0.329s, episode steps:  23, steps per second:  70, episode reward: 38.476, mean reward:  1.673 [-2.802, 32.453], mean action: 4.565 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  333/5000: episode: 13, duration: 0.261s, episode steps:  21, steps per second:  81, episode reward: 42.000, mean reward:  2.000 [-2.467, 30.034], mean action: 1.571 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  358/5000: episode: 14, duration: 0.222s, episode steps:  25, steps per second: 112, episode reward: 44.038, mean reward:  1.762 [-2.165, 32.190], mean action: 3.400 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  375/5000: episode: 15, duration: 0.126s, episode steps:  17, steps per second: 135, episode reward: 44.653, mean reward:  2.627 [-2.179, 32.290], mean action: 2.235 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  394/5000: episode: 16, duration: 0.130s, episode steps:  19, steps per second: 147, episode reward: 41.318, mean reward:  2.175 [-2.543, 32.493], mean action: 5.211 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  413/5000: episode: 17, duration: 0.144s, episode steps:  19, steps per second: 132, episode reward: 41.137, mean reward:  2.165 [-2.152, 32.319], mean action: 2.895 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  429/5000: episode: 18, duration: 0.139s, episode steps:  16, steps per second: 115, episode reward: 41.062, mean reward:  2.566 [-3.000, 32.811], mean action: 4.125 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  454/5000: episode: 19, duration: 0.206s, episode steps:  25, steps per second: 122, episode reward: 44.324, mean reward:  1.773 [-2.059, 31.639], mean action: 2.560 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  474/5000: episode: 20, duration: 0.168s, episode steps:  20, steps per second: 119, episode reward: 44.429, mean reward:  2.221 [-2.264, 32.670], mean action: 3.250 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  503/5000: episode: 21, duration: 0.208s, episode steps:  29, steps per second: 140, episode reward: 44.604, mean reward:  1.538 [-2.035, 32.360], mean action: 4.621 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  532/5000: episode: 22, duration: 0.223s, episode steps:  29, steps per second: 130, episode reward: 37.382, mean reward:  1.289 [-3.000, 31.952], mean action: 3.414 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  554/5000: episode: 23, duration: 0.154s, episode steps:  22, steps per second: 143, episode reward: 34.652, mean reward:  1.575 [-3.000, 32.903], mean action: 5.455 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  589/5000: episode: 24, duration: 0.235s, episode steps:  35, steps per second: 149, episode reward: 37.605, mean reward:  1.074 [-3.000, 32.044], mean action: 5.800 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  604/5000: episode: 25, duration: 0.111s, episode steps:  15, steps per second: 135, episode reward: 41.008, mean reward:  2.734 [-2.817, 32.830], mean action: 2.933 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  629/5000: episode: 26, duration: 0.179s, episode steps:  25, steps per second: 140, episode reward: 41.916, mean reward:  1.677 [-2.323, 32.266], mean action: 3.800 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  656/5000: episode: 27, duration: 0.187s, episode steps:  27, steps per second: 145, episode reward: 38.753, mean reward:  1.435 [-2.564, 32.530], mean action: 5.296 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  689/5000: episode: 28, duration: 0.220s, episode steps:  33, steps per second: 150, episode reward: 39.000, mean reward:  1.182 [-2.570, 32.340], mean action: 5.394 [1.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  702/5000: episode: 29, duration: 0.102s, episode steps:  13, steps per second: 127, episode reward: 44.089, mean reward:  3.391 [-2.583, 32.403], mean action: 3.000 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  725/5000: episode: 30, duration: 0.164s, episode steps:  23, steps per second: 141, episode reward: 41.751, mean reward:  1.815 [-2.103, 32.150], mean action: 3.391 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  744/5000: episode: 31, duration: 0.134s, episode steps:  19, steps per second: 141, episode reward: 44.697, mean reward:  2.352 [-2.775, 32.220], mean action: 4.421 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  757/5000: episode: 32, duration: 0.105s, episode steps:  13, steps per second: 124, episode reward: 44.133, mean reward:  3.395 [-2.119, 32.325], mean action: 3.538 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  785/5000: episode: 33, duration: 0.205s, episode steps:  28, steps per second: 136, episode reward: 38.823, mean reward:  1.387 [-3.000, 32.313], mean action: 3.714 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  804/5000: episode: 34, duration: 0.141s, episode steps:  19, steps per second: 134, episode reward: 43.523, mean reward:  2.291 [-2.602, 32.371], mean action: 3.368 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  830/5000: episode: 35, duration: 0.186s, episode steps:  26, steps per second: 140, episode reward: 42.000, mean reward:  1.615 [-2.294, 32.100], mean action: 2.500 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  857/5000: episode: 36, duration: 0.185s, episode steps:  27, steps per second: 146, episode reward: 40.181, mean reward:  1.488 [-3.000, 31.545], mean action: 3.519 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  886/5000: episode: 37, duration: 0.187s, episode steps:  29, steps per second: 155, episode reward: 42.000, mean reward:  1.448 [-2.291, 33.000], mean action: 3.276 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  915/5000: episode: 38, duration: 0.189s, episode steps:  29, steps per second: 154, episode reward: 41.631, mean reward:  1.436 [-2.458, 32.640], mean action: 3.172 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  924/5000: episode: 39, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward: 47.215, mean reward:  5.246 [-0.078, 32.669], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  959/5000: episode: 40, duration: 0.241s, episode steps:  35, steps per second: 145, episode reward: 41.512, mean reward:  1.186 [-2.246, 31.884], mean action: 2.057 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  985/5000: episode: 41, duration: 0.172s, episode steps:  26, steps per second: 151, episode reward: -37.190, mean reward: -1.430 [-32.910,  2.400], mean action: 7.077 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1009/5000: episode: 42, duration: 0.241s, episode steps:  24, steps per second: 100, episode reward: 42.000, mean reward:  1.750 [-2.286, 32.630], mean action: 7.375 [0.000, 19.000],  loss: 0.024304, mae: 0.399006, mean_q: 0.474583, mean_eps: 0.000000
 1031/5000: episode: 43, duration: 0.574s, episode steps:  22, steps per second:  38, episode reward: 41.490, mean reward:  1.886 [-2.602, 32.090], mean action: 3.409 [0.000, 15.000],  loss: 0.019264, mae: 0.372340, mean_q: 0.521984, mean_eps: 0.000000
 1060/5000: episode: 44, duration: 0.528s, episode steps:  29, steps per second:  55, episode reward: 38.915, mean reward:  1.342 [-2.302, 32.590], mean action: 5.414 [0.000, 14.000],  loss: 0.022027, mae: 0.383131, mean_q: 0.586281, mean_eps: 0.000000
 1087/5000: episode: 45, duration: 0.917s, episode steps:  27, steps per second:  29, episode reward: 42.000, mean reward:  1.556 [-2.607, 32.700], mean action: 3.111 [0.000, 15.000],  loss: 0.019942, mae: 0.377660, mean_q: 0.539419, mean_eps: 0.000000
 1115/5000: episode: 46, duration: 1.338s, episode steps:  28, steps per second:  21, episode reward: 44.250, mean reward:  1.580 [-2.870, 32.086], mean action: 2.036 [0.000, 12.000],  loss: 0.021970, mae: 0.386829, mean_q: 0.518818, mean_eps: 0.000000
 1156/5000: episode: 47, duration: 1.064s, episode steps:  41, steps per second:  39, episode reward: 38.637, mean reward:  0.942 [-2.477, 32.060], mean action: 3.634 [0.000, 15.000],  loss: 0.020686, mae: 0.373602, mean_q: 0.536965, mean_eps: 0.000000
 1183/5000: episode: 48, duration: 0.736s, episode steps:  27, steps per second:  37, episode reward: 43.351, mean reward:  1.606 [-2.357, 32.020], mean action: 0.852 [0.000, 12.000],  loss: 0.022359, mae: 0.378188, mean_q: 0.548356, mean_eps: 0.000000
 1218/5000: episode: 49, duration: 0.733s, episode steps:  35, steps per second:  48, episode reward: 42.329, mean reward:  1.209 [-2.087, 32.151], mean action: 3.657 [0.000, 19.000],  loss: 0.019363, mae: 0.376096, mean_q: 0.499251, mean_eps: 0.000000
 1278/5000: episode: 50, duration: 0.906s, episode steps:  60, steps per second:  66, episode reward: -32.800, mean reward: -0.547 [-32.047,  2.257], mean action: 9.267 [0.000, 20.000],  loss: 0.019153, mae: 0.378301, mean_q: 0.469082, mean_eps: 0.000000
 1308/5000: episode: 51, duration: 0.453s, episode steps:  30, steps per second:  66, episode reward: 35.669, mean reward:  1.189 [-2.475, 32.370], mean action: 4.633 [0.000, 16.000],  loss: 0.025455, mae: 0.401210, mean_q: 0.456403, mean_eps: 0.000000
 1342/5000: episode: 52, duration: 0.442s, episode steps:  34, steps per second:  77, episode reward: 37.988, mean reward:  1.117 [-2.230, 32.090], mean action: 2.676 [0.000, 16.000],  loss: 0.021541, mae: 0.385162, mean_q: 0.477636, mean_eps: 0.000000
 1371/5000: episode: 53, duration: 0.373s, episode steps:  29, steps per second:  78, episode reward: -32.450, mean reward: -1.119 [-31.879,  2.530], mean action: 5.241 [0.000, 16.000],  loss: 0.020166, mae: 0.381581, mean_q: 0.448785, mean_eps: 0.000000
 1391/5000: episode: 54, duration: 0.277s, episode steps:  20, steps per second:  72, episode reward: 41.056, mean reward:  2.053 [-2.099, 32.570], mean action: 4.650 [0.000, 19.000],  loss: 0.025676, mae: 0.399884, mean_q: 0.443340, mean_eps: 0.000000
 1422/5000: episode: 55, duration: 0.394s, episode steps:  31, steps per second:  79, episode reward: 33.000, mean reward:  1.065 [-3.000, 32.090], mean action: 3.290 [0.000, 12.000],  loss: 0.024551, mae: 0.395850, mean_q: 0.478764, mean_eps: 0.000000
 1445/5000: episode: 56, duration: 0.345s, episode steps:  23, steps per second:  67, episode reward: -32.090, mean reward: -1.395 [-32.292,  2.480], mean action: 4.565 [0.000, 16.000],  loss: 0.017836, mae: 0.363507, mean_q: 0.501489, mean_eps: 0.000000
 1470/5000: episode: 57, duration: 0.531s, episode steps:  25, steps per second:  47, episode reward: 34.468, mean reward:  1.379 [-2.780, 32.380], mean action: 4.480 [0.000, 18.000],  loss: 0.020912, mae: 0.372200, mean_q: 0.508065, mean_eps: 0.000000
 1496/5000: episode: 58, duration: 0.343s, episode steps:  26, steps per second:  76, episode reward: 41.465, mean reward:  1.595 [-3.000, 32.170], mean action: 4.423 [1.000, 20.000],  loss: 0.017421, mae: 0.356661, mean_q: 0.529920, mean_eps: 0.000000
 1515/5000: episode: 59, duration: 0.256s, episode steps:  19, steps per second:  74, episode reward: 40.698, mean reward:  2.142 [-2.388, 31.504], mean action: 4.211 [0.000, 21.000],  loss: 0.023274, mae: 0.394455, mean_q: 0.507377, mean_eps: 0.000000
 1534/5000: episode: 60, duration: 0.258s, episode steps:  19, steps per second:  74, episode reward: 44.919, mean reward:  2.364 [-2.568, 32.159], mean action: 2.684 [0.000, 15.000],  loss: 0.020506, mae: 0.373906, mean_q: 0.503849, mean_eps: 0.000000
 1559/5000: episode: 61, duration: 0.381s, episode steps:  25, steps per second:  66, episode reward: 35.354, mean reward:  1.414 [-2.903, 32.070], mean action: 3.520 [0.000, 19.000],  loss: 0.022932, mae: 0.387805, mean_q: 0.517751, mean_eps: 0.000000
 1584/5000: episode: 62, duration: 0.321s, episode steps:  25, steps per second:  78, episode reward: 35.573, mean reward:  1.423 [-2.938, 33.000], mean action: 7.440 [0.000, 21.000],  loss: 0.023417, mae: 0.400815, mean_q: 0.467966, mean_eps: 0.000000
 1607/5000: episode: 63, duration: 0.520s, episode steps:  23, steps per second:  44, episode reward: 41.274, mean reward:  1.795 [-2.230, 31.896], mean action: 3.478 [1.000, 15.000],  loss: 0.018829, mae: 0.372953, mean_q: 0.488569, mean_eps: 0.000000
 1633/5000: episode: 64, duration: 0.375s, episode steps:  26, steps per second:  69, episode reward: 42.000, mean reward:  1.615 [-2.301, 32.190], mean action: 1.385 [0.000, 15.000],  loss: 0.021502, mae: 0.376998, mean_q: 0.458870, mean_eps: 0.000000
 1676/5000: episode: 65, duration: 0.743s, episode steps:  43, steps per second:  58, episode reward: 34.865, mean reward:  0.811 [-3.000, 32.080], mean action: 5.721 [0.000, 19.000],  loss: 0.020230, mae: 0.379421, mean_q: 0.408932, mean_eps: 0.000000
 1697/5000: episode: 66, duration: 0.277s, episode steps:  21, steps per second:  76, episode reward: 44.205, mean reward:  2.105 [-2.558, 31.605], mean action: 4.143 [0.000, 15.000],  loss: 0.018730, mae: 0.374373, mean_q: 0.445389, mean_eps: 0.000000
 1733/5000: episode: 67, duration: 0.457s, episode steps:  36, steps per second:  79, episode reward: 38.731, mean reward:  1.076 [-2.225, 32.260], mean action: 3.417 [0.000, 16.000],  loss: 0.019331, mae: 0.374601, mean_q: 0.452739, mean_eps: 0.000000
 1770/5000: episode: 68, duration: 0.478s, episode steps:  37, steps per second:  77, episode reward: -32.290, mean reward: -0.873 [-32.483,  2.570], mean action: 6.243 [1.000, 16.000],  loss: 0.018800, mae: 0.373746, mean_q: 0.469996, mean_eps: 0.000000
 1794/5000: episode: 69, duration: 0.321s, episode steps:  24, steps per second:  75, episode reward: 38.802, mean reward:  1.617 [-2.500, 32.041], mean action: 5.250 [0.000, 16.000],  loss: 0.022816, mae: 0.387007, mean_q: 0.527976, mean_eps: 0.000000
 1811/5000: episode: 70, duration: 0.230s, episode steps:  17, steps per second:  74, episode reward: 44.332, mean reward:  2.608 [-2.614, 32.390], mean action: 2.118 [0.000, 16.000],  loss: 0.021798, mae: 0.381220, mean_q: 0.575215, mean_eps: 0.000000
 1855/5000: episode: 71, duration: 0.571s, episode steps:  44, steps per second:  77, episode reward: -32.550, mean reward: -0.740 [-31.862,  3.000], mean action: 5.750 [0.000, 21.000],  loss: 0.019461, mae: 0.368939, mean_q: 0.514664, mean_eps: 0.000000
 1875/5000: episode: 72, duration: 0.350s, episode steps:  20, steps per second:  57, episode reward: 44.471, mean reward:  2.224 [-2.520, 32.280], mean action: 2.250 [0.000, 14.000],  loss: 0.022102, mae: 0.372963, mean_q: 0.534538, mean_eps: 0.000000
 1900/5000: episode: 73, duration: 0.402s, episode steps:  25, steps per second:  62, episode reward: 35.562, mean reward:  1.422 [-3.000, 32.100], mean action: 8.160 [1.000, 18.000],  loss: 0.017547, mae: 0.352026, mean_q: 0.490063, mean_eps: 0.000000
 1936/5000: episode: 74, duration: 0.453s, episode steps:  36, steps per second:  80, episode reward: 32.903, mean reward:  0.914 [-3.000, 31.933], mean action: 4.083 [0.000, 16.000],  loss: 0.022582, mae: 0.385887, mean_q: 0.469692, mean_eps: 0.000000
 1968/5000: episode: 75, duration: 0.405s, episode steps:  32, steps per second:  79, episode reward: 35.707, mean reward:  1.116 [-2.556, 32.090], mean action: 4.031 [0.000, 16.000],  loss: 0.019755, mae: 0.375944, mean_q: 0.412803, mean_eps: 0.000000
 2003/5000: episode: 76, duration: 0.442s, episode steps:  35, steps per second:  79, episode reward: 35.186, mean reward:  1.005 [-2.416, 32.138], mean action: 3.743 [0.000, 20.000],  loss: 0.019318, mae: 0.373250, mean_q: 0.457254, mean_eps: 0.000000
 2027/5000: episode: 77, duration: 0.307s, episode steps:  24, steps per second:  78, episode reward: 38.754, mean reward:  1.615 [-2.392, 31.954], mean action: 3.792 [0.000, 16.000],  loss: 0.019846, mae: 0.372125, mean_q: 0.496800, mean_eps: 0.000000
 2053/5000: episode: 78, duration: 0.344s, episode steps:  26, steps per second:  76, episode reward: 40.461, mean reward:  1.556 [-2.388, 32.132], mean action: 2.846 [0.000, 16.000],  loss: 0.017961, mae: 0.359582, mean_q: 0.468473, mean_eps: 0.000000
 2086/5000: episode: 79, duration: 0.822s, episode steps:  33, steps per second:  40, episode reward: -32.080, mean reward: -0.972 [-32.906,  2.450], mean action: 5.273 [0.000, 19.000],  loss: 0.018967, mae: 0.362821, mean_q: 0.526892, mean_eps: 0.000000
 2150/5000: episode: 80, duration: 1.308s, episode steps:  64, steps per second:  49, episode reward: -32.480, mean reward: -0.508 [-31.884,  2.360], mean action: 9.078 [0.000, 20.000],  loss: 0.020493, mae: 0.371861, mean_q: 0.499611, mean_eps: 0.000000
 2176/5000: episode: 81, duration: 1.026s, episode steps:  26, steps per second:  25, episode reward: 40.997, mean reward:  1.577 [-3.000, 32.120], mean action: 1.769 [0.000, 8.000],  loss: 0.019208, mae: 0.365738, mean_q: 0.464144, mean_eps: 0.000000
 2202/5000: episode: 82, duration: 0.480s, episode steps:  26, steps per second:  54, episode reward: 37.993, mean reward:  1.461 [-3.000, 31.256], mean action: 4.423 [0.000, 20.000],  loss: 0.018459, mae: 0.365952, mean_q: 0.461953, mean_eps: 0.000000
 2226/5000: episode: 83, duration: 0.477s, episode steps:  24, steps per second:  50, episode reward: 38.700, mean reward:  1.613 [-3.000, 32.660], mean action: 2.792 [0.000, 14.000],  loss: 0.019080, mae: 0.375533, mean_q: 0.488711, mean_eps: 0.000000
 2256/5000: episode: 84, duration: 0.515s, episode steps:  30, steps per second:  58, episode reward: 37.695, mean reward:  1.257 [-2.581, 31.764], mean action: 6.867 [0.000, 19.000],  loss: 0.023738, mae: 0.388335, mean_q: 0.491482, mean_eps: 0.000000
 2289/5000: episode: 85, duration: 0.460s, episode steps:  33, steps per second:  72, episode reward: 41.801, mean reward:  1.267 [-2.024, 31.981], mean action: 3.818 [1.000, 11.000],  loss: 0.025177, mae: 0.402237, mean_q: 0.501899, mean_eps: 0.000000
 2315/5000: episode: 86, duration: 0.765s, episode steps:  26, steps per second:  34, episode reward: 41.171, mean reward:  1.583 [-2.498, 32.160], mean action: 4.192 [0.000, 15.000],  loss: 0.020224, mae: 0.374122, mean_q: 0.472368, mean_eps: 0.000000
 2335/5000: episode: 87, duration: 0.274s, episode steps:  20, steps per second:  73, episode reward: 41.621, mean reward:  2.081 [-2.129, 31.811], mean action: 2.900 [1.000, 11.000],  loss: 0.021555, mae: 0.385007, mean_q: 0.450827, mean_eps: 0.000000
 2355/5000: episode: 88, duration: 0.307s, episode steps:  20, steps per second:  65, episode reward: 42.000, mean reward:  2.100 [-2.004, 30.911], mean action: 2.850 [0.000, 15.000],  loss: 0.018613, mae: 0.364110, mean_q: 0.457169, mean_eps: 0.000000
 2378/5000: episode: 89, duration: 0.400s, episode steps:  23, steps per second:  58, episode reward: 43.883, mean reward:  1.908 [-2.373, 32.490], mean action: 3.304 [0.000, 15.000],  loss: 0.020099, mae: 0.365481, mean_q: 0.484730, mean_eps: 0.000000
 2409/5000: episode: 90, duration: 0.515s, episode steps:  31, steps per second:  60, episode reward: 37.894, mean reward:  1.222 [-3.000, 32.090], mean action: 4.806 [0.000, 15.000],  loss: 0.021560, mae: 0.374662, mean_q: 0.469105, mean_eps: 0.000000
 2464/5000: episode: 91, duration: 0.922s, episode steps:  55, steps per second:  60, episode reward: 32.781, mean reward:  0.596 [-2.476, 31.971], mean action: 4.091 [0.000, 20.000],  loss: 0.019270, mae: 0.364593, mean_q: 0.510898, mean_eps: 0.000000
 2489/5000: episode: 92, duration: 0.342s, episode steps:  25, steps per second:  73, episode reward: 44.433, mean reward:  1.777 [-2.116, 32.090], mean action: 1.600 [0.000, 11.000],  loss: 0.021221, mae: 0.378089, mean_q: 0.454890, mean_eps: 0.000000
 2518/5000: episode: 93, duration: 0.547s, episode steps:  29, steps per second:  53, episode reward: 41.619, mean reward:  1.435 [-2.451, 31.817], mean action: 3.586 [0.000, 16.000],  loss: 0.018520, mae: 0.354084, mean_q: 0.486323, mean_eps: 0.000000
 2542/5000: episode: 94, duration: 0.349s, episode steps:  24, steps per second:  69, episode reward: 42.000, mean reward:  1.750 [-2.642, 32.270], mean action: 7.125 [0.000, 19.000],  loss: 0.019073, mae: 0.360685, mean_q: 0.502760, mean_eps: 0.000000
 2576/5000: episode: 95, duration: 0.485s, episode steps:  34, steps per second:  70, episode reward: 41.000, mean reward:  1.206 [-2.305, 32.061], mean action: 4.382 [0.000, 19.000],  loss: 0.021503, mae: 0.375215, mean_q: 0.503208, mean_eps: 0.000000
 2600/5000: episode: 96, duration: 0.356s, episode steps:  24, steps per second:  67, episode reward: 41.695, mean reward:  1.737 [-2.206, 32.470], mean action: 5.417 [0.000, 16.000],  loss: 0.021553, mae: 0.377886, mean_q: 0.472926, mean_eps: 0.000000
 2622/5000: episode: 97, duration: 0.314s, episode steps:  22, steps per second:  70, episode reward: 38.444, mean reward:  1.747 [-3.000, 32.170], mean action: 3.636 [0.000, 16.000],  loss: 0.017167, mae: 0.355020, mean_q: 0.505413, mean_eps: 0.000000
 2644/5000: episode: 98, duration: 0.313s, episode steps:  22, steps per second:  70, episode reward: 44.081, mean reward:  2.004 [-2.038, 32.250], mean action: 2.864 [0.000, 16.000],  loss: 0.018465, mae: 0.357290, mean_q: 0.499979, mean_eps: 0.000000
 2661/5000: episode: 99, duration: 0.259s, episode steps:  17, steps per second:  66, episode reward: 44.061, mean reward:  2.592 [-2.045, 32.691], mean action: 2.588 [0.000, 12.000],  loss: 0.023384, mae: 0.374977, mean_q: 0.520420, mean_eps: 0.000000
 2684/5000: episode: 100, duration: 0.323s, episode steps:  23, steps per second:  71, episode reward: 38.904, mean reward:  1.691 [-3.000, 32.354], mean action: 2.739 [0.000, 12.000],  loss: 0.024497, mae: 0.386422, mean_q: 0.494951, mean_eps: 0.000000
 2705/5000: episode: 101, duration: 0.304s, episode steps:  21, steps per second:  69, episode reward: 41.573, mean reward:  1.980 [-2.485, 31.924], mean action: 5.905 [0.000, 14.000],  loss: 0.018451, mae: 0.359762, mean_q: 0.447164, mean_eps: 0.000000
 2740/5000: episode: 102, duration: 0.487s, episode steps:  35, steps per second:  72, episode reward: 40.390, mean reward:  1.154 [-2.711, 32.300], mean action: 3.686 [0.000, 19.000],  loss: 0.020703, mae: 0.374079, mean_q: 0.505707, mean_eps: 0.000000
 2777/5000: episode: 103, duration: 0.876s, episode steps:  37, steps per second:  42, episode reward: 38.229, mean reward:  1.033 [-3.000, 32.020], mean action: 4.162 [0.000, 15.000],  loss: 0.018138, mae: 0.365272, mean_q: 0.475412, mean_eps: 0.000000
 2804/5000: episode: 104, duration: 0.742s, episode steps:  27, steps per second:  36, episode reward: 41.833, mean reward:  1.549 [-2.229, 32.567], mean action: 2.259 [0.000, 12.000],  loss: 0.018444, mae: 0.362491, mean_q: 0.528291, mean_eps: 0.000000
 2831/5000: episode: 105, duration: 1.175s, episode steps:  27, steps per second:  23, episode reward: 41.148, mean reward:  1.524 [-2.628, 31.488], mean action: 3.481 [0.000, 16.000],  loss: 0.020795, mae: 0.376998, mean_q: 0.514514, mean_eps: 0.000000
 2854/5000: episode: 106, duration: 0.513s, episode steps:  23, steps per second:  45, episode reward: 44.204, mean reward:  1.922 [-2.339, 32.100], mean action: 4.391 [0.000, 15.000],  loss: 0.023078, mae: 0.385769, mean_q: 0.515275, mean_eps: 0.000000
 2879/5000: episode: 107, duration: 0.817s, episode steps:  25, steps per second:  31, episode reward: 41.835, mean reward:  1.673 [-2.461, 31.995], mean action: 3.040 [0.000, 12.000],  loss: 0.021160, mae: 0.378917, mean_q: 0.513305, mean_eps: 0.000000
 2909/5000: episode: 108, duration: 0.599s, episode steps:  30, steps per second:  50, episode reward: 35.600, mean reward:  1.187 [-2.310, 32.300], mean action: 4.867 [0.000, 16.000],  loss: 0.025231, mae: 0.402361, mean_q: 0.465460, mean_eps: 0.000000
 2938/5000: episode: 109, duration: 0.563s, episode steps:  29, steps per second:  52, episode reward: 41.066, mean reward:  1.416 [-3.000, 32.052], mean action: 2.586 [0.000, 12.000],  loss: 0.019175, mae: 0.372363, mean_q: 0.428252, mean_eps: 0.000000
 2977/5000: episode: 110, duration: 0.706s, episode steps:  39, steps per second:  55, episode reward: 39.740, mean reward:  1.019 [-2.227, 32.250], mean action: 4.128 [0.000, 14.000],  loss: 0.018697, mae: 0.364495, mean_q: 0.445108, mean_eps: 0.000000
 2998/5000: episode: 111, duration: 0.283s, episode steps:  21, steps per second:  74, episode reward: 41.958, mean reward:  1.998 [-2.564, 32.218], mean action: 3.381 [0.000, 19.000],  loss: 0.022024, mae: 0.382761, mean_q: 0.503794, mean_eps: 0.000000
 3022/5000: episode: 112, duration: 0.453s, episode steps:  24, steps per second:  53, episode reward: 38.526, mean reward:  1.605 [-2.904, 31.956], mean action: 3.208 [0.000, 19.000],  loss: 0.022602, mae: 0.381859, mean_q: 0.504507, mean_eps: 0.000000
 3045/5000: episode: 113, duration: 0.346s, episode steps:  23, steps per second:  66, episode reward: 43.103, mean reward:  1.874 [-2.098, 32.380], mean action: 3.565 [0.000, 19.000],  loss: 0.020972, mae: 0.378078, mean_q: 0.498964, mean_eps: 0.000000
 3066/5000: episode: 114, duration: 0.274s, episode steps:  21, steps per second:  77, episode reward: 43.268, mean reward:  2.060 [-2.008, 32.376], mean action: 3.190 [0.000, 19.000],  loss: 0.021572, mae: 0.378431, mean_q: 0.536580, mean_eps: 0.000000
 3094/5000: episode: 115, duration: 0.694s, episode steps:  28, steps per second:  40, episode reward: 41.937, mean reward:  1.498 [-2.352, 32.270], mean action: 1.429 [0.000, 16.000],  loss: 0.020300, mae: 0.375465, mean_q: 0.470924, mean_eps: 0.000000
 3120/5000: episode: 116, duration: 0.430s, episode steps:  26, steps per second:  61, episode reward: 47.532, mean reward:  1.828 [-0.230, 32.033], mean action: 2.654 [0.000, 8.000],  loss: 0.025796, mae: 0.399687, mean_q: 0.506681, mean_eps: 0.000000
 3155/5000: episode: 117, duration: 0.548s, episode steps:  35, steps per second:  64, episode reward: 41.700, mean reward:  1.191 [-2.080, 32.170], mean action: 2.714 [0.000, 17.000],  loss: 0.019276, mae: 0.375495, mean_q: 0.506992, mean_eps: 0.000000
 3196/5000: episode: 118, duration: 0.533s, episode steps:  41, steps per second:  77, episode reward: 35.474, mean reward:  0.865 [-3.000, 32.040], mean action: 3.268 [0.000, 16.000],  loss: 0.021755, mae: 0.373261, mean_q: 0.482786, mean_eps: 0.000000
 3229/5000: episode: 119, duration: 0.427s, episode steps:  33, steps per second:  77, episode reward: 41.711, mean reward:  1.264 [-2.593, 32.090], mean action: 2.152 [0.000, 13.000],  loss: 0.020846, mae: 0.368874, mean_q: 0.457863, mean_eps: 0.000000
 3252/5000: episode: 120, duration: 0.306s, episode steps:  23, steps per second:  75, episode reward: 41.716, mean reward:  1.814 [-2.434, 32.320], mean action: 5.478 [0.000, 19.000],  loss: 0.017910, mae: 0.356766, mean_q: 0.464446, mean_eps: 0.000000
 3260/5000: episode: 121, duration: 0.128s, episode steps:   8, steps per second:  62, episode reward: 47.168, mean reward:  5.896 [ 0.350, 32.872], mean action: 2.000 [0.000, 14.000],  loss: 0.018638, mae: 0.364996, mean_q: 0.497544, mean_eps: 0.000000
 3280/5000: episode: 122, duration: 0.269s, episode steps:  20, steps per second:  74, episode reward: 47.132, mean reward:  2.357 [-0.030, 32.110], mean action: 2.100 [0.000, 3.000],  loss: 0.016994, mae: 0.366767, mean_q: 0.502662, mean_eps: 0.000000
 3317/5000: episode: 123, duration: 0.475s, episode steps:  37, steps per second:  78, episode reward: 40.598, mean reward:  1.097 [-3.000, 32.130], mean action: 3.973 [0.000, 19.000],  loss: 0.020038, mae: 0.376070, mean_q: 0.510630, mean_eps: 0.000000
 3347/5000: episode: 124, duration: 0.398s, episode steps:  30, steps per second:  75, episode reward: 35.904, mean reward:  1.197 [-3.000, 31.924], mean action: 4.700 [0.000, 13.000],  loss: 0.017630, mae: 0.356827, mean_q: 0.551898, mean_eps: 0.000000
 3374/5000: episode: 125, duration: 0.353s, episode steps:  27, steps per second:  77, episode reward: 47.535, mean reward:  1.761 [-0.312, 32.260], mean action: 3.074 [1.000, 15.000],  loss: 0.019545, mae: 0.375715, mean_q: 0.519823, mean_eps: 0.000000
 3396/5000: episode: 126, duration: 0.283s, episode steps:  22, steps per second:  78, episode reward: 44.900, mean reward:  2.041 [-3.000, 32.540], mean action: 3.955 [1.000, 19.000],  loss: 0.018591, mae: 0.377245, mean_q: 0.431759, mean_eps: 0.000000
 3418/5000: episode: 127, duration: 0.760s, episode steps:  22, steps per second:  29, episode reward: 47.292, mean reward:  2.150 [-0.500, 32.710], mean action: 5.636 [0.000, 19.000],  loss: 0.017715, mae: 0.360310, mean_q: 0.426179, mean_eps: 0.000000
 3450/5000: episode: 128, duration: 0.509s, episode steps:  32, steps per second:  63, episode reward: 41.507, mean reward:  1.297 [-2.735, 32.470], mean action: 3.719 [0.000, 15.000],  loss: 0.020968, mae: 0.370346, mean_q: 0.478910, mean_eps: 0.000000
 3479/5000: episode: 129, duration: 0.457s, episode steps:  29, steps per second:  63, episode reward: 38.826, mean reward:  1.339 [-2.389, 32.410], mean action: 3.034 [0.000, 19.000],  loss: 0.022224, mae: 0.382262, mean_q: 0.516196, mean_eps: 0.000000
 3512/5000: episode: 130, duration: 0.519s, episode steps:  33, steps per second:  64, episode reward: 40.951, mean reward:  1.241 [-2.397, 31.750], mean action: 2.758 [0.000, 19.000],  loss: 0.020058, mae: 0.371069, mean_q: 0.511491, mean_eps: 0.000000
 3550/5000: episode: 131, duration: 0.507s, episode steps:  38, steps per second:  75, episode reward: -34.660, mean reward: -0.912 [-32.820,  3.000], mean action: 5.579 [0.000, 19.000],  loss: 0.021425, mae: 0.376700, mean_q: 0.468532, mean_eps: 0.000000
 3566/5000: episode: 132, duration: 0.213s, episode steps:  16, steps per second:  75, episode reward: 42.000, mean reward:  2.625 [-2.494, 32.160], mean action: 5.875 [0.000, 16.000],  loss: 0.019576, mae: 0.373101, mean_q: 0.434483, mean_eps: 0.000000
 3617/5000: episode: 133, duration: 0.633s, episode steps:  51, steps per second:  81, episode reward: 35.009, mean reward:  0.686 [-2.255, 32.490], mean action: 6.392 [0.000, 16.000],  loss: 0.020751, mae: 0.377526, mean_q: 0.426796, mean_eps: 0.000000
 3642/5000: episode: 134, duration: 0.327s, episode steps:  25, steps per second:  76, episode reward: 35.697, mean reward:  1.428 [-3.000, 32.076], mean action: 5.400 [0.000, 20.000],  loss: 0.017662, mae: 0.368348, mean_q: 0.413430, mean_eps: 0.000000
 3673/5000: episode: 135, duration: 0.526s, episode steps:  31, steps per second:  59, episode reward: 40.078, mean reward:  1.293 [-2.403, 32.307], mean action: 3.194 [0.000, 20.000],  loss: 0.021817, mae: 0.383084, mean_q: 0.466701, mean_eps: 0.000000
 3686/5000: episode: 136, duration: 0.308s, episode steps:  13, steps per second:  42, episode reward: 43.570, mean reward:  3.352 [-3.000, 33.000], mean action: 5.692 [0.000, 19.000],  loss: 0.018785, mae: 0.371889, mean_q: 0.454042, mean_eps: 0.000000
 3710/5000: episode: 137, duration: 0.321s, episode steps:  24, steps per second:  75, episode reward: 44.900, mean reward:  1.871 [-2.217, 32.040], mean action: 1.208 [0.000, 16.000],  loss: 0.018235, mae: 0.366361, mean_q: 0.424711, mean_eps: 0.000000
 3750/5000: episode: 138, duration: 0.500s, episode steps:  40, steps per second:  80, episode reward: 40.610, mean reward:  1.015 [-2.291, 31.379], mean action: 3.875 [0.000, 20.000],  loss: 0.019462, mae: 0.375652, mean_q: 0.407660, mean_eps: 0.000000
 3770/5000: episode: 139, duration: 0.259s, episode steps:  20, steps per second:  77, episode reward: 39.000, mean reward:  1.950 [-2.464, 32.360], mean action: 3.450 [0.000, 16.000],  loss: 0.017218, mae: 0.357774, mean_q: 0.458655, mean_eps: 0.000000
 3789/5000: episode: 140, duration: 0.264s, episode steps:  19, steps per second:  72, episode reward: 42.000, mean reward:  2.211 [-2.154, 33.000], mean action: 2.895 [0.000, 16.000],  loss: 0.024715, mae: 0.389087, mean_q: 0.499924, mean_eps: 0.000000
 3834/5000: episode: 141, duration: 0.560s, episode steps:  45, steps per second:  80, episode reward: 35.801, mean reward:  0.796 [-2.688, 32.280], mean action: 2.067 [0.000, 15.000],  loss: 0.019898, mae: 0.371981, mean_q: 0.488712, mean_eps: 0.000000
 3864/5000: episode: 142, duration: 0.409s, episode steps:  30, steps per second:  73, episode reward: 39.000, mean reward:  1.300 [-2.630, 32.090], mean action: 4.633 [0.000, 16.000],  loss: 0.020974, mae: 0.383134, mean_q: 0.465801, mean_eps: 0.000000
 3889/5000: episode: 143, duration: 0.517s, episode steps:  25, steps per second:  48, episode reward: 38.982, mean reward:  1.559 [-2.486, 32.052], mean action: 3.560 [0.000, 16.000],  loss: 0.023097, mae: 0.382904, mean_q: 0.445281, mean_eps: 0.000000
 3910/5000: episode: 144, duration: 0.295s, episode steps:  21, steps per second:  71, episode reward: 43.799, mean reward:  2.086 [-2.764, 32.090], mean action: 6.476 [0.000, 19.000],  loss: 0.020590, mae: 0.365902, mean_q: 0.482916, mean_eps: 0.000000
 3936/5000: episode: 145, duration: 0.367s, episode steps:  26, steps per second:  71, episode reward: 41.107, mean reward:  1.581 [-2.153, 32.170], mean action: 2.654 [0.000, 11.000],  loss: 0.017530, mae: 0.360266, mean_q: 0.492830, mean_eps: 0.000000
 3967/5000: episode: 146, duration: 0.686s, episode steps:  31, steps per second:  45, episode reward: 37.583, mean reward:  1.212 [-2.713, 32.008], mean action: 3.065 [0.000, 18.000],  loss: 0.017525, mae: 0.368499, mean_q: 0.438749, mean_eps: 0.000000
 3997/5000: episode: 147, duration: 0.693s, episode steps:  30, steps per second:  43, episode reward: 39.000, mean reward:  1.300 [-3.000, 32.430], mean action: 4.633 [0.000, 15.000],  loss: 0.019509, mae: 0.374431, mean_q: 0.442288, mean_eps: 0.000000
 4031/5000: episode: 148, duration: 0.643s, episode steps:  34, steps per second:  53, episode reward: 41.278, mean reward:  1.214 [-2.425, 32.020], mean action: 4.412 [0.000, 15.000],  loss: 0.019021, mae: 0.364147, mean_q: 0.474678, mean_eps: 0.000000
 4057/5000: episode: 149, duration: 0.501s, episode steps:  26, steps per second:  52, episode reward: 41.580, mean reward:  1.599 [-2.630, 32.270], mean action: 2.000 [0.000, 11.000],  loss: 0.022698, mae: 0.387551, mean_q: 0.491701, mean_eps: 0.000000
 4083/5000: episode: 150, duration: 0.607s, episode steps:  26, steps per second:  43, episode reward: 35.759, mean reward:  1.375 [-2.601, 32.650], mean action: 4.923 [0.000, 15.000],  loss: 0.020503, mae: 0.381338, mean_q: 0.453312, mean_eps: 0.000000
 4113/5000: episode: 151, duration: 0.514s, episode steps:  30, steps per second:  58, episode reward: 39.000, mean reward:  1.300 [-3.000, 32.600], mean action: 3.567 [0.000, 12.000],  loss: 0.018907, mae: 0.369145, mean_q: 0.490881, mean_eps: 0.000000
 4145/5000: episode: 152, duration: 0.714s, episode steps:  32, steps per second:  45, episode reward: 32.547, mean reward:  1.017 [-2.493, 31.688], mean action: 4.750 [0.000, 19.000],  loss: 0.019656, mae: 0.373343, mean_q: 0.436561, mean_eps: 0.000000
 4172/5000: episode: 153, duration: 0.787s, episode steps:  27, steps per second:  34, episode reward: 41.533, mean reward:  1.538 [-2.346, 32.110], mean action: 3.889 [1.000, 19.000],  loss: 0.019785, mae: 0.382147, mean_q: 0.460006, mean_eps: 0.000000
 4209/5000: episode: 154, duration: 0.601s, episode steps:  37, steps per second:  62, episode reward: 35.311, mean reward:  0.954 [-3.000, 32.249], mean action: 8.486 [0.000, 20.000],  loss: 0.020650, mae: 0.387228, mean_q: 0.452635, mean_eps: 0.000000
 4238/5000: episode: 155, duration: 0.392s, episode steps:  29, steps per second:  74, episode reward: 32.363, mean reward:  1.116 [-3.000, 32.734], mean action: 7.276 [1.000, 15.000],  loss: 0.022007, mae: 0.378714, mean_q: 0.548494, mean_eps: 0.000000
 4254/5000: episode: 156, duration: 0.228s, episode steps:  16, steps per second:  70, episode reward: 41.372, mean reward:  2.586 [-2.197, 32.190], mean action: 4.938 [1.000, 15.000],  loss: 0.018850, mae: 0.366782, mean_q: 0.542516, mean_eps: 0.000000
 4307/5000: episode: 157, duration: 0.685s, episode steps:  53, steps per second:  77, episode reward: -37.830, mean reward: -0.714 [-32.187,  2.290], mean action: 7.264 [0.000, 21.000],  loss: 0.020071, mae: 0.380240, mean_q: 0.488852, mean_eps: 0.000000
 4341/5000: episode: 158, duration: 0.436s, episode steps:  34, steps per second:  78, episode reward: 38.521, mean reward:  1.133 [-2.708, 32.570], mean action: 5.353 [0.000, 13.000],  loss: 0.017496, mae: 0.355491, mean_q: 0.509572, mean_eps: 0.000000
 4362/5000: episode: 159, duration: 0.272s, episode steps:  21, steps per second:  77, episode reward: 41.825, mean reward:  1.992 [-3.000, 32.007], mean action: 3.476 [0.000, 11.000],  loss: 0.018636, mae: 0.362046, mean_q: 0.466815, mean_eps: 0.000000
 4389/5000: episode: 160, duration: 0.347s, episode steps:  27, steps per second:  78, episode reward: 36.000, mean reward:  1.333 [-2.940, 32.150], mean action: 3.667 [0.000, 11.000],  loss: 0.018966, mae: 0.382037, mean_q: 0.465006, mean_eps: 0.000000
 4410/5000: episode: 161, duration: 0.276s, episode steps:  21, steps per second:  76, episode reward: 42.000, mean reward:  2.000 [-2.213, 32.170], mean action: 4.905 [0.000, 19.000],  loss: 0.023478, mae: 0.396686, mean_q: 0.498544, mean_eps: 0.000000
 4449/5000: episode: 162, duration: 0.632s, episode steps:  39, steps per second:  62, episode reward: 41.070, mean reward:  1.053 [-2.211, 32.461], mean action: 2.923 [0.000, 19.000],  loss: 0.018805, mae: 0.377532, mean_q: 0.483300, mean_eps: 0.000000
 4484/5000: episode: 163, duration: 0.445s, episode steps:  35, steps per second:  79, episode reward: 38.941, mean reward:  1.113 [-3.000, 32.590], mean action: 3.429 [0.000, 20.000],  loss: 0.024337, mae: 0.400996, mean_q: 0.523630, mean_eps: 0.000000
 4513/5000: episode: 164, duration: 0.609s, episode steps:  29, steps per second:  48, episode reward: 43.688, mean reward:  1.506 [-2.281, 32.210], mean action: 5.621 [0.000, 20.000],  loss: 0.022291, mae: 0.390308, mean_q: 0.525670, mean_eps: 0.000000
 4568/5000: episode: 165, duration: 0.920s, episode steps:  55, steps per second:  60, episode reward: 41.600, mean reward:  0.756 [-2.528, 32.140], mean action: 4.327 [0.000, 20.000],  loss: 0.021764, mae: 0.390953, mean_q: 0.458773, mean_eps: 0.000000
 4597/5000: episode: 166, duration: 0.370s, episode steps:  29, steps per second:  78, episode reward: 38.213, mean reward:  1.318 [-3.000, 31.703], mean action: 4.483 [0.000, 19.000],  loss: 0.018875, mae: 0.372229, mean_q: 0.530592, mean_eps: 0.000000
 4621/5000: episode: 167, duration: 0.308s, episode steps:  24, steps per second:  78, episode reward: 35.529, mean reward:  1.480 [-3.000, 32.199], mean action: 2.375 [0.000, 12.000],  loss: 0.019820, mae: 0.379438, mean_q: 0.513991, mean_eps: 0.000000
 4640/5000: episode: 168, duration: 0.246s, episode steps:  19, steps per second:  77, episode reward: 39.000, mean reward:  2.053 [-2.841, 32.020], mean action: 3.526 [0.000, 19.000],  loss: 0.022042, mae: 0.397067, mean_q: 0.543767, mean_eps: 0.000000
 4652/5000: episode: 169, duration: 0.167s, episode steps:  12, steps per second:  72, episode reward: 44.918, mean reward:  3.743 [-2.099, 32.128], mean action: 2.333 [0.000, 11.000],  loss: 0.024160, mae: 0.397582, mean_q: 0.482546, mean_eps: 0.000000
 4685/5000: episode: 170, duration: 0.419s, episode steps:  33, steps per second:  79, episode reward: 46.339, mean reward:  1.404 [-0.450, 32.230], mean action: 1.667 [0.000, 13.000],  loss: 0.019125, mae: 0.375472, mean_q: 0.471011, mean_eps: 0.000000
 4715/5000: episode: 171, duration: 0.387s, episode steps:  30, steps per second:  78, episode reward: 40.811, mean reward:  1.360 [-3.000, 32.280], mean action: 2.967 [0.000, 13.000],  loss: 0.019578, mae: 0.377597, mean_q: 0.470541, mean_eps: 0.000000
 4776/5000: episode: 172, duration: 0.758s, episode steps:  61, steps per second:  80, episode reward: 46.470, mean reward:  0.762 [-0.790, 32.216], mean action: 2.180 [0.000, 13.000],  loss: 0.020797, mae: 0.386251, mean_q: 0.498064, mean_eps: 0.000000
 4792/5000: episode: 173, duration: 0.215s, episode steps:  16, steps per second:  75, episode reward: 42.000, mean reward:  2.625 [-2.471, 32.970], mean action: 3.688 [0.000, 16.000],  loss: 0.020375, mae: 0.396363, mean_q: 0.493320, mean_eps: 0.000000
 4820/5000: episode: 174, duration: 0.355s, episode steps:  28, steps per second:  79, episode reward: 39.000, mean reward:  1.393 [-3.000, 32.280], mean action: 4.607 [0.000, 16.000],  loss: 0.017959, mae: 0.390013, mean_q: 0.464800, mean_eps: 0.000000
 4848/5000: episode: 175, duration: 0.411s, episode steps:  28, steps per second:  68, episode reward: 43.817, mean reward:  1.565 [-2.710, 32.280], mean action: 2.071 [0.000, 20.000],  loss: 0.019379, mae: 0.386293, mean_q: 0.497013, mean_eps: 0.000000
 4869/5000: episode: 176, duration: 0.416s, episode steps:  21, steps per second:  51, episode reward: 38.739, mean reward:  1.845 [-3.000, 32.380], mean action: 3.476 [0.000, 16.000],  loss: 0.020210, mae: 0.380344, mean_q: 0.482134, mean_eps: 0.000000
 4909/5000: episode: 177, duration: 0.683s, episode steps:  40, steps per second:  59, episode reward: 41.470, mean reward:  1.037 [-2.487, 32.272], mean action: 7.075 [0.000, 20.000],  loss: 0.020696, mae: 0.381310, mean_q: 0.551252, mean_eps: 0.000000
 4936/5000: episode: 178, duration: 0.378s, episode steps:  27, steps per second:  71, episode reward: 38.678, mean reward:  1.433 [-2.673, 32.262], mean action: 3.370 [0.000, 16.000],  loss: 0.021975, mae: 0.385855, mean_q: 0.488731, mean_eps: 0.000000
 4964/5000: episode: 179, duration: 0.359s, episode steps:  28, steps per second:  78, episode reward: 35.272, mean reward:  1.260 [-3.000, 32.070], mean action: 4.679 [0.000, 16.000],  loss: 0.018210, mae: 0.369789, mean_q: 0.480556, mean_eps: 0.000000
 4983/5000: episode: 180, duration: 0.276s, episode steps:  19, steps per second:  69, episode reward: 38.670, mean reward:  2.035 [-2.679, 31.940], mean action: 2.737 [0.000, 19.000],  loss: 0.017643, mae: 0.355741, mean_q: 0.473205, mean_eps: 0.000000
done, took 75.174 seconds
DQN Evaluation: 4260 victories out of 5054 episodes
Training for 5000 steps ...
   18/5000: episode: 1, duration: 0.156s, episode steps:  18, steps per second: 115, episode reward: 37.798, mean reward:  2.100 [-2.500, 32.450], mean action: 5.278 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   52/5000: episode: 2, duration: 0.364s, episode steps:  34, steps per second:  93, episode reward: 38.448, mean reward:  1.131 [-2.148, 32.579], mean action: 2.324 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  101/5000: episode: 3, duration: 0.377s, episode steps:  49, steps per second: 130, episode reward: 35.162, mean reward:  0.718 [-2.820, 32.170], mean action: 3.837 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  128/5000: episode: 4, duration: 0.215s, episode steps:  27, steps per second: 125, episode reward: -38.740, mean reward: -1.435 [-32.424,  3.061], mean action: 5.926 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  143/5000: episode: 5, duration: 0.132s, episode steps:  15, steps per second: 114, episode reward: 44.081, mean reward:  2.939 [-3.000, 33.000], mean action: 2.733 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  166/5000: episode: 6, duration: 0.201s, episode steps:  23, steps per second: 115, episode reward: -35.280, mean reward: -1.534 [-31.754,  2.938], mean action: 6.217 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  197/5000: episode: 7, duration: 0.240s, episode steps:  31, steps per second: 129, episode reward: 32.814, mean reward:  1.059 [-3.000, 32.130], mean action: 5.129 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  228/5000: episode: 8, duration: 0.241s, episode steps:  31, steps per second: 128, episode reward: 30.000, mean reward:  0.968 [-2.540, 30.916], mean action: 6.968 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  254/5000: episode: 9, duration: 0.212s, episode steps:  26, steps per second: 123, episode reward: -32.640, mean reward: -1.255 [-32.496,  2.210], mean action: 5.615 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  286/5000: episode: 10, duration: 0.197s, episode steps:  32, steps per second: 162, episode reward: -32.100, mean reward: -1.003 [-32.550,  2.903], mean action: 7.531 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  309/5000: episode: 11, duration: 0.149s, episode steps:  23, steps per second: 154, episode reward: 37.839, mean reward:  1.645 [-2.322, 32.810], mean action: 5.826 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  340/5000: episode: 12, duration: 0.194s, episode steps:  31, steps per second: 160, episode reward: 32.226, mean reward:  1.040 [-2.343, 32.520], mean action: 6.806 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  362/5000: episode: 13, duration: 0.154s, episode steps:  22, steps per second: 142, episode reward: -35.910, mean reward: -1.632 [-30.347,  2.237], mean action: 5.136 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  384/5000: episode: 14, duration: 0.152s, episode steps:  22, steps per second: 144, episode reward: 34.890, mean reward:  1.586 [-2.982, 32.220], mean action: 4.091 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  403/5000: episode: 15, duration: 0.165s, episode steps:  19, steps per second: 115, episode reward: 41.188, mean reward:  2.168 [-3.000, 32.090], mean action: 5.211 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  424/5000: episode: 16, duration: 0.148s, episode steps:  21, steps per second: 142, episode reward: 37.871, mean reward:  1.803 [-2.558, 33.000], mean action: 4.619 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  444/5000: episode: 17, duration: 0.142s, episode steps:  20, steps per second: 141, episode reward: 38.813, mean reward:  1.941 [-2.420, 32.430], mean action: 7.000 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  466/5000: episode: 18, duration: 0.147s, episode steps:  22, steps per second: 150, episode reward: -36.000, mean reward: -1.636 [-32.323,  2.431], mean action: 9.318 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  502/5000: episode: 19, duration: 0.218s, episode steps:  36, steps per second: 165, episode reward: 32.436, mean reward:  0.901 [-2.421, 32.350], mean action: 4.944 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  526/5000: episode: 20, duration: 0.154s, episode steps:  24, steps per second: 155, episode reward: 35.356, mean reward:  1.473 [-3.000, 32.902], mean action: 4.333 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  552/5000: episode: 21, duration: 0.167s, episode steps:  26, steps per second: 156, episode reward: 32.617, mean reward:  1.254 [-2.595, 31.697], mean action: 5.308 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  583/5000: episode: 22, duration: 0.187s, episode steps:  31, steps per second: 165, episode reward: -35.640, mean reward: -1.150 [-31.842,  2.708], mean action: 8.290 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  604/5000: episode: 23, duration: 0.137s, episode steps:  21, steps per second: 153, episode reward: -33.000, mean reward: -1.571 [-32.187,  2.350], mean action: 3.810 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  620/5000: episode: 24, duration: 0.109s, episode steps:  16, steps per second: 147, episode reward: 40.679, mean reward:  2.542 [-2.361, 32.085], mean action: 4.250 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  638/5000: episode: 25, duration: 0.125s, episode steps:  18, steps per second: 144, episode reward: 38.395, mean reward:  2.133 [-2.478, 32.170], mean action: 3.444 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  659/5000: episode: 26, duration: 0.131s, episode steps:  21, steps per second: 161, episode reward: 35.626, mean reward:  1.696 [-3.000, 32.160], mean action: 6.238 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  689/5000: episode: 27, duration: 0.193s, episode steps:  30, steps per second: 155, episode reward: -32.260, mean reward: -1.075 [-31.820,  2.699], mean action: 3.667 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  711/5000: episode: 28, duration: 0.145s, episode steps:  22, steps per second: 152, episode reward: 35.194, mean reward:  1.600 [-2.495, 32.170], mean action: 5.227 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  725/5000: episode: 29, duration: 0.106s, episode steps:  14, steps per second: 132, episode reward: 46.507, mean reward:  3.322 [-0.312, 32.090], mean action: 2.571 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  748/5000: episode: 30, duration: 0.149s, episode steps:  23, steps per second: 154, episode reward: 33.000, mean reward:  1.435 [-2.900, 32.340], mean action: 5.043 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  773/5000: episode: 31, duration: 0.163s, episode steps:  25, steps per second: 154, episode reward: 35.148, mean reward:  1.406 [-2.574, 31.778], mean action: 5.800 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  794/5000: episode: 32, duration: 0.142s, episode steps:  21, steps per second: 147, episode reward: 42.962, mean reward:  2.046 [-2.585, 32.415], mean action: 3.762 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  815/5000: episode: 33, duration: 0.129s, episode steps:  21, steps per second: 163, episode reward: -36.000, mean reward: -1.714 [-32.506,  2.535], mean action: 7.524 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  838/5000: episode: 34, duration: 0.157s, episode steps:  23, steps per second: 147, episode reward: 37.405, mean reward:  1.626 [-2.277, 32.500], mean action: 4.130 [1.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  867/5000: episode: 35, duration: 0.187s, episode steps:  29, steps per second: 155, episode reward: 37.877, mean reward:  1.306 [-2.310, 32.162], mean action: 4.310 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  892/5000: episode: 36, duration: 0.158s, episode steps:  25, steps per second: 158, episode reward: 32.481, mean reward:  1.299 [-2.938, 32.864], mean action: 7.440 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  910/5000: episode: 37, duration: 0.123s, episode steps:  18, steps per second: 146, episode reward: 39.000, mean reward:  2.167 [-2.328, 32.220], mean action: 4.444 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  937/5000: episode: 38, duration: 0.177s, episode steps:  27, steps per second: 152, episode reward: 41.784, mean reward:  1.548 [-2.326, 32.110], mean action: 2.296 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  957/5000: episode: 39, duration: 0.133s, episode steps:  20, steps per second: 150, episode reward: 33.000, mean reward:  1.650 [-2.537, 30.530], mean action: 3.200 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  982/5000: episode: 40, duration: 0.171s, episode steps:  25, steps per second: 147, episode reward: 35.128, mean reward:  1.405 [-2.463, 32.613], mean action: 2.960 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1003/5000: episode: 41, duration: 0.162s, episode steps:  21, steps per second: 129, episode reward: 41.277, mean reward:  1.966 [-2.269, 32.015], mean action: 3.952 [1.000, 14.000],  loss: 0.027246, mae: 0.417864, mean_q: 0.374310, mean_eps: 0.000000
 1028/5000: episode: 42, duration: 0.352s, episode steps:  25, steps per second:  71, episode reward: 35.453, mean reward:  1.418 [-3.000, 32.050], mean action: 6.160 [0.000, 15.000],  loss: 0.019660, mae: 0.372252, mean_q: 0.455252, mean_eps: 0.000000
 1059/5000: episode: 43, duration: 0.419s, episode steps:  31, steps per second:  74, episode reward: 32.611, mean reward:  1.052 [-2.407, 31.834], mean action: 8.258 [0.000, 15.000],  loss: 0.019169, mae: 0.373095, mean_q: 0.463795, mean_eps: 0.000000
 1097/5000: episode: 44, duration: 0.484s, episode steps:  38, steps per second:  78, episode reward: 37.552, mean reward:  0.988 [-2.252, 32.373], mean action: 3.500 [0.000, 15.000],  loss: 0.018797, mae: 0.368037, mean_q: 0.483315, mean_eps: 0.000000
 1135/5000: episode: 45, duration: 0.500s, episode steps:  38, steps per second:  76, episode reward: -35.290, mean reward: -0.929 [-32.063,  2.504], mean action: 6.579 [0.000, 19.000],  loss: 0.020950, mae: 0.380773, mean_q: 0.484834, mean_eps: 0.000000
 1153/5000: episode: 46, duration: 0.241s, episode steps:  18, steps per second:  75, episode reward: 38.876, mean reward:  2.160 [-2.401, 32.140], mean action: 4.056 [0.000, 15.000],  loss: 0.024073, mae: 0.385913, mean_q: 0.475365, mean_eps: 0.000000
 1185/5000: episode: 47, duration: 0.567s, episode steps:  32, steps per second:  56, episode reward: 33.000, mean reward:  1.031 [-2.648, 32.260], mean action: 4.531 [0.000, 16.000],  loss: 0.019182, mae: 0.368548, mean_q: 0.483090, mean_eps: 0.000000
 1206/5000: episode: 48, duration: 1.117s, episode steps:  21, steps per second:  19, episode reward: 35.274, mean reward:  1.680 [-2.466, 31.584], mean action: 4.333 [0.000, 15.000],  loss: 0.017770, mae: 0.366294, mean_q: 0.432637, mean_eps: 0.000000
 1225/5000: episode: 49, duration: 0.286s, episode steps:  19, steps per second:  66, episode reward: 35.273, mean reward:  1.856 [-3.000, 32.133], mean action: 4.526 [0.000, 16.000],  loss: 0.019458, mae: 0.368351, mean_q: 0.451133, mean_eps: 0.000000
 1277/5000: episode: 50, duration: 0.676s, episode steps:  52, steps per second:  77, episode reward: 38.342, mean reward:  0.737 [-2.359, 32.252], mean action: 2.212 [0.000, 16.000],  loss: 0.022785, mae: 0.382902, mean_q: 0.473408, mean_eps: 0.000000
 1307/5000: episode: 51, duration: 0.529s, episode steps:  30, steps per second:  57, episode reward: -32.340, mean reward: -1.078 [-32.077,  3.000], mean action: 4.267 [0.000, 20.000],  loss: 0.020269, mae: 0.379371, mean_q: 0.462826, mean_eps: 0.000000
 1331/5000: episode: 52, duration: 0.489s, episode steps:  24, steps per second:  49, episode reward: 34.336, mean reward:  1.431 [-2.368, 32.730], mean action: 2.375 [0.000, 9.000],  loss: 0.018442, mae: 0.377409, mean_q: 0.474781, mean_eps: 0.000000
 1356/5000: episode: 53, duration: 0.364s, episode steps:  25, steps per second:  69, episode reward: 37.995, mean reward:  1.520 [-2.398, 32.021], mean action: 5.960 [1.000, 20.000],  loss: 0.025969, mae: 0.407414, mean_q: 0.512722, mean_eps: 0.000000
 1381/5000: episode: 54, duration: 0.408s, episode steps:  25, steps per second:  61, episode reward: 32.361, mean reward:  1.294 [-3.000, 32.429], mean action: 4.920 [0.000, 19.000],  loss: 0.023611, mae: 0.392821, mean_q: 0.498260, mean_eps: 0.000000
 1408/5000: episode: 55, duration: 0.421s, episode steps:  27, steps per second:  64, episode reward: 32.459, mean reward:  1.202 [-2.711, 32.812], mean action: 3.370 [0.000, 9.000],  loss: 0.018278, mae: 0.361431, mean_q: 0.501058, mean_eps: 0.000000
 1434/5000: episode: 56, duration: 0.852s, episode steps:  26, steps per second:  31, episode reward: 33.000, mean reward:  1.269 [-3.000, 33.000], mean action: 2.885 [0.000, 12.000],  loss: 0.018208, mae: 0.365412, mean_q: 0.525060, mean_eps: 0.000000
 1464/5000: episode: 57, duration: 0.799s, episode steps:  30, steps per second:  38, episode reward: 34.959, mean reward:  1.165 [-2.412, 32.150], mean action: 4.500 [0.000, 13.000],  loss: 0.018775, mae: 0.369020, mean_q: 0.462825, mean_eps: 0.000000
 1485/5000: episode: 58, duration: 0.676s, episode steps:  21, steps per second:  31, episode reward: 33.000, mean reward:  1.571 [-3.000, 29.929], mean action: 2.476 [0.000, 11.000],  loss: 0.017438, mae: 0.363229, mean_q: 0.464252, mean_eps: 0.000000
 1507/5000: episode: 59, duration: 0.636s, episode steps:  22, steps per second:  35, episode reward: 35.903, mean reward:  1.632 [-3.000, 32.123], mean action: 4.136 [0.000, 12.000],  loss: 0.020619, mae: 0.377629, mean_q: 0.497553, mean_eps: 0.000000
 1525/5000: episode: 60, duration: 0.389s, episode steps:  18, steps per second:  46, episode reward: 38.677, mean reward:  2.149 [-2.551, 31.897], mean action: 4.722 [1.000, 13.000],  loss: 0.019005, mae: 0.371751, mean_q: 0.495106, mean_eps: 0.000000
 1552/5000: episode: 61, duration: 0.412s, episode steps:  27, steps per second:  66, episode reward: 43.854, mean reward:  1.624 [-2.070, 32.099], mean action: 2.815 [0.000, 14.000],  loss: 0.021021, mae: 0.377560, mean_q: 0.513345, mean_eps: 0.000000
 1578/5000: episode: 62, duration: 0.372s, episode steps:  26, steps per second:  70, episode reward: -32.090, mean reward: -1.234 [-31.711,  2.350], mean action: 6.962 [0.000, 14.000],  loss: 0.018273, mae: 0.370714, mean_q: 0.476646, mean_eps: 0.000000
 1596/5000: episode: 63, duration: 0.263s, episode steps:  18, steps per second:  68, episode reward: 38.365, mean reward:  2.131 [-2.435, 32.070], mean action: 6.111 [0.000, 20.000],  loss: 0.016842, mae: 0.369537, mean_q: 0.515860, mean_eps: 0.000000
 1619/5000: episode: 64, duration: 0.310s, episode steps:  23, steps per second:  74, episode reward: 38.785, mean reward:  1.686 [-2.342, 32.445], mean action: 3.957 [0.000, 12.000],  loss: 0.023065, mae: 0.400287, mean_q: 0.503386, mean_eps: 0.000000
 1637/5000: episode: 65, duration: 0.246s, episode steps:  18, steps per second:  73, episode reward: 38.613, mean reward:  2.145 [-2.481, 32.613], mean action: 4.889 [0.000, 14.000],  loss: 0.021369, mae: 0.403808, mean_q: 0.461334, mean_eps: 0.000000
 1651/5000: episode: 66, duration: 0.195s, episode steps:  14, steps per second:  72, episode reward: 40.446, mean reward:  2.889 [-2.505, 32.130], mean action: 5.143 [0.000, 13.000],  loss: 0.020422, mae: 0.392906, mean_q: 0.419424, mean_eps: 0.000000
 1676/5000: episode: 67, duration: 0.322s, episode steps:  25, steps per second:  78, episode reward: 35.287, mean reward:  1.411 [-2.371, 33.000], mean action: 2.440 [0.000, 9.000],  loss: 0.019664, mae: 0.377892, mean_q: 0.462466, mean_eps: 0.000000
 1705/5000: episode: 68, duration: 0.731s, episode steps:  29, steps per second:  40, episode reward: 34.895, mean reward:  1.203 [-2.738, 31.907], mean action: 4.379 [0.000, 14.000],  loss: 0.017230, mae: 0.373649, mean_q: 0.476085, mean_eps: 0.000000
 1720/5000: episode: 69, duration: 0.326s, episode steps:  15, steps per second:  46, episode reward: 44.165, mean reward:  2.944 [-2.004, 33.000], mean action: 3.600 [0.000, 15.000],  loss: 0.015805, mae: 0.356540, mean_q: 0.489959, mean_eps: 0.000000
 1736/5000: episode: 70, duration: 0.306s, episode steps:  16, steps per second:  52, episode reward: 41.800, mean reward:  2.613 [-2.630, 32.900], mean action: 3.312 [0.000, 15.000],  loss: 0.022586, mae: 0.392995, mean_q: 0.446072, mean_eps: 0.000000
 1760/5000: episode: 71, duration: 0.390s, episode steps:  24, steps per second:  62, episode reward: -35.640, mean reward: -1.485 [-32.244,  2.390], mean action: 4.167 [0.000, 12.000],  loss: 0.021566, mae: 0.382332, mean_q: 0.444014, mean_eps: 0.000000
 1781/5000: episode: 72, duration: 0.752s, episode steps:  21, steps per second:  28, episode reward: 34.905, mean reward:  1.662 [-3.000, 32.173], mean action: 6.762 [0.000, 21.000],  loss: 0.020190, mae: 0.368845, mean_q: 0.473369, mean_eps: 0.000000
 1795/5000: episode: 73, duration: 0.353s, episode steps:  14, steps per second:  40, episode reward: 38.390, mean reward:  2.742 [-2.480, 32.161], mean action: 3.857 [0.000, 16.000],  loss: 0.020263, mae: 0.370717, mean_q: 0.466267, mean_eps: 0.000000
 1826/5000: episode: 74, duration: 0.572s, episode steps:  31, steps per second:  54, episode reward: -32.520, mean reward: -1.049 [-31.712,  2.679], mean action: 10.419 [0.000, 16.000],  loss: 0.020439, mae: 0.370623, mean_q: 0.451375, mean_eps: 0.000000
 1857/5000: episode: 75, duration: 0.482s, episode steps:  31, steps per second:  64, episode reward: 37.648, mean reward:  1.214 [-2.715, 32.094], mean action: 5.484 [0.000, 14.000],  loss: 0.022417, mae: 0.379707, mean_q: 0.448557, mean_eps: 0.000000
 1870/5000: episode: 76, duration: 0.186s, episode steps:  13, steps per second:  70, episode reward: 43.788, mean reward:  3.368 [-2.630, 32.080], mean action: 2.692 [0.000, 9.000],  loss: 0.021470, mae: 0.384147, mean_q: 0.461359, mean_eps: 0.000000
 1891/5000: episode: 77, duration: 0.278s, episode steps:  21, steps per second:  75, episode reward: 32.003, mean reward:  1.524 [-2.399, 31.943], mean action: 3.810 [0.000, 16.000],  loss: 0.018433, mae: 0.366215, mean_q: 0.440361, mean_eps: 0.000000
 1912/5000: episode: 78, duration: 0.276s, episode steps:  21, steps per second:  76, episode reward: 38.111, mean reward:  1.815 [-2.632, 32.210], mean action: 5.762 [0.000, 16.000],  loss: 0.022220, mae: 0.394693, mean_q: 0.425342, mean_eps: 0.000000
 1944/5000: episode: 79, duration: 0.423s, episode steps:  32, steps per second:  76, episode reward: -32.060, mean reward: -1.002 [-32.037,  3.000], mean action: 7.062 [0.000, 20.000],  loss: 0.019102, mae: 0.378995, mean_q: 0.453403, mean_eps: 0.000000
 1978/5000: episode: 80, duration: 0.448s, episode steps:  34, steps per second:  76, episode reward: 32.412, mean reward:  0.953 [-2.593, 31.850], mean action: 7.176 [0.000, 19.000],  loss: 0.020148, mae: 0.368174, mean_q: 0.519525, mean_eps: 0.000000
 1997/5000: episode: 81, duration: 0.363s, episode steps:  19, steps per second:  52, episode reward: 38.609, mean reward:  2.032 [-2.300, 32.430], mean action: 5.737 [0.000, 18.000],  loss: 0.017954, mae: 0.358591, mean_q: 0.508120, mean_eps: 0.000000
 2016/5000: episode: 82, duration: 0.318s, episode steps:  19, steps per second:  60, episode reward: -38.040, mean reward: -2.002 [-32.910,  2.712], mean action: 8.158 [0.000, 19.000],  loss: 0.015136, mae: 0.353364, mean_q: 0.501591, mean_eps: 0.000000
 2036/5000: episode: 83, duration: 0.305s, episode steps:  20, steps per second:  66, episode reward: 38.287, mean reward:  1.914 [-2.407, 32.153], mean action: 4.950 [0.000, 15.000],  loss: 0.021131, mae: 0.379408, mean_q: 0.502673, mean_eps: 0.000000
 2060/5000: episode: 84, duration: 0.320s, episode steps:  24, steps per second:  75, episode reward: 32.684, mean reward:  1.362 [-3.000, 32.174], mean action: 5.125 [0.000, 15.000],  loss: 0.018939, mae: 0.368402, mean_q: 0.528976, mean_eps: 0.000000
 2080/5000: episode: 85, duration: 0.268s, episode steps:  20, steps per second:  75, episode reward: -38.050, mean reward: -1.902 [-32.256,  2.398], mean action: 5.200 [0.000, 19.000],  loss: 0.020195, mae: 0.379111, mean_q: 0.487729, mean_eps: 0.000000
 2104/5000: episode: 86, duration: 0.321s, episode steps:  24, steps per second:  75, episode reward: 32.507, mean reward:  1.354 [-2.533, 32.020], mean action: 3.750 [0.000, 12.000],  loss: 0.019778, mae: 0.364034, mean_q: 0.503527, mean_eps: 0.000000
 2128/5000: episode: 87, duration: 0.307s, episode steps:  24, steps per second:  78, episode reward: 32.904, mean reward:  1.371 [-2.723, 32.254], mean action: 6.333 [0.000, 19.000],  loss: 0.019922, mae: 0.367609, mean_q: 0.542537, mean_eps: 0.000000
 2147/5000: episode: 88, duration: 0.249s, episode steps:  19, steps per second:  76, episode reward: 35.909, mean reward:  1.890 [-3.000, 32.160], mean action: 4.421 [0.000, 14.000],  loss: 0.020426, mae: 0.374017, mean_q: 0.485383, mean_eps: 0.000000
 2167/5000: episode: 89, duration: 0.256s, episode steps:  20, steps per second:  78, episode reward: -44.450, mean reward: -2.223 [-32.095,  2.020], mean action: 11.350 [0.000, 19.000],  loss: 0.018065, mae: 0.351692, mean_q: 0.502975, mean_eps: 0.000000
 2187/5000: episode: 90, duration: 0.253s, episode steps:  20, steps per second:  79, episode reward: -36.000, mean reward: -1.800 [-30.630,  2.450], mean action: 7.850 [0.000, 17.000],  loss: 0.022578, mae: 0.386978, mean_q: 0.493588, mean_eps: 0.000000
 2211/5000: episode: 91, duration: 0.319s, episode steps:  24, steps per second:  75, episode reward: -36.000, mean reward: -1.500 [-32.183,  2.327], mean action: 4.125 [0.000, 16.000],  loss: 0.021163, mae: 0.380921, mean_q: 0.493331, mean_eps: 0.000000
 2227/5000: episode: 92, duration: 0.215s, episode steps:  16, steps per second:  74, episode reward: 38.048, mean reward:  2.378 [-3.000, 32.901], mean action: 5.312 [0.000, 16.000],  loss: 0.024580, mae: 0.386579, mean_q: 0.523829, mean_eps: 0.000000
 2256/5000: episode: 93, duration: 0.364s, episode steps:  29, steps per second:  80, episode reward: 37.944, mean reward:  1.308 [-2.610, 32.834], mean action: 8.310 [0.000, 19.000],  loss: 0.018904, mae: 0.366529, mean_q: 0.510461, mean_eps: 0.000000
 2283/5000: episode: 94, duration: 0.342s, episode steps:  27, steps per second:  79, episode reward: -32.340, mean reward: -1.198 [-32.376,  2.384], mean action: 7.148 [0.000, 19.000],  loss: 0.018523, mae: 0.372804, mean_q: 0.455521, mean_eps: 0.000000
 2307/5000: episode: 95, duration: 0.313s, episode steps:  24, steps per second:  77, episode reward: 32.806, mean reward:  1.367 [-3.000, 32.230], mean action: 4.542 [0.000, 16.000],  loss: 0.021458, mae: 0.376625, mean_q: 0.474443, mean_eps: 0.000000
 2337/5000: episode: 96, duration: 0.379s, episode steps:  30, steps per second:  79, episode reward: -38.090, mean reward: -1.270 [-32.496,  2.370], mean action: 8.133 [0.000, 20.000],  loss: 0.016314, mae: 0.343712, mean_q: 0.481409, mean_eps: 0.000000
 2355/5000: episode: 97, duration: 0.258s, episode steps:  18, steps per second:  70, episode reward: 42.000, mean reward:  2.333 [-2.229, 32.500], mean action: 4.167 [0.000, 15.000],  loss: 0.019850, mae: 0.372223, mean_q: 0.485693, mean_eps: 0.000000
 2383/5000: episode: 98, duration: 0.361s, episode steps:  28, steps per second:  77, episode reward: -32.430, mean reward: -1.158 [-32.030,  2.250], mean action: 5.643 [0.000, 15.000],  loss: 0.018420, mae: 0.367851, mean_q: 0.483455, mean_eps: 0.000000
 2408/5000: episode: 99, duration: 0.397s, episode steps:  25, steps per second:  63, episode reward: -35.030, mean reward: -1.401 [-32.020,  2.952], mean action: 9.200 [0.000, 20.000],  loss: 0.021630, mae: 0.383382, mean_q: 0.440322, mean_eps: 0.000000
 2432/5000: episode: 100, duration: 0.312s, episode steps:  24, steps per second:  77, episode reward: 35.197, mean reward:  1.467 [-2.368, 32.146], mean action: 5.125 [0.000, 19.000],  loss: 0.024903, mae: 0.400925, mean_q: 0.491227, mean_eps: 0.000000
 2466/5000: episode: 101, duration: 0.467s, episode steps:  34, steps per second:  73, episode reward: 32.580, mean reward:  0.958 [-3.000, 31.844], mean action: 8.765 [0.000, 20.000],  loss: 0.020774, mae: 0.377917, mean_q: 0.488261, mean_eps: 0.000000
 2490/5000: episode: 102, duration: 0.314s, episode steps:  24, steps per second:  76, episode reward: 32.312, mean reward:  1.346 [-3.000, 32.570], mean action: 3.750 [0.000, 12.000],  loss: 0.023030, mae: 0.384863, mean_q: 0.499981, mean_eps: 0.000000
 2504/5000: episode: 103, duration: 0.434s, episode steps:  14, steps per second:  32, episode reward: 39.000, mean reward:  2.786 [-2.409, 30.956], mean action: 3.429 [0.000, 14.000],  loss: 0.026696, mae: 0.406939, mean_q: 0.503836, mean_eps: 0.000000
 2531/5000: episode: 104, duration: 0.468s, episode steps:  27, steps per second:  58, episode reward: 33.000, mean reward:  1.222 [-2.537, 33.000], mean action: 6.852 [0.000, 19.000],  loss: 0.017040, mae: 0.368561, mean_q: 0.492828, mean_eps: 0.000000
 2560/5000: episode: 105, duration: 0.407s, episode steps:  29, steps per second:  71, episode reward: 34.660, mean reward:  1.195 [-2.297, 32.740], mean action: 7.310 [0.000, 17.000],  loss: 0.019733, mae: 0.371449, mean_q: 0.501511, mean_eps: 0.000000
 2588/5000: episode: 106, duration: 0.387s, episode steps:  28, steps per second:  72, episode reward: 33.000, mean reward:  1.179 [-3.000, 32.160], mean action: 5.214 [0.000, 18.000],  loss: 0.021440, mae: 0.372078, mean_q: 0.474979, mean_eps: 0.000000
 2612/5000: episode: 107, duration: 0.339s, episode steps:  24, steps per second:  71, episode reward: -35.910, mean reward: -1.496 [-32.087,  2.410], mean action: 5.667 [0.000, 19.000],  loss: 0.019095, mae: 0.360463, mean_q: 0.474946, mean_eps: 0.000000
 2636/5000: episode: 108, duration: 0.319s, episode steps:  24, steps per second:  75, episode reward: -32.760, mean reward: -1.365 [-32.310,  3.000], mean action: 8.500 [0.000, 21.000],  loss: 0.024638, mae: 0.390631, mean_q: 0.513012, mean_eps: 0.000000
 2661/5000: episode: 109, duration: 0.351s, episode steps:  25, steps per second:  71, episode reward: 35.444, mean reward:  1.418 [-2.502, 32.474], mean action: 4.600 [0.000, 21.000],  loss: 0.022245, mae: 0.379920, mean_q: 0.540631, mean_eps: 0.000000
 2683/5000: episode: 110, duration: 0.290s, episode steps:  22, steps per second:  76, episode reward: -38.180, mean reward: -1.735 [-32.180,  2.910], mean action: 7.500 [0.000, 16.000],  loss: 0.020311, mae: 0.371219, mean_q: 0.467443, mean_eps: 0.000000
 2694/5000: episode: 111, duration: 0.164s, episode steps:  11, steps per second:  67, episode reward: 46.701, mean reward:  4.246 [-0.317, 32.189], mean action: 1.727 [0.000, 6.000],  loss: 0.018613, mae: 0.364981, mean_q: 0.464477, mean_eps: 0.000000
 2715/5000: episode: 112, duration: 0.365s, episode steps:  21, steps per second:  58, episode reward: 37.895, mean reward:  1.805 [-3.000, 32.200], mean action: 6.286 [0.000, 16.000],  loss: 0.014838, mae: 0.348562, mean_q: 0.479555, mean_eps: 0.000000
 2738/5000: episode: 113, duration: 0.373s, episode steps:  23, steps per second:  62, episode reward: 36.000, mean reward:  1.565 [-2.907, 32.280], mean action: 7.696 [0.000, 18.000],  loss: 0.018626, mae: 0.367126, mean_q: 0.469401, mean_eps: 0.000000
 2758/5000: episode: 114, duration: 0.313s, episode steps:  20, steps per second:  64, episode reward: 32.236, mean reward:  1.612 [-3.000, 32.060], mean action: 5.050 [0.000, 16.000],  loss: 0.017268, mae: 0.359854, mean_q: 0.459472, mean_eps: 0.000000
 2774/5000: episode: 115, duration: 0.234s, episode steps:  16, steps per second:  68, episode reward: 38.845, mean reward:  2.428 [-2.471, 31.865], mean action: 5.000 [0.000, 13.000],  loss: 0.017739, mae: 0.365962, mean_q: 0.427407, mean_eps: 0.000000
 2791/5000: episode: 116, duration: 0.243s, episode steps:  17, steps per second:  70, episode reward: 41.461, mean reward:  2.439 [-2.309, 32.492], mean action: 2.765 [0.000, 11.000],  loss: 0.019527, mae: 0.375341, mean_q: 0.451492, mean_eps: 0.000000
 2826/5000: episode: 117, duration: 0.494s, episode steps:  35, steps per second:  71, episode reward: 32.189, mean reward:  0.920 [-2.478, 32.189], mean action: 3.457 [0.000, 15.000],  loss: 0.020704, mae: 0.368739, mean_q: 0.467581, mean_eps: 0.000000
 2849/5000: episode: 118, duration: 0.308s, episode steps:  23, steps per second:  75, episode reward: 35.085, mean reward:  1.525 [-3.000, 31.990], mean action: 3.478 [0.000, 14.000],  loss: 0.020917, mae: 0.381835, mean_q: 0.434405, mean_eps: 0.000000
 2868/5000: episode: 119, duration: 0.420s, episode steps:  19, steps per second:  45, episode reward: 41.137, mean reward:  2.165 [-2.449, 32.090], mean action: 6.526 [0.000, 20.000],  loss: 0.019807, mae: 0.372913, mean_q: 0.443019, mean_eps: 0.000000
 2897/5000: episode: 120, duration: 0.836s, episode steps:  29, steps per second:  35, episode reward: 32.833, mean reward:  1.132 [-2.614, 32.033], mean action: 5.276 [0.000, 14.000],  loss: 0.021265, mae: 0.380565, mean_q: 0.445626, mean_eps: 0.000000
 2937/5000: episode: 121, duration: 0.717s, episode steps:  40, steps per second:  56, episode reward: 32.836, mean reward:  0.821 [-2.463, 32.260], mean action: 4.800 [0.000, 16.000],  loss: 0.021318, mae: 0.380754, mean_q: 0.475792, mean_eps: 0.000000
 2955/5000: episode: 122, duration: 0.255s, episode steps:  18, steps per second:  70, episode reward: 35.904, mean reward:  1.995 [-2.709, 32.394], mean action: 5.389 [0.000, 15.000],  loss: 0.019266, mae: 0.381714, mean_q: 0.392266, mean_eps: 0.000000
 2996/5000: episode: 123, duration: 0.576s, episode steps:  41, steps per second:  71, episode reward: -35.160, mean reward: -0.858 [-32.355,  3.062], mean action: 5.171 [0.000, 20.000],  loss: 0.022744, mae: 0.393221, mean_q: 0.457396, mean_eps: 0.000000
 3015/5000: episode: 124, duration: 0.280s, episode steps:  19, steps per second:  68, episode reward: 42.000, mean reward:  2.211 [-2.542, 33.000], mean action: 5.263 [0.000, 14.000],  loss: 0.019090, mae: 0.376707, mean_q: 0.462373, mean_eps: 0.000000
 3039/5000: episode: 125, duration: 0.325s, episode steps:  24, steps per second:  74, episode reward: 41.307, mean reward:  1.721 [-2.362, 32.020], mean action: 3.458 [1.000, 19.000],  loss: 0.022915, mae: 0.391526, mean_q: 0.481765, mean_eps: 0.000000
 3062/5000: episode: 126, duration: 0.297s, episode steps:  23, steps per second:  78, episode reward: 35.252, mean reward:  1.533 [-2.584, 32.280], mean action: 5.522 [0.000, 19.000],  loss: 0.023162, mae: 0.384432, mean_q: 0.458558, mean_eps: 0.000000
 3083/5000: episode: 127, duration: 0.275s, episode steps:  21, steps per second:  76, episode reward: 32.211, mean reward:  1.534 [-3.000, 31.471], mean action: 3.810 [0.000, 16.000],  loss: 0.023464, mae: 0.384901, mean_q: 0.450378, mean_eps: 0.000000
 3139/5000: episode: 128, duration: 0.718s, episode steps:  56, steps per second:  78, episode reward: -32.250, mean reward: -0.576 [-32.179,  3.000], mean action: 7.518 [0.000, 18.000],  loss: 0.022193, mae: 0.388444, mean_q: 0.487862, mean_eps: 0.000000
 3157/5000: episode: 129, duration: 0.250s, episode steps:  18, steps per second:  72, episode reward: 33.000, mean reward:  1.833 [-2.576, 29.860], mean action: 5.278 [0.000, 16.000],  loss: 0.020769, mae: 0.379495, mean_q: 0.540273, mean_eps: 0.000000
 3183/5000: episode: 130, duration: 0.333s, episode steps:  26, steps per second:  78, episode reward: -35.810, mean reward: -1.377 [-32.070,  2.760], mean action: 7.231 [0.000, 21.000],  loss: 0.021350, mae: 0.376904, mean_q: 0.497555, mean_eps: 0.000000
 3210/5000: episode: 131, duration: 0.347s, episode steps:  27, steps per second:  78, episode reward: 35.250, mean reward:  1.306 [-2.300, 32.350], mean action: 2.852 [0.000, 15.000],  loss: 0.017235, mae: 0.348108, mean_q: 0.521266, mean_eps: 0.000000
 3229/5000: episode: 132, duration: 0.247s, episode steps:  19, steps per second:  77, episode reward: 35.238, mean reward:  1.855 [-2.314, 32.385], mean action: 5.000 [0.000, 15.000],  loss: 0.017915, mae: 0.357811, mean_q: 0.481661, mean_eps: 0.000000
 3261/5000: episode: 133, duration: 0.414s, episode steps:  32, steps per second:  77, episode reward: 32.054, mean reward:  1.002 [-2.219, 32.180], mean action: 3.500 [0.000, 15.000],  loss: 0.020023, mae: 0.367404, mean_q: 0.480417, mean_eps: 0.000000
 3284/5000: episode: 134, duration: 0.300s, episode steps:  23, steps per second:  77, episode reward: 35.468, mean reward:  1.542 [-2.650, 32.340], mean action: 5.783 [0.000, 15.000],  loss: 0.019479, mae: 0.363392, mean_q: 0.512511, mean_eps: 0.000000
 3304/5000: episode: 135, duration: 0.377s, episode steps:  20, steps per second:  53, episode reward: 35.048, mean reward:  1.752 [-3.000, 32.643], mean action: 5.600 [0.000, 18.000],  loss: 0.023324, mae: 0.383960, mean_q: 0.515209, mean_eps: 0.000000
 3332/5000: episode: 136, duration: 0.397s, episode steps:  28, steps per second:  71, episode reward: 34.756, mean reward:  1.241 [-2.318, 31.822], mean action: 3.714 [0.000, 15.000],  loss: 0.020677, mae: 0.377430, mean_q: 0.483305, mean_eps: 0.000000
 3352/5000: episode: 137, duration: 0.261s, episode steps:  20, steps per second:  77, episode reward: 35.712, mean reward:  1.786 [-2.900, 31.994], mean action: 4.850 [3.000, 19.000],  loss: 0.020556, mae: 0.374038, mean_q: 0.465488, mean_eps: 0.000000
 3371/5000: episode: 138, duration: 0.255s, episode steps:  19, steps per second:  75, episode reward: 38.164, mean reward:  2.009 [-2.753, 32.051], mean action: 5.105 [0.000, 15.000],  loss: 0.017244, mae: 0.359289, mean_q: 0.438005, mean_eps: 0.000000
 3392/5000: episode: 139, duration: 0.326s, episode steps:  21, steps per second:  64, episode reward: -35.540, mean reward: -1.692 [-32.553,  2.900], mean action: 4.000 [0.000, 12.000],  loss: 0.024575, mae: 0.388091, mean_q: 0.439601, mean_eps: 0.000000
 3417/5000: episode: 140, duration: 0.317s, episode steps:  25, steps per second:  79, episode reward: 33.000, mean reward:  1.320 [-2.477, 29.437], mean action: 6.120 [0.000, 20.000],  loss: 0.019969, mae: 0.362677, mean_q: 0.470888, mean_eps: 0.000000
 3438/5000: episode: 141, duration: 0.292s, episode steps:  21, steps per second:  72, episode reward: 35.574, mean reward:  1.694 [-2.379, 31.964], mean action: 4.095 [0.000, 20.000],  loss: 0.021833, mae: 0.369150, mean_q: 0.477756, mean_eps: 0.000000
 3470/5000: episode: 142, duration: 0.478s, episode steps:  32, steps per second:  67, episode reward: 34.663, mean reward:  1.083 [-2.360, 31.988], mean action: 6.781 [0.000, 16.000],  loss: 0.023488, mae: 0.372242, mean_q: 0.478777, mean_eps: 0.000000
 3492/5000: episode: 143, duration: 0.331s, episode steps:  22, steps per second:  67, episode reward: 32.816, mean reward:  1.492 [-2.753, 32.536], mean action: 7.136 [0.000, 16.000],  loss: 0.023666, mae: 0.376202, mean_q: 0.479993, mean_eps: 0.000000
 3514/5000: episode: 144, duration: 0.324s, episode steps:  22, steps per second:  68, episode reward: 40.690, mean reward:  1.850 [-2.064, 32.070], mean action: 5.364 [0.000, 19.000],  loss: 0.014693, mae: 0.335181, mean_q: 0.480633, mean_eps: 0.000000
 3530/5000: episode: 145, duration: 0.230s, episode steps:  16, steps per second:  70, episode reward: 38.522, mean reward:  2.408 [-2.283, 31.982], mean action: 7.375 [1.000, 19.000],  loss: 0.021572, mae: 0.367526, mean_q: 0.492575, mean_eps: 0.000000
 3547/5000: episode: 146, duration: 0.305s, episode steps:  17, steps per second:  56, episode reward: 41.178, mean reward:  2.422 [-2.247, 32.400], mean action: 4.412 [0.000, 21.000],  loss: 0.023465, mae: 0.378593, mean_q: 0.493885, mean_eps: 0.000000
 3570/5000: episode: 147, duration: 0.347s, episode steps:  23, steps per second:  66, episode reward: -38.790, mean reward: -1.687 [-32.068,  2.850], mean action: 7.957 [0.000, 21.000],  loss: 0.020325, mae: 0.365703, mean_q: 0.522148, mean_eps: 0.000000
 3600/5000: episode: 148, duration: 0.419s, episode steps:  30, steps per second:  72, episode reward: -35.280, mean reward: -1.176 [-32.122,  2.590], mean action: 4.933 [0.000, 19.000],  loss: 0.019114, mae: 0.365630, mean_q: 0.471292, mean_eps: 0.000000
 3623/5000: episode: 149, duration: 0.336s, episode steps:  23, steps per second:  68, episode reward: 33.000, mean reward:  1.435 [-3.000, 32.260], mean action: 6.043 [0.000, 19.000],  loss: 0.022338, mae: 0.377836, mean_q: 0.442962, mean_eps: 0.000000
 3645/5000: episode: 150, duration: 0.509s, episode steps:  22, steps per second:  43, episode reward: -37.530, mean reward: -1.706 [-32.036,  2.970], mean action: 10.091 [2.000, 19.000],  loss: 0.018428, mae: 0.355292, mean_q: 0.525038, mean_eps: 0.000000
 3665/5000: episode: 151, duration: 0.504s, episode steps:  20, steps per second:  40, episode reward: 35.903, mean reward:  1.795 [-2.475, 32.593], mean action: 5.400 [1.000, 19.000],  loss: 0.021727, mae: 0.361499, mean_q: 0.550491, mean_eps: 0.000000
 3684/5000: episode: 152, duration: 0.394s, episode steps:  19, steps per second:  48, episode reward: 38.599, mean reward:  2.032 [-2.423, 32.090], mean action: 3.263 [0.000, 9.000],  loss: 0.018065, mae: 0.349653, mean_q: 0.507502, mean_eps: 0.000000
 3696/5000: episode: 153, duration: 0.192s, episode steps:  12, steps per second:  62, episode reward: 44.417, mean reward:  3.701 [-2.181, 32.113], mean action: 4.167 [0.000, 15.000],  loss: 0.015346, mae: 0.347743, mean_q: 0.553534, mean_eps: 0.000000
 3722/5000: episode: 154, duration: 0.401s, episode steps:  26, steps per second:  65, episode reward: -40.720, mean reward: -1.566 [-32.027,  2.580], mean action: 7.269 [0.000, 16.000],  loss: 0.022620, mae: 0.374734, mean_q: 0.511342, mean_eps: 0.000000
 3748/5000: episode: 155, duration: 0.717s, episode steps:  26, steps per second:  36, episode reward: -32.240, mean reward: -1.240 [-32.026,  2.691], mean action: 6.654 [0.000, 17.000],  loss: 0.018230, mae: 0.357248, mean_q: 0.457633, mean_eps: 0.000000
 3780/5000: episode: 156, duration: 0.487s, episode steps:  32, steps per second:  66, episode reward: 32.802, mean reward:  1.025 [-2.712, 32.901], mean action: 3.500 [0.000, 15.000],  loss: 0.019267, mae: 0.360069, mean_q: 0.403367, mean_eps: 0.000000
 3809/5000: episode: 157, duration: 0.406s, episode steps:  29, steps per second:  71, episode reward: 32.689, mean reward:  1.127 [-2.452, 32.480], mean action: 3.931 [0.000, 15.000],  loss: 0.019068, mae: 0.355575, mean_q: 0.466585, mean_eps: 0.000000
 3831/5000: episode: 158, duration: 0.288s, episode steps:  22, steps per second:  76, episode reward: 32.901, mean reward:  1.496 [-3.000, 32.301], mean action: 4.545 [0.000, 14.000],  loss: 0.020585, mae: 0.362559, mean_q: 0.459644, mean_eps: 0.000000
 3857/5000: episode: 159, duration: 0.341s, episode steps:  26, steps per second:  76, episode reward: 36.000, mean reward:  1.385 [-2.313, 32.100], mean action: 5.231 [0.000, 19.000],  loss: 0.022800, mae: 0.372715, mean_q: 0.465123, mean_eps: 0.000000
 3877/5000: episode: 160, duration: 0.264s, episode steps:  20, steps per second:  76, episode reward: 34.818, mean reward:  1.741 [-3.000, 32.050], mean action: 7.500 [0.000, 19.000],  loss: 0.025460, mae: 0.402387, mean_q: 0.424866, mean_eps: 0.000000
 3899/5000: episode: 161, duration: 0.597s, episode steps:  22, steps per second:  37, episode reward: 35.579, mean reward:  1.617 [-3.000, 32.195], mean action: 6.727 [1.000, 19.000],  loss: 0.016774, mae: 0.355679, mean_q: 0.442250, mean_eps: 0.000000
 3920/5000: episode: 162, duration: 0.278s, episode steps:  21, steps per second:  76, episode reward: 32.665, mean reward:  1.555 [-2.613, 32.205], mean action: 4.667 [0.000, 19.000],  loss: 0.016500, mae: 0.347338, mean_q: 0.487487, mean_eps: 0.000000
 3941/5000: episode: 163, duration: 0.435s, episode steps:  21, steps per second:  48, episode reward: 35.053, mean reward:  1.669 [-2.901, 32.056], mean action: 6.857 [0.000, 16.000],  loss: 0.023271, mae: 0.381688, mean_q: 0.478402, mean_eps: 0.000000
 3961/5000: episode: 164, duration: 0.324s, episode steps:  20, steps per second:  62, episode reward: 38.093, mean reward:  1.905 [-2.511, 33.000], mean action: 6.300 [0.000, 16.000],  loss: 0.019518, mae: 0.356620, mean_q: 0.448203, mean_eps: 0.000000
 3985/5000: episode: 165, duration: 0.341s, episode steps:  24, steps per second:  70, episode reward: 35.044, mean reward:  1.460 [-2.573, 32.018], mean action: 4.125 [0.000, 16.000],  loss: 0.019111, mae: 0.356802, mean_q: 0.472573, mean_eps: 0.000000
 4012/5000: episode: 166, duration: 0.343s, episode steps:  27, steps per second:  79, episode reward: -32.850, mean reward: -1.217 [-32.056,  2.385], mean action: 5.815 [0.000, 19.000],  loss: 0.020760, mae: 0.363181, mean_q: 0.408093, mean_eps: 0.000000
 4038/5000: episode: 167, duration: 0.336s, episode steps:  26, steps per second:  77, episode reward: 35.056, mean reward:  1.348 [-3.000, 32.820], mean action: 7.192 [0.000, 19.000],  loss: 0.020734, mae: 0.362160, mean_q: 0.471209, mean_eps: 0.000000
 4055/5000: episode: 168, duration: 0.222s, episode steps:  17, steps per second:  77, episode reward: 42.000, mean reward:  2.471 [-2.389, 32.840], mean action: 3.059 [0.000, 19.000],  loss: 0.023077, mae: 0.366518, mean_q: 0.472076, mean_eps: 0.000000
 4072/5000: episode: 169, duration: 0.231s, episode steps:  17, steps per second:  74, episode reward: 42.000, mean reward:  2.471 [-2.292, 33.000], mean action: 2.706 [0.000, 19.000],  loss: 0.017767, mae: 0.342984, mean_q: 0.455619, mean_eps: 0.000000
 4092/5000: episode: 170, duration: 0.259s, episode steps:  20, steps per second:  77, episode reward: 36.000, mean reward:  1.800 [-3.000, 32.310], mean action: 4.700 [0.000, 19.000],  loss: 0.018307, mae: 0.355878, mean_q: 0.487251, mean_eps: 0.000000
 4110/5000: episode: 171, duration: 0.238s, episode steps:  18, steps per second:  76, episode reward: 38.807, mean reward:  2.156 [-2.361, 32.027], mean action: 5.056 [0.000, 19.000],  loss: 0.019494, mae: 0.351884, mean_q: 0.467305, mean_eps: 0.000000
 4130/5000: episode: 172, duration: 0.301s, episode steps:  20, steps per second:  67, episode reward: 40.762, mean reward:  2.038 [-2.407, 32.063], mean action: 5.200 [0.000, 19.000],  loss: 0.018920, mae: 0.353411, mean_q: 0.454011, mean_eps: 0.000000
 4166/5000: episode: 173, duration: 0.616s, episode steps:  36, steps per second:  58, episode reward: 32.780, mean reward:  0.911 [-2.901, 32.080], mean action: 3.111 [0.000, 19.000],  loss: 0.022466, mae: 0.365928, mean_q: 0.488683, mean_eps: 0.000000
 4185/5000: episode: 174, duration: 0.262s, episode steps:  19, steps per second:  73, episode reward: 38.915, mean reward:  2.048 [-2.240, 32.625], mean action: 3.053 [0.000, 15.000],  loss: 0.017809, mae: 0.342985, mean_q: 0.436345, mean_eps: 0.000000
 4204/5000: episode: 175, duration: 0.297s, episode steps:  19, steps per second:  64, episode reward: 41.656, mean reward:  2.192 [-2.173, 32.656], mean action: 4.842 [0.000, 19.000],  loss: 0.017060, mae: 0.342622, mean_q: 0.455074, mean_eps: 0.000000
 4222/5000: episode: 176, duration: 0.263s, episode steps:  18, steps per second:  68, episode reward: 35.324, mean reward:  1.962 [-3.000, 32.274], mean action: 7.667 [0.000, 19.000],  loss: 0.025310, mae: 0.373335, mean_q: 0.446400, mean_eps: 0.000000
 4254/5000: episode: 177, duration: 0.430s, episode steps:  32, steps per second:  74, episode reward: 38.467, mean reward:  1.202 [-2.570, 32.170], mean action: 4.062 [0.000, 20.000],  loss: 0.018535, mae: 0.346688, mean_q: 0.464586, mean_eps: 0.000000
 4278/5000: episode: 178, duration: 0.323s, episode steps:  24, steps per second:  74, episode reward: 41.041, mean reward:  1.710 [-2.307, 32.220], mean action: 4.042 [0.000, 13.000],  loss: 0.022068, mae: 0.360183, mean_q: 0.483456, mean_eps: 0.000000
 4302/5000: episode: 179, duration: 0.309s, episode steps:  24, steps per second:  78, episode reward: 33.000, mean reward:  1.375 [-2.746, 33.000], mean action: 3.958 [0.000, 12.000],  loss: 0.024465, mae: 0.375875, mean_q: 0.492471, mean_eps: 0.000000
 4328/5000: episode: 180, duration: 0.341s, episode steps:  26, steps per second:  76, episode reward: 33.000, mean reward:  1.269 [-2.496, 32.800], mean action: 5.846 [0.000, 15.000],  loss: 0.020696, mae: 0.352171, mean_q: 0.481941, mean_eps: 0.000000
 4346/5000: episode: 181, duration: 0.243s, episode steps:  18, steps per second:  74, episode reward: 35.607, mean reward:  1.978 [-3.000, 32.517], mean action: 5.444 [0.000, 20.000],  loss: 0.019413, mae: 0.355237, mean_q: 0.430473, mean_eps: 0.000000
 4364/5000: episode: 182, duration: 0.234s, episode steps:  18, steps per second:  77, episode reward: 32.261, mean reward:  1.792 [-3.000, 32.487], mean action: 4.667 [0.000, 19.000],  loss: 0.021127, mae: 0.365033, mean_q: 0.416339, mean_eps: 0.000000
 4391/5000: episode: 183, duration: 0.352s, episode steps:  27, steps per second:  77, episode reward: 32.197, mean reward:  1.192 [-2.611, 31.819], mean action: 8.148 [0.000, 20.000],  loss: 0.020169, mae: 0.355965, mean_q: 0.450285, mean_eps: 0.000000
 4430/5000: episode: 184, duration: 0.511s, episode steps:  39, steps per second:  76, episode reward: -35.340, mean reward: -0.906 [-31.724,  2.169], mean action: 6.205 [0.000, 20.000],  loss: 0.022151, mae: 0.363457, mean_q: 0.479781, mean_eps: 0.000000
 4450/5000: episode: 185, duration: 0.261s, episode steps:  20, steps per second:  77, episode reward: 41.695, mean reward:  2.085 [-2.198, 32.927], mean action: 1.850 [0.000, 9.000],  loss: 0.025123, mae: 0.380614, mean_q: 0.500190, mean_eps: 0.000000
 4473/5000: episode: 186, duration: 0.345s, episode steps:  23, steps per second:  67, episode reward: 38.635, mean reward:  1.680 [-2.122, 32.129], mean action: 3.391 [0.000, 9.000],  loss: 0.024149, mae: 0.374600, mean_q: 0.446247, mean_eps: 0.000000
 4487/5000: episode: 187, duration: 0.221s, episode steps:  14, steps per second:  63, episode reward: 41.833, mean reward:  2.988 [-2.264, 33.000], mean action: 2.143 [0.000, 11.000],  loss: 0.016333, mae: 0.339471, mean_q: 0.421807, mean_eps: 0.000000
 4517/5000: episode: 188, duration: 0.392s, episode steps:  30, steps per second:  76, episode reward: 34.998, mean reward:  1.167 [-2.294, 33.000], mean action: 5.000 [0.000, 13.000],  loss: 0.019144, mae: 0.360488, mean_q: 0.422376, mean_eps: 0.000000
 4539/5000: episode: 189, duration: 0.297s, episode steps:  22, steps per second:  74, episode reward: 38.024, mean reward:  1.728 [-2.181, 32.827], mean action: 4.545 [0.000, 14.000],  loss: 0.021972, mae: 0.372806, mean_q: 0.446136, mean_eps: 0.000000
 4561/5000: episode: 190, duration: 0.291s, episode steps:  22, steps per second:  76, episode reward: 35.366, mean reward:  1.608 [-2.581, 31.796], mean action: 2.500 [0.000, 11.000],  loss: 0.016116, mae: 0.340698, mean_q: 0.493291, mean_eps: 0.000000
 4575/5000: episode: 191, duration: 0.197s, episode steps:  14, steps per second:  71, episode reward: 41.120, mean reward:  2.937 [-2.350, 32.904], mean action: 2.214 [0.000, 16.000],  loss: 0.020127, mae: 0.359669, mean_q: 0.461303, mean_eps: 0.000000
 4592/5000: episode: 192, duration: 0.223s, episode steps:  17, steps per second:  76, episode reward: 35.806, mean reward:  2.106 [-3.000, 33.000], mean action: 6.294 [0.000, 16.000],  loss: 0.019729, mae: 0.354085, mean_q: 0.488344, mean_eps: 0.000000
 4614/5000: episode: 193, duration: 0.284s, episode steps:  22, steps per second:  77, episode reward: -38.310, mean reward: -1.741 [-32.350,  2.480], mean action: 7.818 [0.000, 20.000],  loss: 0.016861, mae: 0.335961, mean_q: 0.489155, mean_eps: 0.000000
 4634/5000: episode: 194, duration: 0.263s, episode steps:  20, steps per second:  76, episode reward: 41.344, mean reward:  2.067 [-2.168, 33.000], mean action: 3.300 [0.000, 14.000],  loss: 0.018984, mae: 0.350673, mean_q: 0.537314, mean_eps: 0.000000
 4663/5000: episode: 195, duration: 0.383s, episode steps:  29, steps per second:  76, episode reward: 41.377, mean reward:  1.427 [-2.282, 32.779], mean action: 3.276 [0.000, 15.000],  loss: 0.020932, mae: 0.364908, mean_q: 0.475348, mean_eps: 0.000000
 4688/5000: episode: 196, duration: 0.326s, episode steps:  25, steps per second:  77, episode reward: 35.647, mean reward:  1.426 [-2.617, 33.000], mean action: 7.640 [1.000, 16.000],  loss: 0.017792, mae: 0.342815, mean_q: 0.498652, mean_eps: 0.000000
 4713/5000: episode: 197, duration: 0.321s, episode steps:  25, steps per second:  78, episode reward: -32.870, mean reward: -1.315 [-32.259,  2.829], mean action: 8.200 [0.000, 16.000],  loss: 0.019854, mae: 0.354076, mean_q: 0.515062, mean_eps: 0.000000
 4736/5000: episode: 198, duration: 0.300s, episode steps:  23, steps per second:  77, episode reward: -33.000, mean reward: -1.435 [-33.000,  2.681], mean action: 8.000 [1.000, 19.000],  loss: 0.022600, mae: 0.372071, mean_q: 0.523170, mean_eps: 0.000000
 4761/5000: episode: 199, duration: 0.324s, episode steps:  25, steps per second:  77, episode reward: 41.601, mean reward:  1.664 [-2.138, 33.000], mean action: 1.720 [0.000, 19.000],  loss: 0.019572, mae: 0.352486, mean_q: 0.545247, mean_eps: 0.000000
 4781/5000: episode: 200, duration: 0.259s, episode steps:  20, steps per second:  77, episode reward: 32.372, mean reward:  1.619 [-3.000, 32.290], mean action: 5.400 [0.000, 16.000],  loss: 0.021646, mae: 0.368854, mean_q: 0.580829, mean_eps: 0.000000
 4812/5000: episode: 201, duration: 0.408s, episode steps:  31, steps per second:  76, episode reward: -32.700, mean reward: -1.055 [-32.091,  2.555], mean action: 5.935 [0.000, 16.000],  loss: 0.019864, mae: 0.353801, mean_q: 0.505710, mean_eps: 0.000000
 4833/5000: episode: 202, duration: 0.277s, episode steps:  21, steps per second:  76, episode reward: 38.184, mean reward:  1.818 [-2.510, 31.864], mean action: 5.476 [0.000, 19.000],  loss: 0.023331, mae: 0.376694, mean_q: 0.479391, mean_eps: 0.000000
 4852/5000: episode: 203, duration: 0.294s, episode steps:  19, steps per second:  65, episode reward: 35.783, mean reward:  1.883 [-2.318, 32.133], mean action: 4.474 [0.000, 19.000],  loss: 0.019309, mae: 0.360411, mean_q: 0.506892, mean_eps: 0.000000
 4894/5000: episode: 204, duration: 0.595s, episode steps:  42, steps per second:  71, episode reward: -32.370, mean reward: -0.771 [-32.005,  2.877], mean action: 4.167 [0.000, 19.000],  loss: 0.018374, mae: 0.361007, mean_q: 0.515906, mean_eps: 0.000000
 4915/5000: episode: 205, duration: 0.303s, episode steps:  21, steps per second:  69, episode reward: 35.095, mean reward:  1.671 [-3.000, 32.350], mean action: 5.190 [0.000, 19.000],  loss: 0.023347, mae: 0.388280, mean_q: 0.453858, mean_eps: 0.000000
 4940/5000: episode: 206, duration: 0.349s, episode steps:  25, steps per second:  72, episode reward: 37.990, mean reward:  1.520 [-2.470, 32.358], mean action: 6.640 [0.000, 21.000],  loss: 0.020898, mae: 0.366842, mean_q: 0.491805, mean_eps: 0.000000
 4964/5000: episode: 207, duration: 0.319s, episode steps:  24, steps per second:  75, episode reward: -35.250, mean reward: -1.469 [-32.320,  2.598], mean action: 4.083 [0.000, 19.000],  loss: 0.021441, mae: 0.363117, mean_q: 0.498650, mean_eps: 0.000000
 5000/5000: episode: 208, duration: 0.467s, episode steps:  36, steps per second:  77, episode reward: 41.179, mean reward:  1.144 [-2.613, 32.412], mean action: 3.306 [0.000, 20.000],  loss: 0.019684, mae: 0.368037, mean_q: 0.460130, mean_eps: 0.000000
done, took 69.436 seconds
DQN Evaluation: 4423 victories out of 5262 episodes
Training for 5000 steps ...
   16/5000: episode: 1, duration: 0.138s, episode steps:  16, steps per second: 116, episode reward: 41.430, mean reward:  2.589 [-2.096, 32.354], mean action: 3.250 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   36/5000: episode: 2, duration: 0.231s, episode steps:  20, steps per second:  87, episode reward: 43.922, mean reward:  2.196 [-2.227, 32.148], mean action: 2.600 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   61/5000: episode: 3, duration: 0.255s, episode steps:  25, steps per second:  98, episode reward: 41.748, mean reward:  1.670 [-2.541, 32.075], mean action: 2.000 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   96/5000: episode: 4, duration: 0.246s, episode steps:  35, steps per second: 142, episode reward: 32.372, mean reward:  0.925 [-3.000, 32.070], mean action: 5.486 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  111/5000: episode: 5, duration: 0.113s, episode steps:  15, steps per second: 132, episode reward: 44.154, mean reward:  2.944 [-2.513, 32.070], mean action: 3.333 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  133/5000: episode: 6, duration: 0.149s, episode steps:  22, steps per second: 148, episode reward: 43.511, mean reward:  1.978 [-2.762, 32.110], mean action: 3.727 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  167/5000: episode: 7, duration: 0.509s, episode steps:  34, steps per second:  67, episode reward: 41.444, mean reward:  1.219 [-2.435, 32.070], mean action: 2.000 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  194/5000: episode: 8, duration: 0.178s, episode steps:  27, steps per second: 152, episode reward: 39.000, mean reward:  1.444 [-2.515, 32.130], mean action: 2.407 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  223/5000: episode: 9, duration: 0.288s, episode steps:  29, steps per second: 101, episode reward: 42.000, mean reward:  1.448 [-2.417, 32.060], mean action: 2.828 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  246/5000: episode: 10, duration: 0.196s, episode steps:  23, steps per second: 118, episode reward: 44.140, mean reward:  1.919 [-3.000, 32.050], mean action: 2.957 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  264/5000: episode: 11, duration: 0.133s, episode steps:  18, steps per second: 135, episode reward: 41.283, mean reward:  2.293 [-3.000, 32.230], mean action: 4.000 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  284/5000: episode: 12, duration: 0.305s, episode steps:  20, steps per second:  66, episode reward: 46.574, mean reward:  2.329 [-0.485, 32.100], mean action: 1.350 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  318/5000: episode: 13, duration: 0.350s, episode steps:  34, steps per second:  97, episode reward: 38.428, mean reward:  1.130 [-3.000, 32.440], mean action: 4.441 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  335/5000: episode: 14, duration: 0.122s, episode steps:  17, steps per second: 139, episode reward: 44.455, mean reward:  2.615 [-2.043, 33.000], mean action: 5.529 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  357/5000: episode: 15, duration: 0.148s, episode steps:  22, steps per second: 149, episode reward: 41.480, mean reward:  1.885 [-2.455, 32.011], mean action: 3.545 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  379/5000: episode: 16, duration: 0.300s, episode steps:  22, steps per second:  73, episode reward: 41.801, mean reward:  1.900 [-3.000, 32.530], mean action: 5.727 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  397/5000: episode: 17, duration: 0.155s, episode steps:  18, steps per second: 116, episode reward: 44.902, mean reward:  2.495 [-2.132, 32.350], mean action: 2.778 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  424/5000: episode: 18, duration: 0.166s, episode steps:  27, steps per second: 162, episode reward: 35.847, mean reward:  1.328 [-2.575, 32.160], mean action: 3.815 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  449/5000: episode: 19, duration: 0.180s, episode steps:  25, steps per second: 139, episode reward: 38.216, mean reward:  1.529 [-3.000, 33.000], mean action: 4.040 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  480/5000: episode: 20, duration: 0.238s, episode steps:  31, steps per second: 130, episode reward: 35.779, mean reward:  1.154 [-2.616, 31.919], mean action: 4.226 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  507/5000: episode: 21, duration: 0.174s, episode steps:  27, steps per second: 155, episode reward: 38.039, mean reward:  1.409 [-2.384, 32.320], mean action: 3.852 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  533/5000: episode: 22, duration: 0.163s, episode steps:  26, steps per second: 159, episode reward: 41.539, mean reward:  1.598 [-2.377, 32.730], mean action: 4.538 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  557/5000: episode: 23, duration: 0.156s, episode steps:  24, steps per second: 154, episode reward: 36.000, mean reward:  1.500 [-3.000, 32.540], mean action: 2.958 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  567/5000: episode: 24, duration: 0.081s, episode steps:  10, steps per second: 124, episode reward: 48.000, mean reward:  4.800 [-0.030, 32.080], mean action: 1.600 [1.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  596/5000: episode: 25, duration: 0.191s, episode steps:  29, steps per second: 152, episode reward: 38.879, mean reward:  1.341 [-2.598, 32.130], mean action: 4.483 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  619/5000: episode: 26, duration: 0.175s, episode steps:  23, steps per second: 132, episode reward: 40.628, mean reward:  1.766 [-2.729, 32.250], mean action: 3.826 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  637/5000: episode: 27, duration: 0.115s, episode steps:  18, steps per second: 156, episode reward: 43.330, mean reward:  2.407 [-2.035, 31.847], mean action: 3.944 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  661/5000: episode: 28, duration: 0.155s, episode steps:  24, steps per second: 155, episode reward: 37.922, mean reward:  1.580 [-2.292, 31.986], mean action: 3.458 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  681/5000: episode: 29, duration: 0.132s, episode steps:  20, steps per second: 152, episode reward: 35.655, mean reward:  1.783 [-2.904, 31.865], mean action: 4.550 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  703/5000: episode: 30, duration: 0.140s, episode steps:  22, steps per second: 157, episode reward: 44.256, mean reward:  2.012 [-2.011, 32.480], mean action: 2.864 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  746/5000: episode: 31, duration: 0.261s, episode steps:  43, steps per second: 164, episode reward: 44.004, mean reward:  1.023 [-2.207, 32.470], mean action: 2.442 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  774/5000: episode: 32, duration: 0.180s, episode steps:  28, steps per second: 155, episode reward: 41.855, mean reward:  1.495 [-2.404, 32.003], mean action: 2.286 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  798/5000: episode: 33, duration: 0.158s, episode steps:  24, steps per second: 152, episode reward: 35.370, mean reward:  1.474 [-3.000, 31.880], mean action: 3.583 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  823/5000: episode: 34, duration: 0.167s, episode steps:  25, steps per second: 150, episode reward: 40.950, mean reward:  1.638 [-2.863, 32.130], mean action: 3.920 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  860/5000: episode: 35, duration: 0.224s, episode steps:  37, steps per second: 165, episode reward: 32.336, mean reward:  0.874 [-2.602, 32.110], mean action: 5.027 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  888/5000: episode: 36, duration: 0.173s, episode steps:  28, steps per second: 162, episode reward: 44.371, mean reward:  1.585 [-2.710, 32.090], mean action: 3.000 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  910/5000: episode: 37, duration: 0.150s, episode steps:  22, steps per second: 147, episode reward: 43.584, mean reward:  1.981 [-2.168, 32.051], mean action: 4.136 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  928/5000: episode: 38, duration: 0.125s, episode steps:  18, steps per second: 144, episode reward: 44.097, mean reward:  2.450 [-2.073, 31.799], mean action: 3.389 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  951/5000: episode: 39, duration: 0.161s, episode steps:  23, steps per second: 143, episode reward: 46.822, mean reward:  2.036 [-0.323, 32.327], mean action: 3.043 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  978/5000: episode: 40, duration: 0.174s, episode steps:  27, steps per second: 156, episode reward: 35.417, mean reward:  1.312 [-2.428, 32.388], mean action: 5.370 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1017/5000: episode: 41, duration: 0.409s, episode steps:  39, steps per second:  95, episode reward: -34.430, mean reward: -0.883 [-32.149,  2.499], mean action: 6.026 [0.000, 19.000],  loss: 0.022398, mae: 0.383773, mean_q: 0.402246, mean_eps: 0.000000
 1045/5000: episode: 42, duration: 0.378s, episode steps:  28, steps per second:  74, episode reward: 43.108, mean reward:  1.540 [-2.625, 32.160], mean action: 6.000 [0.000, 20.000],  loss: 0.020197, mae: 0.381219, mean_q: 0.393346, mean_eps: 0.000000
 1075/5000: episode: 43, duration: 0.389s, episode steps:  30, steps per second:  77, episode reward: 38.448, mean reward:  1.282 [-3.000, 32.490], mean action: 4.333 [0.000, 19.000],  loss: 0.019780, mae: 0.369478, mean_q: 0.449035, mean_eps: 0.000000
 1104/5000: episode: 44, duration: 0.386s, episode steps:  29, steps per second:  75, episode reward: 38.223, mean reward:  1.318 [-2.806, 32.091], mean action: 3.000 [0.000, 19.000],  loss: 0.022408, mae: 0.372159, mean_q: 0.525053, mean_eps: 0.000000
 1129/5000: episode: 45, duration: 0.375s, episode steps:  25, steps per second:  67, episode reward: 38.709, mean reward:  1.548 [-2.190, 31.919], mean action: 3.000 [1.000, 19.000],  loss: 0.019100, mae: 0.357493, mean_q: 0.489925, mean_eps: 0.000000
 1163/5000: episode: 46, duration: 0.440s, episode steps:  34, steps per second:  77, episode reward: 35.305, mean reward:  1.038 [-3.000, 32.048], mean action: 4.500 [0.000, 21.000],  loss: 0.021612, mae: 0.366751, mean_q: 0.504255, mean_eps: 0.000000
 1182/5000: episode: 47, duration: 0.261s, episode steps:  19, steps per second:  73, episode reward: 44.613, mean reward:  2.348 [-2.465, 31.653], mean action: 2.316 [0.000, 15.000],  loss: 0.022654, mae: 0.371748, mean_q: 0.497927, mean_eps: 0.000000
 1219/5000: episode: 48, duration: 0.478s, episode steps:  37, steps per second:  77, episode reward: 35.037, mean reward:  0.947 [-2.676, 31.841], mean action: 2.459 [0.000, 14.000],  loss: 0.019190, mae: 0.356339, mean_q: 0.488765, mean_eps: 0.000000
 1240/5000: episode: 49, duration: 0.275s, episode steps:  21, steps per second:  76, episode reward: 42.000, mean reward:  2.000 [-3.000, 33.000], mean action: 2.190 [0.000, 9.000],  loss: 0.019393, mae: 0.354715, mean_q: 0.510803, mean_eps: 0.000000
 1270/5000: episode: 50, duration: 0.387s, episode steps:  30, steps per second:  77, episode reward: 37.994, mean reward:  1.266 [-3.000, 31.715], mean action: 2.967 [0.000, 15.000],  loss: 0.019755, mae: 0.354631, mean_q: 0.454054, mean_eps: 0.000000
 1307/5000: episode: 51, duration: 0.488s, episode steps:  37, steps per second:  76, episode reward: 40.480, mean reward:  1.094 [-2.653, 32.930], mean action: 2.649 [0.000, 20.000],  loss: 0.022503, mae: 0.370291, mean_q: 0.503120, mean_eps: 0.000000
 1332/5000: episode: 52, duration: 0.340s, episode steps:  25, steps per second:  73, episode reward: 44.048, mean reward:  1.762 [-2.199, 31.753], mean action: 3.920 [0.000, 15.000],  loss: 0.020879, mae: 0.360134, mean_q: 0.480729, mean_eps: 0.000000
 1347/5000: episode: 53, duration: 0.208s, episode steps:  15, steps per second:  72, episode reward: 41.384, mean reward:  2.759 [-2.640, 31.854], mean action: 3.400 [0.000, 11.000],  loss: 0.016561, mae: 0.340349, mean_q: 0.455833, mean_eps: 0.000000
 1376/5000: episode: 54, duration: 0.371s, episode steps:  29, steps per second:  78, episode reward: 38.051, mean reward:  1.312 [-3.000, 32.310], mean action: 4.448 [0.000, 14.000],  loss: 0.020044, mae: 0.359454, mean_q: 0.463952, mean_eps: 0.000000
 1400/5000: episode: 55, duration: 0.315s, episode steps:  24, steps per second:  76, episode reward: 41.167, mean reward:  1.715 [-2.600, 32.139], mean action: 3.917 [1.000, 15.000],  loss: 0.019684, mae: 0.360833, mean_q: 0.484710, mean_eps: 0.000000
 1426/5000: episode: 56, duration: 0.346s, episode steps:  26, steps per second:  75, episode reward: 41.524, mean reward:  1.597 [-3.000, 32.545], mean action: 4.154 [0.000, 19.000],  loss: 0.018137, mae: 0.354933, mean_q: 0.511892, mean_eps: 0.000000
 1450/5000: episode: 57, duration: 0.645s, episode steps:  24, steps per second:  37, episode reward: 39.000, mean reward:  1.625 [-2.404, 32.530], mean action: 3.542 [0.000, 16.000],  loss: 0.021585, mae: 0.371431, mean_q: 0.512426, mean_eps: 0.000000
 1483/5000: episode: 58, duration: 0.418s, episode steps:  33, steps per second:  79, episode reward: 38.559, mean reward:  1.168 [-2.481, 32.349], mean action: 2.121 [0.000, 16.000],  loss: 0.021025, mae: 0.370372, mean_q: 0.577976, mean_eps: 0.000000
 1517/5000: episode: 59, duration: 0.438s, episode steps:  34, steps per second:  78, episode reward: 41.173, mean reward:  1.211 [-2.403, 32.327], mean action: 4.941 [0.000, 20.000],  loss: 0.020085, mae: 0.360048, mean_q: 0.587304, mean_eps: 0.000000
 1549/5000: episode: 60, duration: 0.422s, episode steps:  32, steps per second:  76, episode reward: 40.701, mean reward:  1.272 [-2.446, 32.248], mean action: 3.969 [0.000, 19.000],  loss: 0.024453, mae: 0.380316, mean_q: 0.562447, mean_eps: 0.000000
 1590/5000: episode: 61, duration: 0.533s, episode steps:  41, steps per second:  77, episode reward: 35.110, mean reward:  0.856 [-2.554, 32.360], mean action: 5.634 [0.000, 20.000],  loss: 0.018780, mae: 0.354126, mean_q: 0.529838, mean_eps: 0.000000
 1612/5000: episode: 62, duration: 0.292s, episode steps:  22, steps per second:  75, episode reward: 42.000, mean reward:  1.909 [-2.447, 32.070], mean action: 1.727 [0.000, 12.000],  loss: 0.022489, mae: 0.368259, mean_q: 0.482984, mean_eps: 0.000000
 1642/5000: episode: 63, duration: 0.386s, episode steps:  30, steps per second:  78, episode reward: 38.266, mean reward:  1.276 [-2.744, 32.686], mean action: 4.967 [0.000, 20.000],  loss: 0.022142, mae: 0.376110, mean_q: 0.466652, mean_eps: 0.000000
 1684/5000: episode: 64, duration: 0.541s, episode steps:  42, steps per second:  78, episode reward: 41.538, mean reward:  0.989 [-3.000, 32.210], mean action: 4.667 [0.000, 19.000],  loss: 0.019781, mae: 0.359351, mean_q: 0.463858, mean_eps: 0.000000
 1707/5000: episode: 65, duration: 0.314s, episode steps:  23, steps per second:  73, episode reward: 44.251, mean reward:  1.924 [-2.722, 32.230], mean action: 2.957 [0.000, 19.000],  loss: 0.024666, mae: 0.387646, mean_q: 0.428200, mean_eps: 0.000000
 1729/5000: episode: 66, duration: 0.293s, episode steps:  22, steps per second:  75, episode reward: 35.622, mean reward:  1.619 [-2.704, 32.242], mean action: 3.182 [0.000, 11.000],  loss: 0.017672, mae: 0.354659, mean_q: 0.395986, mean_eps: 0.000000
 1760/5000: episode: 67, duration: 0.556s, episode steps:  31, steps per second:  56, episode reward: 40.148, mean reward:  1.295 [-2.337, 32.040], mean action: 4.161 [2.000, 16.000],  loss: 0.024705, mae: 0.388530, mean_q: 0.445895, mean_eps: 0.000000
 1781/5000: episode: 68, duration: 0.318s, episode steps:  21, steps per second:  66, episode reward: 41.325, mean reward:  1.968 [-2.062, 31.803], mean action: 3.000 [0.000, 14.000],  loss: 0.022455, mae: 0.368375, mean_q: 0.511877, mean_eps: 0.000000
 1817/5000: episode: 69, duration: 0.502s, episode steps:  36, steps per second:  72, episode reward: 35.397, mean reward:  0.983 [-2.500, 32.110], mean action: 6.694 [0.000, 19.000],  loss: 0.018760, mae: 0.346526, mean_q: 0.530674, mean_eps: 0.000000
 1837/5000: episode: 70, duration: 0.274s, episode steps:  20, steps per second:  73, episode reward: 41.326, mean reward:  2.066 [-2.418, 31.835], mean action: 3.300 [0.000, 19.000],  loss: 0.025599, mae: 0.381218, mean_q: 0.514827, mean_eps: 0.000000
 1863/5000: episode: 71, duration: 0.362s, episode steps:  26, steps per second:  72, episode reward: 41.633, mean reward:  1.601 [-3.000, 32.250], mean action: 2.500 [0.000, 12.000],  loss: 0.020071, mae: 0.356432, mean_q: 0.483764, mean_eps: 0.000000
 1893/5000: episode: 72, duration: 0.405s, episode steps:  30, steps per second:  74, episode reward: 44.131, mean reward:  1.471 [-2.220, 31.903], mean action: 6.900 [0.000, 20.000],  loss: 0.020869, mae: 0.358616, mean_q: 0.514538, mean_eps: 0.000000
 1912/5000: episode: 73, duration: 0.258s, episode steps:  19, steps per second:  74, episode reward: 44.100, mean reward:  2.321 [-2.198, 32.190], mean action: 2.789 [1.000, 12.000],  loss: 0.017714, mae: 0.344565, mean_q: 0.483906, mean_eps: 0.000000
 1935/5000: episode: 74, duration: 0.309s, episode steps:  23, steps per second:  74, episode reward: 41.776, mean reward:  1.816 [-3.000, 32.311], mean action: 1.391 [0.000, 12.000],  loss: 0.019143, mae: 0.349471, mean_q: 0.471675, mean_eps: 0.000000
 1951/5000: episode: 75, duration: 0.224s, episode steps:  16, steps per second:  71, episode reward: 41.350, mean reward:  2.584 [-2.758, 32.120], mean action: 3.812 [0.000, 15.000],  loss: 0.018569, mae: 0.350676, mean_q: 0.488770, mean_eps: 0.000000
 1967/5000: episode: 76, duration: 0.226s, episode steps:  16, steps per second:  71, episode reward: 44.388, mean reward:  2.774 [-2.710, 32.648], mean action: 5.562 [0.000, 19.000],  loss: 0.022910, mae: 0.362313, mean_q: 0.501037, mean_eps: 0.000000
 2001/5000: episode: 77, duration: 0.455s, episode steps:  34, steps per second:  75, episode reward: 45.000, mean reward:  1.324 [-2.295, 32.050], mean action: 1.235 [0.000, 15.000],  loss: 0.020396, mae: 0.358027, mean_q: 0.468363, mean_eps: 0.000000
 2011/5000: episode: 78, duration: 0.145s, episode steps:  10, steps per second:  69, episode reward: 48.000, mean reward:  4.800 [ 0.060, 32.660], mean action: 2.400 [0.000, 15.000],  loss: 0.019099, mae: 0.343444, mean_q: 0.491139, mean_eps: 0.000000
 2040/5000: episode: 79, duration: 0.387s, episode steps:  29, steps per second:  75, episode reward: 40.422, mean reward:  1.394 [-2.069, 32.300], mean action: 2.655 [0.000, 13.000],  loss: 0.019186, mae: 0.347190, mean_q: 0.472389, mean_eps: 0.000000
 2061/5000: episode: 80, duration: 0.292s, episode steps:  21, steps per second:  72, episode reward: 42.000, mean reward:  2.000 [-2.099, 32.300], mean action: 3.905 [0.000, 16.000],  loss: 0.019733, mae: 0.353525, mean_q: 0.456953, mean_eps: 0.000000
 2090/5000: episode: 81, duration: 0.376s, episode steps:  29, steps per second:  77, episode reward: 46.119, mean reward:  1.590 [-0.725, 32.100], mean action: 2.828 [1.000, 11.000],  loss: 0.017923, mae: 0.339548, mean_q: 0.441057, mean_eps: 0.000000
 2120/5000: episode: 82, duration: 0.391s, episode steps:  30, steps per second:  77, episode reward: 38.676, mean reward:  1.289 [-2.368, 32.144], mean action: 3.100 [0.000, 16.000],  loss: 0.021651, mae: 0.362775, mean_q: 0.446879, mean_eps: 0.000000
 2161/5000: episode: 83, duration: 0.526s, episode steps:  41, steps per second:  78, episode reward: 41.808, mean reward:  1.020 [-2.206, 32.204], mean action: 0.878 [0.000, 12.000],  loss: 0.018888, mae: 0.350033, mean_q: 0.469753, mean_eps: 0.000000
 2188/5000: episode: 84, duration: 0.346s, episode steps:  27, steps per second:  78, episode reward: 35.817, mean reward:  1.327 [-3.000, 32.070], mean action: 3.667 [0.000, 14.000],  loss: 0.018545, mae: 0.341737, mean_q: 0.496283, mean_eps: 0.000000
 2207/5000: episode: 85, duration: 0.249s, episode steps:  19, steps per second:  76, episode reward: 41.778, mean reward:  2.199 [-2.675, 32.531], mean action: 3.579 [0.000, 15.000],  loss: 0.021888, mae: 0.359966, mean_q: 0.507684, mean_eps: 0.000000
 2253/5000: episode: 86, duration: 0.590s, episode steps:  46, steps per second:  78, episode reward: 36.000, mean reward:  0.783 [-3.000, 32.220], mean action: 2.761 [0.000, 15.000],  loss: 0.021805, mae: 0.360685, mean_q: 0.447773, mean_eps: 0.000000
 2288/5000: episode: 87, duration: 0.448s, episode steps:  35, steps per second:  78, episode reward: 37.953, mean reward:  1.084 [-3.000, 32.060], mean action: 4.914 [0.000, 21.000],  loss: 0.021427, mae: 0.357625, mean_q: 0.493031, mean_eps: 0.000000
 2319/5000: episode: 88, duration: 0.401s, episode steps:  31, steps per second:  77, episode reward: 35.903, mean reward:  1.158 [-2.363, 32.383], mean action: 3.226 [0.000, 11.000],  loss: 0.020577, mae: 0.355958, mean_q: 0.476623, mean_eps: 0.000000
 2336/5000: episode: 89, duration: 0.233s, episode steps:  17, steps per second:  73, episode reward: 42.000, mean reward:  2.471 [-2.503, 32.030], mean action: 1.588 [0.000, 11.000],  loss: 0.021322, mae: 0.361857, mean_q: 0.552793, mean_eps: 0.000000
 2358/5000: episode: 90, duration: 0.324s, episode steps:  22, steps per second:  68, episode reward: 39.000, mean reward:  1.773 [-3.000, 32.210], mean action: 2.864 [0.000, 14.000],  loss: 0.022944, mae: 0.376816, mean_q: 0.544409, mean_eps: 0.000000
 2375/5000: episode: 91, duration: 0.235s, episode steps:  17, steps per second:  72, episode reward: 41.056, mean reward:  2.415 [-2.602, 31.968], mean action: 2.706 [0.000, 12.000],  loss: 0.023805, mae: 0.381361, mean_q: 0.464223, mean_eps: 0.000000
 2397/5000: episode: 92, duration: 2.478s, episode steps:  22, steps per second:   9, episode reward: 44.801, mean reward:  2.036 [-2.518, 31.991], mean action: 1.318 [0.000, 12.000],  loss: 0.019373, mae: 0.364642, mean_q: 0.457323, mean_eps: 0.000000
 2422/5000: episode: 93, duration: 0.340s, episode steps:  25, steps per second:  74, episode reward: -35.370, mean reward: -1.415 [-33.000,  2.965], mean action: 3.680 [0.000, 12.000],  loss: 0.017253, mae: 0.360237, mean_q: 0.492231, mean_eps: 0.000000
 2445/5000: episode: 94, duration: 0.320s, episode steps:  23, steps per second:  72, episode reward: 44.161, mean reward:  1.920 [-2.029, 32.420], mean action: 2.739 [1.000, 19.000],  loss: 0.021630, mae: 0.384583, mean_q: 0.476121, mean_eps: 0.000000
 2480/5000: episode: 95, duration: 0.448s, episode steps:  35, steps per second:  78, episode reward: -32.690, mean reward: -0.934 [-32.293,  2.490], mean action: 4.657 [0.000, 19.000],  loss: 0.020802, mae: 0.374583, mean_q: 0.451567, mean_eps: 0.000000
 2506/5000: episode: 96, duration: 0.356s, episode steps:  26, steps per second:  73, episode reward: 41.648, mean reward:  1.602 [-2.141, 32.040], mean action: 2.962 [0.000, 19.000],  loss: 0.020809, mae: 0.370645, mean_q: 0.478104, mean_eps: 0.000000
 2584/5000: episode: 97, duration: 0.998s, episode steps:  78, steps per second:  78, episode reward: -32.680, mean reward: -0.419 [-31.718,  2.810], mean action: 7.615 [0.000, 19.000],  loss: 0.020424, mae: 0.352320, mean_q: 0.499481, mean_eps: 0.000000
 2612/5000: episode: 98, duration: 0.369s, episode steps:  28, steps per second:  76, episode reward: 41.064, mean reward:  1.467 [-3.000, 32.440], mean action: 2.893 [0.000, 19.000],  loss: 0.022389, mae: 0.357969, mean_q: 0.540681, mean_eps: 0.000000
 2634/5000: episode: 99, duration: 0.296s, episode steps:  22, steps per second:  74, episode reward: 41.598, mean reward:  1.891 [-2.292, 31.638], mean action: 2.591 [0.000, 12.000],  loss: 0.019557, mae: 0.355149, mean_q: 0.558478, mean_eps: 0.000000
 2650/5000: episode: 100, duration: 0.221s, episode steps:  16, steps per second:  72, episode reward: 44.110, mean reward:  2.757 [-2.802, 31.585], mean action: 4.812 [0.000, 16.000],  loss: 0.016799, mae: 0.335260, mean_q: 0.538470, mean_eps: 0.000000
 2669/5000: episode: 101, duration: 0.257s, episode steps:  19, steps per second:  74, episode reward: 44.167, mean reward:  2.325 [-2.082, 31.946], mean action: 1.105 [0.000, 16.000],  loss: 0.022993, mae: 0.367067, mean_q: 0.503417, mean_eps: 0.000000
 2691/5000: episode: 102, duration: 0.294s, episode steps:  22, steps per second:  75, episode reward: 41.196, mean reward:  1.873 [-2.708, 32.478], mean action: 2.545 [1.000, 12.000],  loss: 0.022475, mae: 0.368228, mean_q: 0.522907, mean_eps: 0.000000
 2730/5000: episode: 103, duration: 0.496s, episode steps:  39, steps per second:  79, episode reward: 39.000, mean reward:  1.000 [-2.565, 32.310], mean action: 3.436 [0.000, 14.000],  loss: 0.019223, mae: 0.352559, mean_q: 0.523942, mean_eps: 0.000000
 2752/5000: episode: 104, duration: 0.289s, episode steps:  22, steps per second:  76, episode reward: 41.105, mean reward:  1.868 [-2.370, 32.280], mean action: 2.273 [0.000, 12.000],  loss: 0.023321, mae: 0.374930, mean_q: 0.472671, mean_eps: 0.000000
 2781/5000: episode: 105, duration: 0.381s, episode steps:  29, steps per second:  76, episode reward: 38.603, mean reward:  1.331 [-2.432, 32.420], mean action: 2.897 [0.000, 19.000],  loss: 0.021952, mae: 0.360803, mean_q: 0.442380, mean_eps: 0.000000
 2803/5000: episode: 106, duration: 0.291s, episode steps:  22, steps per second:  76, episode reward: 39.520, mean reward:  1.796 [-2.807, 31.993], mean action: 4.318 [0.000, 20.000],  loss: 0.019739, mae: 0.352313, mean_q: 0.472537, mean_eps: 0.000000
 2851/5000: episode: 107, duration: 0.603s, episode steps:  48, steps per second:  80, episode reward: 41.542, mean reward:  0.865 [-2.765, 32.370], mean action: 3.542 [0.000, 19.000],  loss: 0.021122, mae: 0.358275, mean_q: 0.491606, mean_eps: 0.000000
 2895/5000: episode: 108, duration: 0.557s, episode steps:  44, steps per second:  79, episode reward: 32.901, mean reward:  0.748 [-3.000, 32.260], mean action: 8.773 [0.000, 21.000],  loss: 0.021219, mae: 0.359504, mean_q: 0.534608, mean_eps: 0.000000
 2917/5000: episode: 109, duration: 0.294s, episode steps:  22, steps per second:  75, episode reward: 40.287, mean reward:  1.831 [-2.349, 32.190], mean action: 4.227 [0.000, 19.000],  loss: 0.020784, mae: 0.355991, mean_q: 0.477364, mean_eps: 0.000000
 2941/5000: episode: 110, duration: 0.325s, episode steps:  24, steps per second:  74, episode reward: 40.485, mean reward:  1.687 [-2.141, 32.530], mean action: 4.875 [2.000, 19.000],  loss: 0.018591, mae: 0.352860, mean_q: 0.465453, mean_eps: 0.000000
 2964/5000: episode: 111, duration: 0.301s, episode steps:  23, steps per second:  76, episode reward: 38.637, mean reward:  1.680 [-2.901, 32.150], mean action: 4.913 [0.000, 19.000],  loss: 0.017900, mae: 0.340476, mean_q: 0.528853, mean_eps: 0.000000
 2994/5000: episode: 112, duration: 0.382s, episode steps:  30, steps per second:  79, episode reward: 41.535, mean reward:  1.384 [-2.219, 32.795], mean action: 3.833 [0.000, 14.000],  loss: 0.021689, mae: 0.360157, mean_q: 0.530065, mean_eps: 0.000000
 3023/5000: episode: 113, duration: 0.378s, episode steps:  29, steps per second:  77, episode reward: 35.500, mean reward:  1.224 [-3.000, 32.700], mean action: 4.655 [0.000, 15.000],  loss: 0.023730, mae: 0.363994, mean_q: 0.491268, mean_eps: 0.000000
 3056/5000: episode: 114, duration: 0.430s, episode steps:  33, steps per second:  77, episode reward: 41.940, mean reward:  1.271 [-2.231, 32.050], mean action: 2.697 [0.000, 12.000],  loss: 0.020438, mae: 0.347420, mean_q: 0.513429, mean_eps: 0.000000
 3091/5000: episode: 115, duration: 0.451s, episode steps:  35, steps per second:  78, episode reward: 36.000, mean reward:  1.029 [-2.292, 32.150], mean action: 4.543 [0.000, 19.000],  loss: 0.021742, mae: 0.354758, mean_q: 0.526400, mean_eps: 0.000000
 3132/5000: episode: 116, duration: 0.526s, episode steps:  41, steps per second:  78, episode reward: 38.875, mean reward:  0.948 [-2.475, 32.300], mean action: 1.951 [0.000, 19.000],  loss: 0.020378, mae: 0.344376, mean_q: 0.461814, mean_eps: 0.000000
 3156/5000: episode: 117, duration: 0.314s, episode steps:  24, steps per second:  76, episode reward: 41.278, mean reward:  1.720 [-2.685, 32.040], mean action: 7.417 [0.000, 20.000],  loss: 0.019024, mae: 0.338602, mean_q: 0.511275, mean_eps: 0.000000
 3213/5000: episode: 118, duration: 0.727s, episode steps:  57, steps per second:  78, episode reward: 44.240, mean reward:  0.776 [-2.206, 32.030], mean action: 3.684 [0.000, 16.000],  loss: 0.021753, mae: 0.353868, mean_q: 0.535094, mean_eps: 0.000000
 3240/5000: episode: 119, duration: 0.354s, episode steps:  27, steps per second:  76, episode reward: 35.770, mean reward:  1.325 [-2.452, 31.850], mean action: 3.519 [0.000, 20.000],  loss: 0.019260, mae: 0.342423, mean_q: 0.494192, mean_eps: 0.000000
 3256/5000: episode: 120, duration: 0.371s, episode steps:  16, steps per second:  43, episode reward: 44.895, mean reward:  2.806 [-2.289, 32.500], mean action: 2.312 [0.000, 12.000],  loss: 0.021517, mae: 0.353893, mean_q: 0.472112, mean_eps: 0.000000
 3288/5000: episode: 121, duration: 0.412s, episode steps:  32, steps per second:  78, episode reward: 35.839, mean reward:  1.120 [-2.853, 32.801], mean action: 5.375 [0.000, 17.000],  loss: 0.022070, mae: 0.356953, mean_q: 0.479922, mean_eps: 0.000000
 3346/5000: episode: 122, duration: 0.734s, episode steps:  58, steps per second:  79, episode reward: 35.649, mean reward:  0.615 [-2.394, 32.432], mean action: 8.397 [0.000, 21.000],  loss: 0.022575, mae: 0.358154, mean_q: 0.468132, mean_eps: 0.000000
 3366/5000: episode: 123, duration: 0.269s, episode steps:  20, steps per second:  74, episode reward: 44.878, mean reward:  2.244 [-3.000, 32.280], mean action: 1.050 [0.000, 19.000],  loss: 0.018423, mae: 0.340786, mean_q: 0.513791, mean_eps: 0.000000
 3399/5000: episode: 124, duration: 0.434s, episode steps:  33, steps per second:  76, episode reward: 38.139, mean reward:  1.156 [-2.240, 31.910], mean action: 3.485 [0.000, 19.000],  loss: 0.021379, mae: 0.356235, mean_q: 0.533680, mean_eps: 0.000000
 3430/5000: episode: 125, duration: 0.412s, episode steps:  31, steps per second:  75, episode reward: 42.987, mean reward:  1.387 [-3.000, 32.200], mean action: 3.419 [0.000, 16.000],  loss: 0.019515, mae: 0.352857, mean_q: 0.531126, mean_eps: 0.000000
 3449/5000: episode: 126, duration: 0.262s, episode steps:  19, steps per second:  73, episode reward: 41.460, mean reward:  2.182 [-2.691, 32.400], mean action: 2.895 [0.000, 16.000],  loss: 0.021611, mae: 0.356215, mean_q: 0.487607, mean_eps: 0.000000
 3471/5000: episode: 127, duration: 0.294s, episode steps:  22, steps per second:  75, episode reward: 35.176, mean reward:  1.599 [-3.000, 32.080], mean action: 4.318 [0.000, 16.000],  loss: 0.022057, mae: 0.358234, mean_q: 0.463467, mean_eps: 0.000000
 3491/5000: episode: 128, duration: 0.265s, episode steps:  20, steps per second:  75, episode reward: 41.786, mean reward:  2.089 [-2.579, 32.380], mean action: 3.900 [1.000, 15.000],  loss: 0.022733, mae: 0.359726, mean_q: 0.475649, mean_eps: 0.000000
 3513/5000: episode: 129, duration: 0.291s, episode steps:  22, steps per second:  76, episode reward: 42.000, mean reward:  1.909 [-3.000, 32.160], mean action: 3.045 [2.000, 12.000],  loss: 0.021004, mae: 0.351030, mean_q: 0.522092, mean_eps: 0.000000
 3538/5000: episode: 130, duration: 0.332s, episode steps:  25, steps per second:  75, episode reward: 38.813, mean reward:  1.553 [-2.283, 32.453], mean action: 2.800 [0.000, 16.000],  loss: 0.022088, mae: 0.357423, mean_q: 0.484904, mean_eps: 0.000000
 3582/5000: episode: 131, duration: 0.569s, episode steps:  44, steps per second:  77, episode reward: 38.315, mean reward:  0.871 [-2.144, 32.020], mean action: 4.114 [0.000, 21.000],  loss: 0.019312, mae: 0.344746, mean_q: 0.524844, mean_eps: 0.000000
 3611/5000: episode: 132, duration: 0.387s, episode steps:  29, steps per second:  75, episode reward: 34.827, mean reward:  1.201 [-2.693, 31.666], mean action: 3.276 [0.000, 14.000],  loss: 0.018015, mae: 0.339049, mean_q: 0.520977, mean_eps: 0.000000
 3635/5000: episode: 133, duration: 0.313s, episode steps:  24, steps per second:  77, episode reward: 36.000, mean reward:  1.500 [-3.000, 32.490], mean action: 3.083 [0.000, 12.000],  loss: 0.021841, mae: 0.361729, mean_q: 0.475055, mean_eps: 0.000000
 3658/5000: episode: 134, duration: 0.304s, episode steps:  23, steps per second:  76, episode reward: 38.349, mean reward:  1.667 [-2.603, 32.099], mean action: 2.130 [0.000, 9.000],  loss: 0.023048, mae: 0.367422, mean_q: 0.475204, mean_eps: 0.000000
 3701/5000: episode: 135, duration: 0.550s, episode steps:  43, steps per second:  78, episode reward: 36.000, mean reward:  0.837 [-3.000, 32.290], mean action: 2.744 [0.000, 16.000],  loss: 0.017716, mae: 0.347305, mean_q: 0.424171, mean_eps: 0.000000
 3719/5000: episode: 136, duration: 0.246s, episode steps:  18, steps per second:  73, episode reward: 44.113, mean reward:  2.451 [-2.605, 31.904], mean action: 2.722 [0.000, 15.000],  loss: 0.022863, mae: 0.362442, mean_q: 0.456269, mean_eps: 0.000000
 3746/5000: episode: 137, duration: 0.356s, episode steps:  27, steps per second:  76, episode reward: 41.657, mean reward:  1.543 [-2.231, 32.860], mean action: 3.037 [0.000, 12.000],  loss: 0.022466, mae: 0.362154, mean_q: 0.482007, mean_eps: 0.000000
 3776/5000: episode: 138, duration: 0.383s, episode steps:  30, steps per second:  78, episode reward: 35.782, mean reward:  1.193 [-3.000, 31.992], mean action: 7.567 [0.000, 21.000],  loss: 0.022846, mae: 0.372176, mean_q: 0.495276, mean_eps: 0.000000
 3800/5000: episode: 139, duration: 0.319s, episode steps:  24, steps per second:  75, episode reward: 41.161, mean reward:  1.715 [-2.703, 32.077], mean action: 5.167 [0.000, 21.000],  loss: 0.020690, mae: 0.362493, mean_q: 0.452806, mean_eps: 0.000000
 3815/5000: episode: 140, duration: 0.206s, episode steps:  15, steps per second:  73, episode reward: 41.900, mean reward:  2.793 [-2.518, 32.900], mean action: 3.133 [0.000, 19.000],  loss: 0.023461, mae: 0.371387, mean_q: 0.433978, mean_eps: 0.000000
 3830/5000: episode: 141, duration: 0.209s, episode steps:  15, steps per second:  72, episode reward: 44.218, mean reward:  2.948 [-3.000, 32.039], mean action: 4.600 [0.000, 19.000],  loss: 0.019046, mae: 0.349365, mean_q: 0.462587, mean_eps: 0.000000
 3852/5000: episode: 142, duration: 0.287s, episode steps:  22, steps per second:  77, episode reward: 36.000, mean reward:  1.636 [-3.000, 32.460], mean action: 4.409 [0.000, 19.000],  loss: 0.028400, mae: 0.400204, mean_q: 0.451767, mean_eps: 0.000000
 3898/5000: episode: 143, duration: 0.597s, episode steps:  46, steps per second:  77, episode reward: 35.278, mean reward:  0.767 [-2.521, 32.361], mean action: 4.717 [0.000, 16.000],  loss: 0.023253, mae: 0.371622, mean_q: 0.538830, mean_eps: 0.000000
 3915/5000: episode: 144, duration: 0.232s, episode steps:  17, steps per second:  73, episode reward: 44.487, mean reward:  2.617 [-2.339, 32.340], mean action: 2.059 [1.000, 11.000],  loss: 0.018493, mae: 0.348805, mean_q: 0.553200, mean_eps: 0.000000
 3947/5000: episode: 145, duration: 0.417s, episode steps:  32, steps per second:  77, episode reward: 36.000, mean reward:  1.125 [-2.489, 32.150], mean action: 2.750 [0.000, 20.000],  loss: 0.020529, mae: 0.365220, mean_q: 0.474125, mean_eps: 0.000000
 4004/5000: episode: 146, duration: 0.722s, episode steps:  57, steps per second:  79, episode reward: 35.927, mean reward:  0.630 [-3.000, 32.200], mean action: 3.544 [0.000, 19.000],  loss: 0.020863, mae: 0.363911, mean_q: 0.513199, mean_eps: 0.000000
 4031/5000: episode: 147, duration: 0.349s, episode steps:  27, steps per second:  77, episode reward: 38.912, mean reward:  1.441 [-2.566, 32.300], mean action: 3.000 [0.000, 16.000],  loss: 0.020434, mae: 0.357441, mean_q: 0.526986, mean_eps: 0.000000
 4064/5000: episode: 148, duration: 0.423s, episode steps:  33, steps per second:  78, episode reward: 35.712, mean reward:  1.082 [-2.542, 31.762], mean action: 6.061 [0.000, 16.000],  loss: 0.021803, mae: 0.360958, mean_q: 0.528744, mean_eps: 0.000000
 4105/5000: episode: 149, duration: 0.530s, episode steps:  41, steps per second:  77, episode reward: 38.120, mean reward:  0.930 [-2.676, 32.028], mean action: 6.659 [0.000, 16.000],  loss: 0.021660, mae: 0.363656, mean_q: 0.488683, mean_eps: 0.000000
 4128/5000: episode: 150, duration: 0.310s, episode steps:  23, steps per second:  74, episode reward: 39.000, mean reward:  1.696 [-3.000, 32.010], mean action: 3.739 [0.000, 19.000],  loss: 0.019824, mae: 0.354823, mean_q: 0.493579, mean_eps: 0.000000
 4153/5000: episode: 151, duration: 0.326s, episode steps:  25, steps per second:  77, episode reward: 38.746, mean reward:  1.550 [-2.589, 32.060], mean action: 2.760 [0.000, 14.000],  loss: 0.019938, mae: 0.357078, mean_q: 0.499531, mean_eps: 0.000000
 4182/5000: episode: 152, duration: 0.381s, episode steps:  29, steps per second:  76, episode reward: 38.904, mean reward:  1.342 [-3.000, 32.184], mean action: 2.621 [0.000, 20.000],  loss: 0.018694, mae: 0.355931, mean_q: 0.473100, mean_eps: 0.000000
 4214/5000: episode: 153, duration: 0.414s, episode steps:  32, steps per second:  77, episode reward: 41.042, mean reward:  1.283 [-2.182, 32.430], mean action: 4.844 [0.000, 20.000],  loss: 0.018130, mae: 0.354762, mean_q: 0.492391, mean_eps: 0.000000
 4234/5000: episode: 154, duration: 0.273s, episode steps:  20, steps per second:  73, episode reward: 42.000, mean reward:  2.100 [-2.493, 32.220], mean action: 2.600 [1.000, 9.000],  loss: 0.019654, mae: 0.359829, mean_q: 0.511701, mean_eps: 0.000000
 4257/5000: episode: 155, duration: 0.343s, episode steps:  23, steps per second:  67, episode reward: 38.895, mean reward:  1.691 [-2.569, 31.915], mean action: 2.435 [0.000, 9.000],  loss: 0.022660, mae: 0.369719, mean_q: 0.547659, mean_eps: 0.000000
 4301/5000: episode: 156, duration: 0.566s, episode steps:  44, steps per second:  78, episode reward: 37.733, mean reward:  0.858 [-2.185, 32.310], mean action: 3.864 [1.000, 19.000],  loss: 0.020277, mae: 0.358087, mean_q: 0.496855, mean_eps: 0.000000
 4332/5000: episode: 157, duration: 0.744s, episode steps:  31, steps per second:  42, episode reward: 39.000, mean reward:  1.258 [-2.351, 29.299], mean action: 2.355 [0.000, 19.000],  loss: 0.020808, mae: 0.366229, mean_q: 0.470068, mean_eps: 0.000000
 4357/5000: episode: 158, duration: 0.437s, episode steps:  25, steps per second:  57, episode reward: 38.578, mean reward:  1.543 [-2.622, 32.162], mean action: 4.320 [0.000, 19.000],  loss: 0.019774, mae: 0.355708, mean_q: 0.516220, mean_eps: 0.000000
 4377/5000: episode: 159, duration: 0.267s, episode steps:  20, steps per second:  75, episode reward: 44.337, mean reward:  2.217 [-2.248, 32.040], mean action: 3.050 [1.000, 19.000],  loss: 0.018718, mae: 0.349231, mean_q: 0.481348, mean_eps: 0.000000
 4402/5000: episode: 160, duration: 0.480s, episode steps:  25, steps per second:  52, episode reward: 39.000, mean reward:  1.560 [-2.244, 32.610], mean action: 3.560 [0.000, 19.000],  loss: 0.020670, mae: 0.359665, mean_q: 0.464156, mean_eps: 0.000000
 4421/5000: episode: 161, duration: 0.287s, episode steps:  19, steps per second:  66, episode reward: 41.802, mean reward:  2.200 [-2.221, 32.201], mean action: 2.263 [0.000, 9.000],  loss: 0.023986, mae: 0.365251, mean_q: 0.568671, mean_eps: 0.000000
 4435/5000: episode: 162, duration: 0.283s, episode steps:  14, steps per second:  49, episode reward: 44.371, mean reward:  3.169 [-2.269, 32.970], mean action: 2.214 [0.000, 11.000],  loss: 0.017122, mae: 0.334364, mean_q: 0.553258, mean_eps: 0.000000
 4469/5000: episode: 163, duration: 1.423s, episode steps:  34, steps per second:  24, episode reward: 37.735, mean reward:  1.110 [-2.408, 32.340], mean action: 8.088 [0.000, 20.000],  loss: 0.017263, mae: 0.342392, mean_q: 0.526101, mean_eps: 0.000000
 4488/5000: episode: 164, duration: 0.563s, episode steps:  19, steps per second:  34, episode reward: 41.756, mean reward:  2.198 [-2.650, 32.200], mean action: 2.053 [0.000, 12.000],  loss: 0.019267, mae: 0.364340, mean_q: 0.457892, mean_eps: 0.000000
 4518/5000: episode: 165, duration: 0.437s, episode steps:  30, steps per second:  69, episode reward: 46.238, mean reward:  1.541 [-0.214, 32.043], mean action: 2.800 [1.000, 13.000],  loss: 0.019272, mae: 0.351329, mean_q: 0.490575, mean_eps: 0.000000
 4541/5000: episode: 166, duration: 0.349s, episode steps:  23, steps per second:  66, episode reward: 38.778, mean reward:  1.686 [-2.876, 32.720], mean action: 1.913 [0.000, 14.000],  loss: 0.018853, mae: 0.342464, mean_q: 0.503681, mean_eps: 0.000000
 4562/5000: episode: 167, duration: 0.291s, episode steps:  21, steps per second:  72, episode reward: 44.672, mean reward:  2.127 [-2.179, 31.969], mean action: 3.238 [1.000, 15.000],  loss: 0.020837, mae: 0.360619, mean_q: 0.498924, mean_eps: 0.000000
 4590/5000: episode: 168, duration: 0.981s, episode steps:  28, steps per second:  29, episode reward: 38.595, mean reward:  1.378 [-2.580, 32.070], mean action: 1.643 [0.000, 16.000],  loss: 0.019515, mae: 0.355674, mean_q: 0.505149, mean_eps: 0.000000
 4610/5000: episode: 169, duration: 0.750s, episode steps:  20, steps per second:  27, episode reward: 41.427, mean reward:  2.071 [-2.165, 32.130], mean action: 1.900 [0.000, 15.000],  loss: 0.018684, mae: 0.352664, mean_q: 0.461378, mean_eps: 0.000000
 4644/5000: episode: 170, duration: 0.617s, episode steps:  34, steps per second:  55, episode reward: 37.762, mean reward:  1.111 [-2.900, 32.044], mean action: 2.941 [0.000, 15.000],  loss: 0.020901, mae: 0.363215, mean_q: 0.459120, mean_eps: 0.000000
 4661/5000: episode: 171, duration: 0.260s, episode steps:  17, steps per second:  65, episode reward: 44.903, mean reward:  2.641 [-2.249, 33.412], mean action: 1.765 [0.000, 8.000],  loss: 0.020776, mae: 0.359252, mean_q: 0.492836, mean_eps: 0.000000
 4700/5000: episode: 172, duration: 0.500s, episode steps:  39, steps per second:  78, episode reward: 35.587, mean reward:  0.912 [-3.000, 32.130], mean action: 6.769 [0.000, 21.000],  loss: 0.019039, mae: 0.346510, mean_q: 0.521337, mean_eps: 0.000000
 4723/5000: episode: 173, duration: 0.312s, episode steps:  23, steps per second:  74, episode reward: 44.561, mean reward:  1.937 [-2.534, 32.470], mean action: 1.391 [0.000, 15.000],  loss: 0.019067, mae: 0.354758, mean_q: 0.487063, mean_eps: 0.000000
 4745/5000: episode: 174, duration: 0.296s, episode steps:  22, steps per second:  74, episode reward: 44.630, mean reward:  2.029 [-2.062, 32.700], mean action: 2.227 [0.000, 15.000],  loss: 0.020969, mae: 0.361677, mean_q: 0.449141, mean_eps: 0.000000
 4766/5000: episode: 175, duration: 0.277s, episode steps:  21, steps per second:  76, episode reward: 41.618, mean reward:  1.982 [-2.268, 32.261], mean action: 5.857 [0.000, 16.000],  loss: 0.020200, mae: 0.363899, mean_q: 0.447271, mean_eps: 0.000000
 4783/5000: episode: 176, duration: 0.270s, episode steps:  17, steps per second:  63, episode reward: 41.531, mean reward:  2.443 [-2.511, 31.741], mean action: 3.353 [0.000, 16.000],  loss: 0.020436, mae: 0.358632, mean_q: 0.454706, mean_eps: 0.000000
 4811/5000: episode: 177, duration: 0.362s, episode steps:  28, steps per second:  77, episode reward: 32.816, mean reward:  1.172 [-3.000, 32.340], mean action: 5.714 [0.000, 19.000],  loss: 0.022559, mae: 0.367665, mean_q: 0.464611, mean_eps: 0.000000
 4851/5000: episode: 178, duration: 0.500s, episode steps:  40, steps per second:  80, episode reward: 35.238, mean reward:  0.881 [-2.728, 32.720], mean action: 3.100 [0.000, 19.000],  loss: 0.022553, mae: 0.366676, mean_q: 0.507015, mean_eps: 0.000000
 4876/5000: episode: 179, duration: 0.327s, episode steps:  25, steps per second:  76, episode reward: 41.340, mean reward:  1.654 [-2.227, 32.180], mean action: 2.720 [0.000, 14.000],  loss: 0.021207, mae: 0.365879, mean_q: 0.465566, mean_eps: 0.000000
 4932/5000: episode: 180, duration: 0.908s, episode steps:  56, steps per second:  62, episode reward: 38.237, mean reward:  0.683 [-2.857, 32.150], mean action: 9.714 [0.000, 21.000],  loss: 0.020004, mae: 0.358242, mean_q: 0.502164, mean_eps: 0.000000
 4961/5000: episode: 181, duration: 0.534s, episode steps:  29, steps per second:  54, episode reward: 41.684, mean reward:  1.437 [-2.671, 32.253], mean action: 2.207 [0.000, 12.000],  loss: 0.021806, mae: 0.368120, mean_q: 0.503510, mean_eps: 0.000000
 5000/5000: episode: 182, duration: 0.606s, episode steps:  39, steps per second:  64, episode reward: 41.071, mean reward:  1.053 [-2.643, 31.643], mean action: 2.923 [0.000, 20.000],  loss: 0.019656, mae: 0.359227, mean_q: 0.465805, mean_eps: 0.000000
done, took 67.355 seconds
DQN Evaluation: 4601 victories out of 5445 episodes
Training for 5000 steps ...
   22/5000: episode: 1, duration: 0.181s, episode steps:  22, steps per second: 121, episode reward: 37.968, mean reward:  1.726 [-2.316, 32.340], mean action: 7.091 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   48/5000: episode: 2, duration: 0.179s, episode steps:  26, steps per second: 145, episode reward: -32.070, mean reward: -1.233 [-31.803,  2.640], mean action: 7.462 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   64/5000: episode: 3, duration: 0.117s, episode steps:  16, steps per second: 136, episode reward: 38.658, mean reward:  2.416 [-2.640, 32.188], mean action: 3.062 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   88/5000: episode: 4, duration: 0.156s, episode steps:  24, steps per second: 154, episode reward: -33.000, mean reward: -1.375 [-29.874,  2.660], mean action: 3.000 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  111/5000: episode: 5, duration: 0.151s, episode steps:  23, steps per second: 153, episode reward: -35.580, mean reward: -1.547 [-32.211,  2.615], mean action: 4.348 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  152/5000: episode: 6, duration: 0.255s, episode steps:  41, steps per second: 161, episode reward: -32.320, mean reward: -0.788 [-31.981,  2.965], mean action: 8.439 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  173/5000: episode: 7, duration: 0.154s, episode steps:  21, steps per second: 137, episode reward: 38.593, mean reward:  1.838 [-2.444, 32.260], mean action: 2.429 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  195/5000: episode: 8, duration: 0.145s, episode steps:  22, steps per second: 152, episode reward: 42.000, mean reward:  1.909 [-2.914, 33.000], mean action: 3.682 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  218/5000: episode: 9, duration: 0.150s, episode steps:  23, steps per second: 154, episode reward: -30.000, mean reward: -1.304 [-29.248,  2.796], mean action: 3.739 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  257/5000: episode: 10, duration: 0.249s, episode steps:  39, steps per second: 157, episode reward: 36.000, mean reward:  0.923 [-3.000, 33.000], mean action: 3.641 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  283/5000: episode: 11, duration: 0.157s, episode steps:  26, steps per second: 165, episode reward: 33.000, mean reward:  1.269 [-2.609, 32.470], mean action: 4.000 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  310/5000: episode: 12, duration: 0.172s, episode steps:  27, steps per second: 157, episode reward: -32.770, mean reward: -1.214 [-32.221,  2.650], mean action: 7.593 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  331/5000: episode: 13, duration: 0.142s, episode steps:  21, steps per second: 148, episode reward: 38.300, mean reward:  1.824 [-2.238, 31.904], mean action: 3.810 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  352/5000: episode: 14, duration: 0.143s, episode steps:  21, steps per second: 146, episode reward: 38.496, mean reward:  1.833 [-2.480, 32.178], mean action: 2.619 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  375/5000: episode: 15, duration: 0.147s, episode steps:  23, steps per second: 157, episode reward: 38.715, mean reward:  1.683 [-2.309, 32.180], mean action: 2.913 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  387/5000: episode: 16, duration: 0.091s, episode steps:  12, steps per second: 132, episode reward: 38.532, mean reward:  3.211 [-3.000, 32.442], mean action: 3.917 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  417/5000: episode: 17, duration: 0.187s, episode steps:  30, steps per second: 161, episode reward: 39.808, mean reward:  1.327 [-2.219, 32.013], mean action: 3.000 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  438/5000: episode: 18, duration: 0.138s, episode steps:  21, steps per second: 152, episode reward: 32.863, mean reward:  1.565 [-3.000, 32.370], mean action: 4.143 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  459/5000: episode: 19, duration: 0.162s, episode steps:  21, steps per second: 130, episode reward: 43.918, mean reward:  2.091 [-1.545, 32.420], mean action: 5.000 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  487/5000: episode: 20, duration: 0.175s, episode steps:  28, steps per second: 160, episode reward: -32.100, mean reward: -1.146 [-32.039,  2.323], mean action: 3.714 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  513/5000: episode: 21, duration: 0.163s, episode steps:  26, steps per second: 159, episode reward: 32.395, mean reward:  1.246 [-3.000, 32.740], mean action: 5.192 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  553/5000: episode: 22, duration: 0.238s, episode steps:  40, steps per second: 168, episode reward: 35.682, mean reward:  0.892 [-3.000, 32.502], mean action: 2.475 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  572/5000: episode: 23, duration: 0.129s, episode steps:  19, steps per second: 147, episode reward: 38.551, mean reward:  2.029 [-2.349, 31.973], mean action: 3.421 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  599/5000: episode: 24, duration: 0.177s, episode steps:  27, steps per second: 152, episode reward: 38.239, mean reward:  1.416 [-2.250, 32.017], mean action: 3.852 [2.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  616/5000: episode: 25, duration: 0.117s, episode steps:  17, steps per second: 146, episode reward: 38.848, mean reward:  2.285 [-3.000, 32.410], mean action: 4.294 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  636/5000: episode: 26, duration: 0.137s, episode steps:  20, steps per second: 146, episode reward: 38.625, mean reward:  1.931 [-2.800, 32.220], mean action: 6.200 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  653/5000: episode: 27, duration: 0.116s, episode steps:  17, steps per second: 146, episode reward: 41.388, mean reward:  2.435 [-3.000, 32.040], mean action: 2.588 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  675/5000: episode: 28, duration: 0.150s, episode steps:  22, steps per second: 147, episode reward: -33.000, mean reward: -1.500 [-32.102,  2.910], mean action: 5.091 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  695/5000: episode: 29, duration: 0.143s, episode steps:  20, steps per second: 140, episode reward: 38.515, mean reward:  1.926 [-2.941, 32.031], mean action: 2.950 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  710/5000: episode: 30, duration: 0.104s, episode steps:  15, steps per second: 145, episode reward: 38.807, mean reward:  2.587 [-3.000, 32.904], mean action: 5.133 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  729/5000: episode: 31, duration: 0.126s, episode steps:  19, steps per second: 151, episode reward: 38.236, mean reward:  2.012 [-3.000, 31.823], mean action: 3.158 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  738/5000: episode: 32, duration: 0.080s, episode steps:   9, steps per second: 112, episode reward: 47.220, mean reward:  5.247 [ 0.484, 33.000], mean action: 0.667 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  755/5000: episode: 33, duration: 0.118s, episode steps:  17, steps per second: 145, episode reward: 36.000, mean reward:  2.118 [-2.901, 29.694], mean action: 2.882 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  779/5000: episode: 34, duration: 0.149s, episode steps:  24, steps per second: 161, episode reward: -41.280, mean reward: -1.720 [-32.211,  3.000], mean action: 7.167 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  795/5000: episode: 35, duration: 0.112s, episode steps:  16, steps per second: 143, episode reward: 38.727, mean reward:  2.420 [-2.378, 31.977], mean action: 3.812 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  816/5000: episode: 36, duration: 0.144s, episode steps:  21, steps per second: 146, episode reward: 38.015, mean reward:  1.810 [-2.322, 32.901], mean action: 4.286 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  835/5000: episode: 37, duration: 0.131s, episode steps:  19, steps per second: 145, episode reward: 39.000, mean reward:  2.053 [-2.206, 30.440], mean action: 2.526 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  875/5000: episode: 38, duration: 0.233s, episode steps:  40, steps per second: 171, episode reward: 32.433, mean reward:  0.811 [-2.344, 32.530], mean action: 3.450 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  895/5000: episode: 39, duration: 0.144s, episode steps:  20, steps per second: 139, episode reward: 38.689, mean reward:  1.934 [-2.668, 32.150], mean action: 6.050 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  913/5000: episode: 40, duration: 0.120s, episode steps:  18, steps per second: 150, episode reward: 35.769, mean reward:  1.987 [-3.000, 32.560], mean action: 4.722 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  939/5000: episode: 41, duration: 0.169s, episode steps:  26, steps per second: 154, episode reward: 35.274, mean reward:  1.357 [-2.500, 32.244], mean action: 4.500 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  969/5000: episode: 42, duration: 0.182s, episode steps:  30, steps per second: 165, episode reward: -32.490, mean reward: -1.083 [-32.009,  2.891], mean action: 4.133 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  996/5000: episode: 43, duration: 0.170s, episode steps:  27, steps per second: 159, episode reward: 32.728, mean reward:  1.212 [-2.498, 32.400], mean action: 5.926 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1018/5000: episode: 44, duration: 0.274s, episode steps:  22, steps per second:  80, episode reward: 38.616, mean reward:  1.755 [-3.000, 32.638], mean action: 5.864 [0.000, 15.000],  loss: 0.018770, mae: 0.348591, mean_q: 0.519251, mean_eps: 0.000000
 1044/5000: episode: 45, duration: 0.343s, episode steps:  26, steps per second:  76, episode reward: 35.847, mean reward:  1.379 [-2.546, 32.310], mean action: 4.731 [0.000, 19.000],  loss: 0.021353, mae: 0.360705, mean_q: 0.535053, mean_eps: 0.000000
 1066/5000: episode: 46, duration: 0.284s, episode steps:  22, steps per second:  78, episode reward: 32.264, mean reward:  1.467 [-3.000, 32.050], mean action: 6.409 [0.000, 20.000],  loss: 0.020574, mae: 0.361029, mean_q: 0.529197, mean_eps: 0.000000
 1094/5000: episode: 47, duration: 0.367s, episode steps:  28, steps per second:  76, episode reward: 37.900, mean reward:  1.354 [-2.154, 33.000], mean action: 3.571 [0.000, 19.000],  loss: 0.015206, mae: 0.336685, mean_q: 0.516975, mean_eps: 0.000000
 1123/5000: episode: 48, duration: 0.376s, episode steps:  29, steps per second:  77, episode reward: -32.570, mean reward: -1.123 [-33.000,  2.757], mean action: 3.793 [0.000, 19.000],  loss: 0.019692, mae: 0.355434, mean_q: 0.559575, mean_eps: 0.000000
 1145/5000: episode: 49, duration: 0.297s, episode steps:  22, steps per second:  74, episode reward: 38.080, mean reward:  1.731 [-2.335, 32.599], mean action: 5.682 [1.000, 15.000],  loss: 0.019168, mae: 0.354378, mean_q: 0.550151, mean_eps: 0.000000
 1169/5000: episode: 50, duration: 0.311s, episode steps:  24, steps per second:  77, episode reward: 32.772, mean reward:  1.366 [-2.466, 32.102], mean action: 3.708 [0.000, 14.000],  loss: 0.018293, mae: 0.344438, mean_q: 0.515022, mean_eps: 0.000000
 1192/5000: episode: 51, duration: 0.335s, episode steps:  23, steps per second:  69, episode reward: 35.447, mean reward:  1.541 [-2.320, 31.777], mean action: 4.000 [0.000, 16.000],  loss: 0.020905, mae: 0.354246, mean_q: 0.520241, mean_eps: 0.000000
 1210/5000: episode: 52, duration: 0.248s, episode steps:  18, steps per second:  73, episode reward: 38.063, mean reward:  2.115 [-2.210, 32.193], mean action: 3.500 [0.000, 16.000],  loss: 0.018282, mae: 0.348782, mean_q: 0.478561, mean_eps: 0.000000
 1236/5000: episode: 53, duration: 0.344s, episode steps:  26, steps per second:  75, episode reward: 33.000, mean reward:  1.269 [-2.413, 32.880], mean action: 6.385 [0.000, 19.000],  loss: 0.016063, mae: 0.338385, mean_q: 0.476556, mean_eps: 0.000000
 1256/5000: episode: 54, duration: 0.272s, episode steps:  20, steps per second:  73, episode reward: 35.138, mean reward:  1.757 [-3.000, 32.067], mean action: 5.150 [0.000, 19.000],  loss: 0.016960, mae: 0.341036, mean_q: 0.530170, mean_eps: 0.000000
 1286/5000: episode: 55, duration: 0.427s, episode steps:  30, steps per second:  70, episode reward: 32.186, mean reward:  1.073 [-2.546, 32.372], mean action: 8.133 [0.000, 20.000],  loss: 0.020831, mae: 0.365023, mean_q: 0.512821, mean_eps: 0.000000
 1310/5000: episode: 56, duration: 0.312s, episode steps:  24, steps per second:  77, episode reward: 35.370, mean reward:  1.474 [-2.408, 31.993], mean action: 3.625 [0.000, 12.000],  loss: 0.020066, mae: 0.366670, mean_q: 0.473290, mean_eps: 0.000000
 1342/5000: episode: 57, duration: 0.413s, episode steps:  32, steps per second:  78, episode reward: 32.644, mean reward:  1.020 [-2.682, 31.841], mean action: 7.094 [0.000, 20.000],  loss: 0.019358, mae: 0.363554, mean_q: 0.449011, mean_eps: 0.000000
 1378/5000: episode: 58, duration: 0.467s, episode steps:  36, steps per second:  77, episode reward: -32.260, mean reward: -0.896 [-32.543,  2.180], mean action: 4.194 [0.000, 14.000],  loss: 0.019253, mae: 0.358728, mean_q: 0.472838, mean_eps: 0.000000
 1409/5000: episode: 59, duration: 0.400s, episode steps:  31, steps per second:  77, episode reward: 35.407, mean reward:  1.142 [-2.278, 31.851], mean action: 4.258 [0.000, 19.000],  loss: 0.018391, mae: 0.355151, mean_q: 0.479903, mean_eps: 0.000000
 1433/5000: episode: 60, duration: 0.358s, episode steps:  24, steps per second:  67, episode reward: 35.890, mean reward:  1.495 [-2.268, 32.890], mean action: 3.958 [0.000, 15.000],  loss: 0.020033, mae: 0.369174, mean_q: 0.446718, mean_eps: 0.000000
 1455/5000: episode: 61, duration: 0.302s, episode steps:  22, steps per second:  73, episode reward: 38.073, mean reward:  1.731 [-2.298, 32.903], mean action: 2.909 [0.000, 15.000],  loss: 0.019430, mae: 0.378280, mean_q: 0.478738, mean_eps: 0.000000
 1478/5000: episode: 62, duration: 0.309s, episode steps:  23, steps per second:  74, episode reward: 34.607, mean reward:  1.505 [-2.344, 32.100], mean action: 3.957 [0.000, 19.000],  loss: 0.019714, mae: 0.370262, mean_q: 0.470742, mean_eps: 0.000000
 1502/5000: episode: 63, duration: 0.329s, episode steps:  24, steps per second:  73, episode reward: 38.226, mean reward:  1.593 [-2.823, 32.217], mean action: 5.458 [0.000, 19.000],  loss: 0.019843, mae: 0.362570, mean_q: 0.456281, mean_eps: 0.000000
 1520/5000: episode: 64, duration: 0.259s, episode steps:  18, steps per second:  70, episode reward: 37.698, mean reward:  2.094 [-3.000, 32.724], mean action: 6.000 [0.000, 20.000],  loss: 0.017275, mae: 0.337054, mean_q: 0.512179, mean_eps: 0.000000
 1534/5000: episode: 65, duration: 0.199s, episode steps:  14, steps per second:  70, episode reward: 44.335, mean reward:  3.167 [-2.599, 33.000], mean action: 4.929 [0.000, 15.000],  loss: 0.013750, mae: 0.323135, mean_q: 0.501410, mean_eps: 0.000000
 1559/5000: episode: 66, duration: 0.323s, episode steps:  25, steps per second:  77, episode reward: 32.108, mean reward:  1.284 [-3.000, 32.040], mean action: 5.440 [0.000, 17.000],  loss: 0.019929, mae: 0.359980, mean_q: 0.458854, mean_eps: 0.000000
 1583/5000: episode: 67, duration: 0.314s, episode steps:  24, steps per second:  77, episode reward: -32.450, mean reward: -1.352 [-32.173,  2.730], mean action: 4.375 [0.000, 15.000],  loss: 0.021682, mae: 0.362488, mean_q: 0.484879, mean_eps: 0.000000
 1612/5000: episode: 68, duration: 0.371s, episode steps:  29, steps per second:  78, episode reward: -38.140, mean reward: -1.315 [-32.294,  2.400], mean action: 2.414 [0.000, 12.000],  loss: 0.018858, mae: 0.345130, mean_q: 0.503215, mean_eps: 0.000000
 1641/5000: episode: 69, duration: 0.380s, episode steps:  29, steps per second:  76, episode reward: -32.910, mean reward: -1.135 [-31.992,  2.580], mean action: 5.448 [0.000, 21.000],  loss: 0.019888, mae: 0.357579, mean_q: 0.457378, mean_eps: 0.000000
 1668/5000: episode: 70, duration: 0.345s, episode steps:  27, steps per second:  78, episode reward: 33.000, mean reward:  1.222 [-3.000, 32.250], mean action: 3.074 [0.000, 19.000],  loss: 0.020489, mae: 0.360174, mean_q: 0.454824, mean_eps: 0.000000
 1691/5000: episode: 71, duration: 0.305s, episode steps:  23, steps per second:  75, episode reward: 35.777, mean reward:  1.556 [-2.780, 32.059], mean action: 5.435 [0.000, 19.000],  loss: 0.021393, mae: 0.363590, mean_q: 0.459762, mean_eps: 0.000000
 1714/5000: episode: 72, duration: 0.331s, episode steps:  23, steps per second:  69, episode reward: 35.340, mean reward:  1.537 [-2.567, 32.170], mean action: 5.565 [0.000, 15.000],  loss: 0.020585, mae: 0.362566, mean_q: 0.507683, mean_eps: 0.000000
 1735/5000: episode: 73, duration: 0.282s, episode steps:  21, steps per second:  75, episode reward: 37.676, mean reward:  1.794 [-3.000, 33.000], mean action: 5.429 [0.000, 19.000],  loss: 0.018548, mae: 0.355540, mean_q: 0.466423, mean_eps: 0.000000
 1782/5000: episode: 74, duration: 0.616s, episode steps:  47, steps per second:  76, episode reward: 34.394, mean reward:  0.732 [-2.197, 32.090], mean action: 4.170 [0.000, 15.000],  loss: 0.017054, mae: 0.351555, mean_q: 0.426762, mean_eps: 0.000000
 1798/5000: episode: 75, duration: 0.217s, episode steps:  16, steps per second:  74, episode reward: 38.325, mean reward:  2.395 [-3.000, 32.710], mean action: 5.438 [0.000, 19.000],  loss: 0.022508, mae: 0.379557, mean_q: 0.458800, mean_eps: 0.000000
 1818/5000: episode: 76, duration: 0.261s, episode steps:  20, steps per second:  77, episode reward: 35.575, mean reward:  1.779 [-2.879, 32.866], mean action: 5.650 [0.000, 15.000],  loss: 0.024696, mae: 0.390649, mean_q: 0.490095, mean_eps: 0.000000
 1838/5000: episode: 77, duration: 0.387s, episode steps:  20, steps per second:  52, episode reward: 33.000, mean reward:  1.650 [-2.332, 29.060], mean action: 3.300 [0.000, 11.000],  loss: 0.017859, mae: 0.354395, mean_q: 0.498443, mean_eps: 0.000000
 1858/5000: episode: 78, duration: 0.414s, episode steps:  20, steps per second:  48, episode reward: 37.960, mean reward:  1.898 [-2.630, 31.941], mean action: 3.700 [0.000, 15.000],  loss: 0.019387, mae: 0.361579, mean_q: 0.487843, mean_eps: 0.000000
 1890/5000: episode: 79, duration: 0.421s, episode steps:  32, steps per second:  76, episode reward: -38.320, mean reward: -1.197 [-32.073,  3.000], mean action: 7.438 [0.000, 19.000],  loss: 0.021703, mae: 0.368783, mean_q: 0.438520, mean_eps: 0.000000
 1914/5000: episode: 80, duration: 0.327s, episode steps:  24, steps per second:  73, episode reward: 38.278, mean reward:  1.595 [-2.506, 32.420], mean action: 4.417 [0.000, 13.000],  loss: 0.020914, mae: 0.366367, mean_q: 0.447250, mean_eps: 0.000000
 1937/5000: episode: 81, duration: 0.306s, episode steps:  23, steps per second:  75, episode reward: 38.551, mean reward:  1.676 [-3.000, 32.210], mean action: 3.783 [0.000, 15.000],  loss: 0.019861, mae: 0.355332, mean_q: 0.445227, mean_eps: 0.000000
 1963/5000: episode: 82, duration: 0.343s, episode steps:  26, steps per second:  76, episode reward: 38.877, mean reward:  1.495 [-2.616, 32.240], mean action: 5.769 [0.000, 20.000],  loss: 0.022085, mae: 0.368913, mean_q: 0.503230, mean_eps: 0.000000
 1980/5000: episode: 83, duration: 0.232s, episode steps:  17, steps per second:  73, episode reward: 38.357, mean reward:  2.256 [-2.791, 32.091], mean action: 4.000 [0.000, 16.000],  loss: 0.016782, mae: 0.348840, mean_q: 0.471092, mean_eps: 0.000000
 2000/5000: episode: 84, duration: 0.266s, episode steps:  20, steps per second:  75, episode reward: 35.829, mean reward:  1.791 [-2.540, 32.829], mean action: 4.150 [0.000, 16.000],  loss: 0.018744, mae: 0.361897, mean_q: 0.448269, mean_eps: 0.000000
 2026/5000: episode: 85, duration: 0.342s, episode steps:  26, steps per second:  76, episode reward: 32.356, mean reward:  1.244 [-2.759, 31.838], mean action: 5.462 [0.000, 19.000],  loss: 0.019485, mae: 0.368331, mean_q: 0.459956, mean_eps: 0.000000
 2042/5000: episode: 86, duration: 0.220s, episode steps:  16, steps per second:  73, episode reward: 41.719, mean reward:  2.607 [-2.208, 32.170], mean action: 2.938 [0.000, 16.000],  loss: 0.016260, mae: 0.343763, mean_q: 0.489027, mean_eps: 0.000000
 2073/5000: episode: 87, duration: 0.407s, episode steps:  31, steps per second:  76, episode reward: 35.831, mean reward:  1.156 [-2.432, 32.059], mean action: 5.000 [0.000, 19.000],  loss: 0.020403, mae: 0.361889, mean_q: 0.507046, mean_eps: 0.000000
 2093/5000: episode: 88, duration: 0.267s, episode steps:  20, steps per second:  75, episode reward: 38.490, mean reward:  1.924 [-2.798, 32.390], mean action: 5.200 [0.000, 19.000],  loss: 0.016633, mae: 0.340977, mean_q: 0.505268, mean_eps: 0.000000
 2123/5000: episode: 89, duration: 0.389s, episode steps:  30, steps per second:  77, episode reward: 35.331, mean reward:  1.178 [-3.000, 32.633], mean action: 3.367 [0.000, 19.000],  loss: 0.019897, mae: 0.361538, mean_q: 0.526163, mean_eps: 0.000000
 2139/5000: episode: 90, duration: 0.218s, episode steps:  16, steps per second:  73, episode reward: 41.354, mean reward:  2.585 [-2.283, 32.903], mean action: 5.875 [0.000, 19.000],  loss: 0.023010, mae: 0.379363, mean_q: 0.509944, mean_eps: 0.000000
 2165/5000: episode: 91, duration: 0.343s, episode steps:  26, steps per second:  76, episode reward: -32.810, mean reward: -1.262 [-32.143,  2.343], mean action: 5.808 [0.000, 19.000],  loss: 0.018807, mae: 0.359884, mean_q: 0.490914, mean_eps: 0.000000
 2183/5000: episode: 92, duration: 0.240s, episode steps:  18, steps per second:  75, episode reward: 38.471, mean reward:  2.137 [-2.564, 30.983], mean action: 6.722 [0.000, 15.000],  loss: 0.024435, mae: 0.389591, mean_q: 0.477671, mean_eps: 0.000000
 2199/5000: episode: 93, duration: 0.223s, episode steps:  16, steps per second:  72, episode reward: 38.572, mean reward:  2.411 [-2.656, 32.460], mean action: 5.500 [0.000, 16.000],  loss: 0.021199, mae: 0.369086, mean_q: 0.475378, mean_eps: 0.000000
 2227/5000: episode: 94, duration: 0.365s, episode steps:  28, steps per second:  77, episode reward: 32.951, mean reward:  1.177 [-3.000, 32.301], mean action: 4.821 [0.000, 16.000],  loss: 0.019997, mae: 0.364674, mean_q: 0.492825, mean_eps: 0.000000
 2243/5000: episode: 95, duration: 0.215s, episode steps:  16, steps per second:  74, episode reward: 38.300, mean reward:  2.394 [-2.903, 32.130], mean action: 5.000 [0.000, 15.000],  loss: 0.020068, mae: 0.360000, mean_q: 0.549965, mean_eps: 0.000000
 2269/5000: episode: 96, duration: 0.343s, episode steps:  26, steps per second:  76, episode reward: 32.900, mean reward:  1.265 [-2.533, 32.120], mean action: 5.615 [0.000, 15.000],  loss: 0.019895, mae: 0.361434, mean_q: 0.516636, mean_eps: 0.000000
 2289/5000: episode: 97, duration: 0.267s, episode steps:  20, steps per second:  75, episode reward: 38.826, mean reward:  1.941 [-2.407, 32.056], mean action: 3.500 [0.000, 16.000],  loss: 0.020323, mae: 0.371675, mean_q: 0.466525, mean_eps: 0.000000
 2310/5000: episode: 98, duration: 0.278s, episode steps:  21, steps per second:  75, episode reward: 34.838, mean reward:  1.659 [-3.000, 32.034], mean action: 5.000 [0.000, 16.000],  loss: 0.020053, mae: 0.369782, mean_q: 0.447213, mean_eps: 0.000000
 2324/5000: episode: 99, duration: 0.193s, episode steps:  14, steps per second:  73, episode reward: 41.787, mean reward:  2.985 [-2.639, 33.000], mean action: 5.500 [0.000, 16.000],  loss: 0.017568, mae: 0.373479, mean_q: 0.420240, mean_eps: 0.000000
 2344/5000: episode: 100, duration: 0.262s, episode steps:  20, steps per second:  76, episode reward: 39.000, mean reward:  1.950 [-3.000, 32.470], mean action: 2.800 [0.000, 16.000],  loss: 0.021046, mae: 0.393330, mean_q: 0.406355, mean_eps: 0.000000
 2370/5000: episode: 101, duration: 0.338s, episode steps:  26, steps per second:  77, episode reward: 32.373, mean reward:  1.245 [-3.000, 32.220], mean action: 4.769 [0.000, 19.000],  loss: 0.014702, mae: 0.364155, mean_q: 0.411997, mean_eps: 0.000000
 2390/5000: episode: 102, duration: 0.266s, episode steps:  20, steps per second:  75, episode reward: 41.190, mean reward:  2.060 [-2.184, 32.805], mean action: 2.300 [0.000, 12.000],  loss: 0.019289, mae: 0.368305, mean_q: 0.438570, mean_eps: 0.000000
 2415/5000: episode: 103, duration: 0.632s, episode steps:  25, steps per second:  40, episode reward: 35.274, mean reward:  1.411 [-2.617, 31.739], mean action: 3.160 [0.000, 15.000],  loss: 0.021324, mae: 0.376394, mean_q: 0.460376, mean_eps: 0.000000
 2430/5000: episode: 104, duration: 0.351s, episode steps:  15, steps per second:  43, episode reward: 39.000, mean reward:  2.600 [-2.378, 32.100], mean action: 4.067 [0.000, 15.000],  loss: 0.020793, mae: 0.383615, mean_q: 0.435545, mean_eps: 0.000000
 2457/5000: episode: 105, duration: 0.907s, episode steps:  27, steps per second:  30, episode reward: 32.180, mean reward:  1.192 [-2.902, 32.515], mean action: 4.630 [0.000, 19.000],  loss: 0.018566, mae: 0.365743, mean_q: 0.425302, mean_eps: 0.000000
 2477/5000: episode: 106, duration: 0.644s, episode steps:  20, steps per second:  31, episode reward: 32.601, mean reward:  1.630 [-2.846, 31.974], mean action: 5.450 [0.000, 19.000],  loss: 0.020233, mae: 0.370182, mean_q: 0.461635, mean_eps: 0.000000
 2499/5000: episode: 107, duration: 0.291s, episode steps:  22, steps per second:  76, episode reward: -38.510, mean reward: -1.750 [-32.151,  3.000], mean action: 9.545 [0.000, 15.000],  loss: 0.022343, mae: 0.382610, mean_q: 0.445917, mean_eps: 0.000000
 2535/5000: episode: 108, duration: 0.552s, episode steps:  36, steps per second:  65, episode reward: 35.410, mean reward:  0.984 [-3.000, 32.059], mean action: 3.861 [0.000, 14.000],  loss: 0.019799, mae: 0.361103, mean_q: 0.517001, mean_eps: 0.000000
 2554/5000: episode: 109, duration: 0.310s, episode steps:  19, steps per second:  61, episode reward: 38.785, mean reward:  2.041 [-2.460, 32.200], mean action: 3.211 [0.000, 12.000],  loss: 0.018776, mae: 0.345531, mean_q: 0.504835, mean_eps: 0.000000
 2572/5000: episode: 110, duration: 0.264s, episode steps:  18, steps per second:  68, episode reward: 38.216, mean reward:  2.123 [-2.559, 32.368], mean action: 3.778 [0.000, 16.000],  loss: 0.016182, mae: 0.334079, mean_q: 0.471314, mean_eps: 0.000000
 2595/5000: episode: 111, duration: 0.317s, episode steps:  23, steps per second:  72, episode reward: 32.147, mean reward:  1.398 [-2.815, 32.047], mean action: 4.783 [0.000, 15.000],  loss: 0.019022, mae: 0.351688, mean_q: 0.462025, mean_eps: 0.000000
 2614/5000: episode: 112, duration: 0.257s, episode steps:  19, steps per second:  74, episode reward: 35.521, mean reward:  1.870 [-2.297, 31.711], mean action: 3.789 [0.000, 19.000],  loss: 0.016245, mae: 0.340028, mean_q: 0.465108, mean_eps: 0.000000
 2628/5000: episode: 113, duration: 0.197s, episode steps:  14, steps per second:  71, episode reward: 42.000, mean reward:  3.000 [-2.318, 33.000], mean action: 3.643 [0.000, 15.000],  loss: 0.017246, mae: 0.340105, mean_q: 0.442094, mean_eps: 0.000000
 2649/5000: episode: 114, duration: 0.290s, episode steps:  21, steps per second:  72, episode reward: 35.482, mean reward:  1.690 [-3.000, 32.250], mean action: 6.381 [0.000, 15.000],  loss: 0.018105, mae: 0.343309, mean_q: 0.482472, mean_eps: 0.000000
 2677/5000: episode: 115, duration: 0.357s, episode steps:  28, steps per second:  78, episode reward: -36.000, mean reward: -1.286 [-30.267,  2.500], mean action: 4.143 [0.000, 19.000],  loss: 0.017034, mae: 0.337527, mean_q: 0.509260, mean_eps: 0.000000
 2699/5000: episode: 116, duration: 0.302s, episode steps:  22, steps per second:  73, episode reward: 44.714, mean reward:  2.032 [-2.180, 32.250], mean action: 1.909 [0.000, 14.000],  loss: 0.014681, mae: 0.326287, mean_q: 0.523257, mean_eps: 0.000000
 2728/5000: episode: 117, duration: 0.394s, episode steps:  29, steps per second:  74, episode reward: 32.344, mean reward:  1.115 [-3.000, 32.900], mean action: 5.241 [0.000, 16.000],  loss: 0.019654, mae: 0.353103, mean_q: 0.554613, mean_eps: 0.000000
 2761/5000: episode: 118, duration: 0.583s, episode steps:  33, steps per second:  57, episode reward: 32.675, mean reward:  0.990 [-2.342, 32.025], mean action: 5.242 [0.000, 16.000],  loss: 0.018783, mae: 0.346919, mean_q: 0.548117, mean_eps: 0.000000
 2787/5000: episode: 119, duration: 0.349s, episode steps:  26, steps per second:  74, episode reward: 32.405, mean reward:  1.246 [-3.000, 32.270], mean action: 4.577 [0.000, 16.000],  loss: 0.021474, mae: 0.355026, mean_q: 0.568223, mean_eps: 0.000000
 2806/5000: episode: 120, duration: 0.253s, episode steps:  19, steps per second:  75, episode reward: 35.621, mean reward:  1.875 [-3.000, 32.540], mean action: 4.474 [0.000, 16.000],  loss: 0.018733, mae: 0.345538, mean_q: 0.593757, mean_eps: 0.000000
 2851/5000: episode: 121, duration: 1.194s, episode steps:  45, steps per second:  38, episode reward: 32.836, mean reward:  0.730 [-2.419, 32.030], mean action: 7.622 [0.000, 20.000],  loss: 0.022956, mae: 0.369302, mean_q: 0.534469, mean_eps: 0.000000
 2878/5000: episode: 122, duration: 1.224s, episode steps:  27, steps per second:  22, episode reward: 34.541, mean reward:  1.279 [-2.536, 32.410], mean action: 6.741 [0.000, 15.000],  loss: 0.019398, mae: 0.340625, mean_q: 0.576566, mean_eps: 0.000000
 2894/5000: episode: 123, duration: 0.287s, episode steps:  16, steps per second:  56, episode reward: 38.527, mean reward:  2.408 [-2.496, 32.290], mean action: 4.938 [0.000, 15.000],  loss: 0.017322, mae: 0.330192, mean_q: 0.586001, mean_eps: 0.000000
 2916/5000: episode: 124, duration: 0.619s, episode steps:  22, steps per second:  36, episode reward: 35.884, mean reward:  1.631 [-2.461, 32.004], mean action: 4.182 [0.000, 19.000],  loss: 0.022879, mae: 0.360065, mean_q: 0.542770, mean_eps: 0.000000
 2942/5000: episode: 125, duration: 0.455s, episode steps:  26, steps per second:  57, episode reward: 35.687, mean reward:  1.373 [-2.301, 32.004], mean action: 3.308 [0.000, 16.000],  loss: 0.015540, mae: 0.326677, mean_q: 0.491122, mean_eps: 0.000000
 2969/5000: episode: 126, duration: 0.468s, episode steps:  27, steps per second:  58, episode reward: 38.567, mean reward:  1.428 [-2.291, 32.340], mean action: 2.926 [0.000, 19.000],  loss: 0.019851, mae: 0.349962, mean_q: 0.465442, mean_eps: 0.000000
 2987/5000: episode: 127, duration: 0.333s, episode steps:  18, steps per second:  54, episode reward: 41.903, mean reward:  2.328 [-2.166, 32.193], mean action: 3.889 [0.000, 15.000],  loss: 0.018522, mae: 0.345572, mean_q: 0.435021, mean_eps: 0.000000
 3010/5000: episode: 128, duration: 0.331s, episode steps:  23, steps per second:  69, episode reward: 32.559, mean reward:  1.416 [-2.806, 31.779], mean action: 6.087 [0.000, 16.000],  loss: 0.018230, mae: 0.345950, mean_q: 0.454307, mean_eps: 0.000000
 3030/5000: episode: 129, duration: 0.618s, episode steps:  20, steps per second:  32, episode reward: 38.460, mean reward:  1.923 [-3.000, 33.000], mean action: 3.350 [0.000, 16.000],  loss: 0.018006, mae: 0.350382, mean_q: 0.502433, mean_eps: 0.000000
 3049/5000: episode: 130, duration: 0.291s, episode steps:  19, steps per second:  65, episode reward: 32.259, mean reward:  1.698 [-3.000, 32.460], mean action: 9.105 [0.000, 19.000],  loss: 0.023417, mae: 0.370879, mean_q: 0.511217, mean_eps: 0.000000
 3069/5000: episode: 131, duration: 0.338s, episode steps:  20, steps per second:  59, episode reward: -33.000, mean reward: -1.650 [-30.524,  2.901], mean action: 8.800 [0.000, 16.000],  loss: 0.017183, mae: 0.339959, mean_q: 0.528890, mean_eps: 0.000000
 3086/5000: episode: 132, duration: 0.524s, episode steps:  17, steps per second:  32, episode reward: -35.910, mean reward: -2.112 [-32.735,  2.290], mean action: 6.706 [0.000, 19.000],  loss: 0.021687, mae: 0.361327, mean_q: 0.580169, mean_eps: 0.000000
 3109/5000: episode: 133, duration: 0.425s, episode steps:  23, steps per second:  54, episode reward: 36.000, mean reward:  1.565 [-2.940, 32.290], mean action: 5.913 [0.000, 20.000],  loss: 0.015507, mae: 0.331664, mean_q: 0.524783, mean_eps: 0.000000
 3132/5000: episode: 134, duration: 0.487s, episode steps:  23, steps per second:  47, episode reward: 41.350, mean reward:  1.798 [-3.000, 31.951], mean action: 3.870 [0.000, 19.000],  loss: 0.017593, mae: 0.348222, mean_q: 0.473037, mean_eps: 0.000000
 3172/5000: episode: 135, duration: 0.646s, episode steps:  40, steps per second:  62, episode reward: -32.850, mean reward: -0.821 [-32.085,  2.340], mean action: 6.850 [0.000, 19.000],  loss: 0.019536, mae: 0.361253, mean_q: 0.495893, mean_eps: 0.000000
 3188/5000: episode: 136, duration: 0.280s, episode steps:  16, steps per second:  57, episode reward: 41.368, mean reward:  2.585 [-2.113, 32.360], mean action: 3.375 [0.000, 14.000],  loss: 0.021620, mae: 0.367164, mean_q: 0.448075, mean_eps: 0.000000
 3210/5000: episode: 137, duration: 0.333s, episode steps:  22, steps per second:  66, episode reward: 32.679, mean reward:  1.485 [-3.000, 32.010], mean action: 5.227 [0.000, 15.000],  loss: 0.018415, mae: 0.349851, mean_q: 0.451919, mean_eps: 0.000000
 3239/5000: episode: 138, duration: 0.432s, episode steps:  29, steps per second:  67, episode reward: 33.000, mean reward:  1.138 [-2.693, 33.000], mean action: 5.138 [0.000, 19.000],  loss: 0.019963, mae: 0.363981, mean_q: 0.418461, mean_eps: 0.000000
 3261/5000: episode: 139, duration: 0.306s, episode steps:  22, steps per second:  72, episode reward: 35.015, mean reward:  1.592 [-3.000, 32.213], mean action: 5.364 [0.000, 16.000],  loss: 0.023468, mae: 0.373620, mean_q: 0.410979, mean_eps: 0.000000
 3298/5000: episode: 140, duration: 0.494s, episode steps:  37, steps per second:  75, episode reward: 35.614, mean reward:  0.963 [-2.469, 32.895], mean action: 5.216 [0.000, 20.000],  loss: 0.019100, mae: 0.353725, mean_q: 0.452601, mean_eps: 0.000000
 3316/5000: episode: 141, duration: 0.298s, episode steps:  18, steps per second:  60, episode reward: 38.189, mean reward:  2.122 [-2.998, 31.673], mean action: 4.167 [0.000, 19.000],  loss: 0.019680, mae: 0.346321, mean_q: 0.472801, mean_eps: 0.000000
 3340/5000: episode: 142, duration: 0.357s, episode steps:  24, steps per second:  67, episode reward: 35.070, mean reward:  1.461 [-2.806, 31.798], mean action: 6.292 [0.000, 16.000],  loss: 0.020094, mae: 0.346636, mean_q: 0.496611, mean_eps: 0.000000
 3365/5000: episode: 143, duration: 0.325s, episode steps:  25, steps per second:  77, episode reward: -35.610, mean reward: -1.424 [-32.164,  2.680], mean action: 3.960 [0.000, 16.000],  loss: 0.020384, mae: 0.344948, mean_q: 0.489139, mean_eps: 0.000000
 3395/5000: episode: 144, duration: 0.400s, episode steps:  30, steps per second:  75, episode reward: 41.824, mean reward:  1.394 [-2.038, 33.000], mean action: 1.700 [0.000, 16.000],  loss: 0.021860, mae: 0.362218, mean_q: 0.452900, mean_eps: 0.000000
 3440/5000: episode: 145, duration: 0.555s, episode steps:  45, steps per second:  81, episode reward: -38.580, mean reward: -0.857 [-32.126,  2.422], mean action: 13.267 [0.000, 19.000],  loss: 0.019940, mae: 0.355449, mean_q: 0.467078, mean_eps: 0.000000
 3446/5000: episode: 146, duration: 0.102s, episode steps:   6, steps per second:  59, episode reward: 47.614, mean reward:  7.936 [ 2.614, 33.000], mean action: 2.333 [0.000, 14.000],  loss: 0.022747, mae: 0.369154, mean_q: 0.442832, mean_eps: 0.000000
 3472/5000: episode: 147, duration: 0.343s, episode steps:  26, steps per second:  76, episode reward: 33.000, mean reward:  1.269 [-2.801, 32.450], mean action: 6.500 [0.000, 19.000],  loss: 0.017506, mae: 0.343895, mean_q: 0.489632, mean_eps: 0.000000
 3489/5000: episode: 148, duration: 0.248s, episode steps:  17, steps per second:  69, episode reward: 41.247, mean reward:  2.426 [-2.251, 32.901], mean action: 3.118 [0.000, 19.000],  loss: 0.021262, mae: 0.364722, mean_q: 0.488741, mean_eps: 0.000000
 3509/5000: episode: 149, duration: 0.288s, episode steps:  20, steps per second:  69, episode reward: 32.908, mean reward:  1.645 [-3.000, 32.228], mean action: 5.600 [0.000, 16.000],  loss: 0.019789, mae: 0.349870, mean_q: 0.478664, mean_eps: 0.000000
 3535/5000: episode: 150, duration: 0.493s, episode steps:  26, steps per second:  53, episode reward: 32.136, mean reward:  1.236 [-2.418, 32.190], mean action: 8.808 [0.000, 20.000],  loss: 0.017991, mae: 0.345884, mean_q: 0.455074, mean_eps: 0.000000
 3551/5000: episode: 151, duration: 0.234s, episode steps:  16, steps per second:  68, episode reward: 43.786, mean reward:  2.737 [-2.184, 32.065], mean action: 2.312 [0.000, 15.000],  loss: 0.020054, mae: 0.357937, mean_q: 0.450387, mean_eps: 0.000000
 3566/5000: episode: 152, duration: 0.494s, episode steps:  15, steps per second:  30, episode reward: 38.777, mean reward:  2.585 [-3.000, 33.000], mean action: 2.467 [0.000, 9.000],  loss: 0.020625, mae: 0.356431, mean_q: 0.476934, mean_eps: 0.000000
 3585/5000: episode: 153, duration: 0.324s, episode steps:  19, steps per second:  59, episode reward: 39.000, mean reward:  2.053 [-2.375, 32.560], mean action: 4.105 [1.000, 15.000],  loss: 0.023593, mae: 0.373864, mean_q: 0.439377, mean_eps: 0.000000
 3605/5000: episode: 154, duration: 0.309s, episode steps:  20, steps per second:  65, episode reward: -35.370, mean reward: -1.768 [-32.308,  2.870], mean action: 7.750 [1.000, 15.000],  loss: 0.024285, mae: 0.381363, mean_q: 0.434187, mean_eps: 0.000000
 3627/5000: episode: 155, duration: 0.299s, episode steps:  22, steps per second:  74, episode reward: 32.243, mean reward:  1.466 [-2.697, 32.153], mean action: 7.000 [0.000, 14.000],  loss: 0.019361, mae: 0.357384, mean_q: 0.474417, mean_eps: 0.000000
 3649/5000: episode: 156, duration: 0.320s, episode steps:  22, steps per second:  69, episode reward: -35.210, mean reward: -1.600 [-31.827,  2.400], mean action: 3.364 [0.000, 9.000],  loss: 0.023125, mae: 0.373034, mean_q: 0.545397, mean_eps: 0.000000
 3665/5000: episode: 157, duration: 0.380s, episode steps:  16, steps per second:  42, episode reward: 41.403, mean reward:  2.588 [-2.125, 32.357], mean action: 1.750 [0.000, 9.000],  loss: 0.021870, mae: 0.374159, mean_q: 0.507407, mean_eps: 0.000000
 3691/5000: episode: 158, duration: 0.473s, episode steps:  26, steps per second:  55, episode reward: 32.767, mean reward:  1.260 [-2.522, 32.291], mean action: 5.654 [0.000, 21.000],  loss: 0.022988, mae: 0.384000, mean_q: 0.507264, mean_eps: 0.000000
 3714/5000: episode: 159, duration: 0.434s, episode steps:  23, steps per second:  53, episode reward: -35.450, mean reward: -1.541 [-33.000,  3.061], mean action: 3.739 [0.000, 19.000],  loss: 0.017202, mae: 0.352078, mean_q: 0.476376, mean_eps: 0.000000
 3746/5000: episode: 160, duration: 0.531s, episode steps:  32, steps per second:  60, episode reward: -32.400, mean reward: -1.012 [-32.219,  2.220], mean action: 4.906 [0.000, 19.000],  loss: 0.022353, mae: 0.370547, mean_q: 0.496325, mean_eps: 0.000000
 3771/5000: episode: 161, duration: 0.336s, episode steps:  25, steps per second:  74, episode reward: -38.270, mean reward: -1.531 [-32.195,  2.330], mean action: 8.240 [0.000, 19.000],  loss: 0.021722, mae: 0.370400, mean_q: 0.494557, mean_eps: 0.000000
 3794/5000: episode: 162, duration: 0.375s, episode steps:  23, steps per second:  61, episode reward: 32.701, mean reward:  1.422 [-3.000, 32.551], mean action: 4.783 [0.000, 19.000],  loss: 0.021473, mae: 0.361498, mean_q: 0.469247, mean_eps: 0.000000
 3813/5000: episode: 163, duration: 0.268s, episode steps:  19, steps per second:  71, episode reward: 38.710, mean reward:  2.037 [-2.461, 32.842], mean action: 3.737 [0.000, 19.000],  loss: 0.025127, mae: 0.386630, mean_q: 0.497056, mean_eps: 0.000000
 3846/5000: episode: 164, duration: 0.617s, episode steps:  33, steps per second:  53, episode reward: 32.478, mean reward:  0.984 [-3.000, 32.161], mean action: 8.727 [0.000, 21.000],  loss: 0.021052, mae: 0.365647, mean_q: 0.527844, mean_eps: 0.000000
 3863/5000: episode: 165, duration: 0.373s, episode steps:  17, steps per second:  46, episode reward: 36.000, mean reward:  2.118 [-3.000, 33.000], mean action: 3.824 [0.000, 11.000],  loss: 0.021515, mae: 0.362759, mean_q: 0.550582, mean_eps: 0.000000
 3876/5000: episode: 166, duration: 0.211s, episode steps:  13, steps per second:  62, episode reward: 41.230, mean reward:  3.172 [-2.475, 32.682], mean action: 2.538 [0.000, 11.000],  loss: 0.019288, mae: 0.358912, mean_q: 0.479745, mean_eps: 0.000000
 3900/5000: episode: 167, duration: 0.392s, episode steps:  24, steps per second:  61, episode reward: 35.330, mean reward:  1.472 [-2.637, 31.640], mean action: 4.417 [1.000, 19.000],  loss: 0.022785, mae: 0.377593, mean_q: 0.443703, mean_eps: 0.000000
 3913/5000: episode: 168, duration: 0.184s, episode steps:  13, steps per second:  71, episode reward: 41.327, mean reward:  3.179 [-2.244, 32.327], mean action: 4.077 [1.000, 15.000],  loss: 0.017056, mae: 0.354342, mean_q: 0.478615, mean_eps: 0.000000
 3933/5000: episode: 169, duration: 0.268s, episode steps:  20, steps per second:  75, episode reward: 35.306, mean reward:  1.765 [-2.459, 32.333], mean action: 5.300 [0.000, 16.000],  loss: 0.019846, mae: 0.361676, mean_q: 0.461135, mean_eps: 0.000000
 3959/5000: episode: 170, duration: 0.347s, episode steps:  26, steps per second:  75, episode reward: -35.710, mean reward: -1.373 [-32.379,  2.410], mean action: 4.885 [1.000, 16.000],  loss: 0.018432, mae: 0.354525, mean_q: 0.466612, mean_eps: 0.000000
 3975/5000: episode: 171, duration: 0.219s, episode steps:  16, steps per second:  73, episode reward: 35.708, mean reward:  2.232 [-3.000, 32.003], mean action: 5.250 [0.000, 16.000],  loss: 0.024089, mae: 0.383254, mean_q: 0.500348, mean_eps: 0.000000
 3994/5000: episode: 172, duration: 0.262s, episode steps:  19, steps per second:  72, episode reward: 38.280, mean reward:  2.015 [-2.357, 33.000], mean action: 4.211 [0.000, 15.000],  loss: 0.018596, mae: 0.356359, mean_q: 0.490661, mean_eps: 0.000000
 4011/5000: episode: 173, duration: 0.237s, episode steps:  17, steps per second:  72, episode reward: 44.855, mean reward:  2.639 [-2.324, 32.435], mean action: 2.824 [1.000, 6.000],  loss: 0.018685, mae: 0.362391, mean_q: 0.495010, mean_eps: 0.000000
 4038/5000: episode: 174, duration: 0.362s, episode steps:  27, steps per second:  75, episode reward: -32.620, mean reward: -1.208 [-32.233,  2.654], mean action: 5.852 [0.000, 19.000],  loss: 0.019741, mae: 0.364342, mean_q: 0.503572, mean_eps: 0.000000
 4079/5000: episode: 175, duration: 0.539s, episode steps:  41, steps per second:  76, episode reward: -35.600, mean reward: -0.868 [-32.175,  2.920], mean action: 5.634 [0.000, 19.000],  loss: 0.021588, mae: 0.373133, mean_q: 0.503697, mean_eps: 0.000000
 4103/5000: episode: 176, duration: 0.323s, episode steps:  24, steps per second:  74, episode reward: 36.000, mean reward:  1.500 [-2.344, 33.000], mean action: 2.750 [0.000, 19.000],  loss: 0.024137, mae: 0.387062, mean_q: 0.478670, mean_eps: 0.000000
 4132/5000: episode: 177, duration: 0.402s, episode steps:  29, steps per second:  72, episode reward: 32.484, mean reward:  1.120 [-2.757, 32.060], mean action: 8.345 [0.000, 19.000],  loss: 0.019336, mae: 0.368003, mean_q: 0.467632, mean_eps: 0.000000
 4160/5000: episode: 178, duration: 0.381s, episode steps:  28, steps per second:  73, episode reward: 32.794, mean reward:  1.171 [-2.300, 32.004], mean action: 5.714 [0.000, 19.000],  loss: 0.025190, mae: 0.389192, mean_q: 0.493665, mean_eps: 0.000000
 4177/5000: episode: 179, duration: 0.241s, episode steps:  17, steps per second:  70, episode reward: 38.698, mean reward:  2.276 [-2.310, 32.698], mean action: 3.941 [0.000, 15.000],  loss: 0.023794, mae: 0.381548, mean_q: 0.476838, mean_eps: 0.000000
 4208/5000: episode: 180, duration: 0.404s, episode steps:  31, steps per second:  77, episode reward: -35.620, mean reward: -1.149 [-32.117,  2.240], mean action: 7.065 [0.000, 20.000],  loss: 0.018406, mae: 0.361934, mean_q: 0.474132, mean_eps: 0.000000
 4230/5000: episode: 181, duration: 0.310s, episode steps:  22, steps per second:  71, episode reward: -33.000, mean reward: -1.500 [-30.890,  2.752], mean action: 4.682 [0.000, 14.000],  loss: 0.020395, mae: 0.368827, mean_q: 0.465486, mean_eps: 0.000000
 4247/5000: episode: 182, duration: 0.262s, episode steps:  17, steps per second:  65, episode reward: 42.000, mean reward:  2.471 [-2.088, 32.110], mean action: 2.941 [0.000, 11.000],  loss: 0.019235, mae: 0.366783, mean_q: 0.415308, mean_eps: 0.000000
 4269/5000: episode: 183, duration: 0.390s, episode steps:  22, steps per second:  56, episode reward: 35.261, mean reward:  1.603 [-2.400, 32.311], mean action: 7.000 [0.000, 19.000],  loss: 0.018871, mae: 0.373608, mean_q: 0.390560, mean_eps: 0.000000
 4297/5000: episode: 184, duration: 0.370s, episode steps:  28, steps per second:  76, episode reward: 37.611, mean reward:  1.343 [-2.247, 31.955], mean action: 6.536 [0.000, 20.000],  loss: 0.019546, mae: 0.367070, mean_q: 0.467860, mean_eps: 0.000000
 4324/5000: episode: 185, duration: 0.397s, episode steps:  27, steps per second:  68, episode reward: 35.382, mean reward:  1.310 [-2.815, 32.100], mean action: 4.704 [0.000, 19.000],  loss: 0.022893, mae: 0.382879, mean_q: 0.492757, mean_eps: 0.000000
 4367/5000: episode: 186, duration: 0.593s, episode steps:  43, steps per second:  73, episode reward: 32.209, mean reward:  0.749 [-2.882, 31.907], mean action: 3.907 [0.000, 19.000],  loss: 0.021727, mae: 0.376680, mean_q: 0.527324, mean_eps: 0.000000
 4390/5000: episode: 187, duration: 0.303s, episode steps:  23, steps per second:  76, episode reward: -33.000, mean reward: -1.435 [-32.258,  2.340], mean action: 4.826 [0.000, 14.000],  loss: 0.018479, mae: 0.355058, mean_q: 0.470103, mean_eps: 0.000000
 4414/5000: episode: 188, duration: 0.329s, episode steps:  24, steps per second:  73, episode reward: -32.450, mean reward: -1.352 [-32.082,  2.630], mean action: 4.833 [0.000, 19.000],  loss: 0.019397, mae: 0.359729, mean_q: 0.482963, mean_eps: 0.000000
 4446/5000: episode: 189, duration: 0.433s, episode steps:  32, steps per second:  74, episode reward: 38.227, mean reward:  1.195 [-2.118, 32.028], mean action: 1.562 [0.000, 11.000],  loss: 0.018129, mae: 0.348710, mean_q: 0.480111, mean_eps: 0.000000
 4461/5000: episode: 190, duration: 0.219s, episode steps:  15, steps per second:  69, episode reward: 38.497, mean reward:  2.566 [-2.266, 32.497], mean action: 3.133 [0.000, 12.000],  loss: 0.021067, mae: 0.370343, mean_q: 0.471482, mean_eps: 0.000000
 4480/5000: episode: 191, duration: 0.288s, episode steps:  19, steps per second:  66, episode reward: 38.208, mean reward:  2.011 [-3.000, 33.000], mean action: 4.316 [1.000, 14.000],  loss: 0.021925, mae: 0.378162, mean_q: 0.470242, mean_eps: 0.000000
 4500/5000: episode: 192, duration: 0.331s, episode steps:  20, steps per second:  60, episode reward: 35.460, mean reward:  1.773 [-3.000, 31.792], mean action: 4.050 [0.000, 19.000],  loss: 0.023234, mae: 0.391044, mean_q: 0.449746, mean_eps: 0.000000
 4532/5000: episode: 193, duration: 0.424s, episode steps:  32, steps per second:  75, episode reward: -35.770, mean reward: -1.118 [-32.071,  2.353], mean action: 5.438 [0.000, 19.000],  loss: 0.018903, mae: 0.362680, mean_q: 0.482007, mean_eps: 0.000000
 4553/5000: episode: 194, duration: 0.339s, episode steps:  21, steps per second:  62, episode reward: 37.871, mean reward:  1.803 [-3.000, 31.933], mean action: 3.619 [0.000, 11.000],  loss: 0.021648, mae: 0.371785, mean_q: 0.525131, mean_eps: 0.000000
 4575/5000: episode: 195, duration: 0.313s, episode steps:  22, steps per second:  70, episode reward: -32.540, mean reward: -1.479 [-31.948,  2.399], mean action: 3.318 [0.000, 19.000],  loss: 0.022343, mae: 0.375221, mean_q: 0.508038, mean_eps: 0.000000
 4598/5000: episode: 196, duration: 0.305s, episode steps:  23, steps per second:  75, episode reward: 35.393, mean reward:  1.539 [-2.926, 32.067], mean action: 3.304 [0.000, 19.000],  loss: 0.019883, mae: 0.369628, mean_q: 0.507764, mean_eps: 0.000000
 4620/5000: episode: 197, duration: 0.290s, episode steps:  22, steps per second:  76, episode reward: -32.220, mean reward: -1.465 [-32.251,  2.778], mean action: 7.318 [1.000, 20.000],  loss: 0.020472, mae: 0.364519, mean_q: 0.527977, mean_eps: 0.000000
 4646/5000: episode: 198, duration: 0.367s, episode steps:  26, steps per second:  71, episode reward: -35.710, mean reward: -1.373 [-32.710,  2.200], mean action: 9.923 [0.000, 20.000],  loss: 0.020687, mae: 0.366403, mean_q: 0.533994, mean_eps: 0.000000
 4678/5000: episode: 199, duration: 0.427s, episode steps:  32, steps per second:  75, episode reward: -32.920, mean reward: -1.029 [-32.181,  2.530], mean action: 8.250 [0.000, 19.000],  loss: 0.019717, mae: 0.357922, mean_q: 0.539236, mean_eps: 0.000000
 4698/5000: episode: 200, duration: 0.267s, episode steps:  20, steps per second:  75, episode reward: 38.463, mean reward:  1.923 [-2.318, 32.317], mean action: 5.000 [0.000, 14.000],  loss: 0.018062, mae: 0.349899, mean_q: 0.537098, mean_eps: 0.000000
 4713/5000: episode: 201, duration: 0.212s, episode steps:  15, steps per second:  71, episode reward: 38.220, mean reward:  2.548 [-3.000, 32.103], mean action: 3.800 [0.000, 12.000],  loss: 0.016764, mae: 0.344263, mean_q: 0.594841, mean_eps: 0.000000
 4740/5000: episode: 202, duration: 0.362s, episode steps:  27, steps per second:  75, episode reward: 34.759, mean reward:  1.287 [-3.000, 32.226], mean action: 4.815 [0.000, 19.000],  loss: 0.021297, mae: 0.366135, mean_q: 0.506243, mean_eps: 0.000000
 4765/5000: episode: 203, duration: 0.366s, episode steps:  25, steps per second:  68, episode reward: 37.098, mean reward:  1.484 [-2.560, 32.367], mean action: 5.320 [0.000, 20.000],  loss: 0.019046, mae: 0.357022, mean_q: 0.496938, mean_eps: 0.000000
 4789/5000: episode: 204, duration: 0.357s, episode steps:  24, steps per second:  67, episode reward: 32.742, mean reward:  1.364 [-2.437, 32.321], mean action: 4.333 [0.000, 19.000],  loss: 0.018573, mae: 0.352958, mean_q: 0.484905, mean_eps: 0.000000
 4809/5000: episode: 205, duration: 0.340s, episode steps:  20, steps per second:  59, episode reward: 41.075, mean reward:  2.054 [-2.176, 32.672], mean action: 4.350 [0.000, 12.000],  loss: 0.019076, mae: 0.364786, mean_q: 0.470499, mean_eps: 0.000000
 4828/5000: episode: 206, duration: 0.253s, episode steps:  19, steps per second:  75, episode reward: 35.801, mean reward:  1.884 [-2.642, 32.900], mean action: 5.895 [0.000, 15.000],  loss: 0.020907, mae: 0.372523, mean_q: 0.412756, mean_eps: 0.000000
 4851/5000: episode: 207, duration: 0.314s, episode steps:  23, steps per second:  73, episode reward: 38.122, mean reward:  1.657 [-2.664, 32.026], mean action: 6.043 [0.000, 13.000],  loss: 0.019146, mae: 0.363752, mean_q: 0.478266, mean_eps: 0.000000
 4873/5000: episode: 208, duration: 0.345s, episode steps:  22, steps per second:  64, episode reward: 38.715, mean reward:  1.760 [-2.335, 31.855], mean action: 3.455 [0.000, 15.000],  loss: 0.019238, mae: 0.355446, mean_q: 0.528257, mean_eps: 0.000000
 4889/5000: episode: 209, duration: 0.272s, episode steps:  16, steps per second:  59, episode reward: 38.588, mean reward:  2.412 [-2.524, 32.390], mean action: 3.625 [0.000, 16.000],  loss: 0.019924, mae: 0.355848, mean_q: 0.535592, mean_eps: 0.000000
 4908/5000: episode: 210, duration: 0.272s, episode steps:  19, steps per second:  70, episode reward: 38.902, mean reward:  2.047 [-2.879, 32.012], mean action: 4.316 [0.000, 16.000],  loss: 0.016085, mae: 0.337791, mean_q: 0.565894, mean_eps: 0.000000
 4935/5000: episode: 211, duration: 0.354s, episode steps:  27, steps per second:  76, episode reward: 32.702, mean reward:  1.211 [-3.000, 32.901], mean action: 4.148 [0.000, 16.000],  loss: 0.020280, mae: 0.352394, mean_q: 0.606402, mean_eps: 0.000000
 4956/5000: episode: 212, duration: 0.284s, episode steps:  21, steps per second:  74, episode reward: 35.250, mean reward:  1.679 [-3.000, 31.978], mean action: 4.190 [0.000, 16.000],  loss: 0.017461, mae: 0.338525, mean_q: 0.546273, mean_eps: 0.000000
 4972/5000: episode: 213, duration: 0.224s, episode steps:  16, steps per second:  71, episode reward: 41.067, mean reward:  2.567 [-3.000, 32.690], mean action: 4.250 [0.000, 16.000],  loss: 0.018884, mae: 0.352540, mean_q: 0.570182, mean_eps: 0.000000
done, took 68.120 seconds
DQN Evaluation: 4775 victories out of 5660 episodes
Training for 5000 steps ...
   24/5000: episode: 1, duration: 0.193s, episode steps:  24, steps per second: 124, episode reward: 44.379, mean reward:  1.849 [-2.187, 31.718], mean action: 1.167 [0.000, 6.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   55/5000: episode: 2, duration: 0.208s, episode steps:  31, steps per second: 149, episode reward: 37.345, mean reward:  1.205 [-3.000, 32.520], mean action: 3.935 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   91/5000: episode: 3, duration: 0.240s, episode steps:  36, steps per second: 150, episode reward: -32.160, mean reward: -0.893 [-33.000,  2.960], mean action: 5.583 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  115/5000: episode: 4, duration: 0.161s, episode steps:  24, steps per second: 149, episode reward: 44.624, mean reward:  1.859 [-2.050, 32.150], mean action: 1.917 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  135/5000: episode: 5, duration: 0.152s, episode steps:  20, steps per second: 131, episode reward: 39.000, mean reward:  1.950 [-2.671, 32.380], mean action: 3.350 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  169/5000: episode: 6, duration: 0.372s, episode steps:  34, steps per second:  91, episode reward: 44.916, mean reward:  1.321 [-2.033, 31.956], mean action: 1.500 [1.000, 6.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  184/5000: episode: 7, duration: 0.350s, episode steps:  15, steps per second:  43, episode reward: 44.330, mean reward:  2.955 [-2.153, 32.200], mean action: 2.933 [1.000, 6.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  217/5000: episode: 8, duration: 0.326s, episode steps:  33, steps per second: 101, episode reward: 44.052, mean reward:  1.335 [-2.044, 32.650], mean action: 3.545 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  251/5000: episode: 9, duration: 0.297s, episode steps:  34, steps per second: 114, episode reward: 38.226, mean reward:  1.124 [-2.316, 32.107], mean action: 5.912 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  271/5000: episode: 10, duration: 0.182s, episode steps:  20, steps per second: 110, episode reward: 44.533, mean reward:  2.227 [-2.004, 32.490], mean action: 1.000 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  304/5000: episode: 11, duration: 0.314s, episode steps:  33, steps per second: 105, episode reward: 38.298, mean reward:  1.161 [-3.000, 31.856], mean action: 3.394 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  332/5000: episode: 12, duration: 0.373s, episode steps:  28, steps per second:  75, episode reward: 38.972, mean reward:  1.392 [-2.817, 32.670], mean action: 3.179 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  363/5000: episode: 13, duration: 0.330s, episode steps:  31, steps per second:  94, episode reward: 35.333, mean reward:  1.140 [-2.680, 32.190], mean action: 3.387 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  385/5000: episode: 14, duration: 0.164s, episode steps:  22, steps per second: 134, episode reward: -32.390, mean reward: -1.472 [-32.504,  2.903], mean action: 5.727 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  413/5000: episode: 15, duration: 0.193s, episode steps:  28, steps per second: 145, episode reward: 41.103, mean reward:  1.468 [-2.294, 32.901], mean action: 3.464 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  440/5000: episode: 16, duration: 0.186s, episode steps:  27, steps per second: 145, episode reward: 35.875, mean reward:  1.329 [-3.000, 32.070], mean action: 3.370 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  455/5000: episode: 17, duration: 0.114s, episode steps:  15, steps per second: 132, episode reward: 41.190, mean reward:  2.746 [-3.000, 32.057], mean action: 2.333 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  483/5000: episode: 18, duration: 0.181s, episode steps:  28, steps per second: 155, episode reward: 36.000, mean reward:  1.286 [-2.752, 32.290], mean action: 3.679 [0.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  506/5000: episode: 19, duration: 0.149s, episode steps:  23, steps per second: 154, episode reward: 38.259, mean reward:  1.663 [-3.000, 31.993], mean action: 2.565 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  537/5000: episode: 20, duration: 0.599s, episode steps:  31, steps per second:  52, episode reward: 32.238, mean reward:  1.040 [-3.000, 32.680], mean action: 4.645 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  556/5000: episode: 21, duration: 0.195s, episode steps:  19, steps per second:  97, episode reward: 44.191, mean reward:  2.326 [-2.457, 32.537], mean action: 2.632 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  583/5000: episode: 22, duration: 0.216s, episode steps:  27, steps per second: 125, episode reward: 41.634, mean reward:  1.542 [-3.000, 31.908], mean action: 1.963 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  607/5000: episode: 23, duration: 0.163s, episode steps:  24, steps per second: 148, episode reward: 38.360, mean reward:  1.598 [-2.484, 32.043], mean action: 4.625 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  637/5000: episode: 24, duration: 0.428s, episode steps:  30, steps per second:  70, episode reward: 32.608, mean reward:  1.087 [-2.719, 32.070], mean action: 3.800 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  655/5000: episode: 25, duration: 0.127s, episode steps:  18, steps per second: 142, episode reward: 46.327, mean reward:  2.574 [-0.722, 32.220], mean action: 3.611 [2.000, 8.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  677/5000: episode: 26, duration: 0.148s, episode steps:  22, steps per second: 149, episode reward: 39.000, mean reward:  1.773 [-2.449, 32.200], mean action: 5.136 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  695/5000: episode: 27, duration: 0.125s, episode steps:  18, steps per second: 144, episode reward: 43.838, mean reward:  2.435 [-2.077, 31.722], mean action: 3.056 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  711/5000: episode: 28, duration: 0.117s, episode steps:  16, steps per second: 137, episode reward: 44.753, mean reward:  2.797 [-2.040, 32.203], mean action: 4.000 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  744/5000: episode: 29, duration: 0.209s, episode steps:  33, steps per second: 158, episode reward: 38.876, mean reward:  1.178 [-2.876, 32.480], mean action: 1.879 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  752/5000: episode: 30, duration: 0.074s, episode steps:   8, steps per second: 109, episode reward: 47.149, mean reward:  5.894 [ 0.000, 33.000], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  770/5000: episode: 31, duration: 0.126s, episode steps:  18, steps per second: 143, episode reward: 40.661, mean reward:  2.259 [-2.735, 32.430], mean action: 3.500 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  802/5000: episode: 32, duration: 0.208s, episode steps:  32, steps per second: 154, episode reward: 38.620, mean reward:  1.207 [-3.000, 32.420], mean action: 3.094 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  823/5000: episode: 33, duration: 0.142s, episode steps:  21, steps per second: 148, episode reward: 41.793, mean reward:  1.990 [-2.345, 32.323], mean action: 2.667 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  845/5000: episode: 34, duration: 0.149s, episode steps:  22, steps per second: 148, episode reward: 41.931, mean reward:  1.906 [-2.257, 32.791], mean action: 2.909 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  864/5000: episode: 35, duration: 0.127s, episode steps:  19, steps per second: 149, episode reward: 47.118, mean reward:  2.480 [-0.180, 32.050], mean action: 0.421 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  880/5000: episode: 36, duration: 0.115s, episode steps:  16, steps per second: 139, episode reward: 44.180, mean reward:  2.761 [-2.097, 32.405], mean action: 2.250 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  898/5000: episode: 37, duration: 0.127s, episode steps:  18, steps per second: 142, episode reward: 44.648, mean reward:  2.480 [-2.049, 32.370], mean action: 1.278 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  933/5000: episode: 38, duration: 0.219s, episode steps:  35, steps per second: 160, episode reward: 42.712, mean reward:  1.220 [-2.500, 32.130], mean action: 3.343 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  963/5000: episode: 39, duration: 0.209s, episode steps:  30, steps per second: 143, episode reward: 38.704, mean reward:  1.290 [-2.416, 32.460], mean action: 3.400 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  990/5000: episode: 40, duration: 0.174s, episode steps:  27, steps per second: 155, episode reward: 44.249, mean reward:  1.639 [-2.666, 33.000], mean action: 1.370 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1022/5000: episode: 41, duration: 0.360s, episode steps:  32, steps per second:  89, episode reward: 35.612, mean reward:  1.113 [-3.000, 32.280], mean action: 3.781 [0.000, 16.000],  loss: 0.016315, mae: 0.347889, mean_q: 0.585907, mean_eps: 0.000000
 1039/5000: episode: 42, duration: 0.239s, episode steps:  17, steps per second:  71, episode reward: 42.000, mean reward:  2.471 [-3.000, 33.000], mean action: 2.706 [0.000, 16.000],  loss: 0.020623, mae: 0.363327, mean_q: 0.540291, mean_eps: 0.000000
 1061/5000: episode: 43, duration: 0.714s, episode steps:  22, steps per second:  31, episode reward: 44.326, mean reward:  2.015 [-3.000, 31.991], mean action: 3.682 [0.000, 16.000],  loss: 0.020446, mae: 0.365345, mean_q: 0.497411, mean_eps: 0.000000
 1090/5000: episode: 44, duration: 0.495s, episode steps:  29, steps per second:  59, episode reward: 38.940, mean reward:  1.343 [-2.327, 32.190], mean action: 3.862 [0.000, 16.000],  loss: 0.023719, mae: 0.383723, mean_q: 0.525158, mean_eps: 0.000000
 1125/5000: episode: 45, duration: 0.502s, episode steps:  35, steps per second:  70, episode reward: 39.000, mean reward:  1.114 [-2.403, 32.030], mean action: 3.686 [0.000, 16.000],  loss: 0.020276, mae: 0.371060, mean_q: 0.504649, mean_eps: 0.000000
 1167/5000: episode: 46, duration: 0.566s, episode steps:  42, steps per second:  74, episode reward: 41.070, mean reward:  0.978 [-2.179, 32.210], mean action: 2.762 [0.000, 16.000],  loss: 0.018425, mae: 0.360657, mean_q: 0.482706, mean_eps: 0.000000
 1190/5000: episode: 47, duration: 0.329s, episode steps:  23, steps per second:  70, episode reward: 41.129, mean reward:  1.788 [-2.048, 32.550], mean action: 3.087 [0.000, 16.000],  loss: 0.018478, mae: 0.358958, mean_q: 0.513555, mean_eps: 0.000000
 1227/5000: episode: 48, duration: 0.509s, episode steps:  37, steps per second:  73, episode reward: 38.041, mean reward:  1.028 [-2.639, 32.139], mean action: 3.189 [1.000, 16.000],  loss: 0.023307, mae: 0.380572, mean_q: 0.536548, mean_eps: 0.000000
 1246/5000: episode: 49, duration: 0.266s, episode steps:  19, steps per second:  71, episode reward: 47.417, mean reward:  2.496 [-0.390, 32.180], mean action: 1.737 [0.000, 3.000],  loss: 0.021107, mae: 0.377738, mean_q: 0.501217, mean_eps: 0.000000
 1271/5000: episode: 50, duration: 0.342s, episode steps:  25, steps per second:  73, episode reward: 38.056, mean reward:  1.522 [-3.000, 32.150], mean action: 2.600 [0.000, 16.000],  loss: 0.019285, mae: 0.362520, mean_q: 0.499652, mean_eps: 0.000000
 1287/5000: episode: 51, duration: 0.249s, episode steps:  16, steps per second:  64, episode reward: 45.000, mean reward:  2.813 [-2.438, 32.840], mean action: 3.125 [0.000, 16.000],  loss: 0.019492, mae: 0.358949, mean_q: 0.526057, mean_eps: 0.000000
 1329/5000: episode: 52, duration: 0.550s, episode steps:  42, steps per second:  76, episode reward: 32.478, mean reward:  0.773 [-2.774, 32.539], mean action: 4.476 [0.000, 16.000],  loss: 0.021661, mae: 0.370537, mean_q: 0.484248, mean_eps: 0.000000
 1352/5000: episode: 53, duration: 0.419s, episode steps:  23, steps per second:  55, episode reward: 41.111, mean reward:  1.787 [-3.000, 32.901], mean action: 4.652 [1.000, 19.000],  loss: 0.024675, mae: 0.382306, mean_q: 0.450142, mean_eps: 0.000000
 1374/5000: episode: 54, duration: 0.445s, episode steps:  22, steps per second:  49, episode reward: 44.399, mean reward:  2.018 [-2.142, 31.800], mean action: 2.682 [0.000, 20.000],  loss: 0.023791, mae: 0.376928, mean_q: 0.478024, mean_eps: 0.000000
 1397/5000: episode: 55, duration: 0.324s, episode steps:  23, steps per second:  71, episode reward: 38.676, mean reward:  1.682 [-2.911, 32.201], mean action: 4.174 [0.000, 16.000],  loss: 0.020333, mae: 0.370241, mean_q: 0.518352, mean_eps: 0.000000
 1420/5000: episode: 56, duration: 0.318s, episode steps:  23, steps per second:  72, episode reward: 44.097, mean reward:  1.917 [-2.372, 32.320], mean action: 2.391 [0.000, 16.000],  loss: 0.016721, mae: 0.346442, mean_q: 0.498517, mean_eps: 0.000000
 1459/5000: episode: 57, duration: 0.507s, episode steps:  39, steps per second:  77, episode reward: 38.484, mean reward:  0.987 [-2.138, 32.560], mean action: 3.744 [0.000, 16.000],  loss: 0.019175, mae: 0.363175, mean_q: 0.456977, mean_eps: 0.000000
 1487/5000: episode: 58, duration: 0.372s, episode steps:  28, steps per second:  75, episode reward: 38.447, mean reward:  1.373 [-2.827, 32.015], mean action: 1.429 [0.000, 12.000],  loss: 0.020775, mae: 0.367967, mean_q: 0.446491, mean_eps: 0.000000
 1506/5000: episode: 59, duration: 0.258s, episode steps:  19, steps per second:  74, episode reward: 38.607, mean reward:  2.032 [-3.000, 32.902], mean action: 3.895 [0.000, 16.000],  loss: 0.019593, mae: 0.357924, mean_q: 0.486539, mean_eps: 0.000000
 1552/5000: episode: 60, duration: 0.610s, episode steps:  46, steps per second:  75, episode reward: 44.207, mean reward:  0.961 [-2.217, 31.886], mean action: 2.304 [0.000, 16.000],  loss: 0.017907, mae: 0.346444, mean_q: 0.513839, mean_eps: 0.000000
 1583/5000: episode: 61, duration: 0.401s, episode steps:  31, steps per second:  77, episode reward: 43.754, mean reward:  1.411 [-2.658, 32.332], mean action: 3.194 [1.000, 14.000],  loss: 0.018051, mae: 0.346717, mean_q: 0.537726, mean_eps: 0.000000
 1618/5000: episode: 62, duration: 0.462s, episode steps:  35, steps per second:  76, episode reward: 41.262, mean reward:  1.179 [-2.529, 32.990], mean action: 4.229 [0.000, 20.000],  loss: 0.017114, mae: 0.339091, mean_q: 0.549006, mean_eps: 0.000000
 1656/5000: episode: 63, duration: 0.511s, episode steps:  38, steps per second:  74, episode reward: 33.000, mean reward:  0.868 [-3.000, 32.160], mean action: 5.053 [0.000, 21.000],  loss: 0.022067, mae: 0.364676, mean_q: 0.558980, mean_eps: 0.000000
 1668/5000: episode: 64, duration: 0.174s, episode steps:  12, steps per second:  69, episode reward: 48.000, mean reward:  4.000 [ 0.062, 32.590], mean action: 1.667 [0.000, 14.000],  loss: 0.018097, mae: 0.352638, mean_q: 0.517613, mean_eps: 0.000000
 1689/5000: episode: 65, duration: 0.282s, episode steps:  21, steps per second:  75, episode reward: 40.874, mean reward:  1.946 [-2.575, 32.403], mean action: 1.810 [0.000, 11.000],  loss: 0.018277, mae: 0.352978, mean_q: 0.496786, mean_eps: 0.000000
 1723/5000: episode: 66, duration: 0.453s, episode steps:  34, steps per second:  75, episode reward: 32.961, mean reward:  0.969 [-2.417, 32.091], mean action: 4.441 [0.000, 19.000],  loss: 0.018998, mae: 0.361809, mean_q: 0.483456, mean_eps: 0.000000
 1749/5000: episode: 67, duration: 0.361s, episode steps:  26, steps per second:  72, episode reward: 44.381, mean reward:  1.707 [-2.306, 31.825], mean action: 1.154 [0.000, 12.000],  loss: 0.020902, mae: 0.369328, mean_q: 0.455652, mean_eps: 0.000000
 1764/5000: episode: 68, duration: 0.223s, episode steps:  15, steps per second:  67, episode reward: 41.772, mean reward:  2.785 [-2.415, 32.492], mean action: 2.200 [0.000, 12.000],  loss: 0.020906, mae: 0.375555, mean_q: 0.500900, mean_eps: 0.000000
 1785/5000: episode: 69, duration: 0.316s, episode steps:  21, steps per second:  67, episode reward: 47.459, mean reward:  2.260 [-0.410, 31.916], mean action: 1.619 [0.000, 14.000],  loss: 0.017105, mae: 0.359223, mean_q: 0.489714, mean_eps: 0.000000
 1812/5000: episode: 70, duration: 0.360s, episode steps:  27, steps per second:  75, episode reward: -33.000, mean reward: -1.222 [-32.183,  2.875], mean action: 7.963 [0.000, 21.000],  loss: 0.018882, mae: 0.368696, mean_q: 0.482807, mean_eps: 0.000000
 1834/5000: episode: 71, duration: 0.303s, episode steps:  22, steps per second:  73, episode reward: 38.146, mean reward:  1.734 [-2.504, 32.950], mean action: 3.500 [0.000, 14.000],  loss: 0.018389, mae: 0.366358, mean_q: 0.440950, mean_eps: 0.000000
 1854/5000: episode: 72, duration: 0.281s, episode steps:  20, steps per second:  71, episode reward: 43.561, mean reward:  2.178 [-3.000, 32.336], mean action: 1.550 [0.000, 12.000],  loss: 0.022235, mae: 0.370306, mean_q: 0.449692, mean_eps: 0.000000
 1888/5000: episode: 73, duration: 0.440s, episode steps:  34, steps per second:  77, episode reward: -35.340, mean reward: -1.039 [-32.247,  3.000], mean action: 5.206 [0.000, 14.000],  loss: 0.017601, mae: 0.345396, mean_q: 0.455987, mean_eps: 0.000000
 1925/5000: episode: 74, duration: 0.490s, episode steps:  37, steps per second:  76, episode reward: 35.483, mean reward:  0.959 [-2.810, 32.640], mean action: 5.324 [1.000, 16.000],  loss: 0.021420, mae: 0.366311, mean_q: 0.526617, mean_eps: 0.000000
 1970/5000: episode: 75, duration: 0.583s, episode steps:  45, steps per second:  77, episode reward: 38.341, mean reward:  0.852 [-2.255, 31.815], mean action: 4.911 [0.000, 19.000],  loss: 0.018585, mae: 0.355100, mean_q: 0.494054, mean_eps: 0.000000
 1995/5000: episode: 76, duration: 0.333s, episode steps:  25, steps per second:  75, episode reward: 42.000, mean reward:  1.680 [-2.929, 32.350], mean action: 2.080 [0.000, 11.000],  loss: 0.017545, mae: 0.350180, mean_q: 0.527423, mean_eps: 0.000000
 2011/5000: episode: 77, duration: 0.234s, episode steps:  16, steps per second:  68, episode reward: 42.000, mean reward:  2.625 [-2.031, 29.861], mean action: 2.250 [0.000, 11.000],  loss: 0.021347, mae: 0.360902, mean_q: 0.512135, mean_eps: 0.000000
 2031/5000: episode: 78, duration: 0.289s, episode steps:  20, steps per second:  69, episode reward: 41.021, mean reward:  2.051 [-2.664, 32.172], mean action: 4.250 [0.000, 21.000],  loss: 0.017090, mae: 0.345251, mean_q: 0.518156, mean_eps: 0.000000
 2058/5000: episode: 79, duration: 0.358s, episode steps:  27, steps per second:  75, episode reward: 35.902, mean reward:  1.330 [-3.000, 31.982], mean action: 7.370 [0.000, 19.000],  loss: 0.016071, mae: 0.339926, mean_q: 0.479322, mean_eps: 0.000000
 2079/5000: episode: 80, duration: 0.563s, episode steps:  21, steps per second:  37, episode reward: 46.176, mean reward:  2.199 [-0.555, 32.920], mean action: 3.905 [0.000, 14.000],  loss: 0.022614, mae: 0.373066, mean_q: 0.445371, mean_eps: 0.000000
 2123/5000: episode: 81, duration: 0.777s, episode steps:  44, steps per second:  57, episode reward: 38.065, mean reward:  0.865 [-2.177, 32.720], mean action: 2.977 [0.000, 14.000],  loss: 0.019931, mae: 0.354928, mean_q: 0.450534, mean_eps: 0.000000
 2142/5000: episode: 82, duration: 0.370s, episode steps:  19, steps per second:  51, episode reward: 39.000, mean reward:  2.053 [-2.520, 32.420], mean action: 2.579 [0.000, 12.000],  loss: 0.022807, mae: 0.366852, mean_q: 0.501766, mean_eps: 0.000000
 2169/5000: episode: 83, duration: 0.604s, episode steps:  27, steps per second:  45, episode reward: 44.963, mean reward:  1.665 [-2.724, 32.280], mean action: 2.519 [0.000, 12.000],  loss: 0.020081, mae: 0.352606, mean_q: 0.451531, mean_eps: 0.000000
 2191/5000: episode: 84, duration: 0.332s, episode steps:  22, steps per second:  66, episode reward: 38.817, mean reward:  1.764 [-2.749, 32.290], mean action: 2.591 [0.000, 12.000],  loss: 0.017674, mae: 0.349072, mean_q: 0.434779, mean_eps: 0.000000
 2247/5000: episode: 85, duration: 0.739s, episode steps:  56, steps per second:  76, episode reward: 38.627, mean reward:  0.690 [-3.000, 32.160], mean action: 3.054 [0.000, 12.000],  loss: 0.018385, mae: 0.351514, mean_q: 0.438051, mean_eps: 0.000000
 2283/5000: episode: 86, duration: 0.475s, episode steps:  36, steps per second:  76, episode reward: 40.626, mean reward:  1.128 [-2.296, 31.971], mean action: 3.056 [0.000, 13.000],  loss: 0.022718, mae: 0.366700, mean_q: 0.489736, mean_eps: 0.000000
 2307/5000: episode: 87, duration: 0.335s, episode steps:  24, steps per second:  72, episode reward: 44.273, mean reward:  1.845 [-2.054, 32.020], mean action: 1.625 [0.000, 11.000],  loss: 0.017716, mae: 0.338713, mean_q: 0.453676, mean_eps: 0.000000
 2328/5000: episode: 88, duration: 0.286s, episode steps:  21, steps per second:  73, episode reward: 38.637, mean reward:  1.840 [-3.000, 32.300], mean action: 4.143 [0.000, 15.000],  loss: 0.020001, mae: 0.348812, mean_q: 0.479003, mean_eps: 0.000000
 2358/5000: episode: 89, duration: 0.402s, episode steps:  30, steps per second:  75, episode reward: 44.347, mean reward:  1.478 [-2.144, 32.310], mean action: 0.900 [0.000, 12.000],  loss: 0.018493, mae: 0.341999, mean_q: 0.465519, mean_eps: 0.000000
 2381/5000: episode: 90, duration: 0.318s, episode steps:  23, steps per second:  72, episode reward: 44.628, mean reward:  1.940 [-2.099, 32.110], mean action: 5.478 [0.000, 15.000],  loss: 0.020293, mae: 0.349787, mean_q: 0.452587, mean_eps: 0.000000
 2408/5000: episode: 91, duration: 0.366s, episode steps:  27, steps per second:  74, episode reward: 41.422, mean reward:  1.534 [-2.545, 32.620], mean action: 2.741 [0.000, 16.000],  loss: 0.019956, mae: 0.353718, mean_q: 0.462342, mean_eps: 0.000000
 2453/5000: episode: 92, duration: 0.876s, episode steps:  45, steps per second:  51, episode reward: 32.753, mean reward:  0.728 [-3.000, 32.070], mean action: 5.822 [0.000, 19.000],  loss: 0.022185, mae: 0.354635, mean_q: 0.483568, mean_eps: 0.000000
 2494/5000: episode: 93, duration: 0.890s, episode steps:  41, steps per second:  46, episode reward: -34.660, mean reward: -0.845 [-32.082,  2.514], mean action: 7.561 [0.000, 16.000],  loss: 0.019886, mae: 0.346895, mean_q: 0.493479, mean_eps: 0.000000
 2543/5000: episode: 94, duration: 0.763s, episode steps:  49, steps per second:  64, episode reward: 43.245, mean reward:  0.883 [-2.350, 32.200], mean action: 3.204 [0.000, 20.000],  loss: 0.018750, mae: 0.347822, mean_q: 0.479825, mean_eps: 0.000000
 2568/5000: episode: 95, duration: 0.336s, episode steps:  25, steps per second:  74, episode reward: 41.432, mean reward:  1.657 [-2.940, 32.050], mean action: 3.760 [1.000, 16.000],  loss: 0.021879, mae: 0.355329, mean_q: 0.509064, mean_eps: 0.000000
 2595/5000: episode: 96, duration: 0.384s, episode steps:  27, steps per second:  70, episode reward: 38.143, mean reward:  1.413 [-2.136, 32.140], mean action: 5.000 [0.000, 14.000],  loss: 0.017171, mae: 0.330658, mean_q: 0.474395, mean_eps: 0.000000
 2629/5000: episode: 97, duration: 0.547s, episode steps:  34, steps per second:  62, episode reward: 37.851, mean reward:  1.113 [-2.796, 32.070], mean action: 3.765 [0.000, 16.000],  loss: 0.018195, mae: 0.335689, mean_q: 0.449199, mean_eps: 0.000000
 2652/5000: episode: 98, duration: 0.535s, episode steps:  23, steps per second:  43, episode reward: 39.000, mean reward:  1.696 [-3.000, 32.300], mean action: 3.087 [0.000, 16.000],  loss: 0.017966, mae: 0.337012, mean_q: 0.445712, mean_eps: 0.000000
 2674/5000: episode: 99, duration: 0.395s, episode steps:  22, steps per second:  56, episode reward: 41.282, mean reward:  1.876 [-3.000, 32.140], mean action: 2.227 [0.000, 16.000],  loss: 0.020806, mae: 0.356739, mean_q: 0.486969, mean_eps: 0.000000
 2699/5000: episode: 100, duration: 0.358s, episode steps:  25, steps per second:  70, episode reward: 44.093, mean reward:  1.764 [-3.000, 32.153], mean action: 3.080 [1.000, 11.000],  loss: 0.022366, mae: 0.356094, mean_q: 0.535064, mean_eps: 0.000000
 2727/5000: episode: 101, duration: 1.280s, episode steps:  28, steps per second:  22, episode reward: 35.800, mean reward:  1.279 [-3.000, 32.290], mean action: 2.857 [0.000, 11.000],  loss: 0.017716, mae: 0.335072, mean_q: 0.531211, mean_eps: 0.000000
 2781/5000: episode: 102, duration: 0.953s, episode steps:  54, steps per second:  57, episode reward: 39.000, mean reward:  0.722 [-2.605, 32.550], mean action: 2.574 [0.000, 13.000],  loss: 0.021997, mae: 0.364295, mean_q: 0.487462, mean_eps: 0.000000
 2858/5000: episode: 103, duration: 1.565s, episode steps:  77, steps per second:  49, episode reward: 38.175, mean reward:  0.496 [-2.757, 31.915], mean action: 2.558 [0.000, 12.000],  loss: 0.020708, mae: 0.350649, mean_q: 0.505339, mean_eps: 0.000000
 2921/5000: episode: 104, duration: 1.395s, episode steps:  63, steps per second:  45, episode reward: 32.545, mean reward:  0.517 [-3.000, 32.160], mean action: 7.444 [1.000, 21.000],  loss: 0.019790, mae: 0.351344, mean_q: 0.491670, mean_eps: 0.000000
 2963/5000: episode: 105, duration: 0.626s, episode steps:  42, steps per second:  67, episode reward: 42.000, mean reward:  1.000 [-2.769, 32.080], mean action: 1.976 [0.000, 19.000],  loss: 0.019379, mae: 0.353963, mean_q: 0.492751, mean_eps: 0.000000
 3000/5000: episode: 106, duration: 0.493s, episode steps:  37, steps per second:  75, episode reward: 35.900, mean reward:  0.970 [-2.686, 31.990], mean action: 3.297 [0.000, 19.000],  loss: 0.021837, mae: 0.364231, mean_q: 0.468086, mean_eps: 0.000000
 3025/5000: episode: 107, duration: 0.342s, episode steps:  25, steps per second:  73, episode reward: 38.539, mean reward:  1.542 [-2.757, 31.851], mean action: 3.360 [0.000, 20.000],  loss: 0.016048, mae: 0.340359, mean_q: 0.490037, mean_eps: 0.000000
 3053/5000: episode: 108, duration: 0.374s, episode steps:  28, steps per second:  75, episode reward: 41.160, mean reward:  1.470 [-2.835, 32.400], mean action: 2.714 [0.000, 19.000],  loss: 0.017635, mae: 0.341955, mean_q: 0.467500, mean_eps: 0.000000
 3100/5000: episode: 109, duration: 0.785s, episode steps:  47, steps per second:  60, episode reward: 41.560, mean reward:  0.884 [-2.444, 32.420], mean action: 1.851 [0.000, 15.000],  loss: 0.020073, mae: 0.349175, mean_q: 0.493144, mean_eps: 0.000000
 3115/5000: episode: 110, duration: 0.214s, episode steps:  15, steps per second:  70, episode reward: 47.218, mean reward:  3.148 [ 0.000, 32.004], mean action: 2.333 [0.000, 9.000],  loss: 0.021267, mae: 0.356425, mean_q: 0.528315, mean_eps: 0.000000
 3140/5000: episode: 111, duration: 0.354s, episode steps:  25, steps per second:  71, episode reward: 41.870, mean reward:  1.675 [-2.586, 32.160], mean action: 4.040 [0.000, 19.000],  loss: 0.017064, mae: 0.344272, mean_q: 0.449112, mean_eps: 0.000000
 3159/5000: episode: 112, duration: 0.257s, episode steps:  19, steps per second:  74, episode reward: 41.303, mean reward:  2.174 [-3.000, 32.443], mean action: 3.632 [1.000, 16.000],  loss: 0.018744, mae: 0.353466, mean_q: 0.429821, mean_eps: 0.000000
 3180/5000: episode: 113, duration: 0.294s, episode steps:  21, steps per second:  72, episode reward: 41.740, mean reward:  1.988 [-2.255, 32.901], mean action: 3.571 [0.000, 16.000],  loss: 0.020702, mae: 0.350364, mean_q: 0.466195, mean_eps: 0.000000
 3206/5000: episode: 114, duration: 0.568s, episode steps:  26, steps per second:  46, episode reward: 43.755, mean reward:  1.683 [-2.418, 32.370], mean action: 3.577 [0.000, 16.000],  loss: 0.023614, mae: 0.357293, mean_q: 0.494589, mean_eps: 0.000000
 3232/5000: episode: 115, duration: 0.400s, episode steps:  26, steps per second:  65, episode reward: 42.000, mean reward:  1.615 [-2.397, 32.120], mean action: 2.577 [1.000, 16.000],  loss: 0.018039, mae: 0.338774, mean_q: 0.520536, mean_eps: 0.000000
 3261/5000: episode: 116, duration: 0.403s, episode steps:  29, steps per second:  72, episode reward: 38.471, mean reward:  1.327 [-2.211, 32.460], mean action: 3.517 [0.000, 15.000],  loss: 0.020015, mae: 0.342992, mean_q: 0.482645, mean_eps: 0.000000
 3281/5000: episode: 117, duration: 0.278s, episode steps:  20, steps per second:  72, episode reward: 38.753, mean reward:  1.938 [-3.000, 32.113], mean action: 4.400 [0.000, 15.000],  loss: 0.016651, mae: 0.326432, mean_q: 0.493747, mean_eps: 0.000000
 3308/5000: episode: 118, duration: 0.357s, episode steps:  27, steps per second:  76, episode reward: 44.133, mean reward:  1.635 [-2.058, 32.240], mean action: 2.963 [0.000, 13.000],  loss: 0.020069, mae: 0.341489, mean_q: 0.520467, mean_eps: 0.000000
 3328/5000: episode: 119, duration: 0.283s, episode steps:  20, steps per second:  71, episode reward: 39.000, mean reward:  1.950 [-2.710, 32.530], mean action: 2.800 [0.000, 12.000],  loss: 0.019014, mae: 0.339052, mean_q: 0.553410, mean_eps: 0.000000
 3343/5000: episode: 120, duration: 0.209s, episode steps:  15, steps per second:  72, episode reward: 47.015, mean reward:  3.134 [-0.024, 32.440], mean action: 2.867 [0.000, 12.000],  loss: 0.015952, mae: 0.325528, mean_q: 0.510738, mean_eps: 0.000000
 3362/5000: episode: 121, duration: 0.269s, episode steps:  19, steps per second:  71, episode reward: 41.575, mean reward:  2.188 [-3.000, 32.245], mean action: 2.526 [0.000, 11.000],  loss: 0.021809, mae: 0.350686, mean_q: 0.469770, mean_eps: 0.000000
 3382/5000: episode: 122, duration: 0.279s, episode steps:  20, steps per second:  72, episode reward: 45.000, mean reward:  2.250 [-2.384, 32.540], mean action: 1.500 [0.000, 11.000],  loss: 0.021814, mae: 0.352370, mean_q: 0.469226, mean_eps: 0.000000
 3408/5000: episode: 123, duration: 0.346s, episode steps:  26, steps per second:  75, episode reward: 38.412, mean reward:  1.477 [-2.939, 32.380], mean action: 2.385 [0.000, 19.000],  loss: 0.018758, mae: 0.334665, mean_q: 0.489703, mean_eps: 0.000000
 3431/5000: episode: 124, duration: 0.312s, episode steps:  23, steps per second:  74, episode reward: 36.000, mean reward:  1.565 [-2.904, 32.080], mean action: 3.652 [0.000, 19.000],  loss: 0.022575, mae: 0.354086, mean_q: 0.486245, mean_eps: 0.000000
 3452/5000: episode: 125, duration: 0.300s, episode steps:  21, steps per second:  70, episode reward: 40.495, mean reward:  1.928 [-2.908, 32.420], mean action: 3.190 [0.000, 19.000],  loss: 0.017129, mae: 0.336825, mean_q: 0.449551, mean_eps: 0.000000
 3487/5000: episode: 126, duration: 0.449s, episode steps:  35, steps per second:  78, episode reward: 33.000, mean reward:  0.943 [-3.000, 32.050], mean action: 6.714 [0.000, 16.000],  loss: 0.021310, mae: 0.352767, mean_q: 0.529847, mean_eps: 0.000000
 3519/5000: episode: 127, duration: 0.423s, episode steps:  32, steps per second:  76, episode reward: 38.528, mean reward:  1.204 [-2.198, 31.918], mean action: 2.688 [0.000, 16.000],  loss: 0.023826, mae: 0.363870, mean_q: 0.586071, mean_eps: 0.000000
 3538/5000: episode: 128, duration: 0.256s, episode steps:  19, steps per second:  74, episode reward: 40.936, mean reward:  2.155 [-2.054, 32.154], mean action: 2.053 [0.000, 12.000],  loss: 0.021304, mae: 0.352837, mean_q: 0.588868, mean_eps: 0.000000
 3557/5000: episode: 129, duration: 0.263s, episode steps:  19, steps per second:  72, episode reward: 44.265, mean reward:  2.330 [-2.545, 32.010], mean action: 1.158 [0.000, 9.000],  loss: 0.018336, mae: 0.336205, mean_q: 0.567529, mean_eps: 0.000000
 3575/5000: episode: 130, duration: 0.245s, episode steps:  18, steps per second:  74, episode reward: 41.727, mean reward:  2.318 [-2.277, 32.370], mean action: 1.611 [0.000, 9.000],  loss: 0.023436, mae: 0.355583, mean_q: 0.539958, mean_eps: 0.000000
 3593/5000: episode: 131, duration: 0.245s, episode steps:  18, steps per second:  73, episode reward: 44.769, mean reward:  2.487 [-2.063, 32.840], mean action: 1.222 [0.000, 9.000],  loss: 0.018476, mae: 0.349774, mean_q: 0.471336, mean_eps: 0.000000
 3626/5000: episode: 132, duration: 0.432s, episode steps:  33, steps per second:  76, episode reward: 38.188, mean reward:  1.157 [-2.700, 32.300], mean action: 3.182 [0.000, 19.000],  loss: 0.019309, mae: 0.346234, mean_q: 0.481571, mean_eps: 0.000000
 3654/5000: episode: 133, duration: 0.372s, episode steps:  28, steps per second:  75, episode reward: 41.462, mean reward:  1.481 [-2.356, 32.047], mean action: 4.071 [1.000, 16.000],  loss: 0.021243, mae: 0.351586, mean_q: 0.561841, mean_eps: 0.000000
 3700/5000: episode: 134, duration: 0.595s, episode steps:  46, steps per second:  77, episode reward: 40.419, mean reward:  0.879 [-2.536, 31.877], mean action: 4.370 [0.000, 20.000],  loss: 0.021209, mae: 0.348106, mean_q: 0.517923, mean_eps: 0.000000
 3798/5000: episode: 135, duration: 1.251s, episode steps:  98, steps per second:  78, episode reward: 32.286, mean reward:  0.329 [-2.431, 32.137], mean action: 9.990 [0.000, 21.000],  loss: 0.020273, mae: 0.348207, mean_q: 0.495522, mean_eps: 0.000000
 3818/5000: episode: 136, duration: 0.278s, episode steps:  20, steps per second:  72, episode reward: 40.489, mean reward:  2.024 [-2.207, 32.340], mean action: 3.900 [0.000, 16.000],  loss: 0.021688, mae: 0.352621, mean_q: 0.512508, mean_eps: 0.000000
 3835/5000: episode: 137, duration: 0.239s, episode steps:  17, steps per second:  71, episode reward: 44.440, mean reward:  2.614 [-2.278, 32.340], mean action: 2.000 [0.000, 14.000],  loss: 0.018802, mae: 0.345772, mean_q: 0.533039, mean_eps: 0.000000
 3871/5000: episode: 138, duration: 0.479s, episode steps:  36, steps per second:  75, episode reward: 38.166, mean reward:  1.060 [-2.393, 31.910], mean action: 3.889 [1.000, 12.000],  loss: 0.020544, mae: 0.354708, mean_q: 0.506417, mean_eps: 0.000000
 3897/5000: episode: 139, duration: 0.349s, episode steps:  26, steps per second:  75, episode reward: 38.593, mean reward:  1.484 [-2.997, 32.100], mean action: 3.615 [0.000, 19.000],  loss: 0.020189, mae: 0.359812, mean_q: 0.517810, mean_eps: 0.000000
 3952/5000: episode: 140, duration: 0.700s, episode steps:  55, steps per second:  79, episode reward: 37.991, mean reward:  0.691 [-2.959, 32.303], mean action: 3.782 [0.000, 16.000],  loss: 0.019631, mae: 0.349478, mean_q: 0.505518, mean_eps: 0.000000
 3972/5000: episode: 141, duration: 0.269s, episode steps:  20, steps per second:  74, episode reward: 42.000, mean reward:  2.100 [-2.885, 32.100], mean action: 2.800 [0.000, 15.000],  loss: 0.021513, mae: 0.363400, mean_q: 0.515404, mean_eps: 0.000000
 3987/5000: episode: 142, duration: 0.216s, episode steps:  15, steps per second:  69, episode reward: 43.514, mean reward:  2.901 [-2.238, 31.772], mean action: 2.267 [0.000, 15.000],  loss: 0.022522, mae: 0.369910, mean_q: 0.483347, mean_eps: 0.000000
 4012/5000: episode: 143, duration: 0.338s, episode steps:  25, steps per second:  74, episode reward: 38.089, mean reward:  1.524 [-2.137, 31.987], mean action: 5.840 [0.000, 20.000],  loss: 0.024386, mae: 0.379542, mean_q: 0.499201, mean_eps: 0.000000
 4041/5000: episode: 144, duration: 0.387s, episode steps:  29, steps per second:  75, episode reward: 33.413, mean reward:  1.152 [-2.652, 32.080], mean action: 7.000 [0.000, 15.000],  loss: 0.020290, mae: 0.359516, mean_q: 0.557648, mean_eps: 0.000000
 4081/5000: episode: 145, duration: 0.518s, episode steps:  40, steps per second:  77, episode reward: 35.817, mean reward:  0.895 [-3.000, 32.430], mean action: 5.025 [0.000, 19.000],  loss: 0.022231, mae: 0.366262, mean_q: 0.512424, mean_eps: 0.000000
 4103/5000: episode: 146, duration: 0.304s, episode steps:  22, steps per second:  72, episode reward: 44.855, mean reward:  2.039 [-2.189, 32.320], mean action: 3.273 [0.000, 20.000],  loss: 0.020555, mae: 0.356803, mean_q: 0.538980, mean_eps: 0.000000
 4130/5000: episode: 147, duration: 0.361s, episode steps:  27, steps per second:  75, episode reward: 41.572, mean reward:  1.540 [-2.115, 32.679], mean action: 2.778 [0.000, 15.000],  loss: 0.021361, mae: 0.364882, mean_q: 0.525970, mean_eps: 0.000000
 4157/5000: episode: 148, duration: 0.558s, episode steps:  27, steps per second:  48, episode reward: 44.046, mean reward:  1.631 [-2.263, 32.226], mean action: 1.407 [0.000, 15.000],  loss: 0.024837, mae: 0.373499, mean_q: 0.494786, mean_eps: 0.000000
 4181/5000: episode: 149, duration: 0.381s, episode steps:  24, steps per second:  63, episode reward: 41.842, mean reward:  1.743 [-2.421, 31.942], mean action: 1.875 [0.000, 12.000],  loss: 0.019856, mae: 0.349663, mean_q: 0.484724, mean_eps: 0.000000
 4216/5000: episode: 150, duration: 0.542s, episode steps:  35, steps per second:  65, episode reward: 41.322, mean reward:  1.181 [-2.925, 32.082], mean action: 2.200 [0.000, 16.000],  loss: 0.018527, mae: 0.343339, mean_q: 0.479691, mean_eps: 0.000000
 4237/5000: episode: 151, duration: 0.298s, episode steps:  21, steps per second:  70, episode reward: 44.193, mean reward:  2.104 [-2.325, 32.940], mean action: 2.190 [0.000, 16.000],  loss: 0.018516, mae: 0.339091, mean_q: 0.499789, mean_eps: 0.000000
 4268/5000: episode: 152, duration: 0.418s, episode steps:  31, steps per second:  74, episode reward: 44.056, mean reward:  1.421 [-2.524, 32.201], mean action: 3.645 [0.000, 21.000],  loss: 0.019381, mae: 0.345965, mean_q: 0.495872, mean_eps: 0.000000
 4295/5000: episode: 153, duration: 0.379s, episode steps:  27, steps per second:  71, episode reward: 41.294, mean reward:  1.529 [-2.190, 32.120], mean action: 3.074 [0.000, 19.000],  loss: 0.022750, mae: 0.363258, mean_q: 0.472888, mean_eps: 0.000000
 4327/5000: episode: 154, duration: 0.485s, episode steps:  32, steps per second:  66, episode reward: 38.421, mean reward:  1.201 [-3.000, 32.545], mean action: 4.594 [1.000, 20.000],  loss: 0.021225, mae: 0.352648, mean_q: 0.503596, mean_eps: 0.000000
 4350/5000: episode: 155, duration: 0.549s, episode steps:  23, steps per second:  42, episode reward: 37.882, mean reward:  1.647 [-3.000, 32.479], mean action: 5.087 [0.000, 21.000],  loss: 0.022907, mae: 0.362698, mean_q: 0.538557, mean_eps: 0.000000
 4371/5000: episode: 156, duration: 0.451s, episode steps:  21, steps per second:  47, episode reward: 36.000, mean reward:  1.714 [-3.000, 32.100], mean action: 6.571 [0.000, 16.000],  loss: 0.016592, mae: 0.334105, mean_q: 0.531118, mean_eps: 0.000000
 4386/5000: episode: 157, duration: 0.248s, episode steps:  15, steps per second:  61, episode reward: 44.916, mean reward:  2.994 [-2.405, 33.000], mean action: 2.600 [0.000, 12.000],  loss: 0.020307, mae: 0.358255, mean_q: 0.504384, mean_eps: 0.000000
 4412/5000: episode: 158, duration: 0.480s, episode steps:  26, steps per second:  54, episode reward: 37.592, mean reward:  1.446 [-3.000, 32.110], mean action: 2.923 [0.000, 20.000],  loss: 0.022943, mae: 0.373243, mean_q: 0.510012, mean_eps: 0.000000
 4434/5000: episode: 159, duration: 0.753s, episode steps:  22, steps per second:  29, episode reward: 44.390, mean reward:  2.018 [-2.561, 32.340], mean action: 1.409 [0.000, 16.000],  loss: 0.019114, mae: 0.352991, mean_q: 0.539163, mean_eps: 0.000000
 4454/5000: episode: 160, duration: 0.333s, episode steps:  20, steps per second:  60, episode reward: 47.024, mean reward:  2.351 [-0.170, 32.270], mean action: 2.300 [1.000, 3.000],  loss: 0.019270, mae: 0.352597, mean_q: 0.524388, mean_eps: 0.000000
 4484/5000: episode: 161, duration: 0.651s, episode steps:  30, steps per second:  46, episode reward: 36.000, mean reward:  1.200 [-2.607, 32.480], mean action: 5.967 [0.000, 20.000],  loss: 0.019224, mae: 0.350552, mean_q: 0.580442, mean_eps: 0.000000
 4508/5000: episode: 162, duration: 0.333s, episode steps:  24, steps per second:  72, episode reward: 41.346, mean reward:  1.723 [-2.674, 32.176], mean action: 3.625 [0.000, 16.000],  loss: 0.019823, mae: 0.352915, mean_q: 0.513989, mean_eps: 0.000000
 4531/5000: episode: 163, duration: 0.696s, episode steps:  23, steps per second:  33, episode reward: 39.000, mean reward:  1.696 [-2.285, 32.080], mean action: 3.652 [0.000, 19.000],  loss: 0.016762, mae: 0.343551, mean_q: 0.453615, mean_eps: 0.000000
 4550/5000: episode: 164, duration: 0.314s, episode steps:  19, steps per second:  60, episode reward: 44.085, mean reward:  2.320 [-2.054, 32.430], mean action: 3.158 [0.000, 19.000],  loss: 0.019399, mae: 0.349329, mean_q: 0.571256, mean_eps: 0.000000
 4581/5000: episode: 165, duration: 0.749s, episode steps:  31, steps per second:  41, episode reward: 41.032, mean reward:  1.324 [-2.571, 31.607], mean action: 2.871 [0.000, 20.000],  loss: 0.018085, mae: 0.349394, mean_q: 0.523576, mean_eps: 0.000000
 4603/5000: episode: 166, duration: 0.579s, episode steps:  22, steps per second:  38, episode reward: 41.376, mean reward:  1.881 [-2.105, 32.790], mean action: 3.955 [0.000, 15.000],  loss: 0.025319, mae: 0.378501, mean_q: 0.528924, mean_eps: 0.000000
 4634/5000: episode: 167, duration: 0.609s, episode steps:  31, steps per second:  51, episode reward: 40.574, mean reward:  1.309 [-2.477, 31.932], mean action: 4.194 [1.000, 14.000],  loss: 0.016296, mae: 0.343040, mean_q: 0.533641, mean_eps: 0.000000
 4655/5000: episode: 168, duration: 0.304s, episode steps:  21, steps per second:  69, episode reward: 39.000, mean reward:  1.857 [-2.798, 32.410], mean action: 3.524 [0.000, 19.000],  loss: 0.018542, mae: 0.353641, mean_q: 0.548483, mean_eps: 0.000000
 4676/5000: episode: 169, duration: 0.544s, episode steps:  21, steps per second:  39, episode reward: 41.570, mean reward:  1.980 [-2.382, 32.110], mean action: 2.429 [0.000, 12.000],  loss: 0.018810, mae: 0.353652, mean_q: 0.551793, mean_eps: 0.000000
 4694/5000: episode: 170, duration: 0.291s, episode steps:  18, steps per second:  62, episode reward: 44.278, mean reward:  2.460 [-2.188, 31.593], mean action: 1.333 [0.000, 12.000],  loss: 0.021697, mae: 0.365501, mean_q: 0.548784, mean_eps: 0.000000
 4725/5000: episode: 171, duration: 0.468s, episode steps:  31, steps per second:  66, episode reward: 38.350, mean reward:  1.237 [-2.435, 32.140], mean action: 3.129 [0.000, 12.000],  loss: 0.020840, mae: 0.360634, mean_q: 0.549025, mean_eps: 0.000000
 4742/5000: episode: 172, duration: 0.244s, episode steps:  17, steps per second:  70, episode reward: 38.514, mean reward:  2.266 [-2.971, 32.620], mean action: 3.294 [0.000, 14.000],  loss: 0.017821, mae: 0.342859, mean_q: 0.510241, mean_eps: 0.000000
 4769/5000: episode: 173, duration: 0.358s, episode steps:  27, steps per second:  76, episode reward: 39.000, mean reward:  1.444 [-2.335, 32.340], mean action: 1.741 [0.000, 12.000],  loss: 0.022879, mae: 0.367603, mean_q: 0.569599, mean_eps: 0.000000
 4790/5000: episode: 174, duration: 0.285s, episode steps:  21, steps per second:  74, episode reward: 41.610, mean reward:  1.981 [-2.407, 31.942], mean action: 2.810 [0.000, 12.000],  loss: 0.018485, mae: 0.350584, mean_q: 0.531570, mean_eps: 0.000000
 4810/5000: episode: 175, duration: 0.339s, episode steps:  20, steps per second:  59, episode reward: 42.000, mean reward:  2.100 [-2.199, 32.120], mean action: 4.700 [0.000, 14.000],  loss: 0.018826, mae: 0.352579, mean_q: 0.495441, mean_eps: 0.000000
 4834/5000: episode: 176, duration: 0.321s, episode steps:  24, steps per second:  75, episode reward: 41.171, mean reward:  1.715 [-2.709, 32.025], mean action: 4.000 [0.000, 14.000],  loss: 0.019442, mae: 0.359875, mean_q: 0.495952, mean_eps: 0.000000
 4866/5000: episode: 177, duration: 0.632s, episode steps:  32, steps per second:  51, episode reward: 35.599, mean reward:  1.112 [-2.584, 33.000], mean action: 3.469 [0.000, 16.000],  loss: 0.019600, mae: 0.354367, mean_q: 0.518738, mean_eps: 0.000000
 4887/5000: episode: 178, duration: 0.430s, episode steps:  21, steps per second:  49, episode reward: 41.599, mean reward:  1.981 [-2.247, 32.091], mean action: 2.238 [0.000, 9.000],  loss: 0.022413, mae: 0.362487, mean_q: 0.511090, mean_eps: 0.000000
 4902/5000: episode: 179, duration: 0.328s, episode steps:  15, steps per second:  46, episode reward: 44.876, mean reward:  2.992 [-2.122, 32.320], mean action: 1.933 [0.000, 9.000],  loss: 0.022001, mae: 0.369783, mean_q: 0.500293, mean_eps: 0.000000
 4927/5000: episode: 180, duration: 0.372s, episode steps:  25, steps per second:  67, episode reward: 37.768, mean reward:  1.511 [-2.193, 32.133], mean action: 5.920 [0.000, 20.000],  loss: 0.021433, mae: 0.363078, mean_q: 0.585185, mean_eps: 0.000000
 4951/5000: episode: 181, duration: 0.425s, episode steps:  24, steps per second:  56, episode reward: 46.458, mean reward:  1.936 [-0.411, 32.090], mean action: 3.250 [1.000, 14.000],  loss: 0.023093, mae: 0.372577, mean_q: 0.605177, mean_eps: 0.000000
 4978/5000: episode: 182, duration: 0.466s, episode steps:  27, steps per second:  58, episode reward: 39.000, mean reward:  1.444 [-2.351, 32.090], mean action: 2.000 [0.000, 11.000],  loss: 0.022249, mae: 0.374817, mean_q: 0.523175, mean_eps: 0.000000
done, took 72.413 seconds
DQN Evaluation: 4953 victories out of 5843 episodes
Training for 5000 steps ...
   38/5000: episode: 1, duration: 0.271s, episode steps:  38, steps per second: 140, episode reward: -35.300, mean reward: -0.929 [-32.004,  2.620], mean action: 4.237 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   71/5000: episode: 2, duration: 0.227s, episode steps:  33, steps per second: 145, episode reward: 38.250, mean reward:  1.159 [-2.361, 32.100], mean action: 4.303 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   96/5000: episode: 3, duration: 0.166s, episode steps:  25, steps per second: 151, episode reward: 32.480, mean reward:  1.299 [-2.426, 32.190], mean action: 7.000 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  115/5000: episode: 4, duration: 0.140s, episode steps:  19, steps per second: 136, episode reward: -35.390, mean reward: -1.863 [-32.090,  2.711], mean action: 4.684 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  132/5000: episode: 5, duration: 0.122s, episode steps:  17, steps per second: 139, episode reward: 36.000, mean reward:  2.118 [-3.000, 30.647], mean action: 3.824 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  153/5000: episode: 6, duration: 0.141s, episode steps:  21, steps per second: 149, episode reward: 37.494, mean reward:  1.785 [-2.576, 31.516], mean action: 4.000 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  172/5000: episode: 7, duration: 0.126s, episode steps:  19, steps per second: 150, episode reward: 36.000, mean reward:  1.895 [-2.405, 32.150], mean action: 4.000 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  213/5000: episode: 8, duration: 0.243s, episode steps:  41, steps per second: 169, episode reward: -33.000, mean reward: -0.805 [-32.067,  2.070], mean action: 4.073 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  228/5000: episode: 9, duration: 0.108s, episode steps:  15, steps per second: 139, episode reward: 41.367, mean reward:  2.758 [-2.095, 33.000], mean action: 2.400 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  336/5000: episode: 10, duration: 0.651s, episode steps: 108, steps per second: 166, episode reward: 32.830, mean reward:  0.304 [-2.468, 32.123], mean action: 11.741 [0.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  364/5000: episode: 11, duration: 0.191s, episode steps:  28, steps per second: 147, episode reward: 39.000, mean reward:  1.393 [-2.282, 33.179], mean action: 8.786 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  393/5000: episode: 12, duration: 0.195s, episode steps:  29, steps per second: 149, episode reward: 32.181, mean reward:  1.110 [-2.521, 29.600], mean action: 6.103 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  418/5000: episode: 13, duration: 0.160s, episode steps:  25, steps per second: 156, episode reward: -35.820, mean reward: -1.433 [-32.644,  2.360], mean action: 4.040 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  430/5000: episode: 14, duration: 0.091s, episode steps:  12, steps per second: 132, episode reward: 44.564, mean reward:  3.714 [-2.142, 32.813], mean action: 3.667 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  447/5000: episode: 15, duration: 0.130s, episode steps:  17, steps per second: 131, episode reward: 41.348, mean reward:  2.432 [-2.503, 32.258], mean action: 2.353 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  472/5000: episode: 16, duration: 0.188s, episode steps:  25, steps per second: 133, episode reward: 41.437, mean reward:  1.657 [-2.510, 32.124], mean action: 3.720 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  490/5000: episode: 17, duration: 0.134s, episode steps:  18, steps per second: 134, episode reward: 41.848, mean reward:  2.325 [-2.461, 32.240], mean action: 3.778 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  513/5000: episode: 18, duration: 0.331s, episode steps:  23, steps per second:  70, episode reward: -35.330, mean reward: -1.536 [-31.672,  2.320], mean action: 4.130 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  532/5000: episode: 19, duration: 0.154s, episode steps:  19, steps per second: 123, episode reward: 32.408, mean reward:  1.706 [-3.000, 31.448], mean action: 6.684 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  560/5000: episode: 20, duration: 0.327s, episode steps:  28, steps per second:  86, episode reward: 40.516, mean reward:  1.447 [-2.193, 31.772], mean action: 2.429 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  582/5000: episode: 21, duration: 0.290s, episode steps:  22, steps per second:  76, episode reward: 36.000, mean reward:  1.636 [-3.000, 32.220], mean action: 3.682 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  603/5000: episode: 22, duration: 0.290s, episode steps:  21, steps per second:  72, episode reward: 33.000, mean reward:  1.571 [-3.000, 32.910], mean action: 3.429 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  621/5000: episode: 23, duration: 0.292s, episode steps:  18, steps per second:  62, episode reward: 41.125, mean reward:  2.285 [-2.419, 32.251], mean action: 4.556 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  643/5000: episode: 24, duration: 0.248s, episode steps:  22, steps per second:  89, episode reward: 35.590, mean reward:  1.618 [-3.000, 32.030], mean action: 3.545 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  672/5000: episode: 25, duration: 0.217s, episode steps:  29, steps per second: 134, episode reward: -35.740, mean reward: -1.232 [-32.383,  2.090], mean action: 6.966 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  699/5000: episode: 26, duration: 0.176s, episode steps:  27, steps per second: 153, episode reward: 35.156, mean reward:  1.302 [-3.000, 33.000], mean action: 4.815 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  724/5000: episode: 27, duration: 0.161s, episode steps:  25, steps per second: 156, episode reward: 32.462, mean reward:  1.298 [-3.000, 32.050], mean action: 3.720 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  743/5000: episode: 28, duration: 0.243s, episode steps:  19, steps per second:  78, episode reward: 37.588, mean reward:  1.978 [-2.447, 32.498], mean action: 3.105 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  774/5000: episode: 29, duration: 0.209s, episode steps:  31, steps per second: 148, episode reward: 33.000, mean reward:  1.065 [-2.745, 32.560], mean action: 4.871 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  799/5000: episode: 30, duration: 0.161s, episode steps:  25, steps per second: 156, episode reward: 32.545, mean reward:  1.302 [-3.000, 32.545], mean action: 3.840 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  822/5000: episode: 31, duration: 0.153s, episode steps:  23, steps per second: 151, episode reward: 38.295, mean reward:  1.665 [-3.000, 32.130], mean action: 3.261 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  842/5000: episode: 32, duration: 0.133s, episode steps:  20, steps per second: 151, episode reward: 38.113, mean reward:  1.906 [-3.000, 32.269], mean action: 3.550 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  869/5000: episode: 33, duration: 0.302s, episode steps:  27, steps per second:  89, episode reward: 32.316, mean reward:  1.197 [-3.000, 32.140], mean action: 5.741 [1.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  902/5000: episode: 34, duration: 0.256s, episode steps:  33, steps per second: 129, episode reward: 35.573, mean reward:  1.078 [-2.217, 32.573], mean action: 2.727 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  931/5000: episode: 35, duration: 0.215s, episode steps:  29, steps per second: 135, episode reward: -33.000, mean reward: -1.138 [-32.595,  2.399], mean action: 2.724 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  961/5000: episode: 36, duration: 0.191s, episode steps:  30, steps per second: 157, episode reward: 38.312, mean reward:  1.277 [-2.186, 32.085], mean action: 3.900 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  981/5000: episode: 37, duration: 0.226s, episode steps:  20, steps per second:  89, episode reward: 38.408, mean reward:  1.920 [-3.000, 33.000], mean action: 3.600 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  998/5000: episode: 38, duration: 0.169s, episode steps:  17, steps per second: 101, episode reward: 35.394, mean reward:  2.082 [-2.637, 32.493], mean action: 5.118 [1.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1019/5000: episode: 39, duration: 0.403s, episode steps:  21, steps per second:  52, episode reward: 32.903, mean reward:  1.567 [-3.000, 31.923], mean action: 5.429 [1.000, 14.000],  loss: 0.020063, mae: 0.360164, mean_q: 0.485279, mean_eps: 0.000000
 1038/5000: episode: 40, duration: 0.320s, episode steps:  19, steps per second:  59, episode reward: -36.000, mean reward: -1.895 [-32.221,  2.828], mean action: 4.684 [0.000, 16.000],  loss: 0.019065, mae: 0.354541, mean_q: 0.538749, mean_eps: 0.000000
 1068/5000: episode: 41, duration: 0.501s, episode steps:  30, steps per second:  60, episode reward: 32.793, mean reward:  1.093 [-3.000, 32.190], mean action: 8.233 [0.000, 20.000],  loss: 0.021141, mae: 0.363138, mean_q: 0.509178, mean_eps: 0.000000
 1085/5000: episode: 42, duration: 0.262s, episode steps:  17, steps per second:  65, episode reward: 41.891, mean reward:  2.464 [-2.403, 32.368], mean action: 2.706 [1.000, 12.000],  loss: 0.018550, mae: 0.350713, mean_q: 0.445411, mean_eps: 0.000000
 1104/5000: episode: 43, duration: 0.270s, episode steps:  19, steps per second:  70, episode reward: 38.532, mean reward:  2.028 [-2.242, 32.610], mean action: 6.632 [0.000, 15.000],  loss: 0.022419, mae: 0.367092, mean_q: 0.444671, mean_eps: 0.000000
 1127/5000: episode: 44, duration: 0.317s, episode steps:  23, steps per second:  73, episode reward: 37.669, mean reward:  1.638 [-2.217, 32.680], mean action: 5.217 [0.000, 13.000],  loss: 0.018799, mae: 0.349244, mean_q: 0.526368, mean_eps: 0.000000
 1147/5000: episode: 45, duration: 0.280s, episode steps:  20, steps per second:  71, episode reward: 36.000, mean reward:  1.800 [-2.757, 33.000], mean action: 4.400 [0.000, 15.000],  loss: 0.019974, mae: 0.356890, mean_q: 0.488523, mean_eps: 0.000000
 1190/5000: episode: 46, duration: 0.549s, episode steps:  43, steps per second:  78, episode reward: 32.721, mean reward:  0.761 [-2.558, 29.896], mean action: 11.302 [1.000, 20.000],  loss: 0.020436, mae: 0.360099, mean_q: 0.480280, mean_eps: 0.000000
 1218/5000: episode: 47, duration: 0.465s, episode steps:  28, steps per second:  60, episode reward: 35.904, mean reward:  1.282 [-2.222, 31.944], mean action: 5.571 [1.000, 19.000],  loss: 0.019318, mae: 0.349736, mean_q: 0.510122, mean_eps: 0.000000
 1238/5000: episode: 48, duration: 0.284s, episode steps:  20, steps per second:  70, episode reward: 32.196, mean reward:  1.610 [-2.903, 32.430], mean action: 6.000 [1.000, 19.000],  loss: 0.020162, mae: 0.353969, mean_q: 0.519117, mean_eps: 0.000000
 1263/5000: episode: 49, duration: 0.420s, episode steps:  25, steps per second:  60, episode reward: -41.990, mean reward: -1.680 [-32.095,  2.250], mean action: 7.160 [0.000, 17.000],  loss: 0.019726, mae: 0.359109, mean_q: 0.463751, mean_eps: 0.000000
 1294/5000: episode: 50, duration: 0.503s, episode steps:  31, steps per second:  62, episode reward: 32.131, mean reward:  1.036 [-3.000, 33.000], mean action: 3.355 [0.000, 15.000],  loss: 0.018935, mae: 0.365071, mean_q: 0.449731, mean_eps: 0.000000
 1310/5000: episode: 51, duration: 0.222s, episode steps:  16, steps per second:  72, episode reward: 42.000, mean reward:  2.625 [-2.195, 32.230], mean action: 2.562 [0.000, 11.000],  loss: 0.020836, mae: 0.366131, mean_q: 0.498586, mean_eps: 0.000000
 1334/5000: episode: 52, duration: 0.326s, episode steps:  24, steps per second:  74, episode reward: 38.420, mean reward:  1.601 [-3.000, 32.060], mean action: 2.750 [1.000, 13.000],  loss: 0.019515, mae: 0.349192, mean_q: 0.494507, mean_eps: 0.000000
 1348/5000: episode: 53, duration: 0.198s, episode steps:  14, steps per second:  71, episode reward: 39.000, mean reward:  2.786 [-2.151, 33.000], mean action: 3.786 [0.000, 15.000],  loss: 0.021158, mae: 0.365414, mean_q: 0.515829, mean_eps: 0.000000
 1366/5000: episode: 54, duration: 0.247s, episode steps:  18, steps per second:  73, episode reward: 41.006, mean reward:  2.278 [-2.226, 31.970], mean action: 2.889 [0.000, 15.000],  loss: 0.017143, mae: 0.347206, mean_q: 0.454819, mean_eps: 0.000000
 1385/5000: episode: 55, duration: 0.260s, episode steps:  19, steps per second:  73, episode reward: 37.500, mean reward:  1.974 [-2.506, 32.043], mean action: 7.105 [0.000, 20.000],  loss: 0.017429, mae: 0.342661, mean_q: 0.454402, mean_eps: 0.000000
 1414/5000: episode: 56, duration: 0.415s, episode steps:  29, steps per second:  70, episode reward: 38.054, mean reward:  1.312 [-2.468, 32.910], mean action: 4.172 [0.000, 20.000],  loss: 0.020695, mae: 0.355656, mean_q: 0.460275, mean_eps: 0.000000
 1447/5000: episode: 57, duration: 0.430s, episode steps:  33, steps per second:  77, episode reward: -35.320, mean reward: -1.070 [-31.328,  2.770], mean action: 6.273 [0.000, 20.000],  loss: 0.019854, mae: 0.343675, mean_q: 0.519988, mean_eps: 0.000000
 1462/5000: episode: 58, duration: 0.207s, episode steps:  15, steps per second:  72, episode reward: 41.677, mean reward:  2.778 [-2.349, 32.178], mean action: 2.400 [0.000, 12.000],  loss: 0.020473, mae: 0.355884, mean_q: 0.529215, mean_eps: 0.000000
 1487/5000: episode: 59, duration: 0.327s, episode steps:  25, steps per second:  76, episode reward: -38.220, mean reward: -1.529 [-32.528,  2.239], mean action: 4.400 [1.000, 15.000],  loss: 0.019973, mae: 0.354011, mean_q: 0.512032, mean_eps: 0.000000
 1510/5000: episode: 60, duration: 0.302s, episode steps:  23, steps per second:  76, episode reward: -33.000, mean reward: -1.435 [-32.179,  2.370], mean action: 7.565 [0.000, 16.000],  loss: 0.018145, mae: 0.354171, mean_q: 0.461707, mean_eps: 0.000000
 1550/5000: episode: 61, duration: 0.514s, episode steps:  40, steps per second:  78, episode reward: 34.820, mean reward:  0.870 [-2.465, 31.902], mean action: 4.050 [0.000, 20.000],  loss: 0.018445, mae: 0.354226, mean_q: 0.467096, mean_eps: 0.000000
 1570/5000: episode: 62, duration: 0.274s, episode steps:  20, steps per second:  73, episode reward: 41.057, mean reward:  2.053 [-2.416, 31.838], mean action: 3.500 [0.000, 14.000],  loss: 0.020463, mae: 0.355074, mean_q: 0.514221, mean_eps: 0.000000
 1598/5000: episode: 63, duration: 0.368s, episode steps:  28, steps per second:  76, episode reward: -32.740, mean reward: -1.169 [-32.143,  3.000], mean action: 7.500 [0.000, 19.000],  loss: 0.018086, mae: 0.345209, mean_q: 0.474989, mean_eps: 0.000000
 1621/5000: episode: 64, duration: 0.307s, episode steps:  23, steps per second:  75, episode reward: -33.000, mean reward: -1.435 [-32.243,  3.000], mean action: 6.348 [0.000, 21.000],  loss: 0.021127, mae: 0.353008, mean_q: 0.467494, mean_eps: 0.000000
 1645/5000: episode: 65, duration: 0.381s, episode steps:  24, steps per second:  63, episode reward: 32.904, mean reward:  1.371 [-2.566, 32.264], mean action: 3.417 [0.000, 12.000],  loss: 0.020217, mae: 0.340717, mean_q: 0.521117, mean_eps: 0.000000
 1666/5000: episode: 66, duration: 0.349s, episode steps:  21, steps per second:  60, episode reward: -33.000, mean reward: -1.571 [-32.129,  2.449], mean action: 4.048 [1.000, 12.000],  loss: 0.018498, mae: 0.339312, mean_q: 0.503854, mean_eps: 0.000000
 1701/5000: episode: 67, duration: 0.484s, episode steps:  35, steps per second:  72, episode reward: -32.760, mean reward: -0.936 [-32.001,  2.412], mean action: 4.029 [0.000, 20.000],  loss: 0.019355, mae: 0.343307, mean_q: 0.561518, mean_eps: 0.000000
 1722/5000: episode: 68, duration: 0.293s, episode steps:  21, steps per second:  72, episode reward: 38.311, mean reward:  1.824 [-3.000, 32.230], mean action: 2.286 [0.000, 9.000],  loss: 0.021196, mae: 0.355658, mean_q: 0.513273, mean_eps: 0.000000
 1743/5000: episode: 69, duration: 0.290s, episode steps:  21, steps per second:  73, episode reward: -32.190, mean reward: -1.533 [-32.004,  2.902], mean action: 4.762 [1.000, 10.000],  loss: 0.020146, mae: 0.356285, mean_q: 0.536606, mean_eps: 0.000000
 1766/5000: episode: 70, duration: 0.310s, episode steps:  23, steps per second:  74, episode reward: 35.747, mean reward:  1.554 [-2.491, 32.240], mean action: 2.957 [0.000, 9.000],  loss: 0.016161, mae: 0.332597, mean_q: 0.521652, mean_eps: 0.000000
 1789/5000: episode: 71, duration: 0.309s, episode steps:  23, steps per second:  74, episode reward: 38.881, mean reward:  1.690 [-2.318, 32.300], mean action: 2.957 [0.000, 9.000],  loss: 0.020792, mae: 0.352430, mean_q: 0.477955, mean_eps: 0.000000
 1817/5000: episode: 72, duration: 0.369s, episode steps:  28, steps per second:  76, episode reward: 32.035, mean reward:  1.144 [-2.276, 32.055], mean action: 4.607 [0.000, 16.000],  loss: 0.019223, mae: 0.352968, mean_q: 0.491828, mean_eps: 0.000000
 1842/5000: episode: 73, duration: 0.665s, episode steps:  25, steps per second:  38, episode reward: 34.759, mean reward:  1.390 [-2.682, 32.175], mean action: 4.120 [0.000, 15.000],  loss: 0.023630, mae: 0.367925, mean_q: 0.508308, mean_eps: 0.000000
 1881/5000: episode: 74, duration: 0.516s, episode steps:  39, steps per second:  76, episode reward: 32.167, mean reward:  0.825 [-2.904, 32.220], mean action: 3.615 [0.000, 16.000],  loss: 0.019155, mae: 0.345344, mean_q: 0.531526, mean_eps: 0.000000
 1907/5000: episode: 75, duration: 0.347s, episode steps:  26, steps per second:  75, episode reward: 35.553, mean reward:  1.367 [-3.000, 32.667], mean action: 4.385 [0.000, 16.000],  loss: 0.019704, mae: 0.352139, mean_q: 0.506713, mean_eps: 0.000000
 1942/5000: episode: 76, duration: 0.453s, episode steps:  35, steps per second:  77, episode reward: 32.517, mean reward:  0.929 [-3.000, 32.008], mean action: 7.771 [0.000, 20.000],  loss: 0.018922, mae: 0.343760, mean_q: 0.548408, mean_eps: 0.000000
 1959/5000: episode: 77, duration: 0.233s, episode steps:  17, steps per second:  73, episode reward: 38.228, mean reward:  2.249 [-2.798, 32.557], mean action: 4.412 [0.000, 15.000],  loss: 0.021425, mae: 0.360765, mean_q: 0.477495, mean_eps: 0.000000
 1980/5000: episode: 78, duration: 0.287s, episode steps:  21, steps per second:  73, episode reward: -35.820, mean reward: -1.706 [-32.386,  2.903], mean action: 3.619 [0.000, 9.000],  loss: 0.020178, mae: 0.358457, mean_q: 0.487755, mean_eps: 0.000000
 1992/5000: episode: 79, duration: 0.171s, episode steps:  12, steps per second:  70, episode reward: 41.099, mean reward:  3.425 [-3.000, 32.171], mean action: 4.167 [0.000, 20.000],  loss: 0.019065, mae: 0.354681, mean_q: 0.457817, mean_eps: 0.000000
 2013/5000: episode: 80, duration: 0.285s, episode steps:  21, steps per second:  74, episode reward: 35.204, mean reward:  1.676 [-2.484, 31.898], mean action: 3.429 [0.000, 14.000],  loss: 0.013669, mae: 0.331474, mean_q: 0.491456, mean_eps: 0.000000
 2029/5000: episode: 81, duration: 0.225s, episode steps:  16, steps per second:  71, episode reward: 41.494, mean reward:  2.593 [-2.286, 32.450], mean action: 3.750 [0.000, 14.000],  loss: 0.021507, mae: 0.370696, mean_q: 0.541381, mean_eps: 0.000000
 2065/5000: episode: 82, duration: 0.516s, episode steps:  36, steps per second:  70, episode reward: 32.684, mean reward:  0.908 [-2.341, 32.560], mean action: 4.944 [0.000, 19.000],  loss: 0.020436, mae: 0.367428, mean_q: 0.460681, mean_eps: 0.000000
 2084/5000: episode: 83, duration: 0.287s, episode steps:  19, steps per second:  66, episode reward: 41.004, mean reward:  2.158 [-2.285, 32.300], mean action: 2.158 [0.000, 19.000],  loss: 0.019656, mae: 0.362929, mean_q: 0.447979, mean_eps: 0.000000
 2102/5000: episode: 84, duration: 0.428s, episode steps:  18, steps per second:  42, episode reward: 41.844, mean reward:  2.325 [-2.199, 32.844], mean action: 3.056 [0.000, 16.000],  loss: 0.016865, mae: 0.341674, mean_q: 0.473694, mean_eps: 0.000000
 2134/5000: episode: 85, duration: 0.659s, episode steps:  32, steps per second:  49, episode reward: 34.903, mean reward:  1.091 [-3.000, 33.000], mean action: 6.500 [0.000, 21.000],  loss: 0.020474, mae: 0.354404, mean_q: 0.503011, mean_eps: 0.000000
 2158/5000: episode: 86, duration: 0.491s, episode steps:  24, steps per second:  49, episode reward: 32.399, mean reward:  1.350 [-2.465, 31.928], mean action: 7.333 [0.000, 19.000],  loss: 0.018526, mae: 0.338851, mean_q: 0.529578, mean_eps: 0.000000
 2189/5000: episode: 87, duration: 0.776s, episode steps:  31, steps per second:  40, episode reward: -32.330, mean reward: -1.043 [-32.089,  2.615], mean action: 5.516 [0.000, 19.000],  loss: 0.022007, mae: 0.365801, mean_q: 0.548667, mean_eps: 0.000000
 2212/5000: episode: 88, duration: 0.434s, episode steps:  23, steps per second:  53, episode reward: 33.000, mean reward:  1.435 [-3.000, 32.160], mean action: 4.565 [0.000, 13.000],  loss: 0.020182, mae: 0.349370, mean_q: 0.505188, mean_eps: 0.000000
 2256/5000: episode: 89, duration: 0.619s, episode steps:  44, steps per second:  71, episode reward: -32.860, mean reward: -0.747 [-32.081,  2.281], mean action: 5.955 [0.000, 20.000],  loss: 0.019984, mae: 0.347067, mean_q: 0.550276, mean_eps: 0.000000
 2277/5000: episode: 90, duration: 0.291s, episode steps:  21, steps per second:  72, episode reward: 38.185, mean reward:  1.818 [-2.620, 32.120], mean action: 4.143 [0.000, 19.000],  loss: 0.021400, mae: 0.348852, mean_q: 0.504836, mean_eps: 0.000000
 2298/5000: episode: 91, duration: 0.317s, episode steps:  21, steps per second:  66, episode reward: -35.590, mean reward: -1.695 [-32.057,  3.000], mean action: 5.381 [0.000, 19.000],  loss: 0.019894, mae: 0.345080, mean_q: 0.510046, mean_eps: 0.000000
 2323/5000: episode: 92, duration: 0.341s, episode steps:  25, steps per second:  73, episode reward: 38.871, mean reward:  1.555 [-2.750, 32.781], mean action: 3.320 [0.000, 15.000],  loss: 0.021115, mae: 0.355592, mean_q: 0.522520, mean_eps: 0.000000
 2344/5000: episode: 93, duration: 0.317s, episode steps:  21, steps per second:  66, episode reward: 40.888, mean reward:  1.947 [-2.073, 32.210], mean action: 4.238 [1.000, 20.000],  loss: 0.017854, mae: 0.339934, mean_q: 0.565792, mean_eps: 0.000000
 2364/5000: episode: 94, duration: 0.277s, episode steps:  20, steps per second:  72, episode reward: 41.397, mean reward:  2.070 [-2.205, 31.955], mean action: 3.050 [0.000, 14.000],  loss: 0.014982, mae: 0.322275, mean_q: 0.486759, mean_eps: 0.000000
 2381/5000: episode: 95, duration: 0.237s, episode steps:  17, steps per second:  72, episode reward: 36.000, mean reward:  2.118 [-3.000, 32.810], mean action: 5.765 [0.000, 18.000],  loss: 0.017482, mae: 0.337371, mean_q: 0.469502, mean_eps: 0.000000
 2400/5000: episode: 96, duration: 0.260s, episode steps:  19, steps per second:  73, episode reward: 36.000, mean reward:  1.895 [-3.000, 32.220], mean action: 2.895 [0.000, 9.000],  loss: 0.015942, mae: 0.328231, mean_q: 0.447990, mean_eps: 0.000000
 2484/5000: episode: 97, duration: 1.122s, episode steps:  84, steps per second:  75, episode reward: 32.762, mean reward:  0.390 [-2.900, 32.246], mean action: 8.810 [0.000, 15.000],  loss: 0.019922, mae: 0.351557, mean_q: 0.532787, mean_eps: 0.000000
 2506/5000: episode: 98, duration: 0.315s, episode steps:  22, steps per second:  70, episode reward: 36.000, mean reward:  1.636 [-2.498, 32.810], mean action: 4.727 [0.000, 19.000],  loss: 0.021228, mae: 0.356981, mean_q: 0.576262, mean_eps: 0.000000
 2531/5000: episode: 99, duration: 0.360s, episode steps:  25, steps per second:  69, episode reward: -42.000, mean reward: -1.680 [-32.163,  2.019], mean action: 6.720 [0.000, 15.000],  loss: 0.019985, mae: 0.348230, mean_q: 0.570031, mean_eps: 0.000000
 2553/5000: episode: 100, duration: 0.299s, episode steps:  22, steps per second:  74, episode reward: 35.093, mean reward:  1.595 [-3.000, 32.026], mean action: 5.227 [0.000, 19.000],  loss: 0.023274, mae: 0.368477, mean_q: 0.585962, mean_eps: 0.000000
 2573/5000: episode: 101, duration: 0.269s, episode steps:  20, steps per second:  74, episode reward: 38.069, mean reward:  1.903 [-2.658, 32.012], mean action: 4.300 [0.000, 12.000],  loss: 0.016812, mae: 0.337426, mean_q: 0.556472, mean_eps: 0.000000
 2593/5000: episode: 102, duration: 0.276s, episode steps:  20, steps per second:  72, episode reward: 34.949, mean reward:  1.747 [-2.505, 32.492], mean action: 5.650 [0.000, 13.000],  loss: 0.019498, mae: 0.349745, mean_q: 0.559687, mean_eps: 0.000000
 2615/5000: episode: 103, duration: 0.301s, episode steps:  22, steps per second:  73, episode reward: 32.648, mean reward:  1.484 [-3.000, 32.891], mean action: 4.045 [1.000, 12.000],  loss: 0.021460, mae: 0.360980, mean_q: 0.527330, mean_eps: 0.000000
 2636/5000: episode: 104, duration: 0.299s, episode steps:  21, steps per second:  70, episode reward: 36.000, mean reward:  1.714 [-3.000, 32.280], mean action: 3.476 [0.000, 12.000],  loss: 0.023253, mae: 0.371596, mean_q: 0.564217, mean_eps: 0.000000
 2658/5000: episode: 105, duration: 0.294s, episode steps:  22, steps per second:  75, episode reward: -35.320, mean reward: -1.605 [-32.104,  2.370], mean action: 3.545 [0.000, 15.000],  loss: 0.021703, mae: 0.362011, mean_q: 0.533276, mean_eps: 0.000000
 2684/5000: episode: 106, duration: 0.458s, episode steps:  26, steps per second:  57, episode reward: -33.000, mean reward: -1.269 [-32.496,  2.450], mean action: 4.231 [0.000, 15.000],  loss: 0.019719, mae: 0.358353, mean_q: 0.516097, mean_eps: 0.000000
 2696/5000: episode: 107, duration: 0.229s, episode steps:  12, steps per second:  52, episode reward: 41.253, mean reward:  3.438 [-2.619, 32.110], mean action: 4.000 [0.000, 19.000],  loss: 0.013824, mae: 0.322967, mean_q: 0.518393, mean_eps: 0.000000
 2722/5000: episode: 108, duration: 0.391s, episode steps:  26, steps per second:  66, episode reward: 35.796, mean reward:  1.377 [-2.675, 32.028], mean action: 5.308 [0.000, 19.000],  loss: 0.023849, mae: 0.368066, mean_q: 0.574649, mean_eps: 0.000000
 2751/5000: episode: 109, duration: 0.379s, episode steps:  29, steps per second:  77, episode reward: 32.642, mean reward:  1.126 [-2.903, 33.000], mean action: 6.897 [0.000, 20.000],  loss: 0.020289, mae: 0.351671, mean_q: 0.544360, mean_eps: 0.000000
 2776/5000: episode: 110, duration: 0.368s, episode steps:  25, steps per second:  68, episode reward: 34.649, mean reward:  1.386 [-2.256, 32.200], mean action: 3.880 [0.000, 19.000],  loss: 0.019145, mae: 0.346371, mean_q: 0.522452, mean_eps: 0.000000
 2798/5000: episode: 111, duration: 0.354s, episode steps:  22, steps per second:  62, episode reward: 38.143, mean reward:  1.734 [-2.346, 32.110], mean action: 3.818 [2.000, 11.000],  loss: 0.019871, mae: 0.342916, mean_q: 0.560484, mean_eps: 0.000000
 2821/5000: episode: 112, duration: 0.305s, episode steps:  23, steps per second:  75, episode reward: 38.900, mean reward:  1.691 [-2.375, 31.970], mean action: 3.130 [1.000, 11.000],  loss: 0.018124, mae: 0.341405, mean_q: 0.487634, mean_eps: 0.000000
 2842/5000: episode: 113, duration: 0.297s, episode steps:  21, steps per second:  71, episode reward: 44.134, mean reward:  2.102 [-2.376, 32.010], mean action: 1.333 [0.000, 13.000],  loss: 0.021686, mae: 0.361948, mean_q: 0.495690, mean_eps: 0.000000
 2863/5000: episode: 114, duration: 0.285s, episode steps:  21, steps per second:  74, episode reward: 38.556, mean reward:  1.836 [-3.000, 32.670], mean action: 3.667 [0.000, 13.000],  loss: 0.015538, mae: 0.338923, mean_q: 0.481330, mean_eps: 0.000000
 2896/5000: episode: 115, duration: 0.431s, episode steps:  33, steps per second:  77, episode reward: 39.000, mean reward:  1.182 [-2.464, 32.490], mean action: 3.061 [0.000, 19.000],  loss: 0.018335, mae: 0.347189, mean_q: 0.498931, mean_eps: 0.000000
 2925/5000: episode: 116, duration: 0.382s, episode steps:  29, steps per second:  76, episode reward: 35.828, mean reward:  1.235 [-2.364, 33.000], mean action: 7.448 [0.000, 20.000],  loss: 0.017571, mae: 0.337826, mean_q: 0.488056, mean_eps: 0.000000
 2975/5000: episode: 117, duration: 0.648s, episode steps:  50, steps per second:  77, episode reward: -32.640, mean reward: -0.653 [-32.285,  3.000], mean action: 3.220 [0.000, 16.000],  loss: 0.020272, mae: 0.350945, mean_q: 0.528379, mean_eps: 0.000000
 3008/5000: episode: 118, duration: 0.833s, episode steps:  33, steps per second:  40, episode reward: 35.087, mean reward:  1.063 [-2.279, 31.283], mean action: 5.182 [0.000, 21.000],  loss: 0.018101, mae: 0.345728, mean_q: 0.554574, mean_eps: 0.000000
 3028/5000: episode: 119, duration: 0.324s, episode steps:  20, steps per second:  62, episode reward: 38.769, mean reward:  1.938 [-2.300, 32.769], mean action: 4.200 [0.000, 16.000],  loss: 0.023278, mae: 0.373754, mean_q: 0.549279, mean_eps: 0.000000
 3050/5000: episode: 120, duration: 0.378s, episode steps:  22, steps per second:  58, episode reward: 30.000, mean reward:  1.364 [-2.901, 30.845], mean action: 4.364 [0.000, 19.000],  loss: 0.022900, mae: 0.367289, mean_q: 0.536574, mean_eps: 0.000000
 3084/5000: episode: 121, duration: 0.440s, episode steps:  34, steps per second:  77, episode reward: -38.440, mean reward: -1.131 [-31.870,  2.250], mean action: 3.765 [0.000, 19.000],  loss: 0.022967, mae: 0.371443, mean_q: 0.462411, mean_eps: 0.000000
 3103/5000: episode: 122, duration: 0.255s, episode steps:  19, steps per second:  74, episode reward: 40.547, mean reward:  2.134 [-2.288, 32.043], mean action: 5.263 [1.000, 16.000],  loss: 0.019460, mae: 0.354753, mean_q: 0.498953, mean_eps: 0.000000
 3123/5000: episode: 123, duration: 0.275s, episode steps:  20, steps per second:  73, episode reward: 35.070, mean reward:  1.754 [-2.258, 32.007], mean action: 7.400 [1.000, 17.000],  loss: 0.018650, mae: 0.347453, mean_q: 0.535122, mean_eps: 0.000000
 3155/5000: episode: 124, duration: 0.418s, episode steps:  32, steps per second:  77, episode reward: -32.200, mean reward: -1.006 [-32.800,  2.490], mean action: 4.938 [0.000, 16.000],  loss: 0.018795, mae: 0.352808, mean_q: 0.471663, mean_eps: 0.000000
 3181/5000: episode: 125, duration: 0.346s, episode steps:  26, steps per second:  75, episode reward: 38.202, mean reward:  1.469 [-2.775, 31.858], mean action: 4.615 [0.000, 16.000],  loss: 0.022393, mae: 0.365915, mean_q: 0.524467, mean_eps: 0.000000
 3208/5000: episode: 126, duration: 0.563s, episode steps:  27, steps per second:  48, episode reward: -33.000, mean reward: -1.222 [-33.000,  2.520], mean action: 4.852 [0.000, 17.000],  loss: 0.021152, mae: 0.359544, mean_q: 0.542418, mean_eps: 0.000000
 3225/5000: episode: 127, duration: 0.298s, episode steps:  17, steps per second:  57, episode reward: 38.903, mean reward:  2.288 [-2.336, 32.403], mean action: 4.471 [0.000, 16.000],  loss: 0.016825, mae: 0.339539, mean_q: 0.551389, mean_eps: 0.000000
 3246/5000: episode: 128, duration: 0.286s, episode steps:  21, steps per second:  73, episode reward: 44.662, mean reward:  2.127 [-2.119, 32.420], mean action: 3.429 [0.000, 9.000],  loss: 0.023844, mae: 0.370951, mean_q: 0.533143, mean_eps: 0.000000
 3268/5000: episode: 129, duration: 0.319s, episode steps:  22, steps per second:  69, episode reward: 32.136, mean reward:  1.461 [-2.610, 31.576], mean action: 6.000 [0.000, 20.000],  loss: 0.024431, mae: 0.374640, mean_q: 0.531348, mean_eps: 0.000000
 3282/5000: episode: 130, duration: 0.199s, episode steps:  14, steps per second:  70, episode reward: 36.000, mean reward:  2.571 [-2.545, 29.160], mean action: 4.214 [1.000, 12.000],  loss: 0.023280, mae: 0.375227, mean_q: 0.502795, mean_eps: 0.000000
 3297/5000: episode: 131, duration: 0.213s, episode steps:  15, steps per second:  71, episode reward: 41.366, mean reward:  2.758 [-2.252, 32.599], mean action: 2.867 [0.000, 12.000],  loss: 0.019893, mae: 0.360473, mean_q: 0.469453, mean_eps: 0.000000
 3310/5000: episode: 132, duration: 0.184s, episode steps:  13, steps per second:  71, episode reward: 41.078, mean reward:  3.160 [-2.709, 32.143], mean action: 2.923 [0.000, 12.000],  loss: 0.021949, mae: 0.365165, mean_q: 0.481629, mean_eps: 0.000000
 3340/5000: episode: 133, duration: 0.397s, episode steps:  30, steps per second:  76, episode reward: -32.430, mean reward: -1.081 [-32.207,  2.460], mean action: 7.567 [0.000, 17.000],  loss: 0.020146, mae: 0.356916, mean_q: 0.510886, mean_eps: 0.000000
 3363/5000: episode: 134, duration: 0.309s, episode steps:  23, steps per second:  74, episode reward: 32.532, mean reward:  1.414 [-3.000, 32.170], mean action: 5.739 [0.000, 15.000],  loss: 0.019692, mae: 0.356973, mean_q: 0.543426, mean_eps: 0.000000
 3376/5000: episode: 135, duration: 0.183s, episode steps:  13, steps per second:  71, episode reward: 41.743, mean reward:  3.211 [-2.163, 32.743], mean action: 2.846 [0.000, 15.000],  loss: 0.024696, mae: 0.374875, mean_q: 0.490987, mean_eps: 0.000000
 3397/5000: episode: 136, duration: 0.283s, episode steps:  21, steps per second:  74, episode reward: 32.627, mean reward:  1.554 [-3.000, 32.627], mean action: 4.857 [0.000, 16.000],  loss: 0.021179, mae: 0.363541, mean_q: 0.513157, mean_eps: 0.000000
 3417/5000: episode: 137, duration: 0.274s, episode steps:  20, steps per second:  73, episode reward: 38.198, mean reward:  1.910 [-2.805, 32.360], mean action: 3.600 [0.000, 16.000],  loss: 0.021431, mae: 0.359975, mean_q: 0.553009, mean_eps: 0.000000
 3436/5000: episode: 138, duration: 0.258s, episode steps:  19, steps per second:  74, episode reward: 35.965, mean reward:  1.893 [-3.000, 32.105], mean action: 6.211 [0.000, 16.000],  loss: 0.019007, mae: 0.344939, mean_q: 0.538770, mean_eps: 0.000000
 3456/5000: episode: 139, duration: 0.269s, episode steps:  20, steps per second:  74, episode reward: -35.090, mean reward: -1.755 [-32.910,  3.000], mean action: 5.250 [0.000, 16.000],  loss: 0.020519, mae: 0.347907, mean_q: 0.524663, mean_eps: 0.000000
 3472/5000: episode: 140, duration: 0.224s, episode steps:  16, steps per second:  72, episode reward: 35.880, mean reward:  2.242 [-3.000, 33.000], mean action: 3.438 [0.000, 16.000],  loss: 0.021610, mae: 0.355310, mean_q: 0.507900, mean_eps: 0.000000
 3497/5000: episode: 141, duration: 0.356s, episode steps:  25, steps per second:  70, episode reward: 35.363, mean reward:  1.415 [-2.792, 32.440], mean action: 5.120 [0.000, 14.000],  loss: 0.019339, mae: 0.340754, mean_q: 0.464492, mean_eps: 0.000000
 3520/5000: episode: 142, duration: 0.305s, episode steps:  23, steps per second:  75, episode reward: 35.462, mean reward:  1.542 [-3.000, 32.903], mean action: 5.261 [0.000, 21.000],  loss: 0.022431, mae: 0.358171, mean_q: 0.506677, mean_eps: 0.000000
 3543/5000: episode: 143, duration: 0.308s, episode steps:  23, steps per second:  75, episode reward: 41.239, mean reward:  1.793 [-3.000, 32.050], mean action: 3.391 [0.000, 20.000],  loss: 0.018554, mae: 0.344337, mean_q: 0.447278, mean_eps: 0.000000
 3568/5000: episode: 144, duration: 0.327s, episode steps:  25, steps per second:  76, episode reward: 34.908, mean reward:  1.396 [-3.000, 32.910], mean action: 7.480 [0.000, 20.000],  loss: 0.020026, mae: 0.349829, mean_q: 0.479772, mean_eps: 0.000000
 3597/5000: episode: 145, duration: 0.385s, episode steps:  29, steps per second:  75, episode reward: -32.330, mean reward: -1.115 [-31.799,  2.387], mean action: 4.310 [0.000, 12.000],  loss: 0.018425, mae: 0.347321, mean_q: 0.502328, mean_eps: 0.000000
 3618/5000: episode: 146, duration: 0.283s, episode steps:  21, steps per second:  74, episode reward: -32.360, mean reward: -1.541 [-31.913,  2.390], mean action: 4.476 [0.000, 16.000],  loss: 0.020287, mae: 0.355788, mean_q: 0.536613, mean_eps: 0.000000
 3654/5000: episode: 147, duration: 0.517s, episode steps:  36, steps per second:  70, episode reward: 35.434, mean reward:  0.984 [-3.000, 31.980], mean action: 8.028 [0.000, 21.000],  loss: 0.020058, mae: 0.347319, mean_q: 0.553899, mean_eps: 0.000000
 3684/5000: episode: 148, duration: 0.414s, episode steps:  30, steps per second:  72, episode reward: 43.462, mean reward:  1.449 [-2.103, 32.074], mean action: 3.300 [0.000, 11.000],  loss: 0.017727, mae: 0.338379, mean_q: 0.542378, mean_eps: 0.000000
 3756/5000: episode: 149, duration: 1.038s, episode steps:  72, steps per second:  69, episode reward: 37.447, mean reward:  0.520 [-2.412, 32.848], mean action: 3.333 [0.000, 17.000],  loss: 0.020398, mae: 0.365851, mean_q: 0.487187, mean_eps: 0.000000
 3771/5000: episode: 150, duration: 0.212s, episode steps:  15, steps per second:  71, episode reward: 38.584, mean reward:  2.572 [-3.000, 32.250], mean action: 4.467 [0.000, 12.000],  loss: 0.018937, mae: 0.357575, mean_q: 0.500824, mean_eps: 0.000000
 3804/5000: episode: 151, duration: 0.446s, episode steps:  33, steps per second:  74, episode reward: 35.463, mean reward:  1.075 [-3.000, 31.623], mean action: 5.061 [0.000, 19.000],  loss: 0.018693, mae: 0.348260, mean_q: 0.474272, mean_eps: 0.000000
 3828/5000: episode: 152, duration: 0.439s, episode steps:  24, steps per second:  55, episode reward: 33.000, mean reward:  1.375 [-2.611, 32.180], mean action: 5.500 [0.000, 19.000],  loss: 0.018773, mae: 0.346123, mean_q: 0.457460, mean_eps: 0.000000
 3853/5000: episode: 153, duration: 0.350s, episode steps:  25, steps per second:  71, episode reward: 32.568, mean reward:  1.303 [-2.444, 31.778], mean action: 7.120 [0.000, 19.000],  loss: 0.021183, mae: 0.363527, mean_q: 0.492600, mean_eps: 0.000000
 3869/5000: episode: 154, duration: 0.234s, episode steps:  16, steps per second:  68, episode reward: 41.906, mean reward:  2.619 [-2.558, 32.190], mean action: 6.812 [0.000, 19.000],  loss: 0.018357, mae: 0.347484, mean_q: 0.463784, mean_eps: 0.000000
 3896/5000: episode: 155, duration: 0.373s, episode steps:  27, steps per second:  72, episode reward: 33.000, mean reward:  1.222 [-3.000, 32.630], mean action: 3.741 [0.000, 19.000],  loss: 0.022472, mae: 0.367895, mean_q: 0.475080, mean_eps: 0.000000
 3921/5000: episode: 156, duration: 0.329s, episode steps:  25, steps per second:  76, episode reward: -35.630, mean reward: -1.425 [-33.000,  2.460], mean action: 6.560 [0.000, 19.000],  loss: 0.018994, mae: 0.345204, mean_q: 0.480566, mean_eps: 0.000000
 3941/5000: episode: 157, duration: 0.279s, episode steps:  20, steps per second:  72, episode reward: 35.075, mean reward:  1.754 [-2.369, 32.518], mean action: 3.800 [0.000, 19.000],  loss: 0.020969, mae: 0.352288, mean_q: 0.517208, mean_eps: 0.000000
 3971/5000: episode: 158, duration: 0.406s, episode steps:  30, steps per second:  74, episode reward: 35.126, mean reward:  1.171 [-2.392, 32.170], mean action: 4.233 [0.000, 13.000],  loss: 0.020527, mae: 0.348596, mean_q: 0.536361, mean_eps: 0.000000
 3992/5000: episode: 159, duration: 0.356s, episode steps:  21, steps per second:  59, episode reward: 37.826, mean reward:  1.801 [-2.197, 32.330], mean action: 5.000 [0.000, 15.000],  loss: 0.021650, mae: 0.355044, mean_q: 0.562372, mean_eps: 0.000000
 4039/5000: episode: 160, duration: 0.612s, episode steps:  47, steps per second:  77, episode reward: -32.340, mean reward: -0.688 [-32.357,  2.370], mean action: 7.532 [0.000, 13.000],  loss: 0.019314, mae: 0.348829, mean_q: 0.506347, mean_eps: 0.000000
 4064/5000: episode: 161, duration: 0.341s, episode steps:  25, steps per second:  73, episode reward: 33.000, mean reward:  1.320 [-2.900, 32.190], mean action: 3.760 [0.000, 12.000],  loss: 0.022604, mae: 0.366297, mean_q: 0.506823, mean_eps: 0.000000
 4083/5000: episode: 162, duration: 0.293s, episode steps:  19, steps per second:  65, episode reward: 44.702, mean reward:  2.353 [-2.124, 32.270], mean action: 2.947 [0.000, 12.000],  loss: 0.018588, mae: 0.350350, mean_q: 0.486732, mean_eps: 0.000000
 4108/5000: episode: 163, duration: 0.337s, episode steps:  25, steps per second:  74, episode reward: 32.669, mean reward:  1.307 [-2.588, 32.430], mean action: 6.360 [0.000, 16.000],  loss: 0.018439, mae: 0.349512, mean_q: 0.536856, mean_eps: 0.000000
 4134/5000: episode: 164, duration: 0.350s, episode steps:  26, steps per second:  74, episode reward: 32.883, mean reward:  1.265 [-2.829, 32.050], mean action: 5.885 [0.000, 15.000],  loss: 0.022705, mae: 0.364595, mean_q: 0.526627, mean_eps: 0.000000
 4161/5000: episode: 165, duration: 0.359s, episode steps:  27, steps per second:  75, episode reward: 33.000, mean reward:  1.222 [-2.765, 33.000], mean action: 5.259 [0.000, 15.000],  loss: 0.018164, mae: 0.343484, mean_q: 0.548315, mean_eps: 0.000000
 4181/5000: episode: 166, duration: 0.277s, episode steps:  20, steps per second:  72, episode reward: 38.181, mean reward:  1.909 [-3.000, 32.235], mean action: 3.300 [0.000, 15.000],  loss: 0.017122, mae: 0.337867, mean_q: 0.527283, mean_eps: 0.000000
 4199/5000: episode: 167, duration: 0.257s, episode steps:  18, steps per second:  70, episode reward: 38.444, mean reward:  2.136 [-2.379, 32.184], mean action: 4.056 [0.000, 15.000],  loss: 0.018501, mae: 0.349717, mean_q: 0.509795, mean_eps: 0.000000
 4214/5000: episode: 168, duration: 0.279s, episode steps:  15, steps per second:  54, episode reward: 41.469, mean reward:  2.765 [-2.470, 32.596], mean action: 2.200 [0.000, 12.000],  loss: 0.018779, mae: 0.350536, mean_q: 0.500653, mean_eps: 0.000000
 4227/5000: episode: 169, duration: 0.332s, episode steps:  13, steps per second:  39, episode reward: 39.000, mean reward:  3.000 [-2.333, 30.404], mean action: 3.077 [0.000, 11.000],  loss: 0.019893, mae: 0.358890, mean_q: 0.515566, mean_eps: 0.000000
 4251/5000: episode: 170, duration: 0.358s, episode steps:  24, steps per second:  67, episode reward: 38.072, mean reward:  1.586 [-2.445, 31.890], mean action: 2.250 [0.000, 11.000],  loss: 0.021085, mae: 0.362631, mean_q: 0.472372, mean_eps: 0.000000
 4272/5000: episode: 171, duration: 0.304s, episode steps:  21, steps per second:  69, episode reward: -35.550, mean reward: -1.693 [-32.640,  2.460], mean action: 6.667 [0.000, 15.000],  loss: 0.019724, mae: 0.353384, mean_q: 0.517561, mean_eps: 0.000000
 4286/5000: episode: 172, duration: 0.204s, episode steps:  14, steps per second:  69, episode reward: 44.163, mean reward:  3.154 [-2.146, 32.440], mean action: 3.000 [1.000, 15.000],  loss: 0.018637, mae: 0.346789, mean_q: 0.542692, mean_eps: 0.000000
 4303/5000: episode: 173, duration: 0.237s, episode steps:  17, steps per second:  72, episode reward: -33.000, mean reward: -1.941 [-30.120,  2.939], mean action: 4.471 [0.000, 11.000],  loss: 0.027552, mae: 0.390587, mean_q: 0.551598, mean_eps: 0.000000
 4330/5000: episode: 174, duration: 0.392s, episode steps:  27, steps per second:  69, episode reward: 32.527, mean reward:  1.205 [-3.000, 32.865], mean action: 5.926 [0.000, 20.000],  loss: 0.022689, mae: 0.372256, mean_q: 0.570415, mean_eps: 0.000000
 4348/5000: episode: 175, duration: 0.258s, episode steps:  18, steps per second:  70, episode reward: 35.046, mean reward:  1.947 [-3.000, 31.884], mean action: 3.556 [0.000, 11.000],  loss: 0.017081, mae: 0.343239, mean_q: 0.571647, mean_eps: 0.000000
 4416/5000: episode: 176, duration: 0.895s, episode steps:  68, steps per second:  76, episode reward: -32.500, mean reward: -0.478 [-32.370,  3.000], mean action: 3.500 [0.000, 12.000],  loss: 0.021885, mae: 0.362579, mean_q: 0.594102, mean_eps: 0.000000
 4437/5000: episode: 177, duration: 0.414s, episode steps:  21, steps per second:  51, episode reward: 34.711, mean reward:  1.653 [-3.000, 31.566], mean action: 4.095 [0.000, 16.000],  loss: 0.018990, mae: 0.351727, mean_q: 0.560187, mean_eps: 0.000000
 4457/5000: episode: 178, duration: 0.379s, episode steps:  20, steps per second:  53, episode reward: 38.014, mean reward:  1.901 [-2.549, 32.290], mean action: 5.000 [0.000, 16.000],  loss: 0.025577, mae: 0.388624, mean_q: 0.504351, mean_eps: 0.000000
 4477/5000: episode: 179, duration: 0.273s, episode steps:  20, steps per second:  73, episode reward: 35.792, mean reward:  1.790 [-2.784, 32.150], mean action: 4.300 [0.000, 15.000],  loss: 0.016719, mae: 0.349610, mean_q: 0.484982, mean_eps: 0.000000
 4498/5000: episode: 180, duration: 0.291s, episode steps:  21, steps per second:  72, episode reward: 32.901, mean reward:  1.567 [-3.000, 32.811], mean action: 5.952 [1.000, 15.000],  loss: 0.023734, mae: 0.383013, mean_q: 0.524856, mean_eps: 0.000000
 4522/5000: episode: 181, duration: 0.446s, episode steps:  24, steps per second:  54, episode reward: 32.962, mean reward:  1.373 [-2.998, 29.822], mean action: 5.833 [0.000, 19.000],  loss: 0.020517, mae: 0.372749, mean_q: 0.519564, mean_eps: 0.000000
 4549/5000: episode: 182, duration: 0.366s, episode steps:  27, steps per second:  74, episode reward: 32.247, mean reward:  1.194 [-3.000, 31.697], mean action: 5.222 [0.000, 16.000],  loss: 0.021438, mae: 0.368790, mean_q: 0.517298, mean_eps: 0.000000
 4566/5000: episode: 183, duration: 0.235s, episode steps:  17, steps per second:  72, episode reward: 38.902, mean reward:  2.288 [-2.744, 32.902], mean action: 4.706 [0.000, 16.000],  loss: 0.015578, mae: 0.341476, mean_q: 0.452925, mean_eps: 0.000000
 4607/5000: episode: 184, duration: 0.530s, episode steps:  41, steps per second:  77, episode reward: -35.330, mean reward: -0.862 [-32.127,  3.000], mean action: 4.537 [0.000, 21.000],  loss: 0.020072, mae: 0.362817, mean_q: 0.454410, mean_eps: 0.000000
 4634/5000: episode: 185, duration: 0.378s, episode steps:  27, steps per second:  71, episode reward: -32.800, mean reward: -1.215 [-31.820,  2.335], mean action: 4.667 [0.000, 16.000],  loss: 0.021239, mae: 0.371944, mean_q: 0.480829, mean_eps: 0.000000
 4659/5000: episode: 186, duration: 0.344s, episode steps:  25, steps per second:  73, episode reward: 32.418, mean reward:  1.297 [-2.939, 31.478], mean action: 4.920 [0.000, 19.000],  loss: 0.022411, mae: 0.370171, mean_q: 0.523604, mean_eps: 0.000000
 4679/5000: episode: 187, duration: 0.278s, episode steps:  20, steps per second:  72, episode reward: 38.628, mean reward:  1.931 [-2.684, 32.055], mean action: 5.450 [0.000, 19.000],  loss: 0.020465, mae: 0.366812, mean_q: 0.557208, mean_eps: 0.000000
 4708/5000: episode: 188, duration: 0.394s, episode steps:  29, steps per second:  74, episode reward: -32.110, mean reward: -1.107 [-32.110,  2.970], mean action: 5.793 [0.000, 19.000],  loss: 0.019781, mae: 0.362410, mean_q: 0.535105, mean_eps: 0.000000
 4730/5000: episode: 189, duration: 0.301s, episode steps:  22, steps per second:  73, episode reward: 35.025, mean reward:  1.592 [-2.515, 33.000], mean action: 5.500 [0.000, 14.000],  loss: 0.019885, mae: 0.366120, mean_q: 0.544089, mean_eps: 0.000000
 4757/5000: episode: 190, duration: 0.365s, episode steps:  27, steps per second:  74, episode reward: -32.790, mean reward: -1.214 [-31.917,  2.580], mean action: 6.000 [0.000, 21.000],  loss: 0.022637, mae: 0.384038, mean_q: 0.478660, mean_eps: 0.000000
 4768/5000: episode: 191, duration: 0.166s, episode steps:  11, steps per second:  66, episode reward: 39.000, mean reward:  3.545 [-2.639, 30.450], mean action: 3.455 [1.000, 12.000],  loss: 0.020316, mae: 0.371263, mean_q: 0.471184, mean_eps: 0.000000
 4788/5000: episode: 192, duration: 0.282s, episode steps:  20, steps per second:  71, episode reward: 41.351, mean reward:  2.068 [-2.268, 32.480], mean action: 3.350 [0.000, 14.000],  loss: 0.019307, mae: 0.361896, mean_q: 0.432372, mean_eps: 0.000000
 4814/5000: episode: 193, duration: 0.349s, episode steps:  26, steps per second:  74, episode reward: 41.849, mean reward:  1.610 [-2.399, 32.910], mean action: 5.654 [0.000, 13.000],  loss: 0.015689, mae: 0.339711, mean_q: 0.459541, mean_eps: 0.000000
 4840/5000: episode: 194, duration: 0.395s, episode steps:  26, steps per second:  66, episode reward: -30.000, mean reward: -1.154 [-30.085,  2.851], mean action: 6.231 [0.000, 19.000],  loss: 0.018430, mae: 0.353565, mean_q: 0.501219, mean_eps: 0.000000
 4865/5000: episode: 195, duration: 0.333s, episode steps:  25, steps per second:  75, episode reward: 32.444, mean reward:  1.298 [-3.000, 32.169], mean action: 6.960 [0.000, 19.000],  loss: 0.019901, mae: 0.358898, mean_q: 0.524875, mean_eps: 0.000000
 4891/5000: episode: 196, duration: 0.357s, episode steps:  26, steps per second:  73, episode reward: 33.000, mean reward:  1.269 [-2.480, 32.480], mean action: 3.346 [0.000, 12.000],  loss: 0.022974, mae: 0.373436, mean_q: 0.517252, mean_eps: 0.000000
 4919/5000: episode: 197, duration: 0.402s, episode steps:  28, steps per second:  70, episode reward: -30.000, mean reward: -1.071 [-30.049,  2.521], mean action: 6.607 [0.000, 19.000],  loss: 0.022090, mae: 0.358949, mean_q: 0.524861, mean_eps: 0.000000
 4936/5000: episode: 198, duration: 0.242s, episode steps:  17, steps per second:  70, episode reward: 37.972, mean reward:  2.234 [-2.452, 32.904], mean action: 4.471 [0.000, 19.000],  loss: 0.023999, mae: 0.376483, mean_q: 0.564246, mean_eps: 0.000000
 4958/5000: episode: 199, duration: 0.309s, episode steps:  22, steps per second:  71, episode reward: 38.903, mean reward:  1.768 [-2.191, 32.443], mean action: 3.227 [0.000, 16.000],  loss: 0.019095, mae: 0.356131, mean_q: 0.555790, mean_eps: 0.000000
 4988/5000: episode: 200, duration: 0.418s, episode steps:  30, steps per second:  72, episode reward: -32.690, mean reward: -1.090 [-32.175,  2.232], mean action: 2.533 [0.000, 11.000],  loss: 0.019697, mae: 0.351049, mean_q: 0.552040, mean_eps: 0.000000
done, took 66.774 seconds
DQN Evaluation: 5109 victories out of 6044 episodes
Training for 5000 steps ...
   42/5000: episode: 1, duration: 0.336s, episode steps:  42, steps per second: 125, episode reward: 41.691, mean reward:  0.993 [-2.446, 32.440], mean action: 3.000 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   79/5000: episode: 2, duration: 0.246s, episode steps:  37, steps per second: 151, episode reward: 38.220, mean reward:  1.033 [-2.290, 32.170], mean action: 2.541 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  105/5000: episode: 3, duration: 0.183s, episode steps:  26, steps per second: 142, episode reward: 38.569, mean reward:  1.483 [-2.314, 31.970], mean action: 4.154 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  140/5000: episode: 4, duration: 0.229s, episode steps:  35, steps per second: 153, episode reward: 34.491, mean reward:  0.985 [-2.279, 29.877], mean action: 5.771 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  162/5000: episode: 5, duration: 0.154s, episode steps:  22, steps per second: 143, episode reward: 38.611, mean reward:  1.755 [-2.741, 32.240], mean action: 3.955 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  182/5000: episode: 6, duration: 0.142s, episode steps:  20, steps per second: 141, episode reward: 44.106, mean reward:  2.205 [-3.000, 32.286], mean action: 3.400 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  199/5000: episode: 7, duration: 0.118s, episode steps:  17, steps per second: 145, episode reward: 44.608, mean reward:  2.624 [-2.674, 32.510], mean action: 3.235 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  215/5000: episode: 8, duration: 0.110s, episode steps:  16, steps per second: 145, episode reward: 41.581, mean reward:  2.599 [-2.809, 30.115], mean action: 1.938 [0.000, 8.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  241/5000: episode: 9, duration: 0.190s, episode steps:  26, steps per second: 137, episode reward: 44.345, mean reward:  1.706 [-2.191, 32.290], mean action: 2.154 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  262/5000: episode: 10, duration: 0.149s, episode steps:  21, steps per second: 141, episode reward: 44.543, mean reward:  2.121 [-2.272, 32.100], mean action: 2.190 [1.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  297/5000: episode: 11, duration: 0.261s, episode steps:  35, steps per second: 134, episode reward: 35.259, mean reward:  1.007 [-2.529, 32.243], mean action: 4.943 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  331/5000: episode: 12, duration: 0.212s, episode steps:  34, steps per second: 160, episode reward: 42.000, mean reward:  1.235 [-2.925, 32.010], mean action: 2.206 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  345/5000: episode: 13, duration: 0.098s, episode steps:  14, steps per second: 143, episode reward: 44.545, mean reward:  3.182 [-2.159, 32.050], mean action: 3.643 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  372/5000: episode: 14, duration: 0.172s, episode steps:  27, steps per second: 157, episode reward: 39.000, mean reward:  1.444 [-2.492, 29.671], mean action: 1.926 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  391/5000: episode: 15, duration: 0.140s, episode steps:  19, steps per second: 136, episode reward: 46.527, mean reward:  2.449 [-0.074, 32.211], mean action: 2.474 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  417/5000: episode: 16, duration: 0.171s, episode steps:  26, steps per second: 152, episode reward: 38.631, mean reward:  1.486 [-3.000, 31.841], mean action: 3.154 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  437/5000: episode: 17, duration: 0.132s, episode steps:  20, steps per second: 151, episode reward: 41.876, mean reward:  2.094 [-2.570, 32.245], mean action: 3.300 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  452/5000: episode: 18, duration: 0.110s, episode steps:  15, steps per second: 137, episode reward: 44.741, mean reward:  2.983 [-2.080, 31.861], mean action: 1.467 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  480/5000: episode: 19, duration: 0.188s, episode steps:  28, steps per second: 149, episode reward: 41.581, mean reward:  1.485 [-2.234, 32.290], mean action: 3.107 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  495/5000: episode: 20, duration: 0.105s, episode steps:  15, steps per second: 143, episode reward: 46.552, mean reward:  3.103 [-0.391, 32.334], mean action: 2.267 [0.000, 8.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  520/5000: episode: 21, duration: 0.161s, episode steps:  25, steps per second: 156, episode reward: 38.805, mean reward:  1.552 [-2.324, 32.040], mean action: 3.280 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  552/5000: episode: 22, duration: 0.200s, episode steps:  32, steps per second: 160, episode reward: 44.204, mean reward:  1.381 [-2.059, 31.806], mean action: 3.594 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  576/5000: episode: 23, duration: 0.153s, episode steps:  24, steps per second: 157, episode reward: 40.472, mean reward:  1.686 [-2.360, 32.340], mean action: 4.292 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  603/5000: episode: 24, duration: 0.171s, episode steps:  27, steps per second: 158, episode reward: 41.023, mean reward:  1.519 [-2.462, 31.423], mean action: 2.741 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  635/5000: episode: 25, duration: 0.200s, episode steps:  32, steps per second: 160, episode reward: 34.929, mean reward:  1.092 [-3.000, 32.347], mean action: 6.062 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  662/5000: episode: 26, duration: 0.171s, episode steps:  27, steps per second: 158, episode reward: 41.346, mean reward:  1.531 [-2.083, 31.969], mean action: 2.074 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  688/5000: episode: 27, duration: 0.170s, episode steps:  26, steps per second: 153, episode reward: 35.706, mean reward:  1.373 [-3.000, 32.110], mean action: 3.846 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  707/5000: episode: 28, duration: 0.130s, episode steps:  19, steps per second: 147, episode reward: 47.562, mean reward:  2.503 [-0.463, 32.080], mean action: 2.158 [1.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  777/5000: episode: 29, duration: 0.403s, episode steps:  70, steps per second: 174, episode reward: 46.488, mean reward:  0.664 [-0.570, 32.290], mean action: 1.171 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  800/5000: episode: 30, duration: 0.154s, episode steps:  23, steps per second: 149, episode reward: 35.438, mean reward:  1.541 [-2.665, 31.658], mean action: 5.522 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  834/5000: episode: 31, duration: 0.211s, episode steps:  34, steps per second: 161, episode reward: 38.800, mean reward:  1.141 [-2.506, 32.720], mean action: 2.088 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  861/5000: episode: 32, duration: 0.203s, episode steps:  27, steps per second: 133, episode reward: 36.000, mean reward:  1.333 [-2.580, 32.570], mean action: 2.852 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  888/5000: episode: 33, duration: 0.176s, episode steps:  27, steps per second: 154, episode reward: 38.752, mean reward:  1.435 [-2.876, 32.250], mean action: 5.074 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  913/5000: episode: 34, duration: 0.170s, episode steps:  25, steps per second: 147, episode reward: 41.816, mean reward:  1.673 [-2.312, 32.723], mean action: 2.200 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  943/5000: episode: 35, duration: 0.188s, episode steps:  30, steps per second: 160, episode reward: 40.517, mean reward:  1.351 [-2.789, 32.250], mean action: 2.767 [1.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  959/5000: episode: 36, duration: 0.107s, episode steps:  16, steps per second: 149, episode reward: 41.791, mean reward:  2.612 [-2.358, 32.310], mean action: 3.750 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  985/5000: episode: 37, duration: 0.177s, episode steps:  26, steps per second: 147, episode reward: 47.630, mean reward:  1.832 [-0.300, 32.300], mean action: 3.115 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1023/5000: episode: 38, duration: 0.404s, episode steps:  38, steps per second:  94, episode reward: 41.586, mean reward:  1.094 [-2.197, 32.370], mean action: 2.132 [1.000, 19.000],  loss: 0.016644, mae: 0.338878, mean_q: 0.553879, mean_eps: 0.000000
 1040/5000: episode: 39, duration: 0.241s, episode steps:  17, steps per second:  70, episode reward: 44.344, mean reward:  2.608 [-2.036, 32.200], mean action: 1.765 [0.000, 15.000],  loss: 0.023617, mae: 0.375207, mean_q: 0.550639, mean_eps: 0.000000
 1071/5000: episode: 40, duration: 0.409s, episode steps:  31, steps per second:  76, episode reward: 37.858, mean reward:  1.221 [-2.361, 32.240], mean action: 2.935 [0.000, 15.000],  loss: 0.018251, mae: 0.343967, mean_q: 0.531197, mean_eps: 0.000000
 1122/5000: episode: 41, duration: 0.670s, episode steps:  51, steps per second:  76, episode reward: 41.692, mean reward:  0.817 [-2.394, 32.270], mean action: 1.941 [0.000, 15.000],  loss: 0.018884, mae: 0.343540, mean_q: 0.516712, mean_eps: 0.000000
 1152/5000: episode: 42, duration: 0.401s, episode steps:  30, steps per second:  75, episode reward: 35.330, mean reward:  1.178 [-2.762, 32.323], mean action: 5.133 [0.000, 16.000],  loss: 0.017095, mae: 0.338283, mean_q: 0.509663, mean_eps: 0.000000
 1190/5000: episode: 43, duration: 0.505s, episode steps:  38, steps per second:  75, episode reward: -32.370, mean reward: -0.852 [-31.749,  3.000], mean action: 4.974 [0.000, 19.000],  loss: 0.019981, mae: 0.344445, mean_q: 0.514334, mean_eps: 0.000000
 1217/5000: episode: 44, duration: 0.360s, episode steps:  27, steps per second:  75, episode reward: 38.743, mean reward:  1.435 [-2.600, 32.060], mean action: 2.852 [0.000, 16.000],  loss: 0.022181, mae: 0.359180, mean_q: 0.464143, mean_eps: 0.000000
 1240/5000: episode: 45, duration: 0.309s, episode steps:  23, steps per second:  74, episode reward: 41.181, mean reward:  1.790 [-3.000, 32.720], mean action: 2.739 [0.000, 11.000],  loss: 0.024159, mae: 0.366050, mean_q: 0.529221, mean_eps: 0.000000
 1270/5000: episode: 46, duration: 0.556s, episode steps:  30, steps per second:  54, episode reward: 41.137, mean reward:  1.371 [-3.000, 32.044], mean action: 1.367 [0.000, 11.000],  loss: 0.016248, mae: 0.340544, mean_q: 0.575665, mean_eps: 0.000000
 1292/5000: episode: 47, duration: 0.358s, episode steps:  22, steps per second:  61, episode reward: 44.322, mean reward:  2.015 [-2.807, 32.250], mean action: 2.636 [0.000, 14.000],  loss: 0.021751, mae: 0.367383, mean_q: 0.585231, mean_eps: 0.000000
 1349/5000: episode: 48, duration: 0.749s, episode steps:  57, steps per second:  76, episode reward: -32.860, mean reward: -0.576 [-32.342,  2.880], mean action: 7.070 [0.000, 21.000],  loss: 0.021315, mae: 0.362078, mean_q: 0.550299, mean_eps: 0.000000
 1376/5000: episode: 49, duration: 0.708s, episode steps:  27, steps per second:  38, episode reward: 44.257, mean reward:  1.639 [-2.151, 32.220], mean action: 1.333 [0.000, 9.000],  loss: 0.022249, mae: 0.358506, mean_q: 0.540887, mean_eps: 0.000000
 1405/5000: episode: 50, duration: 0.724s, episode steps:  29, steps per second:  40, episode reward: 30.753, mean reward:  1.060 [-3.000, 31.903], mean action: 3.483 [0.000, 11.000],  loss: 0.019687, mae: 0.352132, mean_q: 0.535670, mean_eps: 0.000000
 1436/5000: episode: 51, duration: 0.585s, episode steps:  31, steps per second:  53, episode reward: 41.169, mean reward:  1.328 [-2.527, 32.357], mean action: 3.323 [1.000, 20.000],  loss: 0.022433, mae: 0.370498, mean_q: 0.516796, mean_eps: 0.000000
 1455/5000: episode: 52, duration: 0.285s, episode steps:  19, steps per second:  67, episode reward: 44.637, mean reward:  2.349 [-2.099, 32.270], mean action: 1.684 [0.000, 9.000],  loss: 0.021158, mae: 0.361900, mean_q: 0.542108, mean_eps: 0.000000
 1511/5000: episode: 53, duration: 0.718s, episode steps:  56, steps per second:  78, episode reward: -35.550, mean reward: -0.635 [-32.648,  2.510], mean action: 4.518 [0.000, 9.000],  loss: 0.018872, mae: 0.354186, mean_q: 0.545795, mean_eps: 0.000000
 1556/5000: episode: 54, duration: 0.597s, episode steps:  45, steps per second:  75, episode reward: 33.000, mean reward:  0.733 [-2.704, 32.150], mean action: 4.689 [0.000, 20.000],  loss: 0.022014, mae: 0.371831, mean_q: 0.472463, mean_eps: 0.000000
 1587/5000: episode: 55, duration: 0.416s, episode steps:  31, steps per second:  75, episode reward: 38.902, mean reward:  1.255 [-3.000, 32.562], mean action: 2.516 [0.000, 19.000],  loss: 0.020085, mae: 0.358324, mean_q: 0.504278, mean_eps: 0.000000
 1615/5000: episode: 56, duration: 0.377s, episode steps:  28, steps per second:  74, episode reward: 41.390, mean reward:  1.478 [-2.752, 31.750], mean action: 3.071 [0.000, 16.000],  loss: 0.022828, mae: 0.374944, mean_q: 0.507378, mean_eps: 0.000000
 1641/5000: episode: 57, duration: 0.355s, episode steps:  26, steps per second:  73, episode reward: 37.644, mean reward:  1.448 [-3.000, 32.120], mean action: 5.731 [0.000, 17.000],  loss: 0.021719, mae: 0.359167, mean_q: 0.522522, mean_eps: 0.000000
 1673/5000: episode: 58, duration: 0.420s, episode steps:  32, steps per second:  76, episode reward: 32.772, mean reward:  1.024 [-3.000, 32.180], mean action: 5.938 [0.000, 18.000],  loss: 0.022230, mae: 0.367447, mean_q: 0.568335, mean_eps: 0.000000
 1704/5000: episode: 59, duration: 0.419s, episode steps:  31, steps per second:  74, episode reward: 41.060, mean reward:  1.325 [-2.546, 31.999], mean action: 2.516 [0.000, 19.000],  loss: 0.020421, mae: 0.359232, mean_q: 0.560561, mean_eps: 0.000000
 1720/5000: episode: 60, duration: 0.223s, episode steps:  16, steps per second:  72, episode reward: 41.599, mean reward:  2.600 [-3.000, 32.450], mean action: 2.688 [0.000, 12.000],  loss: 0.025377, mae: 0.382679, mean_q: 0.575489, mean_eps: 0.000000
 1750/5000: episode: 61, duration: 0.555s, episode steps:  30, steps per second:  54, episode reward: 38.812, mean reward:  1.294 [-2.415, 32.052], mean action: 3.000 [0.000, 12.000],  loss: 0.021414, mae: 0.360885, mean_q: 0.587627, mean_eps: 0.000000
 1779/5000: episode: 62, duration: 0.524s, episode steps:  29, steps per second:  55, episode reward: 38.142, mean reward:  1.315 [-2.637, 32.300], mean action: 3.069 [0.000, 17.000],  loss: 0.019374, mae: 0.359134, mean_q: 0.530040, mean_eps: 0.000000
 1798/5000: episode: 63, duration: 0.446s, episode steps:  19, steps per second:  43, episode reward: 42.000, mean reward:  2.211 [-3.000, 32.380], mean action: 2.579 [0.000, 16.000],  loss: 0.020874, mae: 0.367031, mean_q: 0.517720, mean_eps: 0.000000
 1834/5000: episode: 64, duration: 0.651s, episode steps:  36, steps per second:  55, episode reward: 39.000, mean reward:  1.083 [-3.000, 32.320], mean action: 2.722 [0.000, 16.000],  loss: 0.017485, mae: 0.349114, mean_q: 0.529860, mean_eps: 0.000000
 1857/5000: episode: 65, duration: 0.342s, episode steps:  23, steps per second:  67, episode reward: 38.877, mean reward:  1.690 [-2.462, 32.340], mean action: 5.435 [2.000, 16.000],  loss: 0.020227, mae: 0.365408, mean_q: 0.510857, mean_eps: 0.000000
 1874/5000: episode: 66, duration: 0.301s, episode steps:  17, steps per second:  56, episode reward: 46.629, mean reward:  2.743 [-0.354, 32.140], mean action: 2.118 [0.000, 19.000],  loss: 0.024390, mae: 0.381762, mean_q: 0.584978, mean_eps: 0.000000
 1908/5000: episode: 67, duration: 0.538s, episode steps:  34, steps per second:  63, episode reward: 40.870, mean reward:  1.202 [-2.085, 32.240], mean action: 4.941 [0.000, 16.000],  loss: 0.024081, mae: 0.382693, mean_q: 0.529966, mean_eps: 0.000000
 1931/5000: episode: 68, duration: 0.371s, episode steps:  23, steps per second:  62, episode reward: 45.000, mean reward:  1.957 [-2.479, 32.360], mean action: 1.043 [0.000, 15.000],  loss: 0.019179, mae: 0.355009, mean_q: 0.522003, mean_eps: 0.000000
 1954/5000: episode: 69, duration: 0.408s, episode steps:  23, steps per second:  56, episode reward: 41.264, mean reward:  1.794 [-3.000, 31.835], mean action: 2.957 [0.000, 15.000],  loss: 0.019720, mae: 0.358850, mean_q: 0.519772, mean_eps: 0.000000
 1972/5000: episode: 70, duration: 0.260s, episode steps:  18, steps per second:  69, episode reward: 45.000, mean reward:  2.500 [-2.032, 33.000], mean action: 0.944 [0.000, 11.000],  loss: 0.020090, mae: 0.363031, mean_q: 0.531543, mean_eps: 0.000000
 1998/5000: episode: 71, duration: 0.361s, episode steps:  26, steps per second:  72, episode reward: 44.818, mean reward:  1.724 [-2.453, 31.968], mean action: 1.769 [0.000, 11.000],  loss: 0.020333, mae: 0.359407, mean_q: 0.567439, mean_eps: 0.000000
 2016/5000: episode: 72, duration: 0.255s, episode steps:  18, steps per second:  71, episode reward: 47.544, mean reward:  2.641 [-0.042, 32.220], mean action: 0.667 [0.000, 8.000],  loss: 0.021750, mae: 0.360937, mean_q: 0.589861, mean_eps: 0.000000
 2042/5000: episode: 73, duration: 0.351s, episode steps:  26, steps per second:  74, episode reward: 38.885, mean reward:  1.496 [-2.298, 32.155], mean action: 3.308 [0.000, 16.000],  loss: 0.018195, mae: 0.346609, mean_q: 0.549353, mean_eps: 0.000000
 2069/5000: episode: 74, duration: 0.365s, episode steps:  27, steps per second:  74, episode reward: 41.156, mean reward:  1.524 [-3.000, 32.070], mean action: 1.593 [0.000, 16.000],  loss: 0.021859, mae: 0.361299, mean_q: 0.600825, mean_eps: 0.000000
 2096/5000: episode: 75, duration: 0.361s, episode steps:  27, steps per second:  75, episode reward: 35.534, mean reward:  1.316 [-3.000, 31.694], mean action: 3.704 [0.000, 16.000],  loss: 0.019325, mae: 0.344952, mean_q: 0.597758, mean_eps: 0.000000
 2118/5000: episode: 76, duration: 0.301s, episode steps:  22, steps per second:  73, episode reward: 38.503, mean reward:  1.750 [-3.000, 32.440], mean action: 6.409 [0.000, 19.000],  loss: 0.018158, mae: 0.346273, mean_q: 0.593517, mean_eps: 0.000000
 2141/5000: episode: 77, duration: 0.312s, episode steps:  23, steps per second:  74, episode reward: 38.333, mean reward:  1.667 [-2.167, 32.033], mean action: 3.565 [1.000, 19.000],  loss: 0.021937, mae: 0.371157, mean_q: 0.557715, mean_eps: 0.000000
 2162/5000: episode: 78, duration: 0.292s, episode steps:  21, steps per second:  72, episode reward: 40.806, mean reward:  1.943 [-2.804, 32.090], mean action: 4.524 [0.000, 19.000],  loss: 0.024976, mae: 0.384008, mean_q: 0.539998, mean_eps: 0.000000
 2198/5000: episode: 79, duration: 0.471s, episode steps:  36, steps per second:  76, episode reward: 35.388, mean reward:  0.983 [-2.903, 32.140], mean action: 3.222 [0.000, 19.000],  loss: 0.021110, mae: 0.366696, mean_q: 0.526782, mean_eps: 0.000000
 2233/5000: episode: 80, duration: 0.466s, episode steps:  35, steps per second:  75, episode reward: 37.824, mean reward:  1.081 [-2.536, 32.200], mean action: 4.457 [0.000, 21.000],  loss: 0.020076, mae: 0.365013, mean_q: 0.535902, mean_eps: 0.000000
 2247/5000: episode: 81, duration: 0.202s, episode steps:  14, steps per second:  69, episode reward: 41.749, mean reward:  2.982 [-2.497, 31.809], mean action: 2.857 [1.000, 9.000],  loss: 0.022074, mae: 0.377820, mean_q: 0.491369, mean_eps: 0.000000
 2270/5000: episode: 82, duration: 0.312s, episode steps:  23, steps per second:  74, episode reward: 43.810, mean reward:  1.905 [-2.398, 32.193], mean action: 3.087 [1.000, 16.000],  loss: 0.020195, mae: 0.366733, mean_q: 0.535862, mean_eps: 0.000000
 2292/5000: episode: 83, duration: 0.301s, episode steps:  22, steps per second:  73, episode reward: 41.181, mean reward:  1.872 [-2.727, 31.736], mean action: 2.909 [0.000, 16.000],  loss: 0.020891, mae: 0.369217, mean_q: 0.562088, mean_eps: 0.000000
 2320/5000: episode: 84, duration: 0.371s, episode steps:  28, steps per second:  75, episode reward: 35.651, mean reward:  1.273 [-2.318, 31.831], mean action: 3.214 [0.000, 16.000],  loss: 0.018852, mae: 0.350922, mean_q: 0.529503, mean_eps: 0.000000
 2342/5000: episode: 85, duration: 0.295s, episode steps:  22, steps per second:  74, episode reward: 41.428, mean reward:  1.883 [-2.489, 32.226], mean action: 3.909 [0.000, 19.000],  loss: 0.023920, mae: 0.375104, mean_q: 0.511196, mean_eps: 0.000000
 2368/5000: episode: 86, duration: 0.352s, episode steps:  26, steps per second:  74, episode reward: 38.266, mean reward:  1.472 [-3.000, 32.040], mean action: 5.538 [0.000, 20.000],  loss: 0.021057, mae: 0.359876, mean_q: 0.572457, mean_eps: 0.000000
 2390/5000: episode: 87, duration: 0.303s, episode steps:  22, steps per second:  73, episode reward: 35.704, mean reward:  1.623 [-2.877, 31.989], mean action: 2.727 [0.000, 15.000],  loss: 0.018340, mae: 0.346926, mean_q: 0.536512, mean_eps: 0.000000
 2420/5000: episode: 88, duration: 0.404s, episode steps:  30, steps per second:  74, episode reward: 44.441, mean reward:  1.481 [-2.058, 32.211], mean action: 1.800 [0.000, 19.000],  loss: 0.020087, mae: 0.360452, mean_q: 0.513564, mean_eps: 0.000000
 2444/5000: episode: 89, duration: 0.331s, episode steps:  24, steps per second:  72, episode reward: 38.292, mean reward:  1.595 [-2.292, 31.812], mean action: 1.792 [0.000, 12.000],  loss: 0.020284, mae: 0.360204, mean_q: 0.511854, mean_eps: 0.000000
 2468/5000: episode: 90, duration: 0.319s, episode steps:  24, steps per second:  75, episode reward: 39.000, mean reward:  1.625 [-2.294, 32.200], mean action: 3.833 [0.000, 19.000],  loss: 0.023937, mae: 0.378186, mean_q: 0.524921, mean_eps: 0.000000
 2481/5000: episode: 91, duration: 0.192s, episode steps:  13, steps per second:  68, episode reward: 44.024, mean reward:  3.386 [-2.255, 32.120], mean action: 2.077 [0.000, 19.000],  loss: 0.023968, mae: 0.371013, mean_q: 0.519688, mean_eps: 0.000000
 2495/5000: episode: 92, duration: 0.201s, episode steps:  14, steps per second:  70, episode reward: 41.740, mean reward:  2.981 [-2.696, 32.453], mean action: 5.357 [0.000, 19.000],  loss: 0.017797, mae: 0.338296, mean_q: 0.532477, mean_eps: 0.000000
 2522/5000: episode: 93, duration: 0.362s, episode steps:  27, steps per second:  75, episode reward: 42.000, mean reward:  1.556 [-2.091, 32.130], mean action: 2.111 [0.000, 15.000],  loss: 0.019918, mae: 0.353767, mean_q: 0.616158, mean_eps: 0.000000
 2547/5000: episode: 94, duration: 0.395s, episode steps:  25, steps per second:  63, episode reward: 41.677, mean reward:  1.667 [-2.748, 31.902], mean action: 3.120 [0.000, 15.000],  loss: 0.020968, mae: 0.357113, mean_q: 0.582571, mean_eps: 0.000000
 2570/5000: episode: 95, duration: 0.331s, episode steps:  23, steps per second:  69, episode reward: 39.000, mean reward:  1.696 [-2.848, 32.530], mean action: 4.087 [0.000, 15.000],  loss: 0.018749, mae: 0.347916, mean_q: 0.616741, mean_eps: 0.000000
 2595/5000: episode: 96, duration: 0.331s, episode steps:  25, steps per second:  75, episode reward: 39.000, mean reward:  1.560 [-3.000, 32.140], mean action: 3.760 [2.000, 15.000],  loss: 0.020865, mae: 0.360074, mean_q: 0.620657, mean_eps: 0.000000
 2614/5000: episode: 97, duration: 0.268s, episode steps:  19, steps per second:  71, episode reward: 41.683, mean reward:  2.194 [-2.902, 32.683], mean action: 3.789 [0.000, 19.000],  loss: 0.019573, mae: 0.353180, mean_q: 0.597282, mean_eps: 0.000000
 2633/5000: episode: 98, duration: 0.284s, episode steps:  19, steps per second:  67, episode reward: 47.404, mean reward:  2.495 [-0.299, 32.150], mean action: 4.000 [0.000, 14.000],  loss: 0.020352, mae: 0.360071, mean_q: 0.555037, mean_eps: 0.000000
 2674/5000: episode: 99, duration: 0.552s, episode steps:  41, steps per second:  74, episode reward: 39.000, mean reward:  0.951 [-2.595, 32.240], mean action: 2.927 [0.000, 19.000],  loss: 0.019488, mae: 0.349240, mean_q: 0.526822, mean_eps: 0.000000
 2700/5000: episode: 100, duration: 0.364s, episode steps:  26, steps per second:  71, episode reward: 41.570, mean reward:  1.599 [-2.559, 32.570], mean action: 1.731 [0.000, 19.000],  loss: 0.021152, mae: 0.358341, mean_q: 0.539683, mean_eps: 0.000000
 2723/5000: episode: 101, duration: 0.355s, episode steps:  23, steps per second:  65, episode reward: 45.000, mean reward:  1.957 [-2.053, 32.450], mean action: 2.261 [0.000, 9.000],  loss: 0.018544, mae: 0.344602, mean_q: 0.504045, mean_eps: 0.000000
 2744/5000: episode: 102, duration: 0.291s, episode steps:  21, steps per second:  72, episode reward: 36.000, mean reward:  1.714 [-2.650, 30.000], mean action: 2.810 [0.000, 12.000],  loss: 0.023010, mae: 0.362902, mean_q: 0.534291, mean_eps: 0.000000
 2777/5000: episode: 103, duration: 0.445s, episode steps:  33, steps per second:  74, episode reward: 44.086, mean reward:  1.336 [-2.271, 32.120], mean action: 3.970 [0.000, 15.000],  loss: 0.021844, mae: 0.367919, mean_q: 0.588783, mean_eps: 0.000000
 2803/5000: episode: 104, duration: 0.363s, episode steps:  26, steps per second:  72, episode reward: 45.000, mean reward:  1.731 [-2.339, 32.560], mean action: 1.346 [0.000, 9.000],  loss: 0.017770, mae: 0.350226, mean_q: 0.588215, mean_eps: 0.000000
 2831/5000: episode: 105, duration: 0.385s, episode steps:  28, steps per second:  73, episode reward: 41.332, mean reward:  1.476 [-2.076, 32.210], mean action: 2.214 [0.000, 12.000],  loss: 0.019999, mae: 0.368442, mean_q: 0.507112, mean_eps: 0.000000
 2860/5000: episode: 106, duration: 0.394s, episode steps:  29, steps per second:  74, episode reward: 37.327, mean reward:  1.287 [-2.563, 32.060], mean action: 3.966 [0.000, 16.000],  loss: 0.021402, mae: 0.378164, mean_q: 0.480452, mean_eps: 0.000000
 2880/5000: episode: 107, duration: 0.284s, episode steps:  20, steps per second:  70, episode reward: 44.547, mean reward:  2.227 [-2.583, 32.532], mean action: 1.500 [0.000, 12.000],  loss: 0.019148, mae: 0.372550, mean_q: 0.447736, mean_eps: 0.000000
 2909/5000: episode: 108, duration: 0.390s, episode steps:  29, steps per second:  74, episode reward: 39.000, mean reward:  1.345 [-2.342, 32.170], mean action: 3.724 [1.000, 16.000],  loss: 0.021079, mae: 0.371785, mean_q: 0.523364, mean_eps: 0.000000
 2948/5000: episode: 109, duration: 0.523s, episode steps:  39, steps per second:  75, episode reward: 38.588, mean reward:  0.989 [-2.584, 31.902], mean action: 3.744 [0.000, 16.000],  loss: 0.022049, mae: 0.382042, mean_q: 0.572517, mean_eps: 0.000000
 2980/5000: episode: 110, duration: 0.420s, episode steps:  32, steps per second:  76, episode reward: 39.000, mean reward:  1.219 [-2.193, 32.040], mean action: 2.438 [0.000, 16.000],  loss: 0.017313, mae: 0.349781, mean_q: 0.590775, mean_eps: 0.000000
 3015/5000: episode: 111, duration: 0.473s, episode steps:  35, steps per second:  74, episode reward: 35.901, mean reward:  1.026 [-2.530, 32.401], mean action: 2.600 [0.000, 12.000],  loss: 0.020805, mae: 0.370819, mean_q: 0.593216, mean_eps: 0.000000
 3055/5000: episode: 112, duration: 0.547s, episode steps:  40, steps per second:  73, episode reward: 42.000, mean reward:  1.050 [-2.150, 32.130], mean action: 1.400 [0.000, 11.000],  loss: 0.017435, mae: 0.350920, mean_q: 0.535181, mean_eps: 0.000000
 3080/5000: episode: 113, duration: 0.341s, episode steps:  25, steps per second:  73, episode reward: 35.450, mean reward:  1.418 [-2.605, 31.907], mean action: 6.360 [0.000, 14.000],  loss: 0.021702, mae: 0.374829, mean_q: 0.521914, mean_eps: 0.000000
 3104/5000: episode: 114, duration: 0.324s, episode steps:  24, steps per second:  74, episode reward: 41.013, mean reward:  1.709 [-2.175, 31.713], mean action: 2.667 [0.000, 12.000],  loss: 0.017770, mae: 0.353358, mean_q: 0.578161, mean_eps: 0.000000
 3126/5000: episode: 115, duration: 0.326s, episode steps:  22, steps per second:  67, episode reward: 40.416, mean reward:  1.837 [-2.328, 32.353], mean action: 3.636 [0.000, 19.000],  loss: 0.019076, mae: 0.357802, mean_q: 0.595043, mean_eps: 0.000000
 3176/5000: episode: 116, duration: 0.674s, episode steps:  50, steps per second:  74, episode reward: 38.113, mean reward:  0.762 [-2.529, 32.500], mean action: 2.920 [0.000, 16.000],  loss: 0.019013, mae: 0.359357, mean_q: 0.519717, mean_eps: 0.000000
 3203/5000: episode: 117, duration: 0.379s, episode steps:  27, steps per second:  71, episode reward: 46.893, mean reward:  1.737 [-0.006, 32.037], mean action: 3.259 [0.000, 9.000],  loss: 0.022716, mae: 0.383351, mean_q: 0.491816, mean_eps: 0.000000
 3236/5000: episode: 118, duration: 0.440s, episode steps:  33, steps per second:  75, episode reward: 41.442, mean reward:  1.256 [-2.281, 32.070], mean action: 3.152 [0.000, 16.000],  loss: 0.018777, mae: 0.368773, mean_q: 0.511425, mean_eps: 0.000000
 3264/5000: episode: 119, duration: 0.385s, episode steps:  28, steps per second:  73, episode reward: 35.840, mean reward:  1.280 [-3.000, 32.070], mean action: 3.357 [0.000, 16.000],  loss: 0.019909, mae: 0.373767, mean_q: 0.471941, mean_eps: 0.000000
 3280/5000: episode: 120, duration: 0.240s, episode steps:  16, steps per second:  67, episode reward: 48.000, mean reward:  3.000 [ 0.000, 32.410], mean action: 1.000 [1.000, 1.000],  loss: 0.025202, mae: 0.397813, mean_q: 0.513699, mean_eps: 0.000000
 3321/5000: episode: 121, duration: 1.057s, episode steps:  41, steps per second:  39, episode reward: 41.500, mean reward:  1.012 [-2.687, 31.840], mean action: 2.366 [0.000, 12.000],  loss: 0.019662, mae: 0.369572, mean_q: 0.565096, mean_eps: 0.000000
 3349/5000: episode: 122, duration: 0.487s, episode steps:  28, steps per second:  57, episode reward: 38.158, mean reward:  1.363 [-2.452, 32.150], mean action: 6.036 [0.000, 15.000],  loss: 0.019749, mae: 0.369570, mean_q: 0.562657, mean_eps: 0.000000
 3380/5000: episode: 123, duration: 0.474s, episode steps:  31, steps per second:  65, episode reward: 46.276, mean reward:  1.493 [-0.424, 32.110], mean action: 3.613 [0.000, 19.000],  loss: 0.020059, mae: 0.368917, mean_q: 0.504087, mean_eps: 0.000000
 3409/5000: episode: 124, duration: 0.551s, episode steps:  29, steps per second:  53, episode reward: 42.509, mean reward:  1.466 [-2.385, 32.170], mean action: 4.586 [0.000, 19.000],  loss: 0.019923, mae: 0.363762, mean_q: 0.586566, mean_eps: 0.000000
 3441/5000: episode: 125, duration: 0.589s, episode steps:  32, steps per second:  54, episode reward: 33.000, mean reward:  1.031 [-3.000, 32.090], mean action: 4.906 [0.000, 19.000],  loss: 0.019307, mae: 0.366398, mean_q: 0.580720, mean_eps: 0.000000
 3461/5000: episode: 126, duration: 0.283s, episode steps:  20, steps per second:  71, episode reward: 38.821, mean reward:  1.941 [-2.474, 32.070], mean action: 5.800 [0.000, 19.000],  loss: 0.018842, mae: 0.358090, mean_q: 0.555963, mean_eps: 0.000000
 3488/5000: episode: 127, duration: 0.379s, episode steps:  27, steps per second:  71, episode reward: 46.947, mean reward:  1.739 [-0.265, 32.209], mean action: 4.667 [0.000, 13.000],  loss: 0.018624, mae: 0.357121, mean_q: 0.556530, mean_eps: 0.000000
 3503/5000: episode: 128, duration: 0.227s, episode steps:  15, steps per second:  66, episode reward: 47.481, mean reward:  3.165 [-0.085, 33.000], mean action: 2.467 [1.000, 3.000],  loss: 0.017918, mae: 0.350202, mean_q: 0.514351, mean_eps: 0.000000
 3527/5000: episode: 129, duration: 0.386s, episode steps:  24, steps per second:  62, episode reward: 41.392, mean reward:  1.725 [-2.151, 32.293], mean action: 3.750 [0.000, 16.000],  loss: 0.021357, mae: 0.363043, mean_q: 0.573044, mean_eps: 0.000000
 3543/5000: episode: 130, duration: 0.908s, episode steps:  16, steps per second:  18, episode reward: 42.000, mean reward:  2.625 [-2.192, 30.053], mean action: 1.188 [0.000, 15.000],  loss: 0.019709, mae: 0.354492, mean_q: 0.570832, mean_eps: 0.000000
 3560/5000: episode: 131, duration: 0.463s, episode steps:  17, steps per second:  37, episode reward: 44.105, mean reward:  2.594 [-2.453, 32.332], mean action: 3.000 [0.000, 16.000],  loss: 0.017158, mae: 0.346385, mean_q: 0.538370, mean_eps: 0.000000
 3585/5000: episode: 132, duration: 0.627s, episode steps:  25, steps per second:  40, episode reward: 41.514, mean reward:  1.661 [-3.000, 32.280], mean action: 8.240 [0.000, 20.000],  loss: 0.017339, mae: 0.347121, mean_q: 0.562805, mean_eps: 0.000000
 3623/5000: episode: 133, duration: 0.720s, episode steps:  38, steps per second:  53, episode reward: 38.819, mean reward:  1.022 [-2.641, 32.180], mean action: 2.842 [0.000, 16.000],  loss: 0.017100, mae: 0.345279, mean_q: 0.540215, mean_eps: 0.000000
 3654/5000: episode: 134, duration: 0.417s, episode steps:  31, steps per second:  74, episode reward: 42.966, mean reward:  1.386 [-2.279, 32.169], mean action: 3.161 [0.000, 16.000],  loss: 0.018337, mae: 0.358653, mean_q: 0.510336, mean_eps: 0.000000
 3674/5000: episode: 135, duration: 0.275s, episode steps:  20, steps per second:  73, episode reward: 43.628, mean reward:  2.181 [-2.018, 31.923], mean action: 1.600 [0.000, 12.000],  loss: 0.019673, mae: 0.367054, mean_q: 0.450078, mean_eps: 0.000000
 3692/5000: episode: 136, duration: 0.522s, episode steps:  18, steps per second:  34, episode reward: 47.713, mean reward:  2.651 [-0.000, 32.080], mean action: 1.056 [0.000, 3.000],  loss: 0.019325, mae: 0.359783, mean_q: 0.473485, mean_eps: 0.000000
 3719/5000: episode: 137, duration: 0.797s, episode steps:  27, steps per second:  34, episode reward: 38.848, mean reward:  1.439 [-3.000, 32.070], mean action: 3.444 [0.000, 12.000],  loss: 0.021539, mae: 0.370721, mean_q: 0.468958, mean_eps: 0.000000
 3744/5000: episode: 138, duration: 0.436s, episode steps:  25, steps per second:  57, episode reward: 41.955, mean reward:  1.678 [-2.772, 32.120], mean action: 2.560 [0.000, 19.000],  loss: 0.019244, mae: 0.355055, mean_q: 0.529271, mean_eps: 0.000000
 3770/5000: episode: 139, duration: 0.355s, episode steps:  26, steps per second:  73, episode reward: 42.000, mean reward:  1.615 [-2.137, 32.180], mean action: 2.654 [0.000, 12.000],  loss: 0.019451, mae: 0.359500, mean_q: 0.584546, mean_eps: 0.000000
 3790/5000: episode: 140, duration: 0.293s, episode steps:  20, steps per second:  68, episode reward: 41.058, mean reward:  2.053 [-2.961, 32.390], mean action: 2.450 [0.000, 16.000],  loss: 0.020748, mae: 0.373261, mean_q: 0.565058, mean_eps: 0.000000
 3817/5000: episode: 141, duration: 0.399s, episode steps:  27, steps per second:  68, episode reward: 42.000, mean reward:  1.556 [-2.344, 32.230], mean action: 2.333 [0.000, 19.000],  loss: 0.019684, mae: 0.369402, mean_q: 0.514652, mean_eps: 0.000000
 3839/5000: episode: 142, duration: 0.305s, episode steps:  22, steps per second:  72, episode reward: 41.737, mean reward:  1.897 [-2.241, 32.330], mean action: 3.364 [0.000, 15.000],  loss: 0.022875, mae: 0.381179, mean_q: 0.503650, mean_eps: 0.000000
 3876/5000: episode: 143, duration: 0.689s, episode steps:  37, steps per second:  54, episode reward: 35.749, mean reward:  0.966 [-2.400, 31.959], mean action: 3.216 [0.000, 18.000],  loss: 0.020145, mae: 0.372428, mean_q: 0.488074, mean_eps: 0.000000
 3902/5000: episode: 144, duration: 0.795s, episode steps:  26, steps per second:  33, episode reward: 41.333, mean reward:  1.590 [-2.217, 32.160], mean action: 1.308 [0.000, 11.000],  loss: 0.016781, mae: 0.367134, mean_q: 0.466532, mean_eps: 0.000000
 3926/5000: episode: 145, duration: 1.440s, episode steps:  24, steps per second:  17, episode reward: 41.425, mean reward:  1.726 [-2.189, 32.090], mean action: 4.792 [0.000, 13.000],  loss: 0.022822, mae: 0.390409, mean_q: 0.481168, mean_eps: 0.000000
 3955/5000: episode: 146, duration: 1.624s, episode steps:  29, steps per second:  18, episode reward: -32.800, mean reward: -1.131 [-32.037,  2.660], mean action: 6.483 [0.000, 18.000],  loss: 0.023028, mae: 0.386970, mean_q: 0.469081, mean_eps: 0.000000
 3973/5000: episode: 147, duration: 0.769s, episode steps:  18, steps per second:  23, episode reward: 38.860, mean reward:  2.159 [-2.843, 32.180], mean action: 3.667 [0.000, 12.000],  loss: 0.021544, mae: 0.375024, mean_q: 0.516188, mean_eps: 0.000000
 3998/5000: episode: 148, duration: 0.647s, episode steps:  25, steps per second:  39, episode reward: 35.238, mean reward:  1.410 [-3.000, 32.154], mean action: 4.760 [0.000, 18.000],  loss: 0.020293, mae: 0.364952, mean_q: 0.536486, mean_eps: 0.000000
 4019/5000: episode: 149, duration: 0.944s, episode steps:  21, steps per second:  22, episode reward: 44.360, mean reward:  2.112 [-2.417, 32.300], mean action: 2.190 [0.000, 11.000],  loss: 0.019009, mae: 0.360466, mean_q: 0.566196, mean_eps: 0.000000
 4052/5000: episode: 150, duration: 0.819s, episode steps:  33, steps per second:  40, episode reward: 38.305, mean reward:  1.161 [-3.000, 32.030], mean action: 8.091 [0.000, 19.000],  loss: 0.020070, mae: 0.367900, mean_q: 0.501219, mean_eps: 0.000000
 4073/5000: episode: 151, duration: 0.514s, episode steps:  21, steps per second:  41, episode reward: 39.000, mean reward:  1.857 [-3.000, 32.230], mean action: 3.238 [0.000, 16.000],  loss: 0.019913, mae: 0.363760, mean_q: 0.491197, mean_eps: 0.000000
 4101/5000: episode: 152, duration: 0.384s, episode steps:  28, steps per second:  73, episode reward: 35.643, mean reward:  1.273 [-3.000, 32.060], mean action: 4.286 [0.000, 20.000],  loss: 0.024478, mae: 0.393186, mean_q: 0.497408, mean_eps: 0.000000
 4125/5000: episode: 153, duration: 0.342s, episode steps:  24, steps per second:  70, episode reward: 44.015, mean reward:  1.834 [-2.203, 32.080], mean action: 2.917 [0.000, 19.000],  loss: 0.022087, mae: 0.378902, mean_q: 0.502351, mean_eps: 0.000000
 4170/5000: episode: 154, duration: 0.862s, episode steps:  45, steps per second:  52, episode reward: 32.130, mean reward:  0.714 [-3.000, 32.090], mean action: 5.689 [0.000, 19.000],  loss: 0.019693, mae: 0.370084, mean_q: 0.489515, mean_eps: 0.000000
 4196/5000: episode: 155, duration: 0.456s, episode steps:  26, steps per second:  57, episode reward: 38.150, mean reward:  1.467 [-2.516, 31.981], mean action: 3.731 [0.000, 19.000],  loss: 0.020275, mae: 0.372337, mean_q: 0.519136, mean_eps: 0.000000
 4239/5000: episode: 156, duration: 0.800s, episode steps:  43, steps per second:  54, episode reward: 40.975, mean reward:  0.953 [-2.412, 32.853], mean action: 1.512 [0.000, 12.000],  loss: 0.019614, mae: 0.361143, mean_q: 0.552230, mean_eps: 0.000000
 4258/5000: episode: 157, duration: 0.298s, episode steps:  19, steps per second:  64, episode reward: 42.000, mean reward:  2.211 [-3.000, 32.240], mean action: 3.684 [0.000, 19.000],  loss: 0.020440, mae: 0.366272, mean_q: 0.497072, mean_eps: 0.000000
 4292/5000: episode: 158, duration: 0.517s, episode steps:  34, steps per second:  66, episode reward: 35.899, mean reward:  1.056 [-3.000, 32.300], mean action: 5.647 [0.000, 19.000],  loss: 0.020275, mae: 0.368058, mean_q: 0.511458, mean_eps: 0.000000
 4319/5000: episode: 159, duration: 0.381s, episode steps:  27, steps per second:  71, episode reward: 44.592, mean reward:  1.652 [-2.115, 32.140], mean action: 1.926 [1.000, 9.000],  loss: 0.016362, mae: 0.354786, mean_q: 0.496808, mean_eps: 0.000000
 4350/5000: episode: 160, duration: 0.423s, episode steps:  31, steps per second:  73, episode reward: 41.055, mean reward:  1.324 [-2.312, 32.270], mean action: 2.968 [0.000, 15.000],  loss: 0.020854, mae: 0.367563, mean_q: 0.439234, mean_eps: 0.000000
 4368/5000: episode: 161, duration: 0.262s, episode steps:  18, steps per second:  69, episode reward: 44.683, mean reward:  2.482 [-2.046, 32.620], mean action: 2.778 [0.000, 15.000],  loss: 0.020688, mae: 0.368142, mean_q: 0.466089, mean_eps: 0.000000
 4392/5000: episode: 162, duration: 0.342s, episode steps:  24, steps per second:  70, episode reward: 41.706, mean reward:  1.738 [-3.000, 32.902], mean action: 5.167 [0.000, 15.000],  loss: 0.018180, mae: 0.351932, mean_q: 0.488201, mean_eps: 0.000000
 4412/5000: episode: 163, duration: 0.277s, episode steps:  20, steps per second:  72, episode reward: 41.634, mean reward:  2.082 [-2.742, 32.197], mean action: 3.800 [0.000, 16.000],  loss: 0.020872, mae: 0.371588, mean_q: 0.447515, mean_eps: 0.000000
 4442/5000: episode: 164, duration: 0.467s, episode steps:  30, steps per second:  64, episode reward: 43.296, mean reward:  1.443 [-2.031, 32.200], mean action: 4.367 [0.000, 14.000],  loss: 0.022392, mae: 0.374598, mean_q: 0.471400, mean_eps: 0.000000
 4458/5000: episode: 165, duration: 0.236s, episode steps:  16, steps per second:  68, episode reward: 44.082, mean reward:  2.755 [-2.130, 32.100], mean action: 2.750 [1.000, 12.000],  loss: 0.018779, mae: 0.362236, mean_q: 0.528546, mean_eps: 0.000000
 4483/5000: episode: 166, duration: 0.508s, episode steps:  25, steps per second:  49, episode reward: 44.215, mean reward:  1.769 [-2.268, 32.210], mean action: 2.640 [0.000, 19.000],  loss: 0.022322, mae: 0.376637, mean_q: 0.546869, mean_eps: 0.000000
 4516/5000: episode: 167, duration: 0.499s, episode steps:  33, steps per second:  66, episode reward: 34.826, mean reward:  1.055 [-2.275, 32.247], mean action: 4.394 [0.000, 19.000],  loss: 0.018204, mae: 0.348864, mean_q: 0.517288, mean_eps: 0.000000
 4541/5000: episode: 168, duration: 0.455s, episode steps:  25, steps per second:  55, episode reward: 41.222, mean reward:  1.649 [-2.212, 31.852], mean action: 3.960 [2.000, 14.000],  loss: 0.019802, mae: 0.353767, mean_q: 0.498089, mean_eps: 0.000000
 4560/5000: episode: 169, duration: 0.364s, episode steps:  19, steps per second:  52, episode reward: 44.266, mean reward:  2.330 [-2.772, 32.184], mean action: 4.316 [1.000, 19.000],  loss: 0.018115, mae: 0.342475, mean_q: 0.496418, mean_eps: 0.000000
 4600/5000: episode: 170, duration: 1.085s, episode steps:  40, steps per second:  37, episode reward: 42.000, mean reward:  1.050 [-2.650, 32.090], mean action: 2.800 [0.000, 16.000],  loss: 0.020728, mae: 0.358811, mean_q: 0.520524, mean_eps: 0.000000
 4624/5000: episode: 171, duration: 0.717s, episode steps:  24, steps per second:  33, episode reward: 36.000, mean reward:  1.500 [-3.000, 32.430], mean action: 4.708 [0.000, 16.000],  loss: 0.026543, mae: 0.391648, mean_q: 0.561641, mean_eps: 0.000000
 4648/5000: episode: 172, duration: 0.541s, episode steps:  24, steps per second:  44, episode reward: 44.174, mean reward:  1.841 [-2.370, 32.280], mean action: 1.500 [0.000, 16.000],  loss: 0.021247, mae: 0.366422, mean_q: 0.545989, mean_eps: 0.000000
 4668/5000: episode: 173, duration: 0.684s, episode steps:  20, steps per second:  29, episode reward: 44.744, mean reward:  2.237 [-2.312, 32.253], mean action: 2.750 [0.000, 12.000],  loss: 0.018690, mae: 0.352306, mean_q: 0.516727, mean_eps: 0.000000
 4693/5000: episode: 174, duration: 0.469s, episode steps:  25, steps per second:  53, episode reward: 43.986, mean reward:  1.759 [-2.304, 32.230], mean action: 2.080 [0.000, 14.000],  loss: 0.018850, mae: 0.354035, mean_q: 0.486156, mean_eps: 0.000000
 4714/5000: episode: 175, duration: 0.368s, episode steps:  21, steps per second:  57, episode reward: 41.820, mean reward:  1.991 [-2.819, 32.120], mean action: 1.619 [0.000, 11.000],  loss: 0.020188, mae: 0.368062, mean_q: 0.455216, mean_eps: 0.000000
 4732/5000: episode: 176, duration: 0.272s, episode steps:  18, steps per second:  66, episode reward: 41.838, mean reward:  2.324 [-3.000, 32.038], mean action: 3.611 [1.000, 19.000],  loss: 0.023451, mae: 0.376017, mean_q: 0.490391, mean_eps: 0.000000
 4752/5000: episode: 177, duration: 0.313s, episode steps:  20, steps per second:  64, episode reward: 44.504, mean reward:  2.225 [-2.525, 32.321], mean action: 2.600 [0.000, 15.000],  loss: 0.023754, mae: 0.378552, mean_q: 0.500971, mean_eps: 0.000000
 4770/5000: episode: 178, duration: 0.260s, episode steps:  18, steps per second:  69, episode reward: 41.224, mean reward:  2.290 [-3.000, 32.473], mean action: 4.833 [0.000, 12.000],  loss: 0.023234, mae: 0.383281, mean_q: 0.527986, mean_eps: 0.000000
 4795/5000: episode: 179, duration: 0.362s, episode steps:  25, steps per second:  69, episode reward: 47.398, mean reward:  1.896 [-0.170, 31.965], mean action: 1.160 [0.000, 3.000],  loss: 0.020389, mae: 0.371696, mean_q: 0.504881, mean_eps: 0.000000
 4819/5000: episode: 180, duration: 0.385s, episode steps:  24, steps per second:  62, episode reward: 35.004, mean reward:  1.458 [-3.000, 32.240], mean action: 3.292 [0.000, 15.000],  loss: 0.019133, mae: 0.363045, mean_q: 0.504236, mean_eps: 0.000000
 4849/5000: episode: 181, duration: 0.425s, episode steps:  30, steps per second:  71, episode reward: 44.445, mean reward:  1.481 [-2.129, 33.000], mean action: 3.167 [1.000, 15.000],  loss: 0.022678, mae: 0.386089, mean_q: 0.550129, mean_eps: 0.000000
 4861/5000: episode: 182, duration: 0.180s, episode steps:  12, steps per second:  67, episode reward: 41.901, mean reward:  3.492 [-2.312, 32.081], mean action: 2.833 [0.000, 15.000],  loss: 0.022104, mae: 0.379631, mean_q: 0.524262, mean_eps: 0.000000
 4877/5000: episode: 183, duration: 0.228s, episode steps:  16, steps per second:  70, episode reward: 42.000, mean reward:  2.625 [-2.900, 32.540], mean action: 3.125 [0.000, 16.000],  loss: 0.017778, mae: 0.365691, mean_q: 0.504267, mean_eps: 0.000000
 4902/5000: episode: 184, duration: 0.378s, episode steps:  25, steps per second:  66, episode reward: 41.262, mean reward:  1.650 [-2.108, 32.432], mean action: 2.520 [0.000, 16.000],  loss: 0.016842, mae: 0.359543, mean_q: 0.484734, mean_eps: 0.000000
 4933/5000: episode: 185, duration: 0.439s, episode steps:  31, steps per second:  71, episode reward: 33.000, mean reward:  1.065 [-2.751, 32.200], mean action: 4.387 [0.000, 20.000],  loss: 0.019168, mae: 0.371187, mean_q: 0.503526, mean_eps: 0.000000
 4955/5000: episode: 186, duration: 0.339s, episode steps:  22, steps per second:  65, episode reward: 41.816, mean reward:  1.901 [-2.245, 32.270], mean action: 3.409 [0.000, 15.000],  loss: 0.017069, mae: 0.349697, mean_q: 0.513857, mean_eps: 0.000000
 4979/5000: episode: 187, duration: 0.355s, episode steps:  24, steps per second:  68, episode reward: 41.911, mean reward:  1.746 [-2.303, 32.030], mean action: 3.000 [0.000, 15.000],  loss: 0.021984, mae: 0.375494, mean_q: 0.497503, mean_eps: 0.000000
 4998/5000: episode: 188, duration: 0.277s, episode steps:  19, steps per second:  69, episode reward: 42.000, mean reward:  2.211 [-2.272, 32.320], mean action: 2.632 [0.000, 14.000],  loss: 0.023066, mae: 0.389082, mean_q: 0.499545, mean_eps: 0.000000
done, took 75.052 seconds
DQN Evaluation: 5293 victories out of 6233 episodes
Training for 5000 steps ...
   17/5000: episode: 1, duration: 0.152s, episode steps:  17, steps per second: 112, episode reward: 38.881, mean reward:  2.287 [-2.502, 32.117], mean action: 2.882 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   30/5000: episode: 2, duration: 0.113s, episode steps:  13, steps per second: 115, episode reward: 44.039, mean reward:  3.388 [-2.373, 32.637], mean action: 5.154 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   50/5000: episode: 3, duration: 0.220s, episode steps:  20, steps per second:  91, episode reward: 35.274, mean reward:  1.764 [-3.000, 32.240], mean action: 3.550 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   73/5000: episode: 4, duration: 0.154s, episode steps:  23, steps per second: 149, episode reward: 35.038, mean reward:  1.523 [-2.302, 31.734], mean action: 3.696 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   98/5000: episode: 5, duration: 0.244s, episode steps:  25, steps per second: 103, episode reward: 37.915, mean reward:  1.517 [-2.159, 32.120], mean action: 5.440 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  119/5000: episode: 6, duration: 0.145s, episode steps:  21, steps per second: 144, episode reward: -32.090, mean reward: -1.528 [-31.939,  2.542], mean action: 5.714 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  158/5000: episode: 7, duration: 0.255s, episode steps:  39, steps per second: 153, episode reward: 35.199, mean reward:  0.903 [-2.417, 31.990], mean action: 2.795 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  178/5000: episode: 8, duration: 0.136s, episode steps:  20, steps per second: 147, episode reward: 41.176, mean reward:  2.059 [-2.364, 33.000], mean action: 3.150 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  200/5000: episode: 9, duration: 0.149s, episode steps:  22, steps per second: 148, episode reward: 35.740, mean reward:  1.625 [-2.405, 32.210], mean action: 2.818 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  233/5000: episode: 10, duration: 0.251s, episode steps:  33, steps per second: 131, episode reward: -35.450, mean reward: -1.074 [-32.215,  2.280], mean action: 5.242 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  251/5000: episode: 11, duration: 0.122s, episode steps:  18, steps per second: 148, episode reward: -35.360, mean reward: -1.964 [-31.572,  3.000], mean action: 5.056 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  268/5000: episode: 12, duration: 0.134s, episode steps:  17, steps per second: 127, episode reward: 41.423, mean reward:  2.437 [-2.368, 32.220], mean action: 3.235 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  291/5000: episode: 13, duration: 0.175s, episode steps:  23, steps per second: 131, episode reward: 35.428, mean reward:  1.540 [-3.000, 32.500], mean action: 3.913 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  309/5000: episode: 14, duration: 0.144s, episode steps:  18, steps per second: 125, episode reward: 38.070, mean reward:  2.115 [-3.000, 32.185], mean action: 3.722 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  337/5000: episode: 15, duration: 0.184s, episode steps:  28, steps per second: 152, episode reward: 38.134, mean reward:  1.362 [-2.729, 32.260], mean action: 7.036 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  364/5000: episode: 16, duration: 0.190s, episode steps:  27, steps per second: 142, episode reward: -32.800, mean reward: -1.215 [-32.292,  2.904], mean action: 7.185 [1.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  374/5000: episode: 17, duration: 0.092s, episode steps:  10, steps per second: 108, episode reward: 44.733, mean reward:  4.473 [-2.328, 32.436], mean action: 2.900 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  389/5000: episode: 18, duration: 0.108s, episode steps:  15, steps per second: 138, episode reward: 38.426, mean reward:  2.562 [-3.000, 31.706], mean action: 3.000 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  414/5000: episode: 19, duration: 0.169s, episode steps:  25, steps per second: 148, episode reward: -35.260, mean reward: -1.410 [-32.264,  2.903], mean action: 8.840 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  445/5000: episode: 20, duration: 0.200s, episode steps:  31, steps per second: 155, episode reward: -32.250, mean reward: -1.040 [-32.062,  2.596], mean action: 4.000 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  466/5000: episode: 21, duration: 0.145s, episode steps:  21, steps per second: 144, episode reward: 38.493, mean reward:  1.833 [-2.270, 32.881], mean action: 3.190 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  489/5000: episode: 22, duration: 0.150s, episode steps:  23, steps per second: 153, episode reward: -35.080, mean reward: -1.525 [-30.080,  2.554], mean action: 7.043 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  508/5000: episode: 23, duration: 0.124s, episode steps:  19, steps per second: 154, episode reward: -32.490, mean reward: -1.710 [-32.610,  3.000], mean action: 5.263 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  530/5000: episode: 24, duration: 0.148s, episode steps:  22, steps per second: 148, episode reward: 32.592, mean reward:  1.481 [-3.000, 32.592], mean action: 3.318 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  557/5000: episode: 25, duration: 0.178s, episode steps:  27, steps per second: 152, episode reward: -32.320, mean reward: -1.197 [-32.265,  3.000], mean action: 8.815 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  574/5000: episode: 26, duration: 0.125s, episode steps:  17, steps per second: 136, episode reward: 40.643, mean reward:  2.391 [-2.244, 32.010], mean action: 4.647 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  595/5000: episode: 27, duration: 0.147s, episode steps:  21, steps per second: 143, episode reward: -36.000, mean reward: -1.714 [-32.798,  2.471], mean action: 6.381 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  632/5000: episode: 28, duration: 0.236s, episode steps:  37, steps per second: 157, episode reward: 35.876, mean reward:  0.970 [-2.456, 32.876], mean action: 3.892 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  655/5000: episode: 29, duration: 0.148s, episode steps:  23, steps per second: 156, episode reward: 39.000, mean reward:  1.696 [-2.396, 32.210], mean action: 3.565 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  672/5000: episode: 30, duration: 0.124s, episode steps:  17, steps per second: 137, episode reward: -41.380, mean reward: -2.434 [-33.000,  1.939], mean action: 7.471 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  692/5000: episode: 31, duration: 0.139s, episode steps:  20, steps per second: 144, episode reward: 43.294, mean reward:  2.165 [-2.160, 33.000], mean action: 5.600 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  710/5000: episode: 32, duration: 0.126s, episode steps:  18, steps per second: 143, episode reward: 35.286, mean reward:  1.960 [-3.000, 32.083], mean action: 3.111 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  743/5000: episode: 33, duration: 0.236s, episode steps:  33, steps per second: 140, episode reward: 32.298, mean reward:  0.979 [-2.388, 32.099], mean action: 5.606 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  759/5000: episode: 34, duration: 0.111s, episode steps:  16, steps per second: 144, episode reward: 35.508, mean reward:  2.219 [-3.000, 32.508], mean action: 3.875 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  779/5000: episode: 35, duration: 0.166s, episode steps:  20, steps per second: 121, episode reward: 36.000, mean reward:  1.800 [-2.422, 32.050], mean action: 3.500 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  809/5000: episode: 36, duration: 0.995s, episode steps:  30, steps per second:  30, episode reward: 32.335, mean reward:  1.078 [-3.000, 32.090], mean action: 6.167 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  830/5000: episode: 37, duration: 0.144s, episode steps:  21, steps per second: 146, episode reward: -35.750, mean reward: -1.702 [-32.643,  3.000], mean action: 6.905 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  845/5000: episode: 38, duration: 0.132s, episode steps:  15, steps per second: 114, episode reward: 38.805, mean reward:  2.587 [-2.612, 32.125], mean action: 3.733 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  863/5000: episode: 39, duration: 0.161s, episode steps:  18, steps per second: 112, episode reward: -32.670, mean reward: -1.815 [-32.670,  3.000], mean action: 4.611 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  886/5000: episode: 40, duration: 0.162s, episode steps:  23, steps per second: 142, episode reward: 35.715, mean reward:  1.553 [-2.467, 32.540], mean action: 2.565 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  906/5000: episode: 41, duration: 0.135s, episode steps:  20, steps per second: 148, episode reward: 35.268, mean reward:  1.763 [-2.571, 32.268], mean action: 4.050 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  933/5000: episode: 42, duration: 0.250s, episode steps:  27, steps per second: 108, episode reward: -36.000, mean reward: -1.333 [-32.320,  2.259], mean action: 4.185 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  953/5000: episode: 43, duration: 0.156s, episode steps:  20, steps per second: 128, episode reward: 41.961, mean reward:  2.098 [-2.383, 32.110], mean action: 2.900 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  981/5000: episode: 44, duration: 0.195s, episode steps:  28, steps per second: 144, episode reward: -35.730, mean reward: -1.276 [-32.138,  2.686], mean action: 6.107 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1006/5000: episode: 45, duration: 0.234s, episode steps:  25, steps per second: 107, episode reward: 32.391, mean reward:  1.296 [-2.568, 31.831], mean action: 6.960 [0.000, 21.000],  loss: 0.019547, mae: 0.376893, mean_q: 0.551671, mean_eps: 0.000000
 1022/5000: episode: 46, duration: 0.240s, episode steps:  16, steps per second:  67, episode reward: 41.126, mean reward:  2.570 [-3.000, 32.140], mean action: 1.750 [0.000, 12.000],  loss: 0.019185, mae: 0.367359, mean_q: 0.504141, mean_eps: 0.000000
 1035/5000: episode: 47, duration: 0.189s, episode steps:  13, steps per second:  69, episode reward: 41.185, mean reward:  3.168 [-2.318, 32.691], mean action: 3.000 [0.000, 12.000],  loss: 0.024167, mae: 0.392743, mean_q: 0.498777, mean_eps: 0.000000
 1050/5000: episode: 48, duration: 0.237s, episode steps:  15, steps per second:  63, episode reward: 41.146, mean reward:  2.743 [-2.323, 32.131], mean action: 4.733 [0.000, 14.000],  loss: 0.019444, mae: 0.367857, mean_q: 0.499532, mean_eps: 0.000000
 1065/5000: episode: 49, duration: 0.216s, episode steps:  15, steps per second:  69, episode reward: 41.468, mean reward:  2.765 [-2.318, 31.818], mean action: 4.000 [0.000, 15.000],  loss: 0.017060, mae: 0.357200, mean_q: 0.517395, mean_eps: 0.000000
 1092/5000: episode: 50, duration: 0.478s, episode steps:  27, steps per second:  56, episode reward: 35.533, mean reward:  1.316 [-2.615, 32.213], mean action: 4.000 [0.000, 15.000],  loss: 0.018519, mae: 0.360269, mean_q: 0.512972, mean_eps: 0.000000
 1128/5000: episode: 51, duration: 0.529s, episode steps:  36, steps per second:  68, episode reward: 35.411, mean reward:  0.984 [-2.240, 32.390], mean action: 5.083 [0.000, 15.000],  loss: 0.022281, mae: 0.374346, mean_q: 0.563911, mean_eps: 0.000000
 1149/5000: episode: 52, duration: 0.285s, episode steps:  21, steps per second:  74, episode reward: -35.410, mean reward: -1.686 [-33.000,  2.725], mean action: 3.667 [0.000, 15.000],  loss: 0.020974, mae: 0.374183, mean_q: 0.556330, mean_eps: 0.000000
 1186/5000: episode: 53, duration: 0.513s, episode steps:  37, steps per second:  72, episode reward: -33.000, mean reward: -0.892 [-29.835,  2.460], mean action: 10.027 [0.000, 19.000],  loss: 0.021863, mae: 0.383509, mean_q: 0.543683, mean_eps: 0.000000
 1207/5000: episode: 54, duration: 0.330s, episode steps:  21, steps per second:  64, episode reward: 44.181, mean reward:  2.104 [-2.241, 32.968], mean action: 2.857 [0.000, 19.000],  loss: 0.019422, mae: 0.372199, mean_q: 0.568894, mean_eps: 0.000000
 1231/5000: episode: 55, duration: 0.326s, episode steps:  24, steps per second:  74, episode reward: -36.000, mean reward: -1.500 [-33.000,  2.320], mean action: 5.375 [0.000, 19.000],  loss: 0.016301, mae: 0.351515, mean_q: 0.546418, mean_eps: 0.000000
 1262/5000: episode: 56, duration: 0.414s, episode steps:  31, steps per second:  75, episode reward: 34.277, mean reward:  1.106 [-2.494, 32.903], mean action: 4.581 [0.000, 19.000],  loss: 0.020154, mae: 0.369959, mean_q: 0.509498, mean_eps: 0.000000
 1282/5000: episode: 57, duration: 0.279s, episode steps:  20, steps per second:  72, episode reward: 35.286, mean reward:  1.764 [-2.354, 30.414], mean action: 5.300 [0.000, 19.000],  loss: 0.018138, mae: 0.354042, mean_q: 0.528398, mean_eps: 0.000000
 1304/5000: episode: 58, duration: 0.294s, episode steps:  22, steps per second:  75, episode reward: -38.240, mean reward: -1.738 [-32.062,  2.210], mean action: 5.455 [0.000, 19.000],  loss: 0.018668, mae: 0.359861, mean_q: 0.538149, mean_eps: 0.000000
 1322/5000: episode: 59, duration: 0.247s, episode steps:  18, steps per second:  73, episode reward: 41.379, mean reward:  2.299 [-2.059, 31.973], mean action: 2.222 [0.000, 12.000],  loss: 0.023446, mae: 0.382779, mean_q: 0.530739, mean_eps: 0.000000
 1343/5000: episode: 60, duration: 0.288s, episode steps:  21, steps per second:  73, episode reward: 35.932, mean reward:  1.711 [-3.000, 32.932], mean action: 3.524 [0.000, 12.000],  loss: 0.018513, mae: 0.356966, mean_q: 0.468228, mean_eps: 0.000000
 1362/5000: episode: 61, duration: 0.264s, episode steps:  19, steps per second:  72, episode reward: 36.000, mean reward:  1.895 [-3.000, 33.000], mean action: 4.579 [0.000, 18.000],  loss: 0.021737, mae: 0.374201, mean_q: 0.485586, mean_eps: 0.000000
 1381/5000: episode: 62, duration: 0.289s, episode steps:  19, steps per second:  66, episode reward: 38.684, mean reward:  2.036 [-2.512, 32.420], mean action: 2.737 [0.000, 12.000],  loss: 0.019960, mae: 0.358769, mean_q: 0.485555, mean_eps: 0.000000
 1402/5000: episode: 63, duration: 0.291s, episode steps:  21, steps per second:  72, episode reward: 32.356, mean reward:  1.541 [-3.000, 33.000], mean action: 8.905 [0.000, 20.000],  loss: 0.021900, mae: 0.361059, mean_q: 0.479176, mean_eps: 0.000000
 1416/5000: episode: 64, duration: 0.227s, episode steps:  14, steps per second:  62, episode reward: 39.000, mean reward:  2.786 [-2.480, 30.909], mean action: 3.286 [0.000, 12.000],  loss: 0.020543, mae: 0.365435, mean_q: 0.544940, mean_eps: 0.000000
 1444/5000: episode: 65, duration: 0.378s, episode steps:  28, steps per second:  74, episode reward: -32.690, mean reward: -1.167 [-32.076,  3.000], mean action: 5.500 [0.000, 16.000],  loss: 0.018708, mae: 0.352871, mean_q: 0.538733, mean_eps: 0.000000
 1474/5000: episode: 66, duration: 0.398s, episode steps:  30, steps per second:  75, episode reward: 37.428, mean reward:  1.248 [-2.780, 32.400], mean action: 6.100 [1.000, 15.000],  loss: 0.021466, mae: 0.371908, mean_q: 0.503952, mean_eps: 0.000000
 1495/5000: episode: 67, duration: 0.324s, episode steps:  21, steps per second:  65, episode reward: 38.156, mean reward:  1.817 [-3.000, 32.156], mean action: 3.381 [0.000, 19.000],  loss: 0.022384, mae: 0.369379, mean_q: 0.547234, mean_eps: 0.000000
 1519/5000: episode: 68, duration: 0.356s, episode steps:  24, steps per second:  67, episode reward: 42.000, mean reward:  1.750 [-2.353, 32.120], mean action: 2.917 [0.000, 15.000],  loss: 0.015876, mae: 0.353067, mean_q: 0.549813, mean_eps: 0.000000
 1543/5000: episode: 69, duration: 0.332s, episode steps:  24, steps per second:  72, episode reward: 38.732, mean reward:  1.614 [-2.765, 32.310], mean action: 3.333 [0.000, 19.000],  loss: 0.020165, mae: 0.370772, mean_q: 0.517996, mean_eps: 0.000000
 1567/5000: episode: 70, duration: 0.542s, episode steps:  24, steps per second:  44, episode reward: 32.713, mean reward:  1.363 [-3.000, 32.113], mean action: 5.667 [0.000, 15.000],  loss: 0.018877, mae: 0.363224, mean_q: 0.512771, mean_eps: 0.000000
 1588/5000: episode: 71, duration: 0.310s, episode steps:  21, steps per second:  68, episode reward: 35.383, mean reward:  1.685 [-2.678, 31.913], mean action: 5.905 [0.000, 19.000],  loss: 0.017061, mae: 0.358376, mean_q: 0.447784, mean_eps: 0.000000
 1618/5000: episode: 72, duration: 0.510s, episode steps:  30, steps per second:  59, episode reward: 35.473, mean reward:  1.182 [-2.612, 32.130], mean action: 5.333 [0.000, 15.000],  loss: 0.015906, mae: 0.347821, mean_q: 0.456343, mean_eps: 0.000000
 1654/5000: episode: 73, duration: 0.795s, episode steps:  36, steps per second:  45, episode reward: 35.838, mean reward:  0.995 [-2.302, 32.210], mean action: 6.361 [0.000, 20.000],  loss: 0.019531, mae: 0.366655, mean_q: 0.542526, mean_eps: 0.000000
 1681/5000: episode: 74, duration: 0.708s, episode steps:  27, steps per second:  38, episode reward: 32.121, mean reward:  1.190 [-2.837, 31.698], mean action: 7.593 [0.000, 21.000],  loss: 0.021234, mae: 0.371051, mean_q: 0.540676, mean_eps: 0.000000
 1711/5000: episode: 75, duration: 0.736s, episode steps:  30, steps per second:  41, episode reward: 35.394, mean reward:  1.180 [-2.426, 32.344], mean action: 2.833 [0.000, 19.000],  loss: 0.020352, mae: 0.367341, mean_q: 0.543527, mean_eps: 0.000000
 1729/5000: episode: 76, duration: 0.422s, episode steps:  18, steps per second:  43, episode reward: 41.452, mean reward:  2.303 [-2.151, 31.821], mean action: 2.333 [0.000, 11.000],  loss: 0.020299, mae: 0.375196, mean_q: 0.521265, mean_eps: 0.000000
 1749/5000: episode: 77, duration: 0.514s, episode steps:  20, steps per second:  39, episode reward: 30.000, mean reward:  1.500 [-2.526, 29.910], mean action: 3.400 [0.000, 11.000],  loss: 0.014971, mae: 0.353762, mean_q: 0.487174, mean_eps: 0.000000
 1775/5000: episode: 78, duration: 0.804s, episode steps:  26, steps per second:  32, episode reward: 39.000, mean reward:  1.500 [-2.341, 32.280], mean action: 8.846 [0.000, 20.000],  loss: 0.019002, mae: 0.359667, mean_q: 0.529563, mean_eps: 0.000000
 1798/5000: episode: 79, duration: 0.438s, episode steps:  23, steps per second:  53, episode reward: -36.000, mean reward: -1.565 [-32.901,  2.420], mean action: 3.739 [0.000, 11.000],  loss: 0.023931, mae: 0.377991, mean_q: 0.524888, mean_eps: 0.000000
 1835/5000: episode: 80, duration: 0.572s, episode steps:  37, steps per second:  65, episode reward: 34.913, mean reward:  0.944 [-2.414, 32.430], mean action: 7.297 [0.000, 18.000],  loss: 0.017940, mae: 0.353559, mean_q: 0.536298, mean_eps: 0.000000
 1856/5000: episode: 81, duration: 0.428s, episode steps:  21, steps per second:  49, episode reward: 38.358, mean reward:  1.827 [-2.136, 32.310], mean action: 3.571 [1.000, 15.000],  loss: 0.020786, mae: 0.371134, mean_q: 0.489718, mean_eps: 0.000000
 1880/5000: episode: 82, duration: 0.353s, episode steps:  24, steps per second:  68, episode reward: 40.340, mean reward:  1.681 [-2.265, 33.000], mean action: 4.542 [0.000, 15.000],  loss: 0.019800, mae: 0.361642, mean_q: 0.489432, mean_eps: 0.000000
 1896/5000: episode: 83, duration: 0.241s, episode steps:  16, steps per second:  66, episode reward: 37.768, mean reward:  2.360 [-2.394, 31.916], mean action: 6.312 [0.000, 18.000],  loss: 0.020350, mae: 0.371827, mean_q: 0.543493, mean_eps: 0.000000
 1916/5000: episode: 84, duration: 0.321s, episode steps:  20, steps per second:  62, episode reward: 40.104, mean reward:  2.005 [-2.201, 31.725], mean action: 6.450 [0.000, 15.000],  loss: 0.023747, mae: 0.376657, mean_q: 0.543185, mean_eps: 0.000000
 1935/5000: episode: 85, duration: 0.290s, episode steps:  19, steps per second:  66, episode reward: 35.207, mean reward:  1.853 [-3.000, 32.207], mean action: 5.842 [0.000, 18.000],  loss: 0.022049, mae: 0.365893, mean_q: 0.579097, mean_eps: 0.000000
 1954/5000: episode: 86, duration: 0.321s, episode steps:  19, steps per second:  59, episode reward: 35.895, mean reward:  1.889 [-3.000, 32.185], mean action: 5.368 [0.000, 21.000],  loss: 0.021784, mae: 0.377346, mean_q: 0.533844, mean_eps: 0.000000
 1975/5000: episode: 87, duration: 0.331s, episode steps:  21, steps per second:  63, episode reward: 35.027, mean reward:  1.668 [-3.000, 31.227], mean action: 4.333 [0.000, 19.000],  loss: 0.021055, mae: 0.369913, mean_q: 0.523975, mean_eps: 0.000000
 2005/5000: episode: 88, duration: 0.409s, episode steps:  30, steps per second:  73, episode reward: 35.182, mean reward:  1.173 [-3.000, 31.957], mean action: 3.567 [0.000, 19.000],  loss: 0.021842, mae: 0.382483, mean_q: 0.448704, mean_eps: 0.000000
 2040/5000: episode: 89, duration: 0.469s, episode steps:  35, steps per second:  75, episode reward: -35.450, mean reward: -1.013 [-32.080,  2.700], mean action: 5.286 [0.000, 19.000],  loss: 0.019469, mae: 0.363084, mean_q: 0.565149, mean_eps: 0.000000
 2061/5000: episode: 90, duration: 0.288s, episode steps:  21, steps per second:  73, episode reward: 35.058, mean reward:  1.669 [-2.438, 31.651], mean action: 3.429 [0.000, 11.000],  loss: 0.021850, mae: 0.372614, mean_q: 0.587264, mean_eps: 0.000000
 2093/5000: episode: 91, duration: 0.430s, episode steps:  32, steps per second:  75, episode reward: 32.041, mean reward:  1.001 [-3.000, 32.072], mean action: 6.125 [0.000, 16.000],  loss: 0.019845, mae: 0.366474, mean_q: 0.542818, mean_eps: 0.000000
 2118/5000: episode: 92, duration: 0.345s, episode steps:  25, steps per second:  73, episode reward: 35.251, mean reward:  1.410 [-2.903, 32.091], mean action: 2.800 [0.000, 16.000],  loss: 0.020315, mae: 0.367572, mean_q: 0.520609, mean_eps: 0.000000
 2139/5000: episode: 93, duration: 0.317s, episode steps:  21, steps per second:  66, episode reward: 35.814, mean reward:  1.705 [-3.000, 32.520], mean action: 4.095 [0.000, 16.000],  loss: 0.025012, mae: 0.385941, mean_q: 0.553364, mean_eps: 0.000000
 2158/5000: episode: 94, duration: 0.264s, episode steps:  19, steps per second:  72, episode reward: -36.000, mean reward: -1.895 [-32.120,  2.450], mean action: 5.211 [2.000, 16.000],  loss: 0.021613, mae: 0.371999, mean_q: 0.544408, mean_eps: 0.000000
 2174/5000: episode: 95, duration: 0.459s, episode steps:  16, steps per second:  35, episode reward: 38.903, mean reward:  2.431 [-3.000, 32.573], mean action: 3.562 [1.000, 9.000],  loss: 0.018215, mae: 0.364462, mean_q: 0.578683, mean_eps: 0.000000
 2202/5000: episode: 96, duration: 0.638s, episode steps:  28, steps per second:  44, episode reward: 32.506, mean reward:  1.161 [-2.698, 32.210], mean action: 5.786 [0.000, 19.000],  loss: 0.022799, mae: 0.381714, mean_q: 0.509813, mean_eps: 0.000000
 2222/5000: episode: 97, duration: 0.301s, episode steps:  20, steps per second:  66, episode reward: 35.079, mean reward:  1.754 [-3.000, 32.080], mean action: 3.500 [0.000, 12.000],  loss: 0.016192, mae: 0.356366, mean_q: 0.483785, mean_eps: 0.000000
 2244/5000: episode: 98, duration: 0.336s, episode steps:  22, steps per second:  66, episode reward: 35.535, mean reward:  1.615 [-2.282, 32.535], mean action: 4.045 [0.000, 19.000],  loss: 0.021378, mae: 0.384275, mean_q: 0.484432, mean_eps: 0.000000
 2275/5000: episode: 99, duration: 0.423s, episode steps:  31, steps per second:  73, episode reward: 32.468, mean reward:  1.047 [-2.367, 32.004], mean action: 5.903 [0.000, 20.000],  loss: 0.017847, mae: 0.358899, mean_q: 0.482535, mean_eps: 0.000000
 2317/5000: episode: 100, duration: 0.714s, episode steps:  42, steps per second:  59, episode reward: -32.540, mean reward: -0.775 [-31.976,  2.250], mean action: 6.405 [0.000, 21.000],  loss: 0.020632, mae: 0.372292, mean_q: 0.508432, mean_eps: 0.000000
 2337/5000: episode: 101, duration: 0.284s, episode steps:  20, steps per second:  70, episode reward: 38.562, mean reward:  1.928 [-2.321, 32.248], mean action: 2.550 [0.000, 11.000],  loss: 0.024939, mae: 0.391939, mean_q: 0.504337, mean_eps: 0.000000
 2360/5000: episode: 102, duration: 0.390s, episode steps:  23, steps per second:  59, episode reward: 30.000, mean reward:  1.304 [-2.618, 29.902], mean action: 4.174 [0.000, 15.000],  loss: 0.021645, mae: 0.374626, mean_q: 0.485712, mean_eps: 0.000000
 2382/5000: episode: 103, duration: 0.304s, episode steps:  22, steps per second:  72, episode reward: -32.110, mean reward: -1.460 [-31.862,  2.547], mean action: 7.000 [0.000, 19.000],  loss: 0.021217, mae: 0.367834, mean_q: 0.503245, mean_eps: 0.000000
 2401/5000: episode: 104, duration: 0.537s, episode steps:  19, steps per second:  35, episode reward: 38.751, mean reward:  2.040 [-2.426, 32.491], mean action: 2.211 [0.000, 11.000],  loss: 0.024152, mae: 0.386171, mean_q: 0.517522, mean_eps: 0.000000
 2428/5000: episode: 105, duration: 0.403s, episode steps:  27, steps per second:  67, episode reward: -35.160, mean reward: -1.302 [-31.906,  2.440], mean action: 5.037 [0.000, 20.000],  loss: 0.019980, mae: 0.361744, mean_q: 0.538071, mean_eps: 0.000000
 2448/5000: episode: 106, duration: 0.282s, episode steps:  20, steps per second:  71, episode reward: 38.149, mean reward:  1.907 [-2.786, 32.030], mean action: 2.450 [0.000, 9.000],  loss: 0.015951, mae: 0.348906, mean_q: 0.482711, mean_eps: 0.000000
 2471/5000: episode: 107, duration: 0.363s, episode steps:  23, steps per second:  63, episode reward: -36.000, mean reward: -1.565 [-32.485,  2.414], mean action: 3.739 [0.000, 12.000],  loss: 0.022239, mae: 0.375360, mean_q: 0.498792, mean_eps: 0.000000
 2481/5000: episode: 108, duration: 0.156s, episode steps:  10, steps per second:  64, episode reward: 47.062, mean reward:  4.706 [-0.002, 32.667], mean action: 3.000 [3.000, 3.000],  loss: 0.020184, mae: 0.362930, mean_q: 0.487863, mean_eps: 0.000000
 2501/5000: episode: 109, duration: 0.273s, episode steps:  20, steps per second:  73, episode reward: 37.970, mean reward:  1.899 [-3.000, 31.277], mean action: 4.600 [0.000, 12.000],  loss: 0.020892, mae: 0.369976, mean_q: 0.511390, mean_eps: 0.000000
 2522/5000: episode: 110, duration: 0.284s, episode steps:  21, steps per second:  74, episode reward: -38.470, mean reward: -1.832 [-32.340,  2.941], mean action: 6.571 [0.000, 19.000],  loss: 0.018704, mae: 0.355283, mean_q: 0.485212, mean_eps: 0.000000
 2546/5000: episode: 111, duration: 0.330s, episode steps:  24, steps per second:  73, episode reward: 35.901, mean reward:  1.496 [-2.566, 32.281], mean action: 5.292 [0.000, 21.000],  loss: 0.019098, mae: 0.365269, mean_q: 0.490050, mean_eps: 0.000000
 2574/5000: episode: 112, duration: 0.392s, episode steps:  28, steps per second:  71, episode reward: -32.910, mean reward: -1.175 [-32.126,  2.580], mean action: 8.643 [0.000, 21.000],  loss: 0.019273, mae: 0.365036, mean_q: 0.516503, mean_eps: 0.000000
 2600/5000: episode: 113, duration: 0.366s, episode steps:  26, steps per second:  71, episode reward: -32.910, mean reward: -1.266 [-31.973,  2.181], mean action: 4.577 [0.000, 18.000],  loss: 0.018878, mae: 0.357842, mean_q: 0.529164, mean_eps: 0.000000
 2622/5000: episode: 114, duration: 0.308s, episode steps:  22, steps per second:  71, episode reward: 32.576, mean reward:  1.481 [-3.000, 32.310], mean action: 4.273 [0.000, 15.000],  loss: 0.020484, mae: 0.375740, mean_q: 0.509684, mean_eps: 0.000000
 2669/5000: episode: 115, duration: 0.613s, episode steps:  47, steps per second:  77, episode reward: -33.000, mean reward: -0.702 [-32.030,  2.470], mean action: 4.447 [0.000, 19.000],  loss: 0.020069, mae: 0.370382, mean_q: 0.529467, mean_eps: 0.000000
 2690/5000: episode: 116, duration: 0.289s, episode steps:  21, steps per second:  73, episode reward: 33.000, mean reward:  1.571 [-3.000, 32.810], mean action: 4.571 [0.000, 19.000],  loss: 0.020204, mae: 0.359783, mean_q: 0.577469, mean_eps: 0.000000
 2723/5000: episode: 117, duration: 0.443s, episode steps:  33, steps per second:  74, episode reward: 35.442, mean reward:  1.074 [-2.657, 33.000], mean action: 4.303 [0.000, 19.000],  loss: 0.018903, mae: 0.359745, mean_q: 0.579446, mean_eps: 0.000000
 2743/5000: episode: 118, duration: 0.276s, episode steps:  20, steps per second:  73, episode reward: 35.959, mean reward:  1.798 [-2.374, 32.129], mean action: 5.100 [0.000, 19.000],  loss: 0.017035, mae: 0.355267, mean_q: 0.549996, mean_eps: 0.000000
 2790/5000: episode: 119, duration: 0.646s, episode steps:  47, steps per second:  73, episode reward: 38.256, mean reward:  0.814 [-2.214, 32.370], mean action: 3.468 [0.000, 19.000],  loss: 0.022121, mae: 0.373257, mean_q: 0.529345, mean_eps: 0.000000
 2816/5000: episode: 120, duration: 0.355s, episode steps:  26, steps per second:  73, episode reward: 39.000, mean reward:  1.500 [-2.338, 32.460], mean action: 4.077 [0.000, 19.000],  loss: 0.018514, mae: 0.360644, mean_q: 0.557364, mean_eps: 0.000000
 2839/5000: episode: 121, duration: 0.315s, episode steps:  23, steps per second:  73, episode reward: 35.434, mean reward:  1.541 [-2.610, 32.191], mean action: 4.087 [0.000, 20.000],  loss: 0.021484, mae: 0.374763, mean_q: 0.536864, mean_eps: 0.000000
 2868/5000: episode: 122, duration: 0.389s, episode steps:  29, steps per second:  74, episode reward: 32.409, mean reward:  1.118 [-3.000, 32.400], mean action: 4.690 [0.000, 19.000],  loss: 0.021242, mae: 0.369783, mean_q: 0.565128, mean_eps: 0.000000
 2892/5000: episode: 123, duration: 0.328s, episode steps:  24, steps per second:  73, episode reward: 35.931, mean reward:  1.497 [-2.398, 32.370], mean action: 3.333 [0.000, 15.000],  loss: 0.018967, mae: 0.363381, mean_q: 0.556410, mean_eps: 0.000000
 2909/5000: episode: 124, duration: 0.240s, episode steps:  17, steps per second:  71, episode reward: 41.478, mean reward:  2.440 [-2.331, 32.740], mean action: 2.647 [0.000, 12.000],  loss: 0.022246, mae: 0.375873, mean_q: 0.545477, mean_eps: 0.000000
 2927/5000: episode: 125, duration: 0.255s, episode steps:  18, steps per second:  71, episode reward: 38.353, mean reward:  2.131 [-3.000, 33.000], mean action: 6.000 [0.000, 15.000],  loss: 0.023233, mae: 0.381142, mean_q: 0.522792, mean_eps: 0.000000
 2949/5000: episode: 126, duration: 0.307s, episode steps:  22, steps per second:  72, episode reward: 32.345, mean reward:  1.470 [-2.902, 32.696], mean action: 5.545 [0.000, 15.000],  loss: 0.028283, mae: 0.404942, mean_q: 0.461835, mean_eps: 0.000000
 2977/5000: episode: 127, duration: 0.391s, episode steps:  28, steps per second:  72, episode reward: 35.601, mean reward:  1.271 [-2.384, 32.115], mean action: 6.143 [0.000, 15.000],  loss: 0.021826, mae: 0.381376, mean_q: 0.472928, mean_eps: 0.000000
 3003/5000: episode: 128, duration: 0.350s, episode steps:  26, steps per second:  74, episode reward: 38.004, mean reward:  1.462 [-2.793, 32.160], mean action: 4.885 [0.000, 19.000],  loss: 0.019653, mae: 0.369244, mean_q: 0.502719, mean_eps: 0.000000
 3041/5000: episode: 129, duration: 0.526s, episode steps:  38, steps per second:  72, episode reward: 38.585, mean reward:  1.015 [-2.315, 32.360], mean action: 2.316 [0.000, 17.000],  loss: 0.021307, mae: 0.380426, mean_q: 0.507864, mean_eps: 0.000000
 3071/5000: episode: 130, duration: 0.411s, episode steps:  30, steps per second:  73, episode reward: 32.510, mean reward:  1.084 [-2.571, 32.195], mean action: 6.400 [0.000, 20.000],  loss: 0.017423, mae: 0.362316, mean_q: 0.488904, mean_eps: 0.000000
 3112/5000: episode: 131, duration: 0.902s, episode steps:  41, steps per second:  45, episode reward: 40.910, mean reward:  0.998 [-2.627, 32.190], mean action: 5.610 [0.000, 20.000],  loss: 0.021295, mae: 0.378890, mean_q: 0.499857, mean_eps: 0.000000
 3126/5000: episode: 132, duration: 0.220s, episode steps:  14, steps per second:  64, episode reward: 44.091, mean reward:  3.149 [-2.328, 31.862], mean action: 3.357 [0.000, 15.000],  loss: 0.024806, mae: 0.394847, mean_q: 0.518090, mean_eps: 0.000000
 3151/5000: episode: 133, duration: 0.354s, episode steps:  25, steps per second:  71, episode reward: 32.844, mean reward:  1.314 [-3.000, 33.000], mean action: 6.320 [0.000, 20.000],  loss: 0.019578, mae: 0.369983, mean_q: 0.520238, mean_eps: 0.000000
 3175/5000: episode: 134, duration: 0.364s, episode steps:  24, steps per second:  66, episode reward: 38.118, mean reward:  1.588 [-2.378, 32.210], mean action: 8.375 [1.000, 21.000],  loss: 0.018868, mae: 0.367641, mean_q: 0.534239, mean_eps: 0.000000
 3201/5000: episode: 135, duration: 0.352s, episode steps:  26, steps per second:  74, episode reward: 33.000, mean reward:  1.269 [-2.710, 33.000], mean action: 6.423 [1.000, 16.000],  loss: 0.019788, mae: 0.371825, mean_q: 0.533717, mean_eps: 0.000000
 3224/5000: episode: 136, duration: 0.564s, episode steps:  23, steps per second:  41, episode reward: -38.790, mean reward: -1.687 [-32.129,  2.310], mean action: 6.087 [0.000, 16.000],  loss: 0.023009, mae: 0.383520, mean_q: 0.567368, mean_eps: 0.000000
 3251/5000: episode: 137, duration: 0.417s, episode steps:  27, steps per second:  65, episode reward: -32.780, mean reward: -1.214 [-32.033,  2.430], mean action: 5.444 [0.000, 19.000],  loss: 0.019137, mae: 0.362398, mean_q: 0.593446, mean_eps: 0.000000
 3283/5000: episode: 138, duration: 0.462s, episode steps:  32, steps per second:  69, episode reward: -35.430, mean reward: -1.107 [-31.959,  2.540], mean action: 2.875 [0.000, 16.000],  loss: 0.019320, mae: 0.384445, mean_q: 0.573603, mean_eps: 0.000000
 3303/5000: episode: 139, duration: 0.273s, episode steps:  20, steps per second:  73, episode reward: 32.479, mean reward:  1.624 [-3.000, 32.049], mean action: 4.800 [0.000, 16.000],  loss: 0.020203, mae: 0.377306, mean_q: 0.589816, mean_eps: 0.000000
 3323/5000: episode: 140, duration: 0.270s, episode steps:  20, steps per second:  74, episode reward: 32.886, mean reward:  1.644 [-2.609, 32.066], mean action: 4.900 [0.000, 16.000],  loss: 0.023445, mae: 0.386906, mean_q: 0.618232, mean_eps: 0.000000
 3342/5000: episode: 141, duration: 0.261s, episode steps:  19, steps per second:  73, episode reward: -41.110, mean reward: -2.164 [-32.700,  2.053], mean action: 6.684 [1.000, 16.000],  loss: 0.018180, mae: 0.370716, mean_q: 0.617805, mean_eps: 0.000000
 3363/5000: episode: 142, duration: 0.295s, episode steps:  21, steps per second:  71, episode reward: 38.407, mean reward:  1.829 [-2.819, 33.000], mean action: 2.000 [0.000, 13.000],  loss: 0.018834, mae: 0.367139, mean_q: 0.564645, mean_eps: 0.000000
 3383/5000: episode: 143, duration: 0.293s, episode steps:  20, steps per second:  68, episode reward: -41.770, mean reward: -2.088 [-33.000,  2.680], mean action: 4.700 [0.000, 15.000],  loss: 0.021365, mae: 0.378124, mean_q: 0.570472, mean_eps: 0.000000
 3406/5000: episode: 144, duration: 0.326s, episode steps:  23, steps per second:  71, episode reward: 38.046, mean reward:  1.654 [-2.534, 32.250], mean action: 7.478 [1.000, 21.000],  loss: 0.018452, mae: 0.360389, mean_q: 0.536094, mean_eps: 0.000000
 3443/5000: episode: 145, duration: 0.497s, episode steps:  37, steps per second:  74, episode reward: -38.490, mean reward: -1.040 [-31.916,  2.340], mean action: 9.946 [1.000, 19.000],  loss: 0.019592, mae: 0.366101, mean_q: 0.525101, mean_eps: 0.000000
 3466/5000: episode: 146, duration: 0.339s, episode steps:  23, steps per second:  68, episode reward: 37.308, mean reward:  1.622 [-2.720, 32.420], mean action: 4.304 [0.000, 15.000],  loss: 0.019712, mae: 0.366787, mean_q: 0.538017, mean_eps: 0.000000
 3487/5000: episode: 147, duration: 0.383s, episode steps:  21, steps per second:  55, episode reward: 38.450, mean reward:  1.831 [-2.268, 32.508], mean action: 4.524 [0.000, 15.000],  loss: 0.021410, mae: 0.371037, mean_q: 0.511777, mean_eps: 0.000000
 3514/5000: episode: 148, duration: 0.372s, episode steps:  27, steps per second:  73, episode reward: -32.240, mean reward: -1.194 [-32.248,  2.138], mean action: 4.407 [0.000, 15.000],  loss: 0.018332, mae: 0.358686, mean_q: 0.496863, mean_eps: 0.000000
 3533/5000: episode: 149, duration: 0.261s, episode steps:  19, steps per second:  73, episode reward: 37.560, mean reward:  1.977 [-2.556, 32.772], mean action: 4.632 [0.000, 14.000],  loss: 0.018541, mae: 0.357724, mean_q: 0.494887, mean_eps: 0.000000
 3558/5000: episode: 150, duration: 0.339s, episode steps:  25, steps per second:  74, episode reward: 35.614, mean reward:  1.425 [-3.000, 32.904], mean action: 3.800 [0.000, 14.000],  loss: 0.022121, mae: 0.377803, mean_q: 0.558077, mean_eps: 0.000000
 3574/5000: episode: 151, duration: 0.222s, episode steps:  16, steps per second:  72, episode reward: 38.457, mean reward:  2.404 [-2.186, 32.430], mean action: 3.188 [0.000, 11.000],  loss: 0.017323, mae: 0.360949, mean_q: 0.464479, mean_eps: 0.000000
 3603/5000: episode: 152, duration: 0.399s, episode steps:  29, steps per second:  73, episode reward: -35.870, mean reward: -1.237 [-32.336,  2.502], mean action: 6.241 [0.000, 14.000],  loss: 0.017999, mae: 0.363325, mean_q: 0.506232, mean_eps: 0.000000
 3632/5000: episode: 153, duration: 0.428s, episode steps:  29, steps per second:  68, episode reward: 32.196, mean reward:  1.110 [-2.347, 32.401], mean action: 5.483 [0.000, 19.000],  loss: 0.019376, mae: 0.357798, mean_q: 0.535655, mean_eps: 0.000000
 3649/5000: episode: 154, duration: 0.240s, episode steps:  17, steps per second:  71, episode reward: 41.007, mean reward:  2.412 [-2.536, 32.116], mean action: 2.941 [0.000, 11.000],  loss: 0.019954, mae: 0.360144, mean_q: 0.518899, mean_eps: 0.000000
 3668/5000: episode: 155, duration: 0.272s, episode steps:  19, steps per second:  70, episode reward: 35.590, mean reward:  1.873 [-2.900, 32.280], mean action: 3.526 [0.000, 19.000],  loss: 0.022332, mae: 0.384134, mean_q: 0.541167, mean_eps: 0.000000
 3705/5000: episode: 156, duration: 0.545s, episode steps:  37, steps per second:  68, episode reward: -35.210, mean reward: -0.952 [-33.000,  2.903], mean action: 3.703 [0.000, 19.000],  loss: 0.019767, mae: 0.365169, mean_q: 0.521977, mean_eps: 0.000000
 3723/5000: episode: 157, duration: 0.256s, episode steps:  18, steps per second:  70, episode reward: 38.470, mean reward:  2.137 [-2.462, 32.470], mean action: 3.611 [0.000, 19.000],  loss: 0.023584, mae: 0.382859, mean_q: 0.523569, mean_eps: 0.000000
 3745/5000: episode: 158, duration: 0.322s, episode steps:  22, steps per second:  68, episode reward: 32.471, mean reward:  1.476 [-3.000, 32.420], mean action: 4.727 [0.000, 19.000],  loss: 0.021786, mae: 0.373410, mean_q: 0.527235, mean_eps: 0.000000
 3763/5000: episode: 159, duration: 0.280s, episode steps:  18, steps per second:  64, episode reward: -38.180, mean reward: -2.121 [-31.710,  2.562], mean action: 3.889 [0.000, 16.000],  loss: 0.019512, mae: 0.352291, mean_q: 0.539172, mean_eps: 0.000000
 3787/5000: episode: 160, duration: 0.463s, episode steps:  24, steps per second:  52, episode reward: -36.000, mean reward: -1.500 [-32.280,  2.640], mean action: 6.750 [0.000, 20.000],  loss: 0.019211, mae: 0.357634, mean_q: 0.503878, mean_eps: 0.000000
 3818/5000: episode: 161, duration: 0.413s, episode steps:  31, steps per second:  75, episode reward: -32.170, mean reward: -1.038 [-31.339,  2.471], mean action: 7.710 [0.000, 16.000],  loss: 0.017441, mae: 0.347898, mean_q: 0.499034, mean_eps: 0.000000
 3856/5000: episode: 162, duration: 0.518s, episode steps:  38, steps per second:  73, episode reward: -32.130, mean reward: -0.846 [-32.124,  3.086], mean action: 13.395 [0.000, 20.000],  loss: 0.022461, mae: 0.375462, mean_q: 0.551874, mean_eps: 0.000000
 3874/5000: episode: 163, duration: 0.242s, episode steps:  18, steps per second:  74, episode reward: -36.000, mean reward: -2.000 [-33.000,  3.000], mean action: 8.333 [1.000, 16.000],  loss: 0.021184, mae: 0.360251, mean_q: 0.505675, mean_eps: 0.000000
 3897/5000: episode: 164, duration: 0.320s, episode steps:  23, steps per second:  72, episode reward: -32.870, mean reward: -1.429 [-33.000,  3.000], mean action: 5.696 [0.000, 21.000],  loss: 0.020002, mae: 0.358412, mean_q: 0.512656, mean_eps: 0.000000
 3917/5000: episode: 165, duration: 0.295s, episode steps:  20, steps per second:  68, episode reward: 42.000, mean reward:  2.100 [-2.245, 32.310], mean action: 2.550 [0.000, 16.000],  loss: 0.015448, mae: 0.341326, mean_q: 0.532541, mean_eps: 0.000000
 3942/5000: episode: 166, duration: 0.335s, episode steps:  25, steps per second:  75, episode reward: 32.645, mean reward:  1.306 [-3.000, 32.645], mean action: 4.080 [0.000, 15.000],  loss: 0.019586, mae: 0.364595, mean_q: 0.480884, mean_eps: 0.000000
 3963/5000: episode: 167, duration: 0.457s, episode steps:  21, steps per second:  46, episode reward: -32.900, mean reward: -1.567 [-32.316,  2.320], mean action: 3.476 [0.000, 9.000],  loss: 0.019778, mae: 0.361506, mean_q: 0.470091, mean_eps: 0.000000
 3990/5000: episode: 168, duration: 0.399s, episode steps:  27, steps per second:  68, episode reward: -33.000, mean reward: -1.222 [-32.181,  2.260], mean action: 4.926 [0.000, 15.000],  loss: 0.020251, mae: 0.361028, mean_q: 0.505908, mean_eps: 0.000000
 4010/5000: episode: 169, duration: 0.302s, episode steps:  20, steps per second:  66, episode reward: 38.369, mean reward:  1.918 [-2.969, 31.891], mean action: 3.250 [0.000, 16.000],  loss: 0.020644, mae: 0.360042, mean_q: 0.468073, mean_eps: 0.000000
 4023/5000: episode: 170, duration: 0.191s, episode steps:  13, steps per second:  68, episode reward: 44.620, mean reward:  3.432 [-2.146, 32.360], mean action: 1.923 [0.000, 19.000],  loss: 0.020360, mae: 0.359792, mean_q: 0.483431, mean_eps: 0.000000
 4043/5000: episode: 171, duration: 0.552s, episode steps:  20, steps per second:  36, episode reward: 32.040, mean reward:  1.602 [-3.000, 32.028], mean action: 5.250 [0.000, 19.000],  loss: 0.024159, mae: 0.375923, mean_q: 0.506288, mean_eps: 0.000000
 4063/5000: episode: 172, duration: 0.366s, episode steps:  20, steps per second:  55, episode reward: 35.712, mean reward:  1.786 [-3.000, 32.285], mean action: 5.750 [0.000, 20.000],  loss: 0.021615, mae: 0.367826, mean_q: 0.481650, mean_eps: 0.000000
 4080/5000: episode: 173, duration: 0.231s, episode steps:  17, steps per second:  74, episode reward: -41.470, mean reward: -2.439 [-32.367,  2.540], mean action: 6.059 [0.000, 19.000],  loss: 0.021991, mae: 0.368721, mean_q: 0.527631, mean_eps: 0.000000
 4100/5000: episode: 174, duration: 0.457s, episode steps:  20, steps per second:  44, episode reward: -39.000, mean reward: -1.950 [-32.586,  2.241], mean action: 5.100 [1.000, 16.000],  loss: 0.020390, mae: 0.361094, mean_q: 0.547122, mean_eps: 0.000000
 4130/5000: episode: 175, duration: 0.619s, episode steps:  30, steps per second:  48, episode reward: -32.670, mean reward: -1.089 [-32.113,  2.310], mean action: 7.733 [0.000, 19.000],  loss: 0.020998, mae: 0.374567, mean_q: 0.479062, mean_eps: 0.000000
 4151/5000: episode: 176, duration: 0.286s, episode steps:  21, steps per second:  73, episode reward: 35.376, mean reward:  1.685 [-2.901, 32.100], mean action: 4.381 [0.000, 19.000],  loss: 0.018642, mae: 0.364733, mean_q: 0.487568, mean_eps: 0.000000
 4171/5000: episode: 177, duration: 0.444s, episode steps:  20, steps per second:  45, episode reward: 44.276, mean reward:  2.214 [-2.144, 32.169], mean action: 1.450 [0.000, 19.000],  loss: 0.020409, mae: 0.374834, mean_q: 0.462332, mean_eps: 0.000000
 4206/5000: episode: 178, duration: 0.608s, episode steps:  35, steps per second:  58, episode reward: 35.324, mean reward:  1.009 [-2.264, 32.910], mean action: 4.914 [0.000, 20.000],  loss: 0.018911, mae: 0.364854, mean_q: 0.482784, mean_eps: 0.000000
 4251/5000: episode: 179, duration: 0.593s, episode steps:  45, steps per second:  76, episode reward: 32.983, mean reward:  0.733 [-3.000, 32.282], mean action: 7.111 [0.000, 16.000],  loss: 0.020814, mae: 0.371426, mean_q: 0.511494, mean_eps: 0.000000
 4275/5000: episode: 180, duration: 0.327s, episode steps:  24, steps per second:  73, episode reward: 37.825, mean reward:  1.576 [-3.000, 32.570], mean action: 5.083 [0.000, 19.000],  loss: 0.018357, mae: 0.365797, mean_q: 0.460200, mean_eps: 0.000000
 4292/5000: episode: 181, duration: 0.244s, episode steps:  17, steps per second:  70, episode reward: 38.821, mean reward:  2.284 [-2.170, 32.281], mean action: 3.941 [0.000, 12.000],  loss: 0.020359, mae: 0.375015, mean_q: 0.449228, mean_eps: 0.000000
 4315/5000: episode: 182, duration: 0.323s, episode steps:  23, steps per second:  71, episode reward: 41.084, mean reward:  1.786 [-3.000, 31.957], mean action: 4.087 [0.000, 13.000],  loss: 0.019866, mae: 0.372426, mean_q: 0.484277, mean_eps: 0.000000
 4340/5000: episode: 183, duration: 0.334s, episode steps:  25, steps per second:  75, episode reward: 35.926, mean reward:  1.437 [-2.904, 32.080], mean action: 4.000 [0.000, 12.000],  loss: 0.018833, mae: 0.366803, mean_q: 0.471196, mean_eps: 0.000000
 4362/5000: episode: 184, duration: 0.317s, episode steps:  22, steps per second:  69, episode reward: 32.865, mean reward:  1.494 [-2.292, 32.865], mean action: 4.545 [0.000, 11.000],  loss: 0.027356, mae: 0.404729, mean_q: 0.486291, mean_eps: 0.000000
 4380/5000: episode: 185, duration: 0.250s, episode steps:  18, steps per second:  72, episode reward: -35.040, mean reward: -1.947 [-31.970,  2.940], mean action: 5.667 [0.000, 15.000],  loss: 0.022477, mae: 0.378294, mean_q: 0.514447, mean_eps: 0.000000
 4402/5000: episode: 186, duration: 0.303s, episode steps:  22, steps per second:  73, episode reward: 35.397, mean reward:  1.609 [-3.000, 33.000], mean action: 9.909 [0.000, 20.000],  loss: 0.019645, mae: 0.370546, mean_q: 0.481396, mean_eps: 0.000000
 4429/5000: episode: 187, duration: 0.363s, episode steps:  27, steps per second:  74, episode reward: 37.657, mean reward:  1.395 [-3.000, 32.120], mean action: 7.037 [0.000, 20.000],  loss: 0.020851, mae: 0.374398, mean_q: 0.488771, mean_eps: 0.000000
 4452/5000: episode: 188, duration: 0.309s, episode steps:  23, steps per second:  74, episode reward: -33.000, mean reward: -1.435 [-30.361,  2.612], mean action: 4.609 [0.000, 16.000],  loss: 0.022066, mae: 0.378129, mean_q: 0.530747, mean_eps: 0.000000
 4496/5000: episode: 189, duration: 0.575s, episode steps:  44, steps per second:  77, episode reward: 32.500, mean reward:  0.739 [-2.359, 32.455], mean action: 6.977 [0.000, 16.000],  loss: 0.023352, mae: 0.382174, mean_q: 0.546323, mean_eps: 0.000000
 4515/5000: episode: 190, duration: 0.263s, episode steps:  19, steps per second:  72, episode reward: 35.544, mean reward:  1.871 [-3.000, 32.460], mean action: 4.263 [0.000, 16.000],  loss: 0.022142, mae: 0.369565, mean_q: 0.582188, mean_eps: 0.000000
 4534/5000: episode: 191, duration: 0.274s, episode steps:  19, steps per second:  69, episode reward: 40.132, mean reward:  2.112 [-3.000, 32.240], mean action: 4.316 [0.000, 16.000],  loss: 0.018641, mae: 0.360849, mean_q: 0.575515, mean_eps: 0.000000
 4559/5000: episode: 192, duration: 0.344s, episode steps:  25, steps per second:  73, episode reward: 35.014, mean reward:  1.401 [-2.408, 32.090], mean action: 4.760 [0.000, 19.000],  loss: 0.021107, mae: 0.373053, mean_q: 0.563029, mean_eps: 0.000000
 4579/5000: episode: 193, duration: 0.271s, episode steps:  20, steps per second:  74, episode reward: 35.579, mean reward:  1.779 [-3.000, 31.829], mean action: 4.250 [0.000, 19.000],  loss: 0.019204, mae: 0.365612, mean_q: 0.551602, mean_eps: 0.000000
 4594/5000: episode: 194, duration: 0.215s, episode steps:  15, steps per second:  70, episode reward: 41.458, mean reward:  2.764 [-2.785, 32.754], mean action: 3.600 [0.000, 19.000],  loss: 0.018897, mae: 0.364544, mean_q: 0.536421, mean_eps: 0.000000
 4601/5000: episode: 195, duration: 0.112s, episode steps:   7, steps per second:  62, episode reward: 47.400, mean reward:  6.771 [-0.350, 33.000], mean action: 2.571 [0.000, 12.000],  loss: 0.015598, mae: 0.350595, mean_q: 0.502521, mean_eps: 0.000000
 4625/5000: episode: 196, duration: 0.322s, episode steps:  24, steps per second:  75, episode reward: 35.542, mean reward:  1.481 [-2.848, 32.340], mean action: 5.375 [0.000, 15.000],  loss: 0.023091, mae: 0.387226, mean_q: 0.480592, mean_eps: 0.000000
 4654/5000: episode: 197, duration: 0.391s, episode steps:  29, steps per second:  74, episode reward: 35.895, mean reward:  1.238 [-2.484, 32.105], mean action: 2.103 [0.000, 12.000],  loss: 0.017799, mae: 0.359743, mean_q: 0.543398, mean_eps: 0.000000
 4678/5000: episode: 198, duration: 0.325s, episode steps:  24, steps per second:  74, episode reward: 36.000, mean reward:  1.500 [-3.000, 32.600], mean action: 4.208 [0.000, 18.000],  loss: 0.017843, mae: 0.357508, mean_q: 0.555534, mean_eps: 0.000000
 4702/5000: episode: 199, duration: 0.408s, episode steps:  24, steps per second:  59, episode reward: 38.285, mean reward:  1.595 [-2.170, 32.001], mean action: 5.000 [0.000, 16.000],  loss: 0.018540, mae: 0.360952, mean_q: 0.528533, mean_eps: 0.000000
 4737/5000: episode: 200, duration: 0.519s, episode steps:  35, steps per second:  67, episode reward: -38.510, mean reward: -1.100 [-31.865,  2.739], mean action: 3.886 [0.000, 19.000],  loss: 0.023998, mae: 0.386749, mean_q: 0.535045, mean_eps: 0.000000
 4767/5000: episode: 201, duration: 0.401s, episode steps:  30, steps per second:  75, episode reward: 32.725, mean reward:  1.091 [-2.820, 31.913], mean action: 7.633 [0.000, 20.000],  loss: 0.021339, mae: 0.380817, mean_q: 0.494946, mean_eps: 0.000000
 4792/5000: episode: 202, duration: 0.342s, episode steps:  25, steps per second:  73, episode reward: 32.255, mean reward:  1.290 [-2.867, 32.351], mean action: 4.640 [0.000, 19.000],  loss: 0.017401, mae: 0.363878, mean_q: 0.545541, mean_eps: 0.000000
 4828/5000: episode: 203, duration: 0.474s, episode steps:  36, steps per second:  76, episode reward: 32.660, mean reward:  0.907 [-2.548, 32.070], mean action: 7.500 [0.000, 19.000],  loss: 0.022585, mae: 0.386540, mean_q: 0.536179, mean_eps: 0.000000
 4848/5000: episode: 204, duration: 0.276s, episode steps:  20, steps per second:  73, episode reward: -32.470, mean reward: -1.623 [-32.470,  3.000], mean action: 4.450 [0.000, 19.000],  loss: 0.016112, mae: 0.355843, mean_q: 0.585547, mean_eps: 0.000000
 4871/5000: episode: 205, duration: 0.318s, episode steps:  23, steps per second:  72, episode reward: 35.664, mean reward:  1.551 [-3.000, 32.173], mean action: 4.826 [0.000, 19.000],  loss: 0.020599, mae: 0.369254, mean_q: 0.591292, mean_eps: 0.000000
 4900/5000: episode: 206, duration: 0.389s, episode steps:  29, steps per second:  75, episode reward: -35.030, mean reward: -1.208 [-32.235,  2.280], mean action: 3.931 [0.000, 20.000],  loss: 0.021545, mae: 0.382030, mean_q: 0.539823, mean_eps: 0.000000
 4921/5000: episode: 207, duration: 0.295s, episode steps:  21, steps per second:  71, episode reward: 36.000, mean reward:  1.714 [-3.000, 30.229], mean action: 4.095 [0.000, 16.000],  loss: 0.023568, mae: 0.391959, mean_q: 0.510288, mean_eps: 0.000000
 4944/5000: episode: 208, duration: 0.314s, episode steps:  23, steps per second:  73, episode reward: 35.674, mean reward:  1.551 [-2.304, 33.000], mean action: 4.087 [1.000, 15.000],  loss: 0.020461, mae: 0.374228, mean_q: 0.525484, mean_eps: 0.000000
 4970/5000: episode: 209, duration: 0.362s, episode steps:  26, steps per second:  72, episode reward: 39.743, mean reward:  1.529 [-2.091, 32.362], mean action: 5.692 [1.000, 20.000],  loss: 0.022373, mae: 0.383623, mean_q: 0.537370, mean_eps: 0.000000
 4993/5000: episode: 210, duration: 0.317s, episode steps:  23, steps per second:  73, episode reward: -35.330, mean reward: -1.536 [-32.320,  2.290], mean action: 7.348 [0.000, 16.000],  loss: 0.018507, mae: 0.369155, mean_q: 0.537410, mean_eps: 0.000000
done, took 69.840 seconds
DQN Evaluation: 5446 victories out of 6444 episodes
Training for 5000 steps ...
   26/5000: episode: 1, duration: 0.199s, episode steps:  26, steps per second: 131, episode reward: 42.932, mean reward:  1.651 [-2.396, 31.956], mean action: 5.385 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   58/5000: episode: 2, duration: 0.204s, episode steps:  32, steps per second: 157, episode reward: 41.408, mean reward:  1.294 [-2.424, 32.251], mean action: 2.469 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   83/5000: episode: 3, duration: 0.164s, episode steps:  25, steps per second: 153, episode reward: 35.183, mean reward:  1.407 [-3.000, 32.110], mean action: 3.200 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  103/5000: episode: 4, duration: 0.141s, episode steps:  20, steps per second: 142, episode reward: 41.417, mean reward:  2.071 [-3.000, 31.697], mean action: 5.100 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  123/5000: episode: 5, duration: 0.137s, episode steps:  20, steps per second: 146, episode reward: 41.300, mean reward:  2.065 [-2.113, 32.290], mean action: 3.050 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  146/5000: episode: 6, duration: 0.156s, episode steps:  23, steps per second: 148, episode reward: 41.544, mean reward:  1.806 [-3.000, 32.070], mean action: 2.174 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  163/5000: episode: 7, duration: 0.124s, episode steps:  17, steps per second: 137, episode reward: 43.794, mean reward:  2.576 [-2.416, 31.955], mean action: 2.941 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  236/5000: episode: 8, duration: 0.450s, episode steps:  73, steps per second: 162, episode reward: 32.014, mean reward:  0.439 [-3.000, 32.100], mean action: 2.438 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  253/5000: episode: 9, duration: 0.122s, episode steps:  17, steps per second: 139, episode reward: 40.166, mean reward:  2.363 [-2.322, 32.273], mean action: 4.000 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  276/5000: episode: 10, duration: 0.154s, episode steps:  23, steps per second: 149, episode reward: 40.648, mean reward:  1.767 [-2.819, 32.120], mean action: 7.609 [1.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  299/5000: episode: 11, duration: 0.157s, episode steps:  23, steps per second: 147, episode reward: 38.077, mean reward:  1.656 [-3.000, 32.250], mean action: 3.826 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  324/5000: episode: 12, duration: 0.168s, episode steps:  25, steps per second: 149, episode reward: 38.430, mean reward:  1.537 [-2.998, 31.800], mean action: 5.240 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  346/5000: episode: 13, duration: 0.150s, episode steps:  22, steps per second: 147, episode reward: 41.461, mean reward:  1.885 [-3.000, 31.651], mean action: 1.364 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  369/5000: episode: 14, duration: 0.156s, episode steps:  23, steps per second: 147, episode reward: 41.273, mean reward:  1.794 [-2.149, 32.250], mean action: 2.391 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  395/5000: episode: 15, duration: 0.162s, episode steps:  26, steps per second: 161, episode reward: 39.000, mean reward:  1.500 [-3.000, 32.090], mean action: 3.192 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  415/5000: episode: 16, duration: 0.132s, episode steps:  20, steps per second: 151, episode reward: 41.318, mean reward:  2.066 [-3.000, 33.000], mean action: 4.450 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  439/5000: episode: 17, duration: 0.180s, episode steps:  24, steps per second: 133, episode reward: 38.449, mean reward:  1.602 [-3.000, 32.416], mean action: 4.583 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  491/5000: episode: 18, duration: 0.320s, episode steps:  52, steps per second: 163, episode reward: 34.735, mean reward:  0.668 [-2.504, 32.280], mean action: 5.635 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  605/5000: episode: 19, duration: 0.695s, episode steps: 114, steps per second: 164, episode reward: -35.810, mean reward: -0.314 [-32.442,  2.800], mean action: 7.614 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  635/5000: episode: 20, duration: 0.195s, episode steps:  30, steps per second: 154, episode reward: 41.026, mean reward:  1.368 [-2.732, 31.879], mean action: 2.767 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  666/5000: episode: 21, duration: 0.199s, episode steps:  31, steps per second: 156, episode reward: 41.817, mean reward:  1.349 [-2.783, 32.720], mean action: 3.742 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  707/5000: episode: 22, duration: 0.249s, episode steps:  41, steps per second: 165, episode reward: 35.038, mean reward:  0.855 [-2.353, 32.110], mean action: 6.659 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  730/5000: episode: 23, duration: 0.163s, episode steps:  23, steps per second: 142, episode reward: 38.580, mean reward:  1.677 [-3.000, 31.620], mean action: 4.957 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  760/5000: episode: 24, duration: 0.181s, episode steps:  30, steps per second: 166, episode reward: 40.806, mean reward:  1.360 [-2.803, 31.637], mean action: 3.900 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  783/5000: episode: 25, duration: 0.159s, episode steps:  23, steps per second: 145, episode reward: 46.564, mean reward:  2.025 [-0.236, 32.020], mean action: 5.609 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  809/5000: episode: 26, duration: 0.165s, episode steps:  26, steps per second: 157, episode reward: 38.646, mean reward:  1.486 [-2.270, 31.696], mean action: 4.154 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  854/5000: episode: 27, duration: 0.279s, episode steps:  45, steps per second: 161, episode reward: 42.000, mean reward:  0.933 [-2.952, 32.240], mean action: 1.400 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  885/5000: episode: 28, duration: 0.196s, episode steps:  31, steps per second: 158, episode reward: 40.232, mean reward:  1.298 [-2.139, 31.879], mean action: 3.613 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  909/5000: episode: 29, duration: 0.159s, episode steps:  24, steps per second: 151, episode reward: 38.567, mean reward:  1.607 [-3.000, 31.617], mean action: 6.417 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  940/5000: episode: 30, duration: 0.196s, episode steps:  31, steps per second: 158, episode reward: 36.000, mean reward:  1.161 [-3.000, 32.060], mean action: 3.097 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  981/5000: episode: 31, duration: 0.263s, episode steps:  41, steps per second: 156, episode reward: 41.715, mean reward:  1.017 [-2.400, 32.050], mean action: 2.390 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1005/5000: episode: 32, duration: 0.199s, episode steps:  24, steps per second: 121, episode reward: 41.903, mean reward:  1.746 [-2.245, 32.310], mean action: 2.667 [0.000, 12.000],  loss: 0.023970, mae: 0.386294, mean_q: 0.521271, mean_eps: 0.000000
 1026/5000: episode: 33, duration: 0.292s, episode steps:  21, steps per second:  72, episode reward: 41.645, mean reward:  1.983 [-3.000, 32.510], mean action: 2.095 [0.000, 15.000],  loss: 0.024827, mae: 0.399353, mean_q: 0.518005, mean_eps: 0.000000
 1051/5000: episode: 34, duration: 0.372s, episode steps:  25, steps per second:  67, episode reward: 41.648, mean reward:  1.666 [-3.000, 32.030], mean action: 2.560 [0.000, 20.000],  loss: 0.019031, mae: 0.362301, mean_q: 0.524198, mean_eps: 0.000000
 1067/5000: episode: 35, duration: 0.225s, episode steps:  16, steps per second:  71, episode reward: 44.470, mean reward:  2.779 [-3.000, 32.500], mean action: 4.688 [0.000, 14.000],  loss: 0.022624, mae: 0.372839, mean_q: 0.520508, mean_eps: 0.000000
 1096/5000: episode: 36, duration: 0.389s, episode steps:  29, steps per second:  75, episode reward: 35.668, mean reward:  1.230 [-2.266, 31.846], mean action: 5.103 [0.000, 21.000],  loss: 0.019892, mae: 0.359049, mean_q: 0.537668, mean_eps: 0.000000
 1149/5000: episode: 37, duration: 0.710s, episode steps:  53, steps per second:  75, episode reward: 41.278, mean reward:  0.779 [-2.259, 32.030], mean action: 3.094 [0.000, 15.000],  loss: 0.020830, mae: 0.369596, mean_q: 0.537194, mean_eps: 0.000000
 1184/5000: episode: 38, duration: 0.569s, episode steps:  35, steps per second:  61, episode reward: 40.887, mean reward:  1.168 [-2.364, 32.230], mean action: 2.943 [0.000, 13.000],  loss: 0.022592, mae: 0.382723, mean_q: 0.520303, mean_eps: 0.000000
 1213/5000: episode: 39, duration: 0.394s, episode steps:  29, steps per second:  74, episode reward: 35.939, mean reward:  1.239 [-2.500, 32.340], mean action: 4.517 [0.000, 18.000],  loss: 0.017701, mae: 0.355927, mean_q: 0.559230, mean_eps: 0.000000
 1245/5000: episode: 40, duration: 0.432s, episode steps:  32, steps per second:  74, episode reward: 41.375, mean reward:  1.293 [-3.000, 32.606], mean action: 3.438 [0.000, 14.000],  loss: 0.020488, mae: 0.364051, mean_q: 0.558348, mean_eps: 0.000000
 1281/5000: episode: 41, duration: 0.481s, episode steps:  36, steps per second:  75, episode reward: 44.595, mean reward:  1.239 [-2.204, 32.130], mean action: 3.167 [3.000, 9.000],  loss: 0.020959, mae: 0.366688, mean_q: 0.534453, mean_eps: 0.000000
 1302/5000: episode: 42, duration: 0.307s, episode steps:  21, steps per second:  68, episode reward: 41.478, mean reward:  1.975 [-2.794, 31.959], mean action: 1.762 [0.000, 14.000],  loss: 0.017457, mae: 0.349968, mean_q: 0.509216, mean_eps: 0.000000
 1340/5000: episode: 43, duration: 0.523s, episode steps:  38, steps per second:  73, episode reward: 47.634, mean reward:  1.254 [-0.517, 32.440], mean action: 2.658 [0.000, 14.000],  loss: 0.019733, mae: 0.359102, mean_q: 0.508076, mean_eps: 0.000000
 1414/5000: episode: 44, duration: 0.977s, episode steps:  74, steps per second:  76, episode reward: 44.499, mean reward:  0.601 [-2.188, 32.070], mean action: 1.676 [0.000, 20.000],  loss: 0.018673, mae: 0.363823, mean_q: 0.479986, mean_eps: 0.000000
 1438/5000: episode: 45, duration: 0.350s, episode steps:  24, steps per second:  69, episode reward: 41.401, mean reward:  1.725 [-2.400, 32.390], mean action: 2.375 [0.000, 9.000],  loss: 0.019438, mae: 0.358788, mean_q: 0.523724, mean_eps: 0.000000
 1462/5000: episode: 46, duration: 0.333s, episode steps:  24, steps per second:  72, episode reward: 44.876, mean reward:  1.870 [-2.210, 32.350], mean action: 3.167 [0.000, 15.000],  loss: 0.022333, mae: 0.371392, mean_q: 0.543923, mean_eps: 0.000000
 1486/5000: episode: 47, duration: 0.329s, episode steps:  24, steps per second:  73, episode reward: 41.498, mean reward:  1.729 [-2.887, 32.080], mean action: 3.208 [0.000, 15.000],  loss: 0.018976, mae: 0.358614, mean_q: 0.541365, mean_eps: 0.000000
 1507/5000: episode: 48, duration: 0.299s, episode steps:  21, steps per second:  70, episode reward: 44.785, mean reward:  2.133 [-2.434, 32.030], mean action: 2.333 [0.000, 15.000],  loss: 0.025927, mae: 0.388978, mean_q: 0.483966, mean_eps: 0.000000
 1528/5000: episode: 49, duration: 0.309s, episode steps:  21, steps per second:  68, episode reward: 44.664, mean reward:  2.127 [-2.132, 32.400], mean action: 3.667 [1.000, 15.000],  loss: 0.020333, mae: 0.368222, mean_q: 0.514410, mean_eps: 0.000000
 1600/5000: episode: 50, duration: 0.969s, episode steps:  72, steps per second:  74, episode reward: 35.795, mean reward:  0.497 [-3.000, 31.954], mean action: 1.972 [0.000, 15.000],  loss: 0.022037, mae: 0.384401, mean_q: 0.526284, mean_eps: 0.000000
 1618/5000: episode: 51, duration: 0.260s, episode steps:  18, steps per second:  69, episode reward: 44.691, mean reward:  2.483 [-2.118, 32.120], mean action: 1.056 [0.000, 11.000],  loss: 0.017838, mae: 0.372349, mean_q: 0.495462, mean_eps: 0.000000
 1652/5000: episode: 52, duration: 0.465s, episode steps:  34, steps per second:  73, episode reward: 35.064, mean reward:  1.031 [-3.000, 31.974], mean action: 2.235 [0.000, 11.000],  loss: 0.022460, mae: 0.381835, mean_q: 0.491126, mean_eps: 0.000000
 1679/5000: episode: 53, duration: 0.378s, episode steps:  27, steps per second:  71, episode reward: 38.723, mean reward:  1.434 [-2.322, 29.230], mean action: 3.111 [0.000, 12.000],  loss: 0.019784, mae: 0.365897, mean_q: 0.527115, mean_eps: 0.000000
 1701/5000: episode: 54, duration: 0.312s, episode steps:  22, steps per second:  70, episode reward: 36.000, mean reward:  1.636 [-2.879, 33.000], mean action: 5.455 [0.000, 16.000],  loss: 0.017987, mae: 0.348999, mean_q: 0.549153, mean_eps: 0.000000
 1722/5000: episode: 55, duration: 0.299s, episode steps:  21, steps per second:  70, episode reward: 38.587, mean reward:  1.837 [-3.000, 32.350], mean action: 5.143 [0.000, 16.000],  loss: 0.019439, mae: 0.361370, mean_q: 0.528062, mean_eps: 0.000000
 1763/5000: episode: 56, duration: 0.546s, episode steps:  41, steps per second:  75, episode reward: 35.418, mean reward:  0.864 [-2.345, 32.010], mean action: 2.854 [0.000, 12.000],  loss: 0.022111, mae: 0.372655, mean_q: 0.517283, mean_eps: 0.000000
 1783/5000: episode: 57, duration: 0.284s, episode steps:  20, steps per second:  71, episode reward: 38.354, mean reward:  1.918 [-3.000, 32.330], mean action: 2.200 [0.000, 12.000],  loss: 0.020696, mae: 0.360298, mean_q: 0.522289, mean_eps: 0.000000
 1808/5000: episode: 58, duration: 0.339s, episode steps:  25, steps per second:  74, episode reward: 38.882, mean reward:  1.555 [-2.878, 33.000], mean action: 3.160 [1.000, 16.000],  loss: 0.018631, mae: 0.353331, mean_q: 0.512256, mean_eps: 0.000000
 1833/5000: episode: 59, duration: 0.348s, episode steps:  25, steps per second:  72, episode reward: 38.091, mean reward:  1.524 [-2.618, 32.511], mean action: 3.720 [0.000, 16.000],  loss: 0.018768, mae: 0.353729, mean_q: 0.463638, mean_eps: 0.000000
 1855/5000: episode: 60, duration: 0.343s, episode steps:  22, steps per second:  64, episode reward: 44.131, mean reward:  2.006 [-2.125, 31.943], mean action: 1.955 [0.000, 11.000],  loss: 0.022145, mae: 0.366703, mean_q: 0.517946, mean_eps: 0.000000
 1868/5000: episode: 61, duration: 0.192s, episode steps:  13, steps per second:  68, episode reward: 44.849, mean reward:  3.450 [-2.111, 32.070], mean action: 2.000 [0.000, 14.000],  loss: 0.021214, mae: 0.363861, mean_q: 0.545412, mean_eps: 0.000000
 1897/5000: episode: 62, duration: 0.394s, episode steps:  29, steps per second:  74, episode reward: 35.405, mean reward:  1.221 [-2.806, 32.190], mean action: 3.483 [0.000, 16.000],  loss: 0.020575, mae: 0.367048, mean_q: 0.550962, mean_eps: 0.000000
 1911/5000: episode: 63, duration: 0.199s, episode steps:  14, steps per second:  70, episode reward: 41.016, mean reward:  2.930 [-3.000, 32.250], mean action: 3.143 [0.000, 16.000],  loss: 0.020961, mae: 0.370600, mean_q: 0.546745, mean_eps: 0.000000
 1937/5000: episode: 64, duration: 0.356s, episode steps:  26, steps per second:  73, episode reward: 41.477, mean reward:  1.595 [-2.211, 32.016], mean action: 3.038 [0.000, 21.000],  loss: 0.020200, mae: 0.369895, mean_q: 0.456422, mean_eps: 0.000000
 1965/5000: episode: 65, duration: 0.623s, episode steps:  28, steps per second:  45, episode reward: 38.241, mean reward:  1.366 [-2.771, 31.871], mean action: 1.857 [0.000, 16.000],  loss: 0.021569, mae: 0.376901, mean_q: 0.512360, mean_eps: 0.000000
 1998/5000: episode: 66, duration: 0.460s, episode steps:  33, steps per second:  72, episode reward: 40.205, mean reward:  1.218 [-3.000, 31.753], mean action: 4.061 [0.000, 16.000],  loss: 0.022686, mae: 0.381244, mean_q: 0.532470, mean_eps: 0.000000
 2032/5000: episode: 67, duration: 0.461s, episode steps:  34, steps per second:  74, episode reward: 40.188, mean reward:  1.182 [-3.000, 32.100], mean action: 2.794 [0.000, 16.000],  loss: 0.020377, mae: 0.364036, mean_q: 0.524888, mean_eps: 0.000000
 2053/5000: episode: 68, duration: 0.283s, episode steps:  21, steps per second:  74, episode reward: 41.161, mean reward:  1.960 [-3.000, 32.203], mean action: 2.714 [0.000, 9.000],  loss: 0.018476, mae: 0.356684, mean_q: 0.559346, mean_eps: 0.000000
 2079/5000: episode: 69, duration: 0.353s, episode steps:  26, steps per second:  74, episode reward: 41.903, mean reward:  1.612 [-2.251, 32.543], mean action: 2.808 [0.000, 15.000],  loss: 0.021695, mae: 0.369759, mean_q: 0.541961, mean_eps: 0.000000
 2113/5000: episode: 70, duration: 0.454s, episode steps:  34, steps per second:  75, episode reward: 41.712, mean reward:  1.227 [-2.412, 32.320], mean action: 2.529 [0.000, 11.000],  loss: 0.018559, mae: 0.350031, mean_q: 0.551454, mean_eps: 0.000000
 2132/5000: episode: 71, duration: 0.280s, episode steps:  19, steps per second:  68, episode reward: 44.446, mean reward:  2.339 [-2.004, 32.217], mean action: 2.737 [0.000, 19.000],  loss: 0.017149, mae: 0.344077, mean_q: 0.509097, mean_eps: 0.000000
 2161/5000: episode: 72, duration: 0.424s, episode steps:  29, steps per second:  68, episode reward: 44.593, mean reward:  1.538 [-2.546, 32.170], mean action: 1.069 [0.000, 19.000],  loss: 0.020811, mae: 0.367703, mean_q: 0.525691, mean_eps: 0.000000
 2192/5000: episode: 73, duration: 0.433s, episode steps:  31, steps per second:  72, episode reward: 37.792, mean reward:  1.219 [-2.690, 32.020], mean action: 6.806 [0.000, 21.000],  loss: 0.018775, mae: 0.360937, mean_q: 0.517158, mean_eps: 0.000000
 2214/5000: episode: 74, duration: 0.346s, episode steps:  22, steps per second:  64, episode reward: 41.902, mean reward:  1.905 [-2.231, 32.162], mean action: 4.591 [0.000, 19.000],  loss: 0.020726, mae: 0.372792, mean_q: 0.577015, mean_eps: 0.000000
 2244/5000: episode: 75, duration: 0.447s, episode steps:  30, steps per second:  67, episode reward: 38.699, mean reward:  1.290 [-2.336, 32.529], mean action: 3.733 [0.000, 19.000],  loss: 0.021111, mae: 0.374259, mean_q: 0.510983, mean_eps: 0.000000
 2271/5000: episode: 76, duration: 0.394s, episode steps:  27, steps per second:  68, episode reward: 44.913, mean reward:  1.663 [-2.026, 32.240], mean action: 2.333 [1.000, 19.000],  loss: 0.021442, mae: 0.379104, mean_q: 0.482017, mean_eps: 0.000000
 2296/5000: episode: 77, duration: 0.383s, episode steps:  25, steps per second:  65, episode reward: 33.000, mean reward:  1.320 [-3.000, 32.150], mean action: 4.640 [0.000, 19.000],  loss: 0.020060, mae: 0.364326, mean_q: 0.482502, mean_eps: 0.000000
 2316/5000: episode: 78, duration: 0.290s, episode steps:  20, steps per second:  69, episode reward: 39.000, mean reward:  1.950 [-2.138, 32.050], mean action: 4.250 [0.000, 19.000],  loss: 0.022465, mae: 0.378269, mean_q: 0.479766, mean_eps: 0.000000
 2357/5000: episode: 79, duration: 0.673s, episode steps:  41, steps per second:  61, episode reward: 34.657, mean reward:  0.845 [-2.337, 32.051], mean action: 5.951 [0.000, 19.000],  loss: 0.020209, mae: 0.366263, mean_q: 0.488246, mean_eps: 0.000000
 2383/5000: episode: 80, duration: 0.393s, episode steps:  26, steps per second:  66, episode reward: 37.081, mean reward:  1.426 [-3.000, 32.140], mean action: 5.038 [0.000, 16.000],  loss: 0.019714, mae: 0.360638, mean_q: 0.515588, mean_eps: 0.000000
 2402/5000: episode: 81, duration: 0.287s, episode steps:  19, steps per second:  66, episode reward: 42.000, mean reward:  2.211 [-3.000, 33.000], mean action: 4.895 [0.000, 14.000],  loss: 0.023883, mae: 0.380031, mean_q: 0.483481, mean_eps: 0.000000
 2435/5000: episode: 82, duration: 0.442s, episode steps:  33, steps per second:  75, episode reward: 38.103, mean reward:  1.155 [-2.546, 31.759], mean action: 2.909 [0.000, 15.000],  loss: 0.022731, mae: 0.377672, mean_q: 0.483824, mean_eps: 0.000000
 2450/5000: episode: 83, duration: 0.215s, episode steps:  15, steps per second:  70, episode reward: 44.086, mean reward:  2.939 [-2.438, 32.173], mean action: 3.600 [2.000, 15.000],  loss: 0.023497, mae: 0.387809, mean_q: 0.463915, mean_eps: 0.000000
 2494/5000: episode: 84, duration: 0.588s, episode steps:  44, steps per second:  75, episode reward: 35.587, mean reward:  0.809 [-3.000, 32.100], mean action: 5.750 [0.000, 20.000],  loss: 0.019636, mae: 0.368262, mean_q: 0.485258, mean_eps: 0.000000
 2517/5000: episode: 85, duration: 0.310s, episode steps:  23, steps per second:  74, episode reward: 32.494, mean reward:  1.413 [-3.000, 32.478], mean action: 3.739 [0.000, 16.000],  loss: 0.018207, mae: 0.361722, mean_q: 0.488532, mean_eps: 0.000000
 2539/5000: episode: 86, duration: 0.355s, episode steps:  22, steps per second:  62, episode reward: 44.632, mean reward:  2.029 [-2.242, 31.742], mean action: 5.864 [2.000, 14.000],  loss: 0.019433, mae: 0.363674, mean_q: 0.474135, mean_eps: 0.000000
 2565/5000: episode: 87, duration: 0.351s, episode steps:  26, steps per second:  74, episode reward: 41.703, mean reward:  1.604 [-2.699, 32.380], mean action: 4.115 [0.000, 14.000],  loss: 0.019880, mae: 0.375723, mean_q: 0.435574, mean_eps: 0.000000
 2588/5000: episode: 88, duration: 0.312s, episode steps:  23, steps per second:  74, episode reward: 41.373, mean reward:  1.799 [-2.419, 32.210], mean action: 2.870 [0.000, 14.000],  loss: 0.023198, mae: 0.384584, mean_q: 0.488268, mean_eps: 0.000000
 2611/5000: episode: 89, duration: 0.319s, episode steps:  23, steps per second:  72, episode reward: 47.667, mean reward:  2.072 [-0.478, 32.312], mean action: 3.304 [0.000, 13.000],  loss: 0.020521, mae: 0.366711, mean_q: 0.564360, mean_eps: 0.000000
 2629/5000: episode: 90, duration: 0.250s, episode steps:  18, steps per second:  72, episode reward: 44.047, mean reward:  2.447 [-2.100, 32.460], mean action: 2.389 [0.000, 19.000],  loss: 0.022546, mae: 0.373215, mean_q: 0.640143, mean_eps: 0.000000
 2676/5000: episode: 91, duration: 0.648s, episode steps:  47, steps per second:  73, episode reward: 35.008, mean reward:  0.745 [-2.789, 32.074], mean action: 2.681 [0.000, 19.000],  loss: 0.018403, mae: 0.360080, mean_q: 0.530604, mean_eps: 0.000000
 2701/5000: episode: 92, duration: 0.371s, episode steps:  25, steps per second:  67, episode reward: 38.219, mean reward:  1.529 [-2.409, 31.938], mean action: 4.080 [1.000, 19.000],  loss: 0.023249, mae: 0.383781, mean_q: 0.574048, mean_eps: 0.000000
 2723/5000: episode: 93, duration: 0.311s, episode steps:  22, steps per second:  71, episode reward: 38.770, mean reward:  1.762 [-2.632, 32.300], mean action: 3.364 [0.000, 19.000],  loss: 0.021170, mae: 0.369121, mean_q: 0.558694, mean_eps: 0.000000
 2749/5000: episode: 94, duration: 0.351s, episode steps:  26, steps per second:  74, episode reward: 38.781, mean reward:  1.492 [-3.000, 32.433], mean action: 4.385 [0.000, 19.000],  loss: 0.023048, mae: 0.378361, mean_q: 0.573395, mean_eps: 0.000000
 2772/5000: episode: 95, duration: 0.315s, episode steps:  23, steps per second:  73, episode reward: 35.627, mean reward:  1.549 [-2.456, 31.827], mean action: 4.043 [0.000, 19.000],  loss: 0.019998, mae: 0.364418, mean_q: 0.567731, mean_eps: 0.000000
 2802/5000: episode: 96, duration: 0.428s, episode steps:  30, steps per second:  70, episode reward: 38.182, mean reward:  1.273 [-3.000, 31.799], mean action: 7.500 [0.000, 21.000],  loss: 0.018905, mae: 0.360775, mean_q: 0.540963, mean_eps: 0.000000
 2827/5000: episode: 97, duration: 0.351s, episode steps:  25, steps per second:  71, episode reward: 43.846, mean reward:  1.754 [-2.004, 31.932], mean action: 2.320 [0.000, 19.000],  loss: 0.021277, mae: 0.372644, mean_q: 0.529569, mean_eps: 0.000000
 2852/5000: episode: 98, duration: 0.364s, episode steps:  25, steps per second:  69, episode reward: 38.151, mean reward:  1.526 [-2.498, 32.193], mean action: 3.960 [0.000, 16.000],  loss: 0.021508, mae: 0.371859, mean_q: 0.524714, mean_eps: 0.000000
 2880/5000: episode: 99, duration: 0.527s, episode steps:  28, steps per second:  53, episode reward: 32.377, mean reward:  1.156 [-3.000, 31.877], mean action: 4.393 [0.000, 19.000],  loss: 0.022626, mae: 0.378656, mean_q: 0.568719, mean_eps: 0.000000
 2903/5000: episode: 100, duration: 0.308s, episode steps:  23, steps per second:  75, episode reward: 35.639, mean reward:  1.550 [-3.000, 31.919], mean action: 4.000 [0.000, 19.000],  loss: 0.019711, mae: 0.367389, mean_q: 0.549282, mean_eps: 0.000000
 2958/5000: episode: 101, duration: 0.749s, episode steps:  55, steps per second:  73, episode reward: 44.900, mean reward:  0.816 [-2.190, 32.000], mean action: 1.836 [0.000, 9.000],  loss: 0.021615, mae: 0.377323, mean_q: 0.507782, mean_eps: 0.000000
 2985/5000: episode: 102, duration: 0.396s, episode steps:  27, steps per second:  68, episode reward: 41.422, mean reward:  1.534 [-2.191, 31.974], mean action: 2.926 [0.000, 9.000],  loss: 0.019710, mae: 0.362974, mean_q: 0.576888, mean_eps: 0.000000
 3015/5000: episode: 103, duration: 0.546s, episode steps:  30, steps per second:  55, episode reward: 32.587, mean reward:  1.086 [-3.000, 31.667], mean action: 3.167 [0.000, 16.000],  loss: 0.020544, mae: 0.368751, mean_q: 0.611318, mean_eps: 0.000000
 3041/5000: episode: 104, duration: 0.389s, episode steps:  26, steps per second:  67, episode reward: 42.000, mean reward:  1.615 [-2.271, 32.980], mean action: 3.692 [0.000, 16.000],  loss: 0.020349, mae: 0.371933, mean_q: 0.589055, mean_eps: 0.000000
 3071/5000: episode: 105, duration: 0.443s, episode steps:  30, steps per second:  68, episode reward: 41.527, mean reward:  1.384 [-2.036, 32.794], mean action: 2.967 [0.000, 20.000],  loss: 0.020710, mae: 0.368282, mean_q: 0.604112, mean_eps: 0.000000
 3098/5000: episode: 106, duration: 0.407s, episode steps:  27, steps per second:  66, episode reward: 41.446, mean reward:  1.535 [-2.362, 32.301], mean action: 2.296 [0.000, 15.000],  loss: 0.017209, mae: 0.350889, mean_q: 0.557324, mean_eps: 0.000000
 3111/5000: episode: 107, duration: 0.231s, episode steps:  13, steps per second:  56, episode reward: 44.547, mean reward:  3.427 [-2.467, 32.547], mean action: 3.000 [0.000, 15.000],  loss: 0.018633, mae: 0.362370, mean_q: 0.486693, mean_eps: 0.000000
 3137/5000: episode: 108, duration: 0.617s, episode steps:  26, steps per second:  42, episode reward: 41.326, mean reward:  1.589 [-2.234, 32.567], mean action: 3.077 [0.000, 15.000],  loss: 0.019960, mae: 0.367647, mean_q: 0.456288, mean_eps: 0.000000
 3163/5000: episode: 109, duration: 0.513s, episode steps:  26, steps per second:  51, episode reward: 41.005, mean reward:  1.577 [-2.508, 32.420], mean action: 5.000 [2.000, 15.000],  loss: 0.018005, mae: 0.360470, mean_q: 0.435921, mean_eps: 0.000000
 3196/5000: episode: 110, duration: 0.582s, episode steps:  33, steps per second:  57, episode reward: 41.215, mean reward:  1.249 [-2.343, 32.435], mean action: 4.485 [1.000, 16.000],  loss: 0.021484, mae: 0.366710, mean_q: 0.517028, mean_eps: 0.000000
 3229/5000: episode: 111, duration: 0.539s, episode steps:  33, steps per second:  61, episode reward: -34.920, mean reward: -1.058 [-32.347,  2.090], mean action: 6.788 [0.000, 19.000],  loss: 0.021959, mae: 0.379426, mean_q: 0.491650, mean_eps: 0.000000
 3246/5000: episode: 112, duration: 0.285s, episode steps:  17, steps per second:  60, episode reward: 43.800, mean reward:  2.576 [-2.803, 32.440], mean action: 3.235 [0.000, 19.000],  loss: 0.019121, mae: 0.363258, mean_q: 0.492711, mean_eps: 0.000000
 3271/5000: episode: 113, duration: 0.504s, episode steps:  25, steps per second:  50, episode reward: 39.000, mean reward:  1.560 [-3.000, 30.485], mean action: 2.400 [0.000, 19.000],  loss: 0.021854, mae: 0.373502, mean_q: 0.530834, mean_eps: 0.000000
 3293/5000: episode: 114, duration: 0.341s, episode steps:  22, steps per second:  65, episode reward: 38.876, mean reward:  1.767 [-2.292, 32.020], mean action: 2.864 [0.000, 19.000],  loss: 0.023882, mae: 0.379720, mean_q: 0.574318, mean_eps: 0.000000
 3314/5000: episode: 115, duration: 0.416s, episode steps:  21, steps per second:  50, episode reward: 44.064, mean reward:  2.098 [-2.658, 32.590], mean action: 2.810 [0.000, 15.000],  loss: 0.024630, mae: 0.385515, mean_q: 0.620455, mean_eps: 0.000000
 3345/5000: episode: 116, duration: 0.475s, episode steps:  31, steps per second:  65, episode reward: 41.912, mean reward:  1.352 [-2.641, 32.500], mean action: 2.484 [0.000, 16.000],  loss: 0.017717, mae: 0.350870, mean_q: 0.601597, mean_eps: 0.000000
 3387/5000: episode: 117, duration: 0.697s, episode steps:  42, steps per second:  60, episode reward: 44.791, mean reward:  1.066 [-2.077, 31.837], mean action: 1.048 [0.000, 16.000],  loss: 0.020269, mae: 0.361774, mean_q: 0.545669, mean_eps: 0.000000
 3415/5000: episode: 118, duration: 0.612s, episode steps:  28, steps per second:  46, episode reward: 42.307, mean reward:  1.511 [-2.365, 32.380], mean action: 3.643 [0.000, 16.000],  loss: 0.019763, mae: 0.358726, mean_q: 0.507251, mean_eps: 0.000000
 3449/5000: episode: 119, duration: 0.827s, episode steps:  34, steps per second:  41, episode reward: 35.281, mean reward:  1.038 [-3.000, 32.400], mean action: 5.000 [1.000, 20.000],  loss: 0.018398, mae: 0.353205, mean_q: 0.535661, mean_eps: 0.000000
 3475/5000: episode: 120, duration: 0.600s, episode steps:  26, steps per second:  43, episode reward: 39.000, mean reward:  1.500 [-2.218, 32.180], mean action: 2.615 [0.000, 16.000],  loss: 0.021159, mae: 0.369887, mean_q: 0.487050, mean_eps: 0.000000
 3502/5000: episode: 121, duration: 0.451s, episode steps:  27, steps per second:  60, episode reward: 41.511, mean reward:  1.537 [-2.473, 32.130], mean action: 4.000 [1.000, 19.000],  loss: 0.020403, mae: 0.366285, mean_q: 0.507654, mean_eps: 0.000000
 3524/5000: episode: 122, duration: 0.408s, episode steps:  22, steps per second:  54, episode reward: 37.903, mean reward:  1.723 [-2.741, 31.589], mean action: 4.955 [0.000, 14.000],  loss: 0.021000, mae: 0.367889, mean_q: 0.493057, mean_eps: 0.000000
 3561/5000: episode: 123, duration: 0.523s, episode steps:  37, steps per second:  71, episode reward: 38.927, mean reward:  1.052 [-2.466, 32.130], mean action: 3.865 [0.000, 15.000],  loss: 0.021780, mae: 0.376749, mean_q: 0.476105, mean_eps: 0.000000
 3585/5000: episode: 124, duration: 0.554s, episode steps:  24, steps per second:  43, episode reward: 38.167, mean reward:  1.590 [-3.000, 32.754], mean action: 3.625 [0.000, 15.000],  loss: 0.020602, mae: 0.370924, mean_q: 0.464444, mean_eps: 0.000000
 3597/5000: episode: 125, duration: 0.183s, episode steps:  12, steps per second:  66, episode reward: 47.546, mean reward:  3.962 [ 0.000, 32.087], mean action: 1.583 [0.000, 3.000],  loss: 0.021571, mae: 0.372028, mean_q: 0.541054, mean_eps: 0.000000
 3623/5000: episode: 126, duration: 0.406s, episode steps:  26, steps per second:  64, episode reward: 37.426, mean reward:  1.439 [-2.866, 32.220], mean action: 5.038 [0.000, 20.000],  loss: 0.019210, mae: 0.360236, mean_q: 0.485025, mean_eps: 0.000000
 3673/5000: episode: 127, duration: 0.893s, episode steps:  50, steps per second:  56, episode reward: 45.000, mean reward:  0.900 [-2.107, 32.270], mean action: 1.140 [0.000, 16.000],  loss: 0.021853, mae: 0.375252, mean_q: 0.502376, mean_eps: 0.000000
 3696/5000: episode: 128, duration: 0.324s, episode steps:  23, steps per second:  71, episode reward: 42.000, mean reward:  1.826 [-2.144, 32.100], mean action: 3.304 [0.000, 16.000],  loss: 0.019222, mae: 0.353593, mean_q: 0.636001, mean_eps: 0.000000
 3718/5000: episode: 129, duration: 0.304s, episode steps:  22, steps per second:  72, episode reward: 44.802, mean reward:  2.036 [-2.250, 32.390], mean action: 2.864 [0.000, 19.000],  loss: 0.021921, mae: 0.365370, mean_q: 0.614003, mean_eps: 0.000000
 3735/5000: episode: 130, duration: 5.073s, episode steps:  17, steps per second:   3, episode reward: 44.322, mean reward:  2.607 [-2.264, 32.170], mean action: 2.059 [0.000, 12.000],  loss: 0.016616, mae: 0.344439, mean_q: 0.540134, mean_eps: 0.000000
 3761/5000: episode: 131, duration: 0.367s, episode steps:  26, steps per second:  71, episode reward: 38.854, mean reward:  1.494 [-2.540, 32.264], mean action: 2.962 [1.000, 16.000],  loss: 0.020527, mae: 0.362039, mean_q: 0.535044, mean_eps: 0.000000
 3782/5000: episode: 132, duration: 0.290s, episode steps:  21, steps per second:  72, episode reward: 36.000, mean reward:  1.714 [-3.000, 32.070], mean action: 5.238 [0.000, 16.000],  loss: 0.018667, mae: 0.343968, mean_q: 0.545808, mean_eps: 0.000000
 3809/5000: episode: 133, duration: 0.372s, episode steps:  27, steps per second:  73, episode reward: 43.718, mean reward:  1.619 [-2.095, 32.030], mean action: 3.148 [1.000, 16.000],  loss: 0.023746, mae: 0.376968, mean_q: 0.592805, mean_eps: 0.000000
 3860/5000: episode: 134, duration: 0.747s, episode steps:  51, steps per second:  68, episode reward: 41.247, mean reward:  0.809 [-2.268, 31.903], mean action: 3.137 [0.000, 19.000],  loss: 0.020632, mae: 0.371720, mean_q: 0.572004, mean_eps: 0.000000
 3891/5000: episode: 135, duration: 0.449s, episode steps:  31, steps per second:  69, episode reward: 38.806, mean reward:  1.252 [-2.362, 32.026], mean action: 2.935 [0.000, 16.000],  loss: 0.021285, mae: 0.371204, mean_q: 0.505287, mean_eps: 0.000000
 3918/5000: episode: 136, duration: 0.367s, episode steps:  27, steps per second:  74, episode reward: 39.000, mean reward:  1.444 [-2.494, 32.700], mean action: 4.667 [0.000, 20.000],  loss: 0.021012, mae: 0.368461, mean_q: 0.517838, mean_eps: 0.000000
 3942/5000: episode: 137, duration: 0.322s, episode steps:  24, steps per second:  74, episode reward: 36.000, mean reward:  1.500 [-3.000, 32.110], mean action: 5.083 [1.000, 16.000],  loss: 0.019579, mae: 0.359115, mean_q: 0.556219, mean_eps: 0.000000
 3976/5000: episode: 138, duration: 0.468s, episode steps:  34, steps per second:  73, episode reward: 34.755, mean reward:  1.022 [-3.000, 32.017], mean action: 3.294 [0.000, 16.000],  loss: 0.020504, mae: 0.369522, mean_q: 0.545676, mean_eps: 0.000000
 3999/5000: episode: 139, duration: 0.316s, episode steps:  23, steps per second:  73, episode reward: 35.556, mean reward:  1.546 [-3.000, 32.656], mean action: 4.391 [0.000, 16.000],  loss: 0.019488, mae: 0.361197, mean_q: 0.559172, mean_eps: 0.000000
 4021/5000: episode: 140, duration: 0.310s, episode steps:  22, steps per second:  71, episode reward: 41.542, mean reward:  1.888 [-2.224, 32.250], mean action: 3.409 [0.000, 12.000],  loss: 0.017288, mae: 0.344520, mean_q: 0.538234, mean_eps: 0.000000
 4043/5000: episode: 141, duration: 0.303s, episode steps:  22, steps per second:  73, episode reward: 44.798, mean reward:  2.036 [-2.599, 32.080], mean action: 2.864 [0.000, 12.000],  loss: 0.019922, mae: 0.365079, mean_q: 0.511501, mean_eps: 0.000000
 4072/5000: episode: 142, duration: 0.403s, episode steps:  29, steps per second:  72, episode reward: 41.327, mean reward:  1.425 [-2.711, 32.290], mean action: 4.172 [0.000, 13.000],  loss: 0.022510, mae: 0.377436, mean_q: 0.490827, mean_eps: 0.000000
 4099/5000: episode: 143, duration: 0.360s, episode steps:  27, steps per second:  75, episode reward: 41.763, mean reward:  1.547 [-2.806, 32.210], mean action: 3.333 [0.000, 19.000],  loss: 0.019245, mae: 0.354990, mean_q: 0.512974, mean_eps: 0.000000
 4123/5000: episode: 144, duration: 0.334s, episode steps:  24, steps per second:  72, episode reward: 41.979, mean reward:  1.749 [-2.311, 33.000], mean action: 4.375 [1.000, 19.000],  loss: 0.019449, mae: 0.355044, mean_q: 0.510335, mean_eps: 0.000000
 4143/5000: episode: 145, duration: 0.275s, episode steps:  20, steps per second:  73, episode reward: 38.466, mean reward:  1.923 [-3.000, 31.516], mean action: 2.100 [0.000, 11.000],  loss: 0.023283, mae: 0.373263, mean_q: 0.480853, mean_eps: 0.000000
 4156/5000: episode: 146, duration: 0.194s, episode steps:  13, steps per second:  67, episode reward: 46.525, mean reward:  3.579 [-0.167, 32.300], mean action: 3.385 [0.000, 13.000],  loss: 0.021068, mae: 0.367701, mean_q: 0.516706, mean_eps: 0.000000
 4208/5000: episode: 147, duration: 0.722s, episode steps:  52, steps per second:  72, episode reward: -35.120, mean reward: -0.675 [-31.878,  2.240], mean action: 4.500 [0.000, 20.000],  loss: 0.022698, mae: 0.377073, mean_q: 0.498763, mean_eps: 0.000000
 4247/5000: episode: 148, duration: 0.539s, episode steps:  39, steps per second:  72, episode reward: 38.286, mean reward:  0.982 [-2.468, 32.090], mean action: 2.795 [0.000, 19.000],  loss: 0.017340, mae: 0.360796, mean_q: 0.490973, mean_eps: 0.000000
 4275/5000: episode: 149, duration: 0.392s, episode steps:  28, steps per second:  71, episode reward: 39.000, mean reward:  1.393 [-3.000, 32.310], mean action: 3.571 [0.000, 19.000],  loss: 0.023136, mae: 0.383153, mean_q: 0.529851, mean_eps: 0.000000
 4293/5000: episode: 150, duration: 0.254s, episode steps:  18, steps per second:  71, episode reward: 44.657, mean reward:  2.481 [-2.385, 32.250], mean action: 3.167 [0.000, 19.000],  loss: 0.020944, mae: 0.373418, mean_q: 0.509844, mean_eps: 0.000000
 4329/5000: episode: 151, duration: 0.489s, episode steps:  36, steps per second:  74, episode reward: 40.229, mean reward:  1.117 [-3.000, 32.150], mean action: 3.417 [0.000, 19.000],  loss: 0.017798, mae: 0.364001, mean_q: 0.511640, mean_eps: 0.000000
 4360/5000: episode: 152, duration: 0.414s, episode steps:  31, steps per second:  75, episode reward: 35.778, mean reward:  1.154 [-3.000, 32.148], mean action: 4.806 [0.000, 19.000],  loss: 0.018853, mae: 0.368521, mean_q: 0.483759, mean_eps: 0.000000
 4392/5000: episode: 153, duration: 0.433s, episode steps:  32, steps per second:  74, episode reward: 39.000, mean reward:  1.219 [-2.438, 32.880], mean action: 5.312 [1.000, 13.000],  loss: 0.022335, mae: 0.388863, mean_q: 0.495993, mean_eps: 0.000000
 4440/5000: episode: 154, duration: 0.926s, episode steps:  48, steps per second:  52, episode reward: 41.481, mean reward:  0.864 [-2.164, 32.300], mean action: 2.479 [1.000, 19.000],  loss: 0.022660, mae: 0.378552, mean_q: 0.555532, mean_eps: 0.000000
 4471/5000: episode: 155, duration: 0.420s, episode steps:  31, steps per second:  74, episode reward: 35.876, mean reward:  1.157 [-2.900, 32.100], mean action: 4.387 [0.000, 19.000],  loss: 0.018017, mae: 0.361944, mean_q: 0.595782, mean_eps: 0.000000
 4491/5000: episode: 156, duration: 0.296s, episode steps:  20, steps per second:  67, episode reward: 44.692, mean reward:  2.235 [-2.016, 32.046], mean action: 2.500 [0.000, 19.000],  loss: 0.022081, mae: 0.374809, mean_q: 0.549991, mean_eps: 0.000000
 4513/5000: episode: 157, duration: 0.377s, episode steps:  22, steps per second:  58, episode reward: 36.000, mean reward:  1.636 [-3.000, 29.506], mean action: 3.455 [0.000, 19.000],  loss: 0.020752, mae: 0.377732, mean_q: 0.555566, mean_eps: 0.000000
 4541/5000: episode: 158, duration: 0.423s, episode steps:  28, steps per second:  66, episode reward: 47.009, mean reward:  1.679 [-0.458, 32.120], mean action: 4.929 [1.000, 20.000],  loss: 0.020872, mae: 0.377732, mean_q: 0.502102, mean_eps: 0.000000
 4572/5000: episode: 159, duration: 0.423s, episode steps:  31, steps per second:  73, episode reward: 38.853, mean reward:  1.253 [-3.000, 32.910], mean action: 4.935 [0.000, 19.000],  loss: 0.018834, mae: 0.358403, mean_q: 0.547965, mean_eps: 0.000000
 4629/5000: episode: 160, duration: 1.029s, episode steps:  57, steps per second:  55, episode reward: 32.679, mean reward:  0.573 [-2.618, 32.190], mean action: 8.140 [0.000, 18.000],  loss: 0.021727, mae: 0.382529, mean_q: 0.542238, mean_eps: 0.000000
 4657/5000: episode: 161, duration: 0.381s, episode steps:  28, steps per second:  73, episode reward: 35.936, mean reward:  1.283 [-2.355, 34.440], mean action: 3.679 [0.000, 11.000],  loss: 0.022001, mae: 0.381702, mean_q: 0.492002, mean_eps: 0.000000
 4681/5000: episode: 162, duration: 0.333s, episode steps:  24, steps per second:  72, episode reward: 40.366, mean reward:  1.682 [-3.000, 31.913], mean action: 3.917 [0.000, 15.000],  loss: 0.018998, mae: 0.356657, mean_q: 0.504131, mean_eps: 0.000000
 4719/5000: episode: 163, duration: 0.523s, episode steps:  38, steps per second:  73, episode reward: 44.302, mean reward:  1.166 [-2.073, 32.608], mean action: 1.553 [0.000, 9.000],  loss: 0.022013, mae: 0.369582, mean_q: 0.546152, mean_eps: 0.000000
 4732/5000: episode: 164, duration: 0.194s, episode steps:  13, steps per second:  67, episode reward: 48.000, mean reward:  3.692 [ 0.000, 32.390], mean action: 2.077 [1.000, 3.000],  loss: 0.017337, mae: 0.354364, mean_q: 0.530509, mean_eps: 0.000000
 4770/5000: episode: 165, duration: 0.529s, episode steps:  38, steps per second:  72, episode reward: 32.159, mean reward:  0.846 [-3.000, 32.140], mean action: 6.289 [1.000, 20.000],  loss: 0.018436, mae: 0.369478, mean_q: 0.513143, mean_eps: 0.000000
 4790/5000: episode: 166, duration: 0.278s, episode steps:  20, steps per second:  72, episode reward: 44.096, mean reward:  2.205 [-2.429, 31.911], mean action: 2.900 [1.000, 11.000],  loss: 0.022068, mae: 0.379366, mean_q: 0.547006, mean_eps: 0.000000
 4819/5000: episode: 167, duration: 0.413s, episode steps:  29, steps per second:  70, episode reward: 41.936, mean reward:  1.446 [-2.303, 32.180], mean action: 5.103 [0.000, 15.000],  loss: 0.023919, mae: 0.389938, mean_q: 0.511115, mean_eps: 0.000000
 4845/5000: episode: 168, duration: 0.445s, episode steps:  26, steps per second:  58, episode reward: 42.000, mean reward:  1.615 [-2.168, 32.380], mean action: 2.500 [0.000, 19.000],  loss: 0.016680, mae: 0.354299, mean_q: 0.521861, mean_eps: 0.000000
 4868/5000: episode: 169, duration: 0.571s, episode steps:  23, steps per second:  40, episode reward: 44.339, mean reward:  1.928 [-2.343, 32.190], mean action: 3.435 [0.000, 19.000],  loss: 0.020454, mae: 0.371674, mean_q: 0.484597, mean_eps: 0.000000
 4890/5000: episode: 170, duration: 0.337s, episode steps:  22, steps per second:  65, episode reward: 35.656, mean reward:  1.621 [-3.000, 31.863], mean action: 5.591 [0.000, 16.000],  loss: 0.022340, mae: 0.376852, mean_q: 0.519470, mean_eps: 0.000000
 4913/5000: episode: 171, duration: 0.333s, episode steps:  23, steps per second:  69, episode reward: 44.182, mean reward:  1.921 [-2.288, 32.022], mean action: 1.391 [0.000, 13.000],  loss: 0.021992, mae: 0.377981, mean_q: 0.540612, mean_eps: 0.000000
 4935/5000: episode: 172, duration: 0.312s, episode steps:  22, steps per second:  70, episode reward: 41.151, mean reward:  1.870 [-2.707, 32.090], mean action: 3.818 [0.000, 14.000],  loss: 0.021192, mae: 0.374813, mean_q: 0.524532, mean_eps: 0.000000
 4960/5000: episode: 173, duration: 0.464s, episode steps:  25, steps per second:  54, episode reward: 38.729, mean reward:  1.549 [-2.490, 31.929], mean action: 2.920 [0.000, 14.000],  loss: 0.017779, mae: 0.367859, mean_q: 0.487347, mean_eps: 0.000000
 4983/5000: episode: 174, duration: 0.357s, episode steps:  23, steps per second:  64, episode reward: 45.000, mean reward:  1.957 [-2.270, 32.210], mean action: 2.522 [1.000, 12.000],  loss: 0.020510, mae: 0.374256, mean_q: 0.491065, mean_eps: 0.000000
done, took 71.824 seconds
DQN Evaluation: 5618 victories out of 6619 episodes
Training for 5000 steps ...
   28/5000: episode: 1, duration: 0.211s, episode steps:  28, steps per second: 133, episode reward: -33.000, mean reward: -1.179 [-32.134,  2.920], mean action: 6.179 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   47/5000: episode: 2, duration: 0.130s, episode steps:  19, steps per second: 146, episode reward: 35.522, mean reward:  1.870 [-3.000, 31.747], mean action: 4.316 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   71/5000: episode: 3, duration: 0.159s, episode steps:  24, steps per second: 150, episode reward: 33.000, mean reward:  1.375 [-2.710, 32.080], mean action: 5.667 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   93/5000: episode: 4, duration: 0.168s, episode steps:  22, steps per second: 131, episode reward: 38.316, mean reward:  1.742 [-2.352, 32.090], mean action: 3.545 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  119/5000: episode: 5, duration: 0.171s, episode steps:  26, steps per second: 152, episode reward: -30.000, mean reward: -1.154 [-30.057,  3.000], mean action: 7.115 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  141/5000: episode: 6, duration: 0.140s, episode steps:  22, steps per second: 157, episode reward: -39.000, mean reward: -1.773 [-32.389,  2.470], mean action: 7.591 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  173/5000: episode: 7, duration: 0.203s, episode steps:  32, steps per second: 157, episode reward: -32.020, mean reward: -1.001 [-31.944,  2.610], mean action: 3.781 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  191/5000: episode: 8, duration: 0.140s, episode steps:  18, steps per second: 129, episode reward: 41.361, mean reward:  2.298 [-2.375, 32.903], mean action: 4.278 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  215/5000: episode: 9, duration: 0.168s, episode steps:  24, steps per second: 143, episode reward: -35.750, mean reward: -1.490 [-32.217,  2.211], mean action: 4.958 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  236/5000: episode: 10, duration: 0.146s, episode steps:  21, steps per second: 144, episode reward: 37.631, mean reward:  1.792 [-3.000, 30.434], mean action: 3.667 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  252/5000: episode: 11, duration: 0.122s, episode steps:  16, steps per second: 131, episode reward: 42.000, mean reward:  2.625 [-2.159, 32.080], mean action: 3.938 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  276/5000: episode: 12, duration: 0.158s, episode steps:  24, steps per second: 151, episode reward: -35.940, mean reward: -1.498 [-32.120,  3.000], mean action: 6.708 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  297/5000: episode: 13, duration: 0.141s, episode steps:  21, steps per second: 149, episode reward: 35.856, mean reward:  1.707 [-2.511, 32.331], mean action: 3.619 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  315/5000: episode: 14, duration: 0.137s, episode steps:  18, steps per second: 131, episode reward: 32.820, mean reward:  1.823 [-3.000, 31.970], mean action: 4.556 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  337/5000: episode: 15, duration: 0.154s, episode steps:  22, steps per second: 143, episode reward: -32.040, mean reward: -1.456 [-32.233,  2.770], mean action: 6.773 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  361/5000: episode: 16, duration: 0.161s, episode steps:  24, steps per second: 149, episode reward: 37.970, mean reward:  1.582 [-2.646, 32.440], mean action: 7.167 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  386/5000: episode: 17, duration: 0.162s, episode steps:  25, steps per second: 154, episode reward: 35.405, mean reward:  1.416 [-2.519, 32.813], mean action: 3.160 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  407/5000: episode: 18, duration: 0.162s, episode steps:  21, steps per second: 130, episode reward: 35.901, mean reward:  1.710 [-2.594, 32.291], mean action: 3.095 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  432/5000: episode: 19, duration: 0.172s, episode steps:  25, steps per second: 146, episode reward: 35.568, mean reward:  1.423 [-3.000, 32.160], mean action: 4.160 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  464/5000: episode: 20, duration: 0.512s, episode steps:  32, steps per second:  63, episode reward: -33.000, mean reward: -1.031 [-32.043,  2.678], mean action: 4.812 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  495/5000: episode: 21, duration: 0.296s, episode steps:  31, steps per second: 105, episode reward: -35.150, mean reward: -1.134 [-32.407,  2.251], mean action: 7.613 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  512/5000: episode: 22, duration: 0.256s, episode steps:  17, steps per second:  66, episode reward: 39.000, mean reward:  2.294 [-3.000, 32.350], mean action: 5.176 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  537/5000: episode: 23, duration: 0.350s, episode steps:  25, steps per second:  71, episode reward: 32.136, mean reward:  1.285 [-3.000, 31.716], mean action: 5.160 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  560/5000: episode: 24, duration: 0.166s, episode steps:  23, steps per second: 139, episode reward: 38.700, mean reward:  1.683 [-2.901, 30.000], mean action: 2.174 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  591/5000: episode: 25, duration: 0.216s, episode steps:  31, steps per second: 144, episode reward: 35.579, mean reward:  1.148 [-2.711, 31.649], mean action: 6.677 [2.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  636/5000: episode: 26, duration: 0.419s, episode steps:  45, steps per second: 107, episode reward: 35.223, mean reward:  0.783 [-2.285, 32.230], mean action: 3.378 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  662/5000: episode: 27, duration: 0.184s, episode steps:  26, steps per second: 142, episode reward: 32.552, mean reward:  1.252 [-3.000, 32.320], mean action: 7.731 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  684/5000: episode: 28, duration: 0.148s, episode steps:  22, steps per second: 149, episode reward: 38.487, mean reward:  1.749 [-2.403, 32.300], mean action: 3.636 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  699/5000: episode: 29, duration: 0.103s, episode steps:  15, steps per second: 145, episode reward: 38.612, mean reward:  2.574 [-2.439, 32.903], mean action: 4.000 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  720/5000: episode: 30, duration: 0.144s, episode steps:  21, steps per second: 146, episode reward: 37.632, mean reward:  1.792 [-2.259, 32.083], mean action: 5.381 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  738/5000: episode: 31, duration: 0.123s, episode steps:  18, steps per second: 146, episode reward: 41.010, mean reward:  2.278 [-2.175, 32.304], mean action: 2.333 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  757/5000: episode: 32, duration: 0.133s, episode steps:  19, steps per second: 143, episode reward: 44.776, mean reward:  2.357 [-2.050, 32.370], mean action: 3.316 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  772/5000: episode: 33, duration: 0.115s, episode steps:  15, steps per second: 130, episode reward: 41.707, mean reward:  2.780 [-3.000, 32.051], mean action: 1.933 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  794/5000: episode: 34, duration: 0.147s, episode steps:  22, steps per second: 150, episode reward: 32.266, mean reward:  1.467 [-2.900, 32.260], mean action: 4.727 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  815/5000: episode: 35, duration: 0.156s, episode steps:  21, steps per second: 135, episode reward: 39.000, mean reward:  1.857 [-2.486, 32.010], mean action: 3.714 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  838/5000: episode: 36, duration: 0.155s, episode steps:  23, steps per second: 148, episode reward: 38.226, mean reward:  1.662 [-2.287, 32.340], mean action: 6.522 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  859/5000: episode: 37, duration: 0.143s, episode steps:  21, steps per second: 147, episode reward: 37.382, mean reward:  1.780 [-2.354, 31.823], mean action: 4.762 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  884/5000: episode: 38, duration: 0.160s, episode steps:  25, steps per second: 156, episode reward: 37.678, mean reward:  1.507 [-2.298, 32.243], mean action: 5.520 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  912/5000: episode: 39, duration: 0.182s, episode steps:  28, steps per second: 154, episode reward: 32.540, mean reward:  1.162 [-3.000, 32.280], mean action: 3.321 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  925/5000: episode: 40, duration: 0.096s, episode steps:  13, steps per second: 135, episode reward: 41.055, mean reward:  3.158 [-2.203, 32.013], mean action: 2.692 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  948/5000: episode: 41, duration: 3.181s, episode steps:  23, steps per second:   7, episode reward: 41.276, mean reward:  1.795 [-2.291, 32.070], mean action: 3.087 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  973/5000: episode: 42, duration: 0.160s, episode steps:  25, steps per second: 156, episode reward: -32.840, mean reward: -1.314 [-31.870,  2.531], mean action: 3.960 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1005/5000: episode: 43, duration: 0.253s, episode steps:  32, steps per second: 126, episode reward: 37.790, mean reward:  1.181 [-2.444, 32.003], mean action: 2.312 [0.000, 15.000],  loss: 0.021915, mae: 0.378033, mean_q: 0.511229, mean_eps: 0.000000
 1039/5000: episode: 44, duration: 0.869s, episode steps:  34, steps per second:  39, episode reward: 36.000, mean reward:  1.059 [-3.000, 32.340], mean action: 4.353 [0.000, 15.000],  loss: 0.021493, mae: 0.376769, mean_q: 0.516016, mean_eps: 0.000000
 1073/5000: episode: 45, duration: 0.569s, episode steps:  34, steps per second:  60, episode reward: 32.493, mean reward:  0.956 [-2.455, 32.090], mean action: 5.147 [0.000, 19.000],  loss: 0.019172, mae: 0.366055, mean_q: 0.544370, mean_eps: 0.000000
 1094/5000: episode: 46, duration: 0.617s, episode steps:  21, steps per second:  34, episode reward: 36.000, mean reward:  1.714 [-2.407, 30.607], mean action: 4.429 [0.000, 16.000],  loss: 0.016479, mae: 0.355645, mean_q: 0.571419, mean_eps: 0.000000
 1120/5000: episode: 47, duration: 0.604s, episode steps:  26, steps per second:  43, episode reward: 35.095, mean reward:  1.350 [-3.000, 31.975], mean action: 5.577 [0.000, 19.000],  loss: 0.024563, mae: 0.389822, mean_q: 0.563286, mean_eps: 0.000000
 1145/5000: episode: 48, duration: 0.490s, episode steps:  25, steps per second:  51, episode reward: 35.622, mean reward:  1.425 [-2.682, 32.110], mean action: 4.680 [1.000, 19.000],  loss: 0.019853, mae: 0.362565, mean_q: 0.538784, mean_eps: 0.000000
 1173/5000: episode: 49, duration: 0.433s, episode steps:  28, steps per second:  65, episode reward: 35.612, mean reward:  1.272 [-2.243, 32.150], mean action: 4.250 [0.000, 19.000],  loss: 0.017151, mae: 0.352983, mean_q: 0.566701, mean_eps: 0.000000
 1198/5000: episode: 50, duration: 0.349s, episode steps:  25, steps per second:  72, episode reward: 35.562, mean reward:  1.422 [-2.195, 32.072], mean action: 5.320 [0.000, 19.000],  loss: 0.019464, mae: 0.356492, mean_q: 0.514596, mean_eps: 0.000000
 1225/5000: episode: 51, duration: 0.366s, episode steps:  27, steps per second:  74, episode reward: -38.600, mean reward: -1.430 [-32.389,  2.359], mean action: 6.407 [0.000, 19.000],  loss: 0.021089, mae: 0.369726, mean_q: 0.554750, mean_eps: 0.000000
 1245/5000: episode: 52, duration: 0.317s, episode steps:  20, steps per second:  63, episode reward: 44.372, mean reward:  2.219 [-2.348, 32.030], mean action: 2.200 [1.000, 11.000],  loss: 0.021686, mae: 0.378867, mean_q: 0.590110, mean_eps: 0.000000
 1264/5000: episode: 53, duration: 0.492s, episode steps:  19, steps per second:  39, episode reward: -39.000, mean reward: -2.053 [-32.579,  3.000], mean action: 4.474 [0.000, 16.000],  loss: 0.022356, mae: 0.384395, mean_q: 0.525412, mean_eps: 0.000000
 1286/5000: episode: 54, duration: 0.364s, episode steps:  22, steps per second:  60, episode reward: -36.000, mean reward: -1.636 [-30.402,  2.562], mean action: 5.682 [1.000, 16.000],  loss: 0.016325, mae: 0.354474, mean_q: 0.518038, mean_eps: 0.000000
 1309/5000: episode: 55, duration: 0.315s, episode steps:  23, steps per second:  73, episode reward: -35.170, mean reward: -1.529 [-32.112,  2.452], mean action: 7.130 [0.000, 21.000],  loss: 0.021579, mae: 0.372922, mean_q: 0.524022, mean_eps: 0.000000
 1337/5000: episode: 56, duration: 0.391s, episode steps:  28, steps per second:  72, episode reward: -32.260, mean reward: -1.152 [-32.151,  3.061], mean action: 4.750 [0.000, 15.000],  loss: 0.021537, mae: 0.370882, mean_q: 0.554281, mean_eps: 0.000000
 1352/5000: episode: 57, duration: 0.262s, episode steps:  15, steps per second:  57, episode reward: 41.615, mean reward:  2.774 [-3.000, 32.400], mean action: 2.067 [0.000, 9.000],  loss: 0.019584, mae: 0.363269, mean_q: 0.597282, mean_eps: 0.000000
 1372/5000: episode: 58, duration: 0.483s, episode steps:  20, steps per second:  41, episode reward: 35.144, mean reward:  1.757 [-3.000, 32.374], mean action: 2.800 [0.000, 11.000],  loss: 0.019861, mae: 0.362973, mean_q: 0.609313, mean_eps: 0.000000
 1386/5000: episode: 59, duration: 0.331s, episode steps:  14, steps per second:  42, episode reward: 44.157, mean reward:  3.154 [-3.000, 33.000], mean action: 2.214 [0.000, 3.000],  loss: 0.020913, mae: 0.370855, mean_q: 0.530858, mean_eps: 0.000000
 1410/5000: episode: 60, duration: 0.581s, episode steps:  24, steps per second:  41, episode reward: 35.461, mean reward:  1.478 [-2.449, 32.084], mean action: 4.208 [0.000, 19.000],  loss: 0.017040, mae: 0.349279, mean_q: 0.525614, mean_eps: 0.000000
 1429/5000: episode: 61, duration: 0.276s, episode steps:  19, steps per second:  69, episode reward: 35.656, mean reward:  1.877 [-3.000, 32.263], mean action: 5.526 [0.000, 19.000],  loss: 0.023778, mae: 0.382740, mean_q: 0.518262, mean_eps: 0.000000
 1455/5000: episode: 62, duration: 0.375s, episode steps:  26, steps per second:  69, episode reward: -41.040, mean reward: -1.578 [-33.000,  2.120], mean action: 4.654 [0.000, 19.000],  loss: 0.019114, mae: 0.370189, mean_q: 0.475441, mean_eps: 0.000000
 1475/5000: episode: 63, duration: 0.279s, episode steps:  20, steps per second:  72, episode reward: 35.902, mean reward:  1.795 [-2.308, 32.902], mean action: 4.000 [0.000, 19.000],  loss: 0.021305, mae: 0.374984, mean_q: 0.536995, mean_eps: 0.000000
 1501/5000: episode: 64, duration: 0.362s, episode steps:  26, steps per second:  72, episode reward: 32.903, mean reward:  1.266 [-3.000, 32.433], mean action: 4.038 [0.000, 19.000],  loss: 0.019968, mae: 0.369374, mean_q: 0.548550, mean_eps: 0.000000
 1518/5000: episode: 65, duration: 0.258s, episode steps:  17, steps per second:  66, episode reward: 39.000, mean reward:  2.294 [-2.689, 33.000], mean action: 3.647 [0.000, 19.000],  loss: 0.017022, mae: 0.353644, mean_q: 0.591320, mean_eps: 0.000000
 1549/5000: episode: 66, duration: 0.424s, episode steps:  31, steps per second:  73, episode reward: 35.135, mean reward:  1.133 [-2.399, 32.240], mean action: 6.839 [0.000, 20.000],  loss: 0.017006, mae: 0.352263, mean_q: 0.536279, mean_eps: 0.000000
 1563/5000: episode: 67, duration: 0.201s, episode steps:  14, steps per second:  70, episode reward: 38.309, mean reward:  2.736 [-2.178, 31.579], mean action: 4.143 [0.000, 19.000],  loss: 0.021479, mae: 0.369730, mean_q: 0.534398, mean_eps: 0.000000
 1585/5000: episode: 68, duration: 0.524s, episode steps:  22, steps per second:  42, episode reward: 32.644, mean reward:  1.484 [-3.000, 32.380], mean action: 5.545 [0.000, 19.000],  loss: 0.021787, mae: 0.372383, mean_q: 0.575167, mean_eps: 0.000000
 1607/5000: episode: 69, duration: 0.649s, episode steps:  22, steps per second:  34, episode reward: 33.000, mean reward:  1.500 [-3.000, 32.190], mean action: 3.136 [0.000, 16.000],  loss: 0.018259, mae: 0.355317, mean_q: 0.576393, mean_eps: 0.000000
 1635/5000: episode: 70, duration: 0.413s, episode steps:  28, steps per second:  68, episode reward: 32.183, mean reward:  1.149 [-3.000, 33.000], mean action: 2.786 [0.000, 16.000],  loss: 0.020247, mae: 0.370851, mean_q: 0.529336, mean_eps: 0.000000
 1710/5000: episode: 71, duration: 1.050s, episode steps:  75, steps per second:  71, episode reward: -32.740, mean reward: -0.437 [-32.030,  2.706], mean action: 7.547 [0.000, 12.000],  loss: 0.021133, mae: 0.368883, mean_q: 0.573408, mean_eps: 0.000000
 1722/5000: episode: 72, duration: 0.245s, episode steps:  12, steps per second:  49, episode reward: 47.519, mean reward:  3.960 [ 0.270, 33.000], mean action: 1.000 [0.000, 3.000],  loss: 0.021957, mae: 0.368153, mean_q: 0.494658, mean_eps: 0.000000
 1753/5000: episode: 73, duration: 0.505s, episode steps:  31, steps per second:  61, episode reward: 33.000, mean reward:  1.065 [-2.417, 32.080], mean action: 6.548 [0.000, 19.000],  loss: 0.017899, mae: 0.346949, mean_q: 0.548705, mean_eps: 0.000000
 1768/5000: episode: 74, duration: 0.282s, episode steps:  15, steps per second:  53, episode reward: 44.257, mean reward:  2.950 [-2.297, 33.000], mean action: 1.867 [0.000, 16.000],  loss: 0.019688, mae: 0.365484, mean_q: 0.563628, mean_eps: 0.000000
 1792/5000: episode: 75, duration: 0.580s, episode steps:  24, steps per second:  41, episode reward: -32.140, mean reward: -1.339 [-31.927,  2.390], mean action: 6.208 [0.000, 16.000],  loss: 0.017772, mae: 0.351486, mean_q: 0.549561, mean_eps: 0.000000
 1815/5000: episode: 76, duration: 0.442s, episode steps:  23, steps per second:  52, episode reward: 38.012, mean reward:  1.653 [-2.482, 33.000], mean action: 4.391 [0.000, 16.000],  loss: 0.014457, mae: 0.339629, mean_q: 0.570542, mean_eps: 0.000000
 1854/5000: episode: 77, duration: 0.753s, episode steps:  39, steps per second:  52, episode reward: -39.000, mean reward: -1.000 [-32.401,  2.412], mean action: 4.897 [0.000, 16.000],  loss: 0.021189, mae: 0.375059, mean_q: 0.520379, mean_eps: 0.000000
 1870/5000: episode: 78, duration: 0.272s, episode steps:  16, steps per second:  59, episode reward: 41.093, mean reward:  2.568 [-2.270, 32.130], mean action: 6.312 [0.000, 16.000],  loss: 0.019090, mae: 0.372214, mean_q: 0.514609, mean_eps: 0.000000
 1888/5000: episode: 79, duration: 0.265s, episode steps:  18, steps per second:  68, episode reward: 41.542, mean reward:  2.308 [-2.928, 33.000], mean action: 3.111 [0.000, 11.000],  loss: 0.021635, mae: 0.372467, mean_q: 0.554539, mean_eps: 0.000000
 1908/5000: episode: 80, duration: 0.457s, episode steps:  20, steps per second:  44, episode reward: 35.474, mean reward:  1.774 [-2.324, 32.094], mean action: 3.500 [0.000, 11.000],  loss: 0.025398, mae: 0.385391, mean_q: 0.576510, mean_eps: 0.000000
 1928/5000: episode: 81, duration: 0.351s, episode steps:  20, steps per second:  57, episode reward: 36.000, mean reward:  1.800 [-2.903, 32.090], mean action: 3.900 [0.000, 11.000],  loss: 0.020153, mae: 0.366508, mean_q: 0.530337, mean_eps: 0.000000
 1954/5000: episode: 82, duration: 0.736s, episode steps:  26, steps per second:  35, episode reward: 32.658, mean reward:  1.256 [-3.000, 32.044], mean action: 3.923 [0.000, 15.000],  loss: 0.019067, mae: 0.364880, mean_q: 0.579162, mean_eps: 0.000000
 1964/5000: episode: 83, duration: 0.169s, episode steps:  10, steps per second:  59, episode reward: 47.223, mean reward:  4.722 [-0.486, 32.320], mean action: 2.600 [0.000, 11.000],  loss: 0.020660, mae: 0.366990, mean_q: 0.533005, mean_eps: 0.000000
 1996/5000: episode: 84, duration: 0.465s, episode steps:  32, steps per second:  69, episode reward: 40.723, mean reward:  1.273 [-2.472, 32.050], mean action: 4.938 [0.000, 20.000],  loss: 0.017141, mae: 0.359595, mean_q: 0.503540, mean_eps: 0.000000
 2012/5000: episode: 85, duration: 0.235s, episode steps:  16, steps per second:  68, episode reward: 41.227, mean reward:  2.577 [-2.207, 32.117], mean action: 2.750 [0.000, 12.000],  loss: 0.018154, mae: 0.362960, mean_q: 0.484713, mean_eps: 0.000000
 2034/5000: episode: 86, duration: 0.360s, episode steps:  22, steps per second:  61, episode reward: -36.000, mean reward: -1.636 [-30.018,  2.746], mean action: 8.045 [0.000, 20.000],  loss: 0.019395, mae: 0.369492, mean_q: 0.482023, mean_eps: 0.000000
 2055/5000: episode: 87, duration: 0.328s, episode steps:  21, steps per second:  64, episode reward: 39.000, mean reward:  1.857 [-2.617, 32.080], mean action: 3.571 [0.000, 14.000],  loss: 0.018127, mae: 0.355958, mean_q: 0.500934, mean_eps: 0.000000
 2074/5000: episode: 88, duration: 0.287s, episode steps:  19, steps per second:  66, episode reward: 38.650, mean reward:  2.034 [-3.000, 32.184], mean action: 4.000 [0.000, 19.000],  loss: 0.020651, mae: 0.368752, mean_q: 0.584956, mean_eps: 0.000000
 2108/5000: episode: 89, duration: 0.509s, episode steps:  34, steps per second:  67, episode reward: -32.250, mean reward: -0.949 [-32.251,  3.000], mean action: 7.735 [0.000, 19.000],  loss: 0.019212, mae: 0.362323, mean_q: 0.565322, mean_eps: 0.000000
 2126/5000: episode: 90, duration: 0.357s, episode steps:  18, steps per second:  50, episode reward: 38.901, mean reward:  2.161 [-2.495, 32.371], mean action: 3.389 [0.000, 19.000],  loss: 0.018775, mae: 0.360299, mean_q: 0.485434, mean_eps: 0.000000
 2143/5000: episode: 91, duration: 0.541s, episode steps:  17, steps per second:  31, episode reward: 38.709, mean reward:  2.277 [-3.000, 32.903], mean action: 4.529 [0.000, 19.000],  loss: 0.014500, mae: 0.355749, mean_q: 0.471840, mean_eps: 0.000000
 2163/5000: episode: 92, duration: 0.361s, episode steps:  20, steps per second:  55, episode reward: 38.710, mean reward:  1.935 [-2.472, 32.270], mean action: 7.800 [0.000, 19.000],  loss: 0.020281, mae: 0.383003, mean_q: 0.497586, mean_eps: 0.000000
 2180/5000: episode: 93, duration: 0.392s, episode steps:  17, steps per second:  43, episode reward: 44.779, mean reward:  2.634 [-2.019, 32.200], mean action: 2.588 [0.000, 19.000],  loss: 0.021099, mae: 0.370630, mean_q: 0.500716, mean_eps: 0.000000
 2192/5000: episode: 94, duration: 0.218s, episode steps:  12, steps per second:  55, episode reward: 44.310, mean reward:  3.692 [-2.209, 32.903], mean action: 4.167 [0.000, 14.000],  loss: 0.022815, mae: 0.376639, mean_q: 0.548421, mean_eps: 0.000000
 2216/5000: episode: 95, duration: 0.345s, episode steps:  24, steps per second:  70, episode reward: 35.183, mean reward:  1.466 [-2.384, 31.869], mean action: 3.417 [0.000, 11.000],  loss: 0.022043, mae: 0.371844, mean_q: 0.551890, mean_eps: 0.000000
 2241/5000: episode: 96, duration: 0.355s, episode steps:  25, steps per second:  70, episode reward: -32.750, mean reward: -1.310 [-32.065,  2.280], mean action: 4.920 [0.000, 20.000],  loss: 0.018790, mae: 0.358465, mean_q: 0.542239, mean_eps: 0.000000
 2259/5000: episode: 97, duration: 0.286s, episode steps:  18, steps per second:  63, episode reward: -35.470, mean reward: -1.971 [-32.900,  2.683], mean action: 5.722 [0.000, 19.000],  loss: 0.018067, mae: 0.355682, mean_q: 0.515220, mean_eps: 0.000000
 2284/5000: episode: 98, duration: 0.507s, episode steps:  25, steps per second:  49, episode reward: -32.330, mean reward: -1.293 [-32.027,  3.000], mean action: 5.680 [0.000, 16.000],  loss: 0.019875, mae: 0.362931, mean_q: 0.490674, mean_eps: 0.000000
 2305/5000: episode: 99, duration: 0.319s, episode steps:  21, steps per second:  66, episode reward: 32.755, mean reward:  1.560 [-3.000, 32.589], mean action: 3.048 [0.000, 12.000],  loss: 0.023306, mae: 0.372968, mean_q: 0.529860, mean_eps: 0.000000
 2329/5000: episode: 100, duration: 0.472s, episode steps:  24, steps per second:  51, episode reward: 34.701, mean reward:  1.446 [-3.000, 31.958], mean action: 4.792 [0.000, 14.000],  loss: 0.016822, mae: 0.354056, mean_q: 0.512500, mean_eps: 0.000000
 2355/5000: episode: 101, duration: 0.410s, episode steps:  26, steps per second:  63, episode reward: 41.501, mean reward:  1.596 [-2.337, 31.652], mean action: 2.577 [0.000, 9.000],  loss: 0.018651, mae: 0.361518, mean_q: 0.486929, mean_eps: 0.000000
 2383/5000: episode: 102, duration: 0.403s, episode steps:  28, steps per second:  70, episode reward: -33.000, mean reward: -1.179 [-32.251,  3.000], mean action: 4.321 [0.000, 12.000],  loss: 0.022056, mae: 0.370290, mean_q: 0.554501, mean_eps: 0.000000
 2404/5000: episode: 103, duration: 0.292s, episode steps:  21, steps per second:  72, episode reward: -33.000, mean reward: -1.571 [-29.931,  2.951], mean action: 5.333 [0.000, 16.000],  loss: 0.018001, mae: 0.360971, mean_q: 0.561729, mean_eps: 0.000000
 2427/5000: episode: 104, duration: 0.326s, episode steps:  23, steps per second:  71, episode reward: 37.787, mean reward:  1.643 [-3.000, 32.320], mean action: 4.652 [0.000, 16.000],  loss: 0.020613, mae: 0.366487, mean_q: 0.577478, mean_eps: 0.000000
 2457/5000: episode: 105, duration: 0.468s, episode steps:  30, steps per second:  64, episode reward: 32.395, mean reward:  1.080 [-2.876, 32.039], mean action: 8.633 [0.000, 19.000],  loss: 0.021743, mae: 0.378946, mean_q: 0.582977, mean_eps: 0.000000
 2486/5000: episode: 106, duration: 0.408s, episode steps:  29, steps per second:  71, episode reward: -32.230, mean reward: -1.111 [-32.059,  2.420], mean action: 6.517 [0.000, 19.000],  loss: 0.020142, mae: 0.369532, mean_q: 0.551001, mean_eps: 0.000000
 2513/5000: episode: 107, duration: 0.426s, episode steps:  27, steps per second:  63, episode reward: -35.060, mean reward: -1.299 [-32.670,  2.520], mean action: 7.778 [1.000, 19.000],  loss: 0.020688, mae: 0.374780, mean_q: 0.543854, mean_eps: 0.000000
 2535/5000: episode: 108, duration: 0.311s, episode steps:  22, steps per second:  71, episode reward: 32.589, mean reward:  1.481 [-2.441, 31.919], mean action: 4.909 [0.000, 19.000],  loss: 0.021236, mae: 0.379793, mean_q: 0.517172, mean_eps: 0.000000
 2560/5000: episode: 109, duration: 0.352s, episode steps:  25, steps per second:  71, episode reward: 32.094, mean reward:  1.284 [-3.000, 32.020], mean action: 4.360 [0.000, 15.000],  loss: 0.018338, mae: 0.362327, mean_q: 0.542213, mean_eps: 0.000000
 2575/5000: episode: 110, duration: 0.217s, episode steps:  15, steps per second:  69, episode reward: 41.335, mean reward:  2.756 [-2.265, 32.464], mean action: 5.533 [0.000, 14.000],  loss: 0.019820, mae: 0.361696, mean_q: 0.555988, mean_eps: 0.000000
 2589/5000: episode: 111, duration: 0.227s, episode steps:  14, steps per second:  62, episode reward: 41.141, mean reward:  2.939 [-2.167, 31.687], mean action: 4.214 [0.000, 15.000],  loss: 0.022953, mae: 0.380729, mean_q: 0.532953, mean_eps: 0.000000
 2614/5000: episode: 112, duration: 0.415s, episode steps:  25, steps per second:  60, episode reward: 32.538, mean reward:  1.302 [-3.000, 33.000], mean action: 6.920 [0.000, 19.000],  loss: 0.020452, mae: 0.369802, mean_q: 0.546529, mean_eps: 0.000000
 2644/5000: episode: 113, duration: 0.547s, episode steps:  30, steps per second:  55, episode reward: 34.497, mean reward:  1.150 [-2.418, 32.010], mean action: 7.533 [0.000, 19.000],  loss: 0.020463, mae: 0.360287, mean_q: 0.571507, mean_eps: 0.000000
 2667/5000: episode: 114, duration: 0.371s, episode steps:  23, steps per second:  62, episode reward: 35.091, mean reward:  1.526 [-2.502, 32.106], mean action: 7.130 [0.000, 19.000],  loss: 0.019489, mae: 0.360358, mean_q: 0.583539, mean_eps: 0.000000
 2691/5000: episode: 115, duration: 0.346s, episode steps:  24, steps per second:  69, episode reward: 39.000, mean reward:  1.625 [-2.156, 32.615], mean action: 4.750 [0.000, 19.000],  loss: 0.022345, mae: 0.379791, mean_q: 0.509924, mean_eps: 0.000000
 2716/5000: episode: 116, duration: 0.358s, episode steps:  25, steps per second:  70, episode reward: -35.360, mean reward: -1.414 [-32.010,  2.553], mean action: 6.640 [0.000, 19.000],  loss: 0.023190, mae: 0.378642, mean_q: 0.505439, mean_eps: 0.000000
 2737/5000: episode: 117, duration: 0.304s, episode steps:  21, steps per second:  69, episode reward: 41.329, mean reward:  1.968 [-2.846, 32.070], mean action: 4.952 [0.000, 19.000],  loss: 0.017937, mae: 0.356751, mean_q: 0.547541, mean_eps: 0.000000
 2758/5000: episode: 118, duration: 0.295s, episode steps:  21, steps per second:  71, episode reward: -35.470, mean reward: -1.689 [-31.627,  2.260], mean action: 6.810 [0.000, 21.000],  loss: 0.021958, mae: 0.377859, mean_q: 0.550945, mean_eps: 0.000000
 2783/5000: episode: 119, duration: 0.391s, episode steps:  25, steps per second:  64, episode reward: 38.034, mean reward:  1.521 [-2.126, 32.213], mean action: 2.840 [0.000, 11.000],  loss: 0.019382, mae: 0.367804, mean_q: 0.543853, mean_eps: 0.000000
 2808/5000: episode: 120, duration: 0.624s, episode steps:  25, steps per second:  40, episode reward: -35.940, mean reward: -1.438 [-33.000,  2.534], mean action: 5.560 [0.000, 16.000],  loss: 0.022076, mae: 0.386346, mean_q: 0.511217, mean_eps: 0.000000
 2837/5000: episode: 121, duration: 0.531s, episode steps:  29, steps per second:  55, episode reward: -32.120, mean reward: -1.108 [-32.900,  2.635], mean action: 6.586 [0.000, 19.000],  loss: 0.020989, mae: 0.378748, mean_q: 0.513101, mean_eps: 0.000000
 2869/5000: episode: 122, duration: 0.588s, episode steps:  32, steps per second:  54, episode reward: 37.290, mean reward:  1.165 [-2.447, 32.722], mean action: 5.531 [0.000, 21.000],  loss: 0.020127, mae: 0.377691, mean_q: 0.499000, mean_eps: 0.000000
 2888/5000: episode: 123, duration: 0.459s, episode steps:  19, steps per second:  41, episode reward: 38.247, mean reward:  2.013 [-2.459, 33.000], mean action: 5.579 [0.000, 12.000],  loss: 0.018734, mae: 0.369818, mean_q: 0.559031, mean_eps: 0.000000
 2913/5000: episode: 124, duration: 0.655s, episode steps:  25, steps per second:  38, episode reward: 32.498, mean reward:  1.300 [-3.000, 31.858], mean action: 3.800 [0.000, 12.000],  loss: 0.022836, mae: 0.391639, mean_q: 0.502222, mean_eps: 0.000000
 2934/5000: episode: 125, duration: 0.517s, episode steps:  21, steps per second:  41, episode reward: 35.218, mean reward:  1.677 [-3.000, 33.000], mean action: 3.810 [0.000, 16.000],  loss: 0.020602, mae: 0.378273, mean_q: 0.462749, mean_eps: 0.000000
 2989/5000: episode: 126, duration: 1.133s, episode steps:  55, steps per second:  49, episode reward: 32.475, mean reward:  0.590 [-2.335, 32.002], mean action: 9.055 [0.000, 19.000],  loss: 0.022405, mae: 0.380345, mean_q: 0.541017, mean_eps: 0.000000
 3028/5000: episode: 127, duration: 0.897s, episode steps:  39, steps per second:  43, episode reward: 43.790, mean reward:  1.123 [-2.330, 31.918], mean action: 2.846 [0.000, 12.000],  loss: 0.019848, mae: 0.366305, mean_q: 0.578437, mean_eps: 0.000000
 3047/5000: episode: 128, duration: 0.421s, episode steps:  19, steps per second:  45, episode reward: -30.000, mean reward: -1.579 [-30.000,  3.000], mean action: 4.053 [0.000, 12.000],  loss: 0.016162, mae: 0.352416, mean_q: 0.549745, mean_eps: 0.000000
 3069/5000: episode: 129, duration: 0.408s, episode steps:  22, steps per second:  54, episode reward: 42.000, mean reward:  1.909 [-2.633, 32.460], mean action: 3.682 [0.000, 15.000],  loss: 0.020576, mae: 0.369100, mean_q: 0.571706, mean_eps: 0.000000
 3092/5000: episode: 130, duration: 0.349s, episode steps:  23, steps per second:  66, episode reward: 41.236, mean reward:  1.793 [-2.285, 32.160], mean action: 1.435 [0.000, 15.000],  loss: 0.017668, mae: 0.345920, mean_q: 0.581480, mean_eps: 0.000000
 3127/5000: episode: 131, duration: 0.477s, episode steps:  35, steps per second:  73, episode reward: 32.802, mean reward:  0.937 [-3.000, 32.081], mean action: 5.486 [0.000, 12.000],  loss: 0.019717, mae: 0.361948, mean_q: 0.587618, mean_eps: 0.000000
 3147/5000: episode: 132, duration: 0.303s, episode steps:  20, steps per second:  66, episode reward: 38.430, mean reward:  1.922 [-2.730, 31.725], mean action: 6.300 [0.000, 19.000],  loss: 0.018963, mae: 0.368439, mean_q: 0.494091, mean_eps: 0.000000
 3170/5000: episode: 133, duration: 0.625s, episode steps:  23, steps per second:  37, episode reward: -42.000, mean reward: -1.826 [-32.626,  2.093], mean action: 7.435 [0.000, 19.000],  loss: 0.018196, mae: 0.363325, mean_q: 0.446016, mean_eps: 0.000000
 3193/5000: episode: 134, duration: 0.512s, episode steps:  23, steps per second:  45, episode reward: 41.338, mean reward:  1.797 [-3.000, 32.010], mean action: 5.043 [1.000, 14.000],  loss: 0.015862, mae: 0.356498, mean_q: 0.420997, mean_eps: 0.000000
 3221/5000: episode: 135, duration: 0.657s, episode steps:  28, steps per second:  43, episode reward: 32.645, mean reward:  1.166 [-3.000, 32.210], mean action: 4.607 [0.000, 12.000],  loss: 0.019838, mae: 0.363661, mean_q: 0.509701, mean_eps: 0.000000
 3248/5000: episode: 136, duration: 0.664s, episode steps:  27, steps per second:  41, episode reward: 35.008, mean reward:  1.297 [-2.671, 32.050], mean action: 4.815 [0.000, 16.000],  loss: 0.023305, mae: 0.381421, mean_q: 0.523966, mean_eps: 0.000000
 3277/5000: episode: 137, duration: 0.497s, episode steps:  29, steps per second:  58, episode reward: 35.269, mean reward:  1.216 [-2.488, 31.850], mean action: 4.586 [0.000, 15.000],  loss: 0.021405, mae: 0.367256, mean_q: 0.504901, mean_eps: 0.000000
 3309/5000: episode: 138, duration: 0.450s, episode steps:  32, steps per second:  71, episode reward: 38.513, mean reward:  1.204 [-2.721, 32.001], mean action: 5.562 [0.000, 21.000],  loss: 0.020230, mae: 0.364535, mean_q: 0.523832, mean_eps: 0.000000
 3335/5000: episode: 139, duration: 0.352s, episode steps:  26, steps per second:  74, episode reward: -35.020, mean reward: -1.347 [-32.619,  2.248], mean action: 9.885 [0.000, 21.000],  loss: 0.022675, mae: 0.381387, mean_q: 0.509665, mean_eps: 0.000000
 3358/5000: episode: 140, duration: 0.338s, episode steps:  23, steps per second:  68, episode reward: 37.965, mean reward:  1.651 [-2.589, 32.903], mean action: 5.565 [0.000, 16.000],  loss: 0.021689, mae: 0.370521, mean_q: 0.520827, mean_eps: 0.000000
 3387/5000: episode: 141, duration: 0.445s, episode steps:  29, steps per second:  65, episode reward: 38.348, mean reward:  1.322 [-2.140, 32.070], mean action: 4.310 [0.000, 16.000],  loss: 0.020620, mae: 0.365399, mean_q: 0.473075, mean_eps: 0.000000
 3414/5000: episode: 142, duration: 0.377s, episode steps:  27, steps per second:  72, episode reward: 32.163, mean reward:  1.191 [-3.000, 32.042], mean action: 8.111 [1.000, 16.000],  loss: 0.018764, mae: 0.364014, mean_q: 0.472730, mean_eps: 0.000000
 3438/5000: episode: 143, duration: 0.335s, episode steps:  24, steps per second:  72, episode reward: 35.849, mean reward:  1.494 [-2.470, 32.549], mean action: 5.583 [1.000, 16.000],  loss: 0.020228, mae: 0.363121, mean_q: 0.505255, mean_eps: 0.000000
 3459/5000: episode: 144, duration: 0.300s, episode steps:  21, steps per second:  70, episode reward: 32.602, mean reward:  1.552 [-3.000, 32.796], mean action: 4.810 [0.000, 15.000],  loss: 0.020361, mae: 0.358522, mean_q: 0.509937, mean_eps: 0.000000
 3482/5000: episode: 145, duration: 0.324s, episode steps:  23, steps per second:  71, episode reward: 33.000, mean reward:  1.435 [-2.438, 30.762], mean action: 6.391 [0.000, 20.000],  loss: 0.022726, mae: 0.370170, mean_q: 0.553418, mean_eps: 0.000000
 3504/5000: episode: 146, duration: 0.305s, episode steps:  22, steps per second:  72, episode reward: 37.651, mean reward:  1.711 [-3.000, 33.000], mean action: 4.727 [0.000, 16.000],  loss: 0.022432, mae: 0.372083, mean_q: 0.563842, mean_eps: 0.000000
 3525/5000: episode: 147, duration: 0.299s, episode steps:  21, steps per second:  70, episode reward: 35.430, mean reward:  1.687 [-2.312, 32.300], mean action: 7.286 [0.000, 16.000],  loss: 0.020729, mae: 0.361911, mean_q: 0.565311, mean_eps: 0.000000
 3551/5000: episode: 148, duration: 0.359s, episode steps:  26, steps per second:  72, episode reward: 44.280, mean reward:  1.703 [-2.534, 32.671], mean action: 1.423 [0.000, 16.000],  loss: 0.018782, mae: 0.348703, mean_q: 0.594873, mean_eps: 0.000000
 3570/5000: episode: 149, duration: 0.267s, episode steps:  19, steps per second:  71, episode reward: 36.000, mean reward:  1.895 [-2.900, 32.360], mean action: 3.947 [0.000, 15.000],  loss: 0.021042, mae: 0.364729, mean_q: 0.596239, mean_eps: 0.000000
 3592/5000: episode: 150, duration: 0.318s, episode steps:  22, steps per second:  69, episode reward: 38.936, mean reward:  1.770 [-2.243, 32.206], mean action: 4.818 [0.000, 19.000],  loss: 0.020306, mae: 0.361678, mean_q: 0.559787, mean_eps: 0.000000
 3611/5000: episode: 151, duration: 0.271s, episode steps:  19, steps per second:  70, episode reward: 41.149, mean reward:  2.166 [-3.000, 32.361], mean action: 3.632 [0.000, 19.000],  loss: 0.021406, mae: 0.366481, mean_q: 0.550396, mean_eps: 0.000000
 3643/5000: episode: 152, duration: 0.470s, episode steps:  32, steps per second:  68, episode reward: 35.513, mean reward:  1.110 [-3.000, 32.030], mean action: 3.750 [0.000, 19.000],  loss: 0.017110, mae: 0.345812, mean_q: 0.574222, mean_eps: 0.000000
 3670/5000: episode: 153, duration: 0.371s, episode steps:  27, steps per second:  73, episode reward: 35.250, mean reward:  1.306 [-2.805, 32.560], mean action: 3.815 [0.000, 16.000],  loss: 0.022129, mae: 0.376516, mean_q: 0.527340, mean_eps: 0.000000
 3691/5000: episode: 154, duration: 0.314s, episode steps:  21, steps per second:  67, episode reward: -32.780, mean reward: -1.561 [-32.780,  2.560], mean action: 5.571 [0.000, 19.000],  loss: 0.019247, mae: 0.364757, mean_q: 0.504117, mean_eps: 0.000000
 3708/5000: episode: 155, duration: 0.259s, episode steps:  17, steps per second:  66, episode reward: 38.026, mean reward:  2.237 [-2.234, 31.066], mean action: 4.118 [0.000, 16.000],  loss: 0.019157, mae: 0.375638, mean_q: 0.529510, mean_eps: 0.000000
 3748/5000: episode: 156, duration: 0.681s, episode steps:  40, steps per second:  59, episode reward: 32.261, mean reward:  0.807 [-2.753, 32.540], mean action: 5.300 [0.000, 16.000],  loss: 0.021176, mae: 0.379985, mean_q: 0.549702, mean_eps: 0.000000
 3780/5000: episode: 157, duration: 0.436s, episode steps:  32, steps per second:  73, episode reward: -32.340, mean reward: -1.011 [-32.172,  2.323], mean action: 5.125 [0.000, 20.000],  loss: 0.020945, mae: 0.377859, mean_q: 0.529767, mean_eps: 0.000000
 3798/5000: episode: 158, duration: 0.290s, episode steps:  18, steps per second:  62, episode reward: 35.903, mean reward:  1.995 [-2.580, 32.903], mean action: 4.556 [0.000, 12.000],  loss: 0.017611, mae: 0.360212, mean_q: 0.555903, mean_eps: 0.000000
 3818/5000: episode: 159, duration: 0.302s, episode steps:  20, steps per second:  66, episode reward: 36.000, mean reward:  1.800 [-2.514, 32.050], mean action: 3.900 [0.000, 19.000],  loss: 0.017522, mae: 0.364429, mean_q: 0.562978, mean_eps: 0.000000
 3837/5000: episode: 160, duration: 0.341s, episode steps:  19, steps per second:  56, episode reward: 35.869, mean reward:  1.888 [-3.000, 32.249], mean action: 4.474 [0.000, 19.000],  loss: 0.019918, mae: 0.381683, mean_q: 0.524654, mean_eps: 0.000000
 3861/5000: episode: 161, duration: 0.396s, episode steps:  24, steps per second:  61, episode reward: 41.720, mean reward:  1.738 [-2.622, 32.217], mean action: 2.708 [0.000, 9.000],  loss: 0.023816, mae: 0.394749, mean_q: 0.521770, mean_eps: 0.000000
 3881/5000: episode: 162, duration: 0.285s, episode steps:  20, steps per second:  70, episode reward: 35.150, mean reward:  1.758 [-2.353, 32.360], mean action: 4.500 [0.000, 13.000],  loss: 0.022126, mae: 0.380673, mean_q: 0.560315, mean_eps: 0.000000
 3901/5000: episode: 163, duration: 0.286s, episode steps:  20, steps per second:  70, episode reward: 33.000, mean reward:  1.650 [-3.000, 30.211], mean action: 5.350 [0.000, 14.000],  loss: 0.016036, mae: 0.355253, mean_q: 0.539275, mean_eps: 0.000000
 3919/5000: episode: 164, duration: 0.258s, episode steps:  18, steps per second:  70, episode reward: 39.000, mean reward:  2.167 [-2.593, 32.370], mean action: 3.667 [0.000, 12.000],  loss: 0.020263, mae: 0.379040, mean_q: 0.500493, mean_eps: 0.000000
 3939/5000: episode: 165, duration: 0.310s, episode steps:  20, steps per second:  65, episode reward: 38.180, mean reward:  1.909 [-2.394, 32.203], mean action: 4.250 [0.000, 14.000],  loss: 0.022437, mae: 0.382701, mean_q: 0.538830, mean_eps: 0.000000
 3963/5000: episode: 166, duration: 0.365s, episode steps:  24, steps per second:  66, episode reward: 35.902, mean reward:  1.496 [-2.589, 32.212], mean action: 4.375 [0.000, 13.000],  loss: 0.019500, mae: 0.367157, mean_q: 0.525725, mean_eps: 0.000000
 3984/5000: episode: 167, duration: 0.291s, episode steps:  21, steps per second:  72, episode reward: 36.000, mean reward:  1.714 [-2.171, 32.420], mean action: 3.000 [0.000, 9.000],  loss: 0.019909, mae: 0.372304, mean_q: 0.530067, mean_eps: 0.000000
 4009/5000: episode: 168, duration: 0.345s, episode steps:  25, steps per second:  72, episode reward: -32.010, mean reward: -1.280 [-32.258,  2.579], mean action: 5.160 [0.000, 15.000],  loss: 0.020414, mae: 0.373786, mean_q: 0.536995, mean_eps: 0.000000
 4028/5000: episode: 169, duration: 0.266s, episode steps:  19, steps per second:  71, episode reward: 36.000, mean reward:  1.895 [-3.000, 32.210], mean action: 4.105 [1.000, 15.000],  loss: 0.022946, mae: 0.386936, mean_q: 0.538837, mean_eps: 0.000000
 4054/5000: episode: 170, duration: 0.741s, episode steps:  26, steps per second:  35, episode reward: -32.220, mean reward: -1.239 [-31.530,  2.560], mean action: 4.192 [0.000, 12.000],  loss: 0.020863, mae: 0.370424, mean_q: 0.513301, mean_eps: 0.000000
 4076/5000: episode: 171, duration: 0.457s, episode steps:  22, steps per second:  48, episode reward: -33.000, mean reward: -1.500 [-32.688,  2.694], mean action: 7.455 [0.000, 15.000],  loss: 0.016192, mae: 0.349670, mean_q: 0.484546, mean_eps: 0.000000
 4101/5000: episode: 172, duration: 0.360s, episode steps:  25, steps per second:  69, episode reward: 38.103, mean reward:  1.524 [-3.000, 32.360], mean action: 7.280 [0.000, 16.000],  loss: 0.019874, mae: 0.367756, mean_q: 0.462094, mean_eps: 0.000000
 4120/5000: episode: 173, duration: 0.268s, episode steps:  19, steps per second:  71, episode reward: 37.935, mean reward:  1.997 [-2.344, 32.842], mean action: 3.579 [0.000, 16.000],  loss: 0.022046, mae: 0.379831, mean_q: 0.497008, mean_eps: 0.000000
 4141/5000: episode: 174, duration: 0.755s, episode steps:  21, steps per second:  28, episode reward: 36.000, mean reward:  1.714 [-2.242, 32.450], mean action: 2.762 [0.000, 16.000],  loss: 0.017344, mae: 0.350984, mean_q: 0.526652, mean_eps: 0.000000
 4159/5000: episode: 175, duration: 0.358s, episode steps:  18, steps per second:  50, episode reward: 41.843, mean reward:  2.325 [-2.729, 32.190], mean action: 3.333 [0.000, 16.000],  loss: 0.021271, mae: 0.368702, mean_q: 0.504197, mean_eps: 0.000000
 4186/5000: episode: 176, duration: 0.650s, episode steps:  27, steps per second:  42, episode reward: 38.070, mean reward:  1.410 [-3.000, 32.006], mean action: 2.704 [0.000, 9.000],  loss: 0.022472, mae: 0.380014, mean_q: 0.522109, mean_eps: 0.000000
 4205/5000: episode: 177, duration: 0.401s, episode steps:  19, steps per second:  47, episode reward: 38.942, mean reward:  2.050 [-2.548, 32.300], mean action: 4.579 [0.000, 21.000],  loss: 0.021461, mae: 0.371008, mean_q: 0.529056, mean_eps: 0.000000
 4226/5000: episode: 178, duration: 0.351s, episode steps:  21, steps per second:  60, episode reward: 38.625, mean reward:  1.839 [-2.631, 32.625], mean action: 3.619 [0.000, 20.000],  loss: 0.019834, mae: 0.365885, mean_q: 0.545760, mean_eps: 0.000000
 4253/5000: episode: 179, duration: 0.466s, episode steps:  27, steps per second:  58, episode reward: -38.530, mean reward: -1.427 [-32.200,  2.641], mean action: 2.889 [0.000, 15.000],  loss: 0.018960, mae: 0.368303, mean_q: 0.511412, mean_eps: 0.000000
 4275/5000: episode: 180, duration: 0.384s, episode steps:  22, steps per second:  57, episode reward: -44.840, mean reward: -2.038 [-32.216,  1.762], mean action: 9.455 [2.000, 16.000],  loss: 0.014866, mae: 0.340874, mean_q: 0.460035, mean_eps: 0.000000
 4301/5000: episode: 181, duration: 0.397s, episode steps:  26, steps per second:  66, episode reward: -32.140, mean reward: -1.236 [-31.941,  3.000], mean action: 6.923 [0.000, 21.000],  loss: 0.018161, mae: 0.359845, mean_q: 0.499104, mean_eps: 0.000000
 4323/5000: episode: 182, duration: 0.329s, episode steps:  22, steps per second:  67, episode reward: 40.661, mean reward:  1.848 [-2.330, 32.400], mean action: 3.818 [0.000, 15.000],  loss: 0.018691, mae: 0.359553, mean_q: 0.513426, mean_eps: 0.000000
 4352/5000: episode: 183, duration: 0.399s, episode steps:  29, steps per second:  73, episode reward: -35.200, mean reward: -1.214 [-32.366,  2.375], mean action: 6.448 [0.000, 21.000],  loss: 0.019461, mae: 0.366224, mean_q: 0.483948, mean_eps: 0.000000
 4380/5000: episode: 184, duration: 0.438s, episode steps:  28, steps per second:  64, episode reward: 32.189, mean reward:  1.150 [-2.578, 32.191], mean action: 6.821 [0.000, 19.000],  loss: 0.021315, mae: 0.376191, mean_q: 0.484986, mean_eps: 0.000000
 4400/5000: episode: 185, duration: 0.295s, episode steps:  20, steps per second:  68, episode reward: 35.183, mean reward:  1.759 [-3.000, 32.160], mean action: 7.050 [0.000, 21.000],  loss: 0.016447, mae: 0.347487, mean_q: 0.498940, mean_eps: 0.000000
 4420/5000: episode: 186, duration: 0.300s, episode steps:  20, steps per second:  67, episode reward: 38.482, mean reward:  1.924 [-2.496, 32.180], mean action: 2.700 [0.000, 9.000],  loss: 0.018895, mae: 0.358901, mean_q: 0.519115, mean_eps: 0.000000
 4444/5000: episode: 187, duration: 0.328s, episode steps:  24, steps per second:  73, episode reward: 33.000, mean reward:  1.375 [-2.264, 32.080], mean action: 4.625 [0.000, 19.000],  loss: 0.019254, mae: 0.369787, mean_q: 0.519279, mean_eps: 0.000000
 4464/5000: episode: 188, duration: 0.280s, episode steps:  20, steps per second:  72, episode reward: 38.523, mean reward:  1.926 [-2.525, 32.233], mean action: 3.250 [0.000, 15.000],  loss: 0.023261, mae: 0.390080, mean_q: 0.530672, mean_eps: 0.000000
 4498/5000: episode: 189, duration: 0.503s, episode steps:  34, steps per second:  68, episode reward: -32.980, mean reward: -0.970 [-32.131,  2.810], mean action: 7.676 [0.000, 19.000],  loss: 0.021669, mae: 0.383523, mean_q: 0.544098, mean_eps: 0.000000
 4532/5000: episode: 190, duration: 0.453s, episode steps:  34, steps per second:  75, episode reward: -33.000, mean reward: -0.971 [-32.505,  2.650], mean action: 4.000 [0.000, 16.000],  loss: 0.020061, mae: 0.382164, mean_q: 0.511010, mean_eps: 0.000000
 4553/5000: episode: 191, duration: 0.293s, episode steps:  21, steps per second:  72, episode reward: 35.614, mean reward:  1.696 [-2.318, 31.874], mean action: 2.762 [0.000, 11.000],  loss: 0.022874, mae: 0.396710, mean_q: 0.479934, mean_eps: 0.000000
 4569/5000: episode: 192, duration: 0.234s, episode steps:  16, steps per second:  68, episode reward: 35.056, mean reward:  2.191 [-2.902, 32.900], mean action: 4.625 [0.000, 16.000],  loss: 0.020273, mae: 0.382182, mean_q: 0.532572, mean_eps: 0.000000
 4589/5000: episode: 193, duration: 0.302s, episode steps:  20, steps per second:  66, episode reward: 38.212, mean reward:  1.911 [-2.399, 32.120], mean action: 2.450 [1.000, 11.000],  loss: 0.017006, mae: 0.368180, mean_q: 0.559191, mean_eps: 0.000000
 4618/5000: episode: 194, duration: 0.496s, episode steps:  29, steps per second:  58, episode reward: 33.000, mean reward:  1.138 [-3.000, 32.080], mean action: 3.138 [0.000, 11.000],  loss: 0.017560, mae: 0.367337, mean_q: 0.513323, mean_eps: 0.000000
 4646/5000: episode: 195, duration: 0.685s, episode steps:  28, steps per second:  41, episode reward: -32.450, mean reward: -1.159 [-32.154,  2.660], mean action: 8.214 [0.000, 21.000],  loss: 0.019649, mae: 0.376929, mean_q: 0.481016, mean_eps: 0.000000
 4671/5000: episode: 196, duration: 0.372s, episode steps:  25, steps per second:  67, episode reward: 39.000, mean reward:  1.560 [-2.309, 32.330], mean action: 3.440 [0.000, 21.000],  loss: 0.021026, mae: 0.371752, mean_q: 0.509270, mean_eps: 0.000000
 4688/5000: episode: 197, duration: 0.351s, episode steps:  17, steps per second:  48, episode reward: 35.473, mean reward:  2.087 [-2.579, 32.290], mean action: 3.412 [0.000, 9.000],  loss: 0.025290, mae: 0.391732, mean_q: 0.554343, mean_eps: 0.000000
 4707/5000: episode: 198, duration: 0.309s, episode steps:  19, steps per second:  62, episode reward: -38.310, mean reward: -2.016 [-32.740,  2.233], mean action: 3.684 [0.000, 12.000],  loss: 0.018884, mae: 0.369198, mean_q: 0.571355, mean_eps: 0.000000
 4729/5000: episode: 199, duration: 0.315s, episode steps:  22, steps per second:  70, episode reward: 41.280, mean reward:  1.876 [-2.976, 32.126], mean action: 1.364 [0.000, 9.000],  loss: 0.019093, mae: 0.374967, mean_q: 0.544087, mean_eps: 0.000000
 4746/5000: episode: 200, duration: 0.238s, episode steps:  17, steps per second:  72, episode reward: 38.888, mean reward:  2.288 [-2.538, 32.020], mean action: 3.059 [0.000, 9.000],  loss: 0.018075, mae: 0.371909, mean_q: 0.496134, mean_eps: 0.000000
 4767/5000: episode: 201, duration: 0.289s, episode steps:  21, steps per second:  73, episode reward: 32.562, mean reward:  1.551 [-2.751, 32.562], mean action: 3.095 [0.000, 12.000],  loss: 0.018328, mae: 0.363934, mean_q: 0.553973, mean_eps: 0.000000
 4797/5000: episode: 202, duration: 0.556s, episode steps:  30, steps per second:  54, episode reward: 33.000, mean reward:  1.100 [-3.000, 32.150], mean action: 8.100 [0.000, 17.000],  loss: 0.021886, mae: 0.386100, mean_q: 0.584367, mean_eps: 0.000000
 4817/5000: episode: 203, duration: 0.325s, episode steps:  20, steps per second:  61, episode reward: 37.971, mean reward:  1.899 [-2.578, 32.180], mean action: 4.100 [0.000, 16.000],  loss: 0.020280, mae: 0.376088, mean_q: 0.542128, mean_eps: 0.000000
 4845/5000: episode: 204, duration: 0.441s, episode steps:  28, steps per second:  64, episode reward: -32.910, mean reward: -1.175 [-32.149,  2.296], mean action: 4.036 [0.000, 16.000],  loss: 0.023737, mae: 0.398688, mean_q: 0.538749, mean_eps: 0.000000
 4879/5000: episode: 205, duration: 0.691s, episode steps:  34, steps per second:  49, episode reward: 39.000, mean reward:  1.147 [-2.529, 32.230], mean action: 3.941 [1.000, 16.000],  loss: 0.018141, mae: 0.373442, mean_q: 0.535831, mean_eps: 0.000000
 4902/5000: episode: 206, duration: 0.340s, episode steps:  23, steps per second:  68, episode reward: 37.845, mean reward:  1.645 [-2.206, 32.250], mean action: 2.739 [0.000, 16.000],  loss: 0.017783, mae: 0.364441, mean_q: 0.520298, mean_eps: 0.000000
 4926/5000: episode: 207, duration: 0.465s, episode steps:  24, steps per second:  52, episode reward: 35.897, mean reward:  1.496 [-2.421, 31.996], mean action: 3.542 [0.000, 14.000],  loss: 0.017261, mae: 0.362375, mean_q: 0.595154, mean_eps: 0.000000
 4942/5000: episode: 208, duration: 0.293s, episode steps:  16, steps per second:  55, episode reward: 41.284, mean reward:  2.580 [-2.297, 32.310], mean action: 2.750 [0.000, 9.000],  loss: 0.022140, mae: 0.381335, mean_q: 0.552623, mean_eps: 0.000000
 4961/5000: episode: 209, duration: 0.276s, episode steps:  19, steps per second:  69, episode reward: 38.757, mean reward:  2.040 [-2.349, 32.210], mean action: 4.684 [1.000, 14.000],  loss: 0.019841, mae: 0.371621, mean_q: 0.537622, mean_eps: 0.000000
 4992/5000: episode: 210, duration: 0.420s, episode steps:  31, steps per second:  74, episode reward: -42.000, mean reward: -1.355 [-32.118,  2.102], mean action: 8.710 [1.000, 14.000],  loss: 0.018732, mae: 0.373386, mean_q: 0.523127, mean_eps: 0.000000
done, took 79.867 seconds
DQN Evaluation: 5778 victories out of 6830 episodes
Training for 5000 steps ...
   28/5000: episode: 1, duration: 0.221s, episode steps:  28, steps per second: 127, episode reward: 41.579, mean reward:  1.485 [-2.587, 31.994], mean action: 5.821 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   45/5000: episode: 2, duration: 0.180s, episode steps:  17, steps per second:  95, episode reward: 41.781, mean reward:  2.458 [-2.420, 32.463], mean action: 3.882 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   65/5000: episode: 3, duration: 0.148s, episode steps:  20, steps per second: 135, episode reward: 38.083, mean reward:  1.904 [-2.695, 32.670], mean action: 4.250 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   77/5000: episode: 4, duration: 0.099s, episode steps:  12, steps per second: 122, episode reward: 44.123, mean reward:  3.677 [-2.572, 32.280], mean action: 4.750 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  110/5000: episode: 5, duration: 0.229s, episode steps:  33, steps per second: 144, episode reward: 32.549, mean reward:  0.986 [-2.794, 32.420], mean action: 6.727 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  139/5000: episode: 6, duration: 0.193s, episode steps:  29, steps per second: 151, episode reward: 41.690, mean reward:  1.438 [-2.305, 32.270], mean action: 3.828 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  164/5000: episode: 7, duration: 0.167s, episode steps:  25, steps per second: 150, episode reward: 41.879, mean reward:  1.675 [-2.431, 32.180], mean action: 2.680 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  180/5000: episode: 8, duration: 0.108s, episode steps:  16, steps per second: 148, episode reward: 44.603, mean reward:  2.788 [-2.015, 32.603], mean action: 2.750 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  211/5000: episode: 9, duration: 0.204s, episode steps:  31, steps per second: 152, episode reward: 40.794, mean reward:  1.316 [-2.841, 31.895], mean action: 5.484 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  237/5000: episode: 10, duration: 0.174s, episode steps:  26, steps per second: 150, episode reward: 38.295, mean reward:  1.473 [-2.940, 32.200], mean action: 2.885 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  262/5000: episode: 11, duration: 0.348s, episode steps:  25, steps per second:  72, episode reward: 43.512, mean reward:  1.740 [-2.225, 32.250], mean action: 2.880 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  291/5000: episode: 12, duration: 0.189s, episode steps:  29, steps per second: 153, episode reward: 40.426, mean reward:  1.394 [-2.998, 32.190], mean action: 6.172 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  312/5000: episode: 13, duration: 0.153s, episode steps:  21, steps per second: 137, episode reward: 36.000, mean reward:  1.714 [-2.619, 32.100], mean action: 6.429 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  358/5000: episode: 14, duration: 0.318s, episode steps:  46, steps per second: 145, episode reward: 38.835, mean reward:  0.844 [-2.262, 32.210], mean action: 3.891 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  395/5000: episode: 15, duration: 0.234s, episode steps:  37, steps per second: 158, episode reward: 38.690, mean reward:  1.046 [-3.000, 32.120], mean action: 4.135 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  413/5000: episode: 16, duration: 0.130s, episode steps:  18, steps per second: 139, episode reward: 44.428, mean reward:  2.468 [-2.498, 32.100], mean action: 0.944 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  439/5000: episode: 17, duration: 0.175s, episode steps:  26, steps per second: 148, episode reward: 41.069, mean reward:  1.580 [-2.539, 32.171], mean action: 1.731 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  461/5000: episode: 18, duration: 0.151s, episode steps:  22, steps per second: 145, episode reward: 44.148, mean reward:  2.007 [-1.773, 31.937], mean action: 4.591 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  485/5000: episode: 19, duration: 0.222s, episode steps:  24, steps per second: 108, episode reward: 41.197, mean reward:  1.717 [-2.167, 31.926], mean action: 3.167 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  517/5000: episode: 20, duration: 0.221s, episode steps:  32, steps per second: 145, episode reward: 44.231, mean reward:  1.382 [-2.123, 32.720], mean action: 3.031 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  538/5000: episode: 21, duration: 0.146s, episode steps:  21, steps per second: 144, episode reward: 43.975, mean reward:  2.094 [-2.004, 31.839], mean action: 2.238 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  563/5000: episode: 22, duration: 0.158s, episode steps:  25, steps per second: 158, episode reward: 38.400, mean reward:  1.536 [-3.000, 32.080], mean action: 3.760 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  587/5000: episode: 23, duration: 0.280s, episode steps:  24, steps per second:  86, episode reward: 44.209, mean reward:  1.842 [-2.070, 32.023], mean action: 4.250 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  610/5000: episode: 24, duration: 0.317s, episode steps:  23, steps per second:  73, episode reward: 44.384, mean reward:  1.930 [-2.055, 32.280], mean action: 4.348 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  635/5000: episode: 25, duration: 0.163s, episode steps:  25, steps per second: 153, episode reward: 41.145, mean reward:  1.646 [-2.593, 32.193], mean action: 3.760 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  660/5000: episode: 26, duration: 0.170s, episode steps:  25, steps per second: 147, episode reward: 43.100, mean reward:  1.724 [-2.040, 32.410], mean action: 3.440 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  706/5000: episode: 27, duration: 0.288s, episode steps:  46, steps per second: 160, episode reward: 38.055, mean reward:  0.827 [-2.211, 32.240], mean action: 3.935 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  731/5000: episode: 28, duration: 0.158s, episode steps:  25, steps per second: 158, episode reward: 41.399, mean reward:  1.656 [-2.900, 32.030], mean action: 6.880 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  816/5000: episode: 29, duration: 0.800s, episode steps:  85, steps per second: 106, episode reward: 33.000, mean reward:  0.388 [-2.521, 32.310], mean action: 8.918 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  856/5000: episode: 30, duration: 0.290s, episode steps:  40, steps per second: 138, episode reward: 40.143, mean reward:  1.004 [-2.407, 32.400], mean action: 3.400 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  876/5000: episode: 31, duration: 0.143s, episode steps:  20, steps per second: 140, episode reward: 44.618, mean reward:  2.231 [-2.288, 32.040], mean action: 2.150 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  891/5000: episode: 32, duration: 0.104s, episode steps:  15, steps per second: 144, episode reward: 45.000, mean reward:  3.000 [-2.025, 32.560], mean action: 1.800 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  911/5000: episode: 33, duration: 0.139s, episode steps:  20, steps per second: 144, episode reward: 41.818, mean reward:  2.091 [-3.000, 32.220], mean action: 3.100 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  938/5000: episode: 34, duration: 0.184s, episode steps:  27, steps per second: 147, episode reward: 47.987, mean reward:  1.777 [-0.397, 32.060], mean action: 1.963 [1.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  965/5000: episode: 35, duration: 0.213s, episode steps:  27, steps per second: 127, episode reward: 41.361, mean reward:  1.532 [-2.903, 32.130], mean action: 5.926 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  983/5000: episode: 36, duration: 0.130s, episode steps:  18, steps per second: 138, episode reward: 47.129, mean reward:  2.618 [-0.114, 32.030], mean action: 2.111 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1006/5000: episode: 37, duration: 0.204s, episode steps:  23, steps per second: 113, episode reward: 41.654, mean reward:  1.811 [-2.489, 32.350], mean action: 2.870 [0.000, 19.000],  loss: 0.020399, mae: 0.384520, mean_q: 0.469191, mean_eps: 0.000000
 1031/5000: episode: 38, duration: 0.355s, episode steps:  25, steps per second:  70, episode reward: 36.000, mean reward:  1.440 [-3.000, 32.030], mean action: 4.960 [0.000, 19.000],  loss: 0.020347, mae: 0.383952, mean_q: 0.475538, mean_eps: 0.000000
 1066/5000: episode: 39, duration: 0.505s, episode steps:  35, steps per second:  69, episode reward: 32.432, mean reward:  0.927 [-3.000, 31.955], mean action: 5.971 [0.000, 19.000],  loss: 0.020503, mae: 0.374121, mean_q: 0.494976, mean_eps: 0.000000
 1138/5000: episode: 40, duration: 0.970s, episode steps:  72, steps per second:  74, episode reward: -35.130, mean reward: -0.488 [-32.189,  2.270], mean action: 11.333 [1.000, 20.000],  loss: 0.021961, mae: 0.376228, mean_q: 0.497528, mean_eps: 0.000000
 1164/5000: episode: 41, duration: 0.364s, episode steps:  26, steps per second:  71, episode reward: 41.266, mean reward:  1.587 [-2.702, 32.980], mean action: 3.615 [0.000, 16.000],  loss: 0.020819, mae: 0.372703, mean_q: 0.504232, mean_eps: 0.000000
 1186/5000: episode: 42, duration: 0.312s, episode steps:  22, steps per second:  70, episode reward: 41.903, mean reward:  1.905 [-2.458, 32.023], mean action: 3.227 [0.000, 16.000],  loss: 0.019750, mae: 0.361978, mean_q: 0.583925, mean_eps: 0.000000
 1208/5000: episode: 43, duration: 0.325s, episode steps:  22, steps per second:  68, episode reward: 35.279, mean reward:  1.604 [-3.000, 32.490], mean action: 6.455 [0.000, 19.000],  loss: 0.018799, mae: 0.365714, mean_q: 0.611241, mean_eps: 0.000000
 1232/5000: episode: 44, duration: 0.348s, episode steps:  24, steps per second:  69, episode reward: 42.000, mean reward:  1.750 [-2.715, 32.120], mean action: 2.708 [1.000, 16.000],  loss: 0.018239, mae: 0.360695, mean_q: 0.581774, mean_eps: 0.000000
 1253/5000: episode: 45, duration: 0.310s, episode steps:  21, steps per second:  68, episode reward: 41.485, mean reward:  1.975 [-2.189, 32.077], mean action: 2.905 [0.000, 16.000],  loss: 0.019830, mae: 0.374922, mean_q: 0.534077, mean_eps: 0.000000
 1292/5000: episode: 46, duration: 0.539s, episode steps:  39, steps per second:  72, episode reward: 38.640, mean reward:  0.991 [-2.593, 32.362], mean action: 5.949 [0.000, 20.000],  loss: 0.022704, mae: 0.385071, mean_q: 0.560697, mean_eps: 0.000000
 1314/5000: episode: 47, duration: 0.333s, episode steps:  22, steps per second:  66, episode reward: 38.506, mean reward:  1.750 [-2.383, 31.788], mean action: 3.909 [1.000, 16.000],  loss: 0.019022, mae: 0.367141, mean_q: 0.544425, mean_eps: 0.000000
 1338/5000: episode: 48, duration: 0.333s, episode steps:  24, steps per second:  72, episode reward: 39.000, mean reward:  1.625 [-2.512, 32.310], mean action: 5.500 [0.000, 19.000],  loss: 0.021335, mae: 0.382725, mean_q: 0.558070, mean_eps: 0.000000
 1365/5000: episode: 49, duration: 0.372s, episode steps:  27, steps per second:  73, episode reward: 42.337, mean reward:  1.568 [-2.715, 32.550], mean action: 5.259 [0.000, 16.000],  loss: 0.023123, mae: 0.394555, mean_q: 0.504341, mean_eps: 0.000000
 1392/5000: episode: 50, duration: 0.437s, episode steps:  27, steps per second:  62, episode reward: 41.641, mean reward:  1.542 [-2.651, 32.190], mean action: 2.148 [0.000, 12.000],  loss: 0.019064, mae: 0.377739, mean_q: 0.457457, mean_eps: 0.000000
 1413/5000: episode: 51, duration: 0.305s, episode steps:  21, steps per second:  69, episode reward: 38.472, mean reward:  1.832 [-2.718, 31.642], mean action: 4.952 [0.000, 14.000],  loss: 0.020229, mae: 0.379571, mean_q: 0.440435, mean_eps: 0.000000
 1447/5000: episode: 52, duration: 0.536s, episode steps:  34, steps per second:  63, episode reward: 39.000, mean reward:  1.147 [-2.468, 32.080], mean action: 2.353 [0.000, 15.000],  loss: 0.019600, mae: 0.372472, mean_q: 0.508265, mean_eps: 0.000000
 1468/5000: episode: 53, duration: 0.577s, episode steps:  21, steps per second:  36, episode reward: 38.903, mean reward:  1.853 [-3.000, 32.393], mean action: 3.048 [0.000, 16.000],  loss: 0.024856, mae: 0.392120, mean_q: 0.522994, mean_eps: 0.000000
 1501/5000: episode: 54, duration: 0.638s, episode steps:  33, steps per second:  52, episode reward: 42.000, mean reward:  1.273 [-2.585, 32.180], mean action: 3.606 [0.000, 16.000],  loss: 0.021117, mae: 0.380619, mean_q: 0.580494, mean_eps: 0.000000
 1530/5000: episode: 55, duration: 0.404s, episode steps:  29, steps per second:  72, episode reward: 39.000, mean reward:  1.345 [-2.369, 32.130], mean action: 1.828 [0.000, 16.000],  loss: 0.020339, mae: 0.384735, mean_q: 0.616375, mean_eps: 0.000000
 1545/5000: episode: 56, duration: 0.252s, episode steps:  15, steps per second:  59, episode reward: 42.000, mean reward:  2.800 [-2.607, 32.610], mean action: 4.867 [0.000, 16.000],  loss: 0.017135, mae: 0.374338, mean_q: 0.603632, mean_eps: 0.000000
 1574/5000: episode: 57, duration: 0.455s, episode steps:  29, steps per second:  64, episode reward: 44.101, mean reward:  1.521 [-2.107, 32.510], mean action: 3.828 [0.000, 21.000],  loss: 0.018567, mae: 0.383533, mean_q: 0.556074, mean_eps: 0.000000
 1597/5000: episode: 58, duration: 0.363s, episode steps:  23, steps per second:  63, episode reward: 44.441, mean reward:  1.932 [-2.130, 32.470], mean action: 3.217 [0.000, 16.000],  loss: 0.021084, mae: 0.392563, mean_q: 0.519617, mean_eps: 0.000000
 1616/5000: episode: 59, duration: 0.270s, episode steps:  19, steps per second:  70, episode reward: 44.080, mean reward:  2.320 [-2.633, 32.313], mean action: 3.947 [0.000, 16.000],  loss: 0.021447, mae: 0.398936, mean_q: 0.562715, mean_eps: 0.000000
 1644/5000: episode: 60, duration: 0.512s, episode steps:  28, steps per second:  55, episode reward: 38.417, mean reward:  1.372 [-2.941, 32.026], mean action: 4.429 [1.000, 16.000],  loss: 0.024793, mae: 0.396677, mean_q: 0.535691, mean_eps: 0.000000
 1674/5000: episode: 61, duration: 0.542s, episode steps:  30, steps per second:  55, episode reward: 41.592, mean reward:  1.386 [-2.575, 32.860], mean action: 4.133 [0.000, 16.000],  loss: 0.020589, mae: 0.373826, mean_q: 0.533158, mean_eps: 0.000000
 1709/5000: episode: 62, duration: 0.913s, episode steps:  35, steps per second:  38, episode reward: 35.611, mean reward:  1.017 [-2.282, 32.080], mean action: 4.343 [1.000, 16.000],  loss: 0.019803, mae: 0.375028, mean_q: 0.555326, mean_eps: 0.000000
 1730/5000: episode: 63, duration: 0.556s, episode steps:  21, steps per second:  38, episode reward: 42.773, mean reward:  2.037 [-2.037, 32.017], mean action: 2.857 [0.000, 16.000],  loss: 0.019012, mae: 0.373395, mean_q: 0.569076, mean_eps: 0.000000
 1756/5000: episode: 64, duration: 0.488s, episode steps:  26, steps per second:  53, episode reward: 36.000, mean reward:  1.385 [-2.759, 32.240], mean action: 4.615 [0.000, 16.000],  loss: 0.021506, mae: 0.380848, mean_q: 0.556592, mean_eps: 0.000000
 1779/5000: episode: 65, duration: 0.350s, episode steps:  23, steps per second:  66, episode reward: 39.000, mean reward:  1.696 [-2.474, 32.250], mean action: 6.348 [1.000, 15.000],  loss: 0.022155, mae: 0.377518, mean_q: 0.517008, mean_eps: 0.000000
 1810/5000: episode: 66, duration: 0.675s, episode steps:  31, steps per second:  46, episode reward: 33.000, mean reward:  1.065 [-3.000, 32.690], mean action: 3.194 [0.000, 12.000],  loss: 0.017878, mae: 0.365704, mean_q: 0.486931, mean_eps: 0.000000
 1833/5000: episode: 67, duration: 0.614s, episode steps:  23, steps per second:  37, episode reward: 44.001, mean reward:  1.913 [-2.534, 31.944], mean action: 2.957 [1.000, 12.000],  loss: 0.020875, mae: 0.381878, mean_q: 0.508632, mean_eps: 0.000000
 1858/5000: episode: 68, duration: 0.401s, episode steps:  25, steps per second:  62, episode reward: 38.322, mean reward:  1.533 [-2.701, 32.033], mean action: 3.280 [0.000, 12.000],  loss: 0.020524, mae: 0.380360, mean_q: 0.526098, mean_eps: 0.000000
 1870/5000: episode: 69, duration: 0.193s, episode steps:  12, steps per second:  62, episode reward: 47.414, mean reward:  3.951 [ 0.100, 32.010], mean action: 2.250 [1.000, 3.000],  loss: 0.018883, mae: 0.367560, mean_q: 0.526223, mean_eps: 0.000000
 1902/5000: episode: 70, duration: 0.456s, episode steps:  32, steps per second:  70, episode reward: 41.373, mean reward:  1.293 [-3.000, 32.050], mean action: 2.812 [1.000, 12.000],  loss: 0.019243, mae: 0.368263, mean_q: 0.556293, mean_eps: 0.000000
 1924/5000: episode: 71, duration: 0.305s, episode steps:  22, steps per second:  72, episode reward: 41.401, mean reward:  1.882 [-2.276, 32.340], mean action: 1.955 [0.000, 11.000],  loss: 0.017054, mae: 0.364365, mean_q: 0.529932, mean_eps: 0.000000
 1947/5000: episode: 72, duration: 0.334s, episode steps:  23, steps per second:  69, episode reward: 38.275, mean reward:  1.664 [-2.886, 32.230], mean action: 2.217 [0.000, 11.000],  loss: 0.020477, mae: 0.380247, mean_q: 0.518801, mean_eps: 0.000000
 1998/5000: episode: 73, duration: 0.689s, episode steps:  51, steps per second:  74, episode reward: 32.755, mean reward:  0.642 [-2.436, 32.290], mean action: 7.627 [0.000, 21.000],  loss: 0.023276, mae: 0.391543, mean_q: 0.566660, mean_eps: 0.000000
 2026/5000: episode: 74, duration: 0.408s, episode steps:  28, steps per second:  69, episode reward: 38.313, mean reward:  1.368 [-2.219, 31.985], mean action: 1.786 [0.000, 16.000],  loss: 0.021545, mae: 0.386519, mean_q: 0.569009, mean_eps: 0.000000
 2047/5000: episode: 75, duration: 0.401s, episode steps:  21, steps per second:  52, episode reward: 41.569, mean reward:  1.979 [-2.289, 31.735], mean action: 2.762 [0.000, 15.000],  loss: 0.019010, mae: 0.376380, mean_q: 0.536730, mean_eps: 0.000000
 2071/5000: episode: 76, duration: 0.349s, episode steps:  24, steps per second:  69, episode reward: 36.000, mean reward:  1.500 [-2.329, 32.120], mean action: 4.167 [0.000, 15.000],  loss: 0.022068, mae: 0.388600, mean_q: 0.544728, mean_eps: 0.000000
 2095/5000: episode: 77, duration: 0.342s, episode steps:  24, steps per second:  70, episode reward: 41.568, mean reward:  1.732 [-2.401, 32.053], mean action: 3.375 [0.000, 15.000],  loss: 0.016525, mae: 0.359471, mean_q: 0.492341, mean_eps: 0.000000
 2116/5000: episode: 78, duration: 0.301s, episode steps:  21, steps per second:  70, episode reward: 44.646, mean reward:  2.126 [-2.481, 32.250], mean action: 3.143 [0.000, 15.000],  loss: 0.017900, mae: 0.361440, mean_q: 0.496078, mean_eps: 0.000000
 2146/5000: episode: 79, duration: 0.413s, episode steps:  30, steps per second:  73, episode reward: 38.427, mean reward:  1.281 [-2.278, 32.240], mean action: 4.700 [0.000, 19.000],  loss: 0.019710, mae: 0.368963, mean_q: 0.490584, mean_eps: 0.000000
 2171/5000: episode: 80, duration: 0.347s, episode steps:  25, steps per second:  72, episode reward: 42.000, mean reward:  1.680 [-2.689, 32.720], mean action: 2.960 [0.000, 9.000],  loss: 0.019842, mae: 0.370515, mean_q: 0.548526, mean_eps: 0.000000
 2198/5000: episode: 81, duration: 0.382s, episode steps:  27, steps per second:  71, episode reward: 35.832, mean reward:  1.327 [-3.000, 32.770], mean action: 2.741 [0.000, 11.000],  loss: 0.018502, mae: 0.366758, mean_q: 0.532176, mean_eps: 0.000000
 2212/5000: episode: 82, duration: 0.216s, episode steps:  14, steps per second:  65, episode reward: 47.824, mean reward:  3.416 [ 0.084, 32.113], mean action: 1.500 [1.000, 3.000],  loss: 0.020001, mae: 0.368396, mean_q: 0.511390, mean_eps: 0.000000
 2260/5000: episode: 83, duration: 0.657s, episode steps:  48, steps per second:  73, episode reward: 41.123, mean reward:  0.857 [-2.098, 32.820], mean action: 3.917 [0.000, 13.000],  loss: 0.020234, mae: 0.373302, mean_q: 0.519901, mean_eps: 0.000000
 2297/5000: episode: 84, duration: 0.511s, episode steps:  37, steps per second:  72, episode reward: 36.000, mean reward:  0.973 [-2.417, 32.180], mean action: 3.649 [0.000, 12.000],  loss: 0.022952, mae: 0.387530, mean_q: 0.582198, mean_eps: 0.000000
 2342/5000: episode: 85, duration: 0.604s, episode steps:  45, steps per second:  74, episode reward: -32.600, mean reward: -0.724 [-29.988,  2.425], mean action: 3.222 [0.000, 20.000],  loss: 0.024189, mae: 0.389497, mean_q: 0.578143, mean_eps: 0.000000
 2366/5000: episode: 86, duration: 0.399s, episode steps:  24, steps per second:  60, episode reward: 43.797, mean reward:  1.825 [-2.616, 31.967], mean action: 3.667 [0.000, 12.000],  loss: 0.019977, mae: 0.366819, mean_q: 0.567348, mean_eps: 0.000000
 2411/5000: episode: 87, duration: 0.781s, episode steps:  45, steps per second:  58, episode reward: -34.710, mean reward: -0.771 [-32.052,  2.294], mean action: 3.311 [0.000, 19.000],  loss: 0.018666, mae: 0.363305, mean_q: 0.521451, mean_eps: 0.000000
 2445/5000: episode: 88, duration: 0.470s, episode steps:  34, steps per second:  72, episode reward: 41.673, mean reward:  1.226 [-2.362, 32.094], mean action: 4.941 [0.000, 16.000],  loss: 0.018602, mae: 0.363412, mean_q: 0.514523, mean_eps: 0.000000
 2459/5000: episode: 89, duration: 0.202s, episode steps:  14, steps per second:  69, episode reward: 47.814, mean reward:  3.415 [-0.333, 32.270], mean action: 2.643 [0.000, 16.000],  loss: 0.024047, mae: 0.387939, mean_q: 0.549173, mean_eps: 0.000000
 2496/5000: episode: 90, duration: 0.518s, episode steps:  37, steps per second:  71, episode reward: 45.000, mean reward:  1.216 [-2.660, 32.480], mean action: 1.973 [0.000, 14.000],  loss: 0.021320, mae: 0.371996, mean_q: 0.573577, mean_eps: 0.000000
 2526/5000: episode: 91, duration: 0.416s, episode steps:  30, steps per second:  72, episode reward: 41.130, mean reward:  1.371 [-2.595, 32.881], mean action: 3.800 [0.000, 16.000],  loss: 0.023083, mae: 0.382064, mean_q: 0.631973, mean_eps: 0.000000
 2548/5000: episode: 92, duration: 0.318s, episode steps:  22, steps per second:  69, episode reward: 43.732, mean reward:  1.988 [-2.113, 32.160], mean action: 4.955 [1.000, 15.000],  loss: 0.018871, mae: 0.367497, mean_q: 0.581670, mean_eps: 0.000000
 2576/5000: episode: 93, duration: 0.381s, episode steps:  28, steps per second:  73, episode reward: 32.696, mean reward:  1.168 [-3.000, 31.996], mean action: 3.750 [0.000, 16.000],  loss: 0.016213, mae: 0.348954, mean_q: 0.540636, mean_eps: 0.000000
 2608/5000: episode: 94, duration: 0.505s, episode steps:  32, steps per second:  63, episode reward: 43.882, mean reward:  1.371 [-2.612, 32.403], mean action: 2.812 [0.000, 13.000],  loss: 0.020225, mae: 0.370890, mean_q: 0.531599, mean_eps: 0.000000
 2633/5000: episode: 95, duration: 0.354s, episode steps:  25, steps per second:  71, episode reward: 40.681, mean reward:  1.627 [-2.733, 31.859], mean action: 3.120 [0.000, 9.000],  loss: 0.018727, mae: 0.361669, mean_q: 0.495944, mean_eps: 0.000000
 2656/5000: episode: 96, duration: 0.324s, episode steps:  23, steps per second:  71, episode reward: 47.585, mean reward:  2.069 [-0.510, 32.160], mean action: 3.130 [0.000, 20.000],  loss: 0.020096, mae: 0.366555, mean_q: 0.484588, mean_eps: 0.000000
 2675/5000: episode: 97, duration: 0.264s, episode steps:  19, steps per second:  72, episode reward: 42.000, mean reward:  2.211 [-3.000, 32.610], mean action: 2.684 [0.000, 12.000],  loss: 0.020804, mae: 0.373183, mean_q: 0.536024, mean_eps: 0.000000
 2714/5000: episode: 98, duration: 0.530s, episode steps:  39, steps per second:  74, episode reward: 38.780, mean reward:  0.994 [-2.715, 32.262], mean action: 4.000 [0.000, 16.000],  loss: 0.022314, mae: 0.386124, mean_q: 0.501788, mean_eps: 0.000000
 2741/5000: episode: 99, duration: 0.448s, episode steps:  27, steps per second:  60, episode reward: 37.378, mean reward:  1.384 [-3.000, 32.130], mean action: 4.815 [0.000, 16.000],  loss: 0.020471, mae: 0.374519, mean_q: 0.612676, mean_eps: 0.000000
 2773/5000: episode: 100, duration: 0.455s, episode steps:  32, steps per second:  70, episode reward: 44.320, mean reward:  1.385 [-2.140, 32.154], mean action: 2.281 [0.000, 16.000],  loss: 0.020196, mae: 0.369205, mean_q: 0.585773, mean_eps: 0.000000
 2793/5000: episode: 101, duration: 0.276s, episode steps:  20, steps per second:  72, episode reward: 41.219, mean reward:  2.061 [-2.408, 32.090], mean action: 4.200 [0.000, 16.000],  loss: 0.019003, mae: 0.360345, mean_q: 0.583627, mean_eps: 0.000000
 2814/5000: episode: 102, duration: 0.294s, episode steps:  21, steps per second:  71, episode reward: 38.764, mean reward:  1.846 [-2.322, 32.324], mean action: 4.143 [1.000, 19.000],  loss: 0.016322, mae: 0.347139, mean_q: 0.529703, mean_eps: 0.000000
 2842/5000: episode: 103, duration: 0.395s, episode steps:  28, steps per second:  71, episode reward: 38.019, mean reward:  1.358 [-2.614, 31.730], mean action: 2.964 [0.000, 9.000],  loss: 0.022019, mae: 0.364841, mean_q: 0.537624, mean_eps: 0.000000
 2886/5000: episode: 104, duration: 0.589s, episode steps:  44, steps per second:  75, episode reward: 33.000, mean reward:  0.750 [-2.885, 32.270], mean action: 11.273 [0.000, 20.000],  loss: 0.020894, mae: 0.359428, mean_q: 0.541309, mean_eps: 0.000000
 2927/5000: episode: 105, duration: 0.556s, episode steps:  41, steps per second:  74, episode reward: 35.416, mean reward:  0.864 [-2.641, 32.100], mean action: 7.024 [0.000, 21.000],  loss: 0.022132, mae: 0.369072, mean_q: 0.572820, mean_eps: 0.000000
 2949/5000: episode: 106, duration: 0.310s, episode steps:  22, steps per second:  71, episode reward: 38.584, mean reward:  1.754 [-3.000, 32.680], mean action: 5.455 [0.000, 16.000],  loss: 0.026512, mae: 0.396500, mean_q: 0.553969, mean_eps: 0.000000
 2965/5000: episode: 107, duration: 0.233s, episode steps:  16, steps per second:  69, episode reward: 44.537, mean reward:  2.784 [-3.000, 32.410], mean action: 1.750 [0.000, 12.000],  loss: 0.017288, mae: 0.353575, mean_q: 0.567315, mean_eps: 0.000000
 2989/5000: episode: 108, duration: 0.334s, episode steps:  24, steps per second:  72, episode reward: 38.223, mean reward:  1.593 [-2.102, 32.520], mean action: 4.375 [0.000, 13.000],  loss: 0.019604, mae: 0.359197, mean_q: 0.606004, mean_eps: 0.000000
 3006/5000: episode: 109, duration: 0.247s, episode steps:  17, steps per second:  69, episode reward: 44.231, mean reward:  2.602 [-2.422, 32.430], mean action: 3.529 [1.000, 16.000],  loss: 0.021618, mae: 0.372384, mean_q: 0.576184, mean_eps: 0.000000
 3034/5000: episode: 110, duration: 0.388s, episode steps:  28, steps per second:  72, episode reward: 36.000, mean reward:  1.286 [-2.259, 32.090], mean action: 2.214 [0.000, 15.000],  loss: 0.022909, mae: 0.378966, mean_q: 0.564908, mean_eps: 0.000000
 3052/5000: episode: 111, duration: 0.259s, episode steps:  18, steps per second:  69, episode reward: 42.000, mean reward:  2.333 [-2.053, 32.130], mean action: 3.889 [0.000, 15.000],  loss: 0.017559, mae: 0.351447, mean_q: 0.565568, mean_eps: 0.000000
 3091/5000: episode: 112, duration: 0.546s, episode steps:  39, steps per second:  71, episode reward: 32.105, mean reward:  0.823 [-2.156, 31.315], mean action: 2.718 [0.000, 15.000],  loss: 0.023006, mae: 0.378474, mean_q: 0.520772, mean_eps: 0.000000
 3114/5000: episode: 113, duration: 0.328s, episode steps:  23, steps per second:  70, episode reward: 43.879, mean reward:  1.908 [-2.099, 32.354], mean action: 2.565 [0.000, 19.000],  loss: 0.019745, mae: 0.364637, mean_q: 0.523463, mean_eps: 0.000000
 3146/5000: episode: 114, duration: 0.440s, episode steps:  32, steps per second:  73, episode reward: 35.103, mean reward:  1.097 [-2.416, 31.807], mean action: 4.969 [0.000, 20.000],  loss: 0.022833, mae: 0.380836, mean_q: 0.532857, mean_eps: 0.000000
 3180/5000: episode: 115, duration: 0.467s, episode steps:  34, steps per second:  73, episode reward: -32.020, mean reward: -0.942 [-32.362,  3.000], mean action: 7.500 [0.000, 21.000],  loss: 0.019775, mae: 0.365327, mean_q: 0.527647, mean_eps: 0.000000
 3224/5000: episode: 116, duration: 0.617s, episode steps:  44, steps per second:  71, episode reward: 35.181, mean reward:  0.800 [-2.420, 32.030], mean action: 3.523 [0.000, 19.000],  loss: 0.020422, mae: 0.363718, mean_q: 0.562452, mean_eps: 0.000000
 3245/5000: episode: 117, duration: 0.301s, episode steps:  21, steps per second:  70, episode reward: 41.462, mean reward:  1.974 [-2.531, 31.562], mean action: 4.286 [0.000, 19.000],  loss: 0.022983, mae: 0.372592, mean_q: 0.575776, mean_eps: 0.000000
 3278/5000: episode: 118, duration: 0.441s, episode steps:  33, steps per second:  75, episode reward: 35.259, mean reward:  1.068 [-2.681, 32.431], mean action: 5.909 [0.000, 21.000],  loss: 0.021666, mae: 0.375755, mean_q: 0.631308, mean_eps: 0.000000
 3306/5000: episode: 119, duration: 0.401s, episode steps:  28, steps per second:  70, episode reward: 38.161, mean reward:  1.363 [-2.267, 32.020], mean action: 4.000 [0.000, 16.000],  loss: 0.021371, mae: 0.363643, mean_q: 0.650457, mean_eps: 0.000000
 3333/5000: episode: 120, duration: 0.384s, episode steps:  27, steps per second:  70, episode reward: 44.153, mean reward:  1.635 [-2.311, 32.070], mean action: 2.259 [0.000, 16.000],  loss: 0.021914, mae: 0.377568, mean_q: 0.568637, mean_eps: 0.000000
 3363/5000: episode: 121, duration: 0.413s, episode steps:  30, steps per second:  73, episode reward: 38.089, mean reward:  1.270 [-3.000, 31.439], mean action: 5.900 [0.000, 21.000],  loss: 0.019563, mae: 0.369140, mean_q: 0.467431, mean_eps: 0.000000
 3387/5000: episode: 122, duration: 0.350s, episode steps:  24, steps per second:  69, episode reward: 33.000, mean reward:  1.375 [-3.000, 30.632], mean action: 4.125 [0.000, 19.000],  loss: 0.018351, mae: 0.349655, mean_q: 0.510033, mean_eps: 0.000000
 3424/5000: episode: 123, duration: 0.514s, episode steps:  37, steps per second:  72, episode reward: 39.000, mean reward:  1.054 [-2.378, 29.937], mean action: 2.351 [0.000, 19.000],  loss: 0.020849, mae: 0.363468, mean_q: 0.543903, mean_eps: 0.000000
 3446/5000: episode: 124, duration: 0.319s, episode steps:  22, steps per second:  69, episode reward: 40.827, mean reward:  1.856 [-2.617, 32.240], mean action: 3.455 [0.000, 19.000],  loss: 0.018981, mae: 0.354546, mean_q: 0.557842, mean_eps: 0.000000
 3475/5000: episode: 125, duration: 0.410s, episode steps:  29, steps per second:  71, episode reward: 40.936, mean reward:  1.412 [-2.514, 32.103], mean action: 4.310 [0.000, 20.000],  loss: 0.021983, mae: 0.367433, mean_q: 0.550427, mean_eps: 0.000000
 3506/5000: episode: 126, duration: 0.420s, episode steps:  31, steps per second:  74, episode reward: 38.242, mean reward:  1.234 [-2.096, 32.699], mean action: 3.161 [0.000, 19.000],  loss: 0.022946, mae: 0.369962, mean_q: 0.583419, mean_eps: 0.000000
 3542/5000: episode: 127, duration: 0.785s, episode steps:  36, steps per second:  46, episode reward: 41.088, mean reward:  1.141 [-2.444, 32.134], mean action: 2.778 [0.000, 15.000],  loss: 0.020459, mae: 0.360636, mean_q: 0.579540, mean_eps: 0.000000
 3566/5000: episode: 128, duration: 0.464s, episode steps:  24, steps per second:  52, episode reward: 41.291, mean reward:  1.720 [-2.335, 32.100], mean action: 3.542 [0.000, 20.000],  loss: 0.023422, mae: 0.375571, mean_q: 0.568466, mean_eps: 0.000000
 3599/5000: episode: 129, duration: 0.550s, episode steps:  33, steps per second:  60, episode reward: 38.318, mean reward:  1.161 [-2.179, 32.140], mean action: 2.485 [0.000, 15.000],  loss: 0.022603, mae: 0.370591, mean_q: 0.560310, mean_eps: 0.000000
 3636/5000: episode: 130, duration: 0.708s, episode steps:  37, steps per second:  52, episode reward: 41.171, mean reward:  1.113 [-2.622, 31.760], mean action: 3.892 [0.000, 15.000],  loss: 0.019995, mae: 0.360078, mean_q: 0.513237, mean_eps: 0.000000
 3662/5000: episode: 131, duration: 0.365s, episode steps:  26, steps per second:  71, episode reward: 41.931, mean reward:  1.613 [-2.036, 32.261], mean action: 4.346 [0.000, 16.000],  loss: 0.021755, mae: 0.370137, mean_q: 0.529152, mean_eps: 0.000000
 3692/5000: episode: 132, duration: 0.419s, episode steps:  30, steps per second:  72, episode reward: 37.905, mean reward:  1.264 [-2.471, 32.109], mean action: 3.600 [0.000, 16.000],  loss: 0.021170, mae: 0.362906, mean_q: 0.591585, mean_eps: 0.000000
 3720/5000: episode: 133, duration: 0.390s, episode steps:  28, steps per second:  72, episode reward: 38.478, mean reward:  1.374 [-2.517, 31.991], mean action: 3.357 [0.000, 16.000],  loss: 0.021695, mae: 0.364453, mean_q: 0.572062, mean_eps: 0.000000
 3739/5000: episode: 134, duration: 0.269s, episode steps:  19, steps per second:  71, episode reward: 45.000, mean reward:  2.368 [-2.428, 35.130], mean action: 5.842 [2.000, 15.000],  loss: 0.016250, mae: 0.339852, mean_q: 0.566713, mean_eps: 0.000000
 3763/5000: episode: 135, duration: 0.336s, episode steps:  24, steps per second:  71, episode reward: 42.000, mean reward:  1.750 [-2.785, 32.250], mean action: 2.833 [0.000, 16.000],  loss: 0.019617, mae: 0.354900, mean_q: 0.638919, mean_eps: 0.000000
 3784/5000: episode: 136, duration: 0.296s, episode steps:  21, steps per second:  71, episode reward: 41.849, mean reward:  1.993 [-3.000, 32.200], mean action: 3.381 [0.000, 19.000],  loss: 0.019069, mae: 0.352640, mean_q: 0.645410, mean_eps: 0.000000
 3817/5000: episode: 137, duration: 0.459s, episode steps:  33, steps per second:  72, episode reward: 42.000, mean reward:  1.273 [-2.442, 32.240], mean action: 2.970 [0.000, 16.000],  loss: 0.019413, mae: 0.358506, mean_q: 0.615027, mean_eps: 0.000000
 3844/5000: episode: 138, duration: 0.381s, episode steps:  27, steps per second:  71, episode reward: 43.733, mean reward:  1.620 [-2.337, 32.090], mean action: 3.778 [0.000, 19.000],  loss: 0.022360, mae: 0.376871, mean_q: 0.582294, mean_eps: 0.000000
 3877/5000: episode: 139, duration: 0.467s, episode steps:  33, steps per second:  71, episode reward: 41.875, mean reward:  1.269 [-2.245, 32.530], mean action: 1.667 [0.000, 19.000],  loss: 0.024530, mae: 0.388646, mean_q: 0.595786, mean_eps: 0.000000
 3897/5000: episode: 140, duration: 0.290s, episode steps:  20, steps per second:  69, episode reward: 36.000, mean reward:  1.800 [-3.000, 32.210], mean action: 3.500 [0.000, 12.000],  loss: 0.020471, mae: 0.371732, mean_q: 0.569816, mean_eps: 0.000000
 3913/5000: episode: 141, duration: 0.249s, episode steps:  16, steps per second:  64, episode reward: 41.940, mean reward:  2.621 [-2.582, 32.540], mean action: 3.000 [0.000, 12.000],  loss: 0.021288, mae: 0.376983, mean_q: 0.527908, mean_eps: 0.000000
 3933/5000: episode: 142, duration: 0.421s, episode steps:  20, steps per second:  47, episode reward: 47.347, mean reward:  2.367 [ 0.000, 33.000], mean action: 2.000 [0.000, 8.000],  loss: 0.020731, mae: 0.358670, mean_q: 0.570878, mean_eps: 0.000000
 3956/5000: episode: 143, duration: 0.361s, episode steps:  23, steps per second:  64, episode reward: 38.536, mean reward:  1.675 [-2.479, 32.270], mean action: 2.957 [0.000, 15.000],  loss: 0.022951, mae: 0.367716, mean_q: 0.574975, mean_eps: 0.000000
 3973/5000: episode: 144, duration: 0.255s, episode steps:  17, steps per second:  67, episode reward: 39.000, mean reward:  2.294 [-3.000, 30.315], mean action: 1.765 [0.000, 15.000],  loss: 0.018833, mae: 0.353061, mean_q: 0.546090, mean_eps: 0.000000
 4000/5000: episode: 145, duration: 0.376s, episode steps:  27, steps per second:  72, episode reward: 35.804, mean reward:  1.326 [-3.000, 32.314], mean action: 3.037 [0.000, 15.000],  loss: 0.020538, mae: 0.362038, mean_q: 0.520350, mean_eps: 0.000000
 4019/5000: episode: 146, duration: 0.277s, episode steps:  19, steps per second:  69, episode reward: 41.675, mean reward:  2.193 [-2.153, 32.237], mean action: 2.737 [0.000, 9.000],  loss: 0.017638, mae: 0.348500, mean_q: 0.492453, mean_eps: 0.000000
 4054/5000: episode: 147, duration: 0.491s, episode steps:  35, steps per second:  71, episode reward: 35.434, mean reward:  1.012 [-2.504, 31.739], mean action: 4.257 [0.000, 19.000],  loss: 0.019375, mae: 0.360017, mean_q: 0.506803, mean_eps: 0.000000
 4081/5000: episode: 148, duration: 0.372s, episode steps:  27, steps per second:  73, episode reward: 38.883, mean reward:  1.440 [-2.354, 32.170], mean action: 6.259 [0.000, 20.000],  loss: 0.020908, mae: 0.368041, mean_q: 0.524777, mean_eps: 0.000000
 4112/5000: episode: 149, duration: 0.426s, episode steps:  31, steps per second:  73, episode reward: 35.226, mean reward:  1.136 [-2.875, 32.031], mean action: 3.516 [0.000, 19.000],  loss: 0.021587, mae: 0.370646, mean_q: 0.528150, mean_eps: 0.000000
 4132/5000: episode: 150, duration: 0.292s, episode steps:  20, steps per second:  69, episode reward: 41.453, mean reward:  2.073 [-2.322, 32.373], mean action: 1.750 [0.000, 9.000],  loss: 0.022467, mae: 0.369267, mean_q: 0.566473, mean_eps: 0.000000
 4150/5000: episode: 151, duration: 0.253s, episode steps:  18, steps per second:  71, episode reward: 42.000, mean reward:  2.333 [-3.000, 32.540], mean action: 3.944 [0.000, 14.000],  loss: 0.024412, mae: 0.382315, mean_q: 0.535106, mean_eps: 0.000000
 4170/5000: episode: 152, duration: 0.283s, episode steps:  20, steps per second:  71, episode reward: 41.863, mean reward:  2.093 [-2.067, 32.107], mean action: 2.450 [0.000, 19.000],  loss: 0.022432, mae: 0.374150, mean_q: 0.538659, mean_eps: 0.000000
 4213/5000: episode: 153, duration: 0.599s, episode steps:  43, steps per second:  72, episode reward: 32.878, mean reward:  0.765 [-2.446, 32.340], mean action: 4.349 [0.000, 21.000],  loss: 0.019231, mae: 0.355312, mean_q: 0.523367, mean_eps: 0.000000
 4252/5000: episode: 154, duration: 0.543s, episode steps:  39, steps per second:  72, episode reward: 44.875, mean reward:  1.151 [-2.216, 32.180], mean action: 1.564 [0.000, 20.000],  loss: 0.021619, mae: 0.366835, mean_q: 0.597525, mean_eps: 0.000000
 4273/5000: episode: 155, duration: 0.298s, episode steps:  21, steps per second:  70, episode reward: 43.978, mean reward:  2.094 [-3.000, 32.080], mean action: 3.381 [1.000, 16.000],  loss: 0.016741, mae: 0.336512, mean_q: 0.585492, mean_eps: 0.000000
 4300/5000: episode: 156, duration: 0.375s, episode steps:  27, steps per second:  72, episode reward: 38.543, mean reward:  1.428 [-2.077, 32.220], mean action: 2.593 [0.000, 9.000],  loss: 0.020074, mae: 0.354346, mean_q: 0.583763, mean_eps: 0.000000
 4338/5000: episode: 157, duration: 0.543s, episode steps:  38, steps per second:  70, episode reward: 47.460, mean reward:  1.249 [-0.639, 32.070], mean action: 0.921 [0.000, 2.000],  loss: 0.021059, mae: 0.361531, mean_q: 0.571068, mean_eps: 0.000000
 4366/5000: episode: 158, duration: 0.390s, episode steps:  28, steps per second:  72, episode reward: 43.677, mean reward:  1.560 [-2.397, 32.040], mean action: 2.571 [0.000, 9.000],  loss: 0.023045, mae: 0.371131, mean_q: 0.551421, mean_eps: 0.000000
 4391/5000: episode: 159, duration: 0.452s, episode steps:  25, steps per second:  55, episode reward: 38.197, mean reward:  1.528 [-2.436, 32.080], mean action: 2.520 [0.000, 16.000],  loss: 0.022636, mae: 0.365600, mean_q: 0.566567, mean_eps: 0.000000
 4422/5000: episode: 160, duration: 0.441s, episode steps:  31, steps per second:  70, episode reward: 38.876, mean reward:  1.254 [-2.141, 32.876], mean action: 3.194 [0.000, 15.000],  loss: 0.018402, mae: 0.346508, mean_q: 0.595031, mean_eps: 0.000000
 4436/5000: episode: 161, duration: 0.209s, episode steps:  14, steps per second:  67, episode reward: 41.029, mean reward:  2.931 [-2.078, 32.316], mean action: 3.286 [0.000, 12.000],  loss: 0.019584, mae: 0.349081, mean_q: 0.582850, mean_eps: 0.000000
 4452/5000: episode: 162, duration: 0.231s, episode steps:  16, steps per second:  69, episode reward: 41.957, mean reward:  2.622 [-2.384, 32.270], mean action: 3.000 [0.000, 12.000],  loss: 0.018362, mae: 0.352793, mean_q: 0.550942, mean_eps: 0.000000
 4486/5000: episode: 163, duration: 0.491s, episode steps:  34, steps per second:  69, episode reward: 44.938, mean reward:  1.322 [-2.343, 32.300], mean action: 2.206 [0.000, 12.000],  loss: 0.020509, mae: 0.362249, mean_q: 0.555585, mean_eps: 0.000000
 4531/5000: episode: 164, duration: 0.606s, episode steps:  45, steps per second:  74, episode reward: 32.881, mean reward:  0.731 [-3.000, 32.220], mean action: 2.511 [0.000, 16.000],  loss: 0.018002, mae: 0.344338, mean_q: 0.532458, mean_eps: 0.000000
 4549/5000: episode: 165, duration: 0.330s, episode steps:  18, steps per second:  55, episode reward: 44.040, mean reward:  2.447 [-2.004, 32.160], mean action: 2.333 [0.000, 19.000],  loss: 0.020819, mae: 0.359985, mean_q: 0.525850, mean_eps: 0.000000
 4576/5000: episode: 166, duration: 0.407s, episode steps:  27, steps per second:  66, episode reward: 35.226, mean reward:  1.305 [-2.320, 32.121], mean action: 4.926 [0.000, 19.000],  loss: 0.020010, mae: 0.356068, mean_q: 0.512025, mean_eps: 0.000000
 4588/5000: episode: 167, duration: 0.191s, episode steps:  12, steps per second:  63, episode reward: 44.582, mean reward:  3.715 [-2.187, 33.000], mean action: 4.333 [0.000, 19.000],  loss: 0.016068, mae: 0.332476, mean_q: 0.536101, mean_eps: 0.000000
 4625/5000: episode: 168, duration: 0.514s, episode steps:  37, steps per second:  72, episode reward: 35.904, mean reward:  0.970 [-2.677, 32.004], mean action: 5.459 [0.000, 20.000],  loss: 0.021486, mae: 0.360355, mean_q: 0.507850, mean_eps: 0.000000
 4649/5000: episode: 169, duration: 0.504s, episode steps:  24, steps per second:  48, episode reward: 39.717, mean reward:  1.655 [-2.819, 32.261], mean action: 6.917 [0.000, 21.000],  loss: 0.018626, mae: 0.348173, mean_q: 0.560894, mean_eps: 0.000000
 4678/5000: episode: 170, duration: 0.801s, episode steps:  29, steps per second:  36, episode reward: 41.302, mean reward:  1.424 [-2.267, 31.867], mean action: 2.000 [0.000, 9.000],  loss: 0.019030, mae: 0.338668, mean_q: 0.560780, mean_eps: 0.000000
 4715/5000: episode: 171, duration: 0.593s, episode steps:  37, steps per second:  62, episode reward: 38.746, mean reward:  1.047 [-3.000, 31.946], mean action: 2.838 [0.000, 11.000],  loss: 0.019616, mae: 0.339888, mean_q: 0.542216, mean_eps: 0.000000
 4745/5000: episode: 172, duration: 0.628s, episode steps:  30, steps per second:  48, episode reward: 38.081, mean reward:  1.269 [-3.000, 32.160], mean action: 2.833 [0.000, 15.000],  loss: 0.020154, mae: 0.340162, mean_q: 0.585660, mean_eps: 0.000000
 4771/5000: episode: 173, duration: 0.402s, episode steps:  26, steps per second:  65, episode reward: 38.116, mean reward:  1.466 [-3.000, 32.140], mean action: 5.115 [0.000, 18.000],  loss: 0.018536, mae: 0.338021, mean_q: 0.531982, mean_eps: 0.000000
 4809/5000: episode: 174, duration: 0.527s, episode steps:  38, steps per second:  72, episode reward: 42.000, mean reward:  1.105 [-2.170, 32.480], mean action: 2.553 [1.000, 12.000],  loss: 0.018437, mae: 0.344028, mean_q: 0.519211, mean_eps: 0.000000
 4843/5000: episode: 175, duration: 0.460s, episode steps:  34, steps per second:  74, episode reward: 38.670, mean reward:  1.137 [-2.668, 32.250], mean action: 3.676 [0.000, 21.000],  loss: 0.018997, mae: 0.349567, mean_q: 0.472655, mean_eps: 0.000000
 4878/5000: episode: 176, duration: 0.548s, episode steps:  35, steps per second:  64, episode reward: 38.778, mean reward:  1.108 [-2.155, 32.290], mean action: 2.771 [0.000, 12.000],  loss: 0.020815, mae: 0.358600, mean_q: 0.550639, mean_eps: 0.000000
 4907/5000: episode: 177, duration: 0.400s, episode steps:  29, steps per second:  72, episode reward: 39.000, mean reward:  1.345 [-2.227, 32.520], mean action: 4.069 [0.000, 12.000],  loss: 0.018879, mae: 0.346850, mean_q: 0.550462, mean_eps: 0.000000
 4927/5000: episode: 178, duration: 0.285s, episode steps:  20, steps per second:  70, episode reward: 48.000, mean reward:  2.400 [-0.404, 33.000], mean action: 4.700 [1.000, 13.000],  loss: 0.020956, mae: 0.363697, mean_q: 0.520784, mean_eps: 0.000000
 4959/5000: episode: 179, duration: 0.482s, episode steps:  32, steps per second:  66, episode reward: 41.006, mean reward:  1.281 [-2.242, 32.200], mean action: 5.031 [0.000, 19.000],  loss: 0.019572, mae: 0.355370, mean_q: 0.577348, mean_eps: 0.000000
 4985/5000: episode: 180, duration: 0.400s, episode steps:  26, steps per second:  65, episode reward: 42.000, mean reward:  1.615 [-2.320, 32.120], mean action: 3.077 [0.000, 19.000],  loss: 0.020591, mae: 0.360736, mean_q: 0.536487, mean_eps: 0.000000
done, took 68.806 seconds
DQN Evaluation: 5955 victories out of 7011 episodes
Training for 5000 steps ...
   13/5000: episode: 1, duration: 0.141s, episode steps:  13, steps per second:  92, episode reward: 39.000, mean reward:  3.000 [-2.355, 29.590], mean action: 3.231 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   37/5000: episode: 2, duration: 0.170s, episode steps:  24, steps per second: 141, episode reward: 32.280, mean reward:  1.345 [-3.000, 31.992], mean action: 4.625 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   59/5000: episode: 3, duration: 0.163s, episode steps:  22, steps per second: 135, episode reward: 38.613, mean reward:  1.755 [-3.000, 31.703], mean action: 4.182 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   98/5000: episode: 4, duration: 0.272s, episode steps:  39, steps per second: 143, episode reward: -33.000, mean reward: -0.846 [-32.230,  2.950], mean action: 6.308 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  111/5000: episode: 5, duration: 0.105s, episode steps:  13, steps per second: 124, episode reward: 44.858, mean reward:  3.451 [-2.479, 31.998], mean action: 2.692 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  137/5000: episode: 6, duration: 0.177s, episode steps:  26, steps per second: 147, episode reward: 36.000, mean reward:  1.385 [-2.459, 32.040], mean action: 3.462 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  155/5000: episode: 7, duration: 0.132s, episode steps:  18, steps per second: 137, episode reward: 35.670, mean reward:  1.982 [-3.000, 32.490], mean action: 3.444 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  182/5000: episode: 8, duration: 0.191s, episode steps:  27, steps per second: 141, episode reward: -32.370, mean reward: -1.199 [-32.063,  3.000], mean action: 7.778 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  211/5000: episode: 9, duration: 0.206s, episode steps:  29, steps per second: 141, episode reward: -32.810, mean reward: -1.131 [-32.213,  2.753], mean action: 4.345 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  234/5000: episode: 10, duration: 0.154s, episode steps:  23, steps per second: 149, episode reward: 36.000, mean reward:  1.565 [-3.000, 32.240], mean action: 3.261 [1.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  258/5000: episode: 11, duration: 0.164s, episode steps:  24, steps per second: 146, episode reward: 35.233, mean reward:  1.468 [-3.000, 32.190], mean action: 4.792 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  282/5000: episode: 12, duration: 0.158s, episode steps:  24, steps per second: 152, episode reward: -33.000, mean reward: -1.375 [-32.180,  2.616], mean action: 3.458 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  310/5000: episode: 13, duration: 0.191s, episode steps:  28, steps per second: 147, episode reward: 32.622, mean reward:  1.165 [-2.429, 32.410], mean action: 4.750 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  328/5000: episode: 14, duration: 0.151s, episode steps:  18, steps per second: 119, episode reward: 40.644, mean reward:  2.258 [-2.106, 32.020], mean action: 2.167 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  346/5000: episode: 15, duration: 0.221s, episode steps:  18, steps per second:  82, episode reward: 35.583, mean reward:  1.977 [-3.000, 31.963], mean action: 4.111 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  369/5000: episode: 16, duration: 0.488s, episode steps:  23, steps per second:  47, episode reward: 41.408, mean reward:  1.800 [-2.719, 32.110], mean action: 4.261 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  392/5000: episode: 17, duration: 0.210s, episode steps:  23, steps per second: 109, episode reward: 38.806, mean reward:  1.687 [-2.662, 32.163], mean action: 2.522 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  414/5000: episode: 18, duration: 0.196s, episode steps:  22, steps per second: 112, episode reward: -32.030, mean reward: -1.456 [-31.259,  2.400], mean action: 4.091 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  432/5000: episode: 19, duration: 0.133s, episode steps:  18, steps per second: 136, episode reward: 38.818, mean reward:  2.157 [-3.000, 32.380], mean action: 2.778 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  453/5000: episode: 20, duration: 0.172s, episode steps:  21, steps per second: 122, episode reward: 40.229, mean reward:  1.916 [-2.342, 32.697], mean action: 4.381 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  480/5000: episode: 21, duration: 0.170s, episode steps:  27, steps per second: 159, episode reward: -35.310, mean reward: -1.308 [-32.148,  2.388], mean action: 3.852 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  499/5000: episode: 22, duration: 0.145s, episode steps:  19, steps per second: 131, episode reward: 35.386, mean reward:  1.862 [-3.000, 32.153], mean action: 3.579 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  519/5000: episode: 23, duration: 0.147s, episode steps:  20, steps per second: 136, episode reward: 35.675, mean reward:  1.784 [-3.000, 32.170], mean action: 4.150 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  545/5000: episode: 24, duration: 0.178s, episode steps:  26, steps per second: 146, episode reward: -33.000, mean reward: -1.269 [-30.178,  2.880], mean action: 5.654 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  567/5000: episode: 25, duration: 0.160s, episode steps:  22, steps per second: 138, episode reward: 35.822, mean reward:  1.628 [-2.390, 32.220], mean action: 4.000 [1.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  595/5000: episode: 26, duration: 0.195s, episode steps:  28, steps per second: 143, episode reward: 47.415, mean reward:  1.693 [-0.127, 32.459], mean action: 2.607 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  621/5000: episode: 27, duration: 0.177s, episode steps:  26, steps per second: 147, episode reward: -32.500, mean reward: -1.250 [-31.945,  3.000], mean action: 6.385 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  645/5000: episode: 28, duration: 0.158s, episode steps:  24, steps per second: 152, episode reward: 38.541, mean reward:  1.606 [-2.382, 32.492], mean action: 4.167 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  683/5000: episode: 29, duration: 0.232s, episode steps:  38, steps per second: 164, episode reward: -35.850, mean reward: -0.943 [-32.072,  2.380], mean action: 11.211 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  705/5000: episode: 30, duration: 0.154s, episode steps:  22, steps per second: 143, episode reward: 35.099, mean reward:  1.595 [-2.318, 32.827], mean action: 2.818 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  719/5000: episode: 31, duration: 0.096s, episode steps:  14, steps per second: 146, episode reward: 41.112, mean reward:  2.937 [-2.418, 33.000], mean action: 2.857 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  748/5000: episode: 32, duration: 0.191s, episode steps:  29, steps per second: 152, episode reward: -32.390, mean reward: -1.117 [-32.190,  3.000], mean action: 3.690 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  768/5000: episode: 33, duration: 0.137s, episode steps:  20, steps per second: 146, episode reward: 35.295, mean reward:  1.765 [-3.000, 32.017], mean action: 5.100 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  788/5000: episode: 34, duration: 0.135s, episode steps:  20, steps per second: 149, episode reward: 36.000, mean reward:  1.800 [-3.000, 32.670], mean action: 4.750 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  807/5000: episode: 35, duration: 0.125s, episode steps:  19, steps per second: 152, episode reward: -36.000, mean reward: -1.895 [-29.066,  2.073], mean action: 4.263 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  832/5000: episode: 36, duration: 0.175s, episode steps:  25, steps per second: 143, episode reward: 40.086, mean reward:  1.603 [-2.143, 32.669], mean action: 3.040 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  846/5000: episode: 37, duration: 0.112s, episode steps:  14, steps per second: 125, episode reward: 43.850, mean reward:  3.132 [-3.000, 32.662], mean action: 2.643 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  866/5000: episode: 38, duration: 0.146s, episode steps:  20, steps per second: 137, episode reward: 35.030, mean reward:  1.751 [-2.420, 32.391], mean action: 5.450 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  886/5000: episode: 39, duration: 0.141s, episode steps:  20, steps per second: 142, episode reward: 44.051, mean reward:  2.203 [-2.247, 32.206], mean action: 2.250 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  917/5000: episode: 40, duration: 0.228s, episode steps:  31, steps per second: 136, episode reward: 38.017, mean reward:  1.226 [-2.611, 32.200], mean action: 6.000 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  955/5000: episode: 41, duration: 0.227s, episode steps:  38, steps per second: 167, episode reward: 33.000, mean reward:  0.868 [-2.620, 32.160], mean action: 5.711 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  969/5000: episode: 42, duration: 0.106s, episode steps:  14, steps per second: 132, episode reward: 44.224, mean reward:  3.159 [-2.369, 32.702], mean action: 4.000 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  994/5000: episode: 43, duration: 0.166s, episode steps:  25, steps per second: 150, episode reward: 35.710, mean reward:  1.428 [-2.505, 32.200], mean action: 2.920 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1019/5000: episode: 44, duration: 0.320s, episode steps:  25, steps per second:  78, episode reward: 35.817, mean reward:  1.433 [-3.000, 33.000], mean action: 6.840 [0.000, 20.000],  loss: 0.019728, mae: 0.360046, mean_q: 0.527762, mean_eps: 0.000000
 1034/5000: episode: 45, duration: 0.321s, episode steps:  15, steps per second:  47, episode reward: 41.115, mean reward:  2.741 [-2.303, 32.061], mean action: 4.067 [0.000, 15.000],  loss: 0.020467, mae: 0.367678, mean_q: 0.568698, mean_eps: 0.000000
 1058/5000: episode: 46, duration: 0.352s, episode steps:  24, steps per second:  68, episode reward: -32.340, mean reward: -1.347 [-32.126,  2.685], mean action: 4.208 [0.000, 19.000],  loss: 0.022280, mae: 0.375357, mean_q: 0.568149, mean_eps: 0.000000
 1085/5000: episode: 47, duration: 0.549s, episode steps:  27, steps per second:  49, episode reward: 33.000, mean reward:  1.222 [-2.328, 29.990], mean action: 4.556 [0.000, 14.000],  loss: 0.022558, mae: 0.381108, mean_q: 0.539895, mean_eps: 0.000000
 1101/5000: episode: 48, duration: 0.276s, episode steps:  16, steps per second:  58, episode reward: 36.000, mean reward:  2.250 [-2.455, 30.970], mean action: 2.938 [0.000, 12.000],  loss: 0.018026, mae: 0.367858, mean_q: 0.560514, mean_eps: 0.000000
 1125/5000: episode: 49, duration: 0.565s, episode steps:  24, steps per second:  42, episode reward: 35.529, mean reward:  1.480 [-3.000, 32.078], mean action: 3.708 [0.000, 12.000],  loss: 0.016949, mae: 0.356379, mean_q: 0.534371, mean_eps: 0.000000
 1143/5000: episode: 50, duration: 0.459s, episode steps:  18, steps per second:  39, episode reward: 41.561, mean reward:  2.309 [-3.000, 32.330], mean action: 2.333 [0.000, 12.000],  loss: 0.025230, mae: 0.386029, mean_q: 0.494529, mean_eps: 0.000000
 1175/5000: episode: 51, duration: 0.472s, episode steps:  32, steps per second:  68, episode reward: 32.278, mean reward:  1.009 [-2.444, 31.859], mean action: 9.500 [0.000, 21.000],  loss: 0.019107, mae: 0.369975, mean_q: 0.467553, mean_eps: 0.000000
 1191/5000: episode: 52, duration: 0.237s, episode steps:  16, steps per second:  67, episode reward: 41.621, mean reward:  2.601 [-2.129, 32.450], mean action: 2.875 [0.000, 12.000],  loss: 0.016973, mae: 0.364696, mean_q: 0.421338, mean_eps: 0.000000
 1224/5000: episode: 53, duration: 0.455s, episode steps:  33, steps per second:  73, episode reward: 33.000, mean reward:  1.000 [-2.547, 32.050], mean action: 5.061 [0.000, 16.000],  loss: 0.020854, mae: 0.366971, mean_q: 0.498652, mean_eps: 0.000000
 1250/5000: episode: 54, duration: 0.364s, episode steps:  26, steps per second:  71, episode reward: 35.490, mean reward:  1.365 [-2.679, 29.639], mean action: 7.308 [0.000, 16.000],  loss: 0.020047, mae: 0.362252, mean_q: 0.508627, mean_eps: 0.000000
 1273/5000: episode: 55, duration: 0.320s, episode steps:  23, steps per second:  72, episode reward: 32.467, mean reward:  1.412 [-2.584, 32.190], mean action: 5.391 [0.000, 19.000],  loss: 0.020955, mae: 0.369844, mean_q: 0.543411, mean_eps: 0.000000
 1299/5000: episode: 56, duration: 0.369s, episode steps:  26, steps per second:  70, episode reward: 35.306, mean reward:  1.358 [-2.244, 32.130], mean action: 3.808 [0.000, 15.000],  loss: 0.017894, mae: 0.353768, mean_q: 0.497304, mean_eps: 0.000000
 1319/5000: episode: 57, duration: 0.283s, episode steps:  20, steps per second:  71, episode reward: -32.690, mean reward: -1.634 [-32.750,  3.000], mean action: 5.350 [0.000, 21.000],  loss: 0.020421, mae: 0.361479, mean_q: 0.503491, mean_eps: 0.000000
 1332/5000: episode: 58, duration: 0.194s, episode steps:  13, steps per second:  67, episode reward: 41.516, mean reward:  3.194 [-2.500, 31.980], mean action: 2.846 [0.000, 11.000],  loss: 0.018443, mae: 0.355632, mean_q: 0.489588, mean_eps: 0.000000
 1352/5000: episode: 59, duration: 0.414s, episode steps:  20, steps per second:  48, episode reward: 40.749, mean reward:  2.037 [-2.079, 32.446], mean action: 4.450 [0.000, 14.000],  loss: 0.024007, mae: 0.379386, mean_q: 0.521560, mean_eps: 0.000000
 1374/5000: episode: 60, duration: 0.426s, episode steps:  22, steps per second:  52, episode reward: -35.580, mean reward: -1.617 [-32.027,  2.273], mean action: 6.091 [0.000, 20.000],  loss: 0.020019, mae: 0.358048, mean_q: 0.542113, mean_eps: 0.000000
 1405/5000: episode: 61, duration: 0.435s, episode steps:  31, steps per second:  71, episode reward: 38.458, mean reward:  1.241 [-2.550, 32.080], mean action: 4.097 [0.000, 14.000],  loss: 0.015560, mae: 0.333099, mean_q: 0.542833, mean_eps: 0.000000
 1426/5000: episode: 62, duration: 0.305s, episode steps:  21, steps per second:  69, episode reward: 38.523, mean reward:  1.834 [-3.000, 32.014], mean action: 2.143 [0.000, 16.000],  loss: 0.023843, mae: 0.375025, mean_q: 0.590349, mean_eps: 0.000000
 1448/5000: episode: 63, duration: 0.307s, episode steps:  22, steps per second:  72, episode reward: 38.036, mean reward:  1.729 [-3.000, 31.934], mean action: 3.273 [0.000, 11.000],  loss: 0.019096, mae: 0.353442, mean_q: 0.574844, mean_eps: 0.000000
 1461/5000: episode: 64, duration: 0.194s, episode steps:  13, steps per second:  67, episode reward: 41.242, mean reward:  3.172 [-2.813, 32.380], mean action: 5.000 [0.000, 20.000],  loss: 0.021429, mae: 0.364314, mean_q: 0.614665, mean_eps: 0.000000
 1485/5000: episode: 65, duration: 0.335s, episode steps:  24, steps per second:  72, episode reward: 32.903, mean reward:  1.371 [-3.000, 31.963], mean action: 4.208 [0.000, 16.000],  loss: 0.021938, mae: 0.365753, mean_q: 0.619260, mean_eps: 0.000000
 1505/5000: episode: 66, duration: 0.291s, episode steps:  20, steps per second:  69, episode reward: 44.395, mean reward:  2.220 [-2.110, 32.420], mean action: 2.450 [0.000, 9.000],  loss: 0.018751, mae: 0.350360, mean_q: 0.510945, mean_eps: 0.000000
 1524/5000: episode: 67, duration: 0.264s, episode steps:  19, steps per second:  72, episode reward: -32.290, mean reward: -1.699 [-32.290,  3.000], mean action: 4.105 [0.000, 12.000],  loss: 0.020663, mae: 0.362030, mean_q: 0.481104, mean_eps: 0.000000
 1545/5000: episode: 68, duration: 0.295s, episode steps:  21, steps per second:  71, episode reward: 35.824, mean reward:  1.706 [-2.807, 32.090], mean action: 4.810 [1.000, 14.000],  loss: 0.017985, mae: 0.340480, mean_q: 0.506096, mean_eps: 0.000000
 1565/5000: episode: 69, duration: 0.284s, episode steps:  20, steps per second:  71, episode reward: 38.903, mean reward:  1.945 [-2.582, 32.063], mean action: 3.550 [0.000, 9.000],  loss: 0.022108, mae: 0.363156, mean_q: 0.532756, mean_eps: 0.000000
 1585/5000: episode: 70, duration: 0.282s, episode steps:  20, steps per second:  71, episode reward: 38.350, mean reward:  1.918 [-3.000, 32.333], mean action: 5.150 [0.000, 20.000],  loss: 0.017812, mae: 0.340610, mean_q: 0.556242, mean_eps: 0.000000
 1611/5000: episode: 71, duration: 0.362s, episode steps:  26, steps per second:  72, episode reward: 34.344, mean reward:  1.321 [-3.000, 31.755], mean action: 3.923 [0.000, 13.000],  loss: 0.016031, mae: 0.340028, mean_q: 0.520396, mean_eps: 0.000000
 1633/5000: episode: 72, duration: 0.306s, episode steps:  22, steps per second:  72, episode reward: 35.590, mean reward:  1.618 [-3.000, 32.480], mean action: 3.955 [0.000, 16.000],  loss: 0.021099, mae: 0.363368, mean_q: 0.500827, mean_eps: 0.000000
 1662/5000: episode: 73, duration: 0.403s, episode steps:  29, steps per second:  72, episode reward: -33.000, mean reward: -1.138 [-32.105,  2.281], mean action: 3.828 [0.000, 14.000],  loss: 0.020240, mae: 0.361356, mean_q: 0.433141, mean_eps: 0.000000
 1685/5000: episode: 74, duration: 0.323s, episode steps:  23, steps per second:  71, episode reward: -39.000, mean reward: -1.696 [-32.607,  2.061], mean action: 5.261 [0.000, 16.000],  loss: 0.019876, mae: 0.357252, mean_q: 0.472431, mean_eps: 0.000000
 1714/5000: episode: 75, duration: 0.415s, episode steps:  29, steps per second:  70, episode reward: 34.780, mean reward:  1.199 [-2.360, 32.151], mean action: 4.310 [0.000, 15.000],  loss: 0.021372, mae: 0.363615, mean_q: 0.479820, mean_eps: 0.000000
 1732/5000: episode: 76, duration: 0.257s, episode steps:  18, steps per second:  70, episode reward: 38.605, mean reward:  2.145 [-2.557, 32.392], mean action: 4.778 [0.000, 14.000],  loss: 0.023493, mae: 0.380455, mean_q: 0.546164, mean_eps: 0.000000
 1764/5000: episode: 77, duration: 0.454s, episode steps:  32, steps per second:  71, episode reward: 36.000, mean reward:  1.125 [-2.701, 32.170], mean action: 3.906 [1.000, 16.000],  loss: 0.016303, mae: 0.334336, mean_q: 0.503239, mean_eps: 0.000000
 1788/5000: episode: 78, duration: 0.352s, episode steps:  24, steps per second:  68, episode reward: 36.000, mean reward:  1.500 [-2.807, 30.050], mean action: 4.417 [0.000, 13.000],  loss: 0.018201, mae: 0.348254, mean_q: 0.501321, mean_eps: 0.000000
 1845/5000: episode: 79, duration: 0.755s, episode steps:  57, steps per second:  76, episode reward: 32.587, mean reward:  0.572 [-2.753, 32.837], mean action: 3.526 [0.000, 16.000],  loss: 0.018439, mae: 0.344670, mean_q: 0.508709, mean_eps: 0.000000
 1881/5000: episode: 80, duration: 0.659s, episode steps:  36, steps per second:  55, episode reward: 35.669, mean reward:  0.991 [-3.000, 32.250], mean action: 7.944 [0.000, 20.000],  loss: 0.020064, mae: 0.357874, mean_q: 0.495708, mean_eps: 0.000000
 1904/5000: episode: 81, duration: 0.318s, episode steps:  23, steps per second:  72, episode reward: 35.705, mean reward:  1.552 [-2.563, 32.217], mean action: 4.609 [1.000, 16.000],  loss: 0.017904, mae: 0.353900, mean_q: 0.474578, mean_eps: 0.000000
 1922/5000: episode: 82, duration: 0.260s, episode steps:  18, steps per second:  69, episode reward: 38.295, mean reward:  2.127 [-2.886, 32.240], mean action: 3.611 [0.000, 16.000],  loss: 0.018540, mae: 0.348568, mean_q: 0.490375, mean_eps: 0.000000
 1936/5000: episode: 83, duration: 0.202s, episode steps:  14, steps per second:  69, episode reward: 41.686, mean reward:  2.978 [-2.482, 33.000], mean action: 4.000 [0.000, 16.000],  loss: 0.015328, mae: 0.326315, mean_q: 0.478458, mean_eps: 0.000000
 1955/5000: episode: 84, duration: 0.269s, episode steps:  19, steps per second:  71, episode reward: 38.505, mean reward:  2.027 [-3.000, 32.320], mean action: 7.632 [0.000, 20.000],  loss: 0.019734, mae: 0.352100, mean_q: 0.506864, mean_eps: 0.000000
 1979/5000: episode: 85, duration: 0.332s, episode steps:  24, steps per second:  72, episode reward: 35.173, mean reward:  1.466 [-2.273, 32.240], mean action: 3.292 [0.000, 19.000],  loss: 0.015749, mae: 0.333475, mean_q: 0.485716, mean_eps: 0.000000
 2007/5000: episode: 86, duration: 0.389s, episode steps:  28, steps per second:  72, episode reward: -36.000, mean reward: -1.286 [-33.000,  2.770], mean action: 2.429 [0.000, 15.000],  loss: 0.019242, mae: 0.354950, mean_q: 0.554519, mean_eps: 0.000000
 2026/5000: episode: 87, duration: 0.272s, episode steps:  19, steps per second:  70, episode reward: 35.669, mean reward:  1.877 [-3.000, 32.669], mean action: 5.000 [0.000, 19.000],  loss: 0.019692, mae: 0.357561, mean_q: 0.529009, mean_eps: 0.000000
 2054/5000: episode: 88, duration: 0.385s, episode steps:  28, steps per second:  73, episode reward: 32.322, mean reward:  1.154 [-3.000, 32.080], mean action: 4.393 [0.000, 15.000],  loss: 0.020360, mae: 0.352627, mean_q: 0.536227, mean_eps: 0.000000
 2076/5000: episode: 89, duration: 0.343s, episode steps:  22, steps per second:  64, episode reward: 40.588, mean reward:  1.845 [-3.000, 32.240], mean action: 6.591 [1.000, 15.000],  loss: 0.023310, mae: 0.368061, mean_q: 0.574989, mean_eps: 0.000000
 2100/5000: episode: 90, duration: 0.348s, episode steps:  24, steps per second:  69, episode reward: 35.922, mean reward:  1.497 [-2.393, 32.310], mean action: 4.375 [0.000, 15.000],  loss: 0.016632, mae: 0.339733, mean_q: 0.546186, mean_eps: 0.000000
 2124/5000: episode: 91, duration: 0.336s, episode steps:  24, steps per second:  71, episode reward: 32.585, mean reward:  1.358 [-3.000, 32.320], mean action: 5.583 [0.000, 17.000],  loss: 0.019639, mae: 0.363867, mean_q: 0.530365, mean_eps: 0.000000
 2148/5000: episode: 92, duration: 0.346s, episode steps:  24, steps per second:  69, episode reward: 38.095, mean reward:  1.587 [-3.000, 32.330], mean action: 5.583 [1.000, 16.000],  loss: 0.019089, mae: 0.350309, mean_q: 0.525421, mean_eps: 0.000000
 2173/5000: episode: 93, duration: 0.471s, episode steps:  25, steps per second:  53, episode reward: 35.116, mean reward:  1.405 [-2.673, 32.286], mean action: 6.120 [0.000, 19.000],  loss: 0.018992, mae: 0.352596, mean_q: 0.535814, mean_eps: 0.000000
 2193/5000: episode: 94, duration: 0.473s, episode steps:  20, steps per second:  42, episode reward: 38.009, mean reward:  1.900 [-2.540, 32.137], mean action: 3.600 [0.000, 19.000],  loss: 0.017865, mae: 0.350460, mean_q: 0.511798, mean_eps: 0.000000
 2218/5000: episode: 95, duration: 0.486s, episode steps:  25, steps per second:  51, episode reward: 38.928, mean reward:  1.557 [-2.300, 32.270], mean action: 5.760 [0.000, 16.000],  loss: 0.021971, mae: 0.367978, mean_q: 0.627155, mean_eps: 0.000000
 2255/5000: episode: 96, duration: 0.638s, episode steps:  37, steps per second:  58, episode reward: 33.000, mean reward:  0.892 [-2.254, 32.310], mean action: 9.622 [0.000, 21.000],  loss: 0.018019, mae: 0.346572, mean_q: 0.565127, mean_eps: 0.000000
 2279/5000: episode: 97, duration: 0.506s, episode steps:  24, steps per second:  47, episode reward: -38.510, mean reward: -1.605 [-31.945,  2.430], mean action: 4.458 [0.000, 16.000],  loss: 0.021487, mae: 0.357261, mean_q: 0.511618, mean_eps: 0.000000
 2313/5000: episode: 98, duration: 0.532s, episode steps:  34, steps per second:  64, episode reward: -34.680, mean reward: -1.020 [-32.037,  2.621], mean action: 3.618 [1.000, 16.000],  loss: 0.021076, mae: 0.356607, mean_q: 0.546841, mean_eps: 0.000000
 2331/5000: episode: 99, duration: 0.259s, episode steps:  18, steps per second:  69, episode reward: 38.437, mean reward:  2.135 [-2.307, 32.520], mean action: 2.333 [0.000, 12.000],  loss: 0.018717, mae: 0.342967, mean_q: 0.541688, mean_eps: 0.000000
 2356/5000: episode: 100, duration: 0.699s, episode steps:  25, steps per second:  36, episode reward: 38.508, mean reward:  1.540 [-3.000, 32.904], mean action: 4.320 [0.000, 20.000],  loss: 0.018519, mae: 0.352627, mean_q: 0.476583, mean_eps: 0.000000
 2371/5000: episode: 101, duration: 0.220s, episode steps:  15, steps per second:  68, episode reward: 42.000, mean reward:  2.800 [-2.296, 32.650], mean action: 4.133 [0.000, 16.000],  loss: 0.016519, mae: 0.334782, mean_q: 0.469063, mean_eps: 0.000000
 2395/5000: episode: 102, duration: 0.334s, episode steps:  24, steps per second:  72, episode reward: 35.388, mean reward:  1.474 [-2.963, 32.570], mean action: 3.917 [0.000, 14.000],  loss: 0.021610, mae: 0.356968, mean_q: 0.509129, mean_eps: 0.000000
 2413/5000: episode: 103, duration: 0.265s, episode steps:  18, steps per second:  68, episode reward: 39.000, mean reward:  2.167 [-2.472, 32.460], mean action: 3.500 [0.000, 16.000],  loss: 0.015458, mae: 0.329810, mean_q: 0.553773, mean_eps: 0.000000
 2427/5000: episode: 104, duration: 0.244s, episode steps:  14, steps per second:  57, episode reward: 41.119, mean reward:  2.937 [-2.275, 32.900], mean action: 3.857 [0.000, 11.000],  loss: 0.018960, mae: 0.357752, mean_q: 0.513842, mean_eps: 0.000000
 2441/5000: episode: 105, duration: 0.209s, episode steps:  14, steps per second:  67, episode reward: 44.064, mean reward:  3.147 [-2.183, 32.240], mean action: 2.714 [1.000, 11.000],  loss: 0.021117, mae: 0.366270, mean_q: 0.528197, mean_eps: 0.000000
 2467/5000: episode: 106, duration: 0.398s, episode steps:  26, steps per second:  65, episode reward: 38.725, mean reward:  1.489 [-2.397, 32.070], mean action: 3.654 [0.000, 20.000],  loss: 0.020136, mae: 0.357857, mean_q: 0.557912, mean_eps: 0.000000
 2480/5000: episode: 107, duration: 0.195s, episode steps:  13, steps per second:  67, episode reward: 47.578, mean reward:  3.660 [ 0.177, 32.160], mean action: 1.000 [1.000, 1.000],  loss: 0.019218, mae: 0.349087, mean_q: 0.547849, mean_eps: 0.000000
 2501/5000: episode: 108, duration: 0.304s, episode steps:  21, steps per second:  69, episode reward: 33.000, mean reward:  1.571 [-2.459, 30.171], mean action: 5.619 [0.000, 21.000],  loss: 0.022270, mae: 0.368840, mean_q: 0.480945, mean_eps: 0.000000
 2532/5000: episode: 109, duration: 0.421s, episode steps:  31, steps per second:  74, episode reward: -38.810, mean reward: -1.252 [-32.497,  2.310], mean action: 7.452 [0.000, 20.000],  loss: 0.018658, mae: 0.350360, mean_q: 0.491915, mean_eps: 0.000000
 2566/5000: episode: 110, duration: 0.524s, episode steps:  34, steps per second:  65, episode reward: -32.660, mean reward: -0.961 [-32.096,  2.500], mean action: 7.265 [0.000, 19.000],  loss: 0.020240, mae: 0.350325, mean_q: 0.565414, mean_eps: 0.000000
 2596/5000: episode: 111, duration: 0.444s, episode steps:  30, steps per second:  68, episode reward: 32.126, mean reward:  1.071 [-2.388, 32.415], mean action: 5.133 [0.000, 16.000],  loss: 0.021881, mae: 0.354474, mean_q: 0.570730, mean_eps: 0.000000
 2617/5000: episode: 112, duration: 0.301s, episode steps:  21, steps per second:  70, episode reward: 35.124, mean reward:  1.673 [-2.536, 31.776], mean action: 4.524 [1.000, 16.000],  loss: 0.019600, mae: 0.343844, mean_q: 0.561998, mean_eps: 0.000000
 2639/5000: episode: 113, duration: 0.359s, episode steps:  22, steps per second:  61, episode reward: 38.074, mean reward:  1.731 [-2.295, 32.030], mean action: 7.136 [1.000, 20.000],  loss: 0.015783, mae: 0.323300, mean_q: 0.562492, mean_eps: 0.000000
 2676/5000: episode: 114, duration: 0.887s, episode steps:  37, steps per second:  42, episode reward: 32.600, mean reward:  0.881 [-2.331, 32.420], mean action: 3.135 [0.000, 19.000],  loss: 0.020546, mae: 0.351209, mean_q: 0.579036, mean_eps: 0.000000
 2696/5000: episode: 115, duration: 0.389s, episode steps:  20, steps per second:  51, episode reward: 41.063, mean reward:  2.053 [-3.000, 31.925], mean action: 2.900 [0.000, 19.000],  loss: 0.019715, mae: 0.355630, mean_q: 0.516653, mean_eps: 0.000000
 2726/5000: episode: 116, duration: 0.499s, episode steps:  30, steps per second:  60, episode reward: -32.160, mean reward: -1.072 [-31.904,  3.000], mean action: 6.833 [0.000, 19.000],  loss: 0.016692, mae: 0.344807, mean_q: 0.453171, mean_eps: 0.000000
 2750/5000: episode: 117, duration: 0.343s, episode steps:  24, steps per second:  70, episode reward: 34.508, mean reward:  1.438 [-3.000, 31.832], mean action: 5.958 [0.000, 19.000],  loss: 0.019412, mae: 0.349259, mean_q: 0.539674, mean_eps: 0.000000
 2785/5000: episode: 118, duration: 0.582s, episode steps:  35, steps per second:  60, episode reward: 32.538, mean reward:  0.930 [-2.451, 31.958], mean action: 5.943 [0.000, 16.000],  loss: 0.022948, mae: 0.357558, mean_q: 0.553683, mean_eps: 0.000000
 2821/5000: episode: 119, duration: 0.574s, episode steps:  36, steps per second:  63, episode reward: 34.745, mean reward:  0.965 [-2.894, 31.813], mean action: 8.194 [0.000, 20.000],  loss: 0.018366, mae: 0.337425, mean_q: 0.533695, mean_eps: 0.000000
 2849/5000: episode: 120, duration: 0.459s, episode steps:  28, steps per second:  61, episode reward: -32.240, mean reward: -1.151 [-33.000,  2.210], mean action: 4.357 [0.000, 19.000],  loss: 0.019145, mae: 0.341933, mean_q: 0.548125, mean_eps: 0.000000
 2885/5000: episode: 121, duration: 0.492s, episode steps:  36, steps per second:  73, episode reward: 32.133, mean reward:  0.893 [-2.801, 31.768], mean action: 4.389 [0.000, 16.000],  loss: 0.020012, mae: 0.343916, mean_q: 0.534370, mean_eps: 0.000000
 2907/5000: episode: 122, duration: 0.308s, episode steps:  22, steps per second:  71, episode reward: 35.519, mean reward:  1.614 [-2.396, 31.799], mean action: 4.318 [0.000, 19.000],  loss: 0.019397, mae: 0.337251, mean_q: 0.540350, mean_eps: 0.000000
 2934/5000: episode: 123, duration: 0.728s, episode steps:  27, steps per second:  37, episode reward: -32.380, mean reward: -1.199 [-31.916,  2.581], mean action: 5.259 [0.000, 19.000],  loss: 0.021446, mae: 0.339018, mean_q: 0.511488, mean_eps: 0.000000
 2959/5000: episode: 124, duration: 0.397s, episode steps:  25, steps per second:  63, episode reward: 32.313, mean reward:  1.293 [-2.561, 32.540], mean action: 5.280 [0.000, 16.000],  loss: 0.018823, mae: 0.336583, mean_q: 0.443680, mean_eps: 0.000000
 2975/5000: episode: 125, duration: 0.240s, episode steps:  16, steps per second:  67, episode reward: 36.000, mean reward:  2.250 [-2.460, 32.940], mean action: 4.375 [0.000, 16.000],  loss: 0.019347, mae: 0.345253, mean_q: 0.523549, mean_eps: 0.000000
 2995/5000: episode: 126, duration: 0.281s, episode steps:  20, steps per second:  71, episode reward: 38.545, mean reward:  1.927 [-2.711, 31.959], mean action: 4.550 [0.000, 19.000],  loss: 0.020221, mae: 0.340911, mean_q: 0.483960, mean_eps: 0.000000
 3019/5000: episode: 127, duration: 0.333s, episode steps:  24, steps per second:  72, episode reward: 35.411, mean reward:  1.475 [-2.910, 33.000], mean action: 4.542 [0.000, 19.000],  loss: 0.021490, mae: 0.357715, mean_q: 0.469936, mean_eps: 0.000000
 3051/5000: episode: 128, duration: 0.563s, episode steps:  32, steps per second:  57, episode reward: -41.490, mean reward: -1.297 [-31.751,  2.300], mean action: 4.531 [0.000, 20.000],  loss: 0.020713, mae: 0.348919, mean_q: 0.493496, mean_eps: 0.000000
 3079/5000: episode: 129, duration: 0.524s, episode steps:  28, steps per second:  53, episode reward: -32.580, mean reward: -1.164 [-32.479,  2.480], mean action: 3.714 [0.000, 16.000],  loss: 0.021378, mae: 0.347937, mean_q: 0.473474, mean_eps: 0.000000
 3104/5000: episode: 130, duration: 0.432s, episode steps:  25, steps per second:  58, episode reward: 32.902, mean reward:  1.316 [-3.000, 32.292], mean action: 4.320 [0.000, 16.000],  loss: 0.018354, mae: 0.333334, mean_q: 0.549730, mean_eps: 0.000000
 3133/5000: episode: 131, duration: 0.412s, episode steps:  29, steps per second:  70, episode reward: 36.000, mean reward:  1.241 [-2.514, 32.440], mean action: 3.379 [0.000, 15.000],  loss: 0.019515, mae: 0.351713, mean_q: 0.562405, mean_eps: 0.000000
 3147/5000: episode: 132, duration: 0.211s, episode steps:  14, steps per second:  66, episode reward: 44.401, mean reward:  3.172 [-2.326, 32.910], mean action: 3.929 [0.000, 15.000],  loss: 0.018551, mae: 0.346801, mean_q: 0.543268, mean_eps: 0.000000
 3169/5000: episode: 133, duration: 0.300s, episode steps:  22, steps per second:  73, episode reward: 32.345, mean reward:  1.470 [-3.000, 31.913], mean action: 6.955 [0.000, 20.000],  loss: 0.018176, mae: 0.340028, mean_q: 0.548869, mean_eps: 0.000000
 3193/5000: episode: 134, duration: 0.351s, episode steps:  24, steps per second:  68, episode reward: 36.000, mean reward:  1.500 [-2.903, 30.527], mean action: 3.917 [0.000, 15.000],  loss: 0.021284, mae: 0.359370, mean_q: 0.553642, mean_eps: 0.000000
 3211/5000: episode: 135, duration: 0.265s, episode steps:  18, steps per second:  68, episode reward: 44.063, mean reward:  2.448 [-2.474, 32.080], mean action: 3.778 [0.000, 15.000],  loss: 0.021346, mae: 0.362263, mean_q: 0.500641, mean_eps: 0.000000
 3246/5000: episode: 136, duration: 0.496s, episode steps:  35, steps per second:  71, episode reward: -38.760, mean reward: -1.107 [-32.532,  2.182], mean action: 9.457 [0.000, 20.000],  loss: 0.018468, mae: 0.340688, mean_q: 0.475447, mean_eps: 0.000000
 3267/5000: episode: 137, duration: 0.302s, episode steps:  21, steps per second:  70, episode reward: 38.868, mean reward:  1.851 [-2.660, 32.860], mean action: 5.333 [0.000, 19.000],  loss: 0.020896, mae: 0.354552, mean_q: 0.522658, mean_eps: 0.000000
 3289/5000: episode: 138, duration: 0.324s, episode steps:  22, steps per second:  68, episode reward: -39.000, mean reward: -1.773 [-32.370,  3.000], mean action: 7.864 [0.000, 15.000],  loss: 0.023378, mae: 0.372476, mean_q: 0.516204, mean_eps: 0.000000
 3308/5000: episode: 139, duration: 0.277s, episode steps:  19, steps per second:  69, episode reward: 38.680, mean reward:  2.036 [-3.000, 32.341], mean action: 3.579 [0.000, 12.000],  loss: 0.020136, mae: 0.349612, mean_q: 0.551497, mean_eps: 0.000000
 3331/5000: episode: 140, duration: 0.617s, episode steps:  23, steps per second:  37, episode reward: 44.034, mean reward:  1.915 [-2.049, 32.490], mean action: 1.478 [0.000, 12.000],  loss: 0.022965, mae: 0.368697, mean_q: 0.538813, mean_eps: 0.000000
 3354/5000: episode: 141, duration: 0.332s, episode steps:  23, steps per second:  69, episode reward: 32.770, mean reward:  1.425 [-2.592, 32.030], mean action: 6.609 [0.000, 15.000],  loss: 0.021207, mae: 0.357147, mean_q: 0.509721, mean_eps: 0.000000
 3384/5000: episode: 142, duration: 0.427s, episode steps:  30, steps per second:  70, episode reward: -32.340, mean reward: -1.078 [-32.021,  2.411], mean action: 6.600 [0.000, 15.000],  loss: 0.019393, mae: 0.351382, mean_q: 0.507241, mean_eps: 0.000000
 3403/5000: episode: 143, duration: 0.289s, episode steps:  19, steps per second:  66, episode reward: 37.989, mean reward:  1.999 [-2.901, 31.470], mean action: 7.368 [0.000, 19.000],  loss: 0.021895, mae: 0.366971, mean_q: 0.494468, mean_eps: 0.000000
 3425/5000: episode: 144, duration: 0.344s, episode steps:  22, steps per second:  64, episode reward: 35.762, mean reward:  1.626 [-2.465, 32.482], mean action: 4.909 [0.000, 16.000],  loss: 0.023457, mae: 0.374373, mean_q: 0.522835, mean_eps: 0.000000
 3443/5000: episode: 145, duration: 0.317s, episode steps:  18, steps per second:  57, episode reward: 38.500, mean reward:  2.139 [-2.352, 32.068], mean action: 3.833 [0.000, 15.000],  loss: 0.017896, mae: 0.348259, mean_q: 0.546741, mean_eps: 0.000000
 3458/5000: episode: 146, duration: 0.224s, episode steps:  15, steps per second:  67, episode reward: 41.309, mean reward:  2.754 [-2.588, 32.309], mean action: 3.267 [0.000, 15.000],  loss: 0.022478, mae: 0.362715, mean_q: 0.575601, mean_eps: 0.000000
 3477/5000: episode: 147, duration: 0.295s, episode steps:  19, steps per second:  64, episode reward: 33.000, mean reward:  1.737 [-2.902, 30.856], mean action: 5.105 [0.000, 15.000],  loss: 0.019837, mae: 0.356371, mean_q: 0.522800, mean_eps: 0.000000
 3494/5000: episode: 148, duration: 0.245s, episode steps:  17, steps per second:  70, episode reward: 38.035, mean reward:  2.237 [-3.000, 32.120], mean action: 5.471 [0.000, 15.000],  loss: 0.023416, mae: 0.369128, mean_q: 0.526018, mean_eps: 0.000000
 3517/5000: episode: 149, duration: 0.346s, episode steps:  23, steps per second:  67, episode reward: 32.940, mean reward:  1.432 [-2.283, 32.670], mean action: 5.217 [0.000, 13.000],  loss: 0.023565, mae: 0.373755, mean_q: 0.593231, mean_eps: 0.000000
 3531/5000: episode: 150, duration: 0.228s, episode steps:  14, steps per second:  61, episode reward: 40.652, mean reward:  2.904 [-2.393, 32.874], mean action: 3.214 [0.000, 11.000],  loss: 0.022621, mae: 0.367089, mean_q: 0.610827, mean_eps: 0.000000
 3556/5000: episode: 151, duration: 0.369s, episode steps:  25, steps per second:  68, episode reward: -33.000, mean reward: -1.320 [-32.142,  2.240], mean action: 3.800 [0.000, 16.000],  loss: 0.022684, mae: 0.362615, mean_q: 0.541139, mean_eps: 0.000000
 3574/5000: episode: 152, duration: 0.259s, episode steps:  18, steps per second:  70, episode reward: 38.212, mean reward:  2.123 [-2.685, 31.979], mean action: 3.500 [1.000, 16.000],  loss: 0.019705, mae: 0.351887, mean_q: 0.532359, mean_eps: 0.000000
 3604/5000: episode: 153, duration: 0.425s, episode steps:  30, steps per second:  71, episode reward: 38.737, mean reward:  1.291 [-2.465, 32.816], mean action: 4.700 [0.000, 19.000],  loss: 0.021688, mae: 0.364856, mean_q: 0.568526, mean_eps: 0.000000
 3627/5000: episode: 154, duration: 0.335s, episode steps:  23, steps per second:  69, episode reward: 33.000, mean reward:  1.435 [-3.000, 32.320], mean action: 4.304 [0.000, 19.000],  loss: 0.019182, mae: 0.356892, mean_q: 0.574475, mean_eps: 0.000000
 3659/5000: episode: 155, duration: 0.472s, episode steps:  32, steps per second:  68, episode reward: -41.550, mean reward: -1.298 [-32.169,  2.080], mean action: 9.531 [0.000, 21.000],  loss: 0.019624, mae: 0.361991, mean_q: 0.566584, mean_eps: 0.000000
 3687/5000: episode: 156, duration: 0.429s, episode steps:  28, steps per second:  65, episode reward: 32.541, mean reward:  1.162 [-2.325, 32.100], mean action: 4.286 [0.000, 16.000],  loss: 0.020234, mae: 0.357678, mean_q: 0.501964, mean_eps: 0.000000
 3704/5000: episode: 157, duration: 0.241s, episode steps:  17, steps per second:  70, episode reward: 38.579, mean reward:  2.269 [-3.000, 32.220], mean action: 4.118 [0.000, 16.000],  loss: 0.018538, mae: 0.354119, mean_q: 0.495238, mean_eps: 0.000000
 3717/5000: episode: 158, duration: 0.192s, episode steps:  13, steps per second:  68, episode reward: 41.479, mean reward:  3.191 [-2.249, 32.140], mean action: 2.846 [0.000, 16.000],  loss: 0.028261, mae: 0.397308, mean_q: 0.458626, mean_eps: 0.000000
 3745/5000: episode: 159, duration: 0.421s, episode steps:  28, steps per second:  67, episode reward: 35.642, mean reward:  1.273 [-2.106, 31.732], mean action: 4.179 [0.000, 16.000],  loss: 0.023446, mae: 0.373199, mean_q: 0.528110, mean_eps: 0.000000
 3774/5000: episode: 160, duration: 0.505s, episode steps:  29, steps per second:  57, episode reward: 34.741, mean reward:  1.198 [-3.000, 31.672], mean action: 7.414 [0.000, 16.000],  loss: 0.023512, mae: 0.373590, mean_q: 0.537527, mean_eps: 0.000000
 3796/5000: episode: 161, duration: 0.368s, episode steps:  22, steps per second:  60, episode reward: 35.404, mean reward:  1.609 [-2.451, 32.404], mean action: 5.136 [0.000, 16.000],  loss: 0.019284, mae: 0.358898, mean_q: 0.542339, mean_eps: 0.000000
 3808/5000: episode: 162, duration: 0.184s, episode steps:  12, steps per second:  65, episode reward: 42.000, mean reward:  3.500 [-2.283, 30.292], mean action: 2.917 [0.000, 11.000],  loss: 0.017772, mae: 0.351408, mean_q: 0.513097, mean_eps: 0.000000
 3824/5000: episode: 163, duration: 0.235s, episode steps:  16, steps per second:  68, episode reward: 40.906, mean reward:  2.557 [-2.532, 32.326], mean action: 4.312 [0.000, 14.000],  loss: 0.021373, mae: 0.358101, mean_q: 0.530988, mean_eps: 0.000000
 3846/5000: episode: 164, duration: 0.313s, episode steps:  22, steps per second:  70, episode reward: -38.030, mean reward: -1.729 [-32.413,  2.900], mean action: 7.182 [0.000, 16.000],  loss: 0.019678, mae: 0.350633, mean_q: 0.588281, mean_eps: 0.000000
 3868/5000: episode: 165, duration: 0.315s, episode steps:  22, steps per second:  70, episode reward: 35.083, mean reward:  1.595 [-2.430, 32.200], mean action: 4.364 [0.000, 16.000],  loss: 0.020378, mae: 0.363645, mean_q: 0.529810, mean_eps: 0.000000
 3880/5000: episode: 166, duration: 0.205s, episode steps:  12, steps per second:  58, episode reward: 44.133, mean reward:  3.678 [-2.064, 32.520], mean action: 2.750 [0.000, 16.000],  loss: 0.022576, mae: 0.368187, mean_q: 0.533779, mean_eps: 0.000000
 3902/5000: episode: 167, duration: 0.328s, episode steps:  22, steps per second:  67, episode reward: 35.264, mean reward:  1.603 [-2.410, 32.709], mean action: 5.773 [0.000, 16.000],  loss: 0.021024, mae: 0.363099, mean_q: 0.572933, mean_eps: 0.000000
 3931/5000: episode: 168, duration: 0.498s, episode steps:  29, steps per second:  58, episode reward: 32.709, mean reward:  1.128 [-2.821, 32.780], mean action: 4.552 [0.000, 19.000],  loss: 0.019664, mae: 0.353037, mean_q: 0.523870, mean_eps: 0.000000
 3957/5000: episode: 169, duration: 0.369s, episode steps:  26, steps per second:  70, episode reward: 38.287, mean reward:  1.473 [-2.094, 31.899], mean action: 4.385 [0.000, 16.000],  loss: 0.022633, mae: 0.372022, mean_q: 0.596374, mean_eps: 0.000000
 3983/5000: episode: 170, duration: 0.551s, episode steps:  26, steps per second:  47, episode reward: 36.000, mean reward:  1.385 [-2.423, 32.270], mean action: 3.308 [0.000, 16.000],  loss: 0.022567, mae: 0.379952, mean_q: 0.619072, mean_eps: 0.000000
 4004/5000: episode: 171, duration: 0.303s, episode steps:  21, steps per second:  69, episode reward: 36.000, mean reward:  1.714 [-3.000, 32.010], mean action: 6.810 [0.000, 19.000],  loss: 0.018143, mae: 0.356135, mean_q: 0.539794, mean_eps: 0.000000
 4025/5000: episode: 172, duration: 0.298s, episode steps:  21, steps per second:  70, episode reward: -33.000, mean reward: -1.571 [-30.800,  2.910], mean action: 5.762 [0.000, 19.000],  loss: 0.022798, mae: 0.377110, mean_q: 0.573350, mean_eps: 0.000000
 4035/5000: episode: 173, duration: 0.158s, episode steps:  10, steps per second:  63, episode reward: 47.232, mean reward:  4.723 [ 0.302, 32.320], mean action: 2.300 [0.000, 15.000],  loss: 0.018393, mae: 0.350646, mean_q: 0.502899, mean_eps: 0.000000
 4061/5000: episode: 174, duration: 0.438s, episode steps:  26, steps per second:  59, episode reward: 38.570, mean reward:  1.483 [-2.321, 32.244], mean action: 3.846 [0.000, 15.000],  loss: 0.022931, mae: 0.375254, mean_q: 0.513352, mean_eps: 0.000000
 4082/5000: episode: 175, duration: 0.448s, episode steps:  21, steps per second:  47, episode reward: 43.638, mean reward:  2.078 [-2.065, 32.310], mean action: 3.857 [1.000, 15.000],  loss: 0.014732, mae: 0.335461, mean_q: 0.462019, mean_eps: 0.000000
 4100/5000: episode: 176, duration: 0.300s, episode steps:  18, steps per second:  60, episode reward: 41.379, mean reward:  2.299 [-2.134, 32.680], mean action: 4.056 [0.000, 13.000],  loss: 0.014453, mae: 0.324278, mean_q: 0.508995, mean_eps: 0.000000
 4138/5000: episode: 177, duration: 0.558s, episode steps:  38, steps per second:  68, episode reward: 35.772, mean reward:  0.941 [-2.263, 32.300], mean action: 1.974 [0.000, 12.000],  loss: 0.020070, mae: 0.361360, mean_q: 0.495424, mean_eps: 0.000000
 4164/5000: episode: 178, duration: 0.357s, episode steps:  26, steps per second:  73, episode reward: -38.240, mean reward: -1.471 [-32.314,  2.442], mean action: 7.077 [0.000, 16.000],  loss: 0.021743, mae: 0.362133, mean_q: 0.533684, mean_eps: 0.000000
 4182/5000: episode: 179, duration: 0.271s, episode steps:  18, steps per second:  66, episode reward: 39.000, mean reward:  2.167 [-3.000, 32.470], mean action: 3.889 [0.000, 12.000],  loss: 0.018731, mae: 0.347767, mean_q: 0.539136, mean_eps: 0.000000
 4204/5000: episode: 180, duration: 0.310s, episode steps:  22, steps per second:  71, episode reward: 36.000, mean reward:  1.636 [-2.424, 32.800], mean action: 3.318 [0.000, 11.000],  loss: 0.021313, mae: 0.359062, mean_q: 0.577974, mean_eps: 0.000000
 4227/5000: episode: 181, duration: 0.327s, episode steps:  23, steps per second:  70, episode reward: -36.000, mean reward: -1.565 [-32.294,  2.630], mean action: 3.565 [0.000, 11.000],  loss: 0.016926, mae: 0.348842, mean_q: 0.532620, mean_eps: 0.000000
 4242/5000: episode: 182, duration: 0.256s, episode steps:  15, steps per second:  59, episode reward: 38.735, mean reward:  2.582 [-2.152, 30.106], mean action: 3.267 [0.000, 16.000],  loss: 0.021804, mae: 0.369025, mean_q: 0.590368, mean_eps: 0.000000
 4276/5000: episode: 183, duration: 0.481s, episode steps:  34, steps per second:  71, episode reward: -35.660, mean reward: -1.049 [-32.110,  2.310], mean action: 3.735 [0.000, 16.000],  loss: 0.020554, mae: 0.359108, mean_q: 0.523197, mean_eps: 0.000000
 4300/5000: episode: 184, duration: 0.343s, episode steps:  24, steps per second:  70, episode reward: -42.000, mean reward: -1.750 [-32.211,  3.000], mean action: 10.083 [0.000, 19.000],  loss: 0.019457, mae: 0.362086, mean_q: 0.492264, mean_eps: 0.000000
 4341/5000: episode: 185, duration: 0.561s, episode steps:  41, steps per second:  73, episode reward: 40.900, mean reward:  0.998 [-2.247, 32.311], mean action: 5.171 [0.000, 20.000],  loss: 0.021687, mae: 0.364634, mean_q: 0.549099, mean_eps: 0.000000
 4377/5000: episode: 186, duration: 0.512s, episode steps:  36, steps per second:  70, episode reward: 35.202, mean reward:  0.978 [-2.793, 32.360], mean action: 4.417 [0.000, 19.000],  loss: 0.023405, mae: 0.381428, mean_q: 0.515481, mean_eps: 0.000000
 4398/5000: episode: 187, duration: 0.312s, episode steps:  21, steps per second:  67, episode reward: 38.983, mean reward:  1.856 [-2.111, 30.457], mean action: 5.381 [1.000, 19.000],  loss: 0.021032, mae: 0.369478, mean_q: 0.509148, mean_eps: 0.000000
 4422/5000: episode: 188, duration: 0.338s, episode steps:  24, steps per second:  71, episode reward: 35.586, mean reward:  1.483 [-2.488, 32.440], mean action: 4.042 [1.000, 19.000],  loss: 0.017487, mae: 0.347789, mean_q: 0.520825, mean_eps: 0.000000
 4441/5000: episode: 189, duration: 0.286s, episode steps:  19, steps per second:  67, episode reward: -38.200, mean reward: -2.011 [-32.290,  2.293], mean action: 4.895 [0.000, 19.000],  loss: 0.017962, mae: 0.356127, mean_q: 0.533534, mean_eps: 0.000000
 4462/5000: episode: 190, duration: 0.311s, episode steps:  21, steps per second:  68, episode reward: -32.670, mean reward: -1.556 [-32.323,  3.000], mean action: 6.333 [0.000, 13.000],  loss: 0.020699, mae: 0.368977, mean_q: 0.569713, mean_eps: 0.000000
 4484/5000: episode: 191, duration: 0.381s, episode steps:  22, steps per second:  58, episode reward: 35.778, mean reward:  1.626 [-2.614, 32.360], mean action: 3.136 [0.000, 19.000],  loss: 0.020824, mae: 0.373102, mean_q: 0.570571, mean_eps: 0.000000
 4504/5000: episode: 192, duration: 0.298s, episode steps:  20, steps per second:  67, episode reward: 35.527, mean reward:  1.776 [-2.852, 32.120], mean action: 4.250 [0.000, 19.000],  loss: 0.026070, mae: 0.394702, mean_q: 0.561541, mean_eps: 0.000000
 4524/5000: episode: 193, duration: 0.292s, episode steps:  20, steps per second:  68, episode reward: 35.172, mean reward:  1.759 [-2.316, 32.112], mean action: 3.200 [0.000, 19.000],  loss: 0.022850, mae: 0.382627, mean_q: 0.596734, mean_eps: 0.000000
 4548/5000: episode: 194, duration: 0.339s, episode steps:  24, steps per second:  71, episode reward: 34.574, mean reward:  1.441 [-2.776, 32.050], mean action: 4.208 [0.000, 16.000],  loss: 0.021557, mae: 0.378552, mean_q: 0.565365, mean_eps: 0.000000
 4582/5000: episode: 195, duration: 0.489s, episode steps:  34, steps per second:  69, episode reward: 38.552, mean reward:  1.134 [-2.531, 32.160], mean action: 3.471 [0.000, 16.000],  loss: 0.021171, mae: 0.380360, mean_q: 0.503169, mean_eps: 0.000000
 4609/5000: episode: 196, duration: 0.377s, episode steps:  27, steps per second:  72, episode reward: 32.197, mean reward:  1.192 [-2.781, 32.332], mean action: 6.926 [0.000, 20.000],  loss: 0.022104, mae: 0.378665, mean_q: 0.547420, mean_eps: 0.000000
 4633/5000: episode: 197, duration: 0.340s, episode steps:  24, steps per second:  71, episode reward: -32.020, mean reward: -1.334 [-31.196,  3.062], mean action: 5.250 [0.000, 20.000],  loss: 0.019294, mae: 0.365216, mean_q: 0.541178, mean_eps: 0.000000
 4655/5000: episode: 198, duration: 0.323s, episode steps:  22, steps per second:  68, episode reward: 38.382, mean reward:  1.745 [-2.303, 31.815], mean action: 4.591 [0.000, 19.000],  loss: 0.021273, mae: 0.378141, mean_q: 0.544041, mean_eps: 0.000000
 4678/5000: episode: 199, duration: 0.320s, episode steps:  23, steps per second:  72, episode reward: 32.902, mean reward:  1.431 [-3.000, 32.382], mean action: 5.522 [0.000, 19.000],  loss: 0.017729, mae: 0.362010, mean_q: 0.550808, mean_eps: 0.000000
 4697/5000: episode: 200, duration: 0.270s, episode steps:  19, steps per second:  70, episode reward: 35.800, mean reward:  1.884 [-2.900, 32.150], mean action: 5.316 [0.000, 19.000],  loss: 0.020931, mae: 0.378040, mean_q: 0.527282, mean_eps: 0.000000
 4722/5000: episode: 201, duration: 0.358s, episode steps:  25, steps per second:  70, episode reward: -32.710, mean reward: -1.308 [-32.092,  3.000], mean action: 5.600 [0.000, 17.000],  loss: 0.022254, mae: 0.379884, mean_q: 0.545327, mean_eps: 0.000000
 4748/5000: episode: 202, duration: 0.379s, episode steps:  26, steps per second:  69, episode reward: -33.000, mean reward: -1.269 [-32.422,  2.362], mean action: 3.962 [0.000, 16.000],  loss: 0.018034, mae: 0.356712, mean_q: 0.490123, mean_eps: 0.000000
 4773/5000: episode: 203, duration: 0.382s, episode steps:  25, steps per second:  65, episode reward: 35.428, mean reward:  1.417 [-2.307, 31.913], mean action: 3.880 [0.000, 16.000],  loss: 0.017721, mae: 0.347139, mean_q: 0.504902, mean_eps: 0.000000
 4799/5000: episode: 204, duration: 0.363s, episode steps:  26, steps per second:  72, episode reward: 32.904, mean reward:  1.266 [-2.587, 32.244], mean action: 3.769 [0.000, 15.000],  loss: 0.018850, mae: 0.359542, mean_q: 0.506298, mean_eps: 0.000000
 4821/5000: episode: 205, duration: 0.320s, episode steps:  22, steps per second:  69, episode reward: 35.904, mean reward:  1.632 [-2.901, 32.904], mean action: 2.955 [0.000, 9.000],  loss: 0.019713, mae: 0.366452, mean_q: 0.485019, mean_eps: 0.000000
 4839/5000: episode: 206, duration: 0.280s, episode steps:  18, steps per second:  64, episode reward: 35.969, mean reward:  1.998 [-3.000, 32.310], mean action: 3.778 [0.000, 13.000],  loss: 0.018978, mae: 0.362154, mean_q: 0.556041, mean_eps: 0.000000
 4861/5000: episode: 207, duration: 0.368s, episode steps:  22, steps per second:  60, episode reward: 33.000, mean reward:  1.500 [-3.000, 32.850], mean action: 4.182 [0.000, 14.000],  loss: 0.018259, mae: 0.358755, mean_q: 0.533717, mean_eps: 0.000000
 4882/5000: episode: 208, duration: 0.297s, episode steps:  21, steps per second:  71, episode reward: 32.666, mean reward:  1.556 [-2.534, 33.002], mean action: 5.429 [0.000, 18.000],  loss: 0.020715, mae: 0.366395, mean_q: 0.534218, mean_eps: 0.000000
 4902/5000: episode: 209, duration: 0.301s, episode steps:  20, steps per second:  66, episode reward: 41.445, mean reward:  2.072 [-2.706, 32.003], mean action: 3.200 [0.000, 9.000],  loss: 0.021201, mae: 0.366528, mean_q: 0.539352, mean_eps: 0.000000
 4921/5000: episode: 210, duration: 0.301s, episode steps:  19, steps per second:  63, episode reward: 40.752, mean reward:  2.145 [-2.109, 32.820], mean action: 4.158 [0.000, 14.000],  loss: 0.020809, mae: 0.371104, mean_q: 0.510366, mean_eps: 0.000000
 4948/5000: episode: 211, duration: 0.382s, episode steps:  27, steps per second:  71, episode reward: -35.560, mean reward: -1.317 [-32.159,  2.580], mean action: 2.593 [0.000, 9.000],  loss: 0.019359, mae: 0.363161, mean_q: 0.518241, mean_eps: 0.000000
 4977/5000: episode: 212, duration: 0.418s, episode steps:  29, steps per second:  69, episode reward: -33.000, mean reward: -1.138 [-32.413,  2.376], mean action: 4.517 [1.000, 14.000],  loss: 0.019481, mae: 0.361112, mean_q: 0.519914, mean_eps: 0.000000
 4997/5000: episode: 213, duration: 0.304s, episode steps:  20, steps per second:  66, episode reward: 33.000, mean reward:  1.650 [-2.492, 30.006], mean action: 2.850 [0.000, 9.000],  loss: 0.023525, mae: 0.378966, mean_q: 0.526945, mean_eps: 0.000000
done, took 70.106 seconds
DQN Evaluation: 6124 victories out of 7225 episodes
Training for 5000 steps ...
   21/5000: episode: 1, duration: 0.178s, episode steps:  21, steps per second: 118, episode reward: 46.891, mean reward:  2.233 [-0.493, 32.854], mean action: 1.286 [0.000, 8.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   62/5000: episode: 2, duration: 0.263s, episode steps:  41, steps per second: 156, episode reward: -32.620, mean reward: -0.796 [-31.807,  2.630], mean action: 7.780 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   80/5000: episode: 3, duration: 0.135s, episode steps:  18, steps per second: 133, episode reward: 44.528, mean reward:  2.474 [-2.465, 32.440], mean action: 1.667 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   95/5000: episode: 4, duration: 0.148s, episode steps:  15, steps per second: 102, episode reward: 42.000, mean reward:  2.800 [-2.902, 32.160], mean action: 1.400 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  114/5000: episode: 5, duration: 0.143s, episode steps:  19, steps per second: 133, episode reward: 43.522, mean reward:  2.291 [-2.535, 31.915], mean action: 3.368 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  142/5000: episode: 6, duration: 0.200s, episode steps:  28, steps per second: 140, episode reward: 41.735, mean reward:  1.491 [-2.701, 31.955], mean action: 2.571 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  166/5000: episode: 7, duration: 0.165s, episode steps:  24, steps per second: 146, episode reward: 43.922, mean reward:  1.830 [-2.120, 32.940], mean action: 3.708 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  193/5000: episode: 8, duration: 0.182s, episode steps:  27, steps per second: 148, episode reward: -35.250, mean reward: -1.306 [-32.470,  2.941], mean action: 4.778 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  219/5000: episode: 9, duration: 0.171s, episode steps:  26, steps per second: 152, episode reward: 38.385, mean reward:  1.476 [-2.507, 32.648], mean action: 3.962 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  244/5000: episode: 10, duration: 0.163s, episode steps:  25, steps per second: 153, episode reward: 41.031, mean reward:  1.641 [-2.450, 32.100], mean action: 4.480 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  262/5000: episode: 11, duration: 0.133s, episode steps:  18, steps per second: 135, episode reward: 42.000, mean reward:  2.333 [-2.492, 33.000], mean action: 1.444 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  295/5000: episode: 12, duration: 0.210s, episode steps:  33, steps per second: 157, episode reward: 41.194, mean reward:  1.248 [-2.545, 32.140], mean action: 1.000 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  320/5000: episode: 13, duration: 0.186s, episode steps:  25, steps per second: 134, episode reward: 41.540, mean reward:  1.662 [-2.356, 32.733], mean action: 4.040 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  340/5000: episode: 14, duration: 0.140s, episode steps:  20, steps per second: 142, episode reward: 44.307, mean reward:  2.215 [-2.217, 32.460], mean action: 1.450 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  362/5000: episode: 15, duration: 0.144s, episode steps:  22, steps per second: 153, episode reward: 38.824, mean reward:  1.765 [-2.371, 32.234], mean action: 2.955 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  391/5000: episode: 16, duration: 0.190s, episode steps:  29, steps per second: 153, episode reward: 41.337, mean reward:  1.425 [-2.431, 32.030], mean action: 4.034 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  410/5000: episode: 17, duration: 0.142s, episode steps:  19, steps per second: 134, episode reward: 47.535, mean reward:  2.502 [-0.630, 33.000], mean action: 1.526 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  436/5000: episode: 18, duration: 0.203s, episode steps:  26, steps per second: 128, episode reward: 41.175, mean reward:  1.584 [-2.263, 32.540], mean action: 1.846 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  464/5000: episode: 19, duration: 0.183s, episode steps:  28, steps per second: 153, episode reward: 38.175, mean reward:  1.363 [-2.725, 32.280], mean action: 2.750 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  498/5000: episode: 20, duration: 0.226s, episode steps:  34, steps per second: 150, episode reward: 41.806, mean reward:  1.230 [-2.496, 32.270], mean action: 2.735 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  521/5000: episode: 21, duration: 0.160s, episode steps:  23, steps per second: 143, episode reward: 38.198, mean reward:  1.661 [-3.000, 32.523], mean action: 5.087 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  551/5000: episode: 22, duration: 0.455s, episode steps:  30, steps per second:  66, episode reward: 39.000, mean reward:  1.300 [-3.000, 32.770], mean action: 2.900 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  567/5000: episode: 23, duration: 0.178s, episode steps:  16, steps per second:  90, episode reward: 41.715, mean reward:  2.607 [-2.242, 32.325], mean action: 2.750 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  591/5000: episode: 24, duration: 0.167s, episode steps:  24, steps per second: 144, episode reward: 41.760, mean reward:  1.740 [-2.895, 32.100], mean action: 4.333 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  611/5000: episode: 25, duration: 0.143s, episode steps:  20, steps per second: 140, episode reward: 41.468, mean reward:  2.073 [-2.472, 32.468], mean action: 5.100 [2.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  629/5000: episode: 26, duration: 0.145s, episode steps:  18, steps per second: 124, episode reward: 41.123, mean reward:  2.285 [-2.251, 31.958], mean action: 1.833 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  671/5000: episode: 27, duration: 0.401s, episode steps:  42, steps per second: 105, episode reward: 38.923, mean reward:  0.927 [-2.401, 32.330], mean action: 1.881 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  686/5000: episode: 28, duration: 0.135s, episode steps:  15, steps per second: 111, episode reward: 41.147, mean reward:  2.743 [-3.000, 32.140], mean action: 2.133 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  706/5000: episode: 29, duration: 0.139s, episode steps:  20, steps per second: 144, episode reward: 41.669, mean reward:  2.083 [-2.265, 32.540], mean action: 3.250 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  747/5000: episode: 30, duration: 0.303s, episode steps:  41, steps per second: 136, episode reward: 38.847, mean reward:  0.947 [-2.183, 32.050], mean action: 3.073 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  762/5000: episode: 31, duration: 0.111s, episode steps:  15, steps per second: 135, episode reward: 44.719, mean reward:  2.981 [-2.071, 32.240], mean action: 0.600 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  778/5000: episode: 32, duration: 0.130s, episode steps:  16, steps per second: 123, episode reward: 44.243, mean reward:  2.765 [-2.629, 31.981], mean action: 2.500 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  797/5000: episode: 33, duration: 0.143s, episode steps:  19, steps per second: 133, episode reward: 46.191, mean reward:  2.431 [-0.294, 32.312], mean action: 2.579 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  825/5000: episode: 34, duration: 0.184s, episode steps:  28, steps per second: 152, episode reward: 38.813, mean reward:  1.386 [-3.000, 32.054], mean action: 5.286 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  845/5000: episode: 35, duration: 0.144s, episode steps:  20, steps per second: 139, episode reward: 42.000, mean reward:  2.100 [-3.000, 32.220], mean action: 2.250 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  863/5000: episode: 36, duration: 0.141s, episode steps:  18, steps per second: 128, episode reward: 41.137, mean reward:  2.285 [-3.000, 32.533], mean action: 5.333 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  899/5000: episode: 37, duration: 0.283s, episode steps:  36, steps per second: 127, episode reward: 32.859, mean reward:  0.913 [-3.000, 32.200], mean action: 8.000 [0.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  933/5000: episode: 38, duration: 0.219s, episode steps:  34, steps per second: 155, episode reward: 41.702, mean reward:  1.227 [-2.393, 31.921], mean action: 1.000 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  961/5000: episode: 39, duration: 0.213s, episode steps:  28, steps per second: 131, episode reward: 44.678, mean reward:  1.596 [-2.199, 32.360], mean action: 3.357 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  995/5000: episode: 40, duration: 0.233s, episode steps:  34, steps per second: 146, episode reward: 43.366, mean reward:  1.275 [-2.025, 32.350], mean action: 2.147 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1021/5000: episode: 41, duration: 0.418s, episode steps:  26, steps per second:  62, episode reward: 44.566, mean reward:  1.714 [-2.577, 32.136], mean action: 2.192 [0.000, 9.000],  loss: 0.019446, mae: 0.362917, mean_q: 0.533285, mean_eps: 0.000000
 1043/5000: episode: 42, duration: 0.349s, episode steps:  22, steps per second:  63, episode reward: 41.726, mean reward:  1.897 [-2.611, 32.433], mean action: 3.045 [0.000, 15.000],  loss: 0.018411, mae: 0.356886, mean_q: 0.579377, mean_eps: 0.000000
 1061/5000: episode: 43, duration: 0.301s, episode steps:  18, steps per second:  60, episode reward: 44.237, mean reward:  2.458 [-2.083, 32.100], mean action: 5.500 [1.000, 15.000],  loss: 0.019542, mae: 0.358659, mean_q: 0.540614, mean_eps: 0.000000
 1084/5000: episode: 44, duration: 0.358s, episode steps:  23, steps per second:  64, episode reward: 41.789, mean reward:  1.817 [-2.206, 32.030], mean action: 3.913 [0.000, 15.000],  loss: 0.020726, mae: 0.361169, mean_q: 0.566151, mean_eps: 0.000000
 1136/5000: episode: 45, duration: 0.872s, episode steps:  52, steps per second:  60, episode reward: 32.104, mean reward:  0.617 [-2.606, 31.468], mean action: 9.577 [0.000, 21.000],  loss: 0.021102, mae: 0.370830, mean_q: 0.562084, mean_eps: 0.000000
 1167/5000: episode: 46, duration: 0.430s, episode steps:  31, steps per second:  72, episode reward: -32.720, mean reward: -1.055 [-32.344,  2.506], mean action: 3.968 [0.000, 17.000],  loss: 0.019741, mae: 0.369355, mean_q: 0.524444, mean_eps: 0.000000
 1199/5000: episode: 47, duration: 0.470s, episode steps:  32, steps per second:  68, episode reward: 38.797, mean reward:  1.212 [-2.634, 32.070], mean action: 1.688 [0.000, 11.000],  loss: 0.017938, mae: 0.357825, mean_q: 0.481498, mean_eps: 0.000000
 1236/5000: episode: 48, duration: 0.534s, episode steps:  37, steps per second:  69, episode reward: 40.924, mean reward:  1.106 [-2.194, 31.902], mean action: 2.135 [0.000, 12.000],  loss: 0.020506, mae: 0.364844, mean_q: 0.506819, mean_eps: 0.000000
 1263/5000: episode: 49, duration: 0.389s, episode steps:  27, steps per second:  69, episode reward: 38.449, mean reward:  1.424 [-3.000, 32.063], mean action: 3.778 [0.000, 12.000],  loss: 0.022880, mae: 0.375561, mean_q: 0.589312, mean_eps: 0.000000
 1300/5000: episode: 50, duration: 0.559s, episode steps:  37, steps per second:  66, episode reward: 35.796, mean reward:  0.967 [-2.527, 32.090], mean action: 3.486 [0.000, 20.000],  loss: 0.020893, mae: 0.370116, mean_q: 0.517859, mean_eps: 0.000000
 1327/5000: episode: 51, duration: 0.473s, episode steps:  27, steps per second:  57, episode reward: 36.000, mean reward:  1.333 [-2.628, 32.260], mean action: 2.074 [0.000, 9.000],  loss: 0.018009, mae: 0.355885, mean_q: 0.494885, mean_eps: 0.000000
 1360/5000: episode: 52, duration: 0.485s, episode steps:  33, steps per second:  68, episode reward: -32.360, mean reward: -0.981 [-33.000,  2.606], mean action: 5.333 [0.000, 19.000],  loss: 0.018958, mae: 0.366364, mean_q: 0.433797, mean_eps: 0.000000
 1390/5000: episode: 53, duration: 0.417s, episode steps:  30, steps per second:  72, episode reward: 38.904, mean reward:  1.297 [-2.741, 32.674], mean action: 2.533 [0.000, 19.000],  loss: 0.018873, mae: 0.357851, mean_q: 0.477085, mean_eps: 0.000000
 1418/5000: episode: 54, duration: 0.392s, episode steps:  28, steps per second:  71, episode reward: 40.921, mean reward:  1.461 [-2.194, 32.020], mean action: 3.607 [0.000, 15.000],  loss: 0.019386, mae: 0.357409, mean_q: 0.528743, mean_eps: 0.000000
 1441/5000: episode: 55, duration: 0.322s, episode steps:  23, steps per second:  71, episode reward: 38.903, mean reward:  1.691 [-3.000, 32.343], mean action: 3.826 [0.000, 21.000],  loss: 0.022324, mae: 0.375383, mean_q: 0.543688, mean_eps: 0.000000
 1465/5000: episode: 56, duration: 0.340s, episode steps:  24, steps per second:  71, episode reward: 41.824, mean reward:  1.743 [-2.027, 32.824], mean action: 2.958 [0.000, 15.000],  loss: 0.022397, mae: 0.371282, mean_q: 0.551192, mean_eps: 0.000000
 1488/5000: episode: 57, duration: 0.332s, episode steps:  23, steps per second:  69, episode reward: 45.000, mean reward:  1.957 [-2.040, 32.520], mean action: 1.609 [0.000, 15.000],  loss: 0.021830, mae: 0.377255, mean_q: 0.551340, mean_eps: 0.000000
 1509/5000: episode: 58, duration: 0.302s, episode steps:  21, steps per second:  70, episode reward: 47.027, mean reward:  2.239 [-0.053, 33.000], mean action: 2.143 [1.000, 3.000],  loss: 0.022777, mae: 0.379698, mean_q: 0.533215, mean_eps: 0.000000
 1526/5000: episode: 59, duration: 0.241s, episode steps:  17, steps per second:  70, episode reward: 38.342, mean reward:  2.255 [-3.000, 32.540], mean action: 4.294 [0.000, 16.000],  loss: 0.019402, mae: 0.365632, mean_q: 0.511156, mean_eps: 0.000000
 1547/5000: episode: 60, duration: 0.299s, episode steps:  21, steps per second:  70, episode reward: 44.131, mean reward:  2.101 [-2.562, 32.280], mean action: 2.619 [0.000, 16.000],  loss: 0.019027, mae: 0.361375, mean_q: 0.501938, mean_eps: 0.000000
 1582/5000: episode: 61, duration: 0.485s, episode steps:  35, steps per second:  72, episode reward: 41.331, mean reward:  1.181 [-2.354, 32.650], mean action: 2.171 [0.000, 16.000],  loss: 0.023056, mae: 0.388919, mean_q: 0.551569, mean_eps: 0.000000
 1611/5000: episode: 62, duration: 0.400s, episode steps:  29, steps per second:  73, episode reward: 38.418, mean reward:  1.325 [-2.463, 32.030], mean action: 5.345 [0.000, 16.000],  loss: 0.023336, mae: 0.393376, mean_q: 0.500031, mean_eps: 0.000000
 1642/5000: episode: 63, duration: 0.429s, episode steps:  31, steps per second:  72, episode reward: 40.754, mean reward:  1.315 [-3.000, 31.923], mean action: 5.968 [0.000, 16.000],  loss: 0.021106, mae: 0.381272, mean_q: 0.486782, mean_eps: 0.000000
 1671/5000: episode: 64, duration: 0.403s, episode steps:  29, steps per second:  72, episode reward: 39.000, mean reward:  1.345 [-2.442, 32.430], mean action: 2.759 [0.000, 15.000],  loss: 0.018977, mae: 0.371788, mean_q: 0.472443, mean_eps: 0.000000
 1701/5000: episode: 65, duration: 0.427s, episode steps:  30, steps per second:  70, episode reward: 35.252, mean reward:  1.175 [-2.423, 31.632], mean action: 3.767 [0.000, 15.000],  loss: 0.023144, mae: 0.380878, mean_q: 0.503252, mean_eps: 0.000000
 1737/5000: episode: 66, duration: 0.541s, episode steps:  36, steps per second:  66, episode reward: 32.802, mean reward:  0.911 [-2.544, 31.822], mean action: 5.611 [0.000, 15.000],  loss: 0.023278, mae: 0.386885, mean_q: 0.499949, mean_eps: 0.000000
 1766/5000: episode: 67, duration: 0.412s, episode steps:  29, steps per second:  70, episode reward: 35.847, mean reward:  1.236 [-2.846, 32.200], mean action: 4.414 [0.000, 15.000],  loss: 0.020979, mae: 0.368223, mean_q: 0.505211, mean_eps: 0.000000
 1802/5000: episode: 68, duration: 0.499s, episode steps:  36, steps per second:  72, episode reward: 41.316, mean reward:  1.148 [-2.877, 32.360], mean action: 2.306 [0.000, 16.000],  loss: 0.017033, mae: 0.347889, mean_q: 0.481653, mean_eps: 0.000000
 1831/5000: episode: 69, duration: 0.418s, episode steps:  29, steps per second:  69, episode reward: 35.474, mean reward:  1.223 [-3.000, 32.120], mean action: 3.241 [0.000, 16.000],  loss: 0.022812, mae: 0.369374, mean_q: 0.517520, mean_eps: 0.000000
 1858/5000: episode: 70, duration: 0.373s, episode steps:  27, steps per second:  72, episode reward: 35.107, mean reward:  1.300 [-3.000, 32.050], mean action: 3.148 [0.000, 19.000],  loss: 0.017337, mae: 0.352703, mean_q: 0.556174, mean_eps: 0.000000
 1877/5000: episode: 71, duration: 0.274s, episode steps:  19, steps per second:  69, episode reward: 42.000, mean reward:  2.211 [-2.859, 32.460], mean action: 4.105 [0.000, 15.000],  loss: 0.019287, mae: 0.355718, mean_q: 0.537766, mean_eps: 0.000000
 1901/5000: episode: 72, duration: 0.349s, episode steps:  24, steps per second:  69, episode reward: 42.000, mean reward:  1.750 [-2.106, 32.090], mean action: 2.750 [0.000, 12.000],  loss: 0.017300, mae: 0.345130, mean_q: 0.536004, mean_eps: 0.000000
 1930/5000: episode: 73, duration: 0.403s, episode steps:  29, steps per second:  72, episode reward: 44.570, mean reward:  1.537 [-2.631, 32.130], mean action: 2.586 [1.000, 3.000],  loss: 0.022558, mae: 0.374897, mean_q: 0.582060, mean_eps: 0.000000
 1945/5000: episode: 74, duration: 0.227s, episode steps:  15, steps per second:  66, episode reward: 47.072, mean reward:  3.138 [-0.030, 32.100], mean action: 1.000 [0.000, 3.000],  loss: 0.017021, mae: 0.354296, mean_q: 0.528846, mean_eps: 0.000000
 1990/5000: episode: 75, duration: 0.605s, episode steps:  45, steps per second:  74, episode reward: 32.524, mean reward:  0.723 [-2.562, 32.350], mean action: 4.533 [0.000, 15.000],  loss: 0.021409, mae: 0.377821, mean_q: 0.495701, mean_eps: 0.000000
 2012/5000: episode: 76, duration: 0.319s, episode steps:  22, steps per second:  69, episode reward: 43.850, mean reward:  1.993 [-2.912, 33.000], mean action: 2.864 [0.000, 20.000],  loss: 0.018763, mae: 0.360443, mean_q: 0.489859, mean_eps: 0.000000
 2041/5000: episode: 77, duration: 0.405s, episode steps:  29, steps per second:  72, episode reward: 38.027, mean reward:  1.311 [-2.536, 31.530], mean action: 4.103 [1.000, 14.000],  loss: 0.023119, mae: 0.388734, mean_q: 0.507938, mean_eps: 0.000000
 2068/5000: episode: 78, duration: 0.385s, episode steps:  27, steps per second:  70, episode reward: 41.701, mean reward:  1.544 [-2.794, 32.190], mean action: 3.185 [0.000, 16.000],  loss: 0.022196, mae: 0.381548, mean_q: 0.557513, mean_eps: 0.000000
 2089/5000: episode: 79, duration: 0.299s, episode steps:  21, steps per second:  70, episode reward: 41.282, mean reward:  1.966 [-2.262, 32.340], mean action: 3.286 [0.000, 16.000],  loss: 0.021855, mae: 0.381755, mean_q: 0.568055, mean_eps: 0.000000
 2121/5000: episode: 80, duration: 0.487s, episode steps:  32, steps per second:  66, episode reward: 41.213, mean reward:  1.288 [-2.507, 32.370], mean action: 2.625 [0.000, 16.000],  loss: 0.019996, mae: 0.363791, mean_q: 0.546619, mean_eps: 0.000000
 2152/5000: episode: 81, duration: 0.441s, episode steps:  31, steps per second:  70, episode reward: 38.830, mean reward:  1.253 [-2.430, 32.253], mean action: 2.290 [0.000, 16.000],  loss: 0.020088, mae: 0.366934, mean_q: 0.559627, mean_eps: 0.000000
 2205/5000: episode: 82, duration: 0.712s, episode steps:  53, steps per second:  74, episode reward: 35.587, mean reward:  0.671 [-2.556, 32.170], mean action: 3.189 [0.000, 16.000],  loss: 0.021463, mae: 0.369975, mean_q: 0.516089, mean_eps: 0.000000
 2228/5000: episode: 83, duration: 0.338s, episode steps:  23, steps per second:  68, episode reward: 45.000, mean reward:  1.957 [-2.106, 32.090], mean action: 2.391 [0.000, 15.000],  loss: 0.019978, mae: 0.358863, mean_q: 0.486290, mean_eps: 0.000000
 2259/5000: episode: 84, duration: 0.432s, episode steps:  31, steps per second:  72, episode reward: 38.378, mean reward:  1.238 [-2.271, 31.833], mean action: 4.968 [0.000, 21.000],  loss: 0.018451, mae: 0.351520, mean_q: 0.456078, mean_eps: 0.000000
 2279/5000: episode: 85, duration: 0.415s, episode steps:  20, steps per second:  48, episode reward: 44.652, mean reward:  2.233 [-2.131, 32.100], mean action: 3.800 [1.000, 16.000],  loss: 0.015103, mae: 0.335749, mean_q: 0.490040, mean_eps: 0.000000
 2305/5000: episode: 86, duration: 0.415s, episode steps:  26, steps per second:  63, episode reward: 38.655, mean reward:  1.487 [-3.000, 31.725], mean action: 3.462 [0.000, 16.000],  loss: 0.020797, mae: 0.354000, mean_q: 0.501091, mean_eps: 0.000000
 2330/5000: episode: 87, duration: 0.365s, episode steps:  25, steps per second:  69, episode reward: 38.582, mean reward:  1.543 [-3.000, 32.040], mean action: 3.360 [0.000, 16.000],  loss: 0.023414, mae: 0.361217, mean_q: 0.557338, mean_eps: 0.000000
 2363/5000: episode: 88, duration: 0.463s, episode steps:  33, steps per second:  71, episode reward: 44.424, mean reward:  1.346 [-2.357, 31.671], mean action: 2.212 [0.000, 9.000],  loss: 0.025290, mae: 0.375219, mean_q: 0.577005, mean_eps: 0.000000
 2384/5000: episode: 89, duration: 0.309s, episode steps:  21, steps per second:  68, episode reward: 42.329, mean reward:  2.016 [-2.157, 29.758], mean action: 1.333 [0.000, 9.000],  loss: 0.019488, mae: 0.355665, mean_q: 0.607942, mean_eps: 0.000000
 2421/5000: episode: 90, duration: 0.508s, episode steps:  37, steps per second:  73, episode reward: 35.570, mean reward:  0.961 [-2.589, 32.140], mean action: 4.405 [0.000, 15.000],  loss: 0.018148, mae: 0.350565, mean_q: 0.532894, mean_eps: 0.000000
 2451/5000: episode: 91, duration: 0.427s, episode steps:  30, steps per second:  70, episode reward: 44.591, mean reward:  1.486 [-2.100, 32.730], mean action: 1.067 [0.000, 9.000],  loss: 0.020143, mae: 0.353549, mean_q: 0.571223, mean_eps: 0.000000
 2472/5000: episode: 92, duration: 0.295s, episode steps:  21, steps per second:  71, episode reward: 38.813, mean reward:  1.848 [-2.339, 32.340], mean action: 2.190 [0.000, 15.000],  loss: 0.018776, mae: 0.345811, mean_q: 0.582633, mean_eps: 0.000000
 2507/5000: episode: 93, duration: 0.489s, episode steps:  35, steps per second:  72, episode reward: 35.766, mean reward:  1.022 [-3.000, 32.106], mean action: 4.629 [0.000, 21.000],  loss: 0.021427, mae: 0.364342, mean_q: 0.531178, mean_eps: 0.000000
 2528/5000: episode: 94, duration: 0.309s, episode steps:  21, steps per second:  68, episode reward: 39.000, mean reward:  1.857 [-3.000, 32.260], mean action: 4.190 [0.000, 21.000],  loss: 0.021416, mae: 0.362214, mean_q: 0.456518, mean_eps: 0.000000
 2549/5000: episode: 95, duration: 0.299s, episode steps:  21, steps per second:  70, episode reward: 44.232, mean reward:  2.106 [-2.291, 31.840], mean action: 1.667 [0.000, 19.000],  loss: 0.019091, mae: 0.346981, mean_q: 0.464915, mean_eps: 0.000000
 2573/5000: episode: 96, duration: 0.337s, episode steps:  24, steps per second:  71, episode reward: 41.919, mean reward:  1.747 [-2.389, 32.160], mean action: 1.708 [0.000, 19.000],  loss: 0.016991, mae: 0.339882, mean_q: 0.484730, mean_eps: 0.000000
 2602/5000: episode: 97, duration: 0.409s, episode steps:  29, steps per second:  71, episode reward: 35.022, mean reward:  1.208 [-3.000, 32.900], mean action: 1.931 [0.000, 12.000],  loss: 0.022281, mae: 0.369343, mean_q: 0.526354, mean_eps: 0.000000
 2629/5000: episode: 98, duration: 0.379s, episode steps:  27, steps per second:  71, episode reward: 41.399, mean reward:  1.533 [-3.000, 32.323], mean action: 3.556 [0.000, 16.000],  loss: 0.019076, mae: 0.356651, mean_q: 0.499046, mean_eps: 0.000000
 2647/5000: episode: 99, duration: 0.260s, episode steps:  18, steps per second:  69, episode reward: 44.089, mean reward:  2.449 [-2.175, 32.601], mean action: 2.444 [0.000, 16.000],  loss: 0.015783, mae: 0.339271, mean_q: 0.509801, mean_eps: 0.000000
 2671/5000: episode: 100, duration: 0.332s, episode steps:  24, steps per second:  72, episode reward: 41.845, mean reward:  1.744 [-3.000, 32.260], mean action: 4.000 [0.000, 16.000],  loss: 0.020935, mae: 0.354134, mean_q: 0.559147, mean_eps: 0.000000
 2695/5000: episode: 101, duration: 0.334s, episode steps:  24, steps per second:  72, episode reward: 35.753, mean reward:  1.490 [-2.519, 32.390], mean action: 3.042 [0.000, 16.000],  loss: 0.020786, mae: 0.357272, mean_q: 0.567899, mean_eps: 0.000000
 2734/5000: episode: 102, duration: 0.556s, episode steps:  39, steps per second:  70, episode reward: 38.876, mean reward:  0.997 [-3.000, 32.360], mean action: 1.769 [0.000, 16.000],  loss: 0.022191, mae: 0.370253, mean_q: 0.497765, mean_eps: 0.000000
 2753/5000: episode: 103, duration: 0.293s, episode steps:  19, steps per second:  65, episode reward: 42.000, mean reward:  2.211 [-2.703, 32.120], mean action: 1.316 [0.000, 9.000],  loss: 0.021783, mae: 0.357693, mean_q: 0.557618, mean_eps: 0.000000
 2775/5000: episode: 104, duration: 0.326s, episode steps:  22, steps per second:  67, episode reward: 42.000, mean reward:  1.909 [-2.269, 32.120], mean action: 2.227 [0.000, 9.000],  loss: 0.018661, mae: 0.342232, mean_q: 0.558608, mean_eps: 0.000000
 2801/5000: episode: 105, duration: 0.377s, episode steps:  26, steps per second:  69, episode reward: 37.698, mean reward:  1.450 [-2.878, 32.081], mean action: 4.885 [0.000, 14.000],  loss: 0.017908, mae: 0.341337, mean_q: 0.491712, mean_eps: 0.000000
 2830/5000: episode: 106, duration: 0.405s, episode steps:  29, steps per second:  72, episode reward: 41.223, mean reward:  1.421 [-2.220, 32.145], mean action: 3.000 [0.000, 14.000],  loss: 0.022219, mae: 0.359509, mean_q: 0.494268, mean_eps: 0.000000
 2851/5000: episode: 107, duration: 0.297s, episode steps:  21, steps per second:  71, episode reward: 39.000, mean reward:  1.857 [-2.613, 32.250], mean action: 3.238 [0.000, 11.000],  loss: 0.023711, mae: 0.370010, mean_q: 0.593440, mean_eps: 0.000000
 2880/5000: episode: 108, duration: 0.401s, episode steps:  29, steps per second:  72, episode reward: -33.000, mean reward: -1.138 [-32.689,  3.498], mean action: 3.069 [0.000, 11.000],  loss: 0.019391, mae: 0.344740, mean_q: 0.608958, mean_eps: 0.000000
 2909/5000: episode: 109, duration: 0.406s, episode steps:  29, steps per second:  71, episode reward: 38.381, mean reward:  1.323 [-2.725, 32.020], mean action: 4.276 [0.000, 14.000],  loss: 0.020622, mae: 0.357700, mean_q: 0.571299, mean_eps: 0.000000
 2935/5000: episode: 110, duration: 0.367s, episode steps:  26, steps per second:  71, episode reward: 45.000, mean reward:  1.731 [-2.058, 32.040], mean action: 3.923 [2.000, 15.000],  loss: 0.020199, mae: 0.351770, mean_q: 0.559839, mean_eps: 0.000000
 2962/5000: episode: 111, duration: 0.382s, episode steps:  27, steps per second:  71, episode reward: 41.005, mean reward:  1.519 [-2.164, 32.380], mean action: 3.444 [0.000, 16.000],  loss: 0.018842, mae: 0.353493, mean_q: 0.533063, mean_eps: 0.000000
 2990/5000: episode: 112, duration: 0.460s, episode steps:  28, steps per second:  61, episode reward: 38.418, mean reward:  1.372 [-3.000, 32.610], mean action: 3.286 [0.000, 16.000],  loss: 0.021961, mae: 0.367385, mean_q: 0.530760, mean_eps: 0.000000
 3009/5000: episode: 113, duration: 0.274s, episode steps:  19, steps per second:  69, episode reward: 48.000, mean reward:  2.526 [ 0.070, 32.670], mean action: 1.368 [0.000, 3.000],  loss: 0.020311, mae: 0.358371, mean_q: 0.538078, mean_eps: 0.000000
 3027/5000: episode: 114, duration: 0.260s, episode steps:  18, steps per second:  69, episode reward: 41.068, mean reward:  2.282 [-2.441, 31.599], mean action: 2.556 [0.000, 12.000],  loss: 0.018220, mae: 0.356035, mean_q: 0.487943, mean_eps: 0.000000
 3047/5000: episode: 115, duration: 0.533s, episode steps:  20, steps per second:  38, episode reward: 44.539, mean reward:  2.227 [-0.121, 30.584], mean action: 6.700 [0.000, 14.000],  loss: 0.021069, mae: 0.368116, mean_q: 0.457108, mean_eps: 0.000000
 3073/5000: episode: 116, duration: 0.387s, episode steps:  26, steps per second:  67, episode reward: 38.231, mean reward:  1.470 [-3.000, 32.261], mean action: 4.538 [0.000, 15.000],  loss: 0.024475, mae: 0.380156, mean_q: 0.487528, mean_eps: 0.000000
 3097/5000: episode: 117, duration: 0.338s, episode steps:  24, steps per second:  71, episode reward: 40.138, mean reward:  1.672 [-2.051, 32.253], mean action: 5.125 [0.000, 14.000],  loss: 0.018185, mae: 0.358631, mean_q: 0.511557, mean_eps: 0.000000
 3114/5000: episode: 118, duration: 0.252s, episode steps:  17, steps per second:  68, episode reward: 47.090, mean reward:  2.770 [-0.060, 32.040], mean action: 3.294 [0.000, 14.000],  loss: 0.020287, mae: 0.367719, mean_q: 0.492042, mean_eps: 0.000000
 3148/5000: episode: 119, duration: 0.472s, episode steps:  34, steps per second:  72, episode reward: 38.596, mean reward:  1.135 [-2.284, 32.056], mean action: 3.529 [0.000, 16.000],  loss: 0.020628, mae: 0.361432, mean_q: 0.560698, mean_eps: 0.000000
 3159/5000: episode: 120, duration: 0.169s, episode steps:  11, steps per second:  65, episode reward: 45.000, mean reward:  4.091 [-2.144, 33.000], mean action: 3.455 [1.000, 16.000],  loss: 0.019310, mae: 0.351182, mean_q: 0.570529, mean_eps: 0.000000
 3179/5000: episode: 121, duration: 0.286s, episode steps:  20, steps per second:  70, episode reward: 41.661, mean reward:  2.083 [-2.712, 32.296], mean action: 7.350 [1.000, 16.000],  loss: 0.016501, mae: 0.336992, mean_q: 0.566288, mean_eps: 0.000000
 3206/5000: episode: 122, duration: 0.380s, episode steps:  27, steps per second:  71, episode reward: 41.685, mean reward:  1.544 [-2.288, 32.150], mean action: 3.407 [0.000, 20.000],  loss: 0.022267, mae: 0.363423, mean_q: 0.542842, mean_eps: 0.000000
 3227/5000: episode: 123, duration: 0.307s, episode steps:  21, steps per second:  68, episode reward: 38.005, mean reward:  1.810 [-3.000, 32.353], mean action: 3.333 [0.000, 16.000],  loss: 0.023065, mae: 0.374882, mean_q: 0.489587, mean_eps: 0.000000
 3267/5000: episode: 124, duration: 0.556s, episode steps:  40, steps per second:  72, episode reward: 39.936, mean reward:  0.998 [-2.865, 32.010], mean action: 3.250 [0.000, 14.000],  loss: 0.020961, mae: 0.359997, mean_q: 0.501129, mean_eps: 0.000000
 3303/5000: episode: 125, duration: 0.500s, episode steps:  36, steps per second:  72, episode reward: 41.509, mean reward:  1.153 [-2.812, 32.100], mean action: 3.861 [0.000, 19.000],  loss: 0.019030, mae: 0.354334, mean_q: 0.521418, mean_eps: 0.000000
 3326/5000: episode: 126, duration: 0.328s, episode steps:  23, steps per second:  70, episode reward: 44.506, mean reward:  1.935 [-2.240, 32.220], mean action: 1.696 [0.000, 9.000],  loss: 0.018646, mae: 0.355041, mean_q: 0.506130, mean_eps: 0.000000
 3353/5000: episode: 127, duration: 0.377s, episode steps:  27, steps per second:  72, episode reward: 35.798, mean reward:  1.326 [-3.000, 31.918], mean action: 4.593 [0.000, 14.000],  loss: 0.021422, mae: 0.366906, mean_q: 0.549638, mean_eps: 0.000000
 3365/5000: episode: 128, duration: 0.184s, episode steps:  12, steps per second:  65, episode reward: 47.152, mean reward:  3.929 [ 0.000, 32.070], mean action: 2.667 [2.000, 3.000],  loss: 0.018951, mae: 0.363307, mean_q: 0.511154, mean_eps: 0.000000
 3379/5000: episode: 129, duration: 0.210s, episode steps:  14, steps per second:  67, episode reward: 44.512, mean reward:  3.179 [-2.349, 32.383], mean action: 5.143 [1.000, 19.000],  loss: 0.020534, mae: 0.369820, mean_q: 0.544430, mean_eps: 0.000000
 3399/5000: episode: 130, duration: 0.292s, episode steps:  20, steps per second:  68, episode reward: 44.055, mean reward:  2.203 [-2.941, 32.434], mean action: 2.800 [0.000, 14.000],  loss: 0.018798, mae: 0.354806, mean_q: 0.538111, mean_eps: 0.000000
 3419/5000: episode: 131, duration: 0.295s, episode steps:  20, steps per second:  68, episode reward: 41.903, mean reward:  2.095 [-2.566, 31.923], mean action: 3.300 [0.000, 19.000],  loss: 0.018412, mae: 0.353404, mean_q: 0.500559, mean_eps: 0.000000
 3456/5000: episode: 132, duration: 0.518s, episode steps:  37, steps per second:  71, episode reward: 44.880, mean reward:  1.213 [-3.000, 32.100], mean action: 4.162 [1.000, 19.000],  loss: 0.019463, mae: 0.356691, mean_q: 0.468561, mean_eps: 0.000000
 3482/5000: episode: 133, duration: 0.373s, episode steps:  26, steps per second:  70, episode reward: 38.893, mean reward:  1.496 [-3.000, 32.450], mean action: 2.769 [0.000, 15.000],  loss: 0.021605, mae: 0.357600, mean_q: 0.508297, mean_eps: 0.000000
 3510/5000: episode: 134, duration: 0.419s, episode steps:  28, steps per second:  67, episode reward: 35.120, mean reward:  1.254 [-3.000, 32.603], mean action: 3.429 [0.000, 15.000],  loss: 0.019771, mae: 0.352482, mean_q: 0.487672, mean_eps: 0.000000
 3540/5000: episode: 135, duration: 0.435s, episode steps:  30, steps per second:  69, episode reward: 44.336, mean reward:  1.478 [-2.108, 32.030], mean action: 2.367 [0.000, 19.000],  loss: 0.020385, mae: 0.350737, mean_q: 0.492118, mean_eps: 0.000000
 3581/5000: episode: 136, duration: 0.562s, episode steps:  41, steps per second:  73, episode reward: 41.495, mean reward:  1.012 [-3.000, 32.190], mean action: 4.683 [0.000, 19.000],  loss: 0.020185, mae: 0.350466, mean_q: 0.497693, mean_eps: 0.000000
 3597/5000: episode: 137, duration: 0.241s, episode steps:  16, steps per second:  66, episode reward: 41.766, mean reward:  2.610 [-2.277, 32.076], mean action: 2.438 [0.000, 9.000],  loss: 0.019181, mae: 0.341417, mean_q: 0.472102, mean_eps: 0.000000
 3612/5000: episode: 138, duration: 0.220s, episode steps:  15, steps per second:  68, episode reward: 40.853, mean reward:  2.724 [-3.000, 32.272], mean action: 3.333 [0.000, 9.000],  loss: 0.019807, mae: 0.349749, mean_q: 0.437004, mean_eps: 0.000000
 3656/5000: episode: 139, duration: 0.604s, episode steps:  44, steps per second:  73, episode reward: 41.513, mean reward:  0.943 [-2.315, 32.200], mean action: 2.068 [0.000, 19.000],  loss: 0.019258, mae: 0.348742, mean_q: 0.461622, mean_eps: 0.000000
 3676/5000: episode: 140, duration: 0.292s, episode steps:  20, steps per second:  69, episode reward: 41.536, mean reward:  2.077 [-2.878, 32.310], mean action: 4.700 [0.000, 19.000],  loss: 0.018370, mae: 0.345328, mean_q: 0.540070, mean_eps: 0.000000
 3700/5000: episode: 141, duration: 0.337s, episode steps:  24, steps per second:  71, episode reward: 38.861, mean reward:  1.619 [-2.701, 32.230], mean action: 2.667 [0.000, 16.000],  loss: 0.019958, mae: 0.349797, mean_q: 0.567305, mean_eps: 0.000000
 3728/5000: episode: 142, duration: 0.400s, episode steps:  28, steps per second:  70, episode reward: 39.000, mean reward:  1.393 [-2.595, 32.600], mean action: 3.750 [0.000, 16.000],  loss: 0.021701, mae: 0.366977, mean_q: 0.492457, mean_eps: 0.000000
 3753/5000: episode: 143, duration: 0.789s, episode steps:  25, steps per second:  32, episode reward: 39.000, mean reward:  1.560 [-2.538, 32.830], mean action: 1.920 [0.000, 15.000],  loss: 0.022916, mae: 0.371373, mean_q: 0.491834, mean_eps: 0.000000
 3775/5000: episode: 144, duration: 0.549s, episode steps:  22, steps per second:  40, episode reward: 45.000, mean reward:  2.045 [-2.316, 32.130], mean action: 0.955 [0.000, 9.000],  loss: 0.021397, mae: 0.368875, mean_q: 0.494933, mean_eps: 0.000000
 3794/5000: episode: 145, duration: 0.366s, episode steps:  19, steps per second:  52, episode reward: 41.679, mean reward:  2.194 [-2.229, 32.450], mean action: 2.789 [0.000, 12.000],  loss: 0.021816, mae: 0.366286, mean_q: 0.486791, mean_eps: 0.000000
 3816/5000: episode: 146, duration: 0.508s, episode steps:  22, steps per second:  43, episode reward: 38.607, mean reward:  1.755 [-2.552, 32.090], mean action: 3.318 [0.000, 12.000],  loss: 0.019413, mae: 0.363477, mean_q: 0.534827, mean_eps: 0.000000
 3848/5000: episode: 147, duration: 0.735s, episode steps:  32, steps per second:  44, episode reward: 38.191, mean reward:  1.193 [-2.149, 32.450], mean action: 2.562 [0.000, 12.000],  loss: 0.021962, mae: 0.370475, mean_q: 0.572961, mean_eps: 0.000000
 3869/5000: episode: 148, duration: 0.319s, episode steps:  21, steps per second:  66, episode reward: 44.638, mean reward:  2.126 [-2.166, 32.790], mean action: 2.000 [0.000, 11.000],  loss: 0.019502, mae: 0.354579, mean_q: 0.552184, mean_eps: 0.000000
 3900/5000: episode: 149, duration: 0.671s, episode steps:  31, steps per second:  46, episode reward: 41.649, mean reward:  1.344 [-3.000, 32.720], mean action: 2.903 [0.000, 12.000],  loss: 0.017710, mae: 0.347987, mean_q: 0.564583, mean_eps: 0.000000
 3923/5000: episode: 150, duration: 0.389s, episode steps:  23, steps per second:  59, episode reward: 46.696, mean reward:  2.030 [-0.432, 31.852], mean action: 2.783 [2.000, 6.000],  loss: 0.018723, mae: 0.351807, mean_q: 0.587963, mean_eps: 0.000000
 3936/5000: episode: 151, duration: 0.221s, episode steps:  13, steps per second:  59, episode reward: 46.512, mean reward:  3.578 [-0.260, 31.779], mean action: 2.462 [0.000, 12.000],  loss: 0.020101, mae: 0.357486, mean_q: 0.527405, mean_eps: 0.000000
 3966/5000: episode: 152, duration: 0.937s, episode steps:  30, steps per second:  32, episode reward: 41.642, mean reward:  1.388 [-2.314, 32.320], mean action: 4.167 [0.000, 20.000],  loss: 0.019794, mae: 0.363306, mean_q: 0.510621, mean_eps: 0.000000
 3990/5000: episode: 153, duration: 0.409s, episode steps:  24, steps per second:  59, episode reward: 41.008, mean reward:  1.709 [-3.000, 33.000], mean action: 4.625 [0.000, 21.000],  loss: 0.021389, mae: 0.361376, mean_q: 0.550927, mean_eps: 0.000000
 4006/5000: episode: 154, duration: 0.280s, episode steps:  16, steps per second:  57, episode reward: 44.272, mean reward:  2.767 [-2.204, 32.339], mean action: 3.375 [0.000, 19.000],  loss: 0.018307, mae: 0.343338, mean_q: 0.516638, mean_eps: 0.000000
 4026/5000: episode: 155, duration: 0.312s, episode steps:  20, steps per second:  64, episode reward: 47.079, mean reward:  2.354 [-0.308, 32.390], mean action: 3.950 [0.000, 20.000],  loss: 0.021693, mae: 0.359507, mean_q: 0.513059, mean_eps: 0.000000
 4054/5000: episode: 156, duration: 0.394s, episode steps:  28, steps per second:  71, episode reward: 44.903, mean reward:  1.604 [-2.090, 32.253], mean action: 0.536 [0.000, 9.000],  loss: 0.020870, mae: 0.362732, mean_q: 0.485810, mean_eps: 0.000000
 4088/5000: episode: 157, duration: 0.468s, episode steps:  34, steps per second:  73, episode reward: 36.000, mean reward:  1.059 [-3.000, 32.050], mean action: 3.824 [0.000, 15.000],  loss: 0.023169, mae: 0.367029, mean_q: 0.586242, mean_eps: 0.000000
 4113/5000: episode: 158, duration: 0.357s, episode steps:  25, steps per second:  70, episode reward: 41.298, mean reward:  1.652 [-2.303, 32.490], mean action: 2.960 [0.000, 12.000],  loss: 0.019527, mae: 0.358891, mean_q: 0.609654, mean_eps: 0.000000
 4145/5000: episode: 159, duration: 0.440s, episode steps:  32, steps per second:  73, episode reward: 33.000, mean reward:  1.031 [-3.000, 32.260], mean action: 4.188 [0.000, 17.000],  loss: 0.019807, mae: 0.359018, mean_q: 0.644329, mean_eps: 0.000000
 4169/5000: episode: 160, duration: 0.342s, episode steps:  24, steps per second:  70, episode reward: 39.000, mean reward:  1.625 [-2.406, 32.530], mean action: 4.292 [0.000, 19.000],  loss: 0.020283, mae: 0.355905, mean_q: 0.610905, mean_eps: 0.000000
 4192/5000: episode: 161, duration: 0.330s, episode steps:  23, steps per second:  70, episode reward: 42.000, mean reward:  1.826 [-2.978, 32.310], mean action: 3.174 [0.000, 19.000],  loss: 0.017886, mae: 0.347953, mean_q: 0.508715, mean_eps: 0.000000
 4216/5000: episode: 162, duration: 0.353s, episode steps:  24, steps per second:  68, episode reward: 41.599, mean reward:  1.733 [-3.000, 32.016], mean action: 3.375 [0.000, 16.000],  loss: 0.018953, mae: 0.356750, mean_q: 0.535081, mean_eps: 0.000000
 4250/5000: episode: 163, duration: 0.472s, episode steps:  34, steps per second:  72, episode reward: 41.388, mean reward:  1.217 [-2.677, 32.270], mean action: 2.618 [0.000, 16.000],  loss: 0.022123, mae: 0.372769, mean_q: 0.507993, mean_eps: 0.000000
 4287/5000: episode: 164, duration: 0.515s, episode steps:  37, steps per second:  72, episode reward: 32.569, mean reward:  0.880 [-3.000, 29.792], mean action: 7.622 [0.000, 20.000],  loss: 0.016781, mae: 0.344763, mean_q: 0.480638, mean_eps: 0.000000
 4307/5000: episode: 165, duration: 0.299s, episode steps:  20, steps per second:  67, episode reward: 46.286, mean reward:  2.314 [-0.674, 33.000], mean action: 2.200 [0.000, 10.000],  loss: 0.020938, mae: 0.361410, mean_q: 0.522727, mean_eps: 0.000000
 4337/5000: episode: 166, duration: 0.430s, episode steps:  30, steps per second:  70, episode reward: 47.032, mean reward:  1.568 [-0.500, 32.098], mean action: 2.300 [0.000, 19.000],  loss: 0.016918, mae: 0.337896, mean_q: 0.507281, mean_eps: 0.000000
 4370/5000: episode: 167, duration: 0.523s, episode steps:  33, steps per second:  63, episode reward: 37.684, mean reward:  1.142 [-2.880, 32.380], mean action: 7.364 [0.000, 19.000],  loss: 0.016756, mae: 0.343787, mean_q: 0.577779, mean_eps: 0.000000
 4398/5000: episode: 168, duration: 0.401s, episode steps:  28, steps per second:  70, episode reward: 37.566, mean reward:  1.342 [-3.000, 32.034], mean action: 4.429 [0.000, 16.000],  loss: 0.019799, mae: 0.352244, mean_q: 0.555702, mean_eps: 0.000000
 4418/5000: episode: 169, duration: 0.292s, episode steps:  20, steps per second:  68, episode reward: 41.514, mean reward:  2.076 [-2.591, 32.181], mean action: 5.800 [2.000, 19.000],  loss: 0.022843, mae: 0.366530, mean_q: 0.516040, mean_eps: 0.000000
 4451/5000: episode: 170, duration: 0.472s, episode steps:  33, steps per second:  70, episode reward: 41.179, mean reward:  1.248 [-2.361, 32.113], mean action: 3.333 [0.000, 19.000],  loss: 0.022287, mae: 0.362194, mean_q: 0.573431, mean_eps: 0.000000
 4475/5000: episode: 171, duration: 0.337s, episode steps:  24, steps per second:  71, episode reward: 41.806, mean reward:  1.742 [-2.557, 32.113], mean action: 2.333 [0.000, 15.000],  loss: 0.020434, mae: 0.360446, mean_q: 0.680946, mean_eps: 0.000000
 4497/5000: episode: 172, duration: 0.322s, episode steps:  22, steps per second:  68, episode reward: 41.682, mean reward:  1.895 [-2.511, 32.230], mean action: 2.864 [0.000, 12.000],  loss: 0.018196, mae: 0.347609, mean_q: 0.659924, mean_eps: 0.000000
 4515/5000: episode: 173, duration: 0.284s, episode steps:  18, steps per second:  63, episode reward: 43.741, mean reward:  2.430 [-2.416, 32.130], mean action: 3.111 [0.000, 12.000],  loss: 0.018430, mae: 0.352074, mean_q: 0.636206, mean_eps: 0.000000
 4544/5000: episode: 174, duration: 0.411s, episode steps:  29, steps per second:  71, episode reward: 38.676, mean reward:  1.334 [-2.861, 32.108], mean action: 3.276 [0.000, 15.000],  loss: 0.017887, mae: 0.342132, mean_q: 0.572730, mean_eps: 0.000000
 4563/5000: episode: 175, duration: 0.280s, episode steps:  19, steps per second:  68, episode reward: 41.278, mean reward:  2.173 [-2.127, 32.188], mean action: 2.368 [0.000, 12.000],  loss: 0.018527, mae: 0.341874, mean_q: 0.517884, mean_eps: 0.000000
 4584/5000: episode: 176, duration: 0.314s, episode steps:  21, steps per second:  67, episode reward: 44.548, mean reward:  2.121 [-2.365, 32.050], mean action: 2.667 [0.000, 12.000],  loss: 0.019410, mae: 0.349918, mean_q: 0.476717, mean_eps: 0.000000
 4603/5000: episode: 177, duration: 0.282s, episode steps:  19, steps per second:  67, episode reward: 39.940, mean reward:  2.102 [-2.604, 32.903], mean action: 3.579 [0.000, 12.000],  loss: 0.016936, mae: 0.333142, mean_q: 0.511831, mean_eps: 0.000000
 4628/5000: episode: 178, duration: 0.361s, episode steps:  25, steps per second:  69, episode reward: 41.842, mean reward:  1.674 [-3.000, 29.793], mean action: 3.040 [0.000, 12.000],  loss: 0.017386, mae: 0.333261, mean_q: 0.530834, mean_eps: 0.000000
 4646/5000: episode: 179, duration: 0.264s, episode steps:  18, steps per second:  68, episode reward: 42.000, mean reward:  2.333 [-2.479, 32.280], mean action: 3.000 [0.000, 14.000],  loss: 0.015870, mae: 0.338925, mean_q: 0.524341, mean_eps: 0.000000
 4690/5000: episode: 180, duration: 0.856s, episode steps:  44, steps per second:  51, episode reward: 33.000, mean reward:  0.750 [-2.449, 32.990], mean action: 5.364 [0.000, 15.000],  loss: 0.021441, mae: 0.360341, mean_q: 0.556541, mean_eps: 0.000000
 4712/5000: episode: 181, duration: 0.372s, episode steps:  22, steps per second:  59, episode reward: 44.743, mean reward:  2.034 [-2.204, 32.419], mean action: 2.227 [0.000, 15.000],  loss: 0.021769, mae: 0.363980, mean_q: 0.511549, mean_eps: 0.000000
 4730/5000: episode: 182, duration: 0.458s, episode steps:  18, steps per second:  39, episode reward: 41.751, mean reward:  2.319 [-3.000, 32.110], mean action: 3.667 [0.000, 15.000],  loss: 0.019962, mae: 0.366841, mean_q: 0.457900, mean_eps: 0.000000
 4759/5000: episode: 183, duration: 0.690s, episode steps:  29, steps per second:  42, episode reward: 38.627, mean reward:  1.332 [-2.680, 32.016], mean action: 2.379 [0.000, 15.000],  loss: 0.021385, mae: 0.364799, mean_q: 0.531839, mean_eps: 0.000000
 4796/5000: episode: 184, duration: 0.526s, episode steps:  37, steps per second:  70, episode reward: 41.113, mean reward:  1.111 [-2.201, 32.470], mean action: 3.595 [0.000, 14.000],  loss: 0.020923, mae: 0.359213, mean_q: 0.589566, mean_eps: 0.000000
 4823/5000: episode: 185, duration: 0.382s, episode steps:  27, steps per second:  71, episode reward: 38.435, mean reward:  1.424 [-2.558, 32.382], mean action: 3.111 [0.000, 12.000],  loss: 0.021455, mae: 0.372072, mean_q: 0.576294, mean_eps: 0.000000
 4841/5000: episode: 186, duration: 0.299s, episode steps:  18, steps per second:  60, episode reward: 44.345, mean reward:  2.464 [-2.357, 32.410], mean action: 2.944 [0.000, 12.000],  loss: 0.025739, mae: 0.386922, mean_q: 0.596369, mean_eps: 0.000000
 4871/5000: episode: 187, duration: 0.651s, episode steps:  30, steps per second:  46, episode reward: 38.511, mean reward:  1.284 [-2.240, 31.936], mean action: 4.067 [0.000, 14.000],  loss: 0.018738, mae: 0.352676, mean_q: 0.537965, mean_eps: 0.000000
 4914/5000: episode: 188, duration: 0.722s, episode steps:  43, steps per second:  60, episode reward: 38.700, mean reward:  0.900 [-2.373, 32.150], mean action: 1.721 [0.000, 19.000],  loss: 0.020512, mae: 0.362701, mean_q: 0.492274, mean_eps: 0.000000
 4951/5000: episode: 189, duration: 0.627s, episode steps:  37, steps per second:  59, episode reward: 37.471, mean reward:  1.013 [-2.486, 31.727], mean action: 5.946 [0.000, 15.000],  loss: 0.019411, mae: 0.351635, mean_q: 0.531607, mean_eps: 0.000000
 4983/5000: episode: 190, duration: 0.490s, episode steps:  32, steps per second:  65, episode reward: 40.920, mean reward:  1.279 [-2.320, 32.173], mean action: 3.625 [0.000, 14.000],  loss: 0.021573, mae: 0.368242, mean_q: 0.454688, mean_eps: 0.000000
done, took 69.177 seconds
DQN Evaluation: 6310 victories out of 7416 episodes
Training for 5000 steps ...
   29/5000: episode: 1, duration: 0.218s, episode steps:  29, steps per second: 133, episode reward: 33.000, mean reward:  1.138 [-3.000, 32.220], mean action: 6.138 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   55/5000: episode: 2, duration: 0.181s, episode steps:  26, steps per second: 143, episode reward: 35.284, mean reward:  1.357 [-3.000, 32.227], mean action: 5.500 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   85/5000: episode: 3, duration: 0.187s, episode steps:  30, steps per second: 160, episode reward: -36.000, mean reward: -1.200 [-32.088,  2.802], mean action: 4.500 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  105/5000: episode: 4, duration: 0.143s, episode steps:  20, steps per second: 140, episode reward: 41.120, mean reward:  2.056 [-2.498, 32.010], mean action: 3.000 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  137/5000: episode: 5, duration: 0.219s, episode steps:  32, steps per second: 146, episode reward: -35.010, mean reward: -1.094 [-31.247,  2.250], mean action: 3.781 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  156/5000: episode: 6, duration: 0.130s, episode steps:  19, steps per second: 146, episode reward: 42.000, mean reward:  2.211 [-3.000, 32.180], mean action: 1.947 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  181/5000: episode: 7, duration: 0.179s, episode steps:  25, steps per second: 140, episode reward: 32.796, mean reward:  1.312 [-3.000, 32.136], mean action: 4.280 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  202/5000: episode: 8, duration: 0.148s, episode steps:  21, steps per second: 142, episode reward: 38.522, mean reward:  1.834 [-2.368, 32.130], mean action: 2.429 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  241/5000: episode: 9, duration: 0.241s, episode steps:  39, steps per second: 162, episode reward: 32.778, mean reward:  0.840 [-3.000, 32.120], mean action: 9.231 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  269/5000: episode: 10, duration: 0.222s, episode steps:  28, steps per second: 126, episode reward: -38.730, mean reward: -1.383 [-32.255,  2.270], mean action: 5.607 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  293/5000: episode: 11, duration: 0.160s, episode steps:  24, steps per second: 150, episode reward: 35.249, mean reward:  1.469 [-3.000, 33.000], mean action: 5.833 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  319/5000: episode: 12, duration: 0.177s, episode steps:  26, steps per second: 147, episode reward: 44.063, mean reward:  1.695 [-2.137, 32.120], mean action: 3.346 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  335/5000: episode: 13, duration: 0.121s, episode steps:  16, steps per second: 132, episode reward: 43.502, mean reward:  2.719 [-2.105, 33.000], mean action: 2.875 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  380/5000: episode: 14, duration: 0.275s, episode steps:  45, steps per second: 164, episode reward: 35.987, mean reward:  0.800 [-2.389, 32.090], mean action: 9.556 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  402/5000: episode: 15, duration: 0.149s, episode steps:  22, steps per second: 148, episode reward: 33.000, mean reward:  1.500 [-3.000, 29.916], mean action: 4.409 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  435/5000: episode: 16, duration: 0.222s, episode steps:  33, steps per second: 149, episode reward: 37.645, mean reward:  1.141 [-3.000, 31.904], mean action: 4.848 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  459/5000: episode: 17, duration: 0.169s, episode steps:  24, steps per second: 142, episode reward: 39.000, mean reward:  1.625 [-2.450, 32.510], mean action: 2.625 [1.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  475/5000: episode: 18, duration: 0.113s, episode steps:  16, steps per second: 142, episode reward: -35.480, mean reward: -2.218 [-30.900,  2.848], mean action: 7.062 [0.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  502/5000: episode: 19, duration: 0.169s, episode steps:  27, steps per second: 160, episode reward: 39.000, mean reward:  1.444 [-2.224, 32.130], mean action: 7.000 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  521/5000: episode: 20, duration: 0.138s, episode steps:  19, steps per second: 138, episode reward: 40.876, mean reward:  2.151 [-2.336, 31.910], mean action: 4.684 [1.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  550/5000: episode: 21, duration: 0.189s, episode steps:  29, steps per second: 153, episode reward: 38.070, mean reward:  1.313 [-2.140, 32.904], mean action: 3.828 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  563/5000: episode: 22, duration: 0.094s, episode steps:  13, steps per second: 139, episode reward: 44.313, mean reward:  3.409 [-2.142, 32.131], mean action: 2.538 [1.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  581/5000: episode: 23, duration: 0.270s, episode steps:  18, steps per second:  67, episode reward: 39.000, mean reward:  2.167 [-2.802, 32.570], mean action: 4.500 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  601/5000: episode: 24, duration: 0.248s, episode steps:  20, steps per second:  81, episode reward: -36.000, mean reward: -1.800 [-32.289,  2.855], mean action: 7.100 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  629/5000: episode: 25, duration: 0.208s, episode steps:  28, steps per second: 135, episode reward: 34.967, mean reward:  1.249 [-3.000, 32.176], mean action: 6.107 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  651/5000: episode: 26, duration: 0.159s, episode steps:  22, steps per second: 138, episode reward: 37.759, mean reward:  1.716 [-3.000, 32.262], mean action: 3.273 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  671/5000: episode: 27, duration: 0.141s, episode steps:  20, steps per second: 141, episode reward: 35.386, mean reward:  1.769 [-3.000, 33.000], mean action: 3.050 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  691/5000: episode: 28, duration: 0.142s, episode steps:  20, steps per second: 141, episode reward: -38.660, mean reward: -1.933 [-32.200,  2.352], mean action: 7.750 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  710/5000: episode: 29, duration: 0.183s, episode steps:  19, steps per second: 104, episode reward: -38.210, mean reward: -2.011 [-31.540,  2.860], mean action: 6.789 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  730/5000: episode: 30, duration: 0.181s, episode steps:  20, steps per second: 110, episode reward: 41.732, mean reward:  2.087 [-2.438, 32.220], mean action: 2.700 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  755/5000: episode: 31, duration: 0.547s, episode steps:  25, steps per second:  46, episode reward: 33.000, mean reward:  1.320 [-2.461, 29.971], mean action: 5.240 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  787/5000: episode: 32, duration: 0.434s, episode steps:  32, steps per second:  74, episode reward: 32.729, mean reward:  1.023 [-2.565, 32.120], mean action: 6.281 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  805/5000: episode: 33, duration: 0.133s, episode steps:  18, steps per second: 136, episode reward: 38.854, mean reward:  2.159 [-2.300, 33.000], mean action: 4.444 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  821/5000: episode: 34, duration: 0.119s, episode steps:  16, steps per second: 135, episode reward: 41.412, mean reward:  2.588 [-2.151, 32.420], mean action: 3.188 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  849/5000: episode: 35, duration: 0.181s, episode steps:  28, steps per second: 155, episode reward: 35.661, mean reward:  1.274 [-3.000, 32.061], mean action: 3.250 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  868/5000: episode: 36, duration: 0.129s, episode steps:  19, steps per second: 147, episode reward: 35.902, mean reward:  1.890 [-3.000, 32.422], mean action: 3.105 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  886/5000: episode: 37, duration: 0.118s, episode steps:  18, steps per second: 153, episode reward: 35.567, mean reward:  1.976 [-2.801, 32.260], mean action: 4.167 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  914/5000: episode: 38, duration: 0.174s, episode steps:  28, steps per second: 161, episode reward: -30.000, mean reward: -1.071 [-30.740,  3.000], mean action: 4.464 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  943/5000: episode: 39, duration: 0.219s, episode steps:  29, steps per second: 132, episode reward: 33.000, mean reward:  1.138 [-3.000, 32.230], mean action: 2.759 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  959/5000: episode: 40, duration: 0.118s, episode steps:  16, steps per second: 135, episode reward: 38.694, mean reward:  2.418 [-3.000, 32.214], mean action: 4.000 [1.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  981/5000: episode: 41, duration: 0.145s, episode steps:  22, steps per second: 152, episode reward: 32.814, mean reward:  1.492 [-2.425, 29.922], mean action: 4.136 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1010/5000: episode: 42, duration: 0.262s, episode steps:  29, steps per second: 111, episode reward: 32.136, mean reward:  1.108 [-3.000, 32.300], mean action: 7.552 [0.000, 14.000],  loss: 0.019899, mae: 0.359925, mean_q: 0.517439, mean_eps: 0.000000
 1029/5000: episode: 43, duration: 0.300s, episode steps:  19, steps per second:  63, episode reward: -32.420, mean reward: -1.706 [-31.603,  2.205], mean action: 5.526 [0.000, 15.000],  loss: 0.021938, mae: 0.369466, mean_q: 0.502109, mean_eps: 0.000000
 1051/5000: episode: 44, duration: 0.594s, episode steps:  22, steps per second:  37, episode reward: 38.249, mean reward:  1.739 [-2.447, 33.000], mean action: 3.591 [0.000, 15.000],  loss: 0.021313, mae: 0.360502, mean_q: 0.538063, mean_eps: 0.000000
 1069/5000: episode: 45, duration: 0.454s, episode steps:  18, steps per second:  40, episode reward: 38.687, mean reward:  2.149 [-2.471, 32.237], mean action: 4.167 [0.000, 19.000],  loss: 0.019167, mae: 0.357655, mean_q: 0.512548, mean_eps: 0.000000
 1101/5000: episode: 46, duration: 0.754s, episode steps:  32, steps per second:  42, episode reward: 33.000, mean reward:  1.031 [-3.000, 30.376], mean action: 4.312 [0.000, 19.000],  loss: 0.023629, mae: 0.379203, mean_q: 0.497912, mean_eps: 0.000000
 1124/5000: episode: 47, duration: 0.512s, episode steps:  23, steps per second:  45, episode reward: 35.649, mean reward:  1.550 [-3.000, 32.087], mean action: 8.087 [0.000, 14.000],  loss: 0.020596, mae: 0.362818, mean_q: 0.528683, mean_eps: 0.000000
 1134/5000: episode: 48, duration: 0.175s, episode steps:  10, steps per second:  57, episode reward: 45.000, mean reward:  4.500 [-2.071, 33.000], mean action: 2.100 [0.000, 12.000],  loss: 0.015961, mae: 0.347832, mean_q: 0.588614, mean_eps: 0.000000
 1151/5000: episode: 49, duration: 0.352s, episode steps:  17, steps per second:  48, episode reward: 38.900, mean reward:  2.288 [-2.556, 32.540], mean action: 2.941 [0.000, 12.000],  loss: 0.026638, mae: 0.389018, mean_q: 0.525045, mean_eps: 0.000000
 1175/5000: episode: 50, duration: 0.653s, episode steps:  24, steps per second:  37, episode reward: 32.174, mean reward:  1.341 [-2.386, 31.601], mean action: 6.333 [0.000, 14.000],  loss: 0.019525, mae: 0.357426, mean_q: 0.481587, mean_eps: 0.000000
 1204/5000: episode: 51, duration: 0.423s, episode steps:  29, steps per second:  69, episode reward: -32.590, mean reward: -1.124 [-32.207,  2.551], mean action: 7.103 [0.000, 17.000],  loss: 0.023201, mae: 0.378015, mean_q: 0.498125, mean_eps: 0.000000
 1237/5000: episode: 52, duration: 0.466s, episode steps:  33, steps per second:  71, episode reward: -33.000, mean reward: -1.000 [-32.465,  3.000], mean action: 4.303 [0.000, 19.000],  loss: 0.021051, mae: 0.363383, mean_q: 0.571580, mean_eps: 0.000000
 1260/5000: episode: 53, duration: 0.375s, episode steps:  23, steps per second:  61, episode reward: 35.335, mean reward:  1.536 [-2.661, 32.245], mean action: 3.130 [0.000, 19.000],  loss: 0.016897, mae: 0.341179, mean_q: 0.598137, mean_eps: 0.000000
 1290/5000: episode: 54, duration: 0.418s, episode steps:  30, steps per second:  72, episode reward: 37.825, mean reward:  1.261 [-2.697, 32.230], mean action: 5.800 [0.000, 19.000],  loss: 0.018348, mae: 0.346437, mean_q: 0.561958, mean_eps: 0.000000
 1314/5000: episode: 55, duration: 0.332s, episode steps:  24, steps per second:  72, episode reward: -32.890, mean reward: -1.370 [-32.275,  2.220], mean action: 5.417 [0.000, 20.000],  loss: 0.017483, mae: 0.342313, mean_q: 0.554390, mean_eps: 0.000000
 1348/5000: episode: 56, duration: 0.475s, episode steps:  34, steps per second:  72, episode reward: -32.870, mean reward: -0.967 [-32.421,  2.730], mean action: 8.706 [0.000, 19.000],  loss: 0.020080, mae: 0.360638, mean_q: 0.558186, mean_eps: 0.000000
 1369/5000: episode: 57, duration: 0.298s, episode steps:  21, steps per second:  70, episode reward: 33.000, mean reward:  1.571 [-2.595, 30.979], mean action: 4.952 [0.000, 19.000],  loss: 0.019818, mae: 0.356868, mean_q: 0.547240, mean_eps: 0.000000
 1402/5000: episode: 58, duration: 0.465s, episode steps:  33, steps per second:  71, episode reward: 38.155, mean reward:  1.156 [-2.323, 32.305], mean action: 2.939 [1.000, 14.000],  loss: 0.020087, mae: 0.362802, mean_q: 0.596740, mean_eps: 0.000000
 1422/5000: episode: 59, duration: 0.284s, episode steps:  20, steps per second:  70, episode reward: -35.300, mean reward: -1.765 [-32.110,  3.000], mean action: 4.050 [0.000, 12.000],  loss: 0.016813, mae: 0.347967, mean_q: 0.557104, mean_eps: 0.000000
 1453/5000: episode: 60, duration: 0.444s, episode steps:  31, steps per second:  70, episode reward: -39.000, mean reward: -1.258 [-33.000,  2.280], mean action: 4.710 [0.000, 15.000],  loss: 0.019285, mae: 0.359838, mean_q: 0.533546, mean_eps: 0.000000
 1490/5000: episode: 61, duration: 0.539s, episode steps:  37, steps per second:  69, episode reward: -32.710, mean reward: -0.884 [-32.203,  2.755], mean action: 6.811 [0.000, 19.000],  loss: 0.022159, mae: 0.371562, mean_q: 0.552801, mean_eps: 0.000000
 1514/5000: episode: 62, duration: 0.389s, episode steps:  24, steps per second:  62, episode reward: 36.000, mean reward:  1.500 [-2.531, 32.450], mean action: 4.458 [0.000, 19.000],  loss: 0.021167, mae: 0.374821, mean_q: 0.538538, mean_eps: 0.000000
 1534/5000: episode: 63, duration: 0.591s, episode steps:  20, steps per second:  34, episode reward: -38.300, mean reward: -1.915 [-32.660,  3.000], mean action: 4.600 [0.000, 19.000],  loss: 0.022115, mae: 0.376494, mean_q: 0.480066, mean_eps: 0.000000
 1556/5000: episode: 64, duration: 0.414s, episode steps:  22, steps per second:  53, episode reward: 35.203, mean reward:  1.600 [-3.000, 32.093], mean action: 4.682 [0.000, 20.000],  loss: 0.015662, mae: 0.341684, mean_q: 0.533690, mean_eps: 0.000000
 1581/5000: episode: 65, duration: 0.357s, episode steps:  25, steps per second:  70, episode reward: 40.987, mean reward:  1.639 [-2.313, 32.024], mean action: 6.440 [0.000, 21.000],  loss: 0.019517, mae: 0.361166, mean_q: 0.536440, mean_eps: 0.000000
 1605/5000: episode: 66, duration: 0.356s, episode steps:  24, steps per second:  67, episode reward: 41.091, mean reward:  1.712 [-2.298, 32.026], mean action: 2.667 [0.000, 9.000],  loss: 0.016686, mae: 0.346192, mean_q: 0.562045, mean_eps: 0.000000
 1632/5000: episode: 67, duration: 0.380s, episode steps:  27, steps per second:  71, episode reward: 32.523, mean reward:  1.205 [-2.481, 30.776], mean action: 3.741 [0.000, 16.000],  loss: 0.022736, mae: 0.377433, mean_q: 0.619159, mean_eps: 0.000000
 1657/5000: episode: 68, duration: 0.350s, episode steps:  25, steps per second:  71, episode reward: -36.000, mean reward: -1.440 [-32.029,  2.260], mean action: 3.920 [0.000, 16.000],  loss: 0.015847, mae: 0.344297, mean_q: 0.614361, mean_eps: 0.000000
 1690/5000: episode: 69, duration: 0.466s, episode steps:  33, steps per second:  71, episode reward: 35.490, mean reward:  1.075 [-2.487, 32.159], mean action: 6.182 [0.000, 21.000],  loss: 0.022558, mae: 0.389496, mean_q: 0.504055, mean_eps: 0.000000
 1710/5000: episode: 70, duration: 0.305s, episode steps:  20, steps per second:  66, episode reward: 38.495, mean reward:  1.925 [-2.703, 32.220], mean action: 3.700 [0.000, 12.000],  loss: 0.019842, mae: 0.365247, mean_q: 0.517054, mean_eps: 0.000000
 1735/5000: episode: 71, duration: 0.467s, episode steps:  25, steps per second:  53, episode reward: -33.000, mean reward: -1.320 [-30.109,  2.472], mean action: 3.400 [0.000, 19.000],  loss: 0.024447, mae: 0.381948, mean_q: 0.512011, mean_eps: 0.000000
 1755/5000: episode: 72, duration: 0.319s, episode steps:  20, steps per second:  63, episode reward: 39.000, mean reward:  1.950 [-3.000, 32.060], mean action: 3.800 [0.000, 19.000],  loss: 0.020520, mae: 0.361369, mean_q: 0.541326, mean_eps: 0.000000
 1780/5000: episode: 73, duration: 0.361s, episode steps:  25, steps per second:  69, episode reward: 38.581, mean reward:  1.543 [-2.406, 32.050], mean action: 3.680 [0.000, 19.000],  loss: 0.020561, mae: 0.356485, mean_q: 0.559435, mean_eps: 0.000000
 1808/5000: episode: 74, duration: 0.408s, episode steps:  28, steps per second:  69, episode reward: 44.262, mean reward:  1.581 [-2.453, 32.790], mean action: 4.250 [0.000, 14.000],  loss: 0.019801, mae: 0.353537, mean_q: 0.531966, mean_eps: 0.000000
 1824/5000: episode: 75, duration: 0.237s, episode steps:  16, steps per second:  67, episode reward: 38.953, mean reward:  2.435 [-3.000, 32.403], mean action: 4.562 [0.000, 19.000],  loss: 0.018536, mae: 0.350899, mean_q: 0.467585, mean_eps: 0.000000
 1852/5000: episode: 76, duration: 0.396s, episode steps:  28, steps per second:  71, episode reward: -32.810, mean reward: -1.172 [-32.045,  2.350], mean action: 4.821 [0.000, 16.000],  loss: 0.021449, mae: 0.361468, mean_q: 0.462369, mean_eps: 0.000000
 1869/5000: episode: 77, duration: 0.247s, episode steps:  17, steps per second:  69, episode reward: 38.478, mean reward:  2.263 [-2.420, 31.955], mean action: 3.588 [0.000, 16.000],  loss: 0.017440, mae: 0.341180, mean_q: 0.474351, mean_eps: 0.000000
 1888/5000: episode: 78, duration: 0.288s, episode steps:  19, steps per second:  66, episode reward: 41.031, mean reward:  2.160 [-2.187, 32.250], mean action: 2.158 [0.000, 16.000],  loss: 0.018893, mae: 0.350003, mean_q: 0.516481, mean_eps: 0.000000
 1912/5000: episode: 79, duration: 0.335s, episode steps:  24, steps per second:  72, episode reward: -32.960, mean reward: -1.373 [-32.384,  2.673], mean action: 6.375 [0.000, 19.000],  loss: 0.022531, mae: 0.369057, mean_q: 0.521591, mean_eps: 0.000000
 1941/5000: episode: 80, duration: 0.405s, episode steps:  29, steps per second:  72, episode reward: 32.571, mean reward:  1.123 [-2.467, 32.250], mean action: 6.862 [0.000, 15.000],  loss: 0.020991, mae: 0.366557, mean_q: 0.524133, mean_eps: 0.000000
 1957/5000: episode: 81, duration: 0.238s, episode steps:  16, steps per second:  67, episode reward: 37.906, mean reward:  2.369 [-3.000, 32.160], mean action: 2.688 [0.000, 9.000],  loss: 0.020433, mae: 0.364325, mean_q: 0.504575, mean_eps: 0.000000
 1983/5000: episode: 82, duration: 0.362s, episode steps:  26, steps per second:  72, episode reward: -35.050, mean reward: -1.348 [-32.091,  3.000], mean action: 2.808 [0.000, 12.000],  loss: 0.019370, mae: 0.367823, mean_q: 0.494295, mean_eps: 0.000000
 2002/5000: episode: 83, duration: 0.270s, episode steps:  19, steps per second:  70, episode reward: 38.512, mean reward:  2.027 [-2.618, 33.000], mean action: 3.105 [0.000, 19.000],  loss: 0.021290, mae: 0.374370, mean_q: 0.539472, mean_eps: 0.000000
 2021/5000: episode: 84, duration: 0.274s, episode steps:  19, steps per second:  69, episode reward: 38.234, mean reward:  2.012 [-2.298, 32.340], mean action: 4.526 [0.000, 19.000],  loss: 0.024165, mae: 0.400346, mean_q: 0.496015, mean_eps: 0.000000
 2064/5000: episode: 85, duration: 0.581s, episode steps:  43, steps per second:  74, episode reward: 32.601, mean reward:  0.758 [-2.474, 32.290], mean action: 3.000 [0.000, 19.000],  loss: 0.019600, mae: 0.371515, mean_q: 0.502396, mean_eps: 0.000000
 2082/5000: episode: 86, duration: 0.259s, episode steps:  18, steps per second:  70, episode reward: 38.088, mean reward:  2.116 [-3.000, 32.719], mean action: 3.500 [0.000, 19.000],  loss: 0.020833, mae: 0.365076, mean_q: 0.550883, mean_eps: 0.000000
 2106/5000: episode: 87, duration: 0.336s, episode steps:  24, steps per second:  71, episode reward: 38.538, mean reward:  1.606 [-3.000, 32.335], mean action: 3.250 [0.000, 19.000],  loss: 0.022703, mae: 0.372990, mean_q: 0.520130, mean_eps: 0.000000
 2127/5000: episode: 88, duration: 0.298s, episode steps:  21, steps per second:  70, episode reward: 33.000, mean reward:  1.571 [-2.620, 32.360], mean action: 5.619 [0.000, 19.000],  loss: 0.017405, mae: 0.355468, mean_q: 0.514576, mean_eps: 0.000000
 2150/5000: episode: 89, duration: 0.331s, episode steps:  23, steps per second:  69, episode reward: 32.498, mean reward:  1.413 [-2.606, 32.686], mean action: 6.696 [0.000, 20.000],  loss: 0.017828, mae: 0.351484, mean_q: 0.484152, mean_eps: 0.000000
 2173/5000: episode: 90, duration: 0.311s, episode steps:  23, steps per second:  74, episode reward: -44.200, mean reward: -1.922 [-32.560,  2.270], mean action: 6.783 [1.000, 11.000],  loss: 0.019891, mae: 0.358744, mean_q: 0.557821, mean_eps: 0.000000
 2191/5000: episode: 91, duration: 0.260s, episode steps:  18, steps per second:  69, episode reward: 36.000, mean reward:  2.000 [-2.723, 32.120], mean action: 3.722 [0.000, 11.000],  loss: 0.023459, mae: 0.377535, mean_q: 0.568293, mean_eps: 0.000000
 2214/5000: episode: 92, duration: 0.320s, episode steps:  23, steps per second:  72, episode reward: 35.271, mean reward:  1.534 [-2.378, 31.983], mean action: 2.739 [0.000, 11.000],  loss: 0.021574, mae: 0.368562, mean_q: 0.569840, mean_eps: 0.000000
 2236/5000: episode: 93, duration: 0.322s, episode steps:  22, steps per second:  68, episode reward: -32.810, mean reward: -1.491 [-32.086,  2.502], mean action: 6.455 [0.000, 17.000],  loss: 0.017916, mae: 0.346365, mean_q: 0.560920, mean_eps: 0.000000
 2268/5000: episode: 94, duration: 0.489s, episode steps:  32, steps per second:  65, episode reward: 33.000, mean reward:  1.031 [-3.000, 32.174], mean action: 8.031 [0.000, 17.000],  loss: 0.019725, mae: 0.352844, mean_q: 0.537163, mean_eps: 0.000000
 2289/5000: episode: 95, duration: 0.379s, episode steps:  21, steps per second:  55, episode reward: -32.370, mean reward: -1.541 [-32.370,  3.000], mean action: 4.048 [0.000, 11.000],  loss: 0.019374, mae: 0.352489, mean_q: 0.591988, mean_eps: 0.000000
 2322/5000: episode: 96, duration: 0.462s, episode steps:  33, steps per second:  71, episode reward: 32.311, mean reward:  0.979 [-3.000, 32.320], mean action: 2.788 [0.000, 11.000],  loss: 0.024430, mae: 0.376444, mean_q: 0.599968, mean_eps: 0.000000
 2350/5000: episode: 97, duration: 0.391s, episode steps:  28, steps per second:  72, episode reward: 35.118, mean reward:  1.254 [-2.668, 32.280], mean action: 4.429 [0.000, 15.000],  loss: 0.020229, mae: 0.361239, mean_q: 0.537398, mean_eps: 0.000000
 2370/5000: episode: 98, duration: 0.290s, episode steps:  20, steps per second:  69, episode reward: 35.539, mean reward:  1.777 [-3.000, 32.080], mean action: 4.600 [0.000, 12.000],  loss: 0.017916, mae: 0.349098, mean_q: 0.527607, mean_eps: 0.000000
 2389/5000: episode: 99, duration: 0.274s, episode steps:  19, steps per second:  69, episode reward: 38.754, mean reward:  2.040 [-2.939, 32.380], mean action: 2.842 [0.000, 15.000],  loss: 0.018925, mae: 0.352565, mean_q: 0.480665, mean_eps: 0.000000
 2416/5000: episode: 100, duration: 0.378s, episode steps:  27, steps per second:  71, episode reward: -32.010, mean reward: -1.186 [-33.000,  2.646], mean action: 4.704 [1.000, 16.000],  loss: 0.018277, mae: 0.351731, mean_q: 0.490791, mean_eps: 0.000000
 2439/5000: episode: 101, duration: 0.322s, episode steps:  23, steps per second:  71, episode reward: -38.780, mean reward: -1.686 [-33.000,  2.350], mean action: 4.826 [0.000, 13.000],  loss: 0.020183, mae: 0.355980, mean_q: 0.529889, mean_eps: 0.000000
 2453/5000: episode: 102, duration: 0.209s, episode steps:  14, steps per second:  67, episode reward: 38.642, mean reward:  2.760 [-2.444, 33.000], mean action: 4.071 [0.000, 12.000],  loss: 0.020964, mae: 0.363729, mean_q: 0.542873, mean_eps: 0.000000
 2486/5000: episode: 103, duration: 0.455s, episode steps:  33, steps per second:  73, episode reward: 41.043, mean reward:  1.244 [-2.375, 33.000], mean action: 1.364 [0.000, 12.000],  loss: 0.020305, mae: 0.362169, mean_q: 0.592075, mean_eps: 0.000000
 2511/5000: episode: 104, duration: 0.400s, episode steps:  25, steps per second:  63, episode reward: 35.611, mean reward:  1.424 [-2.451, 30.936], mean action: 3.320 [0.000, 9.000],  loss: 0.021238, mae: 0.372722, mean_q: 0.582259, mean_eps: 0.000000
 2532/5000: episode: 105, duration: 0.299s, episode steps:  21, steps per second:  70, episode reward: 35.465, mean reward:  1.689 [-2.561, 32.230], mean action: 3.143 [0.000, 9.000],  loss: 0.024192, mae: 0.379422, mean_q: 0.577631, mean_eps: 0.000000
 2573/5000: episode: 106, duration: 0.570s, episode steps:  41, steps per second:  72, episode reward: 32.477, mean reward:  0.792 [-2.439, 32.900], mean action: 2.488 [0.000, 12.000],  loss: 0.018362, mae: 0.358531, mean_q: 0.557004, mean_eps: 0.000000
 2600/5000: episode: 107, duration: 0.389s, episode steps:  27, steps per second:  69, episode reward: 38.683, mean reward:  1.433 [-2.519, 32.029], mean action: 2.556 [1.000, 12.000],  loss: 0.020364, mae: 0.372097, mean_q: 0.536857, mean_eps: 0.000000
 2703/5000: episode: 108, duration: 1.375s, episode steps: 103, steps per second:  75, episode reward: 30.057, mean reward:  0.292 [-2.903, 31.876], mean action: 6.068 [0.000, 15.000],  loss: 0.021179, mae: 0.374876, mean_q: 0.553569, mean_eps: 0.000000
 2718/5000: episode: 109, duration: 0.221s, episode steps:  15, steps per second:  68, episode reward: 41.072, mean reward:  2.738 [-3.000, 32.460], mean action: 2.800 [0.000, 15.000],  loss: 0.020142, mae: 0.362821, mean_q: 0.503473, mean_eps: 0.000000
 2732/5000: episode: 110, duration: 0.215s, episode steps:  14, steps per second:  65, episode reward: 41.739, mean reward:  2.981 [-2.805, 33.000], mean action: 3.500 [1.000, 15.000],  loss: 0.023537, mae: 0.383377, mean_q: 0.556191, mean_eps: 0.000000
 2760/5000: episode: 111, duration: 0.533s, episode steps:  28, steps per second:  53, episode reward: -32.710, mean reward: -1.168 [-32.142,  2.501], mean action: 9.143 [0.000, 21.000],  loss: 0.019835, mae: 0.366431, mean_q: 0.537453, mean_eps: 0.000000
 2780/5000: episode: 112, duration: 0.499s, episode steps:  20, steps per second:  40, episode reward: 35.054, mean reward:  1.753 [-3.000, 32.410], mean action: 5.100 [0.000, 19.000],  loss: 0.020051, mae: 0.360686, mean_q: 0.496985, mean_eps: 0.000000
 2806/5000: episode: 113, duration: 0.652s, episode steps:  26, steps per second:  40, episode reward: -32.570, mean reward: -1.253 [-32.455,  2.700], mean action: 7.885 [0.000, 21.000],  loss: 0.020282, mae: 0.360503, mean_q: 0.525359, mean_eps: 0.000000
 2834/5000: episode: 114, duration: 0.873s, episode steps:  28, steps per second:  32, episode reward: -33.000, mean reward: -1.179 [-32.500,  2.830], mean action: 7.393 [0.000, 21.000],  loss: 0.019325, mae: 0.354395, mean_q: 0.506861, mean_eps: 0.000000
 2862/5000: episode: 115, duration: 0.412s, episode steps:  28, steps per second:  68, episode reward: -33.000, mean reward: -1.179 [-32.133,  2.383], mean action: 6.321 [0.000, 16.000],  loss: 0.019707, mae: 0.364264, mean_q: 0.501775, mean_eps: 0.000000
 2880/5000: episode: 116, duration: 0.656s, episode steps:  18, steps per second:  27, episode reward: 38.204, mean reward:  2.122 [-3.000, 32.140], mean action: 4.833 [0.000, 16.000],  loss: 0.021079, mae: 0.368573, mean_q: 0.513080, mean_eps: 0.000000
 2897/5000: episode: 117, duration: 0.583s, episode steps:  17, steps per second:  29, episode reward: -41.910, mean reward: -2.465 [-31.957,  1.805], mean action: 7.529 [0.000, 16.000],  loss: 0.016723, mae: 0.353453, mean_q: 0.507738, mean_eps: 0.000000
 2922/5000: episode: 118, duration: 0.545s, episode steps:  25, steps per second:  46, episode reward: 38.729, mean reward:  1.549 [-2.373, 32.010], mean action: 3.000 [0.000, 16.000],  loss: 0.025417, mae: 0.380120, mean_q: 0.540861, mean_eps: 0.000000
 2938/5000: episode: 119, duration: 0.250s, episode steps:  16, steps per second:  64, episode reward: 44.888, mean reward:  2.805 [-2.374, 32.020], mean action: 3.875 [0.000, 16.000],  loss: 0.022462, mae: 0.366664, mean_q: 0.558998, mean_eps: 0.000000
 2967/5000: episode: 120, duration: 0.423s, episode steps:  29, steps per second:  69, episode reward: 32.836, mean reward:  1.132 [-2.263, 32.230], mean action: 4.966 [0.000, 16.000],  loss: 0.023874, mae: 0.376544, mean_q: 0.560292, mean_eps: 0.000000
 2983/5000: episode: 121, duration: 0.245s, episode steps:  16, steps per second:  65, episode reward: 38.355, mean reward:  2.397 [-2.463, 32.721], mean action: 2.562 [0.000, 16.000],  loss: 0.020355, mae: 0.360436, mean_q: 0.614813, mean_eps: 0.000000
 3019/5000: episode: 122, duration: 0.555s, episode steps:  36, steps per second:  65, episode reward: 43.766, mean reward:  1.216 [-3.000, 32.938], mean action: 3.417 [1.000, 13.000],  loss: 0.020588, mae: 0.367605, mean_q: 0.559040, mean_eps: 0.000000
 3044/5000: episode: 123, duration: 0.925s, episode steps:  25, steps per second:  27, episode reward: 33.000, mean reward:  1.320 [-2.414, 32.580], mean action: 5.280 [0.000, 19.000],  loss: 0.020160, mae: 0.368205, mean_q: 0.484101, mean_eps: 0.000000
 3063/5000: episode: 124, duration: 0.307s, episode steps:  19, steps per second:  62, episode reward: -36.000, mean reward: -1.895 [-33.000,  2.510], mean action: 4.632 [0.000, 16.000],  loss: 0.024703, mae: 0.379658, mean_q: 0.558582, mean_eps: 0.000000
 3085/5000: episode: 125, duration: 0.324s, episode steps:  22, steps per second:  68, episode reward: -35.590, mean reward: -1.618 [-31.932,  2.198], mean action: 4.500 [0.000, 19.000],  loss: 0.021691, mae: 0.364935, mean_q: 0.558154, mean_eps: 0.000000
 3109/5000: episode: 126, duration: 0.356s, episode steps:  24, steps per second:  67, episode reward: 38.070, mean reward:  1.586 [-2.516, 32.081], mean action: 2.083 [0.000, 12.000],  loss: 0.019520, mae: 0.357655, mean_q: 0.549523, mean_eps: 0.000000
 3134/5000: episode: 127, duration: 0.351s, episode steps:  25, steps per second:  71, episode reward: 38.767, mean reward:  1.551 [-2.198, 32.020], mean action: 2.680 [0.000, 11.000],  loss: 0.024506, mae: 0.379272, mean_q: 0.499841, mean_eps: 0.000000
 3156/5000: episode: 128, duration: 0.309s, episode steps:  22, steps per second:  71, episode reward: 33.000, mean reward:  1.500 [-2.430, 33.000], mean action: 3.773 [0.000, 11.000],  loss: 0.021124, mae: 0.362343, mean_q: 0.503784, mean_eps: 0.000000
 3175/5000: episode: 129, duration: 0.272s, episode steps:  19, steps per second:  70, episode reward: 35.425, mean reward:  1.864 [-3.000, 31.860], mean action: 3.316 [0.000, 11.000],  loss: 0.019391, mae: 0.344612, mean_q: 0.551976, mean_eps: 0.000000
 3203/5000: episode: 130, duration: 0.509s, episode steps:  28, steps per second:  55, episode reward: 32.356, mean reward:  1.156 [-3.000, 32.270], mean action: 3.750 [0.000, 11.000],  loss: 0.018298, mae: 0.341127, mean_q: 0.561061, mean_eps: 0.000000
 3235/5000: episode: 131, duration: 0.454s, episode steps:  32, steps per second:  71, episode reward: -33.000, mean reward: -1.031 [-32.294,  2.834], mean action: 9.594 [0.000, 20.000],  loss: 0.019782, mae: 0.345614, mean_q: 0.561322, mean_eps: 0.000000
 3252/5000: episode: 132, duration: 0.240s, episode steps:  17, steps per second:  71, episode reward: -42.000, mean reward: -2.471 [-33.000,  2.477], mean action: 8.941 [1.000, 20.000],  loss: 0.019655, mae: 0.344203, mean_q: 0.541938, mean_eps: 0.000000
 3272/5000: episode: 133, duration: 0.289s, episode steps:  20, steps per second:  69, episode reward: 41.433, mean reward:  2.072 [-2.156, 32.040], mean action: 3.550 [0.000, 14.000],  loss: 0.018889, mae: 0.342211, mean_q: 0.547235, mean_eps: 0.000000
 3300/5000: episode: 134, duration: 0.386s, episode steps:  28, steps per second:  72, episode reward: 38.136, mean reward:  1.362 [-2.473, 32.343], mean action: 2.071 [0.000, 16.000],  loss: 0.019125, mae: 0.345554, mean_q: 0.496782, mean_eps: 0.000000
 3320/5000: episode: 135, duration: 0.322s, episode steps:  20, steps per second:  62, episode reward: 32.903, mean reward:  1.645 [-2.424, 32.253], mean action: 5.400 [0.000, 19.000],  loss: 0.019668, mae: 0.349423, mean_q: 0.537209, mean_eps: 0.000000
 3340/5000: episode: 136, duration: 0.284s, episode steps:  20, steps per second:  70, episode reward: 38.804, mean reward:  1.940 [-2.901, 32.102], mean action: 5.300 [0.000, 19.000],  loss: 0.025867, mae: 0.380545, mean_q: 0.512946, mean_eps: 0.000000
 3359/5000: episode: 137, duration: 0.273s, episode steps:  19, steps per second:  70, episode reward: 37.938, mean reward:  1.997 [-2.703, 32.910], mean action: 6.000 [0.000, 20.000],  loss: 0.021635, mae: 0.370295, mean_q: 0.520187, mean_eps: 0.000000
 3392/5000: episode: 138, duration: 0.452s, episode steps:  33, steps per second:  73, episode reward: -35.770, mean reward: -1.084 [-31.900,  2.240], mean action: 5.485 [0.000, 16.000],  loss: 0.017375, mae: 0.347611, mean_q: 0.560691, mean_eps: 0.000000
 3419/5000: episode: 139, duration: 0.371s, episode steps:  27, steps per second:  73, episode reward: -33.000, mean reward: -1.222 [-32.494,  2.530], mean action: 5.556 [0.000, 18.000],  loss: 0.019466, mae: 0.354891, mean_q: 0.564123, mean_eps: 0.000000
 3458/5000: episode: 140, duration: 0.543s, episode steps:  39, steps per second:  72, episode reward: 32.967, mean reward:  0.845 [-3.000, 31.989], mean action: 2.718 [0.000, 16.000],  loss: 0.021245, mae: 0.363300, mean_q: 0.518979, mean_eps: 0.000000
 3477/5000: episode: 141, duration: 0.333s, episode steps:  19, steps per second:  57, episode reward: 36.000, mean reward:  1.895 [-3.000, 32.200], mean action: 3.158 [0.000, 11.000],  loss: 0.019370, mae: 0.354296, mean_q: 0.491793, mean_eps: 0.000000
 3505/5000: episode: 142, duration: 0.421s, episode steps:  28, steps per second:  67, episode reward: 38.605, mean reward:  1.379 [-2.731, 32.140], mean action: 2.821 [0.000, 11.000],  loss: 0.021729, mae: 0.362681, mean_q: 0.569428, mean_eps: 0.000000
 3529/5000: episode: 143, duration: 0.365s, episode steps:  24, steps per second:  66, episode reward: 32.368, mean reward:  1.349 [-3.000, 32.040], mean action: 4.542 [0.000, 19.000],  loss: 0.021535, mae: 0.356635, mean_q: 0.575529, mean_eps: 0.000000
 3548/5000: episode: 144, duration: 0.284s, episode steps:  19, steps per second:  67, episode reward: -32.910, mean reward: -1.732 [-31.938,  3.000], mean action: 4.000 [0.000, 9.000],  loss: 0.016580, mae: 0.354277, mean_q: 0.522382, mean_eps: 0.000000
 3575/5000: episode: 145, duration: 0.379s, episode steps:  27, steps per second:  71, episode reward: -32.440, mean reward: -1.201 [-32.102,  2.178], mean action: 8.593 [0.000, 19.000],  loss: 0.023004, mae: 0.378917, mean_q: 0.518931, mean_eps: 0.000000
 3593/5000: episode: 146, duration: 0.467s, episode steps:  18, steps per second:  39, episode reward: 38.039, mean reward:  2.113 [-2.442, 31.709], mean action: 3.833 [0.000, 19.000],  loss: 0.019767, mae: 0.358026, mean_q: 0.541063, mean_eps: 0.000000
 3617/5000: episode: 147, duration: 0.356s, episode steps:  24, steps per second:  67, episode reward: -32.560, mean reward: -1.357 [-33.000,  3.000], mean action: 10.750 [0.000, 20.000],  loss: 0.021454, mae: 0.363335, mean_q: 0.548259, mean_eps: 0.000000
 3645/5000: episode: 148, duration: 0.394s, episode steps:  28, steps per second:  71, episode reward: -32.240, mean reward: -1.151 [-31.549,  3.000], mean action: 4.929 [0.000, 19.000],  loss: 0.018325, mae: 0.351090, mean_q: 0.560965, mean_eps: 0.000000
 3663/5000: episode: 149, duration: 0.257s, episode steps:  18, steps per second:  70, episode reward: 41.537, mean reward:  2.308 [-2.233, 32.350], mean action: 1.944 [0.000, 19.000],  loss: 0.016769, mae: 0.338085, mean_q: 0.548012, mean_eps: 0.000000
 3680/5000: episode: 150, duration: 0.251s, episode steps:  17, steps per second:  68, episode reward: 44.155, mean reward:  2.597 [-2.474, 32.079], mean action: 4.000 [0.000, 19.000],  loss: 0.024129, mae: 0.381260, mean_q: 0.540854, mean_eps: 0.000000
 3706/5000: episode: 151, duration: 0.394s, episode steps:  26, steps per second:  66, episode reward: 37.486, mean reward:  1.442 [-2.869, 33.000], mean action: 4.115 [0.000, 20.000],  loss: 0.015619, mae: 0.334156, mean_q: 0.555938, mean_eps: 0.000000
 3753/5000: episode: 152, duration: 0.777s, episode steps:  47, steps per second:  60, episode reward: -32.590, mean reward: -0.693 [-32.250,  2.915], mean action: 4.894 [0.000, 14.000],  loss: 0.021912, mae: 0.365957, mean_q: 0.530936, mean_eps: 0.000000
 3775/5000: episode: 153, duration: 0.456s, episode steps:  22, steps per second:  48, episode reward: -36.000, mean reward: -1.636 [-32.454,  2.770], mean action: 6.455 [0.000, 14.000],  loss: 0.020285, mae: 0.359631, mean_q: 0.553104, mean_eps: 0.000000
 3805/5000: episode: 154, duration: 0.606s, episode steps:  30, steps per second:  49, episode reward: -33.000, mean reward: -1.100 [-32.771,  2.549], mean action: 5.633 [0.000, 20.000],  loss: 0.020425, mae: 0.353958, mean_q: 0.541530, mean_eps: 0.000000
 3827/5000: episode: 155, duration: 0.322s, episode steps:  22, steps per second:  68, episode reward: 35.048, mean reward:  1.593 [-2.463, 32.100], mean action: 5.364 [0.000, 20.000],  loss: 0.019917, mae: 0.347875, mean_q: 0.517120, mean_eps: 0.000000
 3842/5000: episode: 156, duration: 0.321s, episode steps:  15, steps per second:  47, episode reward: 41.903, mean reward:  2.794 [-2.424, 32.090], mean action: 2.800 [0.000, 11.000],  loss: 0.018794, mae: 0.344648, mean_q: 0.601506, mean_eps: 0.000000
 3874/5000: episode: 157, duration: 0.478s, episode steps:  32, steps per second:  67, episode reward: 32.639, mean reward:  1.020 [-2.506, 32.330], mean action: 7.188 [0.000, 13.000],  loss: 0.026792, mae: 0.380659, mean_q: 0.543641, mean_eps: 0.000000
 3903/5000: episode: 158, duration: 0.787s, episode steps:  29, steps per second:  37, episode reward: 33.000, mean reward:  1.138 [-2.491, 32.290], mean action: 3.241 [0.000, 14.000],  loss: 0.020521, mae: 0.352113, mean_q: 0.564380, mean_eps: 0.000000
 3921/5000: episode: 159, duration: 0.381s, episode steps:  18, steps per second:  47, episode reward: 34.582, mean reward:  1.921 [-3.000, 31.738], mean action: 6.000 [0.000, 14.000],  loss: 0.017796, mae: 0.342530, mean_q: 0.599414, mean_eps: 0.000000
 3971/5000: episode: 160, duration: 0.821s, episode steps:  50, steps per second:  61, episode reward: -35.110, mean reward: -0.702 [-32.091,  2.540], mean action: 10.300 [0.000, 21.000],  loss: 0.017650, mae: 0.339657, mean_q: 0.528027, mean_eps: 0.000000
 3997/5000: episode: 161, duration: 0.495s, episode steps:  26, steps per second:  53, episode reward: 35.426, mean reward:  1.363 [-2.452, 32.288], mean action: 8.192 [0.000, 20.000],  loss: 0.022402, mae: 0.357200, mean_q: 0.502489, mean_eps: 0.000000
 4033/5000: episode: 162, duration: 1.124s, episode steps:  36, steps per second:  32, episode reward: -35.150, mean reward: -0.976 [-32.357,  2.414], mean action: 2.444 [0.000, 15.000],  loss: 0.019816, mae: 0.346740, mean_q: 0.557246, mean_eps: 0.000000
 4060/5000: episode: 163, duration: 0.675s, episode steps:  27, steps per second:  40, episode reward: -33.000, mean reward: -1.222 [-32.485,  2.570], mean action: 6.815 [0.000, 17.000],  loss: 0.019796, mae: 0.342676, mean_q: 0.571169, mean_eps: 0.000000
 4085/5000: episode: 164, duration: 0.708s, episode steps:  25, steps per second:  35, episode reward: 35.309, mean reward:  1.412 [-2.940, 32.600], mean action: 4.040 [0.000, 19.000],  loss: 0.019178, mae: 0.340322, mean_q: 0.573290, mean_eps: 0.000000
 4120/5000: episode: 165, duration: 0.723s, episode steps:  35, steps per second:  48, episode reward: -35.360, mean reward: -1.010 [-32.071,  2.500], mean action: 7.343 [0.000, 19.000],  loss: 0.017963, mae: 0.336139, mean_q: 0.540314, mean_eps: 0.000000
 4145/5000: episode: 166, duration: 0.410s, episode steps:  25, steps per second:  61, episode reward: 38.623, mean reward:  1.545 [-2.536, 32.010], mean action: 4.240 [1.000, 19.000],  loss: 0.018539, mae: 0.343054, mean_q: 0.532137, mean_eps: 0.000000
 4174/5000: episode: 167, duration: 0.482s, episode steps:  29, steps per second:  60, episode reward: 37.570, mean reward:  1.296 [-2.594, 32.544], mean action: 4.483 [0.000, 19.000],  loss: 0.016777, mae: 0.340283, mean_q: 0.586496, mean_eps: 0.000000
 4198/5000: episode: 168, duration: 0.389s, episode steps:  24, steps per second:  62, episode reward: 40.164, mean reward:  1.673 [-2.125, 32.170], mean action: 4.333 [0.000, 19.000],  loss: 0.019961, mae: 0.352001, mean_q: 0.580620, mean_eps: 0.000000
 4215/5000: episode: 169, duration: 0.397s, episode steps:  17, steps per second:  43, episode reward: 41.158, mean reward:  2.421 [-2.111, 31.478], mean action: 2.000 [0.000, 12.000],  loss: 0.027055, mae: 0.392109, mean_q: 0.580044, mean_eps: 0.000000
 4260/5000: episode: 170, duration: 0.745s, episode steps:  45, steps per second:  60, episode reward: 35.094, mean reward:  0.780 [-2.421, 32.520], mean action: 5.044 [0.000, 20.000],  loss: 0.020415, mae: 0.362956, mean_q: 0.533457, mean_eps: 0.000000
 4290/5000: episode: 171, duration: 0.706s, episode steps:  30, steps per second:  42, episode reward: 35.381, mean reward:  1.179 [-3.000, 32.470], mean action: 3.467 [0.000, 12.000],  loss: 0.022579, mae: 0.370346, mean_q: 0.568881, mean_eps: 0.000000
 4319/5000: episode: 172, duration: 0.630s, episode steps:  29, steps per second:  46, episode reward: -32.940, mean reward: -1.136 [-32.080,  2.400], mean action: 7.517 [1.000, 19.000],  loss: 0.018500, mae: 0.348231, mean_q: 0.552544, mean_eps: 0.000000
 4351/5000: episode: 173, duration: 0.515s, episode steps:  32, steps per second:  62, episode reward: 33.000, mean reward:  1.031 [-2.488, 30.000], mean action: 3.250 [0.000, 15.000],  loss: 0.020994, mae: 0.362699, mean_q: 0.541941, mean_eps: 0.000000
 4368/5000: episode: 174, duration: 0.262s, episode steps:  17, steps per second:  65, episode reward: 44.212, mean reward:  2.601 [-2.231, 32.855], mean action: 2.176 [0.000, 9.000],  loss: 0.021725, mae: 0.372629, mean_q: 0.566663, mean_eps: 0.000000
 4389/5000: episode: 175, duration: 0.298s, episode steps:  21, steps per second:  70, episode reward: -35.770, mean reward: -1.703 [-32.335,  2.901], mean action: 3.238 [0.000, 12.000],  loss: 0.017011, mae: 0.342032, mean_q: 0.517667, mean_eps: 0.000000
 4420/5000: episode: 176, duration: 0.427s, episode steps:  31, steps per second:  73, episode reward: -32.500, mean reward: -1.048 [-32.050,  2.583], mean action: 6.065 [0.000, 16.000],  loss: 0.022463, mae: 0.371612, mean_q: 0.527138, mean_eps: 0.000000
 4441/5000: episode: 177, duration: 0.301s, episode steps:  21, steps per second:  70, episode reward: -35.560, mean reward: -1.693 [-32.537,  2.779], mean action: 7.571 [0.000, 16.000],  loss: 0.022670, mae: 0.382024, mean_q: 0.530588, mean_eps: 0.000000
 4461/5000: episode: 178, duration: 0.282s, episode steps:  20, steps per second:  71, episode reward: -35.250, mean reward: -1.762 [-31.645,  2.383], mean action: 5.700 [0.000, 15.000],  loss: 0.018435, mae: 0.356554, mean_q: 0.517490, mean_eps: 0.000000
 4495/5000: episode: 179, duration: 0.479s, episode steps:  34, steps per second:  71, episode reward: 35.487, mean reward:  1.044 [-2.346, 32.160], mean action: 3.824 [0.000, 15.000],  loss: 0.021406, mae: 0.370716, mean_q: 0.562412, mean_eps: 0.000000
 4511/5000: episode: 180, duration: 0.237s, episode steps:  16, steps per second:  68, episode reward: 38.647, mean reward:  2.415 [-3.000, 32.260], mean action: 3.438 [0.000, 12.000],  loss: 0.023563, mae: 0.370431, mean_q: 0.567969, mean_eps: 0.000000
 4524/5000: episode: 181, duration: 0.236s, episode steps:  13, steps per second:  55, episode reward: 44.118, mean reward:  3.394 [-2.117, 32.758], mean action: 2.769 [0.000, 12.000],  loss: 0.015550, mae: 0.335250, mean_q: 0.543353, mean_eps: 0.000000
 4539/5000: episode: 182, duration: 0.351s, episode steps:  15, steps per second:  43, episode reward: 44.043, mean reward:  2.936 [-2.410, 32.573], mean action: 4.067 [0.000, 13.000],  loss: 0.017299, mae: 0.345617, mean_q: 0.537726, mean_eps: 0.000000
 4557/5000: episode: 183, duration: 0.269s, episode steps:  18, steps per second:  67, episode reward: 38.100, mean reward:  2.117 [-2.669, 33.000], mean action: 5.833 [0.000, 17.000],  loss: 0.025190, mae: 0.383093, mean_q: 0.540096, mean_eps: 0.000000
 4582/5000: episode: 184, duration: 0.375s, episode steps:  25, steps per second:  67, episode reward: -35.360, mean reward: -1.414 [-32.408,  2.502], mean action: 3.480 [0.000, 19.000],  loss: 0.019364, mae: 0.359147, mean_q: 0.514674, mean_eps: 0.000000
 4601/5000: episode: 185, duration: 0.655s, episode steps:  19, steps per second:  29, episode reward: 41.299, mean reward:  2.174 [-2.462, 32.090], mean action: 1.947 [0.000, 11.000],  loss: 0.025125, mae: 0.384444, mean_q: 0.594383, mean_eps: 0.000000
 4631/5000: episode: 186, duration: 0.649s, episode steps:  30, steps per second:  46, episode reward: 38.742, mean reward:  1.291 [-2.357, 32.230], mean action: 3.267 [0.000, 14.000],  loss: 0.018216, mae: 0.356790, mean_q: 0.594539, mean_eps: 0.000000
 4658/5000: episode: 187, duration: 0.516s, episode steps:  27, steps per second:  52, episode reward: 37.512, mean reward:  1.389 [-2.484, 32.543], mean action: 4.074 [0.000, 13.000],  loss: 0.021730, mae: 0.372609, mean_q: 0.555965, mean_eps: 0.000000
 4679/5000: episode: 188, duration: 0.310s, episode steps:  21, steps per second:  68, episode reward: 36.000, mean reward:  1.714 [-2.508, 32.590], mean action: 4.095 [0.000, 14.000],  loss: 0.017816, mae: 0.349890, mean_q: 0.512758, mean_eps: 0.000000
 4709/5000: episode: 189, duration: 0.416s, episode steps:  30, steps per second:  72, episode reward: -32.520, mean reward: -1.084 [-32.312,  2.200], mean action: 5.267 [0.000, 14.000],  loss: 0.018588, mae: 0.346098, mean_q: 0.511720, mean_eps: 0.000000
 4727/5000: episode: 190, duration: 0.393s, episode steps:  18, steps per second:  46, episode reward: 38.465, mean reward:  2.137 [-3.000, 32.057], mean action: 2.444 [0.000, 12.000],  loss: 0.025800, mae: 0.389852, mean_q: 0.567256, mean_eps: 0.000000
 4757/5000: episode: 191, duration: 0.518s, episode steps:  30, steps per second:  58, episode reward: 35.712, mean reward:  1.190 [-2.852, 32.550], mean action: 6.067 [0.000, 18.000],  loss: 0.019285, mae: 0.359941, mean_q: 0.583777, mean_eps: 0.000000
 4779/5000: episode: 192, duration: 0.356s, episode steps:  22, steps per second:  62, episode reward: -38.340, mean reward: -1.743 [-32.030,  2.370], mean action: 4.773 [0.000, 16.000],  loss: 0.017588, mae: 0.351280, mean_q: 0.530151, mean_eps: 0.000000
 4804/5000: episode: 193, duration: 0.368s, episode steps:  25, steps per second:  68, episode reward: 35.503, mean reward:  1.420 [-2.431, 32.220], mean action: 3.840 [0.000, 16.000],  loss: 0.019438, mae: 0.357596, mean_q: 0.522539, mean_eps: 0.000000
 4839/5000: episode: 194, duration: 0.479s, episode steps:  35, steps per second:  73, episode reward: -35.910, mean reward: -1.026 [-32.239,  2.630], mean action: 6.429 [0.000, 17.000],  loss: 0.018691, mae: 0.354166, mean_q: 0.542079, mean_eps: 0.000000
 4857/5000: episode: 195, duration: 0.264s, episode steps:  18, steps per second:  68, episode reward: 42.000, mean reward:  2.333 [-2.776, 32.140], mean action: 2.778 [0.000, 15.000],  loss: 0.017251, mae: 0.350802, mean_q: 0.524993, mean_eps: 0.000000
 4879/5000: episode: 196, duration: 0.327s, episode steps:  22, steps per second:  67, episode reward: -32.750, mean reward: -1.489 [-32.750,  3.000], mean action: 6.955 [0.000, 17.000],  loss: 0.014681, mae: 0.340265, mean_q: 0.529104, mean_eps: 0.000000
 4905/5000: episode: 197, duration: 0.367s, episode steps:  26, steps per second:  71, episode reward: 35.856, mean reward:  1.379 [-3.000, 32.903], mean action: 3.077 [0.000, 12.000],  loss: 0.021584, mae: 0.374149, mean_q: 0.525989, mean_eps: 0.000000
 4933/5000: episode: 198, duration: 0.396s, episode steps:  28, steps per second:  71, episode reward: 32.435, mean reward:  1.158 [-3.000, 32.200], mean action: 4.536 [0.000, 19.000],  loss: 0.016609, mae: 0.346427, mean_q: 0.539406, mean_eps: 0.000000
 4965/5000: episode: 199, duration: 0.446s, episode steps:  32, steps per second:  72, episode reward: 35.251, mean reward:  1.102 [-2.420, 32.094], mean action: 4.469 [0.000, 19.000],  loss: 0.019698, mae: 0.357331, mean_q: 0.527490, mean_eps: 0.000000
 4982/5000: episode: 200, duration: 0.352s, episode steps:  17, steps per second:  48, episode reward: 35.664, mean reward:  2.098 [-3.000, 32.271], mean action: 3.765 [0.000, 12.000],  loss: 0.019220, mae: 0.356478, mean_q: 0.576839, mean_eps: 0.000000
done, took 75.373 seconds
DQN Evaluation: 6452 victories out of 7617 episodes
Training for 5000 steps ...
   30/5000: episode: 1, duration: 0.239s, episode steps:  30, steps per second: 126, episode reward: 38.727, mean reward:  1.291 [-2.443, 32.130], mean action: 2.767 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   54/5000: episode: 2, duration: 0.171s, episode steps:  24, steps per second: 141, episode reward: 41.349, mean reward:  1.723 [-2.192, 31.569], mean action: 2.125 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   80/5000: episode: 3, duration: 0.174s, episode steps:  26, steps per second: 150, episode reward: 43.945, mean reward:  1.690 [-2.900, 32.110], mean action: 5.115 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  106/5000: episode: 4, duration: 0.171s, episode steps:  26, steps per second: 152, episode reward: 38.298, mean reward:  1.473 [-2.738, 32.240], mean action: 4.962 [2.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  133/5000: episode: 5, duration: 0.180s, episode steps:  27, steps per second: 150, episode reward: 37.334, mean reward:  1.383 [-3.000, 32.260], mean action: 2.630 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  165/5000: episode: 6, duration: 0.316s, episode steps:  32, steps per second: 101, episode reward: 39.000, mean reward:  1.219 [-2.739, 32.180], mean action: 1.438 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  186/5000: episode: 7, duration: 0.150s, episode steps:  21, steps per second: 140, episode reward: 41.843, mean reward:  1.993 [-2.339, 32.512], mean action: 1.667 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  217/5000: episode: 8, duration: 0.216s, episode steps:  31, steps per second: 144, episode reward: 41.543, mean reward:  1.340 [-2.078, 32.207], mean action: 2.742 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  226/5000: episode: 9, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward: 48.000, mean reward:  5.333 [ 0.370, 33.000], mean action: 2.000 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  256/5000: episode: 10, duration: 0.204s, episode steps:  30, steps per second: 147, episode reward: 41.792, mean reward:  1.393 [-2.295, 31.888], mean action: 1.733 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  293/5000: episode: 11, duration: 0.243s, episode steps:  37, steps per second: 152, episode reward: 44.675, mean reward:  1.207 [-2.415, 32.360], mean action: 3.270 [2.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  320/5000: episode: 12, duration: 0.181s, episode steps:  27, steps per second: 149, episode reward: 40.898, mean reward:  1.515 [-2.496, 32.003], mean action: 2.407 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  349/5000: episode: 13, duration: 0.212s, episode steps:  29, steps per second: 137, episode reward: 38.875, mean reward:  1.341 [-2.636, 32.090], mean action: 3.379 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  375/5000: episode: 14, duration: 0.177s, episode steps:  26, steps per second: 147, episode reward: 40.788, mean reward:  1.569 [-2.099, 31.191], mean action: 3.077 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  399/5000: episode: 15, duration: 0.170s, episode steps:  24, steps per second: 141, episode reward: 38.611, mean reward:  1.609 [-2.395, 32.470], mean action: 5.875 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  443/5000: episode: 16, duration: 0.298s, episode steps:  44, steps per second: 148, episode reward: -32.020, mean reward: -0.728 [-32.406,  3.000], mean action: 3.477 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  463/5000: episode: 17, duration: 0.140s, episode steps:  20, steps per second: 143, episode reward: 38.901, mean reward:  1.945 [-2.720, 32.681], mean action: 2.550 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  480/5000: episode: 18, duration: 0.138s, episode steps:  17, steps per second: 124, episode reward: 44.480, mean reward:  2.616 [-3.000, 32.360], mean action: 2.118 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  495/5000: episode: 19, duration: 0.117s, episode steps:  15, steps per second: 128, episode reward: 45.000, mean reward:  3.000 [-2.939, 33.000], mean action: 2.933 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  540/5000: episode: 20, duration: 0.303s, episode steps:  45, steps per second: 149, episode reward: 40.923, mean reward:  0.909 [-2.333, 32.272], mean action: 3.756 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  562/5000: episode: 21, duration: 0.148s, episode steps:  22, steps per second: 149, episode reward: 41.193, mean reward:  1.872 [-2.127, 32.050], mean action: 2.409 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  613/5000: episode: 22, duration: 0.307s, episode steps:  51, steps per second: 166, episode reward: 32.313, mean reward:  0.634 [-2.675, 31.865], mean action: 7.902 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  653/5000: episode: 23, duration: 0.250s, episode steps:  40, steps per second: 160, episode reward: 40.326, mean reward:  1.008 [-2.433, 32.040], mean action: 2.400 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  678/5000: episode: 24, duration: 0.162s, episode steps:  25, steps per second: 155, episode reward: 43.695, mean reward:  1.748 [-2.163, 32.310], mean action: 2.680 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  704/5000: episode: 25, duration: 0.170s, episode steps:  26, steps per second: 153, episode reward: 38.325, mean reward:  1.474 [-2.543, 31.715], mean action: 2.077 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  738/5000: episode: 26, duration: 0.224s, episode steps:  34, steps per second: 152, episode reward: 32.443, mean reward:  0.954 [-2.878, 32.170], mean action: 5.912 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  753/5000: episode: 27, duration: 0.108s, episode steps:  15, steps per second: 138, episode reward: 44.452, mean reward:  2.963 [-2.284, 32.250], mean action: 2.467 [1.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  770/5000: episode: 28, duration: 0.123s, episode steps:  17, steps per second: 139, episode reward: 47.316, mean reward:  2.783 [-0.395, 32.330], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  800/5000: episode: 29, duration: 0.190s, episode steps:  30, steps per second: 158, episode reward: 41.152, mean reward:  1.372 [-2.027, 32.207], mean action: 2.600 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  830/5000: episode: 30, duration: 0.205s, episode steps:  30, steps per second: 146, episode reward: 41.790, mean reward:  1.393 [-2.398, 32.170], mean action: 3.400 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  865/5000: episode: 31, duration: 0.212s, episode steps:  35, steps per second: 165, episode reward: 41.132, mean reward:  1.175 [-2.157, 31.799], mean action: 1.914 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  909/5000: episode: 32, duration: 0.292s, episode steps:  44, steps per second: 151, episode reward: 42.000, mean reward:  0.955 [-2.202, 32.110], mean action: 2.568 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  939/5000: episode: 33, duration: 0.198s, episode steps:  30, steps per second: 151, episode reward: 42.883, mean reward:  1.429 [-2.058, 32.700], mean action: 2.433 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  969/5000: episode: 34, duration: 0.188s, episode steps:  30, steps per second: 159, episode reward: 44.576, mean reward:  1.486 [-2.103, 32.440], mean action: 4.567 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  987/5000: episode: 35, duration: 0.130s, episode steps:  18, steps per second: 139, episode reward: 45.000, mean reward:  2.500 [-2.326, 32.030], mean action: 1.944 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1025/5000: episode: 36, duration: 0.434s, episode steps:  38, steps per second:  88, episode reward: 35.292, mean reward:  0.929 [-2.620, 32.060], mean action: 4.053 [0.000, 14.000],  loss: 0.020399, mae: 0.371285, mean_q: 0.567951, mean_eps: 0.000000
 1048/5000: episode: 37, duration: 0.328s, episode steps:  23, steps per second:  70, episode reward: 41.902, mean reward:  1.822 [-2.510, 32.392], mean action: 6.304 [0.000, 15.000],  loss: 0.024829, mae: 0.383560, mean_q: 0.479699, mean_eps: 0.000000
 1071/5000: episode: 38, duration: 0.328s, episode steps:  23, steps per second:  70, episode reward: 38.815, mean reward:  1.688 [-3.000, 32.150], mean action: 4.391 [0.000, 13.000],  loss: 0.017302, mae: 0.343569, mean_q: 0.503315, mean_eps: 0.000000
 1092/5000: episode: 39, duration: 0.302s, episode steps:  21, steps per second:  70, episode reward: 44.115, mean reward:  2.101 [-2.099, 32.224], mean action: 0.857 [0.000, 11.000],  loss: 0.022831, mae: 0.370416, mean_q: 0.539241, mean_eps: 0.000000
 1109/5000: episode: 40, duration: 0.251s, episode steps:  17, steps per second:  68, episode reward: 43.856, mean reward:  2.580 [-2.290, 31.938], mean action: 4.412 [0.000, 16.000],  loss: 0.022190, mae: 0.370938, mean_q: 0.578921, mean_eps: 0.000000
 1134/5000: episode: 41, duration: 0.352s, episode steps:  25, steps per second:  71, episode reward: 41.195, mean reward:  1.648 [-2.678, 32.150], mean action: 4.160 [0.000, 16.000],  loss: 0.024795, mae: 0.382318, mean_q: 0.613654, mean_eps: 0.000000
 1159/5000: episode: 42, duration: 0.355s, episode steps:  25, steps per second:  71, episode reward: 43.880, mean reward:  1.755 [-2.108, 32.695], mean action: 4.280 [3.000, 15.000],  loss: 0.020494, mae: 0.359880, mean_q: 0.644813, mean_eps: 0.000000
 1184/5000: episode: 43, duration: 0.352s, episode steps:  25, steps per second:  71, episode reward: 40.341, mean reward:  1.614 [-2.728, 31.977], mean action: 3.440 [0.000, 16.000],  loss: 0.018723, mae: 0.358523, mean_q: 0.614487, mean_eps: 0.000000
 1209/5000: episode: 44, duration: 0.354s, episode steps:  25, steps per second:  71, episode reward: 41.126, mean reward:  1.645 [-2.129, 31.933], mean action: 0.960 [0.000, 12.000],  loss: 0.022658, mae: 0.370322, mean_q: 0.571009, mean_eps: 0.000000
 1239/5000: episode: 45, duration: 0.427s, episode steps:  30, steps per second:  70, episode reward: 40.473, mean reward:  1.349 [-2.940, 32.100], mean action: 2.400 [0.000, 12.000],  loss: 0.023164, mae: 0.378999, mean_q: 0.582512, mean_eps: 0.000000
 1267/5000: episode: 46, duration: 0.437s, episode steps:  28, steps per second:  64, episode reward: 38.683, mean reward:  1.382 [-2.602, 32.313], mean action: 3.929 [0.000, 15.000],  loss: 0.021005, mae: 0.367183, mean_q: 0.587244, mean_eps: 0.000000
 1305/5000: episode: 47, duration: 0.682s, episode steps:  38, steps per second:  56, episode reward: 35.004, mean reward:  0.921 [-3.000, 32.120], mean action: 4.026 [0.000, 14.000],  loss: 0.022686, mae: 0.375491, mean_q: 0.560574, mean_eps: 0.000000
 1341/5000: episode: 48, duration: 1.184s, episode steps:  36, steps per second:  30, episode reward: 44.496, mean reward:  1.236 [-2.100, 32.240], mean action: 3.167 [0.000, 16.000],  loss: 0.018150, mae: 0.361535, mean_q: 0.542458, mean_eps: 0.000000
 1359/5000: episode: 49, duration: 0.348s, episode steps:  18, steps per second:  52, episode reward: 38.668, mean reward:  2.148 [-2.647, 31.768], mean action: 5.000 [0.000, 16.000],  loss: 0.018640, mae: 0.364840, mean_q: 0.533324, mean_eps: 0.000000
 1373/5000: episode: 50, duration: 0.362s, episode steps:  14, steps per second:  39, episode reward: 45.000, mean reward:  3.214 [-2.198, 32.220], mean action: 2.571 [0.000, 16.000],  loss: 0.021586, mae: 0.383018, mean_q: 0.508046, mean_eps: 0.000000
 1402/5000: episode: 51, duration: 0.507s, episode steps:  29, steps per second:  57, episode reward: 41.506, mean reward:  1.431 [-2.306, 31.911], mean action: 3.828 [1.000, 16.000],  loss: 0.018488, mae: 0.360993, mean_q: 0.526215, mean_eps: 0.000000
 1421/5000: episode: 52, duration: 0.761s, episode steps:  19, steps per second:  25, episode reward: 46.403, mean reward:  2.442 [-0.765, 32.490], mean action: 5.053 [0.000, 14.000],  loss: 0.020612, mae: 0.364460, mean_q: 0.479283, mean_eps: 0.000000
 1444/5000: episode: 53, duration: 0.999s, episode steps:  23, steps per second:  23, episode reward: 41.931, mean reward:  1.823 [-2.350, 32.140], mean action: 1.870 [0.000, 16.000],  loss: 0.020112, mae: 0.364283, mean_q: 0.533814, mean_eps: 0.000000
 1472/5000: episode: 54, duration: 0.661s, episode steps:  28, steps per second:  42, episode reward: 38.903, mean reward:  1.389 [-2.356, 32.163], mean action: 2.000 [0.000, 16.000],  loss: 0.017959, mae: 0.358163, mean_q: 0.510670, mean_eps: 0.000000
 1490/5000: episode: 55, duration: 0.509s, episode steps:  18, steps per second:  35, episode reward: 41.035, mean reward:  2.280 [-2.400, 32.090], mean action: 3.833 [0.000, 16.000],  loss: 0.018841, mae: 0.355235, mean_q: 0.531161, mean_eps: 0.000000
 1521/5000: episode: 56, duration: 1.055s, episode steps:  31, steps per second:  29, episode reward: 38.337, mean reward:  1.237 [-3.000, 32.264], mean action: 4.065 [0.000, 19.000],  loss: 0.021713, mae: 0.359982, mean_q: 0.515195, mean_eps: 0.000000
 1557/5000: episode: 57, duration: 0.622s, episode steps:  36, steps per second:  58, episode reward: 34.927, mean reward:  0.970 [-2.594, 32.190], mean action: 4.111 [0.000, 16.000],  loss: 0.022114, mae: 0.372171, mean_q: 0.532515, mean_eps: 0.000000
 1572/5000: episode: 58, duration: 0.255s, episode steps:  15, steps per second:  59, episode reward: 44.298, mean reward:  2.953 [-2.094, 32.112], mean action: 6.867 [1.000, 15.000],  loss: 0.021333, mae: 0.366924, mean_q: 0.506102, mean_eps: 0.000000
 1591/5000: episode: 59, duration: 0.283s, episode steps:  19, steps per second:  67, episode reward: 41.753, mean reward:  2.198 [-2.642, 32.435], mean action: 2.526 [0.000, 15.000],  loss: 0.019049, mae: 0.368798, mean_q: 0.566429, mean_eps: 0.000000
 1605/5000: episode: 60, duration: 0.346s, episode steps:  14, steps per second:  40, episode reward: 44.890, mean reward:  3.206 [-2.013, 32.270], mean action: 2.500 [0.000, 15.000],  loss: 0.017138, mae: 0.358193, mean_q: 0.552709, mean_eps: 0.000000
 1632/5000: episode: 61, duration: 0.657s, episode steps:  27, steps per second:  41, episode reward: 33.000, mean reward:  1.222 [-3.000, 32.270], mean action: 4.519 [0.000, 15.000],  loss: 0.023308, mae: 0.386787, mean_q: 0.532645, mean_eps: 0.000000
 1646/5000: episode: 62, duration: 0.219s, episode steps:  14, steps per second:  64, episode reward: 45.000, mean reward:  3.214 [-2.309, 32.220], mean action: 4.143 [1.000, 15.000],  loss: 0.023947, mae: 0.386788, mean_q: 0.536519, mean_eps: 0.000000
 1686/5000: episode: 63, duration: 0.752s, episode steps:  40, steps per second:  53, episode reward: 41.751, mean reward:  1.044 [-2.325, 32.720], mean action: 1.700 [0.000, 19.000],  loss: 0.018225, mae: 0.355575, mean_q: 0.546382, mean_eps: 0.000000
 1706/5000: episode: 64, duration: 0.293s, episode steps:  20, steps per second:  68, episode reward: 38.712, mean reward:  1.936 [-3.000, 32.410], mean action: 3.850 [0.000, 19.000],  loss: 0.020380, mae: 0.373050, mean_q: 0.502394, mean_eps: 0.000000
 1730/5000: episode: 65, duration: 0.351s, episode steps:  24, steps per second:  68, episode reward: 44.018, mean reward:  1.834 [-2.167, 31.854], mean action: 1.208 [0.000, 9.000],  loss: 0.021269, mae: 0.372015, mean_q: 0.502386, mean_eps: 0.000000
 1767/5000: episode: 66, duration: 0.523s, episode steps:  37, steps per second:  71, episode reward: 39.000, mean reward:  1.054 [-2.452, 32.420], mean action: 2.649 [0.000, 9.000],  loss: 0.020064, mae: 0.357529, mean_q: 0.558433, mean_eps: 0.000000
 1804/5000: episode: 67, duration: 0.658s, episode steps:  37, steps per second:  56, episode reward: 41.060, mean reward:  1.110 [-2.164, 32.673], mean action: 3.027 [0.000, 16.000],  loss: 0.020041, mae: 0.365958, mean_q: 0.558391, mean_eps: 0.000000
 1833/5000: episode: 68, duration: 0.757s, episode steps:  29, steps per second:  38, episode reward: 41.815, mean reward:  1.442 [-2.118, 32.740], mean action: 4.517 [0.000, 14.000],  loss: 0.019179, mae: 0.357211, mean_q: 0.547924, mean_eps: 0.000000
 1861/5000: episode: 69, duration: 0.780s, episode steps:  28, steps per second:  36, episode reward: 41.499, mean reward:  1.482 [-2.129, 31.806], mean action: 2.393 [0.000, 14.000],  loss: 0.021091, mae: 0.365886, mean_q: 0.516829, mean_eps: 0.000000
 1879/5000: episode: 70, duration: 0.367s, episode steps:  18, steps per second:  49, episode reward: 39.000, mean reward:  2.167 [-3.000, 33.000], mean action: 3.333 [0.000, 16.000],  loss: 0.019958, mae: 0.356677, mean_q: 0.514975, mean_eps: 0.000000
 1910/5000: episode: 71, duration: 0.518s, episode steps:  31, steps per second:  60, episode reward: 41.651, mean reward:  1.344 [-2.112, 32.280], mean action: 3.806 [0.000, 13.000],  loss: 0.021064, mae: 0.369655, mean_q: 0.586819, mean_eps: 0.000000
 1938/5000: episode: 72, duration: 0.386s, episode steps:  28, steps per second:  72, episode reward: 44.012, mean reward:  1.572 [-2.532, 32.160], mean action: 4.286 [0.000, 19.000],  loss: 0.020252, mae: 0.358576, mean_q: 0.528756, mean_eps: 0.000000
 1955/5000: episode: 73, duration: 0.335s, episode steps:  17, steps per second:  51, episode reward: 38.267, mean reward:  2.251 [-3.000, 31.889], mean action: 5.412 [0.000, 21.000],  loss: 0.022512, mae: 0.379700, mean_q: 0.585164, mean_eps: 0.000000
 1981/5000: episode: 74, duration: 0.855s, episode steps:  26, steps per second:  30, episode reward: 39.000, mean reward:  1.500 [-2.856, 32.190], mean action: 4.231 [0.000, 14.000],  loss: 0.020032, mae: 0.376105, mean_q: 0.581496, mean_eps: 0.000000
 2012/5000: episode: 75, duration: 0.688s, episode steps:  31, steps per second:  45, episode reward: 35.930, mean reward:  1.159 [-3.000, 32.050], mean action: 3.452 [0.000, 19.000],  loss: 0.019322, mae: 0.366686, mean_q: 0.491688, mean_eps: 0.000000
 2049/5000: episode: 76, duration: 0.678s, episode steps:  37, steps per second:  55, episode reward: 35.136, mean reward:  0.950 [-2.517, 32.180], mean action: 6.027 [0.000, 14.000],  loss: 0.021072, mae: 0.375201, mean_q: 0.494629, mean_eps: 0.000000
 2081/5000: episode: 77, duration: 0.685s, episode steps:  32, steps per second:  47, episode reward: 45.000, mean reward:  1.406 [-2.161, 32.410], mean action: 2.156 [0.000, 12.000],  loss: 0.022314, mae: 0.387036, mean_q: 0.507821, mean_eps: 0.000000
 2101/5000: episode: 78, duration: 0.588s, episode steps:  20, steps per second:  34, episode reward: 40.965, mean reward:  2.048 [-2.220, 32.030], mean action: 5.200 [1.000, 13.000],  loss: 0.018916, mae: 0.374997, mean_q: 0.483584, mean_eps: 0.000000
 2119/5000: episode: 79, duration: 0.270s, episode steps:  18, steps per second:  67, episode reward: 42.000, mean reward:  2.333 [-2.800, 32.240], mean action: 2.278 [0.000, 15.000],  loss: 0.020786, mae: 0.382453, mean_q: 0.472639, mean_eps: 0.000000
 2136/5000: episode: 80, duration: 0.253s, episode steps:  17, steps per second:  67, episode reward: 47.349, mean reward:  2.785 [-0.068, 32.360], mean action: 3.941 [1.000, 8.000],  loss: 0.024501, mae: 0.400158, mean_q: 0.461891, mean_eps: 0.000000
 2157/5000: episode: 81, duration: 0.448s, episode steps:  21, steps per second:  47, episode reward: 44.165, mean reward:  2.103 [-2.217, 32.040], mean action: 2.000 [0.000, 15.000],  loss: 0.022058, mae: 0.376654, mean_q: 0.516071, mean_eps: 0.000000
 2177/5000: episode: 82, duration: 0.379s, episode steps:  20, steps per second:  53, episode reward: 43.379, mean reward:  2.169 [-2.531, 32.290], mean action: 2.900 [0.000, 14.000],  loss: 0.020707, mae: 0.371535, mean_q: 0.492394, mean_eps: 0.000000
 2197/5000: episode: 83, duration: 0.401s, episode steps:  20, steps per second:  50, episode reward: 41.655, mean reward:  2.083 [-2.253, 32.371], mean action: 3.350 [0.000, 16.000],  loss: 0.022530, mae: 0.375386, mean_q: 0.529704, mean_eps: 0.000000
 2227/5000: episode: 84, duration: 0.610s, episode steps:  30, steps per second:  49, episode reward: 44.061, mean reward:  1.469 [-2.442, 32.660], mean action: 2.333 [0.000, 13.000],  loss: 0.018760, mae: 0.360897, mean_q: 0.500806, mean_eps: 0.000000
 2243/5000: episode: 85, duration: 0.341s, episode steps:  16, steps per second:  47, episode reward: 44.329, mean reward:  2.771 [-2.527, 32.140], mean action: 4.562 [0.000, 14.000],  loss: 0.020850, mae: 0.373112, mean_q: 0.477911, mean_eps: 0.000000
 2271/5000: episode: 86, duration: 0.558s, episode steps:  28, steps per second:  50, episode reward: 41.941, mean reward:  1.498 [-2.399, 32.161], mean action: 2.357 [0.000, 14.000],  loss: 0.022560, mae: 0.379151, mean_q: 0.508544, mean_eps: 0.000000
 2297/5000: episode: 87, duration: 0.457s, episode steps:  26, steps per second:  57, episode reward: 39.000, mean reward:  1.500 [-2.554, 32.650], mean action: 2.654 [0.000, 16.000],  loss: 0.020028, mae: 0.362274, mean_q: 0.523919, mean_eps: 0.000000
 2331/5000: episode: 88, duration: 0.750s, episode steps:  34, steps per second:  45, episode reward: 33.000, mean reward:  0.971 [-3.000, 32.230], mean action: 3.647 [0.000, 15.000],  loss: 0.021211, mae: 0.380257, mean_q: 0.496645, mean_eps: 0.000000
 2377/5000: episode: 89, duration: 0.661s, episode steps:  46, steps per second:  70, episode reward: 35.509, mean reward:  0.772 [-3.000, 32.000], mean action: 3.370 [0.000, 15.000],  loss: 0.022946, mae: 0.377619, mean_q: 0.526434, mean_eps: 0.000000
 2436/5000: episode: 90, duration: 1.217s, episode steps:  59, steps per second:  48, episode reward: 38.064, mean reward:  0.645 [-3.000, 30.117], mean action: 2.017 [0.000, 15.000],  loss: 0.021415, mae: 0.374823, mean_q: 0.570305, mean_eps: 0.000000
 2464/5000: episode: 91, duration: 0.714s, episode steps:  28, steps per second:  39, episode reward: 34.619, mean reward:  1.236 [-3.000, 31.640], mean action: 5.036 [0.000, 15.000],  loss: 0.016465, mae: 0.356177, mean_q: 0.512333, mean_eps: 0.000000
 2486/5000: episode: 92, duration: 0.653s, episode steps:  22, steps per second:  34, episode reward: 44.242, mean reward:  2.011 [-2.326, 32.143], mean action: 2.045 [0.000, 11.000],  loss: 0.021956, mae: 0.375448, mean_q: 0.528720, mean_eps: 0.000000
 2509/5000: episode: 93, duration: 0.641s, episode steps:  23, steps per second:  36, episode reward: 38.803, mean reward:  1.687 [-2.613, 31.921], mean action: 3.522 [0.000, 11.000],  loss: 0.019852, mae: 0.360077, mean_q: 0.566204, mean_eps: 0.000000
 2530/5000: episode: 94, duration: 0.472s, episode steps:  21, steps per second:  44, episode reward: 42.000, mean reward:  2.000 [-2.785, 33.000], mean action: 3.048 [0.000, 11.000],  loss: 0.022488, mae: 0.376110, mean_q: 0.542704, mean_eps: 0.000000
 2557/5000: episode: 95, duration: 0.397s, episode steps:  27, steps per second:  68, episode reward: 42.000, mean reward:  1.556 [-2.967, 32.350], mean action: 1.852 [0.000, 11.000],  loss: 0.021533, mae: 0.367491, mean_q: 0.537655, mean_eps: 0.000000
 2580/5000: episode: 96, duration: 0.329s, episode steps:  23, steps per second:  70, episode reward: 42.137, mean reward:  1.832 [-2.018, 30.150], mean action: 2.957 [0.000, 19.000],  loss: 0.022273, mae: 0.366369, mean_q: 0.517338, mean_eps: 0.000000
 2613/5000: episode: 97, duration: 0.462s, episode steps:  33, steps per second:  71, episode reward: 41.182, mean reward:  1.248 [-2.204, 32.130], mean action: 4.545 [0.000, 12.000],  loss: 0.021893, mae: 0.364873, mean_q: 0.504888, mean_eps: 0.000000
 2650/5000: episode: 98, duration: 0.747s, episode steps:  37, steps per second:  50, episode reward: 35.985, mean reward:  0.973 [-3.000, 32.250], mean action: 4.432 [0.000, 17.000],  loss: 0.026196, mae: 0.388161, mean_q: 0.575972, mean_eps: 0.000000
 2688/5000: episode: 99, duration: 0.661s, episode steps:  38, steps per second:  57, episode reward: 35.620, mean reward:  0.937 [-2.378, 31.848], mean action: 9.947 [0.000, 20.000],  loss: 0.018223, mae: 0.351544, mean_q: 0.544214, mean_eps: 0.000000
 2704/5000: episode: 100, duration: 0.360s, episode steps:  16, steps per second:  44, episode reward: 44.402, mean reward:  2.775 [-2.040, 31.980], mean action: 2.688 [0.000, 12.000],  loss: 0.023110, mae: 0.367454, mean_q: 0.519020, mean_eps: 0.000000
 2725/5000: episode: 101, duration: 0.511s, episode steps:  21, steps per second:  41, episode reward: 42.000, mean reward:  2.000 [-2.495, 32.640], mean action: 2.048 [0.000, 12.000],  loss: 0.022291, mae: 0.369892, mean_q: 0.560214, mean_eps: 0.000000
 2756/5000: episode: 102, duration: 0.893s, episode steps:  31, steps per second:  35, episode reward: 44.684, mean reward:  1.441 [-2.156, 32.350], mean action: 2.677 [0.000, 12.000],  loss: 0.023227, mae: 0.373726, mean_q: 0.604997, mean_eps: 0.000000
 2779/5000: episode: 103, duration: 0.768s, episode steps:  23, steps per second:  30, episode reward: 44.122, mean reward:  1.918 [-2.191, 32.237], mean action: 2.913 [0.000, 14.000],  loss: 0.021780, mae: 0.359516, mean_q: 0.564564, mean_eps: 0.000000
 2820/5000: episode: 104, duration: 0.923s, episode steps:  41, steps per second:  44, episode reward: 38.300, mean reward:  0.934 [-2.638, 32.680], mean action: 2.976 [0.000, 12.000],  loss: 0.018066, mae: 0.348332, mean_q: 0.554998, mean_eps: 0.000000
 2851/5000: episode: 105, duration: 0.533s, episode steps:  31, steps per second:  58, episode reward: 44.561, mean reward:  1.437 [-2.095, 32.140], mean action: 2.323 [1.000, 12.000],  loss: 0.024765, mae: 0.383177, mean_q: 0.536211, mean_eps: 0.000000
 2877/5000: episode: 106, duration: 0.417s, episode steps:  26, steps per second:  62, episode reward: 38.353, mean reward:  1.475 [-3.000, 32.257], mean action: 2.654 [0.000, 14.000],  loss: 0.022414, mae: 0.366448, mean_q: 0.555289, mean_eps: 0.000000
 2906/5000: episode: 107, duration: 0.413s, episode steps:  29, steps per second:  70, episode reward: 35.469, mean reward:  1.223 [-2.553, 32.038], mean action: 2.690 [0.000, 15.000],  loss: 0.023129, mae: 0.362751, mean_q: 0.531112, mean_eps: 0.000000
 2932/5000: episode: 108, duration: 0.388s, episode steps:  26, steps per second:  67, episode reward: 41.700, mean reward:  1.604 [-2.336, 32.550], mean action: 4.577 [0.000, 16.000],  loss: 0.020289, mae: 0.350402, mean_q: 0.541700, mean_eps: 0.000000
 2952/5000: episode: 109, duration: 0.289s, episode steps:  20, steps per second:  69, episode reward: 41.310, mean reward:  2.066 [-2.939, 31.923], mean action: 3.450 [0.000, 16.000],  loss: 0.016101, mae: 0.335503, mean_q: 0.613525, mean_eps: 0.000000
 2975/5000: episode: 110, duration: 0.344s, episode steps:  23, steps per second:  67, episode reward: 44.364, mean reward:  1.929 [-2.312, 32.404], mean action: 2.652 [1.000, 16.000],  loss: 0.021052, mae: 0.356942, mean_q: 0.622486, mean_eps: 0.000000
 2995/5000: episode: 111, duration: 0.283s, episode steps:  20, steps per second:  71, episode reward: 41.796, mean reward:  2.090 [-2.257, 32.416], mean action: 2.350 [0.000, 16.000],  loss: 0.021675, mae: 0.359045, mean_q: 0.496554, mean_eps: 0.000000
 3033/5000: episode: 112, duration: 0.526s, episode steps:  38, steps per second:  72, episode reward: -32.220, mean reward: -0.848 [-32.954,  2.262], mean action: 5.789 [0.000, 16.000],  loss: 0.018273, mae: 0.344773, mean_q: 0.515985, mean_eps: 0.000000
 3060/5000: episode: 113, duration: 0.403s, episode steps:  27, steps per second:  67, episode reward: 44.681, mean reward:  1.655 [-2.319, 32.180], mean action: 2.407 [0.000, 19.000],  loss: 0.022343, mae: 0.365339, mean_q: 0.522874, mean_eps: 0.000000
 3094/5000: episode: 114, duration: 0.477s, episode steps:  34, steps per second:  71, episode reward: 42.000, mean reward:  1.235 [-2.898, 32.100], mean action: 4.324 [0.000, 19.000],  loss: 0.021071, mae: 0.353253, mean_q: 0.533824, mean_eps: 0.000000
 3119/5000: episode: 115, duration: 0.362s, episode steps:  25, steps per second:  69, episode reward: 38.741, mean reward:  1.550 [-2.463, 32.350], mean action: 5.640 [0.000, 19.000],  loss: 0.017977, mae: 0.338785, mean_q: 0.499068, mean_eps: 0.000000
 3148/5000: episode: 116, duration: 0.410s, episode steps:  29, steps per second:  71, episode reward: 46.988, mean reward:  1.620 [-0.488, 32.340], mean action: 1.621 [0.000, 19.000],  loss: 0.021217, mae: 0.355051, mean_q: 0.517041, mean_eps: 0.000000
 3169/5000: episode: 117, duration: 0.304s, episode steps:  21, steps per second:  69, episode reward: 41.247, mean reward:  1.964 [-3.000, 32.090], mean action: 4.476 [0.000, 13.000],  loss: 0.021092, mae: 0.353800, mean_q: 0.491637, mean_eps: 0.000000
 3197/5000: episode: 118, duration: 0.402s, episode steps:  28, steps per second:  70, episode reward: 43.910, mean reward:  1.568 [-2.324, 32.230], mean action: 2.179 [0.000, 12.000],  loss: 0.019389, mae: 0.352830, mean_q: 0.504571, mean_eps: 0.000000
 3222/5000: episode: 119, duration: 0.351s, episode steps:  25, steps per second:  71, episode reward: 45.000, mean reward:  1.800 [-2.462, 32.190], mean action: 2.120 [0.000, 12.000],  loss: 0.017837, mae: 0.344039, mean_q: 0.534746, mean_eps: 0.000000
 3253/5000: episode: 120, duration: 0.426s, episode steps:  31, steps per second:  73, episode reward: -35.100, mean reward: -1.132 [-32.349,  2.460], mean action: 3.161 [1.000, 12.000],  loss: 0.020859, mae: 0.357157, mean_q: 0.521995, mean_eps: 0.000000
 3271/5000: episode: 121, duration: 0.571s, episode steps:  18, steps per second:  32, episode reward: 44.448, mean reward:  2.469 [-2.228, 32.570], mean action: 1.556 [1.000, 11.000],  loss: 0.019497, mae: 0.354506, mean_q: 0.518091, mean_eps: 0.000000
 3291/5000: episode: 122, duration: 0.542s, episode steps:  20, steps per second:  37, episode reward: 47.263, mean reward:  2.363 [-0.173, 32.069], mean action: 2.750 [0.000, 14.000],  loss: 0.018187, mae: 0.352462, mean_q: 0.500563, mean_eps: 0.000000
 3313/5000: episode: 123, duration: 0.380s, episode steps:  22, steps per second:  58, episode reward: 38.225, mean reward:  1.737 [-2.747, 32.010], mean action: 3.000 [0.000, 15.000],  loss: 0.022336, mae: 0.374291, mean_q: 0.516186, mean_eps: 0.000000
 3335/5000: episode: 124, duration: 0.377s, episode steps:  22, steps per second:  58, episode reward: 44.072, mean reward:  2.003 [-2.003, 32.560], mean action: 2.182 [0.000, 11.000],  loss: 0.024495, mae: 0.371826, mean_q: 0.528931, mean_eps: 0.000000
 3354/5000: episode: 125, duration: 0.350s, episode steps:  19, steps per second:  54, episode reward: 43.730, mean reward:  2.302 [-2.236, 32.590], mean action: 2.211 [0.000, 9.000],  loss: 0.020025, mae: 0.350751, mean_q: 0.583878, mean_eps: 0.000000
 3389/5000: episode: 126, duration: 0.638s, episode steps:  35, steps per second:  55, episode reward: 44.081, mean reward:  1.259 [-2.117, 32.030], mean action: 2.371 [0.000, 15.000],  loss: 0.019412, mae: 0.355126, mean_q: 0.566024, mean_eps: 0.000000
 3412/5000: episode: 127, duration: 0.541s, episode steps:  23, steps per second:  43, episode reward: 39.000, mean reward:  1.696 [-2.646, 32.290], mean action: 3.957 [0.000, 15.000],  loss: 0.016931, mae: 0.342504, mean_q: 0.546938, mean_eps: 0.000000
 3436/5000: episode: 128, duration: 0.552s, episode steps:  24, steps per second:  43, episode reward: 35.773, mean reward:  1.491 [-3.000, 32.313], mean action: 2.375 [0.000, 9.000],  loss: 0.022211, mae: 0.363167, mean_q: 0.567938, mean_eps: 0.000000
 3456/5000: episode: 129, duration: 1.129s, episode steps:  20, steps per second:  18, episode reward: 42.000, mean reward:  2.100 [-2.068, 32.060], mean action: 2.500 [1.000, 9.000],  loss: 0.022126, mae: 0.362363, mean_q: 0.573505, mean_eps: 0.000000
 3482/5000: episode: 130, duration: 0.420s, episode steps:  26, steps per second:  62, episode reward: 41.198, mean reward:  1.585 [-2.424, 32.090], mean action: 2.885 [0.000, 11.000],  loss: 0.021697, mae: 0.360895, mean_q: 0.560002, mean_eps: 0.000000
 3508/5000: episode: 131, duration: 0.424s, episode steps:  26, steps per second:  61, episode reward: 41.739, mean reward:  1.605 [-2.321, 32.154], mean action: 1.769 [0.000, 9.000],  loss: 0.017191, mae: 0.339892, mean_q: 0.536171, mean_eps: 0.000000
 3535/5000: episode: 132, duration: 0.700s, episode steps:  27, steps per second:  39, episode reward: 41.720, mean reward:  1.545 [-2.641, 32.110], mean action: 2.852 [0.000, 16.000],  loss: 0.019342, mae: 0.357307, mean_q: 0.519820, mean_eps: 0.000000
 3573/5000: episode: 133, duration: 0.555s, episode steps:  38, steps per second:  68, episode reward: 43.509, mean reward:  1.145 [-2.217, 32.080], mean action: 3.605 [1.000, 14.000],  loss: 0.020403, mae: 0.352852, mean_q: 0.526799, mean_eps: 0.000000
 3618/5000: episode: 134, duration: 0.660s, episode steps:  45, steps per second:  68, episode reward: 41.026, mean reward:  0.912 [-2.320, 32.233], mean action: 1.356 [0.000, 16.000],  loss: 0.019941, mae: 0.352951, mean_q: 0.542419, mean_eps: 0.000000
 3641/5000: episode: 135, duration: 0.337s, episode steps:  23, steps per second:  68, episode reward: 38.559, mean reward:  1.676 [-2.614, 32.310], mean action: 3.435 [0.000, 16.000],  loss: 0.025620, mae: 0.379033, mean_q: 0.599627, mean_eps: 0.000000
 3667/5000: episode: 136, duration: 0.429s, episode steps:  26, steps per second:  61, episode reward: 41.282, mean reward:  1.588 [-2.413, 32.110], mean action: 2.346 [0.000, 15.000],  loss: 0.017822, mae: 0.344613, mean_q: 0.583152, mean_eps: 0.000000
 3699/5000: episode: 137, duration: 0.493s, episode steps:  32, steps per second:  65, episode reward: 40.963, mean reward:  1.280 [-2.160, 29.449], mean action: 3.469 [0.000, 15.000],  loss: 0.021917, mae: 0.368359, mean_q: 0.593153, mean_eps: 0.000000
 3727/5000: episode: 138, duration: 0.612s, episode steps:  28, steps per second:  46, episode reward: 38.752, mean reward:  1.384 [-3.000, 32.130], mean action: 2.893 [0.000, 15.000],  loss: 0.021669, mae: 0.372884, mean_q: 0.549014, mean_eps: 0.000000
 3747/5000: episode: 139, duration: 0.407s, episode steps:  20, steps per second:  49, episode reward: 41.547, mean reward:  2.077 [-3.000, 32.170], mean action: 2.850 [1.000, 15.000],  loss: 0.021197, mae: 0.361792, mean_q: 0.500986, mean_eps: 0.000000
 3772/5000: episode: 140, duration: 0.500s, episode steps:  25, steps per second:  50, episode reward: 39.000, mean reward:  1.560 [-2.475, 29.375], mean action: 5.200 [0.000, 15.000],  loss: 0.017246, mae: 0.343030, mean_q: 0.531800, mean_eps: 0.000000
 3802/5000: episode: 141, duration: 0.424s, episode steps:  30, steps per second:  71, episode reward: 42.000, mean reward:  1.400 [-2.048, 32.270], mean action: 2.933 [0.000, 15.000],  loss: 0.018973, mae: 0.348460, mean_q: 0.516891, mean_eps: 0.000000
 3828/5000: episode: 142, duration: 0.370s, episode steps:  26, steps per second:  70, episode reward: 41.661, mean reward:  1.602 [-3.000, 32.270], mean action: 3.154 [0.000, 15.000],  loss: 0.020604, mae: 0.362737, mean_q: 0.497520, mean_eps: 0.000000
 3858/5000: episode: 143, duration: 0.432s, episode steps:  30, steps per second:  69, episode reward: 38.333, mean reward:  1.278 [-2.903, 32.560], mean action: 3.933 [0.000, 15.000],  loss: 0.024203, mae: 0.380169, mean_q: 0.503248, mean_eps: 0.000000
 3886/5000: episode: 144, duration: 0.395s, episode steps:  28, steps per second:  71, episode reward: 40.502, mean reward:  1.447 [-2.661, 31.580], mean action: 1.107 [0.000, 9.000],  loss: 0.021272, mae: 0.371196, mean_q: 0.489088, mean_eps: 0.000000
 3915/5000: episode: 145, duration: 0.541s, episode steps:  29, steps per second:  54, episode reward: 38.700, mean reward:  1.334 [-2.525, 32.243], mean action: 3.966 [0.000, 12.000],  loss: 0.018584, mae: 0.350537, mean_q: 0.543030, mean_eps: 0.000000
 3955/5000: episode: 146, duration: 0.574s, episode steps:  40, steps per second:  70, episode reward: 37.767, mean reward:  0.944 [-2.174, 32.006], mean action: 3.150 [0.000, 15.000],  loss: 0.018949, mae: 0.358238, mean_q: 0.510146, mean_eps: 0.000000
 3977/5000: episode: 147, duration: 0.345s, episode steps:  22, steps per second:  64, episode reward: 41.765, mean reward:  1.898 [-2.340, 32.600], mean action: 3.818 [0.000, 19.000],  loss: 0.021641, mae: 0.372555, mean_q: 0.457511, mean_eps: 0.000000
 4003/5000: episode: 148, duration: 0.502s, episode steps:  26, steps per second:  52, episode reward: 44.336, mean reward:  1.705 [-2.159, 32.110], mean action: 0.538 [0.000, 6.000],  loss: 0.020807, mae: 0.353771, mean_q: 0.536523, mean_eps: 0.000000
 4025/5000: episode: 149, duration: 0.612s, episode steps:  22, steps per second:  36, episode reward: 44.702, mean reward:  2.032 [-2.038, 31.909], mean action: 3.409 [3.000, 12.000],  loss: 0.018340, mae: 0.351152, mean_q: 0.519954, mean_eps: 0.000000
 4054/5000: episode: 150, duration: 0.435s, episode steps:  29, steps per second:  67, episode reward: 46.860, mean reward:  1.616 [-0.167, 31.708], mean action: 2.586 [0.000, 12.000],  loss: 0.019495, mae: 0.352547, mean_q: 0.511999, mean_eps: 0.000000
 4079/5000: episode: 151, duration: 0.369s, episode steps:  25, steps per second:  68, episode reward: 41.042, mean reward:  1.642 [-2.193, 32.640], mean action: 3.760 [0.000, 12.000],  loss: 0.022355, mae: 0.364737, mean_q: 0.552964, mean_eps: 0.000000
 4102/5000: episode: 152, duration: 0.322s, episode steps:  23, steps per second:  71, episode reward: 41.576, mean reward:  1.808 [-2.613, 32.660], mean action: 4.174 [0.000, 14.000],  loss: 0.019371, mae: 0.355131, mean_q: 0.569909, mean_eps: 0.000000
 4139/5000: episode: 153, duration: 0.538s, episode steps:  37, steps per second:  69, episode reward: 33.000, mean reward:  0.892 [-3.000, 32.100], mean action: 2.216 [0.000, 12.000],  loss: 0.019812, mae: 0.346549, mean_q: 0.568409, mean_eps: 0.000000
 4157/5000: episode: 154, duration: 0.261s, episode steps:  18, steps per second:  69, episode reward: 42.000, mean reward:  2.333 [-2.434, 32.040], mean action: 3.167 [0.000, 12.000],  loss: 0.017116, mae: 0.341216, mean_q: 0.531586, mean_eps: 0.000000
 4200/5000: episode: 155, duration: 0.590s, episode steps:  43, steps per second:  73, episode reward: 38.441, mean reward:  0.894 [-2.387, 32.470], mean action: 2.581 [0.000, 20.000],  loss: 0.018861, mae: 0.356078, mean_q: 0.541817, mean_eps: 0.000000
 4242/5000: episode: 156, duration: 0.582s, episode steps:  42, steps per second:  72, episode reward: 32.664, mean reward:  0.778 [-2.277, 32.300], mean action: 7.881 [0.000, 20.000],  loss: 0.020239, mae: 0.369878, mean_q: 0.488135, mean_eps: 0.000000
 4296/5000: episode: 157, duration: 0.761s, episode steps:  54, steps per second:  71, episode reward: 41.596, mean reward:  0.770 [-2.206, 32.250], mean action: 1.315 [0.000, 12.000],  loss: 0.018947, mae: 0.357793, mean_q: 0.497116, mean_eps: 0.000000
 4320/5000: episode: 158, duration: 0.339s, episode steps:  24, steps per second:  71, episode reward: 39.000, mean reward:  1.625 [-3.000, 32.520], mean action: 4.083 [0.000, 16.000],  loss: 0.016384, mae: 0.353421, mean_q: 0.528470, mean_eps: 0.000000
 4339/5000: episode: 159, duration: 0.281s, episode steps:  19, steps per second:  68, episode reward: 46.652, mean reward:  2.455 [-0.449, 32.330], mean action: 1.158 [0.000, 8.000],  loss: 0.025657, mae: 0.388877, mean_q: 0.559840, mean_eps: 0.000000
 4358/5000: episode: 160, duration: 0.279s, episode steps:  19, steps per second:  68, episode reward: 41.397, mean reward:  2.179 [-2.558, 31.986], mean action: 3.526 [0.000, 14.000],  loss: 0.022097, mae: 0.375969, mean_q: 0.519766, mean_eps: 0.000000
 4375/5000: episode: 161, duration: 0.263s, episode steps:  17, steps per second:  65, episode reward: 45.000, mean reward:  2.647 [-2.215, 32.180], mean action: 1.882 [0.000, 9.000],  loss: 0.017905, mae: 0.356715, mean_q: 0.488060, mean_eps: 0.000000
 4421/5000: episode: 162, duration: 0.627s, episode steps:  46, steps per second:  73, episode reward: 38.157, mean reward:  0.829 [-2.175, 32.742], mean action: 4.043 [0.000, 16.000],  loss: 0.019555, mae: 0.360498, mean_q: 0.461907, mean_eps: 0.000000
 4443/5000: episode: 163, duration: 0.649s, episode steps:  22, steps per second:  34, episode reward: 38.762, mean reward:  1.762 [-3.000, 31.932], mean action: 3.455 [0.000, 15.000],  loss: 0.015316, mae: 0.338829, mean_q: 0.497115, mean_eps: 0.000000
 4478/5000: episode: 164, duration: 0.644s, episode steps:  35, steps per second:  54, episode reward: 47.116, mean reward:  1.346 [-0.665, 32.500], mean action: 1.714 [0.000, 13.000],  loss: 0.018649, mae: 0.359067, mean_q: 0.448653, mean_eps: 0.000000
 4508/5000: episode: 165, duration: 0.419s, episode steps:  30, steps per second:  72, episode reward: 41.114, mean reward:  1.370 [-3.000, 31.755], mean action: 2.200 [0.000, 12.000],  loss: 0.019975, mae: 0.356484, mean_q: 0.554187, mean_eps: 0.000000
 4533/5000: episode: 166, duration: 0.349s, episode steps:  25, steps per second:  72, episode reward: 36.000, mean reward:  1.440 [-3.000, 32.060], mean action: 4.240 [0.000, 15.000],  loss: 0.019967, mae: 0.361758, mean_q: 0.568613, mean_eps: 0.000000
 4553/5000: episode: 167, duration: 0.435s, episode steps:  20, steps per second:  46, episode reward: 44.880, mean reward:  2.244 [-2.325, 32.180], mean action: 2.550 [0.000, 15.000],  loss: 0.024589, mae: 0.377200, mean_q: 0.542515, mean_eps: 0.000000
 4577/5000: episode: 168, duration: 0.430s, episode steps:  24, steps per second:  56, episode reward: 38.786, mean reward:  1.616 [-2.178, 32.596], mean action: 2.750 [0.000, 19.000],  loss: 0.019792, mae: 0.362009, mean_q: 0.514721, mean_eps: 0.000000
 4594/5000: episode: 169, duration: 0.249s, episode steps:  17, steps per second:  68, episode reward: 44.428, mean reward:  2.613 [-3.000, 32.810], mean action: 3.647 [0.000, 19.000],  loss: 0.019910, mae: 0.366377, mean_q: 0.560410, mean_eps: 0.000000
 4622/5000: episode: 170, duration: 0.386s, episode steps:  28, steps per second:  73, episode reward: 36.000, mean reward:  1.286 [-2.829, 32.200], mean action: 6.107 [0.000, 19.000],  loss: 0.020756, mae: 0.367942, mean_q: 0.536175, mean_eps: 0.000000
 4645/5000: episode: 171, duration: 0.320s, episode steps:  23, steps per second:  72, episode reward: 42.000, mean reward:  1.826 [-2.598, 32.100], mean action: 3.217 [0.000, 19.000],  loss: 0.020843, mae: 0.360345, mean_q: 0.522087, mean_eps: 0.000000
 4684/5000: episode: 172, duration: 0.537s, episode steps:  39, steps per second:  73, episode reward: 32.654, mean reward:  0.837 [-3.000, 32.272], mean action: 3.564 [0.000, 16.000],  loss: 0.019404, mae: 0.353512, mean_q: 0.526121, mean_eps: 0.000000
 4712/5000: episode: 173, duration: 0.392s, episode steps:  28, steps per second:  71, episode reward: 41.173, mean reward:  1.470 [-2.370, 32.420], mean action: 3.857 [0.000, 16.000],  loss: 0.016796, mae: 0.338957, mean_q: 0.546991, mean_eps: 0.000000
 4753/5000: episode: 174, duration: 0.721s, episode steps:  41, steps per second:  57, episode reward: 41.059, mean reward:  1.001 [-2.044, 31.603], mean action: 5.659 [0.000, 20.000],  loss: 0.021715, mae: 0.362907, mean_q: 0.508450, mean_eps: 0.000000
 4794/5000: episode: 175, duration: 0.612s, episode steps:  41, steps per second:  67, episode reward: 35.939, mean reward:  0.877 [-3.000, 29.509], mean action: 3.902 [0.000, 16.000],  loss: 0.024033, mae: 0.372437, mean_q: 0.526403, mean_eps: 0.000000
 4809/5000: episode: 176, duration: 0.224s, episode steps:  15, steps per second:  67, episode reward: 41.690, mean reward:  2.779 [-2.233, 32.010], mean action: 1.933 [0.000, 12.000],  loss: 0.017524, mae: 0.349893, mean_q: 0.488028, mean_eps: 0.000000
 4835/5000: episode: 177, duration: 0.378s, episode steps:  26, steps per second:  69, episode reward: 41.346, mean reward:  1.590 [-2.728, 32.310], mean action: 6.462 [0.000, 15.000],  loss: 0.019518, mae: 0.354155, mean_q: 0.480446, mean_eps: 0.000000
 4892/5000: episode: 178, duration: 0.809s, episode steps:  57, steps per second:  70, episode reward: 32.133, mean reward:  0.564 [-3.000, 32.390], mean action: 6.579 [0.000, 13.000],  loss: 0.018873, mae: 0.351941, mean_q: 0.503707, mean_eps: 0.000000
 4931/5000: episode: 179, duration: 0.556s, episode steps:  39, steps per second:  70, episode reward: 38.690, mean reward:  0.992 [-2.252, 31.909], mean action: 3.128 [0.000, 14.000],  loss: 0.021409, mae: 0.370993, mean_q: 0.511093, mean_eps: 0.000000
 4954/5000: episode: 180, duration: 0.347s, episode steps:  23, steps per second:  66, episode reward: 44.246, mean reward:  1.924 [-2.330, 32.130], mean action: 2.130 [0.000, 16.000],  loss: 0.025262, mae: 0.383208, mean_q: 0.517001, mean_eps: 0.000000
 4978/5000: episode: 181, duration: 0.356s, episode steps:  24, steps per second:  67, episode reward: 47.269, mean reward:  1.970 [-0.420, 32.440], mean action: 2.250 [0.000, 12.000],  loss: 0.018292, mae: 0.341654, mean_q: 0.508706, mean_eps: 0.000000
done, took 80.237 seconds
DQN Evaluation: 6630 victories out of 7799 episodes
Training for 5000 steps ...
   17/5000: episode: 1, duration: 0.154s, episode steps:  17, steps per second: 111, episode reward: 38.801, mean reward:  2.282 [-2.770, 33.000], mean action: 3.059 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   40/5000: episode: 2, duration: 0.162s, episode steps:  23, steps per second: 142, episode reward: 37.874, mean reward:  1.647 [-2.456, 31.964], mean action: 5.652 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   65/5000: episode: 3, duration: 0.203s, episode steps:  25, steps per second: 123, episode reward: -36.000, mean reward: -1.440 [-30.309,  2.270], mean action: 6.280 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   93/5000: episode: 4, duration: 0.343s, episode steps:  28, steps per second:  82, episode reward: -38.070, mean reward: -1.360 [-32.524,  2.410], mean action: 2.786 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  119/5000: episode: 5, duration: 0.220s, episode steps:  26, steps per second: 118, episode reward: 35.413, mean reward:  1.362 [-2.632, 32.140], mean action: 5.308 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  139/5000: episode: 6, duration: 0.169s, episode steps:  20, steps per second: 118, episode reward: 35.900, mean reward:  1.795 [-3.000, 32.480], mean action: 4.400 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  161/5000: episode: 7, duration: 0.166s, episode steps:  22, steps per second: 133, episode reward: 35.343, mean reward:  1.606 [-3.000, 32.303], mean action: 6.182 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  177/5000: episode: 8, duration: 0.123s, episode steps:  16, steps per second: 130, episode reward: 38.903, mean reward:  2.431 [-2.549, 32.153], mean action: 5.500 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  198/5000: episode: 9, duration: 0.145s, episode steps:  21, steps per second: 145, episode reward: 32.615, mean reward:  1.553 [-3.000, 32.340], mean action: 3.667 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  221/5000: episode: 10, duration: 0.181s, episode steps:  23, steps per second: 127, episode reward: 38.039, mean reward:  1.654 [-2.502, 32.060], mean action: 2.609 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  245/5000: episode: 11, duration: 0.168s, episode steps:  24, steps per second: 143, episode reward: -32.540, mean reward: -1.356 [-31.780,  2.560], mean action: 2.792 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  265/5000: episode: 12, duration: 0.157s, episode steps:  20, steps per second: 128, episode reward: 36.000, mean reward:  1.800 [-2.262, 32.010], mean action: 5.250 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  281/5000: episode: 13, duration: 0.112s, episode steps:  16, steps per second: 143, episode reward: 36.000, mean reward:  2.250 [-3.000, 33.000], mean action: 3.375 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  316/5000: episode: 14, duration: 0.234s, episode steps:  35, steps per second: 150, episode reward: 37.837, mean reward:  1.081 [-2.472, 32.952], mean action: 4.829 [1.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  334/5000: episode: 15, duration: 0.126s, episode steps:  18, steps per second: 143, episode reward: 38.903, mean reward:  2.161 [-2.263, 32.843], mean action: 5.000 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  357/5000: episode: 16, duration: 0.140s, episode steps:  23, steps per second: 165, episode reward: 35.427, mean reward:  1.540 [-2.651, 32.320], mean action: 3.043 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  384/5000: episode: 17, duration: 0.178s, episode steps:  27, steps per second: 152, episode reward: -35.080, mean reward: -1.299 [-32.040,  3.000], mean action: 6.259 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  397/5000: episode: 18, duration: 0.099s, episode steps:  13, steps per second: 132, episode reward: 41.720, mean reward:  3.209 [-2.368, 32.360], mean action: 2.538 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  415/5000: episode: 19, duration: 0.131s, episode steps:  18, steps per second: 137, episode reward: 38.038, mean reward:  2.113 [-2.485, 32.570], mean action: 3.222 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  438/5000: episode: 20, duration: 0.150s, episode steps:  23, steps per second: 153, episode reward: 38.293, mean reward:  1.665 [-2.553, 32.910], mean action: 4.565 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  461/5000: episode: 21, duration: 0.155s, episode steps:  23, steps per second: 148, episode reward: 38.498, mean reward:  1.674 [-3.000, 31.598], mean action: 2.696 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  488/5000: episode: 22, duration: 0.175s, episode steps:  27, steps per second: 154, episode reward: -34.870, mean reward: -1.291 [-32.169,  2.677], mean action: 5.222 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  512/5000: episode: 23, duration: 0.168s, episode steps:  24, steps per second: 143, episode reward: 33.000, mean reward:  1.375 [-3.000, 32.160], mean action: 2.708 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  543/5000: episode: 24, duration: 0.203s, episode steps:  31, steps per second: 153, episode reward: -32.910, mean reward: -1.062 [-32.190,  2.412], mean action: 5.613 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  578/5000: episode: 25, duration: 0.223s, episode steps:  35, steps per second: 157, episode reward: 33.000, mean reward:  0.943 [-2.649, 32.210], mean action: 2.543 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  601/5000: episode: 26, duration: 0.155s, episode steps:  23, steps per second: 148, episode reward: 33.000, mean reward:  1.435 [-2.877, 32.570], mean action: 4.826 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  618/5000: episode: 27, duration: 0.120s, episode steps:  17, steps per second: 142, episode reward: 42.000, mean reward:  2.471 [-2.715, 32.040], mean action: 3.765 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  638/5000: episode: 28, duration: 0.138s, episode steps:  20, steps per second: 145, episode reward: 35.470, mean reward:  1.773 [-2.732, 31.796], mean action: 3.050 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  666/5000: episode: 29, duration: 0.185s, episode steps:  28, steps per second: 151, episode reward: 35.563, mean reward:  1.270 [-2.372, 32.053], mean action: 2.536 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  687/5000: episode: 30, duration: 0.145s, episode steps:  21, steps per second: 145, episode reward: 40.840, mean reward:  1.945 [-2.602, 31.968], mean action: 3.190 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  712/5000: episode: 31, duration: 0.165s, episode steps:  25, steps per second: 151, episode reward: 41.427, mean reward:  1.657 [-2.436, 32.033], mean action: 3.440 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  740/5000: episode: 32, duration: 0.180s, episode steps:  28, steps per second: 155, episode reward: 35.150, mean reward:  1.255 [-3.000, 31.916], mean action: 3.429 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  766/5000: episode: 33, duration: 0.173s, episode steps:  26, steps per second: 150, episode reward: -35.860, mean reward: -1.379 [-32.686,  3.000], mean action: 4.077 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  801/5000: episode: 34, duration: 0.209s, episode steps:  35, steps per second: 168, episode reward: 38.412, mean reward:  1.097 [-2.266, 31.532], mean action: 1.829 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  821/5000: episode: 35, duration: 0.134s, episode steps:  20, steps per second: 150, episode reward: -38.820, mean reward: -1.941 [-32.407,  2.500], mean action: 7.500 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  846/5000: episode: 36, duration: 0.163s, episode steps:  25, steps per second: 153, episode reward: 38.626, mean reward:  1.545 [-3.000, 32.190], mean action: 2.360 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  868/5000: episode: 37, duration: 0.158s, episode steps:  22, steps per second: 139, episode reward: 37.238, mean reward:  1.693 [-2.288, 32.128], mean action: 2.364 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  897/5000: episode: 38, duration: 0.190s, episode steps:  29, steps per second: 153, episode reward: 37.872, mean reward:  1.306 [-2.349, 32.710], mean action: 6.345 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  914/5000: episode: 39, duration: 0.115s, episode steps:  17, steps per second: 148, episode reward: 38.701, mean reward:  2.277 [-2.260, 32.900], mean action: 3.000 [1.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  932/5000: episode: 40, duration: 0.130s, episode steps:  18, steps per second: 138, episode reward: -38.120, mean reward: -2.118 [-32.244,  2.224], mean action: 4.000 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  956/5000: episode: 41, duration: 0.157s, episode steps:  24, steps per second: 153, episode reward: 32.761, mean reward:  1.365 [-3.000, 32.319], mean action: 3.458 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  970/5000: episode: 42, duration: 0.103s, episode steps:  14, steps per second: 136, episode reward: 41.393, mean reward:  2.957 [-2.546, 31.765], mean action: 3.357 [1.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  991/5000: episode: 43, duration: 0.143s, episode steps:  21, steps per second: 146, episode reward: 39.000, mean reward:  1.857 [-2.310, 32.150], mean action: 4.905 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1017/5000: episode: 44, duration: 0.297s, episode steps:  26, steps per second:  88, episode reward: 36.000, mean reward:  1.385 [-3.000, 32.310], mean action: 2.000 [0.000, 12.000],  loss: 0.017586, mae: 0.345441, mean_q: 0.485264, mean_eps: 0.000000
 1037/5000: episode: 45, duration: 0.297s, episode steps:  20, steps per second:  67, episode reward: -35.270, mean reward: -1.763 [-31.669,  3.000], mean action: 5.000 [0.000, 19.000],  loss: 0.020563, mae: 0.353754, mean_q: 0.556618, mean_eps: 0.000000
 1056/5000: episode: 46, duration: 0.286s, episode steps:  19, steps per second:  67, episode reward: 38.177, mean reward:  2.009 [-2.213, 32.116], mean action: 5.684 [0.000, 19.000],  loss: 0.022440, mae: 0.364864, mean_q: 0.569648, mean_eps: 0.000000
 1077/5000: episode: 47, duration: 0.312s, episode steps:  21, steps per second:  67, episode reward: 37.934, mean reward:  1.806 [-2.416, 32.812], mean action: 3.571 [0.000, 11.000],  loss: 0.020546, mae: 0.362019, mean_q: 0.485313, mean_eps: 0.000000
 1098/5000: episode: 48, duration: 0.312s, episode steps:  21, steps per second:  67, episode reward: 35.378, mean reward:  1.685 [-2.885, 31.658], mean action: 3.286 [0.000, 11.000],  loss: 0.021808, mae: 0.369571, mean_q: 0.491389, mean_eps: 0.000000
 1130/5000: episode: 49, duration: 0.456s, episode steps:  32, steps per second:  70, episode reward: 32.500, mean reward:  1.016 [-2.256, 31.995], mean action: 5.375 [0.000, 14.000],  loss: 0.023969, mae: 0.375137, mean_q: 0.540617, mean_eps: 0.000000
 1148/5000: episode: 50, duration: 0.257s, episode steps:  18, steps per second:  70, episode reward: 36.000, mean reward:  2.000 [-3.000, 32.120], mean action: 4.722 [0.000, 14.000],  loss: 0.019904, mae: 0.352239, mean_q: 0.577237, mean_eps: 0.000000
 1183/5000: episode: 51, duration: 0.495s, episode steps:  35, steps per second:  71, episode reward: 35.600, mean reward:  1.017 [-3.000, 32.040], mean action: 3.486 [0.000, 19.000],  loss: 0.019474, mae: 0.356817, mean_q: 0.501229, mean_eps: 0.000000
 1212/5000: episode: 52, duration: 0.411s, episode steps:  29, steps per second:  71, episode reward: -32.550, mean reward: -1.122 [-32.479,  2.900], mean action: 3.862 [0.000, 15.000],  loss: 0.020344, mae: 0.363871, mean_q: 0.504270, mean_eps: 0.000000
 1237/5000: episode: 53, duration: 0.366s, episode steps:  25, steps per second:  68, episode reward: -35.010, mean reward: -1.400 [-31.652,  2.642], mean action: 3.680 [0.000, 12.000],  loss: 0.026234, mae: 0.384754, mean_q: 0.509367, mean_eps: 0.000000
 1253/5000: episode: 54, duration: 0.235s, episode steps:  16, steps per second:  68, episode reward: 41.157, mean reward:  2.572 [-2.168, 32.157], mean action: 3.125 [0.000, 12.000],  loss: 0.027154, mae: 0.387037, mean_q: 0.572556, mean_eps: 0.000000
 1273/5000: episode: 55, duration: 0.288s, episode steps:  20, steps per second:  69, episode reward: 36.000, mean reward:  1.800 [-2.475, 32.690], mean action: 2.400 [0.000, 11.000],  loss: 0.018855, mae: 0.347690, mean_q: 0.553085, mean_eps: 0.000000
 1299/5000: episode: 56, duration: 0.362s, episode steps:  26, steps per second:  72, episode reward: -35.260, mean reward: -1.356 [-32.168,  2.463], mean action: 6.038 [0.000, 19.000],  loss: 0.018887, mae: 0.350394, mean_q: 0.500209, mean_eps: 0.000000
 1328/5000: episode: 57, duration: 0.427s, episode steps:  29, steps per second:  68, episode reward: 34.689, mean reward:  1.196 [-2.407, 32.595], mean action: 4.931 [1.000, 13.000],  loss: 0.022313, mae: 0.368495, mean_q: 0.503329, mean_eps: 0.000000
 1348/5000: episode: 58, duration: 0.293s, episode steps:  20, steps per second:  68, episode reward: 35.568, mean reward:  1.778 [-2.548, 32.032], mean action: 4.100 [0.000, 15.000],  loss: 0.017332, mae: 0.338733, mean_q: 0.534066, mean_eps: 0.000000
 1367/5000: episode: 59, duration: 0.280s, episode steps:  19, steps per second:  68, episode reward: 39.000, mean reward:  2.053 [-2.353, 32.700], mean action: 3.000 [0.000, 15.000],  loss: 0.022074, mae: 0.363515, mean_q: 0.546377, mean_eps: 0.000000
 1386/5000: episode: 60, duration: 0.289s, episode steps:  19, steps per second:  66, episode reward: 38.126, mean reward:  2.007 [-2.610, 32.901], mean action: 3.632 [0.000, 15.000],  loss: 0.017172, mae: 0.336857, mean_q: 0.539728, mean_eps: 0.000000
 1413/5000: episode: 61, duration: 0.379s, episode steps:  27, steps per second:  71, episode reward: -39.000, mean reward: -1.444 [-32.251,  2.381], mean action: 2.889 [0.000, 11.000],  loss: 0.020180, mae: 0.359840, mean_q: 0.511576, mean_eps: 0.000000
 1430/5000: episode: 62, duration: 0.245s, episode steps:  17, steps per second:  69, episode reward: 41.662, mean reward:  2.451 [-2.357, 32.170], mean action: 3.000 [0.000, 12.000],  loss: 0.015846, mae: 0.339591, mean_q: 0.527771, mean_eps: 0.000000
 1452/5000: episode: 63, duration: 0.306s, episode steps:  22, steps per second:  72, episode reward: 35.219, mean reward:  1.601 [-3.000, 31.980], mean action: 3.682 [0.000, 16.000],  loss: 0.020461, mae: 0.359439, mean_q: 0.567136, mean_eps: 0.000000
 1486/5000: episode: 64, duration: 0.722s, episode steps:  34, steps per second:  47, episode reward: -32.140, mean reward: -0.945 [-32.223,  3.000], mean action: 6.971 [0.000, 20.000],  loss: 0.019329, mae: 0.352318, mean_q: 0.507655, mean_eps: 0.000000
 1517/5000: episode: 65, duration: 0.454s, episode steps:  31, steps per second:  68, episode reward: 32.402, mean reward:  1.045 [-2.724, 32.250], mean action: 4.161 [0.000, 13.000],  loss: 0.018129, mae: 0.349584, mean_q: 0.483434, mean_eps: 0.000000
 1535/5000: episode: 66, duration: 0.274s, episode steps:  18, steps per second:  66, episode reward: 35.281, mean reward:  1.960 [-2.737, 32.233], mean action: 4.333 [0.000, 14.000],  loss: 0.021330, mae: 0.360722, mean_q: 0.541569, mean_eps: 0.000000
 1551/5000: episode: 67, duration: 0.236s, episode steps:  16, steps per second:  68, episode reward: 41.893, mean reward:  2.618 [-2.849, 33.000], mean action: 3.188 [0.000, 11.000],  loss: 0.016611, mae: 0.339452, mean_q: 0.538778, mean_eps: 0.000000
 1568/5000: episode: 68, duration: 0.242s, episode steps:  17, steps per second:  70, episode reward: -38.570, mean reward: -2.269 [-32.930,  2.306], mean action: 6.000 [0.000, 16.000],  loss: 0.020901, mae: 0.367899, mean_q: 0.596150, mean_eps: 0.000000
 1600/5000: episode: 69, duration: 0.461s, episode steps:  32, steps per second:  69, episode reward: -38.130, mean reward: -1.192 [-32.062,  2.517], mean action: 6.156 [0.000, 14.000],  loss: 0.018319, mae: 0.352715, mean_q: 0.532350, mean_eps: 0.000000
 1617/5000: episode: 70, duration: 0.251s, episode steps:  17, steps per second:  68, episode reward: 38.526, mean reward:  2.266 [-3.000, 32.113], mean action: 4.235 [0.000, 12.000],  loss: 0.017858, mae: 0.356735, mean_q: 0.497624, mean_eps: 0.000000
 1637/5000: episode: 71, duration: 0.310s, episode steps:  20, steps per second:  65, episode reward: 38.258, mean reward:  1.913 [-2.911, 32.450], mean action: 2.350 [0.000, 12.000],  loss: 0.019440, mae: 0.366485, mean_q: 0.464905, mean_eps: 0.000000
 1653/5000: episode: 72, duration: 0.234s, episode steps:  16, steps per second:  68, episode reward: 41.471, mean reward:  2.592 [-2.518, 32.150], mean action: 1.938 [0.000, 12.000],  loss: 0.021863, mae: 0.369116, mean_q: 0.512353, mean_eps: 0.000000
 1675/5000: episode: 73, duration: 0.311s, episode steps:  22, steps per second:  71, episode reward: 38.419, mean reward:  1.746 [-2.490, 32.547], mean action: 4.500 [0.000, 19.000],  loss: 0.019971, mae: 0.356995, mean_q: 0.532677, mean_eps: 0.000000
 1695/5000: episode: 74, duration: 0.329s, episode steps:  20, steps per second:  61, episode reward: 38.762, mean reward:  1.938 [-2.123, 32.762], mean action: 4.250 [0.000, 14.000],  loss: 0.018920, mae: 0.350124, mean_q: 0.515107, mean_eps: 0.000000
 1715/5000: episode: 75, duration: 0.291s, episode steps:  20, steps per second:  69, episode reward: 40.737, mean reward:  2.037 [-2.302, 32.310], mean action: 4.700 [0.000, 14.000],  loss: 0.019581, mae: 0.353541, mean_q: 0.507664, mean_eps: 0.000000
 1747/5000: episode: 76, duration: 0.494s, episode steps:  32, steps per second:  65, episode reward: 35.483, mean reward:  1.109 [-2.896, 32.040], mean action: 2.531 [0.000, 19.000],  loss: 0.020216, mae: 0.352082, mean_q: 0.565111, mean_eps: 0.000000
 1767/5000: episode: 77, duration: 0.352s, episode steps:  20, steps per second:  57, episode reward: -38.080, mean reward: -1.904 [-32.130,  2.365], mean action: 7.600 [0.000, 20.000],  loss: 0.020981, mae: 0.364398, mean_q: 0.564886, mean_eps: 0.000000
 1782/5000: episode: 78, duration: 0.311s, episode steps:  15, steps per second:  48, episode reward: 41.400, mean reward:  2.760 [-2.198, 32.273], mean action: 2.467 [0.000, 9.000],  loss: 0.018190, mae: 0.357538, mean_q: 0.576819, mean_eps: 0.000000
 1806/5000: episode: 79, duration: 0.343s, episode steps:  24, steps per second:  70, episode reward: 33.000, mean reward:  1.375 [-2.545, 32.160], mean action: 3.208 [0.000, 16.000],  loss: 0.023141, mae: 0.375358, mean_q: 0.541406, mean_eps: 0.000000
 1828/5000: episode: 80, duration: 0.329s, episode steps:  22, steps per second:  67, episode reward: -38.550, mean reward: -1.752 [-32.062,  2.464], mean action: 7.227 [0.000, 16.000],  loss: 0.019558, mae: 0.356267, mean_q: 0.572403, mean_eps: 0.000000
 1844/5000: episode: 81, duration: 0.274s, episode steps:  16, steps per second:  58, episode reward: 39.000, mean reward:  2.438 [-3.000, 32.140], mean action: 4.438 [0.000, 16.000],  loss: 0.019560, mae: 0.361464, mean_q: 0.579131, mean_eps: 0.000000
 1867/5000: episode: 82, duration: 0.363s, episode steps:  23, steps per second:  63, episode reward: 37.666, mean reward:  1.638 [-3.000, 32.798], mean action: 3.435 [0.000, 16.000],  loss: 0.023404, mae: 0.378135, mean_q: 0.542790, mean_eps: 0.000000
 1885/5000: episode: 83, duration: 0.276s, episode steps:  18, steps per second:  65, episode reward: 35.329, mean reward:  1.963 [-2.508, 30.901], mean action: 4.889 [0.000, 19.000],  loss: 0.025086, mae: 0.385758, mean_q: 0.528199, mean_eps: 0.000000
 1905/5000: episode: 84, duration: 0.321s, episode steps:  20, steps per second:  62, episode reward: 36.000, mean reward:  1.800 [-3.000, 32.290], mean action: 4.000 [0.000, 15.000],  loss: 0.022807, mae: 0.379584, mean_q: 0.553199, mean_eps: 0.000000
 1930/5000: episode: 85, duration: 0.401s, episode steps:  25, steps per second:  62, episode reward: -32.480, mean reward: -1.299 [-32.341,  2.280], mean action: 4.320 [0.000, 19.000],  loss: 0.020462, mae: 0.363211, mean_q: 0.595885, mean_eps: 0.000000
 1956/5000: episode: 86, duration: 0.360s, episode steps:  26, steps per second:  72, episode reward: 35.655, mean reward:  1.371 [-3.000, 32.630], mean action: 2.500 [0.000, 12.000],  loss: 0.014751, mae: 0.341166, mean_q: 0.530536, mean_eps: 0.000000
 1979/5000: episode: 87, duration: 0.329s, episode steps:  23, steps per second:  70, episode reward: 35.394, mean reward:  1.539 [-3.000, 32.242], mean action: 4.130 [0.000, 19.000],  loss: 0.018825, mae: 0.351252, mean_q: 0.538064, mean_eps: 0.000000
 1997/5000: episode: 88, duration: 0.283s, episode steps:  18, steps per second:  64, episode reward: 41.319, mean reward:  2.295 [-2.197, 32.188], mean action: 3.333 [0.000, 19.000],  loss: 0.022704, mae: 0.376663, mean_q: 0.486915, mean_eps: 0.000000
 2022/5000: episode: 89, duration: 0.365s, episode steps:  25, steps per second:  68, episode reward: 32.702, mean reward:  1.308 [-3.000, 32.160], mean action: 7.000 [0.000, 19.000],  loss: 0.016566, mae: 0.345854, mean_q: 0.441249, mean_eps: 0.000000
 2043/5000: episode: 90, duration: 0.319s, episode steps:  21, steps per second:  66, episode reward: 41.441, mean reward:  1.973 [-2.403, 31.892], mean action: 3.667 [0.000, 19.000],  loss: 0.017929, mae: 0.349500, mean_q: 0.465366, mean_eps: 0.000000
 2064/5000: episode: 91, duration: 0.305s, episode steps:  21, steps per second:  69, episode reward: 40.544, mean reward:  1.931 [-2.300, 32.390], mean action: 3.190 [0.000, 19.000],  loss: 0.021240, mae: 0.365575, mean_q: 0.484091, mean_eps: 0.000000
 2086/5000: episode: 92, duration: 0.315s, episode steps:  22, steps per second:  70, episode reward: 36.000, mean reward:  1.636 [-2.106, 32.260], mean action: 5.773 [0.000, 20.000],  loss: 0.018870, mae: 0.356498, mean_q: 0.544390, mean_eps: 0.000000
 2102/5000: episode: 93, duration: 0.250s, episode steps:  16, steps per second:  64, episode reward: 44.471, mean reward:  2.779 [-2.112, 33.000], mean action: 1.875 [0.000, 9.000],  loss: 0.018873, mae: 0.360471, mean_q: 0.536698, mean_eps: 0.000000
 2126/5000: episode: 94, duration: 0.351s, episode steps:  24, steps per second:  68, episode reward: 38.475, mean reward:  1.603 [-3.000, 32.047], mean action: 4.833 [0.000, 13.000],  loss: 0.017859, mae: 0.356184, mean_q: 0.484228, mean_eps: 0.000000
 2150/5000: episode: 95, duration: 0.347s, episode steps:  24, steps per second:  69, episode reward: -32.420, mean reward: -1.351 [-32.285,  2.820], mean action: 3.708 [0.000, 19.000],  loss: 0.017607, mae: 0.356579, mean_q: 0.550007, mean_eps: 0.000000
 2165/5000: episode: 96, duration: 0.222s, episode steps:  15, steps per second:  67, episode reward: 39.000, mean reward:  2.600 [-3.000, 33.000], mean action: 4.133 [0.000, 19.000],  loss: 0.023722, mae: 0.386721, mean_q: 0.515207, mean_eps: 0.000000
 2193/5000: episode: 97, duration: 0.408s, episode steps:  28, steps per second:  69, episode reward: 32.157, mean reward:  1.148 [-2.206, 32.167], mean action: 5.464 [0.000, 19.000],  loss: 0.016658, mae: 0.357881, mean_q: 0.457059, mean_eps: 0.000000
 2228/5000: episode: 98, duration: 0.480s, episode steps:  35, steps per second:  73, episode reward: 32.696, mean reward:  0.934 [-3.000, 31.726], mean action: 3.886 [0.000, 19.000],  loss: 0.019050, mae: 0.364323, mean_q: 0.448935, mean_eps: 0.000000
 2252/5000: episode: 99, duration: 0.386s, episode steps:  24, steps per second:  62, episode reward: 33.000, mean reward:  1.375 [-2.667, 32.630], mean action: 6.333 [0.000, 18.000],  loss: 0.017053, mae: 0.346508, mean_q: 0.485775, mean_eps: 0.000000
 2340/5000: episode: 100, duration: 1.587s, episode steps:  88, steps per second:  55, episode reward: -32.450, mean reward: -0.369 [-32.070,  2.152], mean action: 10.693 [0.000, 21.000],  loss: 0.020342, mae: 0.359441, mean_q: 0.526510, mean_eps: 0.000000
 2358/5000: episode: 101, duration: 0.266s, episode steps:  18, steps per second:  68, episode reward: 35.627, mean reward:  1.979 [-3.000, 31.997], mean action: 4.111 [0.000, 16.000],  loss: 0.019066, mae: 0.355212, mean_q: 0.515392, mean_eps: 0.000000
 2371/5000: episode: 102, duration: 0.200s, episode steps:  13, steps per second:  65, episode reward: 41.633, mean reward:  3.203 [-2.354, 33.000], mean action: 3.615 [0.000, 16.000],  loss: 0.023202, mae: 0.378750, mean_q: 0.577951, mean_eps: 0.000000
 2394/5000: episode: 103, duration: 0.334s, episode steps:  23, steps per second:  69, episode reward: 38.120, mean reward:  1.657 [-2.528, 32.201], mean action: 3.217 [0.000, 16.000],  loss: 0.022118, mae: 0.364789, mean_q: 0.598341, mean_eps: 0.000000
 2423/5000: episode: 104, duration: 0.418s, episode steps:  29, steps per second:  69, episode reward: -33.000, mean reward: -1.138 [-32.332,  2.470], mean action: 5.103 [0.000, 17.000],  loss: 0.018308, mae: 0.353638, mean_q: 0.586153, mean_eps: 0.000000
 2433/5000: episode: 105, duration: 0.160s, episode steps:  10, steps per second:  62, episode reward: 44.167, mean reward:  4.417 [-2.387, 31.716], mean action: 3.700 [0.000, 12.000],  loss: 0.022843, mae: 0.385696, mean_q: 0.518093, mean_eps: 0.000000
 2476/5000: episode: 106, duration: 0.690s, episode steps:  43, steps per second:  62, episode reward: 35.470, mean reward:  0.825 [-2.482, 31.660], mean action: 2.326 [0.000, 12.000],  loss: 0.019166, mae: 0.361009, mean_q: 0.515762, mean_eps: 0.000000
 2498/5000: episode: 107, duration: 0.345s, episode steps:  22, steps per second:  64, episode reward: 35.972, mean reward:  1.635 [-2.472, 33.000], mean action: 3.000 [0.000, 12.000],  loss: 0.021163, mae: 0.379498, mean_q: 0.500951, mean_eps: 0.000000
 2528/5000: episode: 108, duration: 0.454s, episode steps:  30, steps per second:  66, episode reward: 35.540, mean reward:  1.185 [-2.382, 32.525], mean action: 5.167 [1.000, 19.000],  loss: 0.023096, mae: 0.375430, mean_q: 0.551362, mean_eps: 0.000000
 2555/5000: episode: 109, duration: 0.493s, episode steps:  27, steps per second:  55, episode reward: 32.579, mean reward:  1.207 [-3.000, 32.537], mean action: 3.889 [0.000, 19.000],  loss: 0.023330, mae: 0.374295, mean_q: 0.530338, mean_eps: 0.000000
 2579/5000: episode: 110, duration: 0.374s, episode steps:  24, steps per second:  64, episode reward: 32.711, mean reward:  1.363 [-3.000, 32.384], mean action: 3.708 [0.000, 19.000],  loss: 0.017596, mae: 0.342168, mean_q: 0.482173, mean_eps: 0.000000
 2594/5000: episode: 111, duration: 0.218s, episode steps:  15, steps per second:  69, episode reward: 38.255, mean reward:  2.550 [-2.272, 32.902], mean action: 4.400 [0.000, 19.000],  loss: 0.017834, mae: 0.345900, mean_q: 0.507564, mean_eps: 0.000000
 2621/5000: episode: 112, duration: 0.401s, episode steps:  27, steps per second:  67, episode reward: 32.119, mean reward:  1.190 [-2.566, 32.430], mean action: 4.148 [0.000, 19.000],  loss: 0.022397, mae: 0.367591, mean_q: 0.527966, mean_eps: 0.000000
 2643/5000: episode: 113, duration: 0.303s, episode steps:  22, steps per second:  73, episode reward: -36.000, mean reward: -1.636 [-32.196,  2.538], mean action: 6.909 [0.000, 19.000],  loss: 0.019954, mae: 0.354505, mean_q: 0.528004, mean_eps: 0.000000
 2670/5000: episode: 114, duration: 0.449s, episode steps:  27, steps per second:  60, episode reward: 35.048, mean reward:  1.298 [-2.419, 31.900], mean action: 6.778 [0.000, 19.000],  loss: 0.021922, mae: 0.362502, mean_q: 0.553759, mean_eps: 0.000000
 2700/5000: episode: 115, duration: 0.458s, episode steps:  30, steps per second:  66, episode reward: 33.000, mean reward:  1.100 [-3.000, 32.020], mean action: 5.933 [0.000, 15.000],  loss: 0.022418, mae: 0.369971, mean_q: 0.526106, mean_eps: 0.000000
 2723/5000: episode: 116, duration: 0.354s, episode steps:  23, steps per second:  65, episode reward: -32.100, mean reward: -1.396 [-32.132,  2.903], mean action: 5.087 [0.000, 15.000],  loss: 0.022640, mae: 0.366759, mean_q: 0.565407, mean_eps: 0.000000
 2744/5000: episode: 117, duration: 0.323s, episode steps:  21, steps per second:  65, episode reward: -35.260, mean reward: -1.679 [-32.308,  2.280], mean action: 3.476 [0.000, 11.000],  loss: 0.022066, mae: 0.360189, mean_q: 0.538847, mean_eps: 0.000000
 2770/5000: episode: 118, duration: 0.419s, episode steps:  26, steps per second:  62, episode reward: 39.000, mean reward:  1.500 [-2.575, 32.200], mean action: 1.577 [0.000, 11.000],  loss: 0.022673, mae: 0.368275, mean_q: 0.473966, mean_eps: 0.000000
 2791/5000: episode: 119, duration: 0.315s, episode steps:  21, steps per second:  67, episode reward: -35.830, mean reward: -1.706 [-32.410,  2.530], mean action: 3.095 [0.000, 9.000],  loss: 0.020248, mae: 0.356184, mean_q: 0.481838, mean_eps: 0.000000
 2805/5000: episode: 120, duration: 0.292s, episode steps:  14, steps per second:  48, episode reward: 44.104, mean reward:  3.150 [-2.413, 32.090], mean action: 1.429 [0.000, 9.000],  loss: 0.019903, mae: 0.344470, mean_q: 0.493810, mean_eps: 0.000000
 2838/5000: episode: 121, duration: 0.695s, episode steps:  33, steps per second:  47, episode reward: 41.109, mean reward:  1.246 [-2.301, 32.102], mean action: 2.939 [0.000, 12.000],  loss: 0.019644, mae: 0.345383, mean_q: 0.501511, mean_eps: 0.000000
 2866/5000: episode: 122, duration: 0.475s, episode steps:  28, steps per second:  59, episode reward: -32.150, mean reward: -1.148 [-32.113,  2.720], mean action: 4.714 [0.000, 20.000],  loss: 0.020407, mae: 0.349026, mean_q: 0.547496, mean_eps: 0.000000
 2887/5000: episode: 123, duration: 0.743s, episode steps:  21, steps per second:  28, episode reward: -33.000, mean reward: -1.571 [-32.719,  2.280], mean action: 5.714 [1.000, 16.000],  loss: 0.016340, mae: 0.335368, mean_q: 0.537303, mean_eps: 0.000000
 2911/5000: episode: 124, duration: 0.483s, episode steps:  24, steps per second:  50, episode reward: 35.395, mean reward:  1.475 [-2.240, 32.760], mean action: 3.750 [0.000, 16.000],  loss: 0.018996, mae: 0.352522, mean_q: 0.518519, mean_eps: 0.000000
 2930/5000: episode: 125, duration: 0.551s, episode steps:  19, steps per second:  34, episode reward: 39.000, mean reward:  2.053 [-2.436, 32.140], mean action: 3.947 [1.000, 16.000],  loss: 0.019083, mae: 0.361206, mean_q: 0.479803, mean_eps: 0.000000
 2954/5000: episode: 126, duration: 0.690s, episode steps:  24, steps per second:  35, episode reward: 32.378, mean reward:  1.349 [-2.576, 32.400], mean action: 6.000 [0.000, 19.000],  loss: 0.020991, mae: 0.370178, mean_q: 0.467269, mean_eps: 0.000000
 2977/5000: episode: 127, duration: 0.399s, episode steps:  23, steps per second:  58, episode reward: -35.120, mean reward: -1.527 [-32.610,  2.838], mean action: 7.826 [0.000, 19.000],  loss: 0.017521, mae: 0.353302, mean_q: 0.495922, mean_eps: 0.000000
 2988/5000: episode: 128, duration: 0.180s, episode steps:  11, steps per second:  61, episode reward: 44.500, mean reward:  4.045 [-2.134, 33.000], mean action: 4.273 [0.000, 19.000],  loss: 0.026990, mae: 0.390441, mean_q: 0.550566, mean_eps: 0.000000
 3018/5000: episode: 129, duration: 0.850s, episode steps:  30, steps per second:  35, episode reward: 32.498, mean reward:  1.083 [-3.000, 31.898], mean action: 4.467 [0.000, 12.000],  loss: 0.018156, mae: 0.357369, mean_q: 0.529300, mean_eps: 0.000000
 3040/5000: episode: 130, duration: 0.593s, episode steps:  22, steps per second:  37, episode reward: -32.900, mean reward: -1.495 [-32.900,  2.903], mean action: 5.864 [0.000, 16.000],  loss: 0.020668, mae: 0.368682, mean_q: 0.543941, mean_eps: 0.000000
 3062/5000: episode: 131, duration: 0.340s, episode steps:  22, steps per second:  65, episode reward: 39.000, mean reward:  1.773 [-3.000, 32.180], mean action: 3.955 [0.000, 16.000],  loss: 0.018387, mae: 0.347367, mean_q: 0.575955, mean_eps: 0.000000
 3078/5000: episode: 132, duration: 0.277s, episode steps:  16, steps per second:  58, episode reward: 38.167, mean reward:  2.385 [-2.370, 32.062], mean action: 4.875 [0.000, 16.000],  loss: 0.021002, mae: 0.356674, mean_q: 0.571795, mean_eps: 0.000000
 3100/5000: episode: 133, duration: 0.325s, episode steps:  22, steps per second:  68, episode reward: 35.755, mean reward:  1.625 [-2.509, 32.570], mean action: 6.227 [0.000, 16.000],  loss: 0.016304, mae: 0.349265, mean_q: 0.542327, mean_eps: 0.000000
 3126/5000: episode: 134, duration: 0.421s, episode steps:  26, steps per second:  62, episode reward: -32.690, mean reward: -1.257 [-32.070,  2.714], mean action: 4.615 [0.000, 16.000],  loss: 0.023200, mae: 0.374070, mean_q: 0.547360, mean_eps: 0.000000
 3148/5000: episode: 135, duration: 0.796s, episode steps:  22, steps per second:  28, episode reward: 38.109, mean reward:  1.732 [-2.162, 32.629], mean action: 4.045 [0.000, 16.000],  loss: 0.019736, mae: 0.365205, mean_q: 0.511997, mean_eps: 0.000000
 3173/5000: episode: 136, duration: 0.430s, episode steps:  25, steps per second:  58, episode reward: 35.841, mean reward:  1.434 [-2.634, 32.603], mean action: 7.120 [0.000, 19.000],  loss: 0.018915, mae: 0.361789, mean_q: 0.568045, mean_eps: 0.000000
 3198/5000: episode: 137, duration: 0.572s, episode steps:  25, steps per second:  44, episode reward: 43.798, mean reward:  1.752 [-2.084, 32.901], mean action: 3.880 [0.000, 19.000],  loss: 0.020393, mae: 0.356959, mean_q: 0.560814, mean_eps: 0.000000
 3226/5000: episode: 138, duration: 0.610s, episode steps:  28, steps per second:  46, episode reward: -36.000, mean reward: -1.286 [-32.097,  2.310], mean action: 7.464 [0.000, 19.000],  loss: 0.020520, mae: 0.360658, mean_q: 0.555286, mean_eps: 0.000000
 3242/5000: episode: 139, duration: 0.489s, episode steps:  16, steps per second:  33, episode reward: 41.504, mean reward:  2.594 [-2.327, 32.320], mean action: 4.125 [1.000, 19.000],  loss: 0.015947, mae: 0.337214, mean_q: 0.529094, mean_eps: 0.000000
 3256/5000: episode: 140, duration: 0.280s, episode steps:  14, steps per second:  50, episode reward: 44.717, mean reward:  3.194 [-2.353, 32.050], mean action: 3.357 [0.000, 19.000],  loss: 0.017739, mae: 0.349908, mean_q: 0.539833, mean_eps: 0.000000
 3279/5000: episode: 141, duration: 0.730s, episode steps:  23, steps per second:  32, episode reward: 32.573, mean reward:  1.416 [-3.000, 32.310], mean action: 4.435 [0.000, 19.000],  loss: 0.018446, mae: 0.355273, mean_q: 0.509795, mean_eps: 0.000000
 3301/5000: episode: 142, duration: 0.528s, episode steps:  22, steps per second:  42, episode reward: -36.000, mean reward: -1.636 [-32.063,  2.359], mean action: 10.227 [1.000, 21.000],  loss: 0.021882, mae: 0.373284, mean_q: 0.497228, mean_eps: 0.000000
 3332/5000: episode: 143, duration: 0.995s, episode steps:  31, steps per second:  31, episode reward: -35.590, mean reward: -1.148 [-32.033,  2.535], mean action: 11.774 [0.000, 21.000],  loss: 0.020692, mae: 0.360545, mean_q: 0.528919, mean_eps: 0.000000
 3359/5000: episode: 144, duration: 0.450s, episode steps:  27, steps per second:  60, episode reward: -32.230, mean reward: -1.194 [-32.026,  2.510], mean action: 6.370 [0.000, 20.000],  loss: 0.019232, mae: 0.350585, mean_q: 0.604936, mean_eps: 0.000000
 3376/5000: episode: 145, duration: 0.284s, episode steps:  17, steps per second:  60, episode reward: 38.076, mean reward:  2.240 [-3.000, 32.280], mean action: 6.000 [0.000, 16.000],  loss: 0.025739, mae: 0.384995, mean_q: 0.562851, mean_eps: 0.000000
 3393/5000: episode: 146, duration: 0.271s, episode steps:  17, steps per second:  63, episode reward: 38.614, mean reward:  2.271 [-2.312, 31.984], mean action: 4.118 [0.000, 16.000],  loss: 0.017638, mae: 0.346890, mean_q: 0.515693, mean_eps: 0.000000
 3411/5000: episode: 147, duration: 0.360s, episode steps:  18, steps per second:  50, episode reward: 38.948, mean reward:  2.164 [-3.000, 32.410], mean action: 4.833 [0.000, 16.000],  loss: 0.018576, mae: 0.349883, mean_q: 0.463683, mean_eps: 0.000000
 3436/5000: episode: 148, duration: 0.422s, episode steps:  25, steps per second:  59, episode reward: -32.910, mean reward: -1.316 [-32.717,  2.240], mean action: 3.920 [0.000, 11.000],  loss: 0.020161, mae: 0.362324, mean_q: 0.408935, mean_eps: 0.000000
 3451/5000: episode: 149, duration: 0.343s, episode steps:  15, steps per second:  44, episode reward: 38.901, mean reward:  2.593 [-2.451, 32.331], mean action: 4.533 [0.000, 15.000],  loss: 0.020534, mae: 0.361075, mean_q: 0.477136, mean_eps: 0.000000
 3472/5000: episode: 150, duration: 0.354s, episode steps:  21, steps per second:  59, episode reward: 44.573, mean reward:  2.123 [-2.093, 32.093], mean action: 1.857 [0.000, 16.000],  loss: 0.022519, mae: 0.364633, mean_q: 0.508519, mean_eps: 0.000000
 3491/5000: episode: 151, duration: 0.324s, episode steps:  19, steps per second:  59, episode reward: 38.969, mean reward:  2.051 [-3.000, 32.479], mean action: 6.684 [0.000, 16.000],  loss: 0.023015, mae: 0.360337, mean_q: 0.515583, mean_eps: 0.000000
 3519/5000: episode: 152, duration: 0.395s, episode steps:  28, steps per second:  71, episode reward: -30.000, mean reward: -1.071 [-30.203,  2.420], mean action: 6.179 [0.000, 20.000],  loss: 0.020527, mae: 0.353590, mean_q: 0.463547, mean_eps: 0.000000
 3537/5000: episode: 153, duration: 0.550s, episode steps:  18, steps per second:  33, episode reward: -35.940, mean reward: -1.997 [-33.000,  2.640], mean action: 4.389 [0.000, 16.000],  loss: 0.017098, mae: 0.337034, mean_q: 0.457851, mean_eps: 0.000000
 3564/5000: episode: 154, duration: 0.650s, episode steps:  27, steps per second:  42, episode reward: 38.514, mean reward:  1.426 [-2.544, 32.213], mean action: 6.259 [0.000, 16.000],  loss: 0.019700, mae: 0.351649, mean_q: 0.478017, mean_eps: 0.000000
 3586/5000: episode: 155, duration: 0.320s, episode steps:  22, steps per second:  69, episode reward: 36.000, mean reward:  1.636 [-3.000, 32.210], mean action: 4.318 [0.000, 15.000],  loss: 0.021781, mae: 0.359542, mean_q: 0.556147, mean_eps: 0.000000
 3617/5000: episode: 156, duration: 0.440s, episode steps:  31, steps per second:  70, episode reward: 41.032, mean reward:  1.324 [-2.261, 32.051], mean action: 3.839 [2.000, 15.000],  loss: 0.022835, mae: 0.358423, mean_q: 0.545610, mean_eps: 0.000000
 3635/5000: episode: 157, duration: 0.309s, episode steps:  18, steps per second:  58, episode reward: 38.710, mean reward:  2.151 [-2.135, 33.000], mean action: 4.667 [0.000, 20.000],  loss: 0.018121, mae: 0.339510, mean_q: 0.525072, mean_eps: 0.000000
 3697/5000: episode: 158, duration: 0.920s, episode steps:  62, steps per second:  67, episode reward: -33.000, mean reward: -0.532 [-32.162,  2.320], mean action: 4.306 [0.000, 16.000],  loss: 0.020988, mae: 0.353587, mean_q: 0.459152, mean_eps: 0.000000
 3723/5000: episode: 159, duration: 0.380s, episode steps:  26, steps per second:  68, episode reward: 35.905, mean reward:  1.381 [-2.367, 31.955], mean action: 2.962 [0.000, 11.000],  loss: 0.022175, mae: 0.360215, mean_q: 0.508852, mean_eps: 0.000000
 3747/5000: episode: 160, duration: 0.366s, episode steps:  24, steps per second:  65, episode reward: 35.316, mean reward:  1.471 [-2.685, 31.506], mean action: 4.083 [0.000, 21.000],  loss: 0.020664, mae: 0.349244, mean_q: 0.554597, mean_eps: 0.000000
 3766/5000: episode: 161, duration: 0.325s, episode steps:  19, steps per second:  59, episode reward: 38.642, mean reward:  2.034 [-2.665, 32.021], mean action: 5.000 [0.000, 14.000],  loss: 0.018749, mae: 0.348340, mean_q: 0.507781, mean_eps: 0.000000
 3794/5000: episode: 162, duration: 0.482s, episode steps:  28, steps per second:  58, episode reward: 39.000, mean reward:  1.393 [-2.251, 32.110], mean action: 2.321 [1.000, 12.000],  loss: 0.019704, mae: 0.351280, mean_q: 0.544744, mean_eps: 0.000000
 3813/5000: episode: 163, duration: 0.354s, episode steps:  19, steps per second:  54, episode reward: 35.418, mean reward:  1.864 [-3.000, 32.776], mean action: 3.632 [0.000, 12.000],  loss: 0.022983, mae: 0.368115, mean_q: 0.509188, mean_eps: 0.000000
 3837/5000: episode: 164, duration: 0.363s, episode steps:  24, steps per second:  66, episode reward: -32.650, mean reward: -1.360 [-31.800,  2.390], mean action: 4.792 [0.000, 19.000],  loss: 0.021581, mae: 0.360291, mean_q: 0.504353, mean_eps: 0.000000
 3862/5000: episode: 165, duration: 0.485s, episode steps:  25, steps per second:  52, episode reward: 32.417, mean reward:  1.297 [-2.447, 31.477], mean action: 7.760 [0.000, 21.000],  loss: 0.018473, mae: 0.348327, mean_q: 0.496942, mean_eps: 0.000000
 3893/5000: episode: 166, duration: 0.573s, episode steps:  31, steps per second:  54, episode reward: -32.120, mean reward: -1.036 [-32.110,  2.550], mean action: 3.871 [0.000, 19.000],  loss: 0.021931, mae: 0.359880, mean_q: 0.525023, mean_eps: 0.000000
 3920/5000: episode: 167, duration: 0.388s, episode steps:  27, steps per second:  70, episode reward: 35.327, mean reward:  1.308 [-3.000, 32.140], mean action: 6.000 [0.000, 14.000],  loss: 0.020491, mae: 0.340629, mean_q: 0.532364, mean_eps: 0.000000
 3931/5000: episode: 168, duration: 0.164s, episode steps:  11, steps per second:  67, episode reward: 42.000, mean reward:  3.818 [-3.000, 33.000], mean action: 2.818 [0.000, 9.000],  loss: 0.018533, mae: 0.328823, mean_q: 0.487497, mean_eps: 0.000000
 3953/5000: episode: 169, duration: 0.314s, episode steps:  22, steps per second:  70, episode reward: -39.000, mean reward: -1.773 [-32.061,  2.810], mean action: 7.227 [0.000, 20.000],  loss: 0.019162, mae: 0.339660, mean_q: 0.526784, mean_eps: 0.000000
 3979/5000: episode: 170, duration: 0.709s, episode steps:  26, steps per second:  37, episode reward: -32.180, mean reward: -1.238 [-31.637,  2.191], mean action: 6.577 [0.000, 21.000],  loss: 0.021600, mae: 0.350884, mean_q: 0.540480, mean_eps: 0.000000
 3997/5000: episode: 171, duration: 0.305s, episode steps:  18, steps per second:  59, episode reward: 38.318, mean reward:  2.129 [-2.727, 32.200], mean action: 4.667 [0.000, 18.000],  loss: 0.020928, mae: 0.349186, mean_q: 0.510166, mean_eps: 0.000000
 4023/5000: episode: 172, duration: 0.811s, episode steps:  26, steps per second:  32, episode reward: -35.820, mean reward: -1.378 [-32.171,  2.900], mean action: 12.115 [0.000, 19.000],  loss: 0.019990, mae: 0.339840, mean_q: 0.503218, mean_eps: 0.000000
 4043/5000: episode: 173, duration: 0.367s, episode steps:  20, steps per second:  54, episode reward: 38.817, mean reward:  1.941 [-3.000, 33.000], mean action: 3.150 [0.000, 16.000],  loss: 0.016291, mae: 0.322681, mean_q: 0.480067, mean_eps: 0.000000
 4056/5000: episode: 174, duration: 0.207s, episode steps:  13, steps per second:  63, episode reward: 41.725, mean reward:  3.210 [-3.000, 32.500], mean action: 3.231 [0.000, 16.000],  loss: 0.019725, mae: 0.338393, mean_q: 0.478393, mean_eps: 0.000000
 4076/5000: episode: 175, duration: 0.297s, episode steps:  20, steps per second:  67, episode reward: 38.518, mean reward:  1.926 [-2.903, 32.872], mean action: 4.600 [0.000, 16.000],  loss: 0.026891, mae: 0.369221, mean_q: 0.511575, mean_eps: 0.000000
 4177/5000: episode: 176, duration: 1.602s, episode steps: 101, steps per second:  63, episode reward: -32.380, mean reward: -0.321 [-31.987,  3.000], mean action: 5.238 [0.000, 19.000],  loss: 0.020823, mae: 0.346249, mean_q: 0.510096, mean_eps: 0.000000
 4198/5000: episode: 177, duration: 0.298s, episode steps:  21, steps per second:  70, episode reward: -32.800, mean reward: -1.562 [-32.139,  3.000], mean action: 4.095 [0.000, 17.000],  loss: 0.020691, mae: 0.347759, mean_q: 0.530336, mean_eps: 0.000000
 4218/5000: episode: 178, duration: 0.295s, episode steps:  20, steps per second:  68, episode reward: -35.510, mean reward: -1.775 [-32.510,  2.070], mean action: 4.650 [0.000, 16.000],  loss: 0.022177, mae: 0.355411, mean_q: 0.541379, mean_eps: 0.000000
 4243/5000: episode: 179, duration: 0.382s, episode steps:  25, steps per second:  65, episode reward: -35.730, mean reward: -1.429 [-32.342,  2.681], mean action: 7.800 [0.000, 17.000],  loss: 0.017720, mae: 0.339731, mean_q: 0.555622, mean_eps: 0.000000
 4271/5000: episode: 180, duration: 0.571s, episode steps:  28, steps per second:  49, episode reward: 32.534, mean reward:  1.162 [-2.462, 32.171], mean action: 5.393 [1.000, 16.000],  loss: 0.021443, mae: 0.350786, mean_q: 0.491151, mean_eps: 0.000000
 4294/5000: episode: 181, duration: 0.480s, episode steps:  23, steps per second:  48, episode reward: -32.440, mean reward: -1.410 [-32.276,  2.969], mean action: 5.739 [1.000, 19.000],  loss: 0.021108, mae: 0.350327, mean_q: 0.502515, mean_eps: 0.000000
 4315/5000: episode: 182, duration: 0.364s, episode steps:  21, steps per second:  58, episode reward: 35.731, mean reward:  1.701 [-3.000, 32.731], mean action: 7.238 [0.000, 19.000],  loss: 0.019125, mae: 0.345104, mean_q: 0.509402, mean_eps: 0.000000
 4338/5000: episode: 183, duration: 0.739s, episode steps:  23, steps per second:  31, episode reward: 34.344, mean reward:  1.493 [-2.752, 33.000], mean action: 4.696 [0.000, 15.000],  loss: 0.021330, mae: 0.356828, mean_q: 0.580741, mean_eps: 0.000000
 4361/5000: episode: 184, duration: 0.361s, episode steps:  23, steps per second:  64, episode reward: -33.000, mean reward: -1.435 [-30.081,  3.000], mean action: 5.696 [0.000, 15.000],  loss: 0.021442, mae: 0.351755, mean_q: 0.562722, mean_eps: 0.000000
 4387/5000: episode: 185, duration: 0.472s, episode steps:  26, steps per second:  55, episode reward: 33.000, mean reward:  1.269 [-3.000, 33.000], mean action: 6.154 [0.000, 17.000],  loss: 0.020422, mae: 0.346953, mean_q: 0.552023, mean_eps: 0.000000
 4408/5000: episode: 186, duration: 0.304s, episode steps:  21, steps per second:  69, episode reward: -35.700, mean reward: -1.700 [-32.355,  2.831], mean action: 10.571 [1.000, 18.000],  loss: 0.021500, mae: 0.353909, mean_q: 0.524646, mean_eps: 0.000000
 4429/5000: episode: 187, duration: 0.331s, episode steps:  21, steps per second:  63, episode reward: 38.453, mean reward:  1.831 [-2.554, 30.944], mean action: 5.571 [0.000, 14.000],  loss: 0.020193, mae: 0.344796, mean_q: 0.460803, mean_eps: 0.000000
 4443/5000: episode: 188, duration: 0.234s, episode steps:  14, steps per second:  60, episode reward: 45.000, mean reward:  3.214 [-2.072, 32.540], mean action: 2.071 [0.000, 12.000],  loss: 0.020899, mae: 0.347288, mean_q: 0.485776, mean_eps: 0.000000
 4473/5000: episode: 189, duration: 0.441s, episode steps:  30, steps per second:  68, episode reward: 35.641, mean reward:  1.188 [-2.628, 32.070], mean action: 4.267 [0.000, 19.000],  loss: 0.019767, mae: 0.344635, mean_q: 0.504990, mean_eps: 0.000000
 4495/5000: episode: 190, duration: 0.332s, episode steps:  22, steps per second:  66, episode reward: -32.820, mean reward: -1.492 [-31.856,  2.290], mean action: 6.273 [0.000, 15.000],  loss: 0.020609, mae: 0.343772, mean_q: 0.475212, mean_eps: 0.000000
 4517/5000: episode: 191, duration: 0.327s, episode steps:  22, steps per second:  67, episode reward: 38.339, mean reward:  1.743 [-2.668, 32.300], mean action: 2.545 [0.000, 15.000],  loss: 0.020246, mae: 0.345368, mean_q: 0.525200, mean_eps: 0.000000
 4535/5000: episode: 192, duration: 0.274s, episode steps:  18, steps per second:  66, episode reward: 41.498, mean reward:  2.305 [-2.275, 32.320], mean action: 2.333 [0.000, 15.000],  loss: 0.019295, mae: 0.334975, mean_q: 0.510130, mean_eps: 0.000000
 4561/5000: episode: 193, duration: 0.412s, episode steps:  26, steps per second:  63, episode reward: -32.840, mean reward: -1.263 [-32.335,  2.570], mean action: 6.962 [1.000, 17.000],  loss: 0.023217, mae: 0.353623, mean_q: 0.529113, mean_eps: 0.000000
 4586/5000: episode: 194, duration: 0.377s, episode steps:  25, steps per second:  66, episode reward: 41.066, mean reward:  1.643 [-2.577, 32.280], mean action: 4.880 [0.000, 20.000],  loss: 0.022878, mae: 0.354009, mean_q: 0.582617, mean_eps: 0.000000
 4606/5000: episode: 195, duration: 0.289s, episode steps:  20, steps per second:  69, episode reward: 39.000, mean reward:  1.950 [-2.207, 32.230], mean action: 2.450 [0.000, 16.000],  loss: 0.021100, mae: 0.355940, mean_q: 0.567527, mean_eps: 0.000000
 4625/5000: episode: 196, duration: 0.280s, episode steps:  19, steps per second:  68, episode reward: 44.426, mean reward:  2.338 [-2.111, 32.071], mean action: 3.316 [1.000, 19.000],  loss: 0.017570, mae: 0.341796, mean_q: 0.552778, mean_eps: 0.000000
 4643/5000: episode: 197, duration: 0.293s, episode steps:  18, steps per second:  61, episode reward: 39.000, mean reward:  2.167 [-2.298, 32.200], mean action: 3.333 [0.000, 19.000],  loss: 0.022387, mae: 0.366391, mean_q: 0.539711, mean_eps: 0.000000
 4673/5000: episode: 198, duration: 0.445s, episode steps:  30, steps per second:  67, episode reward: 35.533, mean reward:  1.184 [-2.474, 32.113], mean action: 4.367 [0.000, 19.000],  loss: 0.021763, mae: 0.361050, mean_q: 0.544103, mean_eps: 0.000000
 4702/5000: episode: 199, duration: 0.421s, episode steps:  29, steps per second:  69, episode reward: -32.300, mean reward: -1.114 [-32.088,  2.552], mean action: 6.690 [0.000, 14.000],  loss: 0.017053, mae: 0.340719, mean_q: 0.538743, mean_eps: 0.000000
 4729/5000: episode: 200, duration: 0.382s, episode steps:  27, steps per second:  71, episode reward: 32.841, mean reward:  1.216 [-3.000, 32.274], mean action: 5.037 [0.000, 17.000],  loss: 0.023370, mae: 0.361693, mean_q: 0.504577, mean_eps: 0.000000
 4762/5000: episode: 201, duration: 0.501s, episode steps:  33, steps per second:  66, episode reward: 32.271, mean reward:  0.978 [-3.000, 32.150], mean action: 7.818 [1.000, 14.000],  loss: 0.018956, mae: 0.350338, mean_q: 0.505048, mean_eps: 0.000000
 4784/5000: episode: 202, duration: 0.485s, episode steps:  22, steps per second:  45, episode reward: 32.937, mean reward:  1.497 [-2.745, 32.327], mean action: 5.091 [0.000, 14.000],  loss: 0.021078, mae: 0.361975, mean_q: 0.482133, mean_eps: 0.000000
 4800/5000: episode: 203, duration: 0.629s, episode steps:  16, steps per second:  25, episode reward: 38.219, mean reward:  2.389 [-3.000, 32.156], mean action: 3.438 [0.000, 12.000],  loss: 0.023341, mae: 0.367487, mean_q: 0.517630, mean_eps: 0.000000
 4829/5000: episode: 204, duration: 0.442s, episode steps:  29, steps per second:  66, episode reward: 36.000, mean reward:  1.241 [-3.000, 32.040], mean action: 6.448 [0.000, 17.000],  loss: 0.020087, mae: 0.352835, mean_q: 0.517342, mean_eps: 0.000000
 4852/5000: episode: 205, duration: 0.351s, episode steps:  23, steps per second:  65, episode reward: -38.420, mean reward: -1.670 [-32.264,  2.322], mean action: 5.174 [0.000, 15.000],  loss: 0.016348, mae: 0.334027, mean_q: 0.551440, mean_eps: 0.000000
 4873/5000: episode: 206, duration: 0.304s, episode steps:  21, steps per second:  69, episode reward: -32.450, mean reward: -1.545 [-32.276,  2.566], mean action: 8.048 [0.000, 15.000],  loss: 0.022961, mae: 0.371062, mean_q: 0.492736, mean_eps: 0.000000
 4913/5000: episode: 207, duration: 0.578s, episode steps:  40, steps per second:  69, episode reward: -38.160, mean reward: -0.954 [-32.191,  2.303], mean action: 13.875 [0.000, 20.000],  loss: 0.019526, mae: 0.358574, mean_q: 0.489766, mean_eps: 0.000000
 4937/5000: episode: 208, duration: 0.374s, episode steps:  24, steps per second:  64, episode reward: 32.997, mean reward:  1.375 [-3.000, 32.440], mean action: 5.042 [0.000, 15.000],  loss: 0.021495, mae: 0.363087, mean_q: 0.510748, mean_eps: 0.000000
 4956/5000: episode: 209, duration: 0.286s, episode steps:  19, steps per second:  67, episode reward: 35.178, mean reward:  1.851 [-3.000, 32.410], mean action: 4.368 [0.000, 16.000],  loss: 0.021075, mae: 0.355793, mean_q: 0.492522, mean_eps: 0.000000
 4974/5000: episode: 210, duration: 0.269s, episode steps:  18, steps per second:  67, episode reward: 40.028, mean reward:  2.224 [-2.902, 32.470], mean action: 6.111 [0.000, 12.000],  loss: 0.020413, mae: 0.357941, mean_q: 0.490860, mean_eps: 0.000000
 4997/5000: episode: 211, duration: 0.351s, episode steps:  23, steps per second:  66, episode reward: 35.364, mean reward:  1.538 [-2.289, 29.510], mean action: 5.348 [0.000, 19.000],  loss: 0.018326, mae: 0.352347, mean_q: 0.489128, mean_eps: 0.000000
done, took 75.975 seconds
DQN Evaluation: 6783 victories out of 8011 episodes
Training for 5000 steps ...
   18/5000: episode: 1, duration: 0.376s, episode steps:  18, steps per second:  48, episode reward: 45.000, mean reward:  2.500 [-2.077, 32.070], mean action: 2.944 [1.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   36/5000: episode: 2, duration: 0.138s, episode steps:  18, steps per second: 131, episode reward: 39.000, mean reward:  2.167 [-2.704, 32.320], mean action: 4.556 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   70/5000: episode: 3, duration: 0.230s, episode steps:  34, steps per second: 148, episode reward: 37.442, mean reward:  1.101 [-2.365, 32.027], mean action: 6.500 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  100/5000: episode: 4, duration: 0.263s, episode steps:  30, steps per second: 114, episode reward: 41.464, mean reward:  1.382 [-2.407, 32.441], mean action: 6.867 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  127/5000: episode: 5, duration: 0.168s, episode steps:  27, steps per second: 161, episode reward: 41.176, mean reward:  1.525 [-2.679, 32.300], mean action: 5.148 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  158/5000: episode: 6, duration: 0.234s, episode steps:  31, steps per second: 133, episode reward: 47.200, mean reward:  1.523 [-0.350, 32.260], mean action: 2.355 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  187/5000: episode: 7, duration: 0.204s, episode steps:  29, steps per second: 142, episode reward: 40.622, mean reward:  1.401 [-2.464, 32.110], mean action: 5.931 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  217/5000: episode: 8, duration: 0.209s, episode steps:  30, steps per second: 143, episode reward: 45.000, mean reward:  1.500 [-2.045, 32.130], mean action: 1.300 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  244/5000: episode: 9, duration: 0.207s, episode steps:  27, steps per second: 131, episode reward: 42.000, mean reward:  1.556 [-2.178, 32.280], mean action: 4.704 [1.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  270/5000: episode: 10, duration: 0.208s, episode steps:  26, steps per second: 125, episode reward: 40.825, mean reward:  1.570 [-2.664, 32.340], mean action: 3.846 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  307/5000: episode: 11, duration: 0.390s, episode steps:  37, steps per second:  95, episode reward: 41.259, mean reward:  1.115 [-2.417, 32.220], mean action: 4.568 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  323/5000: episode: 12, duration: 0.257s, episode steps:  16, steps per second:  62, episode reward: 42.000, mean reward:  2.625 [-3.000, 32.500], mean action: 3.375 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  354/5000: episode: 13, duration: 0.220s, episode steps:  31, steps per second: 141, episode reward: 32.231, mean reward:  1.040 [-3.000, 29.543], mean action: 4.839 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  375/5000: episode: 14, duration: 0.149s, episode steps:  21, steps per second: 141, episode reward: 41.729, mean reward:  1.987 [-2.514, 32.090], mean action: 2.524 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  410/5000: episode: 15, duration: 0.210s, episode steps:  35, steps per second: 166, episode reward: 38.754, mean reward:  1.107 [-3.000, 32.200], mean action: 4.257 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  429/5000: episode: 16, duration: 0.135s, episode steps:  19, steps per second: 141, episode reward: 42.000, mean reward:  2.211 [-2.122, 32.660], mean action: 4.421 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  445/5000: episode: 17, duration: 0.116s, episode steps:  16, steps per second: 138, episode reward: 41.361, mean reward:  2.585 [-2.260, 32.218], mean action: 3.438 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  478/5000: episode: 18, duration: 0.359s, episode steps:  33, steps per second:  92, episode reward: 41.129, mean reward:  1.246 [-2.363, 32.902], mean action: 3.515 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  493/5000: episode: 19, duration: 0.145s, episode steps:  15, steps per second: 103, episode reward: 43.417, mean reward:  2.894 [-2.039, 32.674], mean action: 5.000 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  530/5000: episode: 20, duration: 0.242s, episode steps:  37, steps per second: 153, episode reward: 41.361, mean reward:  1.118 [-2.285, 31.941], mean action: 2.162 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  631/5000: episode: 21, duration: 0.614s, episode steps: 101, steps per second: 164, episode reward: -32.360, mean reward: -0.320 [-31.956,  2.830], mean action: 6.248 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  654/5000: episode: 22, duration: 0.156s, episode steps:  23, steps per second: 147, episode reward: 41.161, mean reward:  1.790 [-2.335, 32.466], mean action: 3.435 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  691/5000: episode: 23, duration: 0.232s, episode steps:  37, steps per second: 160, episode reward: 35.835, mean reward:  0.969 [-3.000, 32.710], mean action: 6.514 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  714/5000: episode: 24, duration: 0.151s, episode steps:  23, steps per second: 152, episode reward: 35.587, mean reward:  1.547 [-3.000, 32.127], mean action: 4.609 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  737/5000: episode: 25, duration: 0.156s, episode steps:  23, steps per second: 147, episode reward: 41.144, mean reward:  1.789 [-2.239, 31.823], mean action: 2.304 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  772/5000: episode: 26, duration: 0.311s, episode steps:  35, steps per second: 112, episode reward: 33.000, mean reward:  0.943 [-2.761, 29.528], mean action: 7.400 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  786/5000: episode: 27, duration: 0.594s, episode steps:  14, steps per second:  24, episode reward: 46.269, mean reward:  3.305 [-0.878, 32.020], mean action: 4.214 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  817/5000: episode: 28, duration: 0.424s, episode steps:  31, steps per second:  73, episode reward: 40.367, mean reward:  1.302 [-2.712, 31.817], mean action: 5.452 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  842/5000: episode: 29, duration: 0.326s, episode steps:  25, steps per second:  77, episode reward: 44.668, mean reward:  1.787 [-2.207, 32.580], mean action: 2.400 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  880/5000: episode: 30, duration: 0.396s, episode steps:  38, steps per second:  96, episode reward: 35.058, mean reward:  0.923 [-3.000, 31.930], mean action: 4.184 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  908/5000: episode: 31, duration: 0.230s, episode steps:  28, steps per second: 122, episode reward: 37.768, mean reward:  1.349 [-2.494, 32.320], mean action: 7.214 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  938/5000: episode: 32, duration: 0.208s, episode steps:  30, steps per second: 144, episode reward: 41.384, mean reward:  1.379 [-2.115, 31.882], mean action: 2.867 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  963/5000: episode: 33, duration: 0.203s, episode steps:  25, steps per second: 123, episode reward: 41.373, mean reward:  1.655 [-2.288, 32.750], mean action: 4.240 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  987/5000: episode: 34, duration: 0.323s, episode steps:  24, steps per second:  74, episode reward: 44.038, mean reward:  1.835 [-2.027, 32.180], mean action: 4.000 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1012/5000: episode: 35, duration: 0.269s, episode steps:  25, steps per second:  93, episode reward: 38.506, mean reward:  1.540 [-2.477, 32.048], mean action: 3.440 [0.000, 16.000],  loss: 0.018065, mae: 0.351122, mean_q: 0.459008, mean_eps: 0.000000
 1036/5000: episode: 36, duration: 0.454s, episode steps:  24, steps per second:  53, episode reward: 35.681, mean reward:  1.487 [-3.000, 32.121], mean action: 4.875 [0.000, 16.000],  loss: 0.020358, mae: 0.350872, mean_q: 0.495130, mean_eps: 0.000000
 1053/5000: episode: 37, duration: 0.277s, episode steps:  17, steps per second:  61, episode reward: 42.000, mean reward:  2.471 [-3.000, 32.370], mean action: 3.176 [0.000, 16.000],  loss: 0.020007, mae: 0.351436, mean_q: 0.530368, mean_eps: 0.000000
 1076/5000: episode: 38, duration: 0.936s, episode steps:  23, steps per second:  25, episode reward: 41.940, mean reward:  1.823 [-2.563, 32.510], mean action: 3.435 [0.000, 16.000],  loss: 0.021135, mae: 0.348677, mean_q: 0.511872, mean_eps: 0.000000
 1098/5000: episode: 39, duration: 0.373s, episode steps:  22, steps per second:  59, episode reward: 41.157, mean reward:  1.871 [-2.102, 32.290], mean action: 3.455 [0.000, 16.000],  loss: 0.019079, mae: 0.347352, mean_q: 0.520160, mean_eps: 0.000000
 1122/5000: episode: 40, duration: 0.348s, episode steps:  24, steps per second:  69, episode reward: 44.612, mean reward:  1.859 [-2.570, 32.173], mean action: 2.583 [0.000, 16.000],  loss: 0.023281, mae: 0.368697, mean_q: 0.496629, mean_eps: 0.000000
 1147/5000: episode: 41, duration: 0.361s, episode steps:  25, steps per second:  69, episode reward: 47.436, mean reward:  1.897 [-0.087, 32.904], mean action: 1.800 [0.000, 12.000],  loss: 0.019746, mae: 0.349522, mean_q: 0.475899, mean_eps: 0.000000
 1184/5000: episode: 42, duration: 0.520s, episode steps:  37, steps per second:  71, episode reward: 41.119, mean reward:  1.111 [-2.391, 32.050], mean action: 1.324 [0.000, 16.000],  loss: 0.019172, mae: 0.355318, mean_q: 0.451797, mean_eps: 0.000000
 1220/5000: episode: 43, duration: 0.504s, episode steps:  36, steps per second:  71, episode reward: 47.620, mean reward:  1.323 [-0.060, 32.140], mean action: 1.111 [0.000, 14.000],  loss: 0.018847, mae: 0.348172, mean_q: 0.500806, mean_eps: 0.000000
 1238/5000: episode: 44, duration: 0.275s, episode steps:  18, steps per second:  65, episode reward: 44.817, mean reward:  2.490 [-0.030, 29.959], mean action: 2.278 [0.000, 12.000],  loss: 0.022450, mae: 0.377999, mean_q: 0.471048, mean_eps: 0.000000
 1258/5000: episode: 45, duration: 0.297s, episode steps:  20, steps per second:  67, episode reward: 41.338, mean reward:  2.067 [-2.419, 31.670], mean action: 3.100 [1.000, 16.000],  loss: 0.022008, mae: 0.368658, mean_q: 0.481710, mean_eps: 0.000000
 1277/5000: episode: 46, duration: 0.380s, episode steps:  19, steps per second:  50, episode reward: 44.098, mean reward:  2.321 [-2.635, 32.380], mean action: 5.895 [0.000, 13.000],  loss: 0.023594, mae: 0.368052, mean_q: 0.510423, mean_eps: 0.000000
 1303/5000: episode: 47, duration: 0.367s, episode steps:  26, steps per second:  71, episode reward: 33.000, mean reward:  1.269 [-2.941, 32.110], mean action: 4.885 [0.000, 16.000],  loss: 0.017059, mae: 0.343481, mean_q: 0.548560, mean_eps: 0.000000
 1333/5000: episode: 48, duration: 0.432s, episode steps:  30, steps per second:  69, episode reward: 40.825, mean reward:  1.361 [-2.435, 32.204], mean action: 4.733 [0.000, 16.000],  loss: 0.018794, mae: 0.353275, mean_q: 0.518401, mean_eps: 0.000000
 1384/5000: episode: 49, duration: 0.702s, episode steps:  51, steps per second:  73, episode reward: -32.730, mean reward: -0.642 [-32.414,  2.423], mean action: 7.137 [0.000, 19.000],  loss: 0.017984, mae: 0.349608, mean_q: 0.487681, mean_eps: 0.000000
 1400/5000: episode: 50, duration: 0.243s, episode steps:  16, steps per second:  66, episode reward: 47.219, mean reward:  2.951 [-0.291, 32.453], mean action: 2.188 [0.000, 12.000],  loss: 0.020890, mae: 0.360342, mean_q: 0.522226, mean_eps: 0.000000
 1438/5000: episode: 51, duration: 0.595s, episode steps:  38, steps per second:  64, episode reward: 35.621, mean reward:  0.937 [-3.000, 32.240], mean action: 3.921 [0.000, 16.000],  loss: 0.019496, mae: 0.351283, mean_q: 0.514473, mean_eps: 0.000000
 1460/5000: episode: 52, duration: 0.659s, episode steps:  22, steps per second:  33, episode reward: 35.842, mean reward:  1.629 [-2.938, 32.093], mean action: 4.818 [0.000, 19.000],  loss: 0.017640, mae: 0.348733, mean_q: 0.516478, mean_eps: 0.000000
 1497/5000: episode: 53, duration: 0.588s, episode steps:  37, steps per second:  63, episode reward: 38.420, mean reward:  1.038 [-2.893, 32.210], mean action: 2.730 [0.000, 16.000],  loss: 0.019435, mae: 0.358459, mean_q: 0.467755, mean_eps: 0.000000
 1518/5000: episode: 54, duration: 0.307s, episode steps:  21, steps per second:  68, episode reward: 41.897, mean reward:  1.995 [-2.206, 32.075], mean action: 4.238 [2.000, 16.000],  loss: 0.020339, mae: 0.356788, mean_q: 0.463955, mean_eps: 0.000000
 1533/5000: episode: 55, duration: 0.226s, episode steps:  15, steps per second:  66, episode reward: 44.473, mean reward:  2.965 [-2.232, 32.940], mean action: 3.467 [0.000, 12.000],  loss: 0.016477, mae: 0.335725, mean_q: 0.480167, mean_eps: 0.000000
 1555/5000: episode: 56, duration: 0.324s, episode steps:  22, steps per second:  68, episode reward: 41.545, mean reward:  1.888 [-2.287, 32.075], mean action: 3.545 [0.000, 12.000],  loss: 0.017577, mae: 0.338557, mean_q: 0.486706, mean_eps: 0.000000
 1576/5000: episode: 57, duration: 0.323s, episode steps:  21, steps per second:  65, episode reward: 41.877, mean reward:  1.994 [-3.000, 32.480], mean action: 2.286 [0.000, 12.000],  loss: 0.018929, mae: 0.346755, mean_q: 0.476291, mean_eps: 0.000000
 1601/5000: episode: 58, duration: 0.359s, episode steps:  25, steps per second:  70, episode reward: 38.081, mean reward:  1.523 [-2.511, 31.845], mean action: 2.520 [0.000, 9.000],  loss: 0.018488, mae: 0.348112, mean_q: 0.483649, mean_eps: 0.000000
 1633/5000: episode: 59, duration: 0.485s, episode steps:  32, steps per second:  66, episode reward: 38.614, mean reward:  1.207 [-3.000, 32.154], mean action: 2.312 [0.000, 9.000],  loss: 0.018410, mae: 0.345364, mean_q: 0.511956, mean_eps: 0.000000
 1658/5000: episode: 60, duration: 0.393s, episode steps:  25, steps per second:  64, episode reward: 47.811, mean reward:  1.912 [-0.216, 32.873], mean action: 0.880 [0.000, 2.000],  loss: 0.017968, mae: 0.339758, mean_q: 0.550340, mean_eps: 0.000000
 1688/5000: episode: 61, duration: 0.467s, episode steps:  30, steps per second:  64, episode reward: 43.947, mean reward:  1.465 [-2.285, 31.731], mean action: 3.500 [0.000, 9.000],  loss: 0.020083, mae: 0.352477, mean_q: 0.503877, mean_eps: 0.000000
 1728/5000: episode: 62, duration: 0.561s, episode steps:  40, steps per second:  71, episode reward: 38.388, mean reward:  0.960 [-2.890, 32.210], mean action: 3.825 [0.000, 15.000],  loss: 0.022081, mae: 0.363426, mean_q: 0.518916, mean_eps: 0.000000
 1750/5000: episode: 63, duration: 0.338s, episode steps:  22, steps per second:  65, episode reward: 44.218, mean reward:  2.010 [-2.863, 32.812], mean action: 1.636 [0.000, 9.000],  loss: 0.019046, mae: 0.350808, mean_q: 0.569234, mean_eps: 0.000000
 1777/5000: episode: 64, duration: 0.391s, episode steps:  27, steps per second:  69, episode reward: 35.553, mean reward:  1.317 [-2.238, 31.803], mean action: 2.741 [0.000, 19.000],  loss: 0.019645, mae: 0.345522, mean_q: 0.547189, mean_eps: 0.000000
 1801/5000: episode: 65, duration: 0.623s, episode steps:  24, steps per second:  39, episode reward: 40.344, mean reward:  1.681 [-3.000, 32.064], mean action: 3.083 [0.000, 12.000],  loss: 0.020884, mae: 0.346862, mean_q: 0.547815, mean_eps: 0.000000
 1827/5000: episode: 66, duration: 0.538s, episode steps:  26, steps per second:  48, episode reward: 41.135, mean reward:  1.582 [-3.000, 31.817], mean action: 3.538 [0.000, 13.000],  loss: 0.020765, mae: 0.351794, mean_q: 0.564994, mean_eps: 0.000000
 1858/5000: episode: 67, duration: 0.439s, episode steps:  31, steps per second:  71, episode reward: 41.441, mean reward:  1.337 [-2.440, 32.201], mean action: 3.290 [0.000, 16.000],  loss: 0.022022, mae: 0.353717, mean_q: 0.491553, mean_eps: 0.000000
 1888/5000: episode: 68, duration: 0.433s, episode steps:  30, steps per second:  69, episode reward: 47.187, mean reward:  1.573 [-0.177, 32.390], mean action: 2.533 [0.000, 19.000],  loss: 0.016928, mae: 0.338508, mean_q: 0.460108, mean_eps: 0.000000
 1910/5000: episode: 69, duration: 0.317s, episode steps:  22, steps per second:  69, episode reward: 44.818, mean reward:  2.037 [-2.119, 32.240], mean action: 2.773 [0.000, 14.000],  loss: 0.015545, mae: 0.326495, mean_q: 0.436322, mean_eps: 0.000000
 1938/5000: episode: 70, duration: 0.395s, episode steps:  28, steps per second:  71, episode reward: 41.445, mean reward:  1.480 [-2.642, 32.150], mean action: 2.357 [0.000, 12.000],  loss: 0.017965, mae: 0.333791, mean_q: 0.454393, mean_eps: 0.000000
 1961/5000: episode: 71, duration: 0.332s, episode steps:  23, steps per second:  69, episode reward: 41.464, mean reward:  1.803 [-2.596, 32.335], mean action: 2.957 [1.000, 9.000],  loss: 0.023173, mae: 0.349026, mean_q: 0.514225, mean_eps: 0.000000
 1987/5000: episode: 72, duration: 0.368s, episode steps:  26, steps per second:  71, episode reward: 42.000, mean reward:  1.615 [-2.376, 32.230], mean action: 4.115 [2.000, 15.000],  loss: 0.022488, mae: 0.355341, mean_q: 0.472072, mean_eps: 0.000000
 2008/5000: episode: 73, duration: 0.302s, episode steps:  21, steps per second:  70, episode reward: 46.387, mean reward:  2.209 [-0.847, 32.204], mean action: 5.095 [1.000, 15.000],  loss: 0.024910, mae: 0.365965, mean_q: 0.447752, mean_eps: 0.000000
 2028/5000: episode: 74, duration: 0.290s, episode steps:  20, steps per second:  69, episode reward: 41.833, mean reward:  2.092 [-3.000, 32.330], mean action: 4.600 [0.000, 13.000],  loss: 0.017118, mae: 0.334509, mean_q: 0.469072, mean_eps: 0.000000
 2049/5000: episode: 75, duration: 0.329s, episode steps:  21, steps per second:  64, episode reward: 47.208, mean reward:  2.248 [-0.340, 32.290], mean action: 2.857 [0.000, 12.000],  loss: 0.016737, mae: 0.331770, mean_q: 0.449129, mean_eps: 0.000000
 2078/5000: episode: 76, duration: 0.420s, episode steps:  29, steps per second:  69, episode reward: 38.809, mean reward:  1.338 [-2.143, 32.189], mean action: 4.414 [0.000, 19.000],  loss: 0.017692, mae: 0.338400, mean_q: 0.461657, mean_eps: 0.000000
 2117/5000: episode: 77, duration: 0.531s, episode steps:  39, steps per second:  73, episode reward: 38.434, mean reward:  0.985 [-2.489, 32.230], mean action: 5.872 [0.000, 19.000],  loss: 0.019854, mae: 0.347437, mean_q: 0.542748, mean_eps: 0.000000
 2149/5000: episode: 78, duration: 0.456s, episode steps:  32, steps per second:  70, episode reward: 35.734, mean reward:  1.117 [-2.322, 32.223], mean action: 4.938 [0.000, 19.000],  loss: 0.023574, mae: 0.359777, mean_q: 0.616404, mean_eps: 0.000000
 2165/5000: episode: 79, duration: 0.246s, episode steps:  16, steps per second:  65, episode reward: 44.176, mean reward:  2.761 [-2.709, 31.707], mean action: 2.000 [0.000, 15.000],  loss: 0.024410, mae: 0.368184, mean_q: 0.634586, mean_eps: 0.000000
 2186/5000: episode: 80, duration: 0.301s, episode steps:  21, steps per second:  70, episode reward: 38.023, mean reward:  1.811 [-3.000, 32.290], mean action: 4.429 [0.000, 15.000],  loss: 0.017816, mae: 0.343935, mean_q: 0.596349, mean_eps: 0.000000
 2201/5000: episode: 81, duration: 0.230s, episode steps:  15, steps per second:  65, episode reward: 44.417, mean reward:  2.961 [-2.328, 32.500], mean action: 1.800 [0.000, 15.000],  loss: 0.019573, mae: 0.347818, mean_q: 0.598859, mean_eps: 0.000000
 2224/5000: episode: 82, duration: 0.327s, episode steps:  23, steps per second:  70, episode reward: 44.663, mean reward:  1.942 [-2.140, 32.083], mean action: 1.174 [0.000, 9.000],  loss: 0.024239, mae: 0.368699, mean_q: 0.559487, mean_eps: 0.000000
 2251/5000: episode: 83, duration: 0.380s, episode steps:  27, steps per second:  71, episode reward: 35.417, mean reward:  1.312 [-3.000, 32.443], mean action: 6.889 [0.000, 19.000],  loss: 0.022274, mae: 0.359545, mean_q: 0.512162, mean_eps: 0.000000
 2279/5000: episode: 84, duration: 0.422s, episode steps:  28, steps per second:  66, episode reward: 38.713, mean reward:  1.383 [-2.540, 32.114], mean action: 3.036 [0.000, 19.000],  loss: 0.021614, mae: 0.353495, mean_q: 0.525549, mean_eps: 0.000000
 2312/5000: episode: 85, duration: 0.474s, episode steps:  33, steps per second:  70, episode reward: 38.183, mean reward:  1.157 [-2.641, 32.110], mean action: 3.000 [0.000, 14.000],  loss: 0.020959, mae: 0.346555, mean_q: 0.589004, mean_eps: 0.000000
 2336/5000: episode: 86, duration: 0.368s, episode steps:  24, steps per second:  65, episode reward: 44.084, mean reward:  1.837 [-2.650, 32.100], mean action: 2.125 [0.000, 15.000],  loss: 0.024383, mae: 0.362876, mean_q: 0.536610, mean_eps: 0.000000
 2359/5000: episode: 87, duration: 0.340s, episode steps:  23, steps per second:  68, episode reward: 45.000, mean reward:  1.957 [-2.738, 32.800], mean action: 1.957 [0.000, 12.000],  loss: 0.023068, mae: 0.362108, mean_q: 0.529877, mean_eps: 0.000000
 2393/5000: episode: 88, duration: 0.499s, episode steps:  34, steps per second:  68, episode reward: 34.362, mean reward:  1.011 [-3.000, 32.020], mean action: 3.676 [0.000, 12.000],  loss: 0.022199, mae: 0.357006, mean_q: 0.534938, mean_eps: 0.000000
 2431/5000: episode: 89, duration: 0.867s, episode steps:  38, steps per second:  44, episode reward: 35.617, mean reward:  0.937 [-3.000, 32.124], mean action: 2.553 [0.000, 19.000],  loss: 0.022521, mae: 0.364083, mean_q: 0.540700, mean_eps: 0.000000
 2460/5000: episode: 90, duration: 0.638s, episode steps:  29, steps per second:  45, episode reward: 41.069, mean reward:  1.416 [-2.751, 32.150], mean action: 1.793 [0.000, 13.000],  loss: 0.021338, mae: 0.358040, mean_q: 0.552160, mean_eps: 0.000000
 2479/5000: episode: 91, duration: 0.322s, episode steps:  19, steps per second:  59, episode reward: 47.258, mean reward:  2.487 [-0.415, 32.080], mean action: 0.789 [0.000, 1.000],  loss: 0.019004, mae: 0.347736, mean_q: 0.538794, mean_eps: 0.000000
 2510/5000: episode: 92, duration: 0.556s, episode steps:  31, steps per second:  56, episode reward: 44.863, mean reward:  1.447 [-2.537, 32.187], mean action: 2.774 [0.000, 11.000],  loss: 0.023681, mae: 0.366677, mean_q: 0.554169, mean_eps: 0.000000
 2527/5000: episode: 93, duration: 0.396s, episode steps:  17, steps per second:  43, episode reward: 42.000, mean reward:  2.471 [-2.474, 32.140], mean action: 3.471 [0.000, 9.000],  loss: 0.021061, mae: 0.359414, mean_q: 0.579390, mean_eps: 0.000000
 2554/5000: episode: 94, duration: 0.900s, episode steps:  27, steps per second:  30, episode reward: 38.142, mean reward:  1.413 [-2.338, 31.982], mean action: 2.593 [0.000, 16.000],  loss: 0.019148, mae: 0.353222, mean_q: 0.600079, mean_eps: 0.000000
 2569/5000: episode: 95, duration: 0.425s, episode steps:  15, steps per second:  35, episode reward: 47.141, mean reward:  3.143 [-0.088, 33.000], mean action: 1.267 [0.000, 3.000],  loss: 0.016133, mae: 0.337096, mean_q: 0.513618, mean_eps: 0.000000
 2595/5000: episode: 96, duration: 0.447s, episode steps:  26, steps per second:  58, episode reward: 40.308, mean reward:  1.550 [-2.499, 32.060], mean action: 4.615 [0.000, 20.000],  loss: 0.023090, mae: 0.364935, mean_q: 0.503321, mean_eps: 0.000000
 2616/5000: episode: 97, duration: 0.315s, episode steps:  21, steps per second:  67, episode reward: 39.000, mean reward:  1.857 [-2.148, 32.210], mean action: 1.952 [0.000, 9.000],  loss: 0.022218, mae: 0.360333, mean_q: 0.573445, mean_eps: 0.000000
 2638/5000: episode: 98, duration: 0.435s, episode steps:  22, steps per second:  51, episode reward: -32.750, mean reward: -1.489 [-31.976,  2.819], mean action: 4.227 [0.000, 16.000],  loss: 0.018710, mae: 0.339127, mean_q: 0.516806, mean_eps: 0.000000
 2662/5000: episode: 99, duration: 0.393s, episode steps:  24, steps per second:  61, episode reward: 39.000, mean reward:  1.625 [-2.088, 32.100], mean action: 3.208 [0.000, 19.000],  loss: 0.020873, mae: 0.352646, mean_q: 0.489631, mean_eps: 0.000000
 2692/5000: episode: 100, duration: 0.738s, episode steps:  30, steps per second:  41, episode reward: 42.000, mean reward:  1.400 [-3.000, 32.330], mean action: 2.133 [0.000, 11.000],  loss: 0.020384, mae: 0.348101, mean_q: 0.508270, mean_eps: 0.000000
 2715/5000: episode: 101, duration: 0.347s, episode steps:  23, steps per second:  66, episode reward: 40.613, mean reward:  1.766 [-2.939, 32.104], mean action: 3.000 [0.000, 12.000],  loss: 0.023067, mae: 0.365205, mean_q: 0.503667, mean_eps: 0.000000
 2735/5000: episode: 102, duration: 0.295s, episode steps:  20, steps per second:  68, episode reward: 38.370, mean reward:  1.919 [-2.840, 31.765], mean action: 4.750 [0.000, 16.000],  loss: 0.021836, mae: 0.360842, mean_q: 0.477201, mean_eps: 0.000000
 2780/5000: episode: 103, duration: 0.633s, episode steps:  45, steps per second:  71, episode reward: 40.742, mean reward:  0.905 [-2.545, 32.120], mean action: 1.822 [0.000, 16.000],  loss: 0.019214, mae: 0.347309, mean_q: 0.524650, mean_eps: 0.000000
 2804/5000: episode: 104, duration: 0.359s, episode steps:  24, steps per second:  67, episode reward: 38.232, mean reward:  1.593 [-3.000, 32.140], mean action: 5.250 [0.000, 16.000],  loss: 0.024181, mae: 0.368239, mean_q: 0.489610, mean_eps: 0.000000
 2821/5000: episode: 105, duration: 0.262s, episode steps:  17, steps per second:  65, episode reward: 41.783, mean reward:  2.458 [-2.176, 32.360], mean action: 4.471 [1.000, 20.000],  loss: 0.022525, mae: 0.360180, mean_q: 0.506899, mean_eps: 0.000000
 2869/5000: episode: 106, duration: 0.698s, episode steps:  48, steps per second:  69, episode reward: 39.000, mean reward:  0.812 [-3.000, 32.070], mean action: 3.021 [0.000, 16.000],  loss: 0.019775, mae: 0.348919, mean_q: 0.501920, mean_eps: 0.000000
 2885/5000: episode: 107, duration: 0.239s, episode steps:  16, steps per second:  67, episode reward: 44.317, mean reward:  2.770 [-2.029, 31.427], mean action: 2.125 [0.000, 9.000],  loss: 0.018818, mae: 0.344663, mean_q: 0.540543, mean_eps: 0.000000
 2899/5000: episode: 108, duration: 0.211s, episode steps:  14, steps per second:  66, episode reward: 44.878, mean reward:  3.206 [-2.034, 33.000], mean action: 1.071 [0.000, 9.000],  loss: 0.020185, mae: 0.352876, mean_q: 0.519987, mean_eps: 0.000000
 2930/5000: episode: 109, duration: 0.618s, episode steps:  31, steps per second:  50, episode reward: 32.430, mean reward:  1.046 [-3.000, 32.040], mean action: 6.129 [0.000, 20.000],  loss: 0.020881, mae: 0.355399, mean_q: 0.510644, mean_eps: 0.000000
 2951/5000: episode: 110, duration: 0.320s, episode steps:  21, steps per second:  66, episode reward: 41.547, mean reward:  1.978 [-2.514, 32.270], mean action: 3.048 [0.000, 16.000],  loss: 0.017766, mae: 0.342139, mean_q: 0.508110, mean_eps: 0.000000
 2983/5000: episode: 111, duration: 0.775s, episode steps:  32, steps per second:  41, episode reward: 44.451, mean reward:  1.389 [-2.228, 32.140], mean action: 2.906 [0.000, 13.000],  loss: 0.021377, mae: 0.364431, mean_q: 0.496007, mean_eps: 0.000000
 3003/5000: episode: 112, duration: 0.355s, episode steps:  20, steps per second:  56, episode reward: 44.742, mean reward:  2.237 [-2.775, 33.000], mean action: 3.750 [0.000, 13.000],  loss: 0.017430, mae: 0.345768, mean_q: 0.457419, mean_eps: 0.000000
 3029/5000: episode: 113, duration: 0.467s, episode steps:  26, steps per second:  56, episode reward: 44.179, mean reward:  1.699 [-2.198, 32.060], mean action: 1.692 [0.000, 12.000],  loss: 0.017422, mae: 0.346194, mean_q: 0.467650, mean_eps: 0.000000
 3054/5000: episode: 114, duration: 0.412s, episode steps:  25, steps per second:  61, episode reward: 41.104, mean reward:  1.644 [-3.000, 31.815], mean action: 2.040 [0.000, 13.000],  loss: 0.020205, mae: 0.358474, mean_q: 0.473049, mean_eps: 0.000000
 3079/5000: episode: 115, duration: 0.357s, episode steps:  25, steps per second:  70, episode reward: 36.000, mean reward:  1.440 [-2.898, 32.170], mean action: 4.080 [0.000, 20.000],  loss: 0.016626, mae: 0.342486, mean_q: 0.523126, mean_eps: 0.000000
 3109/5000: episode: 116, duration: 0.642s, episode steps:  30, steps per second:  47, episode reward: 41.637, mean reward:  1.388 [-2.765, 32.110], mean action: 4.167 [0.000, 20.000],  loss: 0.020573, mae: 0.356356, mean_q: 0.496256, mean_eps: 0.000000
 3138/5000: episode: 117, duration: 0.872s, episode steps:  29, steps per second:  33, episode reward: 33.000, mean reward:  1.138 [-3.000, 32.090], mean action: 4.655 [1.000, 19.000],  loss: 0.022400, mae: 0.356667, mean_q: 0.520056, mean_eps: 0.000000
 3176/5000: episode: 118, duration: 0.566s, episode steps:  38, steps per second:  67, episode reward: 35.336, mean reward:  0.930 [-2.668, 32.290], mean action: 3.342 [0.000, 15.000],  loss: 0.019049, mae: 0.346007, mean_q: 0.557410, mean_eps: 0.000000
 3203/5000: episode: 119, duration: 0.641s, episode steps:  27, steps per second:  42, episode reward: 41.904, mean reward:  1.552 [-2.288, 32.294], mean action: 3.593 [2.000, 15.000],  loss: 0.020891, mae: 0.361746, mean_q: 0.539119, mean_eps: 0.000000
 3222/5000: episode: 120, duration: 1.239s, episode steps:  19, steps per second:  15, episode reward: 42.000, mean reward:  2.211 [-2.411, 32.260], mean action: 2.684 [0.000, 12.000],  loss: 0.021256, mae: 0.363652, mean_q: 0.470945, mean_eps: 0.000000
 3259/5000: episode: 121, duration: 0.705s, episode steps:  37, steps per second:  52, episode reward: -32.190, mean reward: -0.870 [-32.900,  2.123], mean action: 4.162 [0.000, 15.000],  loss: 0.022558, mae: 0.362092, mean_q: 0.557494, mean_eps: 0.000000
 3288/5000: episode: 122, duration: 0.627s, episode steps:  29, steps per second:  46, episode reward: 44.468, mean reward:  1.533 [-2.125, 32.300], mean action: 2.966 [1.000, 12.000],  loss: 0.019276, mae: 0.352623, mean_q: 0.516291, mean_eps: 0.000000
 3321/5000: episode: 123, duration: 0.497s, episode steps:  33, steps per second:  66, episode reward: 43.954, mean reward:  1.332 [-2.327, 32.219], mean action: 2.606 [0.000, 9.000],  loss: 0.019143, mae: 0.350767, mean_q: 0.523625, mean_eps: 0.000000
 3353/5000: episode: 124, duration: 0.631s, episode steps:  32, steps per second:  51, episode reward: 44.728, mean reward:  1.398 [-2.215, 32.350], mean action: 1.344 [0.000, 19.000],  loss: 0.018880, mae: 0.358223, mean_q: 0.558906, mean_eps: 0.000000
 3395/5000: episode: 125, duration: 0.789s, episode steps:  42, steps per second:  53, episode reward: 39.000, mean reward:  0.929 [-2.230, 32.040], mean action: 2.833 [0.000, 19.000],  loss: 0.021926, mae: 0.365864, mean_q: 0.567255, mean_eps: 0.000000
 3419/5000: episode: 126, duration: 0.349s, episode steps:  24, steps per second:  69, episode reward: 43.802, mean reward:  1.825 [-2.287, 32.264], mean action: 5.708 [0.000, 19.000],  loss: 0.020314, mae: 0.352636, mean_q: 0.556091, mean_eps: 0.000000
 3443/5000: episode: 127, duration: 0.399s, episode steps:  24, steps per second:  60, episode reward: 48.000, mean reward:  2.000 [-0.185, 32.020], mean action: 5.042 [0.000, 14.000],  loss: 0.018539, mae: 0.353277, mean_q: 0.570796, mean_eps: 0.000000
 3470/5000: episode: 128, duration: 0.415s, episode steps:  27, steps per second:  65, episode reward: 41.876, mean reward:  1.551 [-2.550, 32.020], mean action: 4.741 [0.000, 21.000],  loss: 0.020609, mae: 0.354283, mean_q: 0.562621, mean_eps: 0.000000
 3490/5000: episode: 129, duration: 0.358s, episode steps:  20, steps per second:  56, episode reward: 45.000, mean reward:  2.250 [-2.122, 32.210], mean action: 1.800 [0.000, 19.000],  loss: 0.022865, mae: 0.376919, mean_q: 0.590422, mean_eps: 0.000000
 3511/5000: episode: 130, duration: 0.346s, episode steps:  21, steps per second:  61, episode reward: 41.519, mean reward:  1.977 [-3.000, 31.981], mean action: 3.762 [0.000, 19.000],  loss: 0.020149, mae: 0.357109, mean_q: 0.595764, mean_eps: 0.000000
 3531/5000: episode: 131, duration: 0.291s, episode steps:  20, steps per second:  69, episode reward: 43.728, mean reward:  2.186 [-2.940, 32.400], mean action: 7.200 [2.000, 15.000],  loss: 0.019346, mae: 0.356555, mean_q: 0.581416, mean_eps: 0.000000
 3570/5000: episode: 132, duration: 0.570s, episode steps:  39, steps per second:  68, episode reward: -32.830, mean reward: -0.842 [-31.984,  2.910], mean action: 8.692 [0.000, 19.000],  loss: 0.020038, mae: 0.359018, mean_q: 0.503034, mean_eps: 0.000000
 3603/5000: episode: 133, duration: 0.466s, episode steps:  33, steps per second:  71, episode reward: 34.995, mean reward:  1.060 [-3.000, 32.107], mean action: 5.636 [0.000, 15.000],  loss: 0.021393, mae: 0.352690, mean_q: 0.504980, mean_eps: 0.000000
 3615/5000: episode: 134, duration: 0.181s, episode steps:  12, steps per second:  66, episode reward: 41.952, mean reward:  3.496 [-2.566, 31.962], mean action: 4.417 [0.000, 15.000],  loss: 0.020070, mae: 0.345513, mean_q: 0.497233, mean_eps: 0.000000
 3643/5000: episode: 135, duration: 0.443s, episode steps:  28, steps per second:  63, episode reward: 40.845, mean reward:  1.459 [-2.938, 31.980], mean action: 3.250 [0.000, 15.000],  loss: 0.018349, mae: 0.342340, mean_q: 0.505432, mean_eps: 0.000000
 3685/5000: episode: 136, duration: 0.580s, episode steps:  42, steps per second:  72, episode reward: -32.970, mean reward: -0.785 [-32.384,  3.000], mean action: 6.286 [0.000, 20.000],  loss: 0.020254, mae: 0.355278, mean_q: 0.445548, mean_eps: 0.000000
 3707/5000: episode: 137, duration: 0.329s, episode steps:  22, steps per second:  67, episode reward: 44.752, mean reward:  2.034 [-2.469, 32.040], mean action: 5.182 [0.000, 16.000],  loss: 0.022713, mae: 0.365576, mean_q: 0.493189, mean_eps: 0.000000
 3737/5000: episode: 138, duration: 0.443s, episode steps:  30, steps per second:  68, episode reward: 35.464, mean reward:  1.182 [-3.000, 32.054], mean action: 2.800 [0.000, 16.000],  loss: 0.022233, mae: 0.357078, mean_q: 0.570658, mean_eps: 0.000000
 3762/5000: episode: 139, duration: 0.360s, episode steps:  25, steps per second:  69, episode reward: 41.831, mean reward:  1.673 [-2.063, 32.201], mean action: 1.360 [0.000, 11.000],  loss: 0.024797, mae: 0.372378, mean_q: 0.576428, mean_eps: 0.000000
 3801/5000: episode: 140, duration: 0.543s, episode steps:  39, steps per second:  72, episode reward: 41.537, mean reward:  1.065 [-2.217, 32.150], mean action: 1.641 [0.000, 9.000],  loss: 0.017571, mae: 0.343365, mean_q: 0.505377, mean_eps: 0.000000
 3822/5000: episode: 141, duration: 0.305s, episode steps:  21, steps per second:  69, episode reward: 38.903, mean reward:  1.853 [-2.714, 32.373], mean action: 2.524 [0.000, 16.000],  loss: 0.019446, mae: 0.341980, mean_q: 0.532180, mean_eps: 0.000000
 3860/5000: episode: 142, duration: 0.530s, episode steps:  38, steps per second:  72, episode reward: 41.851, mean reward:  1.101 [-2.452, 32.270], mean action: 2.158 [0.000, 19.000],  loss: 0.020688, mae: 0.352959, mean_q: 0.534825, mean_eps: 0.000000
 3885/5000: episode: 143, duration: 0.368s, episode steps:  25, steps per second:  68, episode reward: 32.831, mean reward:  1.313 [-3.000, 31.971], mean action: 8.720 [0.000, 21.000],  loss: 0.022105, mae: 0.359711, mean_q: 0.520149, mean_eps: 0.000000
 3919/5000: episode: 144, duration: 0.474s, episode steps:  34, steps per second:  72, episode reward: 44.490, mean reward:  1.309 [-3.000, 32.310], mean action: 3.941 [0.000, 19.000],  loss: 0.021947, mae: 0.360322, mean_q: 0.532705, mean_eps: 0.000000
 3959/5000: episode: 145, duration: 0.550s, episode steps:  40, steps per second:  73, episode reward: 44.895, mean reward:  1.122 [-2.406, 32.470], mean action: 2.025 [1.000, 9.000],  loss: 0.020166, mae: 0.348244, mean_q: 0.537845, mean_eps: 0.000000
 3995/5000: episode: 146, duration: 0.511s, episode steps:  36, steps per second:  70, episode reward: 40.908, mean reward:  1.136 [-2.369, 32.187], mean action: 2.944 [0.000, 19.000],  loss: 0.021684, mae: 0.358794, mean_q: 0.545043, mean_eps: 0.000000
 4027/5000: episode: 147, duration: 0.456s, episode steps:  32, steps per second:  70, episode reward: 35.900, mean reward:  1.122 [-2.548, 32.460], mean action: 4.531 [0.000, 19.000],  loss: 0.021429, mae: 0.357032, mean_q: 0.538672, mean_eps: 0.000000
 4046/5000: episode: 148, duration: 0.279s, episode steps:  19, steps per second:  68, episode reward: 41.459, mean reward:  2.182 [-2.998, 32.540], mean action: 3.842 [0.000, 19.000],  loss: 0.023789, mae: 0.368867, mean_q: 0.490639, mean_eps: 0.000000
 4084/5000: episode: 149, duration: 0.532s, episode steps:  38, steps per second:  71, episode reward: 38.142, mean reward:  1.004 [-2.692, 32.480], mean action: 4.395 [0.000, 16.000],  loss: 0.021268, mae: 0.358614, mean_q: 0.472554, mean_eps: 0.000000
 4113/5000: episode: 150, duration: 0.409s, episode steps:  29, steps per second:  71, episode reward: 43.594, mean reward:  1.503 [-3.000, 31.920], mean action: 3.207 [0.000, 15.000],  loss: 0.018188, mae: 0.346976, mean_q: 0.527156, mean_eps: 0.000000
 4135/5000: episode: 151, duration: 0.327s, episode steps:  22, steps per second:  67, episode reward: 47.588, mean reward:  2.163 [-0.357, 32.098], mean action: 1.227 [0.000, 3.000],  loss: 0.021614, mae: 0.363608, mean_q: 0.512468, mean_eps: 0.000000
 4147/5000: episode: 152, duration: 0.198s, episode steps:  12, steps per second:  61, episode reward: 44.711, mean reward:  3.726 [-2.762, 32.670], mean action: 3.250 [0.000, 15.000],  loss: 0.021015, mae: 0.355859, mean_q: 0.538986, mean_eps: 0.000000
 4195/5000: episode: 153, duration: 1.003s, episode steps:  48, steps per second:  48, episode reward: 41.238, mean reward:  0.859 [-2.094, 32.185], mean action: 2.042 [0.000, 15.000],  loss: 0.020262, mae: 0.352496, mean_q: 0.527410, mean_eps: 0.000000
 4216/5000: episode: 154, duration: 0.368s, episode steps:  21, steps per second:  57, episode reward: 41.241, mean reward:  1.964 [-2.229, 32.220], mean action: 2.476 [0.000, 16.000],  loss: 0.017372, mae: 0.339569, mean_q: 0.563202, mean_eps: 0.000000
 4233/5000: episode: 155, duration: 0.247s, episode steps:  17, steps per second:  69, episode reward: 42.000, mean reward:  2.471 [-2.240, 32.030], mean action: 4.353 [0.000, 16.000],  loss: 0.017601, mae: 0.341417, mean_q: 0.524235, mean_eps: 0.000000
 4250/5000: episode: 156, duration: 0.342s, episode steps:  17, steps per second:  50, episode reward: 44.151, mean reward:  2.597 [-2.510, 32.600], mean action: 2.588 [0.000, 15.000],  loss: 0.015570, mae: 0.327228, mean_q: 0.570873, mean_eps: 0.000000
 4278/5000: episode: 157, duration: 0.458s, episode steps:  28, steps per second:  61, episode reward: 46.717, mean reward:  1.668 [-0.636, 32.040], mean action: 1.893 [0.000, 12.000],  loss: 0.019425, mae: 0.346768, mean_q: 0.524566, mean_eps: 0.000000
 4313/5000: episode: 158, duration: 0.669s, episode steps:  35, steps per second:  52, episode reward: 41.504, mean reward:  1.186 [-2.546, 32.570], mean action: 2.229 [0.000, 19.000],  loss: 0.022556, mae: 0.355120, mean_q: 0.516660, mean_eps: 0.000000
 4335/5000: episode: 159, duration: 0.332s, episode steps:  22, steps per second:  66, episode reward: 47.000, mean reward:  2.136 [-0.336, 32.190], mean action: 5.864 [0.000, 19.000],  loss: 0.018349, mae: 0.339917, mean_q: 0.547224, mean_eps: 0.000000
 4358/5000: episode: 160, duration: 0.443s, episode steps:  23, steps per second:  52, episode reward: 40.635, mean reward:  1.767 [-3.000, 32.370], mean action: 3.739 [0.000, 19.000],  loss: 0.021888, mae: 0.362344, mean_q: 0.514049, mean_eps: 0.000000
 4380/5000: episode: 161, duration: 0.322s, episode steps:  22, steps per second:  68, episode reward: 41.163, mean reward:  1.871 [-2.059, 29.983], mean action: 3.909 [0.000, 14.000],  loss: 0.023623, mae: 0.357423, mean_q: 0.479668, mean_eps: 0.000000
 4405/5000: episode: 162, duration: 0.373s, episode steps:  25, steps per second:  67, episode reward: 38.675, mean reward:  1.547 [-2.654, 32.190], mean action: 3.640 [0.000, 15.000],  loss: 0.020088, mae: 0.346476, mean_q: 0.543244, mean_eps: 0.000000
 4437/5000: episode: 163, duration: 0.458s, episode steps:  32, steps per second:  70, episode reward: 41.399, mean reward:  1.294 [-3.000, 32.130], mean action: 2.781 [0.000, 19.000],  loss: 0.017871, mae: 0.337933, mean_q: 0.518314, mean_eps: 0.000000
 4461/5000: episode: 164, duration: 0.348s, episode steps:  24, steps per second:  69, episode reward: 42.000, mean reward:  1.750 [-3.000, 32.350], mean action: 1.917 [0.000, 15.000],  loss: 0.022157, mae: 0.348618, mean_q: 0.492358, mean_eps: 0.000000
 4502/5000: episode: 165, duration: 0.559s, episode steps:  41, steps per second:  73, episode reward: 40.802, mean reward:  0.995 [-2.104, 32.142], mean action: 3.317 [1.000, 19.000],  loss: 0.019667, mae: 0.350666, mean_q: 0.519469, mean_eps: 0.000000
 4524/5000: episode: 166, duration: 0.321s, episode steps:  22, steps per second:  68, episode reward: 43.073, mean reward:  1.958 [-2.097, 32.350], mean action: 4.636 [0.000, 15.000],  loss: 0.020302, mae: 0.359300, mean_q: 0.490306, mean_eps: 0.000000
 4539/5000: episode: 167, duration: 0.226s, episode steps:  15, steps per second:  66, episode reward: 45.000, mean reward:  3.000 [-2.048, 32.070], mean action: 2.800 [0.000, 15.000],  loss: 0.025147, mae: 0.371400, mean_q: 0.562844, mean_eps: 0.000000
 4580/5000: episode: 168, duration: 0.575s, episode steps:  41, steps per second:  71, episode reward: 41.261, mean reward:  1.006 [-2.154, 32.220], mean action: 2.585 [0.000, 15.000],  loss: 0.020424, mae: 0.350995, mean_q: 0.514939, mean_eps: 0.000000
 4600/5000: episode: 169, duration: 0.307s, episode steps:  20, steps per second:  65, episode reward: 41.502, mean reward:  2.075 [-2.486, 32.039], mean action: 2.950 [0.000, 15.000],  loss: 0.018823, mae: 0.348507, mean_q: 0.500044, mean_eps: 0.000000
 4617/5000: episode: 170, duration: 0.284s, episode steps:  17, steps per second:  60, episode reward: 41.103, mean reward:  2.418 [-2.237, 32.901], mean action: 2.235 [0.000, 16.000],  loss: 0.019395, mae: 0.349331, mean_q: 0.456753, mean_eps: 0.000000
 4646/5000: episode: 171, duration: 0.430s, episode steps:  29, steps per second:  67, episode reward: 34.994, mean reward:  1.207 [-2.296, 32.020], mean action: 4.586 [0.000, 16.000],  loss: 0.021560, mae: 0.351913, mean_q: 0.480485, mean_eps: 0.000000
 4667/5000: episode: 172, duration: 0.321s, episode steps:  21, steps per second:  65, episode reward: 36.000, mean reward:  1.714 [-3.000, 32.510], mean action: 4.333 [0.000, 15.000],  loss: 0.017970, mae: 0.325312, mean_q: 0.528600, mean_eps: 0.000000
 4696/5000: episode: 173, duration: 0.423s, episode steps:  29, steps per second:  68, episode reward: 43.582, mean reward:  1.503 [-2.508, 32.340], mean action: 5.069 [0.000, 14.000],  loss: 0.023456, mae: 0.354391, mean_q: 0.542838, mean_eps: 0.000000
 4714/5000: episode: 174, duration: 0.325s, episode steps:  18, steps per second:  55, episode reward: 41.705, mean reward:  2.317 [-2.395, 31.715], mean action: 2.000 [0.000, 15.000],  loss: 0.020360, mae: 0.347813, mean_q: 0.573317, mean_eps: 0.000000
 4739/5000: episode: 175, duration: 0.364s, episode steps:  25, steps per second:  69, episode reward: 38.130, mean reward:  1.525 [-3.000, 32.130], mean action: 4.680 [0.000, 14.000],  loss: 0.021960, mae: 0.355614, mean_q: 0.563409, mean_eps: 0.000000
 4760/5000: episode: 176, duration: 0.339s, episode steps:  21, steps per second:  62, episode reward: 44.768, mean reward:  2.132 [-2.333, 31.798], mean action: 1.238 [0.000, 14.000],  loss: 0.017548, mae: 0.339508, mean_q: 0.546057, mean_eps: 0.000000
 4781/5000: episode: 177, duration: 0.375s, episode steps:  21, steps per second:  56, episode reward: 38.389, mean reward:  1.828 [-2.636, 32.083], mean action: 4.095 [0.000, 12.000],  loss: 0.023659, mae: 0.373952, mean_q: 0.539939, mean_eps: 0.000000
 4795/5000: episode: 178, duration: 0.217s, episode steps:  14, steps per second:  65, episode reward: 46.874, mean reward:  3.348 [-0.040, 32.862], mean action: 3.786 [0.000, 13.000],  loss: 0.020223, mae: 0.356564, mean_q: 0.563869, mean_eps: 0.000000
 4823/5000: episode: 179, duration: 0.446s, episode steps:  28, steps per second:  63, episode reward: 39.000, mean reward:  1.393 [-3.000, 32.180], mean action: 2.536 [0.000, 12.000],  loss: 0.021218, mae: 0.364193, mean_q: 0.540557, mean_eps: 0.000000
 4836/5000: episode: 180, duration: 0.221s, episode steps:  13, steps per second:  59, episode reward: 44.182, mean reward:  3.399 [-2.753, 32.508], mean action: 1.385 [0.000, 11.000],  loss: 0.025145, mae: 0.378808, mean_q: 0.574902, mean_eps: 0.000000
 4864/5000: episode: 181, duration: 0.405s, episode steps:  28, steps per second:  69, episode reward: 40.726, mean reward:  1.454 [-2.446, 32.230], mean action: 2.571 [0.000, 20.000],  loss: 0.020262, mae: 0.358969, mean_q: 0.588311, mean_eps: 0.000000
 4889/5000: episode: 182, duration: 0.605s, episode steps:  25, steps per second:  41, episode reward: 32.266, mean reward:  1.291 [-3.000, 31.446], mean action: 4.360 [0.000, 15.000],  loss: 0.025833, mae: 0.382089, mean_q: 0.537754, mean_eps: 0.000000
 4917/5000: episode: 183, duration: 0.393s, episode steps:  28, steps per second:  71, episode reward: 36.000, mean reward:  1.286 [-2.630, 29.090], mean action: 2.536 [0.000, 15.000],  loss: 0.016934, mae: 0.338788, mean_q: 0.522377, mean_eps: 0.000000
 4946/5000: episode: 184, duration: 0.485s, episode steps:  29, steps per second:  60, episode reward: 41.876, mean reward:  1.444 [-2.399, 32.830], mean action: 2.828 [1.000, 15.000],  loss: 0.021843, mae: 0.369742, mean_q: 0.487920, mean_eps: 0.000000
 4968/5000: episode: 185, duration: 0.318s, episode steps:  22, steps per second:  69, episode reward: 38.332, mean reward:  1.742 [-2.875, 32.019], mean action: 3.773 [0.000, 16.000],  loss: 0.027915, mae: 0.403399, mean_q: 0.540930, mean_eps: 0.000000
 4992/5000: episode: 186, duration: 0.366s, episode steps:  24, steps per second:  66, episode reward: 38.940, mean reward:  1.623 [-2.255, 32.100], mean action: 3.500 [0.000, 16.000],  loss: 0.020411, mae: 0.369167, mean_q: 0.524639, mean_eps: 0.000000
done, took 75.784 seconds
DQN Evaluation: 6964 victories out of 8198 episodes
Training for 5000 steps ...
   24/5000: episode: 1, duration: 0.199s, episode steps:  24, steps per second: 121, episode reward: 39.000, mean reward:  1.625 [-2.103, 32.350], mean action: 6.375 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   43/5000: episode: 2, duration: 0.142s, episode steps:  19, steps per second: 134, episode reward: 38.209, mean reward:  2.011 [-2.258, 32.077], mean action: 3.421 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   63/5000: episode: 3, duration: 0.156s, episode steps:  20, steps per second: 128, episode reward: 35.342, mean reward:  1.767 [-3.000, 32.150], mean action: 6.200 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   84/5000: episode: 4, duration: 0.152s, episode steps:  21, steps per second: 138, episode reward: 38.007, mean reward:  1.810 [-2.321, 32.067], mean action: 6.905 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  103/5000: episode: 5, duration: 0.152s, episode steps:  19, steps per second: 125, episode reward: 41.453, mean reward:  2.182 [-2.341, 32.118], mean action: 2.895 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  122/5000: episode: 6, duration: 0.146s, episode steps:  19, steps per second: 130, episode reward: 33.000, mean reward:  1.737 [-3.000, 32.140], mean action: 5.158 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  152/5000: episode: 7, duration: 0.200s, episode steps:  30, steps per second: 150, episode reward: 38.536, mean reward:  1.285 [-2.296, 32.130], mean action: 3.667 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  178/5000: episode: 8, duration: 0.172s, episode steps:  26, steps per second: 151, episode reward: -32.910, mean reward: -1.266 [-32.251,  2.770], mean action: 3.885 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  195/5000: episode: 9, duration: 0.126s, episode steps:  17, steps per second: 135, episode reward: 38.113, mean reward:  2.242 [-2.307, 32.027], mean action: 4.235 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  222/5000: episode: 10, duration: 0.181s, episode steps:  27, steps per second: 149, episode reward: 40.978, mean reward:  1.518 [-2.137, 32.377], mean action: 5.111 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  249/5000: episode: 11, duration: 0.177s, episode steps:  27, steps per second: 153, episode reward: 35.189, mean reward:  1.303 [-2.219, 32.220], mean action: 6.667 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  278/5000: episode: 12, duration: 0.187s, episode steps:  29, steps per second: 155, episode reward: 32.663, mean reward:  1.126 [-2.611, 32.050], mean action: 4.103 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  300/5000: episode: 13, duration: 0.149s, episode steps:  22, steps per second: 148, episode reward: 33.000, mean reward:  1.500 [-2.320, 30.325], mean action: 4.045 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  322/5000: episode: 14, duration: 0.145s, episode steps:  22, steps per second: 151, episode reward: 32.535, mean reward:  1.479 [-3.000, 32.100], mean action: 5.545 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  342/5000: episode: 15, duration: 0.135s, episode steps:  20, steps per second: 148, episode reward: 41.300, mean reward:  2.065 [-2.705, 31.453], mean action: 5.250 [1.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  364/5000: episode: 16, duration: 0.142s, episode steps:  22, steps per second: 155, episode reward: -33.000, mean reward: -1.500 [-32.239,  2.320], mean action: 6.000 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  385/5000: episode: 17, duration: 0.145s, episode steps:  21, steps per second: 145, episode reward: 39.000, mean reward:  1.857 [-2.301, 30.033], mean action: 3.000 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  414/5000: episode: 18, duration: 0.188s, episode steps:  29, steps per second: 154, episode reward: 36.000, mean reward:  1.241 [-2.456, 30.612], mean action: 3.655 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  437/5000: episode: 19, duration: 0.165s, episode steps:  23, steps per second: 139, episode reward: 32.901, mean reward:  1.430 [-3.000, 32.041], mean action: 4.652 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  455/5000: episode: 20, duration: 0.128s, episode steps:  18, steps per second: 141, episode reward: 43.628, mean reward:  2.424 [-3.000, 32.230], mean action: 4.833 [2.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  481/5000: episode: 21, duration: 0.185s, episode steps:  26, steps per second: 141, episode reward: -35.310, mean reward: -1.358 [-32.576,  2.910], mean action: 6.192 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  505/5000: episode: 22, duration: 0.183s, episode steps:  24, steps per second: 131, episode reward: -35.760, mean reward: -1.490 [-32.604,  2.281], mean action: 6.833 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  527/5000: episode: 23, duration: 0.170s, episode steps:  22, steps per second: 129, episode reward: 34.867, mean reward:  1.585 [-2.939, 31.654], mean action: 6.636 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  554/5000: episode: 24, duration: 0.179s, episode steps:  27, steps per second: 151, episode reward: 38.367, mean reward:  1.421 [-2.227, 32.410], mean action: 3.778 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  587/5000: episode: 25, duration: 0.220s, episode steps:  33, steps per second: 150, episode reward: 38.611, mean reward:  1.170 [-2.222, 32.020], mean action: 3.576 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  612/5000: episode: 26, duration: 0.168s, episode steps:  25, steps per second: 148, episode reward: 32.162, mean reward:  1.286 [-2.403, 32.160], mean action: 5.760 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  634/5000: episode: 27, duration: 0.142s, episode steps:  22, steps per second: 155, episode reward: -33.000, mean reward: -1.500 [-30.000,  2.225], mean action: 5.727 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  663/5000: episode: 28, duration: 0.206s, episode steps:  29, steps per second: 140, episode reward: 37.184, mean reward:  1.282 [-2.370, 31.613], mean action: 6.103 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  681/5000: episode: 29, duration: 0.127s, episode steps:  18, steps per second: 141, episode reward: 41.493, mean reward:  2.305 [-2.438, 32.620], mean action: 6.833 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  703/5000: episode: 30, duration: 0.182s, episode steps:  22, steps per second: 121, episode reward: 41.344, mean reward:  1.879 [-2.191, 32.700], mean action: 6.318 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  723/5000: episode: 31, duration: 0.140s, episode steps:  20, steps per second: 142, episode reward: 35.976, mean reward:  1.799 [-3.000, 32.120], mean action: 5.550 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  745/5000: episode: 32, duration: 0.152s, episode steps:  22, steps per second: 145, episode reward: 32.376, mean reward:  1.472 [-3.000, 32.950], mean action: 5.364 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  770/5000: episode: 33, duration: 0.175s, episode steps:  25, steps per second: 143, episode reward: 32.556, mean reward:  1.302 [-3.000, 32.556], mean action: 4.080 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  800/5000: episode: 34, duration: 0.197s, episode steps:  30, steps per second: 152, episode reward: 32.419, mean reward:  1.081 [-2.448, 32.197], mean action: 5.900 [1.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  818/5000: episode: 35, duration: 0.130s, episode steps:  18, steps per second: 138, episode reward: 40.407, mean reward:  2.245 [-2.171, 33.000], mean action: 7.889 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  834/5000: episode: 36, duration: 0.109s, episode steps:  16, steps per second: 147, episode reward: 38.257, mean reward:  2.391 [-2.569, 32.900], mean action: 3.812 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  857/5000: episode: 37, duration: 0.153s, episode steps:  23, steps per second: 150, episode reward: 36.000, mean reward:  1.565 [-2.204, 32.040], mean action: 6.087 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  875/5000: episode: 38, duration: 0.131s, episode steps:  18, steps per second: 138, episode reward: 37.826, mean reward:  2.101 [-3.000, 32.130], mean action: 4.667 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  895/5000: episode: 39, duration: 0.126s, episode steps:  20, steps per second: 159, episode reward: -33.000, mean reward: -1.650 [-30.940,  3.000], mean action: 4.550 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  945/5000: episode: 40, duration: 0.292s, episode steps:  50, steps per second: 171, episode reward: -38.960, mean reward: -0.779 [-32.048,  2.250], mean action: 6.100 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  964/5000: episode: 41, duration: 0.129s, episode steps:  19, steps per second: 147, episode reward: 39.000, mean reward:  2.053 [-2.346, 32.380], mean action: 3.737 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1005/5000: episode: 42, duration: 0.281s, episode steps:  41, steps per second: 146, episode reward: 35.259, mean reward:  0.860 [-2.360, 32.200], mean action: 5.732 [0.000, 20.000],  loss: 0.017579, mae: 0.354689, mean_q: 0.412580, mean_eps: 0.000000
 1030/5000: episode: 43, duration: 0.353s, episode steps:  25, steps per second:  71, episode reward: 35.902, mean reward:  1.436 [-2.313, 31.952], mean action: 3.800 [0.000, 16.000],  loss: 0.018343, mae: 0.358749, mean_q: 0.478570, mean_eps: 0.000000
 1055/5000: episode: 44, duration: 0.364s, episode steps:  25, steps per second:  69, episode reward: 35.128, mean reward:  1.405 [-2.312, 32.059], mean action: 5.720 [0.000, 20.000],  loss: 0.023227, mae: 0.378756, mean_q: 0.531573, mean_eps: 0.000000
 1074/5000: episode: 45, duration: 0.284s, episode steps:  19, steps per second:  67, episode reward: 38.114, mean reward:  2.006 [-2.548, 32.333], mean action: 4.105 [1.000, 16.000],  loss: 0.018288, mae: 0.351231, mean_q: 0.520405, mean_eps: 0.000000
 1097/5000: episode: 46, duration: 0.334s, episode steps:  23, steps per second:  69, episode reward: 32.898, mean reward:  1.430 [-3.000, 32.178], mean action: 7.087 [0.000, 16.000],  loss: 0.017537, mae: 0.344548, mean_q: 0.513558, mean_eps: 0.000000
 1122/5000: episode: 47, duration: 0.381s, episode steps:  25, steps per second:  66, episode reward: 35.461, mean reward:  1.418 [-3.000, 32.180], mean action: 8.720 [0.000, 20.000],  loss: 0.020753, mae: 0.356620, mean_q: 0.541951, mean_eps: 0.000000
 1144/5000: episode: 48, duration: 0.335s, episode steps:  22, steps per second:  66, episode reward: 35.372, mean reward:  1.608 [-2.431, 31.972], mean action: 4.273 [0.000, 16.000],  loss: 0.020672, mae: 0.362131, mean_q: 0.534203, mean_eps: 0.000000
 1165/5000: episode: 49, duration: 0.343s, episode steps:  21, steps per second:  61, episode reward: 40.799, mean reward:  1.943 [-2.514, 32.827], mean action: 3.905 [0.000, 19.000],  loss: 0.022183, mae: 0.372376, mean_q: 0.514493, mean_eps: 0.000000
 1190/5000: episode: 50, duration: 0.384s, episode steps:  25, steps per second:  65, episode reward: -35.100, mean reward: -1.404 [-32.034,  2.300], mean action: 4.720 [1.000, 19.000],  loss: 0.027530, mae: 0.403855, mean_q: 0.532613, mean_eps: 0.000000
 1221/5000: episode: 51, duration: 0.473s, episode steps:  31, steps per second:  66, episode reward: 38.157, mean reward:  1.231 [-2.263, 32.230], mean action: 4.129 [0.000, 13.000],  loss: 0.020449, mae: 0.368697, mean_q: 0.547378, mean_eps: 0.000000
 1244/5000: episode: 52, duration: 0.366s, episode steps:  23, steps per second:  63, episode reward: 37.968, mean reward:  1.651 [-2.744, 32.293], mean action: 5.783 [0.000, 19.000],  loss: 0.023482, mae: 0.383972, mean_q: 0.527250, mean_eps: 0.000000
 1262/5000: episode: 53, duration: 0.265s, episode steps:  18, steps per second:  68, episode reward: 35.903, mean reward:  1.995 [-3.000, 32.903], mean action: 4.222 [0.000, 16.000],  loss: 0.019141, mae: 0.367981, mean_q: 0.556706, mean_eps: 0.000000
 1285/5000: episode: 54, duration: 0.347s, episode steps:  23, steps per second:  66, episode reward: 35.305, mean reward:  1.535 [-3.000, 32.090], mean action: 3.304 [0.000, 16.000],  loss: 0.022789, mae: 0.386851, mean_q: 0.548010, mean_eps: 0.000000
 1300/5000: episode: 55, duration: 0.226s, episode steps:  15, steps per second:  66, episode reward: 44.189, mean reward:  2.946 [-2.225, 31.889], mean action: 2.333 [0.000, 9.000],  loss: 0.017295, mae: 0.353613, mean_q: 0.568859, mean_eps: 0.000000
 1323/5000: episode: 56, duration: 0.347s, episode steps:  23, steps per second:  66, episode reward: 35.396, mean reward:  1.539 [-2.684, 32.310], mean action: 3.696 [0.000, 16.000],  loss: 0.020691, mae: 0.368369, mean_q: 0.554595, mean_eps: 0.000000
 1353/5000: episode: 57, duration: 0.432s, episode steps:  30, steps per second:  69, episode reward: 36.000, mean reward:  1.200 [-2.605, 32.500], mean action: 2.900 [0.000, 12.000],  loss: 0.021755, mae: 0.371217, mean_q: 0.565229, mean_eps: 0.000000
 1383/5000: episode: 58, duration: 0.479s, episode steps:  30, steps per second:  63, episode reward: -32.910, mean reward: -1.097 [-32.355,  2.445], mean action: 3.567 [0.000, 15.000],  loss: 0.020516, mae: 0.363264, mean_q: 0.559363, mean_eps: 0.000000
 1407/5000: episode: 59, duration: 0.581s, episode steps:  24, steps per second:  41, episode reward: 35.868, mean reward:  1.494 [-2.506, 32.253], mean action: 4.042 [0.000, 19.000],  loss: 0.018326, mae: 0.353320, mean_q: 0.514255, mean_eps: 0.000000
 1423/5000: episode: 60, duration: 0.281s, episode steps:  16, steps per second:  57, episode reward: 41.173, mean reward:  2.573 [-2.167, 31.858], mean action: 3.688 [0.000, 19.000],  loss: 0.019678, mae: 0.367895, mean_q: 0.522776, mean_eps: 0.000000
 1445/5000: episode: 61, duration: 0.314s, episode steps:  22, steps per second:  70, episode reward: 36.000, mean reward:  1.636 [-2.271, 32.090], mean action: 4.727 [0.000, 15.000],  loss: 0.017735, mae: 0.356006, mean_q: 0.482831, mean_eps: 0.000000
 1473/5000: episode: 62, duration: 0.460s, episode steps:  28, steps per second:  61, episode reward: 35.901, mean reward:  1.282 [-2.431, 32.551], mean action: 2.607 [0.000, 15.000],  loss: 0.019665, mae: 0.366901, mean_q: 0.507425, mean_eps: 0.000000
 1491/5000: episode: 63, duration: 0.265s, episode steps:  18, steps per second:  68, episode reward: 38.485, mean reward:  2.138 [-2.716, 32.090], mean action: 3.556 [0.000, 12.000],  loss: 0.016818, mae: 0.354292, mean_q: 0.491446, mean_eps: 0.000000
 1528/5000: episode: 64, duration: 0.510s, episode steps:  37, steps per second:  73, episode reward: 32.686, mean reward:  0.883 [-3.000, 31.946], mean action: 4.865 [0.000, 15.000],  loss: 0.016327, mae: 0.340731, mean_q: 0.532831, mean_eps: 0.000000
 1545/5000: episode: 65, duration: 0.251s, episode steps:  17, steps per second:  68, episode reward: 38.840, mean reward:  2.285 [-2.207, 32.040], mean action: 3.529 [0.000, 16.000],  loss: 0.022261, mae: 0.366248, mean_q: 0.561946, mean_eps: 0.000000
 1572/5000: episode: 66, duration: 0.447s, episode steps:  27, steps per second:  60, episode reward: 41.831, mean reward:  1.549 [-2.162, 32.290], mean action: 2.815 [0.000, 16.000],  loss: 0.019825, mae: 0.355857, mean_q: 0.520778, mean_eps: 0.000000
 1597/5000: episode: 67, duration: 0.397s, episode steps:  25, steps per second:  63, episode reward: 32.444, mean reward:  1.298 [-2.475, 32.696], mean action: 3.800 [0.000, 14.000],  loss: 0.019916, mae: 0.348456, mean_q: 0.521355, mean_eps: 0.000000
 1622/5000: episode: 68, duration: 0.360s, episode steps:  25, steps per second:  69, episode reward: 37.499, mean reward:  1.500 [-3.000, 32.060], mean action: 4.200 [0.000, 13.000],  loss: 0.022292, mae: 0.356941, mean_q: 0.518060, mean_eps: 0.000000
 1671/5000: episode: 69, duration: 0.734s, episode steps:  49, steps per second:  67, episode reward: 32.145, mean reward:  0.656 [-2.650, 31.776], mean action: 9.245 [0.000, 18.000],  loss: 0.019859, mae: 0.349903, mean_q: 0.515514, mean_eps: 0.000000
 1703/5000: episode: 70, duration: 0.479s, episode steps:  32, steps per second:  67, episode reward: -35.450, mean reward: -1.108 [-32.222,  2.257], mean action: 4.969 [1.000, 16.000],  loss: 0.023200, mae: 0.367929, mean_q: 0.506960, mean_eps: 0.000000
 1727/5000: episode: 71, duration: 0.352s, episode steps:  24, steps per second:  68, episode reward: 41.161, mean reward:  1.715 [-2.105, 31.938], mean action: 2.250 [0.000, 16.000],  loss: 0.027931, mae: 0.387827, mean_q: 0.503385, mean_eps: 0.000000
 1750/5000: episode: 72, duration: 0.376s, episode steps:  23, steps per second:  61, episode reward: -36.000, mean reward: -1.565 [-33.000,  2.250], mean action: 4.565 [0.000, 16.000],  loss: 0.024031, mae: 0.371276, mean_q: 0.565047, mean_eps: 0.000000
 1770/5000: episode: 73, duration: 0.290s, episode steps:  20, steps per second:  69, episode reward: 38.428, mean reward:  1.921 [-2.331, 32.027], mean action: 3.450 [0.000, 19.000],  loss: 0.022930, mae: 0.362901, mean_q: 0.595930, mean_eps: 0.000000
 1794/5000: episode: 74, duration: 0.383s, episode steps:  24, steps per second:  63, episode reward: 35.717, mean reward:  1.488 [-2.541, 32.120], mean action: 3.875 [0.000, 19.000],  loss: 0.018427, mae: 0.360310, mean_q: 0.548775, mean_eps: 0.000000
 1825/5000: episode: 75, duration: 0.452s, episode steps:  31, steps per second:  69, episode reward: 41.124, mean reward:  1.327 [-2.571, 32.660], mean action: 2.677 [0.000, 19.000],  loss: 0.026457, mae: 0.388729, mean_q: 0.512857, mean_eps: 0.000000
 1845/5000: episode: 76, duration: 0.289s, episode steps:  20, steps per second:  69, episode reward: -32.490, mean reward: -1.625 [-32.290,  2.712], mean action: 4.600 [0.000, 19.000],  loss: 0.018751, mae: 0.349404, mean_q: 0.540336, mean_eps: 0.000000
 1871/5000: episode: 77, duration: 0.371s, episode steps:  26, steps per second:  70, episode reward: -32.370, mean reward: -1.245 [-32.274,  2.374], mean action: 2.846 [0.000, 12.000],  loss: 0.021635, mae: 0.362372, mean_q: 0.550286, mean_eps: 0.000000
 1899/5000: episode: 78, duration: 0.418s, episode steps:  28, steps per second:  67, episode reward: 32.133, mean reward:  1.148 [-2.509, 32.337], mean action: 6.214 [0.000, 17.000],  loss: 0.017948, mae: 0.336468, mean_q: 0.544763, mean_eps: 0.000000
 1921/5000: episode: 79, duration: 0.326s, episode steps:  22, steps per second:  67, episode reward: 35.338, mean reward:  1.606 [-2.612, 32.319], mean action: 3.864 [0.000, 19.000],  loss: 0.016673, mae: 0.336584, mean_q: 0.503416, mean_eps: 0.000000
 1939/5000: episode: 80, duration: 0.262s, episode steps:  18, steps per second:  69, episode reward: 38.102, mean reward:  2.117 [-2.395, 32.220], mean action: 3.444 [0.000, 12.000],  loss: 0.021573, mae: 0.358038, mean_q: 0.522178, mean_eps: 0.000000
 1967/5000: episode: 81, duration: 0.392s, episode steps:  28, steps per second:  71, episode reward: 32.142, mean reward:  1.148 [-3.000, 32.022], mean action: 6.571 [0.000, 19.000],  loss: 0.020855, mae: 0.353909, mean_q: 0.530882, mean_eps: 0.000000
 1993/5000: episode: 82, duration: 0.370s, episode steps:  26, steps per second:  70, episode reward: 32.989, mean reward:  1.269 [-3.000, 32.170], mean action: 4.038 [0.000, 19.000],  loss: 0.024519, mae: 0.369416, mean_q: 0.574385, mean_eps: 0.000000
 2012/5000: episode: 83, duration: 0.266s, episode steps:  19, steps per second:  71, episode reward: -38.120, mean reward: -2.006 [-32.185,  2.080], mean action: 5.474 [0.000, 19.000],  loss: 0.020014, mae: 0.356552, mean_q: 0.530692, mean_eps: 0.000000
 2034/5000: episode: 84, duration: 0.311s, episode steps:  22, steps per second:  71, episode reward: -32.440, mean reward: -1.475 [-31.491,  2.220], mean action: 4.636 [0.000, 19.000],  loss: 0.016376, mae: 0.331408, mean_q: 0.545644, mean_eps: 0.000000
 2061/5000: episode: 85, duration: 0.375s, episode steps:  27, steps per second:  72, episode reward: -35.810, mean reward: -1.326 [-31.983,  2.440], mean action: 4.259 [0.000, 19.000],  loss: 0.022611, mae: 0.360456, mean_q: 0.544521, mean_eps: 0.000000
 2081/5000: episode: 86, duration: 0.282s, episode steps:  20, steps per second:  71, episode reward: 35.181, mean reward:  1.759 [-2.518, 32.617], mean action: 4.000 [0.000, 15.000],  loss: 0.018423, mae: 0.343463, mean_q: 0.564490, mean_eps: 0.000000
 2106/5000: episode: 87, duration: 0.358s, episode steps:  25, steps per second:  70, episode reward: 35.615, mean reward:  1.425 [-2.459, 32.340], mean action: 3.600 [0.000, 15.000],  loss: 0.019477, mae: 0.346347, mean_q: 0.518365, mean_eps: 0.000000
 2125/5000: episode: 88, duration: 0.276s, episode steps:  19, steps per second:  69, episode reward: 35.950, mean reward:  1.892 [-2.614, 32.410], mean action: 4.368 [0.000, 15.000],  loss: 0.020020, mae: 0.350953, mean_q: 0.543513, mean_eps: 0.000000
 2145/5000: episode: 89, duration: 0.297s, episode steps:  20, steps per second:  67, episode reward: 32.756, mean reward:  1.638 [-3.000, 32.390], mean action: 4.950 [1.000, 15.000],  loss: 0.023480, mae: 0.361207, mean_q: 0.505937, mean_eps: 0.000000
 2170/5000: episode: 90, duration: 0.358s, episode steps:  25, steps per second:  70, episode reward: 32.804, mean reward:  1.312 [-2.300, 32.020], mean action: 3.080 [0.000, 12.000],  loss: 0.020422, mae: 0.347868, mean_q: 0.501411, mean_eps: 0.000000
 2186/5000: episode: 91, duration: 3.446s, episode steps:  16, steps per second:   5, episode reward: 44.650, mean reward:  2.791 [-2.413, 32.749], mean action: 3.000 [0.000, 12.000],  loss: 0.018146, mae: 0.331792, mean_q: 0.518867, mean_eps: 0.000000
 2208/5000: episode: 92, duration: 0.458s, episode steps:  22, steps per second:  48, episode reward: 33.000, mean reward:  1.500 [-2.403, 30.000], mean action: 3.545 [0.000, 19.000],  loss: 0.025524, mae: 0.365918, mean_q: 0.499888, mean_eps: 0.000000
 2232/5000: episode: 93, duration: 0.461s, episode steps:  24, steps per second:  52, episode reward: 33.000, mean reward:  1.375 [-2.267, 30.385], mean action: 5.167 [0.000, 15.000],  loss: 0.021059, mae: 0.343554, mean_q: 0.521053, mean_eps: 0.000000
 2278/5000: episode: 94, duration: 0.778s, episode steps:  46, steps per second:  59, episode reward: 41.309, mean reward:  0.898 [-2.401, 32.170], mean action: 4.848 [1.000, 20.000],  loss: 0.019530, mae: 0.343592, mean_q: 0.479306, mean_eps: 0.000000
 2302/5000: episode: 95, duration: 0.403s, episode steps:  24, steps per second:  60, episode reward: -33.000, mean reward: -1.375 [-32.257,  3.000], mean action: 8.500 [0.000, 16.000],  loss: 0.018388, mae: 0.340418, mean_q: 0.437482, mean_eps: 0.000000
 2324/5000: episode: 96, duration: 0.324s, episode steps:  22, steps per second:  68, episode reward: 40.702, mean reward:  1.850 [-2.610, 32.210], mean action: 4.864 [0.000, 16.000],  loss: 0.020088, mae: 0.350703, mean_q: 0.468456, mean_eps: 0.000000
 2347/5000: episode: 97, duration: 0.385s, episode steps:  23, steps per second:  60, episode reward: 37.882, mean reward:  1.647 [-2.638, 33.000], mean action: 8.565 [0.000, 20.000],  loss: 0.018640, mae: 0.345227, mean_q: 0.499671, mean_eps: 0.000000
 2370/5000: episode: 98, duration: 0.331s, episode steps:  23, steps per second:  69, episode reward: 35.150, mean reward:  1.528 [-2.805, 32.454], mean action: 6.826 [0.000, 16.000],  loss: 0.021722, mae: 0.356990, mean_q: 0.546135, mean_eps: 0.000000
 2393/5000: episode: 99, duration: 0.347s, episode steps:  23, steps per second:  66, episode reward: 33.000, mean reward:  1.435 [-3.000, 32.280], mean action: 8.652 [0.000, 17.000],  loss: 0.016678, mae: 0.334656, mean_q: 0.552142, mean_eps: 0.000000
 2425/5000: episode: 100, duration: 0.444s, episode steps:  32, steps per second:  72, episode reward: -32.370, mean reward: -1.012 [-31.972,  2.560], mean action: 7.469 [0.000, 20.000],  loss: 0.017196, mae: 0.333623, mean_q: 0.533554, mean_eps: 0.000000
 2448/5000: episode: 101, duration: 0.323s, episode steps:  23, steps per second:  71, episode reward: -35.350, mean reward: -1.537 [-32.144,  2.280], mean action: 6.435 [0.000, 20.000],  loss: 0.020773, mae: 0.349317, mean_q: 0.548680, mean_eps: 0.000000
 2469/5000: episode: 102, duration: 0.311s, episode steps:  21, steps per second:  68, episode reward: 44.359, mean reward:  2.112 [-2.093, 33.000], mean action: 4.000 [0.000, 16.000],  loss: 0.023174, mae: 0.354403, mean_q: 0.520136, mean_eps: 0.000000
 2494/5000: episode: 103, duration: 0.364s, episode steps:  25, steps per second:  69, episode reward: -41.630, mean reward: -1.665 [-32.526,  2.250], mean action: 7.560 [0.000, 15.000],  loss: 0.018852, mae: 0.340129, mean_q: 0.493469, mean_eps: 0.000000
 2510/5000: episode: 104, duration: 0.254s, episode steps:  16, steps per second:  63, episode reward: 38.693, mean reward:  2.418 [-2.298, 31.963], mean action: 5.812 [0.000, 20.000],  loss: 0.019066, mae: 0.351570, mean_q: 0.522958, mean_eps: 0.000000
 2553/5000: episode: 105, duration: 0.600s, episode steps:  43, steps per second:  72, episode reward: 32.787, mean reward:  0.762 [-2.395, 31.987], mean action: 4.535 [0.000, 12.000],  loss: 0.019977, mae: 0.349061, mean_q: 0.525323, mean_eps: 0.000000
 2569/5000: episode: 106, duration: 0.235s, episode steps:  16, steps per second:  68, episode reward: 40.869, mean reward:  2.554 [-2.437, 32.834], mean action: 4.312 [0.000, 16.000],  loss: 0.017778, mae: 0.339421, mean_q: 0.506499, mean_eps: 0.000000
 2596/5000: episode: 107, duration: 0.426s, episode steps:  27, steps per second:  63, episode reward: 32.121, mean reward:  1.190 [-2.519, 31.913], mean action: 4.444 [0.000, 19.000],  loss: 0.021979, mae: 0.358938, mean_q: 0.514608, mean_eps: 0.000000
 2608/5000: episode: 108, duration: 0.237s, episode steps:  12, steps per second:  51, episode reward: 41.439, mean reward:  3.453 [-2.375, 33.000], mean action: 3.833 [0.000, 12.000],  loss: 0.013808, mae: 0.314646, mean_q: 0.521125, mean_eps: 0.000000
 2661/5000: episode: 109, duration: 0.924s, episode steps:  53, steps per second:  57, episode reward: 32.253, mean reward:  0.609 [-2.361, 32.060], mean action: 7.000 [0.000, 19.000],  loss: 0.018579, mae: 0.349103, mean_q: 0.489053, mean_eps: 0.000000
 2684/5000: episode: 110, duration: 0.335s, episode steps:  23, steps per second:  69, episode reward: -32.910, mean reward: -1.431 [-31.951,  2.718], mean action: 5.130 [0.000, 19.000],  loss: 0.019305, mae: 0.346899, mean_q: 0.559003, mean_eps: 0.000000
 2708/5000: episode: 111, duration: 0.366s, episode steps:  24, steps per second:  66, episode reward: 35.718, mean reward:  1.488 [-3.000, 32.718], mean action: 5.208 [0.000, 19.000],  loss: 0.018663, mae: 0.349707, mean_q: 0.529573, mean_eps: 0.000000
 2726/5000: episode: 112, duration: 0.252s, episode steps:  18, steps per second:  72, episode reward: -41.370, mean reward: -2.298 [-32.309,  2.685], mean action: 6.056 [0.000, 19.000],  loss: 0.021421, mae: 0.353021, mean_q: 0.486539, mean_eps: 0.000000
 2740/5000: episode: 113, duration: 0.226s, episode steps:  14, steps per second:  62, episode reward: 39.000, mean reward:  2.786 [-3.000, 33.000], mean action: 6.929 [0.000, 20.000],  loss: 0.016886, mae: 0.337405, mean_q: 0.501225, mean_eps: 0.000000
 2763/5000: episode: 114, duration: 0.342s, episode steps:  23, steps per second:  67, episode reward: 38.845, mean reward:  1.689 [-3.000, 32.115], mean action: 3.261 [0.000, 19.000],  loss: 0.020554, mae: 0.366693, mean_q: 0.500588, mean_eps: 0.000000
 2779/5000: episode: 115, duration: 0.241s, episode steps:  16, steps per second:  66, episode reward: 38.945, mean reward:  2.434 [-3.000, 33.000], mean action: 4.938 [0.000, 19.000],  loss: 0.019084, mae: 0.357980, mean_q: 0.544817, mean_eps: 0.000000
 2794/5000: episode: 116, duration: 0.224s, episode steps:  15, steps per second:  67, episode reward: 41.477, mean reward:  2.765 [-2.360, 32.598], mean action: 5.267 [0.000, 19.000],  loss: 0.021994, mae: 0.372258, mean_q: 0.545155, mean_eps: 0.000000
 2834/5000: episode: 117, duration: 0.552s, episode steps:  40, steps per second:  72, episode reward: 32.658, mean reward:  0.816 [-2.476, 32.392], mean action: 9.025 [0.000, 16.000],  loss: 0.020477, mae: 0.363256, mean_q: 0.509664, mean_eps: 0.000000
 2857/5000: episode: 118, duration: 0.338s, episode steps:  23, steps per second:  68, episode reward: 38.926, mean reward:  1.692 [-2.277, 32.066], mean action: 4.261 [1.000, 16.000],  loss: 0.021471, mae: 0.362621, mean_q: 0.486308, mean_eps: 0.000000
 2882/5000: episode: 119, duration: 0.352s, episode steps:  25, steps per second:  71, episode reward: 35.753, mean reward:  1.430 [-3.000, 32.540], mean action: 6.800 [0.000, 19.000],  loss: 0.018578, mae: 0.347693, mean_q: 0.519227, mean_eps: 0.000000
 2904/5000: episode: 120, duration: 0.329s, episode steps:  22, steps per second:  67, episode reward: 35.714, mean reward:  1.623 [-2.418, 32.903], mean action: 7.682 [0.000, 20.000],  loss: 0.020308, mae: 0.354612, mean_q: 0.481020, mean_eps: 0.000000
 2930/5000: episode: 121, duration: 0.382s, episode steps:  26, steps per second:  68, episode reward: 35.761, mean reward:  1.375 [-3.000, 32.100], mean action: 8.462 [0.000, 20.000],  loss: 0.021009, mae: 0.363013, mean_q: 0.446842, mean_eps: 0.000000
 2945/5000: episode: 122, duration: 0.230s, episode steps:  15, steps per second:  65, episode reward: 44.654, mean reward:  2.977 [-2.148, 32.320], mean action: 3.467 [2.000, 9.000],  loss: 0.025829, mae: 0.382491, mean_q: 0.478138, mean_eps: 0.000000
 2970/5000: episode: 123, duration: 0.390s, episode steps:  25, steps per second:  64, episode reward: 32.056, mean reward:  1.282 [-3.000, 30.503], mean action: 6.680 [0.000, 20.000],  loss: 0.021289, mae: 0.356169, mean_q: 0.553251, mean_eps: 0.000000
 2999/5000: episode: 124, duration: 0.420s, episode steps:  29, steps per second:  69, episode reward: -39.000, mean reward: -1.345 [-32.411,  2.080], mean action: 4.586 [0.000, 16.000],  loss: 0.019407, mae: 0.347333, mean_q: 0.552128, mean_eps: 0.000000
 3029/5000: episode: 125, duration: 0.469s, episode steps:  30, steps per second:  64, episode reward: -33.000, mean reward: -1.100 [-32.320,  2.430], mean action: 4.267 [0.000, 15.000],  loss: 0.017003, mae: 0.339111, mean_q: 0.534605, mean_eps: 0.000000
 3049/5000: episode: 126, duration: 0.289s, episode steps:  20, steps per second:  69, episode reward: 38.875, mean reward:  1.944 [-2.470, 33.000], mean action: 4.550 [0.000, 14.000],  loss: 0.020803, mae: 0.359481, mean_q: 0.469837, mean_eps: 0.000000
 3067/5000: episode: 127, duration: 0.264s, episode steps:  18, steps per second:  68, episode reward: 38.160, mean reward:  2.120 [-2.504, 31.868], mean action: 3.611 [0.000, 20.000],  loss: 0.024811, mae: 0.375953, mean_q: 0.498846, mean_eps: 0.000000
 3108/5000: episode: 128, duration: 0.593s, episode steps:  41, steps per second:  69, episode reward: 38.454, mean reward:  0.938 [-3.000, 32.100], mean action: 4.122 [0.000, 20.000],  loss: 0.020368, mae: 0.356406, mean_q: 0.510620, mean_eps: 0.000000
 3125/5000: episode: 129, duration: 0.259s, episode steps:  17, steps per second:  66, episode reward: -39.000, mean reward: -2.294 [-30.910,  2.897], mean action: 4.000 [0.000, 12.000],  loss: 0.020962, mae: 0.353583, mean_q: 0.561235, mean_eps: 0.000000
 3151/5000: episode: 130, duration: 0.403s, episode steps:  26, steps per second:  64, episode reward: 39.926, mean reward:  1.536 [-2.181, 32.240], mean action: 7.692 [1.000, 19.000],  loss: 0.022510, mae: 0.361259, mean_q: 0.587420, mean_eps: 0.000000
 3166/5000: episode: 131, duration: 0.228s, episode steps:  15, steps per second:  66, episode reward: 39.000, mean reward:  2.600 [-3.000, 32.490], mean action: 5.667 [0.000, 15.000],  loss: 0.019890, mae: 0.349161, mean_q: 0.599372, mean_eps: 0.000000
 3190/5000: episode: 132, duration: 0.356s, episode steps:  24, steps per second:  68, episode reward: 37.929, mean reward:  1.580 [-2.432, 31.858], mean action: 4.125 [0.000, 19.000],  loss: 0.020193, mae: 0.350579, mean_q: 0.526699, mean_eps: 0.000000
 3208/5000: episode: 133, duration: 0.281s, episode steps:  18, steps per second:  64, episode reward: 41.951, mean reward:  2.331 [-2.900, 32.219], mean action: 2.889 [1.000, 9.000],  loss: 0.018607, mae: 0.345510, mean_q: 0.513922, mean_eps: 0.000000
 3230/5000: episode: 134, duration: 0.636s, episode steps:  22, steps per second:  35, episode reward: 32.258, mean reward:  1.466 [-2.844, 31.378], mean action: 7.182 [0.000, 20.000],  loss: 0.019630, mae: 0.349909, mean_q: 0.591488, mean_eps: 0.000000
 3250/5000: episode: 135, duration: 0.297s, episode steps:  20, steps per second:  67, episode reward: -36.000, mean reward: -1.800 [-32.090,  2.163], mean action: 5.050 [0.000, 19.000],  loss: 0.019388, mae: 0.341311, mean_q: 0.556891, mean_eps: 0.000000
 3288/5000: episode: 136, duration: 0.546s, episode steps:  38, steps per second:  70, episode reward: 38.599, mean reward:  1.016 [-2.342, 32.061], mean action: 2.789 [0.000, 16.000],  loss: 0.018118, mae: 0.345270, mean_q: 0.546399, mean_eps: 0.000000
 3299/5000: episode: 137, duration: 0.167s, episode steps:  11, steps per second:  66, episode reward: 45.000, mean reward:  4.091 [-3.000, 33.000], mean action: 2.455 [0.000, 12.000],  loss: 0.020159, mae: 0.352548, mean_q: 0.560286, mean_eps: 0.000000
 3318/5000: episode: 138, duration: 0.270s, episode steps:  19, steps per second:  70, episode reward: 38.056, mean reward:  2.003 [-2.508, 32.423], mean action: 3.000 [1.000, 16.000],  loss: 0.020977, mae: 0.353913, mean_q: 0.549811, mean_eps: 0.000000
 3331/5000: episode: 139, duration: 0.201s, episode steps:  13, steps per second:  65, episode reward: 41.927, mean reward:  3.225 [-2.199, 32.327], mean action: 3.385 [0.000, 12.000],  loss: 0.020081, mae: 0.354189, mean_q: 0.557071, mean_eps: 0.000000
 3348/5000: episode: 140, duration: 0.258s, episode steps:  17, steps per second:  66, episode reward: 35.580, mean reward:  2.093 [-3.000, 32.160], mean action: 6.706 [0.000, 14.000],  loss: 0.018431, mae: 0.352001, mean_q: 0.572770, mean_eps: 0.000000
 3366/5000: episode: 141, duration: 0.368s, episode steps:  18, steps per second:  49, episode reward: 39.000, mean reward:  2.167 [-2.538, 32.440], mean action: 3.056 [0.000, 12.000],  loss: 0.023443, mae: 0.366891, mean_q: 0.551041, mean_eps: 0.000000
 3382/5000: episode: 142, duration: 0.356s, episode steps:  16, steps per second:  45, episode reward: 38.349, mean reward:  2.397 [-2.903, 31.830], mean action: 5.188 [0.000, 14.000],  loss: 0.022252, mae: 0.356929, mean_q: 0.551308, mean_eps: 0.000000
 3406/5000: episode: 143, duration: 0.341s, episode steps:  24, steps per second:  70, episode reward: 33.000, mean reward:  1.375 [-2.751, 32.330], mean action: 5.375 [0.000, 14.000],  loss: 0.020465, mae: 0.358641, mean_q: 0.611269, mean_eps: 0.000000
 3426/5000: episode: 144, duration: 0.299s, episode steps:  20, steps per second:  67, episode reward: 38.231, mean reward:  1.912 [-2.403, 32.297], mean action: 4.900 [0.000, 14.000],  loss: 0.022325, mae: 0.368304, mean_q: 0.611945, mean_eps: 0.000000
 3447/5000: episode: 145, duration: 0.306s, episode steps:  21, steps per second:  69, episode reward: 38.658, mean reward:  1.841 [-2.656, 32.281], mean action: 3.048 [1.000, 12.000],  loss: 0.022823, mae: 0.366408, mean_q: 0.611628, mean_eps: 0.000000
 3468/5000: episode: 146, duration: 0.308s, episode steps:  21, steps per second:  68, episode reward: 40.968, mean reward:  1.951 [-3.000, 32.340], mean action: 3.238 [0.000, 12.000],  loss: 0.024092, mae: 0.364552, mean_q: 0.584593, mean_eps: 0.000000
 3488/5000: episode: 147, duration: 0.286s, episode steps:  20, steps per second:  70, episode reward: 32.675, mean reward:  1.634 [-3.000, 32.575], mean action: 5.500 [0.000, 16.000],  loss: 0.019794, mae: 0.350788, mean_q: 0.546609, mean_eps: 0.000000
 3509/5000: episode: 148, duration: 0.317s, episode steps:  21, steps per second:  66, episode reward: 38.819, mean reward:  1.849 [-2.233, 31.889], mean action: 4.238 [0.000, 16.000],  loss: 0.022343, mae: 0.361396, mean_q: 0.555822, mean_eps: 0.000000
 3525/5000: episode: 149, duration: 0.257s, episode steps:  16, steps per second:  62, episode reward: 41.586, mean reward:  2.599 [-2.247, 32.203], mean action: 4.312 [0.000, 15.000],  loss: 0.021062, mae: 0.361630, mean_q: 0.584320, mean_eps: 0.000000
 3551/5000: episode: 150, duration: 0.402s, episode steps:  26, steps per second:  65, episode reward: -33.000, mean reward: -1.269 [-32.293,  2.627], mean action: 5.154 [0.000, 16.000],  loss: 0.020711, mae: 0.348229, mean_q: 0.610807, mean_eps: 0.000000
 3580/5000: episode: 151, duration: 0.528s, episode steps:  29, steps per second:  55, episode reward: -32.230, mean reward: -1.111 [-32.148,  2.391], mean action: 4.172 [0.000, 15.000],  loss: 0.022488, mae: 0.358369, mean_q: 0.583558, mean_eps: 0.000000
 3607/5000: episode: 152, duration: 0.409s, episode steps:  27, steps per second:  66, episode reward: -36.000, mean reward: -1.333 [-33.000,  2.070], mean action: 4.407 [0.000, 16.000],  loss: 0.022107, mae: 0.361981, mean_q: 0.580286, mean_eps: 0.000000
 3646/5000: episode: 153, duration: 0.649s, episode steps:  39, steps per second:  60, episode reward: -32.540, mean reward: -0.834 [-32.014,  3.000], mean action: 5.949 [0.000, 17.000],  loss: 0.018553, mae: 0.347140, mean_q: 0.541387, mean_eps: 0.000000
 3670/5000: episode: 154, duration: 0.393s, episode steps:  24, steps per second:  61, episode reward: 35.691, mean reward:  1.487 [-2.548, 31.981], mean action: 3.708 [1.000, 16.000],  loss: 0.023037, mae: 0.368540, mean_q: 0.508235, mean_eps: 0.000000
 3693/5000: episode: 155, duration: 0.336s, episode steps:  23, steps per second:  69, episode reward: -35.060, mean reward: -1.524 [-32.710,  2.138], mean action: 4.000 [0.000, 16.000],  loss: 0.021086, mae: 0.358353, mean_q: 0.554958, mean_eps: 0.000000
 3715/5000: episode: 156, duration: 0.346s, episode steps:  22, steps per second:  63, episode reward: 41.054, mean reward:  1.866 [-2.533, 32.280], mean action: 4.682 [0.000, 16.000],  loss: 0.018695, mae: 0.338415, mean_q: 0.538361, mean_eps: 0.000000
 3744/5000: episode: 157, duration: 0.434s, episode steps:  29, steps per second:  67, episode reward: 32.879, mean reward:  1.134 [-2.416, 32.080], mean action: 8.034 [0.000, 20.000],  loss: 0.021445, mae: 0.357654, mean_q: 0.529076, mean_eps: 0.000000
 3767/5000: episode: 158, duration: 0.371s, episode steps:  23, steps per second:  62, episode reward: 35.479, mean reward:  1.543 [-2.403, 31.963], mean action: 4.087 [0.000, 19.000],  loss: 0.017381, mae: 0.335593, mean_q: 0.456120, mean_eps: 0.000000
 3807/5000: episode: 159, duration: 0.552s, episode steps:  40, steps per second:  72, episode reward: -32.250, mean reward: -0.806 [-32.289,  2.852], mean action: 4.375 [0.000, 20.000],  loss: 0.017754, mae: 0.335683, mean_q: 0.472951, mean_eps: 0.000000
 3834/5000: episode: 160, duration: 0.374s, episode steps:  27, steps per second:  72, episode reward: -32.880, mean reward: -1.218 [-32.185,  3.000], mean action: 7.444 [0.000, 20.000],  loss: 0.023785, mae: 0.363014, mean_q: 0.512421, mean_eps: 0.000000
 3861/5000: episode: 161, duration: 0.387s, episode steps:  27, steps per second:  70, episode reward: 36.000, mean reward:  1.333 [-2.319, 32.240], mean action: 2.963 [0.000, 16.000],  loss: 0.020448, mae: 0.350148, mean_q: 0.518108, mean_eps: 0.000000
 3886/5000: episode: 162, duration: 0.352s, episode steps:  25, steps per second:  71, episode reward: 38.903, mean reward:  1.556 [-2.273, 32.313], mean action: 2.800 [0.000, 15.000],  loss: 0.022241, mae: 0.354981, mean_q: 0.548940, mean_eps: 0.000000
 3914/5000: episode: 163, duration: 0.404s, episode steps:  28, steps per second:  69, episode reward: 34.360, mean reward:  1.227 [-2.177, 32.032], mean action: 3.857 [0.000, 16.000],  loss: 0.020124, mae: 0.346417, mean_q: 0.556305, mean_eps: 0.000000
 3935/5000: episode: 164, duration: 0.304s, episode steps:  21, steps per second:  69, episode reward: 38.352, mean reward:  1.826 [-2.478, 32.010], mean action: 3.048 [0.000, 19.000],  loss: 0.020957, mae: 0.360176, mean_q: 0.497629, mean_eps: 0.000000
 3956/5000: episode: 165, duration: 0.300s, episode steps:  21, steps per second:  70, episode reward: 35.843, mean reward:  1.707 [-2.482, 32.143], mean action: 3.714 [0.000, 12.000],  loss: 0.016917, mae: 0.343094, mean_q: 0.449618, mean_eps: 0.000000
 3972/5000: episode: 166, duration: 0.242s, episode steps:  16, steps per second:  66, episode reward: 38.635, mean reward:  2.415 [-3.000, 32.635], mean action: 2.312 [0.000, 12.000],  loss: 0.020754, mae: 0.371749, mean_q: 0.424670, mean_eps: 0.000000
 3993/5000: episode: 167, duration: 0.298s, episode steps:  21, steps per second:  70, episode reward: 32.203, mean reward:  1.533 [-2.799, 32.270], mean action: 4.143 [0.000, 16.000],  loss: 0.020360, mae: 0.355962, mean_q: 0.437225, mean_eps: 0.000000
 4021/5000: episode: 168, duration: 0.394s, episode steps:  28, steps per second:  71, episode reward: 35.737, mean reward:  1.276 [-2.450, 32.080], mean action: 3.821 [0.000, 14.000],  loss: 0.019419, mae: 0.351747, mean_q: 0.477425, mean_eps: 0.000000
 4040/5000: episode: 169, duration: 0.273s, episode steps:  19, steps per second:  70, episode reward: 39.000, mean reward:  2.053 [-2.639, 32.410], mean action: 5.895 [0.000, 14.000],  loss: 0.018719, mae: 0.346339, mean_q: 0.550265, mean_eps: 0.000000
 4067/5000: episode: 170, duration: 0.442s, episode steps:  27, steps per second:  61, episode reward: 35.350, mean reward:  1.309 [-2.530, 32.410], mean action: 4.630 [0.000, 14.000],  loss: 0.022625, mae: 0.363913, mean_q: 0.591138, mean_eps: 0.000000
 4094/5000: episode: 171, duration: 0.401s, episode steps:  27, steps per second:  67, episode reward: 42.000, mean reward:  1.556 [-2.427, 32.170], mean action: 3.037 [0.000, 16.000],  loss: 0.020445, mae: 0.357847, mean_q: 0.629418, mean_eps: 0.000000
 4122/5000: episode: 172, duration: 0.397s, episode steps:  28, steps per second:  70, episode reward: -35.950, mean reward: -1.284 [-32.075,  3.000], mean action: 6.607 [0.000, 17.000],  loss: 0.024071, mae: 0.377635, mean_q: 0.572212, mean_eps: 0.000000
 4135/5000: episode: 173, duration: 0.199s, episode steps:  13, steps per second:  65, episode reward: 38.113, mean reward:  2.932 [-3.000, 33.000], mean action: 4.846 [0.000, 16.000],  loss: 0.018816, mae: 0.360230, mean_q: 0.551960, mean_eps: 0.000000
 4157/5000: episode: 174, duration: 0.312s, episode steps:  22, steps per second:  70, episode reward: 33.000, mean reward:  1.500 [-3.000, 32.150], mean action: 5.091 [0.000, 16.000],  loss: 0.018829, mae: 0.350008, mean_q: 0.511958, mean_eps: 0.000000
 4176/5000: episode: 175, duration: 0.284s, episode steps:  19, steps per second:  67, episode reward: 32.147, mean reward:  1.692 [-3.000, 32.380], mean action: 5.421 [0.000, 19.000],  loss: 0.022475, mae: 0.366342, mean_q: 0.571787, mean_eps: 0.000000
 4201/5000: episode: 176, duration: 0.363s, episode steps:  25, steps per second:  69, episode reward: 44.648, mean reward:  1.786 [-2.360, 33.000], mean action: 4.400 [0.000, 12.000],  loss: 0.019781, mae: 0.350485, mean_q: 0.557236, mean_eps: 0.000000
 4221/5000: episode: 177, duration: 0.287s, episode steps:  20, steps per second:  70, episode reward: 36.000, mean reward:  1.800 [-2.715, 32.330], mean action: 5.450 [0.000, 16.000],  loss: 0.024021, mae: 0.371826, mean_q: 0.554336, mean_eps: 0.000000
 4237/5000: episode: 178, duration: 0.239s, episode steps:  16, steps per second:  67, episode reward: 41.170, mean reward:  2.573 [-2.391, 32.430], mean action: 5.125 [0.000, 16.000],  loss: 0.019564, mae: 0.351514, mean_q: 0.536525, mean_eps: 0.000000
 4275/5000: episode: 179, duration: 0.534s, episode steps:  38, steps per second:  71, episode reward: 32.046, mean reward:  0.843 [-2.148, 32.150], mean action: 4.026 [0.000, 16.000],  loss: 0.021106, mae: 0.357873, mean_q: 0.520621, mean_eps: 0.000000
 4298/5000: episode: 180, duration: 0.328s, episode steps:  23, steps per second:  70, episode reward: 36.000, mean reward:  1.565 [-3.000, 32.940], mean action: 5.043 [0.000, 16.000],  loss: 0.020827, mae: 0.360537, mean_q: 0.478137, mean_eps: 0.000000
 4312/5000: episode: 181, duration: 0.216s, episode steps:  14, steps per second:  65, episode reward: 44.227, mean reward:  3.159 [-2.007, 32.116], mean action: 2.571 [0.000, 19.000],  loss: 0.019501, mae: 0.349965, mean_q: 0.438724, mean_eps: 0.000000
 4336/5000: episode: 182, duration: 0.344s, episode steps:  24, steps per second:  70, episode reward: 38.032, mean reward:  1.585 [-2.217, 32.603], mean action: 2.750 [0.000, 11.000],  loss: 0.022961, mae: 0.368058, mean_q: 0.481383, mean_eps: 0.000000
 4350/5000: episode: 183, duration: 0.210s, episode steps:  14, steps per second:  67, episode reward: 38.372, mean reward:  2.741 [-2.662, 32.042], mean action: 3.286 [0.000, 16.000],  loss: 0.028040, mae: 0.397878, mean_q: 0.466887, mean_eps: 0.000000
 4373/5000: episode: 184, duration: 0.327s, episode steps:  23, steps per second:  70, episode reward: 32.906, mean reward:  1.431 [-3.000, 32.906], mean action: 3.304 [0.000, 16.000],  loss: 0.021372, mae: 0.372220, mean_q: 0.490945, mean_eps: 0.000000
 4393/5000: episode: 185, duration: 0.283s, episode steps:  20, steps per second:  71, episode reward: 43.125, mean reward:  2.156 [-2.205, 32.050], mean action: 2.700 [0.000, 12.000],  loss: 0.018631, mae: 0.351625, mean_q: 0.448689, mean_eps: 0.000000
 4420/5000: episode: 186, duration: 0.378s, episode steps:  27, steps per second:  71, episode reward: 32.459, mean reward:  1.202 [-2.274, 32.050], mean action: 5.741 [0.000, 16.000],  loss: 0.020545, mae: 0.360332, mean_q: 0.455756, mean_eps: 0.000000
 4449/5000: episode: 187, duration: 0.427s, episode steps:  29, steps per second:  68, episode reward: -38.180, mean reward: -1.317 [-31.511,  2.401], mean action: 5.862 [0.000, 16.000],  loss: 0.018684, mae: 0.338786, mean_q: 0.454706, mean_eps: 0.000000
 4467/5000: episode: 188, duration: 0.259s, episode steps:  18, steps per second:  69, episode reward: 35.025, mean reward:  1.946 [-3.000, 31.677], mean action: 4.000 [0.000, 16.000],  loss: 0.019428, mae: 0.347439, mean_q: 0.445505, mean_eps: 0.000000
 4493/5000: episode: 189, duration: 0.367s, episode steps:  26, steps per second:  71, episode reward: -35.700, mean reward: -1.373 [-32.374,  2.480], mean action: 8.231 [0.000, 16.000],  loss: 0.027370, mae: 0.375122, mean_q: 0.522220, mean_eps: 0.000000
 4532/5000: episode: 190, duration: 0.534s, episode steps:  39, steps per second:  73, episode reward: -32.590, mean reward: -0.836 [-32.100,  2.380], mean action: 3.769 [0.000, 16.000],  loss: 0.019826, mae: 0.347898, mean_q: 0.551484, mean_eps: 0.000000
 4560/5000: episode: 191, duration: 0.389s, episode steps:  28, steps per second:  72, episode reward: -33.000, mean reward: -1.179 [-32.363,  2.530], mean action: 4.000 [0.000, 16.000],  loss: 0.025566, mae: 0.375652, mean_q: 0.463826, mean_eps: 0.000000
 4570/5000: episode: 192, duration: 0.158s, episode steps:  10, steps per second:  63, episode reward: 44.171, mean reward:  4.417 [-2.379, 33.000], mean action: 3.500 [0.000, 15.000],  loss: 0.020189, mae: 0.356140, mean_q: 0.455358, mean_eps: 0.000000
 4594/5000: episode: 193, duration: 0.339s, episode steps:  24, steps per second:  71, episode reward: 35.529, mean reward:  1.480 [-2.810, 32.300], mean action: 4.417 [0.000, 15.000],  loss: 0.019149, mae: 0.348418, mean_q: 0.496602, mean_eps: 0.000000
 4619/5000: episode: 194, duration: 0.366s, episode steps:  25, steps per second:  68, episode reward: 32.614, mean reward:  1.305 [-2.782, 31.963], mean action: 6.840 [0.000, 15.000],  loss: 0.020339, mae: 0.352314, mean_q: 0.533944, mean_eps: 0.000000
 4641/5000: episode: 195, duration: 0.326s, episode steps:  22, steps per second:  67, episode reward: 40.664, mean reward:  1.848 [-3.000, 33.000], mean action: 5.000 [0.000, 14.000],  loss: 0.015981, mae: 0.326836, mean_q: 0.530669, mean_eps: 0.000000
 4665/5000: episode: 196, duration: 0.345s, episode steps:  24, steps per second:  70, episode reward: 35.026, mean reward:  1.459 [-3.000, 32.272], mean action: 3.375 [0.000, 11.000],  loss: 0.020493, mae: 0.351741, mean_q: 0.547595, mean_eps: 0.000000
 4686/5000: episode: 197, duration: 0.297s, episode steps:  21, steps per second:  71, episode reward: 35.419, mean reward:  1.687 [-3.000, 32.250], mean action: 5.333 [0.000, 20.000],  loss: 0.021698, mae: 0.355468, mean_q: 0.522113, mean_eps: 0.000000
 4715/5000: episode: 198, duration: 0.411s, episode steps:  29, steps per second:  71, episode reward: 32.079, mean reward:  1.106 [-3.000, 32.079], mean action: 4.207 [0.000, 20.000],  loss: 0.019269, mae: 0.350045, mean_q: 0.506910, mean_eps: 0.000000
 4782/5000: episode: 199, duration: 0.879s, episode steps:  67, steps per second:  76, episode reward: -38.590, mean reward: -0.576 [-32.793,  2.413], mean action: 2.910 [0.000, 15.000],  loss: 0.019832, mae: 0.344686, mean_q: 0.493850, mean_eps: 0.000000
 4813/5000: episode: 200, duration: 0.433s, episode steps:  31, steps per second:  72, episode reward: -35.210, mean reward: -1.136 [-32.124,  2.410], mean action: 9.548 [0.000, 20.000],  loss: 0.020750, mae: 0.342423, mean_q: 0.528451, mean_eps: 0.000000
 4838/5000: episode: 201, duration: 0.366s, episode steps:  25, steps per second:  68, episode reward: 32.699, mean reward:  1.308 [-3.000, 30.079], mean action: 3.920 [0.000, 20.000],  loss: 0.023130, mae: 0.357077, mean_q: 0.554432, mean_eps: 0.000000
 4860/5000: episode: 202, duration: 0.369s, episode steps:  22, steps per second:  60, episode reward: 35.007, mean reward:  1.591 [-3.000, 32.007], mean action: 4.182 [0.000, 19.000],  loss: 0.021428, mae: 0.352769, mean_q: 0.566028, mean_eps: 0.000000
 4897/5000: episode: 203, duration: 0.534s, episode steps:  37, steps per second:  69, episode reward: -32.500, mean reward: -0.878 [-32.123,  2.462], mean action: 5.757 [0.000, 20.000],  loss: 0.022490, mae: 0.367149, mean_q: 0.569393, mean_eps: 0.000000
 4913/5000: episode: 204, duration: 0.246s, episode steps:  16, steps per second:  65, episode reward: 45.000, mean reward:  2.812 [-2.255, 32.140], mean action: 1.875 [0.000, 12.000],  loss: 0.017180, mae: 0.336882, mean_q: 0.559577, mean_eps: 0.000000
 4939/5000: episode: 205, duration: 0.557s, episode steps:  26, steps per second:  47, episode reward: -32.030, mean reward: -1.232 [-33.000,  2.602], mean action: 3.538 [0.000, 12.000],  loss: 0.017928, mae: 0.345549, mean_q: 0.524260, mean_eps: 0.000000
 4964/5000: episode: 206, duration: 0.402s, episode steps:  25, steps per second:  62, episode reward: 36.000, mean reward:  1.440 [-2.305, 32.100], mean action: 3.360 [1.000, 9.000],  loss: 0.020035, mae: 0.356490, mean_q: 0.554000, mean_eps: 0.000000
 4993/5000: episode: 207, duration: 0.417s, episode steps:  29, steps per second:  69, episode reward: 32.917, mean reward:  1.135 [-3.000, 32.270], mean action: 6.483 [0.000, 20.000],  loss: 0.018879, mae: 0.350607, mean_q: 0.585333, mean_eps: 0.000000
done, took 70.869 seconds
DQN Evaluation: 7129 victories out of 8406 episodes
Training for 5000 steps ...
   26/5000: episode: 1, duration: 0.202s, episode steps:  26, steps per second: 129, episode reward: 46.936, mean reward:  1.805 [-0.207, 32.170], mean action: 2.769 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   51/5000: episode: 2, duration: 0.332s, episode steps:  25, steps per second:  75, episode reward: 40.620, mean reward:  1.625 [-2.385, 32.391], mean action: 5.920 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   70/5000: episode: 3, duration: 0.325s, episode steps:  19, steps per second:  59, episode reward: 40.595, mean reward:  2.137 [-2.663, 32.020], mean action: 5.105 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  107/5000: episode: 4, duration: 0.262s, episode steps:  37, steps per second: 141, episode reward: 37.883, mean reward:  1.024 [-2.330, 31.931], mean action: 6.297 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  135/5000: episode: 5, duration: 0.226s, episode steps:  28, steps per second: 124, episode reward: 40.632, mean reward:  1.451 [-3.000, 31.639], mean action: 3.929 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  164/5000: episode: 6, duration: 0.198s, episode steps:  29, steps per second: 147, episode reward: 42.000, mean reward:  1.448 [-2.242, 32.200], mean action: 2.103 [1.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  191/5000: episode: 7, duration: 0.188s, episode steps:  27, steps per second: 143, episode reward: 38.377, mean reward:  1.421 [-2.655, 32.086], mean action: 3.148 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  209/5000: episode: 8, duration: 0.155s, episode steps:  18, steps per second: 116, episode reward: 43.948, mean reward:  2.442 [-2.381, 32.030], mean action: 4.444 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  230/5000: episode: 9, duration: 0.150s, episode steps:  21, steps per second: 140, episode reward: 44.489, mean reward:  2.119 [-2.110, 32.040], mean action: 2.714 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  259/5000: episode: 10, duration: 0.188s, episode steps:  29, steps per second: 154, episode reward: 39.000, mean reward:  1.345 [-2.506, 32.570], mean action: 3.931 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  297/5000: episode: 11, duration: 0.264s, episode steps:  38, steps per second: 144, episode reward: 38.264, mean reward:  1.007 [-2.660, 32.130], mean action: 3.237 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  319/5000: episode: 12, duration: 0.155s, episode steps:  22, steps per second: 141, episode reward: 47.366, mean reward:  2.153 [-0.750, 32.350], mean action: 2.409 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  343/5000: episode: 13, duration: 0.169s, episode steps:  24, steps per second: 142, episode reward: 46.992, mean reward:  1.958 [-0.407, 32.270], mean action: 4.042 [2.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  366/5000: episode: 14, duration: 0.172s, episode steps:  23, steps per second: 134, episode reward: 38.238, mean reward:  1.663 [-2.182, 31.892], mean action: 3.391 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  392/5000: episode: 15, duration: 0.160s, episode steps:  26, steps per second: 162, episode reward: 38.240, mean reward:  1.471 [-2.803, 32.230], mean action: 4.577 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  415/5000: episode: 16, duration: 0.156s, episode steps:  23, steps per second: 147, episode reward: 41.527, mean reward:  1.806 [-3.000, 32.310], mean action: 3.913 [2.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  438/5000: episode: 17, duration: 0.153s, episode steps:  23, steps per second: 151, episode reward: 38.655, mean reward:  1.681 [-2.603, 31.715], mean action: 4.348 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  468/5000: episode: 18, duration: 0.198s, episode steps:  30, steps per second: 152, episode reward: 38.293, mean reward:  1.276 [-3.000, 32.147], mean action: 5.067 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  487/5000: episode: 19, duration: 0.131s, episode steps:  19, steps per second: 145, episode reward: 41.944, mean reward:  2.208 [-2.807, 30.063], mean action: 4.789 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  515/5000: episode: 20, duration: 0.191s, episode steps:  28, steps per second: 146, episode reward: -34.640, mean reward: -1.237 [-32.210,  3.000], mean action: 5.821 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  533/5000: episode: 21, duration: 0.127s, episode steps:  18, steps per second: 142, episode reward: 44.140, mean reward:  2.452 [-2.939, 31.869], mean action: 2.944 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  559/5000: episode: 22, duration: 0.175s, episode steps:  26, steps per second: 149, episode reward: 38.381, mean reward:  1.476 [-2.634, 31.801], mean action: 2.962 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  592/5000: episode: 23, duration: 0.225s, episode steps:  33, steps per second: 147, episode reward: 35.735, mean reward:  1.083 [-3.000, 31.985], mean action: 5.061 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  625/5000: episode: 24, duration: 0.210s, episode steps:  33, steps per second: 157, episode reward: 41.267, mean reward:  1.251 [-2.638, 32.256], mean action: 3.424 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  644/5000: episode: 25, duration: 0.132s, episode steps:  19, steps per second: 144, episode reward: 44.210, mean reward:  2.327 [-2.074, 32.213], mean action: 2.842 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  667/5000: episode: 26, duration: 0.161s, episode steps:  23, steps per second: 143, episode reward: 38.902, mean reward:  1.691 [-2.488, 32.112], mean action: 3.913 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  692/5000: episode: 27, duration: 0.168s, episode steps:  25, steps per second: 149, episode reward: 38.605, mean reward:  1.544 [-3.000, 31.991], mean action: 4.720 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  728/5000: episode: 28, duration: 0.238s, episode steps:  36, steps per second: 151, episode reward: 41.340, mean reward:  1.148 [-2.569, 31.880], mean action: 4.472 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  764/5000: episode: 29, duration: 0.232s, episode steps:  36, steps per second: 155, episode reward: 32.450, mean reward:  0.901 [-3.000, 32.160], mean action: 4.167 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  822/5000: episode: 30, duration: 0.359s, episode steps:  58, steps per second: 162, episode reward: 46.686, mean reward:  0.805 [-0.557, 32.170], mean action: 3.500 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  839/5000: episode: 31, duration: 0.140s, episode steps:  17, steps per second: 121, episode reward: 44.807, mean reward:  2.636 [-2.004, 32.383], mean action: 3.706 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  856/5000: episode: 32, duration: 0.139s, episode steps:  17, steps per second: 123, episode reward: 41.477, mean reward:  2.440 [-2.647, 31.947], mean action: 4.647 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  885/5000: episode: 33, duration: 0.188s, episode steps:  29, steps per second: 154, episode reward: 35.478, mean reward:  1.223 [-2.248, 32.267], mean action: 3.621 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  912/5000: episode: 34, duration: 0.229s, episode steps:  27, steps per second: 118, episode reward: 38.882, mean reward:  1.440 [-3.000, 32.170], mean action: 5.444 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  933/5000: episode: 35, duration: 0.145s, episode steps:  21, steps per second: 145, episode reward: 41.333, mean reward:  1.968 [-2.494, 33.000], mean action: 4.048 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  990/5000: episode: 36, duration: 0.479s, episode steps:  57, steps per second: 119, episode reward: 38.536, mean reward:  0.676 [-2.214, 32.293], mean action: 3.579 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1015/5000: episode: 37, duration: 0.315s, episode steps:  25, steps per second:  79, episode reward: 44.500, mean reward:  1.780 [-2.047, 32.240], mean action: 3.000 [0.000, 16.000],  loss: 0.018866, mae: 0.345198, mean_q: 0.585546, mean_eps: 0.000000
 1036/5000: episode: 38, duration: 0.311s, episode steps:  21, steps per second:  68, episode reward: 41.917, mean reward:  1.996 [-2.108, 32.227], mean action: 2.810 [0.000, 16.000],  loss: 0.018058, mae: 0.348040, mean_q: 0.570452, mean_eps: 0.000000
 1058/5000: episode: 39, duration: 0.328s, episode steps:  22, steps per second:  67, episode reward: 38.701, mean reward:  1.759 [-2.178, 32.190], mean action: 5.182 [0.000, 20.000],  loss: 0.021906, mae: 0.366678, mean_q: 0.547379, mean_eps: 0.000000
 1091/5000: episode: 40, duration: 0.463s, episode steps:  33, steps per second:  71, episode reward: 41.709, mean reward:  1.264 [-2.166, 32.100], mean action: 2.606 [0.000, 20.000],  loss: 0.022235, mae: 0.368455, mean_q: 0.514815, mean_eps: 0.000000
 1129/5000: episode: 41, duration: 0.543s, episode steps:  38, steps per second:  70, episode reward: 38.847, mean reward:  1.022 [-2.418, 32.330], mean action: 5.868 [0.000, 20.000],  loss: 0.019788, mae: 0.352090, mean_q: 0.558567, mean_eps: 0.000000
 1211/5000: episode: 42, duration: 1.171s, episode steps:  82, steps per second:  70, episode reward: 40.937, mean reward:  0.499 [-3.000, 32.320], mean action: 3.024 [0.000, 16.000],  loss: 0.019531, mae: 0.355525, mean_q: 0.537528, mean_eps: 0.000000
 1231/5000: episode: 43, duration: 0.310s, episode steps:  20, steps per second:  64, episode reward: 44.487, mean reward:  2.224 [-2.620, 32.903], mean action: 3.850 [0.000, 13.000],  loss: 0.020273, mae: 0.356933, mean_q: 0.499285, mean_eps: 0.000000
 1261/5000: episode: 44, duration: 0.544s, episode steps:  30, steps per second:  55, episode reward: 41.686, mean reward:  1.390 [-2.756, 32.160], mean action: 2.767 [0.000, 12.000],  loss: 0.025270, mae: 0.383055, mean_q: 0.496998, mean_eps: 0.000000
 1276/5000: episode: 45, duration: 0.225s, episode steps:  15, steps per second:  67, episode reward: 46.790, mean reward:  3.119 [-0.211, 32.136], mean action: 3.867 [0.000, 15.000],  loss: 0.019219, mae: 0.356848, mean_q: 0.419400, mean_eps: 0.000000
 1330/5000: episode: 46, duration: 1.108s, episode steps:  54, steps per second:  49, episode reward: 40.674, mean reward:  0.753 [-3.000, 32.010], mean action: 3.722 [0.000, 16.000],  loss: 0.019339, mae: 0.345718, mean_q: 0.485665, mean_eps: 0.000000
 1372/5000: episode: 47, duration: 0.677s, episode steps:  42, steps per second:  62, episode reward: 37.868, mean reward:  0.902 [-3.000, 31.476], mean action: 4.452 [0.000, 20.000],  loss: 0.020684, mae: 0.344945, mean_q: 0.498389, mean_eps: 0.000000
 1398/5000: episode: 48, duration: 0.422s, episode steps:  26, steps per second:  62, episode reward: 38.905, mean reward:  1.496 [-2.518, 32.145], mean action: 4.231 [0.000, 19.000],  loss: 0.020957, mae: 0.351259, mean_q: 0.499253, mean_eps: 0.000000
 1425/5000: episode: 49, duration: 0.436s, episode steps:  27, steps per second:  62, episode reward: 38.795, mean reward:  1.437 [-2.306, 30.013], mean action: 3.222 [0.000, 16.000],  loss: 0.018873, mae: 0.341665, mean_q: 0.562429, mean_eps: 0.000000
 1452/5000: episode: 50, duration: 0.478s, episode steps:  27, steps per second:  56, episode reward: 35.386, mean reward:  1.311 [-3.000, 32.120], mean action: 3.630 [0.000, 19.000],  loss: 0.019363, mae: 0.345475, mean_q: 0.575571, mean_eps: 0.000000
 1475/5000: episode: 51, duration: 0.343s, episode steps:  23, steps per second:  67, episode reward: 38.903, mean reward:  1.691 [-3.000, 32.473], mean action: 3.348 [0.000, 19.000],  loss: 0.021607, mae: 0.360000, mean_q: 0.532180, mean_eps: 0.000000
 1501/5000: episode: 52, duration: 0.430s, episode steps:  26, steps per second:  60, episode reward: 35.545, mean reward:  1.367 [-2.903, 32.070], mean action: 7.192 [0.000, 20.000],  loss: 0.017734, mae: 0.342640, mean_q: 0.522061, mean_eps: 0.000000
 1525/5000: episode: 53, duration: 0.391s, episode steps:  24, steps per second:  61, episode reward: 44.547, mean reward:  1.856 [-2.181, 32.570], mean action: 3.208 [1.000, 16.000],  loss: 0.020595, mae: 0.353871, mean_q: 0.502951, mean_eps: 0.000000
 1554/5000: episode: 54, duration: 0.623s, episode steps:  29, steps per second:  47, episode reward: 36.000, mean reward:  1.241 [-2.628, 32.300], mean action: 4.276 [0.000, 16.000],  loss: 0.024238, mae: 0.367035, mean_q: 0.511120, mean_eps: 0.000000
 1584/5000: episode: 55, duration: 0.488s, episode steps:  30, steps per second:  62, episode reward: 41.307, mean reward:  1.377 [-2.266, 32.123], mean action: 4.433 [1.000, 15.000],  loss: 0.019592, mae: 0.350159, mean_q: 0.568746, mean_eps: 0.000000
 1609/5000: episode: 56, duration: 0.391s, episode steps:  25, steps per second:  64, episode reward: 38.346, mean reward:  1.534 [-3.000, 31.854], mean action: 7.600 [0.000, 20.000],  loss: 0.019832, mae: 0.348055, mean_q: 0.560913, mean_eps: 0.000000
 1621/5000: episode: 57, duration: 0.207s, episode steps:  12, steps per second:  58, episode reward: 44.111, mean reward:  3.676 [-2.238, 33.000], mean action: 2.750 [0.000, 19.000],  loss: 0.025133, mae: 0.367406, mean_q: 0.582257, mean_eps: 0.000000
 1637/5000: episode: 58, duration: 0.237s, episode steps:  16, steps per second:  67, episode reward: 42.000, mean reward:  2.625 [-2.724, 30.925], mean action: 2.000 [0.000, 19.000],  loss: 0.020022, mae: 0.342776, mean_q: 0.585850, mean_eps: 0.000000
 1660/5000: episode: 59, duration: 0.384s, episode steps:  23, steps per second:  60, episode reward: 41.446, mean reward:  1.802 [-3.000, 32.240], mean action: 3.000 [0.000, 19.000],  loss: 0.021555, mae: 0.355593, mean_q: 0.547013, mean_eps: 0.000000
 1688/5000: episode: 60, duration: 0.431s, episode steps:  28, steps per second:  65, episode reward: 38.412, mean reward:  1.372 [-3.000, 31.950], mean action: 3.500 [0.000, 19.000],  loss: 0.020893, mae: 0.356120, mean_q: 0.514182, mean_eps: 0.000000
 1725/5000: episode: 61, duration: 0.555s, episode steps:  37, steps per second:  67, episode reward: 38.631, mean reward:  1.044 [-2.346, 31.935], mean action: 4.324 [0.000, 21.000],  loss: 0.021538, mae: 0.350931, mean_q: 0.526516, mean_eps: 0.000000
 1753/5000: episode: 62, duration: 0.432s, episode steps:  28, steps per second:  65, episode reward: 41.352, mean reward:  1.477 [-2.074, 30.030], mean action: 3.214 [0.000, 19.000],  loss: 0.017742, mae: 0.337701, mean_q: 0.503319, mean_eps: 0.000000
 1792/5000: episode: 63, duration: 0.573s, episode steps:  39, steps per second:  68, episode reward: 38.149, mean reward:  0.978 [-2.195, 32.040], mean action: 3.667 [0.000, 19.000],  loss: 0.021256, mae: 0.363368, mean_q: 0.560220, mean_eps: 0.000000
 1811/5000: episode: 64, duration: 0.290s, episode steps:  19, steps per second:  65, episode reward: 44.331, mean reward:  2.333 [-2.047, 32.010], mean action: 2.263 [0.000, 15.000],  loss: 0.017201, mae: 0.352696, mean_q: 0.487579, mean_eps: 0.000000
 1834/5000: episode: 65, duration: 0.361s, episode steps:  23, steps per second:  64, episode reward: 44.142, mean reward:  1.919 [-2.064, 32.380], mean action: 2.609 [0.000, 19.000],  loss: 0.022671, mae: 0.373298, mean_q: 0.538356, mean_eps: 0.000000
 1880/5000: episode: 66, duration: 0.865s, episode steps:  46, steps per second:  53, episode reward: 39.000, mean reward:  0.848 [-2.295, 32.030], mean action: 1.609 [0.000, 15.000],  loss: 0.018767, mae: 0.345968, mean_q: 0.540183, mean_eps: 0.000000
 1903/5000: episode: 67, duration: 0.342s, episode steps:  23, steps per second:  67, episode reward: 41.882, mean reward:  1.821 [-3.000, 32.053], mean action: 6.783 [0.000, 20.000],  loss: 0.021468, mae: 0.368464, mean_q: 0.541935, mean_eps: 0.000000
 1921/5000: episode: 68, duration: 0.286s, episode steps:  18, steps per second:  63, episode reward: 41.458, mean reward:  2.303 [-2.251, 32.490], mean action: 5.278 [0.000, 15.000],  loss: 0.019915, mae: 0.355828, mean_q: 0.551408, mean_eps: 0.000000
 1949/5000: episode: 69, duration: 0.440s, episode steps:  28, steps per second:  64, episode reward: 41.336, mean reward:  1.476 [-2.310, 32.170], mean action: 3.429 [0.000, 15.000],  loss: 0.015491, mae: 0.336717, mean_q: 0.563341, mean_eps: 0.000000
 1976/5000: episode: 70, duration: 0.398s, episode steps:  27, steps per second:  68, episode reward: 47.061, mean reward:  1.743 [-0.307, 32.056], mean action: 4.704 [0.000, 21.000],  loss: 0.024825, mae: 0.384817, mean_q: 0.480376, mean_eps: 0.000000
 2010/5000: episode: 71, duration: 0.504s, episode steps:  34, steps per second:  67, episode reward: 38.622, mean reward:  1.136 [-2.903, 31.900], mean action: 2.853 [0.000, 20.000],  loss: 0.018117, mae: 0.347759, mean_q: 0.530901, mean_eps: 0.000000
 2038/5000: episode: 72, duration: 0.427s, episode steps:  28, steps per second:  66, episode reward: 40.147, mean reward:  1.434 [-2.315, 32.286], mean action: 4.643 [0.000, 20.000],  loss: 0.016960, mae: 0.346002, mean_q: 0.464217, mean_eps: 0.000000
 2064/5000: episode: 73, duration: 0.403s, episode steps:  26, steps per second:  65, episode reward: 41.861, mean reward:  1.610 [-2.267, 32.100], mean action: 5.000 [0.000, 15.000],  loss: 0.020209, mae: 0.360375, mean_q: 0.467936, mean_eps: 0.000000
 2087/5000: episode: 74, duration: 0.436s, episode steps:  23, steps per second:  53, episode reward: 38.578, mean reward:  1.677 [-2.598, 32.190], mean action: 3.870 [0.000, 15.000],  loss: 0.019144, mae: 0.346702, mean_q: 0.556768, mean_eps: 0.000000
 2106/5000: episode: 75, duration: 0.281s, episode steps:  19, steps per second:  67, episode reward: 44.727, mean reward:  2.354 [-2.447, 32.089], mean action: 3.842 [0.000, 15.000],  loss: 0.017829, mae: 0.344084, mean_q: 0.517195, mean_eps: 0.000000
 2121/5000: episode: 76, duration: 0.228s, episode steps:  15, steps per second:  66, episode reward: 44.010, mean reward:  2.934 [-2.261, 32.320], mean action: 1.067 [0.000, 2.000],  loss: 0.023277, mae: 0.368706, mean_q: 0.529653, mean_eps: 0.000000
 2145/5000: episode: 77, duration: 0.351s, episode steps:  24, steps per second:  68, episode reward: 42.000, mean reward:  1.750 [-2.319, 32.250], mean action: 3.875 [0.000, 16.000],  loss: 0.019342, mae: 0.347833, mean_q: 0.538986, mean_eps: 0.000000
 2161/5000: episode: 78, duration: 0.237s, episode steps:  16, steps per second:  67, episode reward: 40.552, mean reward:  2.535 [-3.000, 32.410], mean action: 2.562 [0.000, 16.000],  loss: 0.025884, mae: 0.375690, mean_q: 0.542582, mean_eps: 0.000000
 2182/5000: episode: 79, duration: 0.307s, episode steps:  21, steps per second:  68, episode reward: 41.441, mean reward:  1.973 [-2.490, 32.250], mean action: 4.619 [0.000, 16.000],  loss: 0.017532, mae: 0.340218, mean_q: 0.564080, mean_eps: 0.000000
 2206/5000: episode: 80, duration: 0.348s, episode steps:  24, steps per second:  69, episode reward: 41.589, mean reward:  1.733 [-2.193, 31.973], mean action: 1.458 [0.000, 16.000],  loss: 0.019666, mae: 0.349487, mean_q: 0.622674, mean_eps: 0.000000
 2222/5000: episode: 81, duration: 0.235s, episode steps:  16, steps per second:  68, episode reward: 41.798, mean reward:  2.612 [-2.487, 32.090], mean action: 4.812 [0.000, 15.000],  loss: 0.021793, mae: 0.355867, mean_q: 0.590870, mean_eps: 0.000000
 2250/5000: episode: 82, duration: 0.394s, episode steps:  28, steps per second:  71, episode reward: 41.344, mean reward:  1.477 [-3.000, 32.123], mean action: 4.250 [0.000, 20.000],  loss: 0.018429, mae: 0.346839, mean_q: 0.556434, mean_eps: 0.000000
 2276/5000: episode: 83, duration: 0.372s, episode steps:  26, steps per second:  70, episode reward: 41.507, mean reward:  1.596 [-2.561, 32.320], mean action: 3.308 [0.000, 19.000],  loss: 0.021181, mae: 0.359059, mean_q: 0.490545, mean_eps: 0.000000
 2314/5000: episode: 84, duration: 0.680s, episode steps:  38, steps per second:  56, episode reward: 34.581, mean reward:  0.910 [-3.000, 32.090], mean action: 5.158 [0.000, 20.000],  loss: 0.023867, mae: 0.373627, mean_q: 0.478304, mean_eps: 0.000000
 2337/5000: episode: 85, duration: 0.374s, episode steps:  23, steps per second:  62, episode reward: 42.000, mean reward:  1.826 [-2.239, 33.000], mean action: 4.130 [0.000, 20.000],  loss: 0.019064, mae: 0.350080, mean_q: 0.544809, mean_eps: 0.000000
 2362/5000: episode: 86, duration: 0.385s, episode steps:  25, steps per second:  65, episode reward: 41.729, mean reward:  1.669 [-2.863, 32.029], mean action: 1.920 [0.000, 9.000],  loss: 0.020239, mae: 0.349243, mean_q: 0.571368, mean_eps: 0.000000
 2398/5000: episode: 87, duration: 0.574s, episode steps:  36, steps per second:  63, episode reward: 41.404, mean reward:  1.150 [-2.128, 32.120], mean action: 2.917 [0.000, 11.000],  loss: 0.021752, mae: 0.363127, mean_q: 0.599618, mean_eps: 0.000000
 2427/5000: episode: 88, duration: 0.509s, episode steps:  29, steps per second:  57, episode reward: 35.412, mean reward:  1.221 [-3.000, 31.752], mean action: 6.000 [0.000, 19.000],  loss: 0.022028, mae: 0.358425, mean_q: 0.536947, mean_eps: 0.000000
 2455/5000: episode: 89, duration: 0.405s, episode steps:  28, steps per second:  69, episode reward: 40.966, mean reward:  1.463 [-2.938, 32.270], mean action: 3.750 [0.000, 19.000],  loss: 0.022233, mae: 0.360432, mean_q: 0.547008, mean_eps: 0.000000
 2477/5000: episode: 90, duration: 0.330s, episode steps:  22, steps per second:  67, episode reward: 39.000, mean reward:  1.773 [-2.276, 30.129], mean action: 3.727 [0.000, 19.000],  loss: 0.019751, mae: 0.346973, mean_q: 0.513720, mean_eps: 0.000000
 2492/5000: episode: 91, duration: 0.221s, episode steps:  15, steps per second:  68, episode reward: 44.523, mean reward:  2.968 [-2.076, 31.943], mean action: 1.800 [0.000, 12.000],  loss: 0.017722, mae: 0.332052, mean_q: 0.526948, mean_eps: 0.000000
 2515/5000: episode: 92, duration: 0.329s, episode steps:  23, steps per second:  70, episode reward: 37.669, mean reward:  1.638 [-2.407, 31.243], mean action: 4.652 [0.000, 15.000],  loss: 0.021056, mae: 0.354481, mean_q: 0.517843, mean_eps: 0.000000
 2553/5000: episode: 93, duration: 0.758s, episode steps:  38, steps per second:  50, episode reward: 43.351, mean reward:  1.141 [-2.266, 32.040], mean action: 5.105 [0.000, 13.000],  loss: 0.024964, mae: 0.376159, mean_q: 0.496256, mean_eps: 0.000000
 2581/5000: episode: 94, duration: 0.415s, episode steps:  28, steps per second:  67, episode reward: 35.415, mean reward:  1.265 [-3.000, 32.570], mean action: 6.500 [0.000, 19.000],  loss: 0.019067, mae: 0.350287, mean_q: 0.531665, mean_eps: 0.000000
 2616/5000: episode: 95, duration: 0.489s, episode steps:  35, steps per second:  72, episode reward: 40.824, mean reward:  1.166 [-3.000, 32.200], mean action: 4.314 [0.000, 19.000],  loss: 0.019545, mae: 0.353514, mean_q: 0.557061, mean_eps: 0.000000
 2644/5000: episode: 96, duration: 0.500s, episode steps:  28, steps per second:  56, episode reward: 36.000, mean reward:  1.286 [-3.000, 32.220], mean action: 3.679 [1.000, 16.000],  loss: 0.018047, mae: 0.341496, mean_q: 0.487642, mean_eps: 0.000000
 2671/5000: episode: 97, duration: 0.379s, episode steps:  27, steps per second:  71, episode reward: 39.000, mean reward:  1.444 [-2.810, 32.300], mean action: 3.037 [0.000, 12.000],  loss: 0.018858, mae: 0.348148, mean_q: 0.505133, mean_eps: 0.000000
 2699/5000: episode: 98, duration: 0.429s, episode steps:  28, steps per second:  65, episode reward: 38.784, mean reward:  1.385 [-3.000, 32.080], mean action: 6.143 [0.000, 16.000],  loss: 0.020332, mae: 0.347891, mean_q: 0.524919, mean_eps: 0.000000
 2729/5000: episode: 99, duration: 0.445s, episode steps:  30, steps per second:  67, episode reward: 37.933, mean reward:  1.264 [-2.557, 31.635], mean action: 5.400 [0.000, 20.000],  loss: 0.021519, mae: 0.355241, mean_q: 0.495598, mean_eps: 0.000000
 2759/5000: episode: 100, duration: 0.503s, episode steps:  30, steps per second:  60, episode reward: 36.000, mean reward:  1.200 [-3.000, 32.420], mean action: 3.533 [0.000, 16.000],  loss: 0.016776, mae: 0.325043, mean_q: 0.535822, mean_eps: 0.000000
 2783/5000: episode: 101, duration: 0.411s, episode steps:  24, steps per second:  58, episode reward: 38.815, mean reward:  1.617 [-3.000, 32.180], mean action: 3.333 [0.000, 16.000],  loss: 0.018091, mae: 0.335555, mean_q: 0.647615, mean_eps: 0.000000
 2808/5000: episode: 102, duration: 0.415s, episode steps:  25, steps per second:  60, episode reward: 38.702, mean reward:  1.548 [-2.347, 32.037], mean action: 2.960 [0.000, 11.000],  loss: 0.018975, mae: 0.338772, mean_q: 0.599693, mean_eps: 0.000000
 2837/5000: episode: 103, duration: 0.543s, episode steps:  29, steps per second:  53, episode reward: 35.424, mean reward:  1.222 [-3.000, 32.313], mean action: 6.379 [1.000, 20.000],  loss: 0.023643, mae: 0.359109, mean_q: 0.562985, mean_eps: 0.000000
 2866/5000: episode: 104, duration: 0.496s, episode steps:  29, steps per second:  58, episode reward: 44.520, mean reward:  1.535 [-2.221, 32.195], mean action: 2.690 [0.000, 20.000],  loss: 0.018931, mae: 0.335532, mean_q: 0.554320, mean_eps: 0.000000
 2898/5000: episode: 105, duration: 0.446s, episode steps:  32, steps per second:  72, episode reward: 35.235, mean reward:  1.101 [-2.573, 32.060], mean action: 3.438 [0.000, 12.000],  loss: 0.017326, mae: 0.333992, mean_q: 0.511277, mean_eps: 0.000000
 2926/5000: episode: 106, duration: 0.480s, episode steps:  28, steps per second:  58, episode reward: 41.333, mean reward:  1.476 [-2.127, 31.423], mean action: 2.000 [0.000, 16.000],  loss: 0.021054, mae: 0.359388, mean_q: 0.494524, mean_eps: 0.000000
 2946/5000: episode: 107, duration: 0.337s, episode steps:  20, steps per second:  59, episode reward: 47.013, mean reward:  2.351 [-0.266, 32.480], mean action: 2.050 [1.000, 3.000],  loss: 0.022944, mae: 0.357020, mean_q: 0.535401, mean_eps: 0.000000
 2963/5000: episode: 108, duration: 0.270s, episode steps:  17, steps per second:  63, episode reward: 42.000, mean reward:  2.471 [-2.266, 32.250], mean action: 2.529 [0.000, 20.000],  loss: 0.019609, mae: 0.350005, mean_q: 0.500249, mean_eps: 0.000000
 2993/5000: episode: 109, duration: 0.422s, episode steps:  30, steps per second:  71, episode reward: 40.739, mean reward:  1.358 [-2.402, 32.060], mean action: 4.067 [0.000, 20.000],  loss: 0.019208, mae: 0.351058, mean_q: 0.487017, mean_eps: 0.000000
 3009/5000: episode: 110, duration: 0.237s, episode steps:  16, steps per second:  68, episode reward: 44.089, mean reward:  2.756 [-2.663, 32.895], mean action: 3.250 [0.000, 11.000],  loss: 0.024600, mae: 0.374892, mean_q: 0.487836, mean_eps: 0.000000
 3040/5000: episode: 111, duration: 0.438s, episode steps:  31, steps per second:  71, episode reward: 41.164, mean reward:  1.328 [-2.317, 32.520], mean action: 4.613 [0.000, 21.000],  loss: 0.021215, mae: 0.358390, mean_q: 0.507783, mean_eps: 0.000000
 3074/5000: episode: 112, duration: 0.479s, episode steps:  34, steps per second:  71, episode reward: 35.401, mean reward:  1.041 [-3.000, 32.090], mean action: 5.471 [0.000, 19.000],  loss: 0.020851, mae: 0.351644, mean_q: 0.461004, mean_eps: 0.000000
 3091/5000: episode: 113, duration: 0.246s, episode steps:  17, steps per second:  69, episode reward: 38.807, mean reward:  2.283 [-2.461, 31.994], mean action: 3.765 [0.000, 19.000],  loss: 0.023352, mae: 0.357608, mean_q: 0.446076, mean_eps: 0.000000
 3109/5000: episode: 114, duration: 0.263s, episode steps:  18, steps per second:  68, episode reward: 44.052, mean reward:  2.447 [-2.070, 31.950], mean action: 3.056 [0.000, 19.000],  loss: 0.021318, mae: 0.356050, mean_q: 0.468701, mean_eps: 0.000000
 3137/5000: episode: 115, duration: 0.394s, episode steps:  28, steps per second:  71, episode reward: 38.896, mean reward:  1.389 [-2.238, 32.206], mean action: 4.179 [0.000, 19.000],  loss: 0.021631, mae: 0.361938, mean_q: 0.503814, mean_eps: 0.000000
 3155/5000: episode: 116, duration: 0.388s, episode steps:  18, steps per second:  46, episode reward: 41.504, mean reward:  2.306 [-3.000, 31.741], mean action: 3.000 [0.000, 16.000],  loss: 0.019159, mae: 0.350481, mean_q: 0.579812, mean_eps: 0.000000
 3191/5000: episode: 117, duration: 0.579s, episode steps:  36, steps per second:  62, episode reward: 38.721, mean reward:  1.076 [-2.574, 32.160], mean action: 3.556 [0.000, 20.000],  loss: 0.019449, mae: 0.346072, mean_q: 0.529201, mean_eps: 0.000000
 3219/5000: episode: 118, duration: 0.396s, episode steps:  28, steps per second:  71, episode reward: 38.473, mean reward:  1.374 [-2.754, 32.169], mean action: 5.321 [0.000, 20.000],  loss: 0.019479, mae: 0.346448, mean_q: 0.500975, mean_eps: 0.000000
 3243/5000: episode: 119, duration: 0.344s, episode steps:  24, steps per second:  70, episode reward: 42.000, mean reward:  1.750 [-2.110, 32.260], mean action: 2.333 [0.000, 19.000],  loss: 0.023136, mae: 0.366545, mean_q: 0.485845, mean_eps: 0.000000
 3263/5000: episode: 120, duration: 0.286s, episode steps:  20, steps per second:  70, episode reward: 39.000, mean reward:  1.950 [-2.283, 32.200], mean action: 4.700 [1.000, 19.000],  loss: 0.023045, mae: 0.360932, mean_q: 0.476861, mean_eps: 0.000000
 3305/5000: episode: 121, duration: 0.662s, episode steps:  42, steps per second:  63, episode reward: 38.284, mean reward:  0.912 [-2.620, 32.270], mean action: 2.905 [0.000, 16.000],  loss: 0.019347, mae: 0.342612, mean_q: 0.531624, mean_eps: 0.000000
 3329/5000: episode: 122, duration: 0.341s, episode steps:  24, steps per second:  70, episode reward: 41.597, mean reward:  1.733 [-3.000, 32.070], mean action: 3.000 [0.000, 19.000],  loss: 0.018936, mae: 0.342424, mean_q: 0.464650, mean_eps: 0.000000
 3365/5000: episode: 123, duration: 0.505s, episode steps:  36, steps per second:  71, episode reward: 38.563, mean reward:  1.071 [-2.423, 31.937], mean action: 4.000 [0.000, 20.000],  loss: 0.019136, mae: 0.346725, mean_q: 0.470435, mean_eps: 0.000000
 3384/5000: episode: 124, duration: 0.275s, episode steps:  19, steps per second:  69, episode reward: 45.000, mean reward:  2.368 [-2.054, 32.760], mean action: 1.684 [0.000, 9.000],  loss: 0.018758, mae: 0.340560, mean_q: 0.517666, mean_eps: 0.000000
 3415/5000: episode: 125, duration: 0.429s, episode steps:  31, steps per second:  72, episode reward: 32.415, mean reward:  1.046 [-2.704, 32.030], mean action: 5.355 [0.000, 19.000],  loss: 0.019670, mae: 0.342572, mean_q: 0.575355, mean_eps: 0.000000
 3434/5000: episode: 126, duration: 0.281s, episode steps:  19, steps per second:  68, episode reward: 44.591, mean reward:  2.347 [-2.120, 32.150], mean action: 3.842 [0.000, 19.000],  loss: 0.017276, mae: 0.327363, mean_q: 0.508380, mean_eps: 0.000000
 3463/5000: episode: 127, duration: 0.416s, episode steps:  29, steps per second:  70, episode reward: 42.000, mean reward:  1.448 [-2.218, 32.610], mean action: 1.931 [0.000, 12.000],  loss: 0.019729, mae: 0.344519, mean_q: 0.487634, mean_eps: 0.000000
 3482/5000: episode: 128, duration: 0.283s, episode steps:  19, steps per second:  67, episode reward: 47.122, mean reward:  2.480 [ 0.000, 31.972], mean action: 2.895 [1.000, 7.000],  loss: 0.026215, mae: 0.376410, mean_q: 0.496480, mean_eps: 0.000000
 3503/5000: episode: 129, duration: 0.299s, episode steps:  21, steps per second:  70, episode reward: 41.718, mean reward:  1.987 [-2.768, 31.808], mean action: 2.143 [0.000, 13.000],  loss: 0.021236, mae: 0.352981, mean_q: 0.510938, mean_eps: 0.000000
 3528/5000: episode: 130, duration: 0.360s, episode steps:  25, steps per second:  69, episode reward: 38.176, mean reward:  1.527 [-2.428, 32.064], mean action: 1.480 [0.000, 12.000],  loss: 0.019783, mae: 0.343526, mean_q: 0.518167, mean_eps: 0.000000
 3573/5000: episode: 131, duration: 0.624s, episode steps:  45, steps per second:  72, episode reward: 40.653, mean reward:  0.903 [-2.156, 32.350], mean action: 4.089 [0.000, 12.000],  loss: 0.025224, mae: 0.371065, mean_q: 0.544756, mean_eps: 0.000000
 3602/5000: episode: 132, duration: 0.441s, episode steps:  29, steps per second:  66, episode reward: 35.506, mean reward:  1.224 [-2.763, 32.410], mean action: 5.276 [0.000, 16.000],  loss: 0.020190, mae: 0.344846, mean_q: 0.596936, mean_eps: 0.000000
 3625/5000: episode: 133, duration: 0.334s, episode steps:  23, steps per second:  69, episode reward: 41.611, mean reward:  1.809 [-2.195, 31.953], mean action: 2.435 [0.000, 16.000],  loss: 0.020636, mae: 0.355032, mean_q: 0.591657, mean_eps: 0.000000
 3644/5000: episode: 134, duration: 0.275s, episode steps:  19, steps per second:  69, episode reward: 44.353, mean reward:  2.334 [-2.123, 32.350], mean action: 1.789 [0.000, 16.000],  loss: 0.019247, mae: 0.350264, mean_q: 0.574975, mean_eps: 0.000000
 3668/5000: episode: 135, duration: 0.345s, episode steps:  24, steps per second:  70, episode reward: 45.000, mean reward:  1.875 [-2.321, 32.190], mean action: 2.167 [0.000, 11.000],  loss: 0.018765, mae: 0.343664, mean_q: 0.533344, mean_eps: 0.000000
 3687/5000: episode: 136, duration: 0.303s, episode steps:  19, steps per second:  63, episode reward: 44.033, mean reward:  2.318 [-2.637, 32.020], mean action: 4.158 [0.000, 14.000],  loss: 0.021567, mae: 0.353497, mean_q: 0.560841, mean_eps: 0.000000
 3700/5000: episode: 137, duration: 0.279s, episode steps:  13, steps per second:  47, episode reward: 45.000, mean reward:  3.462 [-2.590, 32.090], mean action: 1.692 [0.000, 11.000],  loss: 0.026265, mae: 0.372535, mean_q: 0.479014, mean_eps: 0.000000
 3734/5000: episode: 138, duration: 0.679s, episode steps:  34, steps per second:  50, episode reward: 37.305, mean reward:  1.097 [-3.000, 31.732], mean action: 5.206 [0.000, 15.000],  loss: 0.020857, mae: 0.355408, mean_q: 0.529153, mean_eps: 0.000000
 3759/5000: episode: 139, duration: 0.452s, episode steps:  25, steps per second:  55, episode reward: 41.561, mean reward:  1.662 [-2.517, 32.500], mean action: 3.320 [0.000, 15.000],  loss: 0.017199, mae: 0.340387, mean_q: 0.467378, mean_eps: 0.000000
 3784/5000: episode: 140, duration: 0.809s, episode steps:  25, steps per second:  31, episode reward: 43.993, mean reward:  1.760 [-2.524, 32.113], mean action: 2.600 [0.000, 19.000],  loss: 0.021375, mae: 0.359116, mean_q: 0.482040, mean_eps: 0.000000
 3807/5000: episode: 141, duration: 0.503s, episode steps:  23, steps per second:  46, episode reward: 35.291, mean reward:  1.534 [-3.000, 32.083], mean action: 2.304 [0.000, 15.000],  loss: 0.022084, mae: 0.364805, mean_q: 0.558469, mean_eps: 0.000000
 3834/5000: episode: 142, duration: 0.506s, episode steps:  27, steps per second:  53, episode reward: 44.092, mean reward:  1.633 [-2.213, 32.860], mean action: 4.111 [0.000, 15.000],  loss: 0.014982, mae: 0.334195, mean_q: 0.547202, mean_eps: 0.000000
 3870/5000: episode: 143, duration: 0.502s, episode steps:  36, steps per second:  72, episode reward: 38.756, mean reward:  1.077 [-2.688, 32.210], mean action: 2.472 [0.000, 9.000],  loss: 0.018916, mae: 0.351829, mean_q: 0.544722, mean_eps: 0.000000
 3894/5000: episode: 144, duration: 0.478s, episode steps:  24, steps per second:  50, episode reward: 46.944, mean reward:  1.956 [-0.661, 32.120], mean action: 2.125 [0.000, 12.000],  loss: 0.021710, mae: 0.364854, mean_q: 0.578047, mean_eps: 0.000000
 3918/5000: episode: 145, duration: 0.384s, episode steps:  24, steps per second:  62, episode reward: 39.000, mean reward:  1.625 [-3.000, 32.820], mean action: 3.333 [1.000, 12.000],  loss: 0.021502, mae: 0.366343, mean_q: 0.550481, mean_eps: 0.000000
 3940/5000: episode: 146, duration: 0.394s, episode steps:  22, steps per second:  56, episode reward: 44.206, mean reward:  2.009 [-2.432, 31.981], mean action: 1.409 [0.000, 2.000],  loss: 0.020986, mae: 0.366539, mean_q: 0.548382, mean_eps: 0.000000
 3970/5000: episode: 147, duration: 0.479s, episode steps:  30, steps per second:  63, episode reward: 40.711, mean reward:  1.357 [-2.333, 32.060], mean action: 3.467 [0.000, 15.000],  loss: 0.019457, mae: 0.357000, mean_q: 0.549091, mean_eps: 0.000000
 4027/5000: episode: 148, duration: 0.950s, episode steps:  57, steps per second:  60, episode reward: 37.990, mean reward:  0.666 [-3.000, 32.820], mean action: 2.281 [0.000, 19.000],  loss: 0.020358, mae: 0.355823, mean_q: 0.540346, mean_eps: 0.000000
 4047/5000: episode: 149, duration: 0.369s, episode steps:  20, steps per second:  54, episode reward: 44.212, mean reward:  2.211 [-2.094, 33.000], mean action: 2.600 [1.000, 16.000],  loss: 0.018694, mae: 0.347579, mean_q: 0.560615, mean_eps: 0.000000
 4097/5000: episode: 150, duration: 0.834s, episode steps:  50, steps per second:  60, episode reward: 38.458, mean reward:  0.769 [-2.384, 32.200], mean action: 3.320 [1.000, 19.000],  loss: 0.018619, mae: 0.344256, mean_q: 0.562272, mean_eps: 0.000000
 4120/5000: episode: 151, duration: 0.402s, episode steps:  23, steps per second:  57, episode reward: 36.000, mean reward:  1.565 [-3.000, 32.510], mean action: 4.304 [0.000, 19.000],  loss: 0.021465, mae: 0.352723, mean_q: 0.573318, mean_eps: 0.000000
 4148/5000: episode: 152, duration: 0.481s, episode steps:  28, steps per second:  58, episode reward: 41.901, mean reward:  1.496 [-2.224, 32.040], mean action: 4.607 [0.000, 13.000],  loss: 0.020580, mae: 0.352193, mean_q: 0.600908, mean_eps: 0.000000
 4170/5000: episode: 153, duration: 0.400s, episode steps:  22, steps per second:  55, episode reward: 38.687, mean reward:  1.759 [-3.000, 32.270], mean action: 3.864 [0.000, 19.000],  loss: 0.021059, mae: 0.351201, mean_q: 0.545376, mean_eps: 0.000000
 4201/5000: episode: 154, duration: 0.507s, episode steps:  31, steps per second:  61, episode reward: 44.253, mean reward:  1.428 [-2.341, 32.177], mean action: 4.613 [0.000, 19.000],  loss: 0.020293, mae: 0.351026, mean_q: 0.507057, mean_eps: 0.000000
 4248/5000: episode: 155, duration: 0.802s, episode steps:  47, steps per second:  59, episode reward: 41.171, mean reward:  0.876 [-2.535, 32.133], mean action: 3.596 [0.000, 15.000],  loss: 0.022624, mae: 0.364052, mean_q: 0.553338, mean_eps: 0.000000
 4264/5000: episode: 156, duration: 0.643s, episode steps:  16, steps per second:  25, episode reward: 46.347, mean reward:  2.897 [-0.328, 32.220], mean action: 3.000 [0.000, 20.000],  loss: 0.021255, mae: 0.359353, mean_q: 0.555325, mean_eps: 0.000000
 4294/5000: episode: 157, duration: 0.433s, episode steps:  30, steps per second:  69, episode reward: 41.753, mean reward:  1.392 [-2.418, 32.140], mean action: 2.233 [0.000, 11.000],  loss: 0.023197, mae: 0.374132, mean_q: 0.540063, mean_eps: 0.000000
 4317/5000: episode: 158, duration: 0.325s, episode steps:  23, steps per second:  71, episode reward: 38.298, mean reward:  1.665 [-3.000, 32.130], mean action: 4.304 [0.000, 20.000],  loss: 0.018559, mae: 0.353307, mean_q: 0.514644, mean_eps: 0.000000
 4356/5000: episode: 159, duration: 0.637s, episode steps:  39, steps per second:  61, episode reward: -32.030, mean reward: -0.821 [-32.187,  2.400], mean action: 5.718 [0.000, 16.000],  loss: 0.020093, mae: 0.362403, mean_q: 0.466981, mean_eps: 0.000000
 4381/5000: episode: 160, duration: 0.423s, episode steps:  25, steps per second:  59, episode reward: 39.000, mean reward:  1.560 [-2.396, 32.260], mean action: 3.880 [0.000, 20.000],  loss: 0.022211, mae: 0.370319, mean_q: 0.492013, mean_eps: 0.000000
 4402/5000: episode: 161, duration: 0.386s, episode steps:  21, steps per second:  54, episode reward: 44.785, mean reward:  2.133 [-2.438, 32.040], mean action: 2.905 [0.000, 14.000],  loss: 0.024959, mae: 0.375952, mean_q: 0.511353, mean_eps: 0.000000
 4427/5000: episode: 162, duration: 0.431s, episode steps:  25, steps per second:  58, episode reward: 38.004, mean reward:  1.520 [-3.000, 32.200], mean action: 2.600 [0.000, 9.000],  loss: 0.019796, mae: 0.361413, mean_q: 0.511151, mean_eps: 0.000000
 4463/5000: episode: 163, duration: 1.162s, episode steps:  36, steps per second:  31, episode reward: 41.308, mean reward:  1.147 [-2.560, 32.190], mean action: 2.722 [0.000, 15.000],  loss: 0.022334, mae: 0.377657, mean_q: 0.516119, mean_eps: 0.000000
 4492/5000: episode: 164, duration: 0.501s, episode steps:  29, steps per second:  58, episode reward: 44.552, mean reward:  1.536 [-2.192, 31.642], mean action: 0.966 [0.000, 15.000],  loss: 0.022421, mae: 0.371550, mean_q: 0.504498, mean_eps: 0.000000
 4513/5000: episode: 165, duration: 0.339s, episode steps:  21, steps per second:  62, episode reward: 39.000, mean reward:  1.857 [-2.597, 32.750], mean action: 3.619 [0.000, 15.000],  loss: 0.020199, mae: 0.365388, mean_q: 0.533955, mean_eps: 0.000000
 4535/5000: episode: 166, duration: 0.331s, episode steps:  22, steps per second:  66, episode reward: 44.460, mean reward:  2.021 [-2.115, 31.794], mean action: 1.727 [0.000, 15.000],  loss: 0.019468, mae: 0.359320, mean_q: 0.586239, mean_eps: 0.000000
 4562/5000: episode: 167, duration: 0.445s, episode steps:  27, steps per second:  61, episode reward: 42.000, mean reward:  1.556 [-2.687, 32.020], mean action: 1.667 [0.000, 15.000],  loss: 0.019566, mae: 0.361163, mean_q: 0.552104, mean_eps: 0.000000
 4582/5000: episode: 168, duration: 0.312s, episode steps:  20, steps per second:  64, episode reward: 43.795, mean reward:  2.190 [-3.000, 31.893], mean action: 2.900 [0.000, 15.000],  loss: 0.022377, mae: 0.380810, mean_q: 0.526468, mean_eps: 0.000000
 4608/5000: episode: 169, duration: 0.379s, episode steps:  26, steps per second:  69, episode reward: 38.504, mean reward:  1.481 [-3.000, 32.120], mean action: 5.192 [0.000, 19.000],  loss: 0.022873, mae: 0.374167, mean_q: 0.557568, mean_eps: 0.000000
 4640/5000: episode: 170, duration: 0.455s, episode steps:  32, steps per second:  70, episode reward: 33.000, mean reward:  1.031 [-3.000, 32.270], mean action: 4.719 [0.000, 19.000],  loss: 0.020965, mae: 0.362329, mean_q: 0.600325, mean_eps: 0.000000
 4682/5000: episode: 171, duration: 0.588s, episode steps:  42, steps per second:  71, episode reward: 38.496, mean reward:  0.917 [-3.000, 32.280], mean action: 4.310 [0.000, 19.000],  loss: 0.018123, mae: 0.355573, mean_q: 0.574930, mean_eps: 0.000000
 4701/5000: episode: 172, duration: 0.281s, episode steps:  19, steps per second:  68, episode reward: 41.188, mean reward:  2.168 [-2.692, 31.888], mean action: 4.789 [1.000, 15.000],  loss: 0.020869, mae: 0.366340, mean_q: 0.551161, mean_eps: 0.000000
 4728/5000: episode: 173, duration: 0.384s, episode steps:  27, steps per second:  70, episode reward: 42.000, mean reward:  1.556 [-2.204, 32.440], mean action: 2.481 [1.000, 15.000],  loss: 0.019084, mae: 0.357882, mean_q: 0.572693, mean_eps: 0.000000
 4741/5000: episode: 174, duration: 0.201s, episode steps:  13, steps per second:  65, episode reward: 44.675, mean reward:  3.437 [-2.195, 32.413], mean action: 4.308 [0.000, 15.000],  loss: 0.020776, mae: 0.376855, mean_q: 0.485156, mean_eps: 0.000000
 4759/5000: episode: 175, duration: 0.263s, episode steps:  18, steps per second:  68, episode reward: 41.047, mean reward:  2.280 [-3.000, 32.026], mean action: 1.167 [0.000, 16.000],  loss: 0.019960, mae: 0.364204, mean_q: 0.493344, mean_eps: 0.000000
 4796/5000: episode: 176, duration: 0.514s, episode steps:  37, steps per second:  72, episode reward: -32.190, mean reward: -0.870 [-32.498,  2.633], mean action: 4.811 [0.000, 15.000],  loss: 0.017300, mae: 0.347008, mean_q: 0.522323, mean_eps: 0.000000
 4828/5000: episode: 177, duration: 0.462s, episode steps:  32, steps per second:  69, episode reward: 35.876, mean reward:  1.121 [-3.000, 32.550], mean action: 4.312 [1.000, 15.000],  loss: 0.020436, mae: 0.361209, mean_q: 0.512907, mean_eps: 0.000000
 4878/5000: episode: 178, duration: 0.680s, episode steps:  50, steps per second:  74, episode reward: 32.403, mean reward:  0.648 [-2.550, 32.470], mean action: 3.340 [0.000, 15.000],  loss: 0.019441, mae: 0.355904, mean_q: 0.506785, mean_eps: 0.000000
 4897/5000: episode: 179, duration: 0.286s, episode steps:  19, steps per second:  66, episode reward: 47.653, mean reward:  2.508 [ 0.000, 32.586], mean action: 1.474 [0.000, 2.000],  loss: 0.019037, mae: 0.343757, mean_q: 0.522226, mean_eps: 0.000000
 4928/5000: episode: 180, duration: 0.440s, episode steps:  31, steps per second:  70, episode reward: 42.000, mean reward:  1.355 [-2.397, 32.360], mean action: 1.226 [0.000, 11.000],  loss: 0.021295, mae: 0.358722, mean_q: 0.552637, mean_eps: 0.000000
 4959/5000: episode: 181, duration: 0.444s, episode steps:  31, steps per second:  70, episode reward: 40.344, mean reward:  1.301 [-2.059, 32.107], mean action: 4.839 [0.000, 14.000],  loss: 0.023913, mae: 0.376346, mean_q: 0.513092, mean_eps: 0.000000
 4983/5000: episode: 182, duration: 0.415s, episode steps:  24, steps per second:  58, episode reward: 35.672, mean reward:  1.486 [-2.813, 32.171], mean action: 5.958 [0.000, 19.000],  loss: 0.022544, mae: 0.365674, mean_q: 0.577667, mean_eps: 0.000000
done, took 72.135 seconds
DQN Evaluation: 7309 victories out of 8589 episodes
Training for 5000 steps ...
   28/5000: episode: 1, duration: 0.215s, episode steps:  28, steps per second: 130, episode reward: -36.000, mean reward: -1.286 [-32.027,  2.113], mean action: 4.607 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   52/5000: episode: 2, duration: 0.171s, episode steps:  24, steps per second: 140, episode reward: 35.280, mean reward:  1.470 [-3.000, 32.659], mean action: 4.958 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   68/5000: episode: 3, duration: 0.119s, episode steps:  16, steps per second: 135, episode reward: 38.097, mean reward:  2.381 [-3.000, 31.916], mean action: 4.250 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   92/5000: episode: 4, duration: 0.169s, episode steps:  24, steps per second: 142, episode reward: -35.410, mean reward: -1.475 [-32.050,  2.320], mean action: 6.167 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  109/5000: episode: 5, duration: 0.121s, episode steps:  17, steps per second: 140, episode reward: 41.274, mean reward:  2.428 [-2.252, 32.026], mean action: 3.941 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  140/5000: episode: 6, duration: 0.215s, episode steps:  31, steps per second: 144, episode reward: 32.452, mean reward:  1.047 [-2.681, 32.300], mean action: 5.452 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  163/5000: episode: 7, duration: 0.142s, episode steps:  23, steps per second: 161, episode reward: 34.971, mean reward:  1.520 [-2.602, 32.275], mean action: 5.652 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  184/5000: episode: 8, duration: 0.144s, episode steps:  21, steps per second: 145, episode reward: 37.804, mean reward:  1.800 [-2.522, 32.103], mean action: 4.810 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  212/5000: episode: 9, duration: 0.180s, episode steps:  28, steps per second: 156, episode reward: 32.541, mean reward:  1.162 [-2.345, 31.972], mean action: 6.679 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  234/5000: episode: 10, duration: 0.157s, episode steps:  22, steps per second: 141, episode reward: 41.313, mean reward:  1.878 [-2.164, 32.046], mean action: 6.955 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  257/5000: episode: 11, duration: 0.154s, episode steps:  23, steps per second: 150, episode reward: 35.504, mean reward:  1.544 [-2.184, 29.630], mean action: 3.696 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  286/5000: episode: 12, duration: 0.193s, episode steps:  29, steps per second: 150, episode reward: 38.292, mean reward:  1.320 [-2.127, 32.360], mean action: 6.862 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  309/5000: episode: 13, duration: 0.172s, episode steps:  23, steps per second: 134, episode reward: 38.613, mean reward:  1.679 [-3.000, 32.768], mean action: 3.391 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  328/5000: episode: 14, duration: 0.131s, episode steps:  19, steps per second: 145, episode reward: 39.000, mean reward:  2.053 [-2.914, 30.397], mean action: 2.632 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  345/5000: episode: 15, duration: 0.124s, episode steps:  17, steps per second: 137, episode reward: 38.646, mean reward:  2.273 [-2.344, 31.866], mean action: 3.588 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  370/5000: episode: 16, duration: 0.183s, episode steps:  25, steps per second: 137, episode reward: 32.317, mean reward:  1.293 [-3.000, 31.810], mean action: 6.720 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  388/5000: episode: 17, duration: 0.132s, episode steps:  18, steps per second: 137, episode reward: 40.493, mean reward:  2.250 [-2.251, 32.182], mean action: 5.056 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  416/5000: episode: 18, duration: 0.186s, episode steps:  28, steps per second: 150, episode reward: 35.878, mean reward:  1.281 [-2.648, 29.744], mean action: 3.179 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  437/5000: episode: 19, duration: 0.146s, episode steps:  21, steps per second: 144, episode reward: 41.192, mean reward:  1.962 [-2.088, 32.410], mean action: 3.381 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  462/5000: episode: 20, duration: 0.165s, episode steps:  25, steps per second: 151, episode reward: 37.372, mean reward:  1.495 [-2.352, 32.490], mean action: 5.640 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  493/5000: episode: 21, duration: 0.197s, episode steps:  31, steps per second: 157, episode reward: -33.000, mean reward: -1.065 [-32.305,  2.290], mean action: 6.161 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  529/5000: episode: 22, duration: 0.221s, episode steps:  36, steps per second: 163, episode reward: -32.570, mean reward: -0.905 [-32.269,  2.590], mean action: 9.861 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  546/5000: episode: 23, duration: 0.123s, episode steps:  17, steps per second: 138, episode reward: 38.020, mean reward:  2.236 [-2.177, 32.904], mean action: 4.118 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  565/5000: episode: 24, duration: 0.134s, episode steps:  19, steps per second: 142, episode reward: 36.000, mean reward:  1.895 [-2.671, 32.860], mean action: 4.737 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  589/5000: episode: 25, duration: 0.165s, episode steps:  24, steps per second: 145, episode reward: 35.372, mean reward:  1.474 [-2.274, 32.902], mean action: 5.458 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  596/5000: episode: 26, duration: 0.076s, episode steps:   7, steps per second:  92, episode reward: 47.015, mean reward:  6.716 [ 0.236, 32.536], mean action: 4.429 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  614/5000: episode: 27, duration: 0.125s, episode steps:  18, steps per second: 145, episode reward: 38.794, mean reward:  2.155 [-2.460, 32.084], mean action: 3.833 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  642/5000: episode: 28, duration: 0.188s, episode steps:  28, steps per second: 149, episode reward: 32.398, mean reward:  1.157 [-3.000, 32.101], mean action: 5.607 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  670/5000: episode: 29, duration: 0.189s, episode steps:  28, steps per second: 148, episode reward: 35.257, mean reward:  1.259 [-2.671, 32.068], mean action: 5.286 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  699/5000: episode: 30, duration: 0.179s, episode steps:  29, steps per second: 162, episode reward: 32.186, mean reward:  1.110 [-3.000, 32.040], mean action: 3.931 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  720/5000: episode: 31, duration: 0.149s, episode steps:  21, steps per second: 141, episode reward: 35.602, mean reward:  1.695 [-2.456, 33.164], mean action: 5.762 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  753/5000: episode: 32, duration: 0.204s, episode steps:  33, steps per second: 161, episode reward: 32.564, mean reward:  0.987 [-2.364, 32.330], mean action: 6.515 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  772/5000: episode: 33, duration: 0.130s, episode steps:  19, steps per second: 147, episode reward: 35.660, mean reward:  1.877 [-3.000, 32.904], mean action: 8.842 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  793/5000: episode: 34, duration: 0.162s, episode steps:  21, steps per second: 129, episode reward: 38.015, mean reward:  1.810 [-2.460, 31.963], mean action: 4.667 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  821/5000: episode: 35, duration: 0.189s, episode steps:  28, steps per second: 149, episode reward: 37.737, mean reward:  1.348 [-2.506, 32.170], mean action: 3.464 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  837/5000: episode: 36, duration: 0.112s, episode steps:  16, steps per second: 142, episode reward: 38.626, mean reward:  2.414 [-2.640, 32.951], mean action: 5.125 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  872/5000: episode: 37, duration: 0.222s, episode steps:  35, steps per second: 157, episode reward: 40.363, mean reward:  1.153 [-2.382, 32.100], mean action: 7.286 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  896/5000: episode: 38, duration: 0.163s, episode steps:  24, steps per second: 148, episode reward: 38.658, mean reward:  1.611 [-2.154, 33.000], mean action: 6.208 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  920/5000: episode: 39, duration: 0.153s, episode steps:  24, steps per second: 157, episode reward: -38.080, mean reward: -1.587 [-32.114,  2.140], mean action: 8.875 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  947/5000: episode: 40, duration: 0.174s, episode steps:  27, steps per second: 156, episode reward: 38.530, mean reward:  1.427 [-2.929, 33.000], mean action: 3.741 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  972/5000: episode: 41, duration: 0.168s, episode steps:  25, steps per second: 149, episode reward: 35.072, mean reward:  1.403 [-2.292, 32.381], mean action: 7.960 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  997/5000: episode: 42, duration: 0.159s, episode steps:  25, steps per second: 157, episode reward: 40.217, mean reward:  1.609 [-2.496, 32.310], mean action: 5.680 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1015/5000: episode: 43, duration: 0.249s, episode steps:  18, steps per second:  72, episode reward: 38.537, mean reward:  2.141 [-2.681, 32.340], mean action: 4.556 [0.000, 19.000],  loss: 0.024658, mae: 0.373651, mean_q: 0.560926, mean_eps: 0.000000
 1036/5000: episode: 44, duration: 0.289s, episode steps:  21, steps per second:  73, episode reward: -41.700, mean reward: -1.986 [-32.211,  2.200], mean action: 6.143 [0.000, 19.000],  loss: 0.021447, mae: 0.363572, mean_q: 0.576988, mean_eps: 0.000000
 1055/5000: episode: 45, duration: 0.280s, episode steps:  19, steps per second:  68, episode reward: 38.267, mean reward:  2.014 [-2.395, 32.190], mean action: 4.474 [0.000, 15.000],  loss: 0.020835, mae: 0.361186, mean_q: 0.576865, mean_eps: 0.000000
 1068/5000: episode: 46, duration: 0.194s, episode steps:  13, steps per second:  67, episode reward: 41.056, mean reward:  3.158 [-2.562, 31.828], mean action: 3.385 [0.000, 15.000],  loss: 0.021670, mae: 0.363924, mean_q: 0.596893, mean_eps: 0.000000
 1089/5000: episode: 47, duration: 0.306s, episode steps:  21, steps per second:  69, episode reward: -35.250, mean reward: -1.679 [-32.150,  2.480], mean action: 4.619 [0.000, 15.000],  loss: 0.025108, mae: 0.380338, mean_q: 0.534099, mean_eps: 0.000000
 1117/5000: episode: 48, duration: 0.414s, episode steps:  28, steps per second:  68, episode reward: -32.810, mean reward: -1.172 [-32.353,  3.060], mean action: 5.536 [0.000, 20.000],  loss: 0.022330, mae: 0.370469, mean_q: 0.568637, mean_eps: 0.000000
 1134/5000: episode: 49, duration: 0.256s, episode steps:  17, steps per second:  66, episode reward: 39.000, mean reward:  2.294 [-2.224, 29.826], mean action: 2.882 [0.000, 9.000],  loss: 0.019582, mae: 0.358418, mean_q: 0.611733, mean_eps: 0.000000
 1159/5000: episode: 50, duration: 0.358s, episode steps:  25, steps per second:  70, episode reward: -32.760, mean reward: -1.310 [-32.409,  2.349], mean action: 5.200 [0.000, 12.000],  loss: 0.019409, mae: 0.360586, mean_q: 0.566735, mean_eps: 0.000000
 1178/5000: episode: 51, duration: 0.271s, episode steps:  19, steps per second:  70, episode reward: 41.807, mean reward:  2.200 [-2.189, 32.903], mean action: 3.316 [1.000, 9.000],  loss: 0.021630, mae: 0.368968, mean_q: 0.585746, mean_eps: 0.000000
 1207/5000: episode: 52, duration: 0.551s, episode steps:  29, steps per second:  53, episode reward: 32.213, mean reward:  1.111 [-3.000, 33.000], mean action: 4.724 [0.000, 15.000],  loss: 0.015184, mae: 0.337960, mean_q: 0.519215, mean_eps: 0.000000
 1235/5000: episode: 53, duration: 0.538s, episode steps:  28, steps per second:  52, episode reward: 38.357, mean reward:  1.370 [-2.520, 32.300], mean action: 5.964 [0.000, 19.000],  loss: 0.019276, mae: 0.359511, mean_q: 0.539228, mean_eps: 0.000000
 1270/5000: episode: 54, duration: 0.987s, episode steps:  35, steps per second:  35, episode reward: 32.903, mean reward:  0.940 [-2.345, 32.563], mean action: 4.571 [0.000, 19.000],  loss: 0.020684, mae: 0.355716, mean_q: 0.505488, mean_eps: 0.000000
 1296/5000: episode: 55, duration: 0.400s, episode steps:  26, steps per second:  65, episode reward: -35.550, mean reward: -1.367 [-32.477,  2.380], mean action: 4.654 [0.000, 15.000],  loss: 0.018754, mae: 0.351372, mean_q: 0.528483, mean_eps: 0.000000
 1311/5000: episode: 56, duration: 0.253s, episode steps:  15, steps per second:  59, episode reward: 41.444, mean reward:  2.763 [-2.420, 32.901], mean action: 4.867 [0.000, 15.000],  loss: 0.027461, mae: 0.390011, mean_q: 0.538179, mean_eps: 0.000000
 1342/5000: episode: 57, duration: 0.727s, episode steps:  31, steps per second:  43, episode reward: 33.000, mean reward:  1.065 [-2.241, 32.250], mean action: 3.613 [0.000, 19.000],  loss: 0.022008, mae: 0.362154, mean_q: 0.564725, mean_eps: 0.000000
 1367/5000: episode: 58, duration: 0.663s, episode steps:  25, steps per second:  38, episode reward: 35.943, mean reward:  1.438 [-2.241, 32.410], mean action: 3.640 [0.000, 19.000],  loss: 0.015528, mae: 0.331074, mean_q: 0.513663, mean_eps: 0.000000
 1401/5000: episode: 59, duration: 0.490s, episode steps:  34, steps per second:  69, episode reward: -35.280, mean reward: -1.038 [-31.784,  2.460], mean action: 9.118 [0.000, 20.000],  loss: 0.019238, mae: 0.351982, mean_q: 0.549028, mean_eps: 0.000000
 1426/5000: episode: 60, duration: 0.362s, episode steps:  25, steps per second:  69, episode reward: 47.969, mean reward:  1.919 [-0.025, 32.140], mean action: 3.600 [0.000, 13.000],  loss: 0.024435, mae: 0.369708, mean_q: 0.501120, mean_eps: 0.000000
 1443/5000: episode: 61, duration: 0.254s, episode steps:  17, steps per second:  67, episode reward: 37.187, mean reward:  2.187 [-2.389, 31.988], mean action: 4.647 [0.000, 13.000],  loss: 0.019197, mae: 0.347132, mean_q: 0.531366, mean_eps: 0.000000
 1468/5000: episode: 62, duration: 0.352s, episode steps:  25, steps per second:  71, episode reward: 38.970, mean reward:  1.559 [-3.000, 33.508], mean action: 8.560 [0.000, 19.000],  loss: 0.019869, mae: 0.357709, mean_q: 0.579638, mean_eps: 0.000000
 1485/5000: episode: 63, duration: 0.247s, episode steps:  17, steps per second:  69, episode reward: 38.158, mean reward:  2.245 [-2.338, 31.338], mean action: 3.529 [0.000, 16.000],  loss: 0.021801, mae: 0.368796, mean_q: 0.484080, mean_eps: 0.000000
 1514/5000: episode: 64, duration: 0.410s, episode steps:  29, steps per second:  71, episode reward: 34.689, mean reward:  1.196 [-2.284, 33.000], mean action: 7.172 [0.000, 20.000],  loss: 0.021710, mae: 0.360957, mean_q: 0.506939, mean_eps: 0.000000
 1532/5000: episode: 65, duration: 0.255s, episode steps:  18, steps per second:  70, episode reward: -35.690, mean reward: -1.983 [-32.225,  2.405], mean action: 5.278 [0.000, 19.000],  loss: 0.020624, mae: 0.354451, mean_q: 0.516013, mean_eps: 0.000000
 1560/5000: episode: 66, duration: 0.465s, episode steps:  28, steps per second:  60, episode reward: 35.518, mean reward:  1.269 [-2.502, 32.193], mean action: 2.321 [0.000, 12.000],  loss: 0.016802, mae: 0.336372, mean_q: 0.527740, mean_eps: 0.000000
 1575/5000: episode: 67, duration: 0.216s, episode steps:  15, steps per second:  70, episode reward: -45.000, mean reward: -3.000 [-32.320,  1.420], mean action: 5.000 [0.000, 16.000],  loss: 0.019751, mae: 0.359655, mean_q: 0.493149, mean_eps: 0.000000
 1603/5000: episode: 68, duration: 0.405s, episode steps:  28, steps per second:  69, episode reward: -33.000, mean reward: -1.179 [-32.349,  2.445], mean action: 3.964 [0.000, 16.000],  loss: 0.024596, mae: 0.381430, mean_q: 0.475299, mean_eps: 0.000000
 1626/5000: episode: 69, duration: 0.349s, episode steps:  23, steps per second:  66, episode reward: 38.143, mean reward:  1.658 [-2.398, 32.085], mean action: 2.478 [1.000, 9.000],  loss: 0.020723, mae: 0.366894, mean_q: 0.504424, mean_eps: 0.000000
 1650/5000: episode: 70, duration: 0.369s, episode steps:  24, steps per second:  65, episode reward: 35.455, mean reward:  1.477 [-2.304, 32.090], mean action: 2.583 [0.000, 15.000],  loss: 0.021108, mae: 0.359963, mean_q: 0.526621, mean_eps: 0.000000
 1674/5000: episode: 71, duration: 0.721s, episode steps:  24, steps per second:  33, episode reward: -32.500, mean reward: -1.354 [-31.947,  2.725], mean action: 8.208 [0.000, 19.000],  loss: 0.023250, mae: 0.377983, mean_q: 0.521463, mean_eps: 0.000000
 1689/5000: episode: 72, duration: 0.248s, episode steps:  15, steps per second:  61, episode reward: 41.652, mean reward:  2.777 [-2.192, 32.652], mean action: 4.533 [0.000, 19.000],  loss: 0.023369, mae: 0.377428, mean_q: 0.516750, mean_eps: 0.000000
 1714/5000: episode: 73, duration: 0.348s, episode steps:  25, steps per second:  72, episode reward: -33.000, mean reward: -1.320 [-29.449,  2.769], mean action: 8.120 [0.000, 19.000],  loss: 0.028967, mae: 0.394351, mean_q: 0.567914, mean_eps: 0.000000
 1737/5000: episode: 74, duration: 0.370s, episode steps:  23, steps per second:  62, episode reward: 34.906, mean reward:  1.518 [-2.412, 32.541], mean action: 4.174 [0.000, 15.000],  loss: 0.020407, mae: 0.356012, mean_q: 0.571012, mean_eps: 0.000000
 1756/5000: episode: 75, duration: 0.671s, episode steps:  19, steps per second:  28, episode reward: 33.000, mean reward:  1.737 [-3.000, 33.000], mean action: 4.789 [0.000, 15.000],  loss: 0.019893, mae: 0.355681, mean_q: 0.593095, mean_eps: 0.000000
 1774/5000: episode: 76, duration: 0.320s, episode steps:  18, steps per second:  56, episode reward: 41.532, mean reward:  2.307 [-2.233, 32.225], mean action: 2.889 [1.000, 15.000],  loss: 0.019659, mae: 0.359293, mean_q: 0.597826, mean_eps: 0.000000
 1793/5000: episode: 77, duration: 0.323s, episode steps:  19, steps per second:  59, episode reward: 35.414, mean reward:  1.864 [-2.465, 33.000], mean action: 5.421 [0.000, 15.000],  loss: 0.022560, mae: 0.369195, mean_q: 0.489294, mean_eps: 0.000000
 1817/5000: episode: 78, duration: 0.497s, episode steps:  24, steps per second:  48, episode reward: -36.000, mean reward: -1.500 [-32.425,  2.900], mean action: 6.583 [0.000, 15.000],  loss: 0.017794, mae: 0.352909, mean_q: 0.483061, mean_eps: 0.000000
 1838/5000: episode: 79, duration: 0.585s, episode steps:  21, steps per second:  36, episode reward: 38.717, mean reward:  1.844 [-2.766, 32.040], mean action: 3.571 [0.000, 12.000],  loss: 0.020903, mae: 0.364421, mean_q: 0.504629, mean_eps: 0.000000
 1860/5000: episode: 80, duration: 0.389s, episode steps:  22, steps per second:  57, episode reward: 38.067, mean reward:  1.730 [-2.397, 31.837], mean action: 3.182 [0.000, 16.000],  loss: 0.022397, mae: 0.365786, mean_q: 0.561047, mean_eps: 0.000000
 1877/5000: episode: 81, duration: 0.294s, episode steps:  17, steps per second:  58, episode reward: 38.443, mean reward:  2.261 [-2.668, 32.216], mean action: 3.882 [0.000, 16.000],  loss: 0.019826, mae: 0.350605, mean_q: 0.549410, mean_eps: 0.000000
 1904/5000: episode: 82, duration: 0.430s, episode steps:  27, steps per second:  63, episode reward: 32.273, mean reward:  1.195 [-2.511, 31.962], mean action: 4.593 [0.000, 19.000],  loss: 0.023618, mae: 0.366694, mean_q: 0.534496, mean_eps: 0.000000
 1918/5000: episode: 83, duration: 0.246s, episode steps:  14, steps per second:  57, episode reward: 44.671, mean reward:  3.191 [-3.000, 32.016], mean action: 3.214 [0.000, 19.000],  loss: 0.021470, mae: 0.362554, mean_q: 0.498652, mean_eps: 0.000000
 1940/5000: episode: 84, duration: 0.365s, episode steps:  22, steps per second:  60, episode reward: 35.794, mean reward:  1.627 [-2.592, 32.297], mean action: 6.273 [0.000, 20.000],  loss: 0.020857, mae: 0.360919, mean_q: 0.507139, mean_eps: 0.000000
 1991/5000: episode: 85, duration: 1.329s, episode steps:  51, steps per second:  38, episode reward: 41.148, mean reward:  0.807 [-3.000, 32.053], mean action: 2.647 [0.000, 19.000],  loss: 0.019958, mae: 0.361609, mean_q: 0.508604, mean_eps: 0.000000
 2006/5000: episode: 86, duration: 0.312s, episode steps:  15, steps per second:  48, episode reward: 41.345, mean reward:  2.756 [-2.641, 32.102], mean action: 5.133 [0.000, 16.000],  loss: 0.018434, mae: 0.351174, mean_q: 0.509428, mean_eps: 0.000000
 2029/5000: episode: 87, duration: 0.411s, episode steps:  23, steps per second:  56, episode reward: 32.747, mean reward:  1.424 [-2.901, 32.880], mean action: 6.913 [0.000, 16.000],  loss: 0.018814, mae: 0.351503, mean_q: 0.506981, mean_eps: 0.000000
 2071/5000: episode: 88, duration: 0.672s, episode steps:  42, steps per second:  62, episode reward: 34.810, mean reward:  0.829 [-2.889, 31.553], mean action: 4.452 [0.000, 15.000],  loss: 0.019483, mae: 0.355456, mean_q: 0.537315, mean_eps: 0.000000
 2094/5000: episode: 89, duration: 0.371s, episode steps:  23, steps per second:  62, episode reward: 36.000, mean reward:  1.565 [-2.588, 32.080], mean action: 3.739 [0.000, 15.000],  loss: 0.018265, mae: 0.356344, mean_q: 0.483324, mean_eps: 0.000000
 2124/5000: episode: 90, duration: 0.679s, episode steps:  30, steps per second:  44, episode reward: 32.494, mean reward:  1.083 [-3.000, 32.215], mean action: 8.400 [0.000, 21.000],  loss: 0.021124, mae: 0.365494, mean_q: 0.490651, mean_eps: 0.000000
 2148/5000: episode: 91, duration: 0.940s, episode steps:  24, steps per second:  26, episode reward: 32.825, mean reward:  1.368 [-2.321, 32.390], mean action: 6.042 [0.000, 17.000],  loss: 0.018464, mae: 0.344627, mean_q: 0.497119, mean_eps: 0.000000
 2166/5000: episode: 92, duration: 0.736s, episode steps:  18, steps per second:  24, episode reward: 36.000, mean reward:  2.000 [-2.461, 29.905], mean action: 3.611 [0.000, 16.000],  loss: 0.017804, mae: 0.335556, mean_q: 0.455736, mean_eps: 0.000000
 2192/5000: episode: 93, duration: 1.007s, episode steps:  26, steps per second:  26, episode reward: 35.418, mean reward:  1.362 [-2.226, 31.997], mean action: 3.923 [0.000, 16.000],  loss: 0.020080, mae: 0.349030, mean_q: 0.476663, mean_eps: 0.000000
 2208/5000: episode: 94, duration: 0.698s, episode steps:  16, steps per second:  23, episode reward: 41.191, mean reward:  2.574 [-2.234, 32.500], mean action: 4.875 [0.000, 19.000],  loss: 0.026507, mae: 0.379625, mean_q: 0.489788, mean_eps: 0.000000
 2234/5000: episode: 95, duration: 0.759s, episode steps:  26, steps per second:  34, episode reward: 32.751, mean reward:  1.260 [-2.353, 32.440], mean action: 4.192 [0.000, 15.000],  loss: 0.023063, mae: 0.356049, mean_q: 0.459513, mean_eps: 0.000000
 2260/5000: episode: 96, duration: 0.399s, episode steps:  26, steps per second:  65, episode reward: 35.764, mean reward:  1.376 [-2.390, 32.480], mean action: 3.538 [0.000, 19.000],  loss: 0.018684, mae: 0.343789, mean_q: 0.479657, mean_eps: 0.000000
 2276/5000: episode: 97, duration: 0.257s, episode steps:  16, steps per second:  62, episode reward: 41.167, mean reward:  2.573 [-2.584, 32.308], mean action: 4.500 [1.000, 15.000],  loss: 0.019709, mae: 0.346141, mean_q: 0.484861, mean_eps: 0.000000
 2291/5000: episode: 98, duration: 0.226s, episode steps:  15, steps per second:  66, episode reward: 41.333, mean reward:  2.756 [-3.000, 32.646], mean action: 2.600 [0.000, 11.000],  loss: 0.023637, mae: 0.363911, mean_q: 0.476025, mean_eps: 0.000000
 2309/5000: episode: 99, duration: 0.302s, episode steps:  18, steps per second:  60, episode reward: 41.484, mean reward:  2.305 [-2.255, 32.977], mean action: 5.389 [1.000, 14.000],  loss: 0.022573, mae: 0.360684, mean_q: 0.461790, mean_eps: 0.000000
 2331/5000: episode: 100, duration: 0.699s, episode steps:  22, steps per second:  31, episode reward: -35.150, mean reward: -1.598 [-32.150,  2.713], mean action: 3.545 [0.000, 12.000],  loss: 0.021175, mae: 0.353132, mean_q: 0.459728, mean_eps: 0.000000
 2348/5000: episode: 101, duration: 0.509s, episode steps:  17, steps per second:  33, episode reward: 41.045, mean reward:  2.414 [-2.399, 32.280], mean action: 2.941 [0.000, 12.000],  loss: 0.022754, mae: 0.363846, mean_q: 0.468731, mean_eps: 0.000000
 2369/5000: episode: 102, duration: 0.364s, episode steps:  21, steps per second:  58, episode reward: 38.397, mean reward:  1.828 [-2.578, 32.199], mean action: 2.952 [0.000, 12.000],  loss: 0.019317, mae: 0.349384, mean_q: 0.461579, mean_eps: 0.000000
 2391/5000: episode: 103, duration: 0.384s, episode steps:  22, steps per second:  57, episode reward: 36.000, mean reward:  1.636 [-2.359, 32.840], mean action: 6.500 [0.000, 15.000],  loss: 0.023114, mae: 0.369993, mean_q: 0.484444, mean_eps: 0.000000
 2406/5000: episode: 104, duration: 0.393s, episode steps:  15, steps per second:  38, episode reward: 43.348, mean reward:  2.890 [-2.396, 31.315], mean action: 3.133 [0.000, 8.000],  loss: 0.020350, mae: 0.361558, mean_q: 0.463521, mean_eps: 0.000000
 2422/5000: episode: 105, duration: 0.417s, episode steps:  16, steps per second:  38, episode reward: 38.901, mean reward:  2.431 [-2.534, 32.901], mean action: 4.812 [0.000, 19.000],  loss: 0.022783, mae: 0.365385, mean_q: 0.485369, mean_eps: 0.000000
 2444/5000: episode: 106, duration: 0.423s, episode steps:  22, steps per second:  52, episode reward: 38.810, mean reward:  1.764 [-2.567, 32.230], mean action: 4.182 [0.000, 19.000],  loss: 0.022696, mae: 0.359654, mean_q: 0.545980, mean_eps: 0.000000
 2462/5000: episode: 107, duration: 0.331s, episode steps:  18, steps per second:  54, episode reward: 33.000, mean reward:  1.833 [-3.000, 29.778], mean action: 3.500 [0.000, 19.000],  loss: 0.021546, mae: 0.354768, mean_q: 0.548251, mean_eps: 0.000000
 2478/5000: episode: 108, duration: 0.300s, episode steps:  16, steps per second:  53, episode reward: 41.123, mean reward:  2.570 [-2.609, 32.370], mean action: 3.375 [0.000, 19.000],  loss: 0.017998, mae: 0.346865, mean_q: 0.474796, mean_eps: 0.000000
 2497/5000: episode: 109, duration: 0.310s, episode steps:  19, steps per second:  61, episode reward: 36.000, mean reward:  1.895 [-3.000, 32.190], mean action: 3.053 [0.000, 11.000],  loss: 0.017059, mae: 0.326465, mean_q: 0.493690, mean_eps: 0.000000
 2518/5000: episode: 110, duration: 0.334s, episode steps:  21, steps per second:  63, episode reward: 44.131, mean reward:  2.101 [-2.519, 32.170], mean action: 2.000 [1.000, 3.000],  loss: 0.019969, mae: 0.349509, mean_q: 0.519295, mean_eps: 0.000000
 2546/5000: episode: 111, duration: 1.304s, episode steps:  28, steps per second:  21, episode reward: -32.810, mean reward: -1.172 [-32.119,  2.480], mean action: 5.714 [0.000, 21.000],  loss: 0.020520, mae: 0.351871, mean_q: 0.494574, mean_eps: 0.000000
 2569/5000: episode: 112, duration: 0.403s, episode steps:  23, steps per second:  57, episode reward: -32.080, mean reward: -1.395 [-32.080,  2.340], mean action: 3.087 [0.000, 12.000],  loss: 0.018389, mae: 0.348320, mean_q: 0.464540, mean_eps: 0.000000
 2585/5000: episode: 113, duration: 0.254s, episode steps:  16, steps per second:  63, episode reward: 44.282, mean reward:  2.768 [-2.082, 32.240], mean action: 0.938 [0.000, 15.000],  loss: 0.015428, mae: 0.337736, mean_q: 0.441294, mean_eps: 0.000000
 2596/5000: episode: 114, duration: 0.183s, episode steps:  11, steps per second:  60, episode reward: 44.167, mean reward:  4.015 [-2.153, 32.830], mean action: 2.273 [0.000, 15.000],  loss: 0.014685, mae: 0.336725, mean_q: 0.403721, mean_eps: 0.000000
 2610/5000: episode: 115, duration: 0.243s, episode steps:  14, steps per second:  58, episode reward: 41.570, mean reward:  2.969 [-2.147, 30.569], mean action: 2.286 [0.000, 11.000],  loss: 0.018840, mae: 0.353628, mean_q: 0.470253, mean_eps: 0.000000
 2630/5000: episode: 116, duration: 0.408s, episode steps:  20, steps per second:  49, episode reward: 39.000, mean reward:  1.950 [-2.287, 32.040], mean action: 4.300 [0.000, 16.000],  loss: 0.019434, mae: 0.353552, mean_q: 0.448632, mean_eps: 0.000000
 2642/5000: episode: 117, duration: 0.243s, episode steps:  12, steps per second:  49, episode reward: 46.895, mean reward:  3.908 [-0.359, 33.000], mean action: 3.500 [0.000, 14.000],  loss: 0.016734, mae: 0.335385, mean_q: 0.514159, mean_eps: 0.000000
 2669/5000: episode: 118, duration: 0.464s, episode steps:  27, steps per second:  58, episode reward: 32.190, mean reward:  1.192 [-2.714, 31.510], mean action: 3.852 [0.000, 16.000],  loss: 0.018050, mae: 0.348623, mean_q: 0.532829, mean_eps: 0.000000
 2691/5000: episode: 119, duration: 0.333s, episode steps:  22, steps per second:  66, episode reward: 38.652, mean reward:  1.757 [-2.235, 32.103], mean action: 2.364 [0.000, 16.000],  loss: 0.023780, mae: 0.366155, mean_q: 0.533454, mean_eps: 0.000000
 2732/5000: episode: 120, duration: 0.605s, episode steps:  41, steps per second:  68, episode reward: 35.362, mean reward:  0.862 [-3.000, 32.092], mean action: 2.829 [0.000, 16.000],  loss: 0.019181, mae: 0.354203, mean_q: 0.491718, mean_eps: 0.000000
 2754/5000: episode: 121, duration: 0.327s, episode steps:  22, steps per second:  67, episode reward: 35.825, mean reward:  1.628 [-2.764, 32.315], mean action: 5.500 [0.000, 20.000],  loss: 0.021484, mae: 0.372750, mean_q: 0.488227, mean_eps: 0.000000
 2781/5000: episode: 122, duration: 0.397s, episode steps:  27, steps per second:  68, episode reward: -32.780, mean reward: -1.214 [-32.386,  2.520], mean action: 4.111 [0.000, 19.000],  loss: 0.020359, mae: 0.369937, mean_q: 0.498974, mean_eps: 0.000000
 2801/5000: episode: 123, duration: 0.334s, episode steps:  20, steps per second:  60, episode reward: 41.078, mean reward:  2.054 [-2.242, 32.034], mean action: 1.800 [0.000, 11.000],  loss: 0.016874, mae: 0.343894, mean_q: 0.560999, mean_eps: 0.000000
 2823/5000: episode: 124, duration: 0.409s, episode steps:  22, steps per second:  54, episode reward: 33.000, mean reward:  1.500 [-3.000, 32.650], mean action: 2.727 [0.000, 12.000],  loss: 0.024886, mae: 0.377604, mean_q: 0.553502, mean_eps: 0.000000
 2840/5000: episode: 125, duration: 0.308s, episode steps:  17, steps per second:  55, episode reward: 38.324, mean reward:  2.254 [-3.000, 31.684], mean action: 3.412 [1.000, 15.000],  loss: 0.019513, mae: 0.354950, mean_q: 0.572507, mean_eps: 0.000000
 2863/5000: episode: 126, duration: 0.466s, episode steps:  23, steps per second:  49, episode reward: -35.440, mean reward: -1.541 [-32.504,  2.419], mean action: 3.783 [0.000, 15.000],  loss: 0.020857, mae: 0.356240, mean_q: 0.578860, mean_eps: 0.000000
 2885/5000: episode: 127, duration: 0.402s, episode steps:  22, steps per second:  55, episode reward: 36.000, mean reward:  1.636 [-2.194, 32.570], mean action: 4.091 [0.000, 20.000],  loss: 0.020585, mae: 0.355798, mean_q: 0.576433, mean_eps: 0.000000
 2904/5000: episode: 128, duration: 0.680s, episode steps:  19, steps per second:  28, episode reward: 41.258, mean reward:  2.171 [-2.627, 32.001], mean action: 2.789 [0.000, 15.000],  loss: 0.019294, mae: 0.353254, mean_q: 0.584801, mean_eps: 0.000000
 2932/5000: episode: 129, duration: 0.463s, episode steps:  28, steps per second:  60, episode reward: 32.180, mean reward:  1.149 [-2.390, 31.530], mean action: 3.500 [0.000, 20.000],  loss: 0.020863, mae: 0.356141, mean_q: 0.569555, mean_eps: 0.000000
 2959/5000: episode: 130, duration: 0.441s, episode steps:  27, steps per second:  61, episode reward: 33.000, mean reward:  1.222 [-3.000, 32.570], mean action: 3.148 [0.000, 15.000],  loss: 0.019829, mae: 0.358489, mean_q: 0.533701, mean_eps: 0.000000
 2990/5000: episode: 131, duration: 0.486s, episode steps:  31, steps per second:  64, episode reward: 36.000, mean reward:  1.161 [-2.590, 32.010], mean action: 3.258 [0.000, 19.000],  loss: 0.018923, mae: 0.348052, mean_q: 0.549883, mean_eps: 0.000000
 3005/5000: episode: 132, duration: 0.241s, episode steps:  15, steps per second:  62, episode reward: 41.725, mean reward:  2.782 [-2.409, 32.690], mean action: 4.267 [1.000, 15.000],  loss: 0.017338, mae: 0.345453, mean_q: 0.630416, mean_eps: 0.000000
 3041/5000: episode: 133, duration: 0.501s, episode steps:  36, steps per second:  72, episode reward: 33.000, mean reward:  0.917 [-3.000, 32.030], mean action: 5.722 [0.000, 15.000],  loss: 0.020215, mae: 0.355056, mean_q: 0.564926, mean_eps: 0.000000
 3064/5000: episode: 134, duration: 0.342s, episode steps:  23, steps per second:  67, episode reward: 32.329, mean reward:  1.406 [-2.442, 31.459], mean action: 4.652 [0.000, 15.000],  loss: 0.023167, mae: 0.365656, mean_q: 0.529333, mean_eps: 0.000000
 3086/5000: episode: 135, duration: 0.320s, episode steps:  22, steps per second:  69, episode reward: 35.238, mean reward:  1.602 [-3.000, 32.362], mean action: 10.273 [0.000, 20.000],  loss: 0.021504, mae: 0.368468, mean_q: 0.517980, mean_eps: 0.000000
 3113/5000: episode: 136, duration: 0.385s, episode steps:  27, steps per second:  70, episode reward: 32.389, mean reward:  1.200 [-2.477, 32.333], mean action: 6.778 [0.000, 20.000],  loss: 0.019480, mae: 0.355827, mean_q: 0.509106, mean_eps: 0.000000
 3135/5000: episode: 137, duration: 0.328s, episode steps:  22, steps per second:  67, episode reward: 37.612, mean reward:  1.710 [-2.619, 32.114], mean action: 1.773 [0.000, 9.000],  loss: 0.018013, mae: 0.349757, mean_q: 0.552087, mean_eps: 0.000000
 3169/5000: episode: 138, duration: 0.587s, episode steps:  34, steps per second:  58, episode reward: 33.000, mean reward:  0.971 [-2.697, 33.351], mean action: 5.912 [0.000, 18.000],  loss: 0.023994, mae: 0.379922, mean_q: 0.475365, mean_eps: 0.000000
 3186/5000: episode: 139, duration: 0.561s, episode steps:  17, steps per second:  30, episode reward: 40.148, mean reward:  2.362 [-2.252, 32.080], mean action: 5.000 [0.000, 16.000],  loss: 0.024220, mae: 0.374714, mean_q: 0.482325, mean_eps: 0.000000
 3219/5000: episode: 140, duration: 0.884s, episode steps:  33, steps per second:  37, episode reward: -32.100, mean reward: -0.973 [-32.176,  2.710], mean action: 7.848 [1.000, 16.000],  loss: 0.021393, mae: 0.365016, mean_q: 0.504382, mean_eps: 0.000000
 3243/5000: episode: 141, duration: 0.463s, episode steps:  24, steps per second:  52, episode reward: 33.000, mean reward:  1.375 [-2.543, 33.000], mean action: 4.417 [0.000, 12.000],  loss: 0.017760, mae: 0.354513, mean_q: 0.490863, mean_eps: 0.000000
 3260/5000: episode: 142, duration: 0.407s, episode steps:  17, steps per second:  42, episode reward: 41.617, mean reward:  2.448 [-2.375, 32.329], mean action: 3.000 [0.000, 9.000],  loss: 0.024715, mae: 0.398751, mean_q: 0.491176, mean_eps: 0.000000
 3284/5000: episode: 143, duration: 0.896s, episode steps:  24, steps per second:  27, episode reward: 35.287, mean reward:  1.470 [-2.352, 31.928], mean action: 2.750 [0.000, 12.000],  loss: 0.021011, mae: 0.374914, mean_q: 0.485305, mean_eps: 0.000000
 3304/5000: episode: 144, duration: 0.334s, episode steps:  20, steps per second:  60, episode reward: 38.458, mean reward:  1.923 [-2.275, 31.818], mean action: 3.450 [0.000, 15.000],  loss: 0.019142, mae: 0.363234, mean_q: 0.595115, mean_eps: 0.000000
 3322/5000: episode: 145, duration: 0.531s, episode steps:  18, steps per second:  34, episode reward: 38.088, mean reward:  2.116 [-2.610, 32.150], mean action: 4.833 [0.000, 19.000],  loss: 0.020191, mae: 0.369808, mean_q: 0.551664, mean_eps: 0.000000
 3340/5000: episode: 146, duration: 0.466s, episode steps:  18, steps per second:  39, episode reward: 35.903, mean reward:  1.995 [-3.000, 32.143], mean action: 4.889 [0.000, 15.000],  loss: 0.022619, mae: 0.378922, mean_q: 0.513219, mean_eps: 0.000000
 3375/5000: episode: 147, duration: 0.886s, episode steps:  35, steps per second:  39, episode reward: -32.740, mean reward: -0.935 [-32.016,  2.490], mean action: 7.171 [0.000, 20.000],  loss: 0.018935, mae: 0.356020, mean_q: 0.494966, mean_eps: 0.000000
 3400/5000: episode: 148, duration: 0.503s, episode steps:  25, steps per second:  50, episode reward: -39.000, mean reward: -1.560 [-32.176,  2.410], mean action: 5.880 [0.000, 15.000],  loss: 0.019099, mae: 0.353135, mean_q: 0.544627, mean_eps: 0.000000
 3431/5000: episode: 149, duration: 0.457s, episode steps:  31, steps per second:  68, episode reward: -41.830, mean reward: -1.349 [-32.027,  2.302], mean action: 8.258 [0.000, 19.000],  loss: 0.018588, mae: 0.343161, mean_q: 0.540860, mean_eps: 0.000000
 3453/5000: episode: 150, duration: 0.317s, episode steps:  22, steps per second:  69, episode reward: 35.532, mean reward:  1.615 [-2.547, 32.440], mean action: 6.091 [0.000, 19.000],  loss: 0.018770, mae: 0.341471, mean_q: 0.530914, mean_eps: 0.000000
 3487/5000: episode: 151, duration: 0.499s, episode steps:  34, steps per second:  68, episode reward: 41.067, mean reward:  1.208 [-2.188, 32.543], mean action: 2.500 [1.000, 12.000],  loss: 0.017233, mae: 0.336545, mean_q: 0.562232, mean_eps: 0.000000
 3514/5000: episode: 152, duration: 0.927s, episode steps:  27, steps per second:  29, episode reward: -32.460, mean reward: -1.202 [-32.407,  2.910], mean action: 3.778 [0.000, 12.000],  loss: 0.020661, mae: 0.352718, mean_q: 0.542883, mean_eps: 0.000000
 3543/5000: episode: 153, duration: 0.422s, episode steps:  29, steps per second:  69, episode reward: -32.160, mean reward: -1.109 [-32.423,  2.264], mean action: 6.345 [1.000, 20.000],  loss: 0.020934, mae: 0.358116, mean_q: 0.510290, mean_eps: 0.000000
 3583/5000: episode: 154, duration: 0.569s, episode steps:  40, steps per second:  70, episode reward: 41.557, mean reward:  1.039 [-3.000, 31.963], mean action: 2.850 [0.000, 15.000],  loss: 0.020686, mae: 0.356553, mean_q: 0.577367, mean_eps: 0.000000
 3606/5000: episode: 155, duration: 0.335s, episode steps:  23, steps per second:  69, episode reward: 36.000, mean reward:  1.565 [-2.726, 33.000], mean action: 3.304 [0.000, 15.000],  loss: 0.023290, mae: 0.374337, mean_q: 0.525891, mean_eps: 0.000000
 3630/5000: episode: 156, duration: 0.351s, episode steps:  24, steps per second:  68, episode reward: 33.000, mean reward:  1.375 [-3.000, 30.208], mean action: 3.542 [0.000, 16.000],  loss: 0.021343, mae: 0.377937, mean_q: 0.464196, mean_eps: 0.000000
 3649/5000: episode: 157, duration: 0.299s, episode steps:  19, steps per second:  64, episode reward: 32.710, mean reward:  1.722 [-3.000, 32.093], mean action: 4.737 [0.000, 16.000],  loss: 0.018542, mae: 0.357915, mean_q: 0.529535, mean_eps: 0.000000
 3673/5000: episode: 158, duration: 0.342s, episode steps:  24, steps per second:  70, episode reward: -41.470, mean reward: -1.728 [-33.000,  2.750], mean action: 8.583 [0.000, 20.000],  loss: 0.021223, mae: 0.364801, mean_q: 0.495567, mean_eps: 0.000000
 3694/5000: episode: 159, duration: 0.310s, episode steps:  21, steps per second:  68, episode reward: 41.750, mean reward:  1.988 [-2.067, 31.900], mean action: 2.905 [0.000, 16.000],  loss: 0.018940, mae: 0.361564, mean_q: 0.454051, mean_eps: 0.000000
 3710/5000: episode: 160, duration: 0.232s, episode steps:  16, steps per second:  69, episode reward: 38.591, mean reward:  2.412 [-2.498, 32.340], mean action: 3.938 [0.000, 16.000],  loss: 0.021992, mae: 0.375020, mean_q: 0.476226, mean_eps: 0.000000
 3736/5000: episode: 161, duration: 0.361s, episode steps:  26, steps per second:  72, episode reward: -35.440, mean reward: -1.363 [-32.116,  2.684], mean action: 3.808 [0.000, 16.000],  loss: 0.021040, mae: 0.368773, mean_q: 0.484224, mean_eps: 0.000000
 3757/5000: episode: 162, duration: 0.311s, episode steps:  21, steps per second:  68, episode reward: -35.470, mean reward: -1.689 [-32.335,  2.320], mean action: 4.762 [0.000, 16.000],  loss: 0.019832, mae: 0.351676, mean_q: 0.509983, mean_eps: 0.000000
 3783/5000: episode: 163, duration: 0.366s, episode steps:  26, steps per second:  71, episode reward: 37.065, mean reward:  1.426 [-3.000, 32.370], mean action: 6.346 [0.000, 16.000],  loss: 0.020610, mae: 0.360878, mean_q: 0.470654, mean_eps: 0.000000
 3808/5000: episode: 164, duration: 0.357s, episode steps:  25, steps per second:  70, episode reward: 30.244, mean reward:  1.210 [-3.000, 32.561], mean action: 4.480 [1.000, 19.000],  loss: 0.021167, mae: 0.368723, mean_q: 0.498262, mean_eps: 0.000000
 3830/5000: episode: 165, duration: 0.311s, episode steps:  22, steps per second:  71, episode reward: -35.390, mean reward: -1.609 [-32.045,  2.580], mean action: 5.864 [0.000, 20.000],  loss: 0.020579, mae: 0.357860, mean_q: 0.515041, mean_eps: 0.000000
 3857/5000: episode: 166, duration: 0.377s, episode steps:  27, steps per second:  72, episode reward: -35.850, mean reward: -1.328 [-32.352,  2.829], mean action: 6.630 [0.000, 15.000],  loss: 0.021678, mae: 0.366417, mean_q: 0.524744, mean_eps: 0.000000
 3874/5000: episode: 167, duration: 0.253s, episode steps:  17, steps per second:  67, episode reward: 38.111, mean reward:  2.242 [-3.000, 33.000], mean action: 4.824 [0.000, 15.000],  loss: 0.020443, mae: 0.360203, mean_q: 0.578960, mean_eps: 0.000000
 3888/5000: episode: 168, duration: 0.212s, episode steps:  14, steps per second:  66, episode reward: 40.735, mean reward:  2.910 [-2.601, 32.744], mean action: 4.071 [1.000, 15.000],  loss: 0.018369, mae: 0.347016, mean_q: 0.581699, mean_eps: 0.000000
 3930/5000: episode: 169, duration: 0.600s, episode steps:  42, steps per second:  70, episode reward: 35.264, mean reward:  0.840 [-2.526, 32.045], mean action: 4.619 [0.000, 19.000],  loss: 0.019686, mae: 0.357513, mean_q: 0.574425, mean_eps: 0.000000
 3942/5000: episode: 170, duration: 0.182s, episode steps:  12, steps per second:  66, episode reward: 41.878, mean reward:  3.490 [-3.000, 32.295], mean action: 3.500 [0.000, 15.000],  loss: 0.020329, mae: 0.364682, mean_q: 0.523624, mean_eps: 0.000000
 3964/5000: episode: 171, duration: 0.318s, episode steps:  22, steps per second:  69, episode reward: 40.812, mean reward:  1.855 [-2.496, 31.927], mean action: 2.727 [0.000, 11.000],  loss: 0.023703, mae: 0.392680, mean_q: 0.506650, mean_eps: 0.000000
 3983/5000: episode: 172, duration: 0.271s, episode steps:  19, steps per second:  70, episode reward: 35.741, mean reward:  1.881 [-3.000, 32.320], mean action: 2.842 [0.000, 11.000],  loss: 0.020790, mae: 0.368210, mean_q: 0.493948, mean_eps: 0.000000
 4006/5000: episode: 173, duration: 0.333s, episode steps:  23, steps per second:  69, episode reward: 32.027, mean reward:  1.392 [-2.517, 31.634], mean action: 5.043 [0.000, 16.000],  loss: 0.019780, mae: 0.359354, mean_q: 0.516996, mean_eps: 0.000000
 4029/5000: episode: 174, duration: 0.804s, episode steps:  23, steps per second:  29, episode reward: 32.635, mean reward:  1.419 [-2.458, 32.085], mean action: 5.478 [0.000, 15.000],  loss: 0.023496, mae: 0.380583, mean_q: 0.544840, mean_eps: 0.000000
 4047/5000: episode: 175, duration: 0.714s, episode steps:  18, steps per second:  25, episode reward: 39.000, mean reward:  2.167 [-2.181, 32.490], mean action: 2.833 [0.000, 15.000],  loss: 0.026204, mae: 0.393319, mean_q: 0.519890, mean_eps: 0.000000
 4069/5000: episode: 176, duration: 0.347s, episode steps:  22, steps per second:  63, episode reward: 32.541, mean reward:  1.479 [-2.795, 31.791], mean action: 4.318 [0.000, 15.000],  loss: 0.023923, mae: 0.379517, mean_q: 0.503336, mean_eps: 0.000000
 4103/5000: episode: 177, duration: 0.521s, episode steps:  34, steps per second:  65, episode reward: -32.190, mean reward: -0.947 [-32.670,  2.783], mean action: 5.324 [0.000, 17.000],  loss: 0.020687, mae: 0.365995, mean_q: 0.561205, mean_eps: 0.000000
 4133/5000: episode: 178, duration: 0.480s, episode steps:  30, steps per second:  62, episode reward: 32.210, mean reward:  1.074 [-2.900, 32.220], mean action: 5.667 [0.000, 15.000],  loss: 0.020830, mae: 0.361921, mean_q: 0.577684, mean_eps: 0.000000
 4163/5000: episode: 179, duration: 0.453s, episode steps:  30, steps per second:  66, episode reward: -32.100, mean reward: -1.070 [-32.017,  2.910], mean action: 7.100 [0.000, 20.000],  loss: 0.019008, mae: 0.355421, mean_q: 0.584117, mean_eps: 0.000000
 4197/5000: episode: 180, duration: 0.522s, episode steps:  34, steps per second:  65, episode reward: 35.349, mean reward:  1.040 [-2.256, 33.000], mean action: 4.000 [0.000, 12.000],  loss: 0.018463, mae: 0.362098, mean_q: 0.538333, mean_eps: 0.000000
 4214/5000: episode: 181, duration: 0.270s, episode steps:  17, steps per second:  63, episode reward: 41.447, mean reward:  2.438 [-2.351, 32.634], mean action: 2.588 [0.000, 12.000],  loss: 0.023925, mae: 0.383781, mean_q: 0.503115, mean_eps: 0.000000
 4234/5000: episode: 182, duration: 0.312s, episode steps:  20, steps per second:  64, episode reward: 38.708, mean reward:  1.935 [-2.090, 31.938], mean action: 3.150 [0.000, 14.000],  loss: 0.024695, mae: 0.380107, mean_q: 0.579585, mean_eps: 0.000000
 4251/5000: episode: 183, duration: 0.260s, episode steps:  17, steps per second:  65, episode reward: 38.051, mean reward:  2.238 [-2.721, 32.440], mean action: 3.471 [0.000, 12.000],  loss: 0.019478, mae: 0.354843, mean_q: 0.556384, mean_eps: 0.000000
 4269/5000: episode: 184, duration: 0.375s, episode steps:  18, steps per second:  48, episode reward: 38.328, mean reward:  2.129 [-2.560, 32.320], mean action: 3.000 [0.000, 12.000],  loss: 0.021810, mae: 0.369121, mean_q: 0.570256, mean_eps: 0.000000
 4320/5000: episode: 185, duration: 0.815s, episode steps:  51, steps per second:  63, episode reward: -32.540, mean reward: -0.638 [-32.031,  2.410], mean action: 4.765 [0.000, 20.000],  loss: 0.017888, mae: 0.349629, mean_q: 0.551920, mean_eps: 0.000000
 4331/5000: episode: 186, duration: 0.173s, episode steps:  11, steps per second:  64, episode reward: 41.902, mean reward:  3.809 [-2.599, 32.902], mean action: 2.727 [0.000, 15.000],  loss: 0.020109, mae: 0.360950, mean_q: 0.555481, mean_eps: 0.000000
 4354/5000: episode: 187, duration: 0.329s, episode steps:  23, steps per second:  70, episode reward: -35.230, mean reward: -1.532 [-31.763,  3.000], mean action: 2.913 [0.000, 16.000],  loss: 0.018657, mae: 0.347357, mean_q: 0.473482, mean_eps: 0.000000
 4380/5000: episode: 188, duration: 0.756s, episode steps:  26, steps per second:  34, episode reward: 37.735, mean reward:  1.451 [-3.000, 32.130], mean action: 1.962 [0.000, 15.000],  loss: 0.016613, mae: 0.335790, mean_q: 0.515571, mean_eps: 0.000000
 4411/5000: episode: 189, duration: 0.559s, episode steps:  31, steps per second:  55, episode reward: 35.732, mean reward:  1.153 [-2.678, 33.000], mean action: 3.387 [0.000, 20.000],  loss: 0.019804, mae: 0.350675, mean_q: 0.494040, mean_eps: 0.000000
 4428/5000: episode: 190, duration: 0.241s, episode steps:  17, steps per second:  70, episode reward: -39.000, mean reward: -2.294 [-33.000,  2.694], mean action: 6.059 [0.000, 16.000],  loss: 0.018982, mae: 0.350851, mean_q: 0.496981, mean_eps: 0.000000
 4458/5000: episode: 191, duration: 0.599s, episode steps:  30, steps per second:  50, episode reward: 32.573, mean reward:  1.086 [-3.000, 32.097], mean action: 4.700 [0.000, 18.000],  loss: 0.018245, mae: 0.357691, mean_q: 0.515533, mean_eps: 0.000000
 4480/5000: episode: 192, duration: 0.351s, episode steps:  22, steps per second:  63, episode reward: -36.000, mean reward: -1.636 [-32.120,  2.460], mean action: 5.182 [0.000, 20.000],  loss: 0.027812, mae: 0.406229, mean_q: 0.558705, mean_eps: 0.000000
 4503/5000: episode: 193, duration: 0.858s, episode steps:  23, steps per second:  27, episode reward: 35.215, mean reward:  1.531 [-3.000, 31.895], mean action: 3.522 [0.000, 15.000],  loss: 0.019229, mae: 0.360630, mean_q: 0.583775, mean_eps: 0.000000
 4528/5000: episode: 194, duration: 0.547s, episode steps:  25, steps per second:  46, episode reward: -32.840, mean reward: -1.314 [-32.058,  2.524], mean action: 6.000 [0.000, 19.000],  loss: 0.019731, mae: 0.360310, mean_q: 0.560352, mean_eps: 0.000000
 4559/5000: episode: 195, duration: 1.314s, episode steps:  31, steps per second:  24, episode reward: -33.000, mean reward: -1.065 [-32.080,  2.430], mean action: 4.806 [0.000, 14.000],  loss: 0.020150, mae: 0.356975, mean_q: 0.545445, mean_eps: 0.000000
 4575/5000: episode: 196, duration: 0.752s, episode steps:  16, steps per second:  21, episode reward: 38.044, mean reward:  2.378 [-3.000, 32.745], mean action: 3.188 [0.000, 16.000],  loss: 0.019020, mae: 0.357675, mean_q: 0.520822, mean_eps: 0.000000
 4596/5000: episode: 197, duration: 0.486s, episode steps:  21, steps per second:  43, episode reward: -39.000, mean reward: -1.857 [-32.366,  3.000], mean action: 5.048 [0.000, 19.000],  loss: 0.020386, mae: 0.357305, mean_q: 0.574554, mean_eps: 0.000000
 4614/5000: episode: 198, duration: 0.371s, episode steps:  18, steps per second:  49, episode reward: 36.000, mean reward:  2.000 [-2.807, 32.130], mean action: 3.944 [0.000, 16.000],  loss: 0.016644, mae: 0.342979, mean_q: 0.485910, mean_eps: 0.000000
 4630/5000: episode: 199, duration: 0.517s, episode steps:  16, steps per second:  31, episode reward: 41.344, mean reward:  2.584 [-2.470, 32.560], mean action: 3.562 [0.000, 19.000],  loss: 0.020608, mae: 0.357506, mean_q: 0.544947, mean_eps: 0.000000
 4650/5000: episode: 200, duration: 0.822s, episode steps:  20, steps per second:  24, episode reward: 36.000, mean reward:  1.800 [-3.000, 32.600], mean action: 4.200 [0.000, 15.000],  loss: 0.023286, mae: 0.371070, mean_q: 0.473592, mean_eps: 0.000000
 4683/5000: episode: 201, duration: 0.486s, episode steps:  33, steps per second:  68, episode reward: 38.852, mean reward:  1.177 [-2.457, 32.762], mean action: 2.455 [0.000, 15.000],  loss: 0.016039, mae: 0.341166, mean_q: 0.507731, mean_eps: 0.000000
 4712/5000: episode: 202, duration: 0.637s, episode steps:  29, steps per second:  46, episode reward: 38.541, mean reward:  1.329 [-2.201, 33.000], mean action: 3.069 [0.000, 19.000],  loss: 0.016551, mae: 0.335641, mean_q: 0.519718, mean_eps: 0.000000
 4739/5000: episode: 203, duration: 0.479s, episode steps:  27, steps per second:  56, episode reward: 35.419, mean reward:  1.312 [-3.000, 32.080], mean action: 4.370 [0.000, 15.000],  loss: 0.021220, mae: 0.350844, mean_q: 0.539222, mean_eps: 0.000000
 4775/5000: episode: 204, duration: 0.806s, episode steps:  36, steps per second:  45, episode reward: 32.889, mean reward:  0.914 [-2.294, 30.080], mean action: 9.639 [0.000, 21.000],  loss: 0.018929, mae: 0.337795, mean_q: 0.525194, mean_eps: 0.000000
 4791/5000: episode: 205, duration: 0.642s, episode steps:  16, steps per second:  25, episode reward: 41.144, mean reward:  2.572 [-2.465, 33.000], mean action: 3.125 [0.000, 11.000],  loss: 0.019138, mae: 0.339288, mean_q: 0.493633, mean_eps: 0.000000
 4820/5000: episode: 206, duration: 1.331s, episode steps:  29, steps per second:  22, episode reward: -32.550, mean reward: -1.122 [-32.267,  2.637], mean action: 7.379 [0.000, 15.000],  loss: 0.018214, mae: 0.339129, mean_q: 0.500564, mean_eps: 0.000000
 4840/5000: episode: 207, duration: 0.565s, episode steps:  20, steps per second:  35, episode reward: -41.290, mean reward: -2.064 [-32.071,  2.382], mean action: 6.750 [0.000, 15.000],  loss: 0.023729, mae: 0.371988, mean_q: 0.459323, mean_eps: 0.000000
 4861/5000: episode: 208, duration: 0.395s, episode steps:  21, steps per second:  53, episode reward: 38.401, mean reward:  1.829 [-2.747, 32.841], mean action: 6.619 [0.000, 15.000],  loss: 0.018783, mae: 0.341293, mean_q: 0.454633, mean_eps: 0.000000
 4882/5000: episode: 209, duration: 0.834s, episode steps:  21, steps per second:  25, episode reward: 35.907, mean reward:  1.710 [-2.392, 32.597], mean action: 6.143 [0.000, 15.000],  loss: 0.019288, mae: 0.340184, mean_q: 0.466752, mean_eps: 0.000000
 4909/5000: episode: 210, duration: 0.633s, episode steps:  27, steps per second:  43, episode reward: 35.292, mean reward:  1.307 [-2.476, 32.620], mean action: 5.444 [0.000, 13.000],  loss: 0.017808, mae: 0.329238, mean_q: 0.506513, mean_eps: 0.000000
 4936/5000: episode: 211, duration: 0.756s, episode steps:  27, steps per second:  36, episode reward: 32.774, mean reward:  1.214 [-2.413, 31.986], mean action: 3.296 [0.000, 16.000],  loss: 0.023894, mae: 0.358979, mean_q: 0.486012, mean_eps: 0.000000
 4957/5000: episode: 212, duration: 0.850s, episode steps:  21, steps per second:  25, episode reward: 35.887, mean reward:  1.709 [-3.000, 32.887], mean action: 2.714 [0.000, 12.000],  loss: 0.022987, mae: 0.352341, mean_q: 0.533065, mean_eps: 0.000000
 4973/5000: episode: 213, duration: 0.257s, episode steps:  16, steps per second:  62, episode reward: 41.554, mean reward:  2.597 [-2.294, 32.903], mean action: 2.625 [0.000, 12.000],  loss: 0.020140, mae: 0.342245, mean_q: 0.507068, mean_eps: 0.000000
 4995/5000: episode: 214, duration: 0.438s, episode steps:  22, steps per second:  50, episode reward: -32.570, mean reward: -1.480 [-32.052,  3.000], mean action: 7.909 [0.000, 14.000],  loss: 0.021704, mae: 0.352540, mean_q: 0.537430, mean_eps: 0.000000
done, took 88.036 seconds
DQN Evaluation: 7478 victories out of 8804 episodes
Training for 5000 steps ...
   19/5000: episode: 1, duration: 0.322s, episode steps:  19, steps per second:  59, episode reward: 39.000, mean reward:  2.053 [-2.431, 29.175], mean action: 2.263 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   49/5000: episode: 2, duration: 0.275s, episode steps:  30, steps per second: 109, episode reward: 32.506, mean reward:  1.084 [-3.000, 32.053], mean action: 3.800 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  111/5000: episode: 3, duration: 0.744s, episode steps:  62, steps per second:  83, episode reward: 44.248, mean reward:  0.714 [-2.095, 31.390], mean action: 1.419 [1.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  146/5000: episode: 4, duration: 0.520s, episode steps:  35, steps per second:  67, episode reward: 41.876, mean reward:  1.196 [-2.940, 32.200], mean action: 3.600 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  171/5000: episode: 5, duration: 0.274s, episode steps:  25, steps per second:  91, episode reward: 41.305, mean reward:  1.652 [-2.237, 32.390], mean action: 4.080 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  196/5000: episode: 6, duration: 0.201s, episode steps:  25, steps per second: 125, episode reward: 38.903, mean reward:  1.556 [-2.825, 32.573], mean action: 3.080 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  217/5000: episode: 7, duration: 0.172s, episode steps:  21, steps per second: 122, episode reward: 42.000, mean reward:  2.000 [-2.370, 29.594], mean action: 1.095 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  254/5000: episode: 8, duration: 0.641s, episode steps:  37, steps per second:  58, episode reward: 35.341, mean reward:  0.955 [-3.000, 32.180], mean action: 4.486 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  310/5000: episode: 9, duration: 0.590s, episode steps:  56, steps per second:  95, episode reward: 46.846, mean reward:  0.837 [-0.759, 32.417], mean action: 2.536 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  339/5000: episode: 10, duration: 0.252s, episode steps:  29, steps per second: 115, episode reward: 32.275, mean reward:  1.113 [-3.000, 32.030], mean action: 5.379 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  362/5000: episode: 11, duration: 0.254s, episode steps:  23, steps per second:  91, episode reward: 45.000, mean reward:  1.957 [-3.000, 32.280], mean action: 2.348 [1.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  393/5000: episode: 12, duration: 0.282s, episode steps:  31, steps per second: 110, episode reward: 38.091, mean reward:  1.229 [-2.498, 32.140], mean action: 4.161 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  422/5000: episode: 13, duration: 0.362s, episode steps:  29, steps per second:  80, episode reward: 40.746, mean reward:  1.405 [-2.143, 32.042], mean action: 3.483 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  446/5000: episode: 14, duration: 0.336s, episode steps:  24, steps per second:  71, episode reward: 37.689, mean reward:  1.570 [-2.337, 31.746], mean action: 3.375 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  473/5000: episode: 15, duration: 0.339s, episode steps:  27, steps per second:  80, episode reward: 38.681, mean reward:  1.433 [-2.353, 31.761], mean action: 3.481 [1.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  500/5000: episode: 16, duration: 0.201s, episode steps:  27, steps per second: 134, episode reward: 38.224, mean reward:  1.416 [-3.000, 32.049], mean action: 2.778 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  529/5000: episode: 17, duration: 0.215s, episode steps:  29, steps per second: 135, episode reward: 41.281, mean reward:  1.423 [-3.000, 32.365], mean action: 1.483 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  560/5000: episode: 18, duration: 0.214s, episode steps:  31, steps per second: 145, episode reward: 36.000, mean reward:  1.161 [-2.734, 29.140], mean action: 2.613 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  587/5000: episode: 19, duration: 0.176s, episode steps:  27, steps per second: 153, episode reward: 35.007, mean reward:  1.297 [-3.000, 31.832], mean action: 3.000 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  608/5000: episode: 20, duration: 0.146s, episode steps:  21, steps per second: 143, episode reward: 43.431, mean reward:  2.068 [-2.483, 32.144], mean action: 3.048 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  666/5000: episode: 21, duration: 0.377s, episode steps:  58, steps per second: 154, episode reward: 38.829, mean reward:  0.669 [-2.165, 31.999], mean action: 2.638 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  686/5000: episode: 22, duration: 0.143s, episode steps:  20, steps per second: 140, episode reward: 38.024, mean reward:  1.901 [-2.449, 32.362], mean action: 4.850 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  710/5000: episode: 23, duration: 0.165s, episode steps:  24, steps per second: 146, episode reward: 40.352, mean reward:  1.681 [-3.000, 32.030], mean action: 3.042 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  733/5000: episode: 24, duration: 0.154s, episode steps:  23, steps per second: 149, episode reward: 38.319, mean reward:  1.666 [-2.191, 32.512], mean action: 3.391 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  761/5000: episode: 25, duration: 0.181s, episode steps:  28, steps per second: 154, episode reward: 38.772, mean reward:  1.385 [-2.171, 32.470], mean action: 3.036 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  798/5000: episode: 26, duration: 0.238s, episode steps:  37, steps per second: 155, episode reward: 38.708, mean reward:  1.046 [-3.000, 31.896], mean action: 2.649 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  823/5000: episode: 27, duration: 0.169s, episode steps:  25, steps per second: 148, episode reward: 44.494, mean reward:  1.780 [-2.412, 32.260], mean action: 3.520 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  849/5000: episode: 28, duration: 0.175s, episode steps:  26, steps per second: 149, episode reward: 42.000, mean reward:  1.615 [-2.375, 32.110], mean action: 2.808 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  875/5000: episode: 29, duration: 0.176s, episode steps:  26, steps per second: 148, episode reward: 45.000, mean reward:  1.731 [-3.000, 32.080], mean action: 2.846 [2.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  901/5000: episode: 30, duration: 0.180s, episode steps:  26, steps per second: 145, episode reward: 44.076, mean reward:  1.695 [-2.252, 31.535], mean action: 1.269 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  924/5000: episode: 31, duration: 0.152s, episode steps:  23, steps per second: 151, episode reward: 40.656, mean reward:  1.768 [-3.000, 31.797], mean action: 4.478 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  968/5000: episode: 32, duration: 0.275s, episode steps:  44, steps per second: 160, episode reward: -32.990, mean reward: -0.750 [-32.187,  2.578], mean action: 7.500 [1.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1001/5000: episode: 33, duration: 0.231s, episode steps:  33, steps per second: 143, episode reward: 36.000, mean reward:  1.091 [-2.745, 32.220], mean action: 4.697 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1019/5000: episode: 34, duration: 0.279s, episode steps:  18, steps per second:  65, episode reward: 41.422, mean reward:  2.301 [-2.663, 32.470], mean action: 3.056 [0.000, 16.000],  loss: 0.019376, mae: 0.339171, mean_q: 0.601903, mean_eps: 0.000000
 1060/5000: episode: 35, duration: 0.581s, episode steps:  41, steps per second:  71, episode reward: 41.885, mean reward:  1.022 [-2.197, 32.175], mean action: 1.268 [0.000, 12.000],  loss: 0.016981, mae: 0.338292, mean_q: 0.561511, mean_eps: 0.000000
 1079/5000: episode: 36, duration: 0.288s, episode steps:  19, steps per second:  66, episode reward: 38.626, mean reward:  2.033 [-2.878, 32.439], mean action: 4.316 [0.000, 14.000],  loss: 0.024239, mae: 0.375572, mean_q: 0.552138, mean_eps: 0.000000
 1114/5000: episode: 37, duration: 0.494s, episode steps:  35, steps per second:  71, episode reward: 44.128, mean reward:  1.261 [-0.701, 30.260], mean action: 5.086 [0.000, 15.000],  loss: 0.020694, mae: 0.352481, mean_q: 0.556718, mean_eps: 0.000000
 1139/5000: episode: 38, duration: 0.367s, episode steps:  25, steps per second:  68, episode reward: 41.681, mean reward:  1.667 [-2.370, 32.150], mean action: 4.200 [0.000, 16.000],  loss: 0.020950, mae: 0.358050, mean_q: 0.538408, mean_eps: 0.000000
 1158/5000: episode: 39, duration: 0.757s, episode steps:  19, steps per second:  25, episode reward: 44.167, mean reward:  2.325 [-2.296, 32.290], mean action: 2.579 [1.000, 3.000],  loss: 0.023060, mae: 0.377862, mean_q: 0.507368, mean_eps: 0.000000
 1184/5000: episode: 40, duration: 0.741s, episode steps:  26, steps per second:  35, episode reward: 41.963, mean reward:  1.614 [-2.349, 32.090], mean action: 8.115 [0.000, 16.000],  loss: 0.020803, mae: 0.362097, mean_q: 0.574131, mean_eps: 0.000000
 1217/5000: episode: 41, duration: 0.601s, episode steps:  33, steps per second:  55, episode reward: 38.119, mean reward:  1.155 [-2.640, 32.070], mean action: 3.818 [0.000, 16.000],  loss: 0.021322, mae: 0.363305, mean_q: 0.535917, mean_eps: 0.000000
 1238/5000: episode: 42, duration: 0.558s, episode steps:  21, steps per second:  38, episode reward: 44.502, mean reward:  2.119 [-2.425, 32.392], mean action: 2.714 [0.000, 15.000],  loss: 0.021548, mae: 0.362826, mean_q: 0.509712, mean_eps: 0.000000
 1261/5000: episode: 43, duration: 0.430s, episode steps:  23, steps per second:  54, episode reward: 41.797, mean reward:  1.817 [-2.668, 32.097], mean action: 3.217 [0.000, 15.000],  loss: 0.020846, mae: 0.354293, mean_q: 0.507802, mean_eps: 0.000000
 1277/5000: episode: 44, duration: 0.308s, episode steps:  16, steps per second:  52, episode reward: 42.000, mean reward:  2.625 [-2.613, 30.293], mean action: 2.125 [0.000, 12.000],  loss: 0.020277, mae: 0.356164, mean_q: 0.550709, mean_eps: 0.000000
 1300/5000: episode: 45, duration: 0.422s, episode steps:  23, steps per second:  55, episode reward: 41.903, mean reward:  1.822 [-2.570, 32.230], mean action: 4.652 [0.000, 19.000],  loss: 0.022659, mae: 0.371261, mean_q: 0.541343, mean_eps: 0.000000
 1324/5000: episode: 46, duration: 0.424s, episode steps:  24, steps per second:  57, episode reward: 35.489, mean reward:  1.479 [-3.000, 32.160], mean action: 3.042 [0.000, 19.000],  loss: 0.017223, mae: 0.344546, mean_q: 0.529355, mean_eps: 0.000000
 1353/5000: episode: 47, duration: 0.649s, episode steps:  29, steps per second:  45, episode reward: 38.803, mean reward:  1.338 [-2.900, 32.200], mean action: 4.172 [0.000, 19.000],  loss: 0.019722, mae: 0.359330, mean_q: 0.521888, mean_eps: 0.000000
 1404/5000: episode: 48, duration: 0.818s, episode steps:  51, steps per second:  62, episode reward: 38.280, mean reward:  0.751 [-2.491, 31.897], mean action: 1.922 [0.000, 15.000],  loss: 0.020995, mae: 0.363410, mean_q: 0.503621, mean_eps: 0.000000
 1437/5000: episode: 49, duration: 0.850s, episode steps:  33, steps per second:  39, episode reward: 35.491, mean reward:  1.075 [-2.544, 31.951], mean action: 2.576 [0.000, 12.000],  loss: 0.020100, mae: 0.350869, mean_q: 0.525645, mean_eps: 0.000000
 1457/5000: episode: 50, duration: 0.383s, episode steps:  20, steps per second:  52, episode reward: 41.543, mean reward:  2.077 [-2.740, 32.241], mean action: 1.650 [0.000, 12.000],  loss: 0.025272, mae: 0.383045, mean_q: 0.539488, mean_eps: 0.000000
 1485/5000: episode: 51, duration: 0.695s, episode steps:  28, steps per second:  40, episode reward: 35.080, mean reward:  1.253 [-3.000, 32.140], mean action: 3.286 [0.000, 15.000],  loss: 0.021946, mae: 0.364133, mean_q: 0.612685, mean_eps: 0.000000
 1510/5000: episode: 52, duration: 0.570s, episode steps:  25, steps per second:  44, episode reward: 38.392, mean reward:  1.536 [-2.275, 31.532], mean action: 2.680 [0.000, 12.000],  loss: 0.023743, mae: 0.374435, mean_q: 0.632938, mean_eps: 0.000000
 1539/5000: episode: 53, duration: 0.704s, episode steps:  29, steps per second:  41, episode reward: 38.217, mean reward:  1.318 [-2.617, 32.380], mean action: 3.241 [0.000, 21.000],  loss: 0.022098, mae: 0.371365, mean_q: 0.528140, mean_eps: 0.000000
 1561/5000: episode: 54, duration: 0.474s, episode steps:  22, steps per second:  46, episode reward: 38.350, mean reward:  1.743 [-3.000, 32.017], mean action: 3.864 [0.000, 20.000],  loss: 0.019104, mae: 0.351053, mean_q: 0.519012, mean_eps: 0.000000
 1584/5000: episode: 55, duration: 0.412s, episode steps:  23, steps per second:  56, episode reward: 41.614, mean reward:  1.809 [-2.245, 31.734], mean action: 1.913 [0.000, 12.000],  loss: 0.019546, mae: 0.349965, mean_q: 0.526368, mean_eps: 0.000000
 1602/5000: episode: 56, duration: 0.325s, episode steps:  18, steps per second:  55, episode reward: 47.042, mean reward:  2.613 [-0.030, 32.181], mean action: 4.611 [0.000, 14.000],  loss: 0.023858, mae: 0.366290, mean_q: 0.487816, mean_eps: 0.000000
 1637/5000: episode: 57, duration: 0.601s, episode steps:  35, steps per second:  58, episode reward: 32.899, mean reward:  0.940 [-3.000, 32.478], mean action: 4.600 [0.000, 19.000],  loss: 0.018150, mae: 0.347077, mean_q: 0.507286, mean_eps: 0.000000
 1672/5000: episode: 58, duration: 0.608s, episode steps:  35, steps per second:  58, episode reward: 36.000, mean reward:  1.029 [-2.817, 32.200], mean action: 5.086 [0.000, 19.000],  loss: 0.019401, mae: 0.350149, mean_q: 0.521775, mean_eps: 0.000000
 1722/5000: episode: 59, duration: 1.119s, episode steps:  50, steps per second:  45, episode reward: 34.967, mean reward:  0.699 [-2.739, 32.306], mean action: 3.420 [0.000, 19.000],  loss: 0.023429, mae: 0.358381, mean_q: 0.593316, mean_eps: 0.000000
 1747/5000: episode: 60, duration: 0.419s, episode steps:  25, steps per second:  60, episode reward: 44.694, mean reward:  1.788 [-3.000, 32.010], mean action: 3.160 [1.000, 19.000],  loss: 0.018885, mae: 0.341103, mean_q: 0.599410, mean_eps: 0.000000
 1780/5000: episode: 61, duration: 0.558s, episode steps:  33, steps per second:  59, episode reward: 35.368, mean reward:  1.072 [-3.000, 32.110], mean action: 4.970 [0.000, 19.000],  loss: 0.018330, mae: 0.341004, mean_q: 0.514776, mean_eps: 0.000000
 1801/5000: episode: 62, duration: 0.468s, episode steps:  21, steps per second:  45, episode reward: 40.873, mean reward:  1.946 [-2.494, 32.346], mean action: 4.000 [0.000, 19.000],  loss: 0.020144, mae: 0.347202, mean_q: 0.560968, mean_eps: 0.000000
 1831/5000: episode: 63, duration: 0.893s, episode steps:  30, steps per second:  34, episode reward: 41.159, mean reward:  1.372 [-2.339, 32.180], mean action: 2.300 [0.000, 19.000],  loss: 0.020356, mae: 0.349183, mean_q: 0.576235, mean_eps: 0.000000
 1854/5000: episode: 64, duration: 0.535s, episode steps:  23, steps per second:  43, episode reward: 44.175, mean reward:  1.921 [-2.333, 32.080], mean action: 1.652 [0.000, 19.000],  loss: 0.018460, mae: 0.337753, mean_q: 0.496654, mean_eps: 0.000000
 1887/5000: episode: 65, duration: 0.786s, episode steps:  33, steps per second:  42, episode reward: 35.697, mean reward:  1.082 [-2.591, 32.020], mean action: 4.303 [0.000, 19.000],  loss: 0.018304, mae: 0.338580, mean_q: 0.496199, mean_eps: 0.000000
 1910/5000: episode: 66, duration: 0.528s, episode steps:  23, steps per second:  44, episode reward: 38.544, mean reward:  1.676 [-2.888, 32.030], mean action: 3.870 [0.000, 19.000],  loss: 0.019244, mae: 0.347248, mean_q: 0.532384, mean_eps: 0.000000
 1929/5000: episode: 67, duration: 0.638s, episode steps:  19, steps per second:  30, episode reward: 44.271, mean reward:  2.330 [-2.051, 32.190], mean action: 4.053 [0.000, 15.000],  loss: 0.024642, mae: 0.368236, mean_q: 0.518565, mean_eps: 0.000000
 1955/5000: episode: 68, duration: 0.537s, episode steps:  26, steps per second:  48, episode reward: 39.000, mean reward:  1.500 [-2.476, 30.017], mean action: 3.692 [1.000, 15.000],  loss: 0.021752, mae: 0.353529, mean_q: 0.521191, mean_eps: 0.000000
 1976/5000: episode: 69, duration: 0.382s, episode steps:  21, steps per second:  55, episode reward: 41.304, mean reward:  1.967 [-3.000, 32.023], mean action: 4.095 [0.000, 15.000],  loss: 0.019961, mae: 0.347448, mean_q: 0.532465, mean_eps: 0.000000
 2003/5000: episode: 70, duration: 0.468s, episode steps:  27, steps per second:  58, episode reward: 38.901, mean reward:  1.441 [-2.857, 32.601], mean action: 3.852 [0.000, 15.000],  loss: 0.024950, mae: 0.374220, mean_q: 0.519954, mean_eps: 0.000000
 2029/5000: episode: 71, duration: 0.479s, episode steps:  26, steps per second:  54, episode reward: 39.000, mean reward:  1.500 [-2.762, 32.250], mean action: 2.885 [0.000, 15.000],  loss: 0.020050, mae: 0.356832, mean_q: 0.485264, mean_eps: 0.000000
 2046/5000: episode: 72, duration: 0.326s, episode steps:  17, steps per second:  52, episode reward: 41.636, mean reward:  2.449 [-2.191, 32.240], mean action: 4.647 [0.000, 15.000],  loss: 0.017441, mae: 0.346394, mean_q: 0.479384, mean_eps: 0.000000
 2076/5000: episode: 73, duration: 0.735s, episode steps:  30, steps per second:  41, episode reward: 35.571, mean reward:  1.186 [-2.703, 32.169], mean action: 6.900 [0.000, 15.000],  loss: 0.018972, mae: 0.350618, mean_q: 0.469824, mean_eps: 0.000000
 2115/5000: episode: 74, duration: 1.031s, episode steps:  39, steps per second:  38, episode reward: 32.621, mean reward:  0.836 [-3.000, 32.424], mean action: 3.333 [0.000, 16.000],  loss: 0.019032, mae: 0.344877, mean_q: 0.508986, mean_eps: 0.000000
 2133/5000: episode: 75, duration: 0.621s, episode steps:  18, steps per second:  29, episode reward: 43.434, mean reward:  2.413 [-2.115, 31.834], mean action: 1.500 [0.000, 4.000],  loss: 0.015159, mae: 0.331056, mean_q: 0.482636, mean_eps: 0.000000
 2165/5000: episode: 76, duration: 0.547s, episode steps:  32, steps per second:  59, episode reward: 38.592, mean reward:  1.206 [-2.571, 32.457], mean action: 5.406 [0.000, 21.000],  loss: 0.020445, mae: 0.348741, mean_q: 0.537129, mean_eps: 0.000000
 2187/5000: episode: 77, duration: 0.614s, episode steps:  22, steps per second:  36, episode reward: 41.805, mean reward:  1.900 [-2.323, 32.040], mean action: 2.455 [0.000, 16.000],  loss: 0.019153, mae: 0.349559, mean_q: 0.519051, mean_eps: 0.000000
 2227/5000: episode: 78, duration: 1.311s, episode steps:  40, steps per second:  31, episode reward: 41.121, mean reward:  1.028 [-2.145, 32.040], mean action: 1.625 [0.000, 20.000],  loss: 0.020015, mae: 0.345049, mean_q: 0.541692, mean_eps: 0.000000
 2255/5000: episode: 79, duration: 0.665s, episode steps:  28, steps per second:  42, episode reward: 32.901, mean reward:  1.175 [-2.609, 32.071], mean action: 4.250 [0.000, 16.000],  loss: 0.020402, mae: 0.352356, mean_q: 0.549694, mean_eps: 0.000000
 2281/5000: episode: 80, duration: 0.753s, episode steps:  26, steps per second:  35, episode reward: 47.242, mean reward:  1.817 [-0.188, 31.423], mean action: 1.846 [0.000, 3.000],  loss: 0.018780, mae: 0.342785, mean_q: 0.531661, mean_eps: 0.000000
 2305/5000: episode: 81, duration: 0.390s, episode steps:  24, steps per second:  62, episode reward: 40.851, mean reward:  1.702 [-2.077, 31.912], mean action: 4.333 [0.000, 14.000],  loss: 0.018696, mae: 0.340411, mean_q: 0.554180, mean_eps: 0.000000
 2325/5000: episode: 82, duration: 0.401s, episode steps:  20, steps per second:  50, episode reward: 44.477, mean reward:  2.224 [-2.338, 31.575], mean action: 1.800 [0.000, 3.000],  loss: 0.020924, mae: 0.351293, mean_q: 0.513814, mean_eps: 0.000000
 2348/5000: episode: 83, duration: 0.368s, episode steps:  23, steps per second:  62, episode reward: 41.495, mean reward:  1.804 [-3.000, 31.876], mean action: 2.913 [0.000, 19.000],  loss: 0.017980, mae: 0.341980, mean_q: 0.561452, mean_eps: 0.000000
 2373/5000: episode: 84, duration: 0.813s, episode steps:  25, steps per second:  31, episode reward: 46.926, mean reward:  1.877 [-0.228, 32.390], mean action: 1.840 [0.000, 13.000],  loss: 0.019025, mae: 0.340715, mean_q: 0.542389, mean_eps: 0.000000
 2390/5000: episode: 85, duration: 0.450s, episode steps:  17, steps per second:  38, episode reward: 43.421, mean reward:  2.554 [-2.903, 32.720], mean action: 5.588 [0.000, 15.000],  loss: 0.024758, mae: 0.367675, mean_q: 0.490782, mean_eps: 0.000000
 2427/5000: episode: 86, duration: 0.617s, episode steps:  37, steps per second:  60, episode reward: 38.566, mean reward:  1.042 [-2.538, 32.044], mean action: 4.432 [0.000, 21.000],  loss: 0.020717, mae: 0.351786, mean_q: 0.531089, mean_eps: 0.000000
 2453/5000: episode: 87, duration: 0.846s, episode steps:  26, steps per second:  31, episode reward: 43.238, mean reward:  1.663 [-2.329, 32.280], mean action: 3.615 [0.000, 15.000],  loss: 0.022392, mae: 0.356442, mean_q: 0.530531, mean_eps: 0.000000
 2471/5000: episode: 88, duration: 0.639s, episode steps:  18, steps per second:  28, episode reward: 44.671, mean reward:  2.482 [-2.122, 32.050], mean action: 3.000 [0.000, 15.000],  loss: 0.020943, mae: 0.343055, mean_q: 0.529378, mean_eps: 0.000000
 2513/5000: episode: 89, duration: 0.620s, episode steps:  42, steps per second:  68, episode reward: 37.922, mean reward:  0.903 [-2.381, 31.707], mean action: 3.000 [0.000, 15.000],  loss: 0.022338, mae: 0.360007, mean_q: 0.581896, mean_eps: 0.000000
 2542/5000: episode: 90, duration: 0.424s, episode steps:  29, steps per second:  68, episode reward: 44.054, mean reward:  1.519 [-2.008, 32.100], mean action: 3.483 [0.000, 15.000],  loss: 0.018490, mae: 0.348507, mean_q: 0.577577, mean_eps: 0.000000
 2573/5000: episode: 91, duration: 0.451s, episode steps:  31, steps per second:  69, episode reward: 38.207, mean reward:  1.232 [-2.448, 32.853], mean action: 5.581 [0.000, 21.000],  loss: 0.017760, mae: 0.344169, mean_q: 0.560847, mean_eps: 0.000000
 2596/5000: episode: 92, duration: 0.347s, episode steps:  23, steps per second:  66, episode reward: 46.109, mean reward:  2.005 [-0.411, 32.150], mean action: 5.565 [0.000, 13.000],  loss: 0.021791, mae: 0.369103, mean_q: 0.584139, mean_eps: 0.000000
 2621/5000: episode: 93, duration: 0.359s, episode steps:  25, steps per second:  70, episode reward: 41.035, mean reward:  1.641 [-2.241, 32.390], mean action: 3.720 [0.000, 19.000],  loss: 0.020216, mae: 0.366406, mean_q: 0.544785, mean_eps: 0.000000
 2651/5000: episode: 94, duration: 0.442s, episode steps:  30, steps per second:  68, episode reward: 40.045, mean reward:  1.335 [-2.066, 31.902], mean action: 6.367 [0.000, 19.000],  loss: 0.020024, mae: 0.363613, mean_q: 0.546973, mean_eps: 0.000000
 2670/5000: episode: 95, duration: 0.286s, episode steps:  19, steps per second:  66, episode reward: 41.698, mean reward:  2.195 [-2.518, 33.000], mean action: 2.947 [0.000, 19.000],  loss: 0.022333, mae: 0.372983, mean_q: 0.512452, mean_eps: 0.000000
 2689/5000: episode: 96, duration: 0.283s, episode steps:  19, steps per second:  67, episode reward: 47.472, mean reward:  2.499 [-0.234, 31.872], mean action: 4.368 [1.000, 19.000],  loss: 0.023665, mae: 0.380753, mean_q: 0.507609, mean_eps: 0.000000
 2726/5000: episode: 97, duration: 0.913s, episode steps:  37, steps per second:  41, episode reward: 42.000, mean reward:  1.135 [-2.566, 32.060], mean action: 4.000 [0.000, 19.000],  loss: 0.021696, mae: 0.362441, mean_q: 0.505450, mean_eps: 0.000000
 2755/5000: episode: 98, duration: 0.765s, episode steps:  29, steps per second:  38, episode reward: 38.853, mean reward:  1.340 [-2.407, 32.330], mean action: 3.897 [0.000, 15.000],  loss: 0.020878, mae: 0.356607, mean_q: 0.554563, mean_eps: 0.000000
 2776/5000: episode: 99, duration: 0.523s, episode steps:  21, steps per second:  40, episode reward: 44.002, mean reward:  2.095 [-2.176, 32.120], mean action: 3.524 [0.000, 15.000],  loss: 0.020594, mae: 0.357107, mean_q: 0.496184, mean_eps: 0.000000
 2797/5000: episode: 100, duration: 0.412s, episode steps:  21, steps per second:  51, episode reward: 41.351, mean reward:  1.969 [-2.636, 31.980], mean action: 1.952 [0.000, 15.000],  loss: 0.021918, mae: 0.370953, mean_q: 0.529359, mean_eps: 0.000000
 2814/5000: episode: 101, duration: 0.810s, episode steps:  17, steps per second:  21, episode reward: 44.785, mean reward:  2.634 [-2.043, 32.902], mean action: 2.471 [0.000, 15.000],  loss: 0.023252, mae: 0.382892, mean_q: 0.538413, mean_eps: 0.000000
 2854/5000: episode: 102, duration: 0.706s, episode steps:  40, steps per second:  57, episode reward: 38.275, mean reward:  0.957 [-3.000, 32.090], mean action: 3.850 [0.000, 16.000],  loss: 0.021387, mae: 0.371430, mean_q: 0.502266, mean_eps: 0.000000
 2879/5000: episode: 103, duration: 0.382s, episode steps:  25, steps per second:  66, episode reward: 42.000, mean reward:  1.680 [-2.234, 32.190], mean action: 1.600 [0.000, 16.000],  loss: 0.018935, mae: 0.358447, mean_q: 0.496880, mean_eps: 0.000000
 2897/5000: episode: 104, duration: 0.279s, episode steps:  18, steps per second:  65, episode reward: 38.467, mean reward:  2.137 [-2.412, 32.660], mean action: 2.556 [0.000, 16.000],  loss: 0.017123, mae: 0.344786, mean_q: 0.493065, mean_eps: 0.000000
 2915/5000: episode: 105, duration: 0.486s, episode steps:  18, steps per second:  37, episode reward: 46.809, mean reward:  2.601 [-0.207, 32.323], mean action: 2.389 [0.000, 12.000],  loss: 0.022345, mae: 0.373893, mean_q: 0.521311, mean_eps: 0.000000
 2933/5000: episode: 106, duration: 0.595s, episode steps:  18, steps per second:  30, episode reward: 41.164, mean reward:  2.287 [-2.429, 32.090], mean action: 3.056 [1.000, 11.000],  loss: 0.020176, mae: 0.365663, mean_q: 0.526509, mean_eps: 0.000000
 2983/5000: episode: 107, duration: 1.362s, episode steps:  50, steps per second:  37, episode reward: 41.319, mean reward:  0.826 [-2.877, 32.660], mean action: 2.580 [0.000, 19.000],  loss: 0.021478, mae: 0.363882, mean_q: 0.508653, mean_eps: 0.000000
 3011/5000: episode: 108, duration: 0.574s, episode steps:  28, steps per second:  49, episode reward: 36.000, mean reward:  1.286 [-3.000, 32.350], mean action: 3.857 [0.000, 19.000],  loss: 0.018956, mae: 0.336884, mean_q: 0.497939, mean_eps: 0.000000
 3035/5000: episode: 109, duration: 0.834s, episode steps:  24, steps per second:  29, episode reward: 41.771, mean reward:  1.740 [-2.813, 31.984], mean action: 2.292 [1.000, 19.000],  loss: 0.020187, mae: 0.348040, mean_q: 0.506763, mean_eps: 0.000000
 3074/5000: episode: 110, duration: 0.583s, episode steps:  39, steps per second:  67, episode reward: 36.000, mean reward:  0.923 [-2.708, 32.240], mean action: 2.897 [0.000, 19.000],  loss: 0.020581, mae: 0.342053, mean_q: 0.543954, mean_eps: 0.000000
 3102/5000: episode: 111, duration: 0.394s, episode steps:  28, steps per second:  71, episode reward: 39.000, mean reward:  1.393 [-2.442, 32.230], mean action: 3.821 [0.000, 16.000],  loss: 0.020169, mae: 0.344711, mean_q: 0.588383, mean_eps: 0.000000
 3126/5000: episode: 112, duration: 0.449s, episode steps:  24, steps per second:  53, episode reward: 41.348, mean reward:  1.723 [-3.000, 32.241], mean action: 2.250 [0.000, 11.000],  loss: 0.017235, mae: 0.334556, mean_q: 0.580006, mean_eps: 0.000000
 3158/5000: episode: 113, duration: 0.612s, episode steps:  32, steps per second:  52, episode reward: 37.266, mean reward:  1.165 [-2.465, 32.150], mean action: 3.750 [0.000, 16.000],  loss: 0.018164, mae: 0.345823, mean_q: 0.533283, mean_eps: 0.000000
 3193/5000: episode: 114, duration: 1.144s, episode steps:  35, steps per second:  31, episode reward: 41.817, mean reward:  1.195 [-3.000, 32.220], mean action: 3.257 [0.000, 12.000],  loss: 0.022322, mae: 0.361156, mean_q: 0.575523, mean_eps: 0.000000
 3219/5000: episode: 115, duration: 0.980s, episode steps:  26, steps per second:  27, episode reward: 32.903, mean reward:  1.265 [-2.585, 32.003], mean action: 3.269 [0.000, 12.000],  loss: 0.018910, mae: 0.345219, mean_q: 0.601402, mean_eps: 0.000000
 3234/5000: episode: 116, duration: 0.363s, episode steps:  15, steps per second:  41, episode reward: 45.000, mean reward:  3.000 [-2.138, 32.720], mean action: 2.400 [1.000, 12.000],  loss: 0.020219, mae: 0.352969, mean_q: 0.526887, mean_eps: 0.000000
 3262/5000: episode: 117, duration: 0.571s, episode steps:  28, steps per second:  49, episode reward: 44.765, mean reward:  1.599 [-3.000, 32.030], mean action: 1.571 [0.000, 19.000],  loss: 0.019903, mae: 0.354463, mean_q: 0.523538, mean_eps: 0.000000
 3290/5000: episode: 118, duration: 0.908s, episode steps:  28, steps per second:  31, episode reward: 35.632, mean reward:  1.273 [-2.709, 32.210], mean action: 4.393 [0.000, 18.000],  loss: 0.019691, mae: 0.350884, mean_q: 0.505537, mean_eps: 0.000000
 3318/5000: episode: 119, duration: 0.530s, episode steps:  28, steps per second:  53, episode reward: 44.741, mean reward:  1.598 [-2.766, 32.250], mean action: 4.000 [2.000, 19.000],  loss: 0.018714, mae: 0.339560, mean_q: 0.506031, mean_eps: 0.000000
 3341/5000: episode: 120, duration: 0.400s, episode steps:  23, steps per second:  57, episode reward: 41.035, mean reward:  1.784 [-2.809, 32.530], mean action: 2.522 [0.000, 19.000],  loss: 0.020330, mae: 0.349998, mean_q: 0.508941, mean_eps: 0.000000
 3380/5000: episode: 121, duration: 0.692s, episode steps:  39, steps per second:  56, episode reward: 38.533, mean reward:  0.988 [-2.965, 32.180], mean action: 6.744 [0.000, 19.000],  loss: 0.020311, mae: 0.346252, mean_q: 0.525849, mean_eps: 0.000000
 3398/5000: episode: 122, duration: 0.394s, episode steps:  18, steps per second:  46, episode reward: 44.626, mean reward:  2.479 [-2.220, 32.150], mean action: 2.556 [0.000, 16.000],  loss: 0.019117, mae: 0.332731, mean_q: 0.512644, mean_eps: 0.000000
 3429/5000: episode: 123, duration: 0.701s, episode steps:  31, steps per second:  44, episode reward: 38.802, mean reward:  1.252 [-2.561, 32.130], mean action: 2.903 [0.000, 19.000],  loss: 0.021702, mae: 0.351448, mean_q: 0.474637, mean_eps: 0.000000
 3455/5000: episode: 124, duration: 0.408s, episode steps:  26, steps per second:  64, episode reward: 38.748, mean reward:  1.490 [-2.821, 32.550], mean action: 3.346 [0.000, 19.000],  loss: 0.022699, mae: 0.351468, mean_q: 0.463429, mean_eps: 0.000000
 3483/5000: episode: 125, duration: 0.424s, episode steps:  28, steps per second:  66, episode reward: 44.507, mean reward:  1.590 [-2.397, 31.983], mean action: 3.179 [0.000, 19.000],  loss: 0.018377, mae: 0.334183, mean_q: 0.547184, mean_eps: 0.000000
 3514/5000: episode: 126, duration: 0.485s, episode steps:  31, steps per second:  64, episode reward: 41.303, mean reward:  1.332 [-2.496, 31.836], mean action: 2.290 [0.000, 19.000],  loss: 0.017860, mae: 0.335581, mean_q: 0.542775, mean_eps: 0.000000
 3526/5000: episode: 127, duration: 0.193s, episode steps:  12, steps per second:  62, episode reward: 45.000, mean reward:  3.750 [-2.375, 32.300], mean action: 1.083 [0.000, 9.000],  loss: 0.024045, mae: 0.364884, mean_q: 0.486954, mean_eps: 0.000000
 3549/5000: episode: 128, duration: 0.340s, episode steps:  23, steps per second:  68, episode reward: 41.846, mean reward:  1.819 [-2.247, 32.250], mean action: 4.087 [1.000, 19.000],  loss: 0.018871, mae: 0.353484, mean_q: 0.409423, mean_eps: 0.000000
 3580/5000: episode: 129, duration: 0.432s, episode steps:  31, steps per second:  72, episode reward: 37.898, mean reward:  1.223 [-3.000, 32.080], mean action: 5.226 [0.000, 19.000],  loss: 0.020412, mae: 0.351709, mean_q: 0.460045, mean_eps: 0.000000
 3594/5000: episode: 130, duration: 0.217s, episode steps:  14, steps per second:  65, episode reward: 41.860, mean reward:  2.990 [-2.510, 33.000], mean action: 5.500 [0.000, 15.000],  loss: 0.022610, mae: 0.358336, mean_q: 0.473706, mean_eps: 0.000000
 3626/5000: episode: 131, duration: 0.476s, episode steps:  32, steps per second:  67, episode reward: 32.199, mean reward:  1.006 [-2.598, 31.506], mean action: 6.312 [0.000, 18.000],  loss: 0.020646, mae: 0.349341, mean_q: 0.506803, mean_eps: 0.000000
 3653/5000: episode: 132, duration: 0.445s, episode steps:  27, steps per second:  61, episode reward: 34.964, mean reward:  1.295 [-3.000, 31.553], mean action: 5.444 [0.000, 16.000],  loss: 0.021444, mae: 0.351734, mean_q: 0.533870, mean_eps: 0.000000
 3687/5000: episode: 133, duration: 0.572s, episode steps:  34, steps per second:  59, episode reward: 32.776, mean reward:  0.964 [-2.442, 31.920], mean action: 3.500 [0.000, 16.000],  loss: 0.020034, mae: 0.352660, mean_q: 0.488478, mean_eps: 0.000000
 3704/5000: episode: 134, duration: 0.304s, episode steps:  17, steps per second:  56, episode reward: 44.503, mean reward:  2.618 [-2.726, 32.480], mean action: 4.176 [1.000, 16.000],  loss: 0.018481, mae: 0.354589, mean_q: 0.375840, mean_eps: 0.000000
 3748/5000: episode: 135, duration: 0.634s, episode steps:  44, steps per second:  69, episode reward: 32.132, mean reward:  0.730 [-2.683, 31.642], mean action: 5.159 [0.000, 16.000],  loss: 0.021274, mae: 0.355471, mean_q: 0.450250, mean_eps: 0.000000
 3794/5000: episode: 136, duration: 0.654s, episode steps:  46, steps per second:  70, episode reward: 35.354, mean reward:  0.769 [-2.416, 31.434], mean action: 1.587 [0.000, 12.000],  loss: 0.017810, mae: 0.335959, mean_q: 0.477386, mean_eps: 0.000000
 3819/5000: episode: 137, duration: 0.373s, episode steps:  25, steps per second:  67, episode reward: 36.000, mean reward:  1.440 [-2.398, 32.300], mean action: 2.800 [0.000, 15.000],  loss: 0.022651, mae: 0.358234, mean_q: 0.491986, mean_eps: 0.000000
 3845/5000: episode: 138, duration: 0.389s, episode steps:  26, steps per second:  67, episode reward: 38.391, mean reward:  1.477 [-2.452, 31.844], mean action: 3.077 [0.000, 16.000],  loss: 0.023916, mae: 0.373437, mean_q: 0.484401, mean_eps: 0.000000
 3867/5000: episode: 139, duration: 0.317s, episode steps:  22, steps per second:  70, episode reward: 37.542, mean reward:  1.706 [-2.903, 32.901], mean action: 4.364 [0.000, 15.000],  loss: 0.022303, mae: 0.381887, mean_q: 0.512169, mean_eps: 0.000000
 3888/5000: episode: 140, duration: 0.331s, episode steps:  21, steps per second:  63, episode reward: 43.582, mean reward:  2.075 [-2.375, 32.332], mean action: 3.571 [0.000, 14.000],  loss: 0.023186, mae: 0.373113, mean_q: 0.477270, mean_eps: 0.000000
 3922/5000: episode: 141, duration: 0.525s, episode steps:  34, steps per second:  65, episode reward: 41.170, mean reward:  1.211 [-2.416, 32.170], mean action: 2.265 [0.000, 12.000],  loss: 0.017721, mae: 0.341673, mean_q: 0.503106, mean_eps: 0.000000
 3947/5000: episode: 142, duration: 0.352s, episode steps:  25, steps per second:  71, episode reward: 41.036, mean reward:  1.641 [-2.616, 32.160], mean action: 2.240 [0.000, 9.000],  loss: 0.018948, mae: 0.344878, mean_q: 0.531969, mean_eps: 0.000000
 3972/5000: episode: 143, duration: 0.374s, episode steps:  25, steps per second:  67, episode reward: 40.824, mean reward:  1.633 [-2.798, 32.020], mean action: 2.720 [0.000, 13.000],  loss: 0.021398, mae: 0.351053, mean_q: 0.544320, mean_eps: 0.000000
 3989/5000: episode: 144, duration: 0.251s, episode steps:  17, steps per second:  68, episode reward: 44.254, mean reward:  2.603 [-2.004, 32.670], mean action: 2.588 [2.000, 3.000],  loss: 0.018416, mae: 0.337858, mean_q: 0.637091, mean_eps: 0.000000
 4019/5000: episode: 145, duration: 0.419s, episode steps:  30, steps per second:  72, episode reward: 40.914, mean reward:  1.364 [-2.951, 31.896], mean action: 3.833 [0.000, 15.000],  loss: 0.020492, mae: 0.351552, mean_q: 0.573723, mean_eps: 0.000000
 4058/5000: episode: 146, duration: 0.550s, episode steps:  39, steps per second:  71, episode reward: -32.560, mean reward: -0.835 [-32.207,  2.404], mean action: 8.026 [0.000, 18.000],  loss: 0.019016, mae: 0.336875, mean_q: 0.527487, mean_eps: 0.000000
 4080/5000: episode: 147, duration: 0.331s, episode steps:  22, steps per second:  66, episode reward: 38.786, mean reward:  1.763 [-2.400, 31.946], mean action: 2.682 [0.000, 19.000],  loss: 0.018796, mae: 0.335890, mean_q: 0.497491, mean_eps: 0.000000
 4119/5000: episode: 148, duration: 0.552s, episode steps:  39, steps per second:  71, episode reward: 37.902, mean reward:  0.972 [-2.532, 32.070], mean action: 1.436 [0.000, 9.000],  loss: 0.019669, mae: 0.335511, mean_q: 0.492571, mean_eps: 0.000000
 4139/5000: episode: 149, duration: 0.297s, episode steps:  20, steps per second:  67, episode reward: 42.595, mean reward:  2.130 [-2.088, 32.060], mean action: 4.850 [0.000, 13.000],  loss: 0.019694, mae: 0.348018, mean_q: 0.485733, mean_eps: 0.000000
 4165/5000: episode: 150, duration: 0.366s, episode steps:  26, steps per second:  71, episode reward: 38.043, mean reward:  1.463 [-2.711, 32.310], mean action: 4.154 [0.000, 12.000],  loss: 0.018669, mae: 0.343496, mean_q: 0.449722, mean_eps: 0.000000
 4205/5000: episode: 151, duration: 0.571s, episode steps:  40, steps per second:  70, episode reward: 41.202, mean reward:  1.030 [-2.105, 32.701], mean action: 2.000 [0.000, 19.000],  loss: 0.021816, mae: 0.350526, mean_q: 0.494239, mean_eps: 0.000000
 4228/5000: episode: 152, duration: 0.339s, episode steps:  23, steps per second:  68, episode reward: 44.350, mean reward:  1.928 [-2.399, 32.103], mean action: 4.870 [1.000, 20.000],  loss: 0.021212, mae: 0.342144, mean_q: 0.543440, mean_eps: 0.000000
 4245/5000: episode: 153, duration: 0.253s, episode steps:  17, steps per second:  67, episode reward: 44.103, mean reward:  2.594 [-2.453, 32.210], mean action: 2.882 [0.000, 8.000],  loss: 0.021026, mae: 0.343464, mean_q: 0.507800, mean_eps: 0.000000
 4280/5000: episode: 154, duration: 0.485s, episode steps:  35, steps per second:  72, episode reward: 40.860, mean reward:  1.167 [-2.190, 32.313], mean action: 2.400 [0.000, 19.000],  loss: 0.022762, mae: 0.355938, mean_q: 0.508093, mean_eps: 0.000000
 4311/5000: episode: 155, duration: 0.454s, episode steps:  31, steps per second:  68, episode reward: 44.630, mean reward:  1.440 [-2.307, 32.090], mean action: 2.387 [0.000, 19.000],  loss: 0.021372, mae: 0.352373, mean_q: 0.542016, mean_eps: 0.000000
 4334/5000: episode: 156, duration: 0.352s, episode steps:  23, steps per second:  65, episode reward: 42.000, mean reward:  1.826 [-2.416, 32.410], mean action: 4.261 [3.000, 19.000],  loss: 0.016215, mae: 0.326839, mean_q: 0.532176, mean_eps: 0.000000
 4358/5000: episode: 157, duration: 0.348s, episode steps:  24, steps per second:  69, episode reward: 38.768, mean reward:  1.615 [-2.313, 32.130], mean action: 3.042 [0.000, 11.000],  loss: 0.024003, mae: 0.364801, mean_q: 0.539719, mean_eps: 0.000000
 4389/5000: episode: 158, duration: 0.440s, episode steps:  31, steps per second:  70, episode reward: 46.929, mean reward:  1.514 [-0.252, 32.073], mean action: 2.065 [0.000, 11.000],  loss: 0.022352, mae: 0.355056, mean_q: 0.492213, mean_eps: 0.000000
 4411/5000: episode: 159, duration: 0.340s, episode steps:  22, steps per second:  65, episode reward: 44.215, mean reward:  2.010 [-2.007, 32.110], mean action: 3.773 [0.000, 14.000],  loss: 0.023193, mae: 0.347615, mean_q: 0.529775, mean_eps: 0.000000
 4453/5000: episode: 160, duration: 0.591s, episode steps:  42, steps per second:  71, episode reward: 41.260, mean reward:  0.982 [-3.000, 32.410], mean action: 4.143 [0.000, 14.000],  loss: 0.020736, mae: 0.338652, mean_q: 0.471498, mean_eps: 0.000000
 4478/5000: episode: 161, duration: 0.359s, episode steps:  25, steps per second:  70, episode reward: 41.858, mean reward:  1.674 [-2.093, 32.588], mean action: 2.720 [0.000, 12.000],  loss: 0.023139, mae: 0.351541, mean_q: 0.510857, mean_eps: 0.000000
 4505/5000: episode: 162, duration: 0.403s, episode steps:  27, steps per second:  67, episode reward: 38.739, mean reward:  1.435 [-2.754, 32.550], mean action: 2.889 [0.000, 20.000],  loss: 0.025560, mae: 0.359398, mean_q: 0.536390, mean_eps: 0.000000
 4531/5000: episode: 163, duration: 0.371s, episode steps:  26, steps per second:  70, episode reward: 44.587, mean reward:  1.715 [-2.076, 32.770], mean action: 2.115 [0.000, 12.000],  loss: 0.020386, mae: 0.345119, mean_q: 0.541135, mean_eps: 0.000000
 4555/5000: episode: 164, duration: 0.392s, episode steps:  24, steps per second:  61, episode reward: 44.020, mean reward:  1.834 [-2.435, 32.337], mean action: 1.750 [0.000, 11.000],  loss: 0.024466, mae: 0.362496, mean_q: 0.564991, mean_eps: 0.000000
 4610/5000: episode: 165, duration: 1.613s, episode steps:  55, steps per second:  34, episode reward: 35.818, mean reward:  0.651 [-2.649, 32.230], mean action: 3.618 [0.000, 11.000],  loss: 0.024061, mae: 0.364048, mean_q: 0.601669, mean_eps: 0.000000
 4637/5000: episode: 166, duration: 0.733s, episode steps:  27, steps per second:  37, episode reward: 41.734, mean reward:  1.546 [-2.168, 32.170], mean action: 2.667 [0.000, 12.000],  loss: 0.021879, mae: 0.353347, mean_q: 0.595621, mean_eps: 0.000000
 4672/5000: episode: 167, duration: 0.640s, episode steps:  35, steps per second:  55, episode reward: 41.247, mean reward:  1.178 [-2.730, 32.460], mean action: 2.971 [0.000, 12.000],  loss: 0.017989, mae: 0.340997, mean_q: 0.544033, mean_eps: 0.000000
 4695/5000: episode: 168, duration: 0.334s, episode steps:  23, steps per second:  69, episode reward: 46.968, mean reward:  2.042 [-0.376, 32.127], mean action: 2.826 [2.000, 7.000],  loss: 0.020434, mae: 0.351262, mean_q: 0.504522, mean_eps: 0.000000
 4714/5000: episode: 169, duration: 0.278s, episode steps:  19, steps per second:  68, episode reward: 41.919, mean reward:  2.206 [-2.416, 32.359], mean action: 3.526 [0.000, 15.000],  loss: 0.016995, mae: 0.334928, mean_q: 0.513109, mean_eps: 0.000000
 4750/5000: episode: 170, duration: 0.652s, episode steps:  36, steps per second:  55, episode reward: 44.024, mean reward:  1.223 [-2.111, 32.030], mean action: 2.806 [0.000, 15.000],  loss: 0.021240, mae: 0.352148, mean_q: 0.514158, mean_eps: 0.000000
 4772/5000: episode: 171, duration: 0.432s, episode steps:  22, steps per second:  51, episode reward: 41.309, mean reward:  1.878 [-3.000, 32.501], mean action: 5.000 [0.000, 16.000],  loss: 0.022353, mae: 0.352839, mean_q: 0.544896, mean_eps: 0.000000
 4798/5000: episode: 172, duration: 0.431s, episode steps:  26, steps per second:  60, episode reward: 38.836, mean reward:  1.494 [-2.660, 32.026], mean action: 3.500 [0.000, 16.000],  loss: 0.017032, mae: 0.332663, mean_q: 0.493247, mean_eps: 0.000000
 4815/5000: episode: 173, duration: 0.567s, episode steps:  17, steps per second:  30, episode reward: 48.000, mean reward:  2.824 [-0.020, 32.580], mean action: 1.059 [0.000, 3.000],  loss: 0.024038, mae: 0.386085, mean_q: 0.495922, mean_eps: 0.000000
 4855/5000: episode: 174, duration: 1.528s, episode steps:  40, steps per second:  26, episode reward: 44.879, mean reward:  1.122 [-2.164, 32.320], mean action: 0.900 [0.000, 15.000],  loss: 0.022784, mae: 0.365717, mean_q: 0.512999, mean_eps: 0.000000
 4883/5000: episode: 175, duration: 1.090s, episode steps:  28, steps per second:  26, episode reward: 38.459, mean reward:  1.374 [-3.000, 32.060], mean action: 4.643 [0.000, 16.000],  loss: 0.022003, mae: 0.363801, mean_q: 0.546506, mean_eps: 0.000000
 4903/5000: episode: 176, duration: 0.860s, episode steps:  20, steps per second:  23, episode reward: 45.000, mean reward:  2.250 [-2.135, 32.690], mean action: 3.700 [0.000, 14.000],  loss: 0.021200, mae: 0.357393, mean_q: 0.580325, mean_eps: 0.000000
 4929/5000: episode: 177, duration: 0.568s, episode steps:  26, steps per second:  46, episode reward: 35.721, mean reward:  1.374 [-2.631, 32.376], mean action: 3.538 [0.000, 12.000],  loss: 0.018746, mae: 0.346089, mean_q: 0.580671, mean_eps: 0.000000
 4944/5000: episode: 178, duration: 0.301s, episode steps:  15, steps per second:  50, episode reward: 47.926, mean reward:  3.195 [ 0.000, 32.420], mean action: 2.800 [0.000, 3.000],  loss: 0.020250, mae: 0.354532, mean_q: 0.493402, mean_eps: 0.000000
 4963/5000: episode: 179, duration: 0.484s, episode steps:  19, steps per second:  39, episode reward: 40.928, mean reward:  2.154 [-2.441, 32.450], mean action: 4.789 [0.000, 15.000],  loss: 0.021402, mae: 0.362154, mean_q: 0.496262, mean_eps: 0.000000
 4979/5000: episode: 180, duration: 0.655s, episode steps:  16, steps per second:  24, episode reward: 47.266, mean reward:  2.954 [-0.135, 32.020], mean action: 2.312 [1.000, 3.000],  loss: 0.022145, mae: 0.363414, mean_q: 0.521183, mean_eps: 0.000000
done, took 90.504 seconds
DQN Evaluation: 7656 victories out of 8985 episodes
Training for 5000 steps ...
   20/5000: episode: 1, duration: 0.176s, episode steps:  20, steps per second: 113, episode reward: 32.675, mean reward:  1.634 [-2.904, 31.915], mean action: 4.400 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   42/5000: episode: 2, duration: 0.165s, episode steps:  22, steps per second: 133, episode reward: 40.617, mean reward:  1.846 [-3.000, 33.000], mean action: 3.955 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   63/5000: episode: 3, duration: 0.144s, episode steps:  21, steps per second: 146, episode reward: 35.515, mean reward:  1.691 [-3.000, 32.080], mean action: 5.857 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   95/5000: episode: 4, duration: 0.217s, episode steps:  32, steps per second: 147, episode reward: -32.380, mean reward: -1.012 [-32.477,  2.251], mean action: 5.156 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  122/5000: episode: 5, duration: 0.183s, episode steps:  27, steps per second: 147, episode reward: -32.170, mean reward: -1.191 [-31.657,  2.697], mean action: 3.704 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  143/5000: episode: 6, duration: 0.145s, episode steps:  21, steps per second: 145, episode reward: 34.861, mean reward:  1.660 [-3.000, 31.723], mean action: 7.000 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  165/5000: episode: 7, duration: 0.141s, episode steps:  22, steps per second: 156, episode reward: 33.000, mean reward:  1.500 [-3.000, 32.430], mean action: 6.182 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  193/5000: episode: 8, duration: 0.190s, episode steps:  28, steps per second: 148, episode reward: -32.420, mean reward: -1.158 [-32.802,  2.280], mean action: 4.893 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  215/5000: episode: 9, duration: 0.160s, episode steps:  22, steps per second: 137, episode reward: 33.000, mean reward:  1.500 [-2.266, 30.781], mean action: 2.773 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  240/5000: episode: 10, duration: 0.167s, episode steps:  25, steps per second: 149, episode reward: 32.069, mean reward:  1.283 [-2.511, 31.779], mean action: 4.680 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  260/5000: episode: 11, duration: 0.154s, episode steps:  20, steps per second: 130, episode reward: 38.623, mean reward:  1.931 [-3.000, 32.647], mean action: 3.450 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  276/5000: episode: 12, duration: 0.114s, episode steps:  16, steps per second: 141, episode reward: -35.190, mean reward: -2.199 [-31.214,  2.576], mean action: 5.875 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  298/5000: episode: 13, duration: 0.157s, episode steps:  22, steps per second: 141, episode reward: 35.574, mean reward:  1.617 [-2.413, 31.998], mean action: 5.545 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  318/5000: episode: 14, duration: 0.143s, episode steps:  20, steps per second: 140, episode reward: 41.496, mean reward:  2.075 [-3.000, 32.067], mean action: 3.550 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  332/5000: episode: 15, duration: 0.109s, episode steps:  14, steps per second: 129, episode reward: 44.459, mean reward:  3.176 [-2.232, 33.000], mean action: 2.786 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  354/5000: episode: 16, duration: 0.158s, episode steps:  22, steps per second: 139, episode reward: 35.284, mean reward:  1.604 [-2.833, 31.929], mean action: 3.545 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  381/5000: episode: 17, duration: 0.173s, episode steps:  27, steps per second: 156, episode reward: -38.410, mean reward: -1.423 [-32.016,  2.613], mean action: 6.741 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  407/5000: episode: 18, duration: 0.169s, episode steps:  26, steps per second: 154, episode reward: -33.000, mean reward: -1.269 [-33.000,  2.760], mean action: 8.808 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  441/5000: episode: 19, duration: 0.220s, episode steps:  34, steps per second: 155, episode reward: 32.070, mean reward:  0.943 [-3.000, 32.330], mean action: 4.235 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  464/5000: episode: 20, duration: 0.164s, episode steps:  23, steps per second: 140, episode reward: 32.797, mean reward:  1.426 [-2.998, 32.060], mean action: 5.304 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  479/5000: episode: 21, duration: 0.118s, episode steps:  15, steps per second: 127, episode reward: 41.219, mean reward:  2.748 [-3.000, 32.290], mean action: 3.200 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  499/5000: episode: 22, duration: 0.175s, episode steps:  20, steps per second: 114, episode reward: 35.903, mean reward:  1.795 [-2.910, 33.000], mean action: 5.800 [1.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  519/5000: episode: 23, duration: 0.135s, episode steps:  20, steps per second: 148, episode reward: 32.303, mean reward:  1.615 [-3.000, 32.221], mean action: 7.050 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  549/5000: episode: 24, duration: 0.201s, episode steps:  30, steps per second: 149, episode reward: 38.214, mean reward:  1.274 [-2.833, 32.414], mean action: 6.067 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  578/5000: episode: 25, duration: 0.195s, episode steps:  29, steps per second: 148, episode reward: 38.440, mean reward:  1.326 [-2.131, 32.400], mean action: 4.069 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  601/5000: episode: 26, duration: 0.176s, episode steps:  23, steps per second: 131, episode reward: 38.039, mean reward:  1.654 [-2.691, 32.440], mean action: 4.087 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  627/5000: episode: 27, duration: 0.178s, episode steps:  26, steps per second: 146, episode reward: 33.000, mean reward:  1.269 [-3.000, 32.080], mean action: 9.192 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  647/5000: episode: 28, duration: 0.137s, episode steps:  20, steps per second: 146, episode reward: 39.000, mean reward:  1.950 [-2.484, 32.260], mean action: 3.200 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  677/5000: episode: 29, duration: 0.188s, episode steps:  30, steps per second: 160, episode reward: -35.650, mean reward: -1.188 [-32.240,  2.213], mean action: 4.233 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  693/5000: episode: 30, duration: 0.114s, episode steps:  16, steps per second: 140, episode reward: 41.056, mean reward:  2.566 [-2.096, 33.000], mean action: 2.125 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  723/5000: episode: 31, duration: 0.195s, episode steps:  30, steps per second: 154, episode reward: -36.000, mean reward: -1.200 [-32.487,  2.466], mean action: 5.133 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  742/5000: episode: 32, duration: 0.138s, episode steps:  19, steps per second: 138, episode reward: 41.137, mean reward:  2.165 [-3.000, 32.180], mean action: 3.579 [2.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  765/5000: episode: 33, duration: 0.162s, episode steps:  23, steps per second: 142, episode reward: 37.554, mean reward:  1.633 [-2.457, 32.459], mean action: 5.391 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  788/5000: episode: 34, duration: 0.159s, episode steps:  23, steps per second: 145, episode reward: 35.612, mean reward:  1.548 [-2.450, 31.734], mean action: 3.522 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  812/5000: episode: 35, duration: 0.163s, episode steps:  24, steps per second: 147, episode reward: 32.705, mean reward:  1.363 [-2.770, 31.994], mean action: 4.375 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  840/5000: episode: 36, duration: 0.187s, episode steps:  28, steps per second: 150, episode reward: 32.823, mean reward:  1.172 [-2.191, 31.977], mean action: 5.143 [2.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  866/5000: episode: 37, duration: 0.170s, episode steps:  26, steps per second: 153, episode reward: 35.227, mean reward:  1.355 [-2.753, 32.021], mean action: 5.115 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  886/5000: episode: 38, duration: 0.133s, episode steps:  20, steps per second: 151, episode reward: -35.810, mean reward: -1.791 [-32.288,  3.000], mean action: 6.850 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  913/5000: episode: 39, duration: 0.195s, episode steps:  27, steps per second: 139, episode reward: -32.910, mean reward: -1.219 [-32.028,  2.723], mean action: 5.037 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  940/5000: episode: 40, duration: 0.182s, episode steps:  27, steps per second: 148, episode reward: -32.730, mean reward: -1.212 [-31.958,  3.000], mean action: 6.148 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  972/5000: episode: 41, duration: 0.205s, episode steps:  32, steps per second: 156, episode reward: 40.873, mean reward:  1.277 [-2.426, 32.293], mean action: 4.188 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  990/5000: episode: 42, duration: 0.127s, episode steps:  18, steps per second: 141, episode reward: 44.725, mean reward:  2.485 [-2.177, 32.196], mean action: 4.167 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1013/5000: episode: 43, duration: 0.284s, episode steps:  23, steps per second:  81, episode reward: 35.745, mean reward:  1.554 [-2.868, 32.225], mean action: 6.696 [0.000, 19.000],  loss: 0.021670, mae: 0.355445, mean_q: 0.494322, mean_eps: 0.000000
 1035/5000: episode: 44, duration: 0.339s, episode steps:  22, steps per second:  65, episode reward: 37.884, mean reward:  1.722 [-3.000, 33.000], mean action: 5.318 [0.000, 20.000],  loss: 0.019749, mae: 0.349178, mean_q: 0.523036, mean_eps: 0.000000
 1050/5000: episode: 45, duration: 0.225s, episode steps:  15, steps per second:  67, episode reward: 41.945, mean reward:  2.796 [-2.354, 32.475], mean action: 2.133 [0.000, 11.000],  loss: 0.019508, mae: 0.342693, mean_q: 0.552673, mean_eps: 0.000000
 1066/5000: episode: 46, duration: 0.245s, episode steps:  16, steps per second:  65, episode reward: 44.407, mean reward:  2.775 [-2.168, 33.000], mean action: 1.438 [0.000, 11.000],  loss: 0.018027, mae: 0.332516, mean_q: 0.601448, mean_eps: 0.000000
 1092/5000: episode: 47, duration: 0.375s, episode steps:  26, steps per second:  69, episode reward: -35.320, mean reward: -1.358 [-33.000,  2.410], mean action: 5.462 [0.000, 19.000],  loss: 0.020073, mae: 0.347174, mean_q: 0.625057, mean_eps: 0.000000
 1113/5000: episode: 48, duration: 0.310s, episode steps:  21, steps per second:  68, episode reward: 41.232, mean reward:  1.963 [-2.441, 32.053], mean action: 5.095 [0.000, 15.000],  loss: 0.021091, mae: 0.352344, mean_q: 0.594412, mean_eps: 0.000000
 1130/5000: episode: 49, duration: 0.256s, episode steps:  17, steps per second:  66, episode reward: 38.511, mean reward:  2.265 [-2.574, 32.010], mean action: 2.941 [0.000, 11.000],  loss: 0.020497, mae: 0.350020, mean_q: 0.626729, mean_eps: 0.000000
 1156/5000: episode: 50, duration: 0.372s, episode steps:  26, steps per second:  70, episode reward: 32.536, mean reward:  1.251 [-3.000, 32.030], mean action: 4.962 [0.000, 15.000],  loss: 0.019737, mae: 0.349157, mean_q: 0.552788, mean_eps: 0.000000
 1182/5000: episode: 51, duration: 0.383s, episode steps:  26, steps per second:  68, episode reward: -32.400, mean reward: -1.246 [-32.045,  3.000], mean action: 5.654 [0.000, 19.000],  loss: 0.018772, mae: 0.347842, mean_q: 0.549705, mean_eps: 0.000000
 1222/5000: episode: 52, duration: 0.569s, episode steps:  40, steps per second:  70, episode reward: 32.383, mean reward:  0.810 [-2.387, 32.391], mean action: 4.300 [0.000, 19.000],  loss: 0.022239, mae: 0.356031, mean_q: 0.532259, mean_eps: 0.000000
 1243/5000: episode: 53, duration: 0.312s, episode steps:  21, steps per second:  67, episode reward: 40.294, mean reward:  1.919 [-2.374, 32.180], mean action: 7.810 [0.000, 19.000],  loss: 0.020340, mae: 0.352561, mean_q: 0.524431, mean_eps: 0.000000
 1267/5000: episode: 54, duration: 0.349s, episode steps:  24, steps per second:  69, episode reward: -32.050, mean reward: -1.335 [-31.154,  2.901], mean action: 4.167 [0.000, 15.000],  loss: 0.021360, mae: 0.364884, mean_q: 0.535283, mean_eps: 0.000000
 1292/5000: episode: 55, duration: 0.354s, episode steps:  25, steps per second:  71, episode reward: -38.270, mean reward: -1.531 [-32.086,  2.210], mean action: 3.920 [0.000, 15.000],  loss: 0.022606, mae: 0.366642, mean_q: 0.517042, mean_eps: 0.000000
 1317/5000: episode: 56, duration: 0.364s, episode steps:  25, steps per second:  69, episode reward: -32.620, mean reward: -1.305 [-32.317,  2.466], mean action: 3.880 [0.000, 15.000],  loss: 0.019787, mae: 0.350668, mean_q: 0.540518, mean_eps: 0.000000
 1338/5000: episode: 57, duration: 0.315s, episode steps:  21, steps per second:  67, episode reward: 43.888, mean reward:  2.090 [-2.041, 32.078], mean action: 4.286 [0.000, 19.000],  loss: 0.017329, mae: 0.342101, mean_q: 0.510202, mean_eps: 0.000000
 1362/5000: episode: 58, duration: 0.362s, episode steps:  24, steps per second:  66, episode reward: 38.371, mean reward:  1.599 [-2.256, 32.566], mean action: 2.542 [0.000, 19.000],  loss: 0.020628, mae: 0.353130, mean_q: 0.475520, mean_eps: 0.000000
 1395/5000: episode: 59, duration: 0.467s, episode steps:  33, steps per second:  71, episode reward: 35.322, mean reward:  1.070 [-2.691, 32.099], mean action: 4.364 [0.000, 15.000],  loss: 0.020889, mae: 0.357689, mean_q: 0.510521, mean_eps: 0.000000
 1426/5000: episode: 60, duration: 0.444s, episode steps:  31, steps per second:  70, episode reward: 32.907, mean reward:  1.062 [-2.352, 32.105], mean action: 10.710 [0.000, 20.000],  loss: 0.020803, mae: 0.362287, mean_q: 0.480651, mean_eps: 0.000000
 1450/5000: episode: 61, duration: 0.344s, episode steps:  24, steps per second:  70, episode reward: 35.391, mean reward:  1.475 [-2.160, 32.323], mean action: 3.042 [0.000, 15.000],  loss: 0.019294, mae: 0.352690, mean_q: 0.517831, mean_eps: 0.000000
 1463/5000: episode: 62, duration: 0.197s, episode steps:  13, steps per second:  66, episode reward: 41.647, mean reward:  3.204 [-3.000, 31.981], mean action: 2.692 [0.000, 11.000],  loss: 0.015573, mae: 0.334119, mean_q: 0.557034, mean_eps: 0.000000
 1478/5000: episode: 63, duration: 0.238s, episode steps:  15, steps per second:  63, episode reward: 41.390, mean reward:  2.759 [-2.143, 32.030], mean action: 3.200 [0.000, 15.000],  loss: 0.014403, mae: 0.328564, mean_q: 0.556281, mean_eps: 0.000000
 1492/5000: episode: 64, duration: 0.213s, episode steps:  14, steps per second:  66, episode reward: 41.644, mean reward:  2.975 [-2.305, 32.380], mean action: 2.357 [0.000, 12.000],  loss: 0.022172, mae: 0.359954, mean_q: 0.582290, mean_eps: 0.000000
 1519/5000: episode: 65, duration: 0.381s, episode steps:  27, steps per second:  71, episode reward: 35.771, mean reward:  1.325 [-2.437, 32.400], mean action: 2.370 [0.000, 9.000],  loss: 0.020883, mae: 0.353848, mean_q: 0.525492, mean_eps: 0.000000
 1540/5000: episode: 66, duration: 0.305s, episode steps:  21, steps per second:  69, episode reward: 41.111, mean reward:  1.958 [-2.069, 33.000], mean action: 1.762 [0.000, 16.000],  loss: 0.018579, mae: 0.334303, mean_q: 0.545926, mean_eps: 0.000000
 1553/5000: episode: 67, duration: 0.234s, episode steps:  13, steps per second:  56, episode reward: 44.004, mean reward:  3.385 [-2.106, 32.903], mean action: 2.231 [0.000, 12.000],  loss: 0.031481, mae: 0.397114, mean_q: 0.563088, mean_eps: 0.000000
 1583/5000: episode: 68, duration: 0.422s, episode steps:  30, steps per second:  71, episode reward: 35.051, mean reward:  1.168 [-2.416, 31.672], mean action: 7.067 [0.000, 15.000],  loss: 0.021522, mae: 0.355266, mean_q: 0.539457, mean_eps: 0.000000
 1597/5000: episode: 69, duration: 0.212s, episode steps:  14, steps per second:  66, episode reward: 41.195, mean reward:  2.942 [-2.359, 32.820], mean action: 2.929 [0.000, 15.000],  loss: 0.021429, mae: 0.355640, mean_q: 0.458028, mean_eps: 0.000000
 1639/5000: episode: 70, duration: 0.592s, episode steps:  42, steps per second:  71, episode reward: 38.061, mean reward:  0.906 [-2.397, 32.940], mean action: 6.000 [0.000, 20.000],  loss: 0.021005, mae: 0.348626, mean_q: 0.509867, mean_eps: 0.000000
 1664/5000: episode: 71, duration: 0.358s, episode steps:  25, steps per second:  70, episode reward: 41.528, mean reward:  1.661 [-2.169, 32.350], mean action: 3.200 [0.000, 11.000],  loss: 0.018687, mae: 0.340065, mean_q: 0.540666, mean_eps: 0.000000
 1689/5000: episode: 72, duration: 1.010s, episode steps:  25, steps per second:  25, episode reward: 34.725, mean reward:  1.389 [-2.338, 31.657], mean action: 5.240 [0.000, 20.000],  loss: 0.021537, mae: 0.347081, mean_q: 0.527717, mean_eps: 0.000000
 1705/5000: episode: 73, duration: 0.511s, episode steps:  16, steps per second:  31, episode reward: 41.060, mean reward:  2.566 [-2.465, 32.110], mean action: 1.562 [0.000, 16.000],  loss: 0.016846, mae: 0.324591, mean_q: 0.536747, mean_eps: 0.000000
 1728/5000: episode: 74, duration: 0.354s, episode steps:  23, steps per second:  65, episode reward: 35.437, mean reward:  1.541 [-2.655, 32.510], mean action: 3.826 [0.000, 20.000],  loss: 0.020118, mae: 0.347517, mean_q: 0.525586, mean_eps: 0.000000
 1756/5000: episode: 75, duration: 0.436s, episode steps:  28, steps per second:  64, episode reward: -33.000, mean reward: -1.179 [-32.739,  2.480], mean action: 2.964 [0.000, 16.000],  loss: 0.021936, mae: 0.353507, mean_q: 0.536308, mean_eps: 0.000000
 1778/5000: episode: 76, duration: 0.364s, episode steps:  22, steps per second:  60, episode reward: 40.931, mean reward:  1.860 [-2.998, 32.556], mean action: 2.045 [0.000, 15.000],  loss: 0.025527, mae: 0.363308, mean_q: 0.552326, mean_eps: 0.000000
 1799/5000: episode: 77, duration: 0.331s, episode steps:  21, steps per second:  64, episode reward: 36.000, mean reward:  1.714 [-2.292, 30.033], mean action: 4.000 [0.000, 13.000],  loss: 0.017353, mae: 0.323231, mean_q: 0.494739, mean_eps: 0.000000
 1825/5000: episode: 78, duration: 0.440s, episode steps:  26, steps per second:  59, episode reward: 33.000, mean reward:  1.269 [-3.000, 33.000], mean action: 4.038 [0.000, 15.000],  loss: 0.023354, mae: 0.354793, mean_q: 0.534213, mean_eps: 0.000000
 1845/5000: episode: 79, duration: 0.395s, episode steps:  20, steps per second:  51, episode reward: 39.000, mean reward:  1.950 [-2.360, 30.371], mean action: 3.800 [2.000, 14.000],  loss: 0.019149, mae: 0.340389, mean_q: 0.540702, mean_eps: 0.000000
 1877/5000: episode: 80, duration: 0.552s, episode steps:  32, steps per second:  58, episode reward: -32.430, mean reward: -1.013 [-32.142,  2.689], mean action: 6.938 [0.000, 18.000],  loss: 0.023564, mae: 0.364426, mean_q: 0.562814, mean_eps: 0.000000
 1896/5000: episode: 81, duration: 0.298s, episode steps:  19, steps per second:  64, episode reward: 36.000, mean reward:  1.895 [-3.000, 32.500], mean action: 3.000 [0.000, 11.000],  loss: 0.019458, mae: 0.346992, mean_q: 0.589029, mean_eps: 0.000000
 1913/5000: episode: 82, duration: 0.286s, episode steps:  17, steps per second:  59, episode reward: 36.000, mean reward:  2.118 [-3.000, 32.740], mean action: 4.118 [0.000, 15.000],  loss: 0.018402, mae: 0.347894, mean_q: 0.562481, mean_eps: 0.000000
 1925/5000: episode: 83, duration: 0.209s, episode steps:  12, steps per second:  57, episode reward: 41.503, mean reward:  3.459 [-2.675, 32.206], mean action: 4.833 [1.000, 15.000],  loss: 0.021561, mae: 0.362126, mean_q: 0.607608, mean_eps: 0.000000
 1944/5000: episode: 84, duration: 0.307s, episode steps:  19, steps per second:  62, episode reward: 38.275, mean reward:  2.014 [-2.451, 32.275], mean action: 2.632 [0.000, 15.000],  loss: 0.020227, mae: 0.346548, mean_q: 0.598639, mean_eps: 0.000000
 1972/5000: episode: 85, duration: 0.637s, episode steps:  28, steps per second:  44, episode reward: -32.480, mean reward: -1.160 [-32.030,  2.903], mean action: 4.571 [0.000, 16.000],  loss: 0.019831, mae: 0.349776, mean_q: 0.595663, mean_eps: 0.000000
 2006/5000: episode: 86, duration: 0.571s, episode steps:  34, steps per second:  60, episode reward: 35.062, mean reward:  1.031 [-3.000, 32.290], mean action: 3.235 [0.000, 15.000],  loss: 0.020678, mae: 0.349633, mean_q: 0.615911, mean_eps: 0.000000
 2037/5000: episode: 87, duration: 0.496s, episode steps:  31, steps per second:  62, episode reward: 33.000, mean reward:  1.065 [-2.347, 32.540], mean action: 3.581 [0.000, 15.000],  loss: 0.019635, mae: 0.346688, mean_q: 0.565171, mean_eps: 0.000000
 2059/5000: episode: 88, duration: 0.323s, episode steps:  22, steps per second:  68, episode reward: 35.194, mean reward:  1.600 [-3.000, 33.000], mean action: 4.091 [0.000, 15.000],  loss: 0.022991, mae: 0.365431, mean_q: 0.558867, mean_eps: 0.000000
 2085/5000: episode: 89, duration: 0.391s, episode steps:  26, steps per second:  66, episode reward: 32.338, mean reward:  1.244 [-3.000, 32.319], mean action: 4.962 [0.000, 15.000],  loss: 0.020780, mae: 0.358533, mean_q: 0.514425, mean_eps: 0.000000
 2110/5000: episode: 90, duration: 0.377s, episode steps:  25, steps per second:  66, episode reward: 41.383, mean reward:  1.655 [-2.199, 32.194], mean action: 2.000 [0.000, 15.000],  loss: 0.018989, mae: 0.349547, mean_q: 0.549346, mean_eps: 0.000000
 2139/5000: episode: 91, duration: 0.445s, episode steps:  29, steps per second:  65, episode reward: 38.139, mean reward:  1.315 [-2.429, 32.254], mean action: 4.414 [1.000, 15.000],  loss: 0.020104, mae: 0.358913, mean_q: 0.533916, mean_eps: 0.000000
 2162/5000: episode: 92, duration: 0.346s, episode steps:  23, steps per second:  66, episode reward: 36.000, mean reward:  1.565 [-2.286, 33.000], mean action: 3.739 [0.000, 15.000],  loss: 0.020081, mae: 0.354806, mean_q: 0.534202, mean_eps: 0.000000
 2181/5000: episode: 93, duration: 0.279s, episode steps:  19, steps per second:  68, episode reward: 38.704, mean reward:  2.037 [-2.548, 31.959], mean action: 3.842 [0.000, 15.000],  loss: 0.017177, mae: 0.335710, mean_q: 0.548683, mean_eps: 0.000000
 2201/5000: episode: 94, duration: 0.292s, episode steps:  20, steps per second:  69, episode reward: 39.000, mean reward:  1.950 [-2.138, 29.753], mean action: 2.850 [0.000, 15.000],  loss: 0.018066, mae: 0.333777, mean_q: 0.579186, mean_eps: 0.000000
 2226/5000: episode: 95, duration: 0.391s, episode steps:  25, steps per second:  64, episode reward: 38.224, mean reward:  1.529 [-3.000, 33.000], mean action: 3.760 [0.000, 15.000],  loss: 0.022141, mae: 0.352057, mean_q: 0.550639, mean_eps: 0.000000
 2258/5000: episode: 96, duration: 0.446s, episode steps:  32, steps per second:  72, episode reward: 32.852, mean reward:  1.027 [-2.502, 32.170], mean action: 4.281 [0.000, 15.000],  loss: 0.021395, mae: 0.349887, mean_q: 0.594403, mean_eps: 0.000000
 2284/5000: episode: 97, duration: 0.369s, episode steps:  26, steps per second:  71, episode reward: 32.045, mean reward:  1.232 [-2.495, 32.075], mean action: 5.500 [0.000, 15.000],  loss: 0.022407, mae: 0.356690, mean_q: 0.569433, mean_eps: 0.000000
 2302/5000: episode: 98, duration: 0.261s, episode steps:  18, steps per second:  69, episode reward: 39.000, mean reward:  2.167 [-2.801, 32.360], mean action: 3.000 [0.000, 19.000],  loss: 0.019535, mae: 0.345829, mean_q: 0.565571, mean_eps: 0.000000
 2333/5000: episode: 99, duration: 0.440s, episode steps:  31, steps per second:  70, episode reward: 37.978, mean reward:  1.225 [-2.770, 31.793], mean action: 3.774 [0.000, 14.000],  loss: 0.020017, mae: 0.342591, mean_q: 0.587407, mean_eps: 0.000000
 2349/5000: episode: 100, duration: 0.239s, episode steps:  16, steps per second:  67, episode reward: 37.000, mean reward:  2.312 [-2.506, 32.340], mean action: 4.188 [0.000, 15.000],  loss: 0.019283, mae: 0.339164, mean_q: 0.592000, mean_eps: 0.000000
 2359/5000: episode: 101, duration: 0.158s, episode steps:  10, steps per second:  63, episode reward: 42.000, mean reward:  4.200 [-2.593, 30.981], mean action: 3.000 [0.000, 15.000],  loss: 0.023870, mae: 0.367079, mean_q: 0.574121, mean_eps: 0.000000
 2378/5000: episode: 102, duration: 0.266s, episode steps:  19, steps per second:  72, episode reward: -41.260, mean reward: -2.172 [-32.069,  2.240], mean action: 4.737 [0.000, 15.000],  loss: 0.022487, mae: 0.355391, mean_q: 0.592920, mean_eps: 0.000000
 2399/5000: episode: 103, duration: 0.305s, episode steps:  21, steps per second:  69, episode reward: 35.901, mean reward:  1.710 [-2.505, 32.111], mean action: 4.000 [0.000, 15.000],  loss: 0.016123, mae: 0.329681, mean_q: 0.581334, mean_eps: 0.000000
 2428/5000: episode: 104, duration: 0.471s, episode steps:  29, steps per second:  62, episode reward: 32.440, mean reward:  1.119 [-3.000, 31.530], mean action: 4.828 [0.000, 19.000],  loss: 0.020104, mae: 0.348463, mean_q: 0.543503, mean_eps: 0.000000
 2455/5000: episode: 105, duration: 0.385s, episode steps:  27, steps per second:  70, episode reward: 32.261, mean reward:  1.195 [-2.449, 31.936], mean action: 4.741 [0.000, 15.000],  loss: 0.017721, mae: 0.343895, mean_q: 0.521742, mean_eps: 0.000000
 2480/5000: episode: 106, duration: 0.358s, episode steps:  25, steps per second:  70, episode reward: -35.520, mean reward: -1.421 [-31.824,  2.090], mean action: 8.080 [0.000, 20.000],  loss: 0.022901, mae: 0.367924, mean_q: 0.566104, mean_eps: 0.000000
 2499/5000: episode: 107, duration: 0.276s, episode steps:  19, steps per second:  69, episode reward: 35.705, mean reward:  1.879 [-3.000, 32.705], mean action: 4.684 [1.000, 12.000],  loss: 0.024904, mae: 0.377576, mean_q: 0.555646, mean_eps: 0.000000
 2531/5000: episode: 108, duration: 0.449s, episode steps:  32, steps per second:  71, episode reward: 40.694, mean reward:  1.272 [-2.730, 32.880], mean action: 5.750 [0.000, 13.000],  loss: 0.017887, mae: 0.342728, mean_q: 0.575607, mean_eps: 0.000000
 2556/5000: episode: 109, duration: 0.350s, episode steps:  25, steps per second:  72, episode reward: -35.440, mean reward: -1.418 [-31.796,  2.599], mean action: 5.160 [0.000, 14.000],  loss: 0.020632, mae: 0.353945, mean_q: 0.558779, mean_eps: 0.000000
 2568/5000: episode: 110, duration: 0.185s, episode steps:  12, steps per second:  65, episode reward: 38.843, mean reward:  3.237 [-3.000, 30.896], mean action: 4.917 [0.000, 14.000],  loss: 0.020762, mae: 0.346252, mean_q: 0.570421, mean_eps: 0.000000
 2589/5000: episode: 111, duration: 0.307s, episode steps:  21, steps per second:  68, episode reward: 41.263, mean reward:  1.965 [-2.496, 32.260], mean action: 4.238 [0.000, 14.000],  loss: 0.020015, mae: 0.351418, mean_q: 0.569588, mean_eps: 0.000000
 2623/5000: episode: 112, duration: 0.481s, episode steps:  34, steps per second:  71, episode reward: 32.730, mean reward:  0.963 [-2.179, 31.980], mean action: 4.647 [0.000, 21.000],  loss: 0.019674, mae: 0.346722, mean_q: 0.540849, mean_eps: 0.000000
 2641/5000: episode: 113, duration: 0.267s, episode steps:  18, steps per second:  67, episode reward: 35.125, mean reward:  1.951 [-2.787, 31.675], mean action: 3.389 [0.000, 12.000],  loss: 0.019126, mae: 0.350130, mean_q: 0.519187, mean_eps: 0.000000
 2662/5000: episode: 114, duration: 0.316s, episode steps:  21, steps per second:  66, episode reward: 36.000, mean reward:  1.714 [-2.550, 32.100], mean action: 3.524 [0.000, 11.000],  loss: 0.021142, mae: 0.355955, mean_q: 0.510321, mean_eps: 0.000000
 2687/5000: episode: 115, duration: 0.362s, episode steps:  25, steps per second:  69, episode reward: 32.903, mean reward:  1.316 [-2.660, 32.113], mean action: 5.840 [0.000, 20.000],  loss: 0.021816, mae: 0.361361, mean_q: 0.508388, mean_eps: 0.000000
 2704/5000: episode: 116, duration: 0.248s, episode steps:  17, steps per second:  69, episode reward: 37.565, mean reward:  2.210 [-2.883, 32.410], mean action: 4.588 [0.000, 15.000],  loss: 0.019868, mae: 0.351161, mean_q: 0.489147, mean_eps: 0.000000
 2720/5000: episode: 117, duration: 0.237s, episode steps:  16, steps per second:  68, episode reward: 38.363, mean reward:  2.398 [-2.610, 32.280], mean action: 3.875 [0.000, 19.000],  loss: 0.018766, mae: 0.343761, mean_q: 0.486083, mean_eps: 0.000000
 2757/5000: episode: 118, duration: 0.521s, episode steps:  37, steps per second:  71, episode reward: 32.441, mean reward:  0.877 [-2.486, 32.018], mean action: 6.946 [0.000, 20.000],  loss: 0.019289, mae: 0.344443, mean_q: 0.531672, mean_eps: 0.000000
 2785/5000: episode: 119, duration: 0.408s, episode steps:  28, steps per second:  69, episode reward: -35.850, mean reward: -1.280 [-32.049,  2.220], mean action: 4.929 [0.000, 16.000],  loss: 0.018883, mae: 0.343090, mean_q: 0.534518, mean_eps: 0.000000
 2805/5000: episode: 120, duration: 0.299s, episode steps:  20, steps per second:  67, episode reward: 40.934, mean reward:  2.047 [-2.315, 31.906], mean action: 3.100 [0.000, 16.000],  loss: 0.023065, mae: 0.366384, mean_q: 0.499456, mean_eps: 0.000000
 2837/5000: episode: 121, duration: 0.453s, episode steps:  32, steps per second:  71, episode reward: 33.000, mean reward:  1.031 [-2.652, 32.180], mean action: 3.719 [1.000, 16.000],  loss: 0.024218, mae: 0.363465, mean_q: 0.544793, mean_eps: 0.000000
 2860/5000: episode: 122, duration: 0.332s, episode steps:  23, steps per second:  69, episode reward: -32.560, mean reward: -1.416 [-31.878,  2.522], mean action: 3.826 [0.000, 16.000],  loss: 0.018433, mae: 0.337879, mean_q: 0.536061, mean_eps: 0.000000
 2869/5000: episode: 123, duration: 0.142s, episode steps:   9, steps per second:  63, episode reward: 44.349, mean reward:  4.928 [-2.901, 33.000], mean action: 3.444 [0.000, 12.000],  loss: 0.021598, mae: 0.351313, mean_q: 0.528906, mean_eps: 0.000000
 2897/5000: episode: 124, duration: 0.404s, episode steps:  28, steps per second:  69, episode reward: 32.808, mean reward:  1.172 [-2.462, 32.904], mean action: 4.536 [0.000, 15.000],  loss: 0.022224, mae: 0.357274, mean_q: 0.563095, mean_eps: 0.000000
 2977/5000: episode: 125, duration: 1.034s, episode steps:  80, steps per second:  77, episode reward: 44.514, mean reward:  0.556 [-2.237, 33.000], mean action: 5.263 [0.000, 12.000],  loss: 0.017732, mae: 0.346285, mean_q: 0.512773, mean_eps: 0.000000
 3004/5000: episode: 126, duration: 0.382s, episode steps:  27, steps per second:  71, episode reward: 32.888, mean reward:  1.218 [-2.660, 32.210], mean action: 5.148 [0.000, 14.000],  loss: 0.020684, mae: 0.363040, mean_q: 0.460559, mean_eps: 0.000000
 3033/5000: episode: 127, duration: 0.403s, episode steps:  29, steps per second:  72, episode reward: -35.600, mean reward: -1.228 [-32.080,  2.260], mean action: 3.069 [0.000, 12.000],  loss: 0.018819, mae: 0.357240, mean_q: 0.453558, mean_eps: 0.000000
 3048/5000: episode: 128, duration: 0.225s, episode steps:  15, steps per second:  67, episode reward: 38.249, mean reward:  2.550 [-2.437, 32.070], mean action: 3.467 [0.000, 12.000],  loss: 0.019137, mae: 0.350827, mean_q: 0.470149, mean_eps: 0.000000
 3062/5000: episode: 129, duration: 0.211s, episode steps:  14, steps per second:  66, episode reward: 42.000, mean reward:  3.000 [-2.149, 30.581], mean action: 2.143 [0.000, 9.000],  loss: 0.021182, mae: 0.359669, mean_q: 0.448714, mean_eps: 0.000000
 3086/5000: episode: 130, duration: 0.468s, episode steps:  24, steps per second:  51, episode reward: 38.440, mean reward:  1.602 [-2.336, 32.793], mean action: 3.333 [0.000, 16.000],  loss: 0.019214, mae: 0.347040, mean_q: 0.526483, mean_eps: 0.000000
 3121/5000: episode: 131, duration: 0.490s, episode steps:  35, steps per second:  71, episode reward: 34.615, mean reward:  0.989 [-2.115, 32.510], mean action: 4.057 [0.000, 19.000],  loss: 0.019671, mae: 0.350646, mean_q: 0.471806, mean_eps: 0.000000
 3149/5000: episode: 132, duration: 0.406s, episode steps:  28, steps per second:  69, episode reward: 38.431, mean reward:  1.373 [-2.266, 33.000], mean action: 6.643 [0.000, 20.000],  loss: 0.021515, mae: 0.357043, mean_q: 0.511578, mean_eps: 0.000000
 3186/5000: episode: 133, duration: 0.799s, episode steps:  37, steps per second:  46, episode reward: 32.354, mean reward:  0.874 [-3.000, 32.314], mean action: 8.054 [0.000, 17.000],  loss: 0.023197, mae: 0.363666, mean_q: 0.524696, mean_eps: 0.000000
 3207/5000: episode: 134, duration: 0.315s, episode steps:  21, steps per second:  67, episode reward: 38.294, mean reward:  1.824 [-3.000, 32.060], mean action: 4.429 [0.000, 16.000],  loss: 0.019625, mae: 0.347107, mean_q: 0.524299, mean_eps: 0.000000
 3239/5000: episode: 135, duration: 0.459s, episode steps:  32, steps per second:  70, episode reward: -33.000, mean reward: -1.031 [-30.980,  2.379], mean action: 7.594 [0.000, 20.000],  loss: 0.022659, mae: 0.363584, mean_q: 0.540043, mean_eps: 0.000000
 3257/5000: episode: 136, duration: 0.473s, episode steps:  18, steps per second:  38, episode reward: 41.902, mean reward:  2.328 [-2.234, 32.092], mean action: 3.278 [0.000, 15.000],  loss: 0.020606, mae: 0.352129, mean_q: 0.576134, mean_eps: 0.000000
 3284/5000: episode: 137, duration: 0.460s, episode steps:  27, steps per second:  59, episode reward: 38.502, mean reward:  1.426 [-2.498, 31.910], mean action: 3.519 [0.000, 16.000],  loss: 0.020820, mae: 0.362274, mean_q: 0.602798, mean_eps: 0.000000
 3313/5000: episode: 138, duration: 0.454s, episode steps:  29, steps per second:  64, episode reward: 38.700, mean reward:  1.334 [-2.341, 32.170], mean action: 6.103 [0.000, 20.000],  loss: 0.018292, mae: 0.350870, mean_q: 0.519548, mean_eps: 0.000000
 3334/5000: episode: 139, duration: 0.304s, episode steps:  21, steps per second:  69, episode reward: 32.131, mean reward:  1.530 [-3.000, 31.381], mean action: 4.619 [0.000, 15.000],  loss: 0.019832, mae: 0.361746, mean_q: 0.526304, mean_eps: 0.000000
 3356/5000: episode: 140, duration: 0.315s, episode steps:  22, steps per second:  70, episode reward: 39.000, mean reward:  1.773 [-2.460, 32.050], mean action: 3.545 [0.000, 15.000],  loss: 0.019299, mae: 0.351277, mean_q: 0.505751, mean_eps: 0.000000
 3379/5000: episode: 141, duration: 0.321s, episode steps:  23, steps per second:  72, episode reward: 35.603, mean reward:  1.548 [-3.000, 33.000], mean action: 3.739 [0.000, 12.000],  loss: 0.021822, mae: 0.370174, mean_q: 0.491941, mean_eps: 0.000000
 3425/5000: episode: 142, duration: 0.643s, episode steps:  46, steps per second:  72, episode reward: 32.947, mean reward:  0.716 [-2.369, 31.957], mean action: 6.348 [0.000, 16.000],  loss: 0.020610, mae: 0.366771, mean_q: 0.536985, mean_eps: 0.000000
 3450/5000: episode: 143, duration: 0.355s, episode steps:  25, steps per second:  70, episode reward: 37.708, mean reward:  1.508 [-2.450, 32.705], mean action: 5.560 [0.000, 19.000],  loss: 0.018683, mae: 0.355410, mean_q: 0.537264, mean_eps: 0.000000
 3474/5000: episode: 144, duration: 0.332s, episode steps:  24, steps per second:  72, episode reward: -38.180, mean reward: -1.591 [-31.871,  2.433], mean action: 4.583 [0.000, 19.000],  loss: 0.021636, mae: 0.371573, mean_q: 0.530440, mean_eps: 0.000000
 3493/5000: episode: 145, duration: 0.280s, episode steps:  19, steps per second:  68, episode reward: 41.723, mean reward:  2.196 [-2.193, 33.000], mean action: 2.737 [0.000, 15.000],  loss: 0.019357, mae: 0.356995, mean_q: 0.511844, mean_eps: 0.000000
 3548/5000: episode: 146, duration: 0.751s, episode steps:  55, steps per second:  73, episode reward: 32.244, mean reward:  0.586 [-2.224, 32.028], mean action: 9.873 [0.000, 20.000],  loss: 0.021772, mae: 0.361680, mean_q: 0.523190, mean_eps: 0.000000
 3566/5000: episode: 147, duration: 0.264s, episode steps:  18, steps per second:  68, episode reward: 35.838, mean reward:  1.991 [-2.802, 32.298], mean action: 5.000 [0.000, 19.000],  loss: 0.022274, mae: 0.361586, mean_q: 0.523104, mean_eps: 0.000000
 3586/5000: episode: 148, duration: 0.287s, episode steps:  20, steps per second:  70, episode reward: -33.000, mean reward: -1.650 [-32.903,  2.593], mean action: 4.450 [0.000, 19.000],  loss: 0.022827, mae: 0.364152, mean_q: 0.550792, mean_eps: 0.000000
 3624/5000: episode: 149, duration: 0.527s, episode steps:  38, steps per second:  72, episode reward: 32.331, mean reward:  0.851 [-2.342, 32.372], mean action: 4.342 [0.000, 19.000],  loss: 0.019817, mae: 0.346709, mean_q: 0.559988, mean_eps: 0.000000
 3660/5000: episode: 150, duration: 0.504s, episode steps:  36, steps per second:  71, episode reward: 41.631, mean reward:  1.156 [-2.288, 32.875], mean action: 2.000 [0.000, 19.000],  loss: 0.021144, mae: 0.347818, mean_q: 0.549050, mean_eps: 0.000000
 3684/5000: episode: 151, duration: 0.339s, episode steps:  24, steps per second:  71, episode reward: -35.360, mean reward: -1.473 [-31.679,  2.740], mean action: 7.542 [0.000, 19.000],  loss: 0.020904, mae: 0.356735, mean_q: 0.514805, mean_eps: 0.000000
 3710/5000: episode: 152, duration: 0.375s, episode steps:  26, steps per second:  69, episode reward: 33.000, mean reward:  1.269 [-2.903, 32.070], mean action: 4.692 [0.000, 19.000],  loss: 0.017255, mae: 0.333688, mean_q: 0.517434, mean_eps: 0.000000
 3731/5000: episode: 153, duration: 0.299s, episode steps:  21, steps per second:  70, episode reward: -35.100, mean reward: -1.671 [-32.393,  3.000], mean action: 6.333 [0.000, 19.000],  loss: 0.020702, mae: 0.359281, mean_q: 0.479163, mean_eps: 0.000000
 3747/5000: episode: 154, duration: 0.226s, episode steps:  16, steps per second:  71, episode reward: -38.470, mean reward: -2.404 [-33.000,  3.000], mean action: 6.375 [0.000, 16.000],  loss: 0.019444, mae: 0.353122, mean_q: 0.555027, mean_eps: 0.000000
 3764/5000: episode: 155, duration: 0.249s, episode steps:  17, steps per second:  68, episode reward: 38.241, mean reward:  2.249 [-3.000, 31.766], mean action: 4.176 [0.000, 15.000],  loss: 0.019762, mae: 0.345001, mean_q: 0.539839, mean_eps: 0.000000
 3786/5000: episode: 156, duration: 0.313s, episode steps:  22, steps per second:  70, episode reward: -33.000, mean reward: -1.500 [-33.000,  2.272], mean action: 5.636 [0.000, 20.000],  loss: 0.018758, mae: 0.335915, mean_q: 0.528523, mean_eps: 0.000000
 3813/5000: episode: 157, duration: 0.382s, episode steps:  27, steps per second:  71, episode reward: 32.683, mean reward:  1.210 [-2.804, 31.893], mean action: 4.778 [0.000, 19.000],  loss: 0.020162, mae: 0.338300, mean_q: 0.522823, mean_eps: 0.000000
 3834/5000: episode: 158, duration: 0.307s, episode steps:  21, steps per second:  68, episode reward: 36.000, mean reward:  1.714 [-2.464, 32.240], mean action: 3.667 [0.000, 19.000],  loss: 0.021258, mae: 0.349567, mean_q: 0.512619, mean_eps: 0.000000
 3853/5000: episode: 159, duration: 0.273s, episode steps:  19, steps per second:  70, episode reward: 33.000, mean reward:  1.737 [-2.902, 29.300], mean action: 5.579 [0.000, 19.000],  loss: 0.020582, mae: 0.351517, mean_q: 0.542854, mean_eps: 0.000000
 3881/5000: episode: 160, duration: 0.406s, episode steps:  28, steps per second:  69, episode reward: 32.601, mean reward:  1.164 [-2.242, 32.501], mean action: 4.214 [0.000, 12.000],  loss: 0.022683, mae: 0.366317, mean_q: 0.564318, mean_eps: 0.000000
 3908/5000: episode: 161, duration: 0.384s, episode steps:  27, steps per second:  70, episode reward: 37.892, mean reward:  1.403 [-2.404, 32.880], mean action: 4.926 [0.000, 20.000],  loss: 0.021893, mae: 0.363468, mean_q: 0.563330, mean_eps: 0.000000
 3929/5000: episode: 162, duration: 0.294s, episode steps:  21, steps per second:  71, episode reward: 32.169, mean reward:  1.532 [-2.814, 32.430], mean action: 4.714 [0.000, 19.000],  loss: 0.020474, mae: 0.352498, mean_q: 0.548547, mean_eps: 0.000000
 3951/5000: episode: 163, duration: 0.315s, episode steps:  22, steps per second:  70, episode reward: 35.809, mean reward:  1.628 [-3.000, 32.619], mean action: 2.909 [0.000, 15.000],  loss: 0.023181, mae: 0.371524, mean_q: 0.589290, mean_eps: 0.000000
 3973/5000: episode: 164, duration: 0.313s, episode steps:  22, steps per second:  70, episode reward: 35.904, mean reward:  1.632 [-2.234, 32.024], mean action: 4.318 [0.000, 15.000],  loss: 0.019964, mae: 0.356993, mean_q: 0.571019, mean_eps: 0.000000
 4014/5000: episode: 165, duration: 0.561s, episode steps:  41, steps per second:  73, episode reward: 34.222, mean reward:  0.835 [-2.903, 32.373], mean action: 2.756 [0.000, 15.000],  loss: 0.019217, mae: 0.347998, mean_q: 0.535507, mean_eps: 0.000000
 4039/5000: episode: 166, duration: 0.352s, episode steps:  25, steps per second:  71, episode reward: 32.400, mean reward:  1.296 [-3.000, 31.990], mean action: 3.960 [0.000, 19.000],  loss: 0.024848, mae: 0.375967, mean_q: 0.572428, mean_eps: 0.000000
 4058/5000: episode: 167, duration: 0.278s, episode steps:  19, steps per second:  68, episode reward: 38.148, mean reward:  2.008 [-2.588, 32.412], mean action: 4.158 [0.000, 15.000],  loss: 0.022226, mae: 0.360414, mean_q: 0.593459, mean_eps: 0.000000
 4079/5000: episode: 168, duration: 0.303s, episode steps:  21, steps per second:  69, episode reward: 38.911, mean reward:  1.853 [-2.298, 32.401], mean action: 4.238 [0.000, 15.000],  loss: 0.019902, mae: 0.343351, mean_q: 0.569824, mean_eps: 0.000000
 4091/5000: episode: 169, duration: 0.171s, episode steps:  12, steps per second:  70, episode reward: -47.260, mean reward: -3.938 [-32.365,  0.310], mean action: 7.083 [0.000, 15.000],  loss: 0.022454, mae: 0.363520, mean_q: 0.574526, mean_eps: 0.000000
 4108/5000: episode: 170, duration: 0.249s, episode steps:  17, steps per second:  68, episode reward: 38.401, mean reward:  2.259 [-2.903, 32.060], mean action: 3.941 [1.000, 15.000],  loss: 0.018968, mae: 0.340012, mean_q: 0.570373, mean_eps: 0.000000
 4135/5000: episode: 171, duration: 0.381s, episode steps:  27, steps per second:  71, episode reward: 36.000, mean reward:  1.333 [-2.326, 32.210], mean action: 5.778 [1.000, 15.000],  loss: 0.019008, mae: 0.337555, mean_q: 0.530924, mean_eps: 0.000000
 4155/5000: episode: 172, duration: 0.302s, episode steps:  20, steps per second:  66, episode reward: 40.810, mean reward:  2.040 [-2.571, 32.160], mean action: 5.200 [0.000, 19.000],  loss: 0.021410, mae: 0.357085, mean_q: 0.525347, mean_eps: 0.000000
 4173/5000: episode: 173, duration: 0.265s, episode steps:  18, steps per second:  68, episode reward: 38.844, mean reward:  2.158 [-3.000, 32.310], mean action: 4.111 [0.000, 19.000],  loss: 0.024774, mae: 0.374850, mean_q: 0.499075, mean_eps: 0.000000
 4192/5000: episode: 174, duration: 0.271s, episode steps:  19, steps per second:  70, episode reward: 36.000, mean reward:  1.895 [-2.361, 33.000], mean action: 4.526 [0.000, 19.000],  loss: 0.021885, mae: 0.359761, mean_q: 0.483189, mean_eps: 0.000000
 4228/5000: episode: 175, duration: 0.496s, episode steps:  36, steps per second:  73, episode reward: 36.000, mean reward:  1.000 [-2.319, 32.110], mean action: 2.556 [0.000, 19.000],  loss: 0.019001, mae: 0.342449, mean_q: 0.561221, mean_eps: 0.000000
 4279/5000: episode: 176, duration: 0.702s, episode steps:  51, steps per second:  73, episode reward: 32.904, mean reward:  0.645 [-3.000, 32.444], mean action: 2.627 [0.000, 19.000],  loss: 0.021317, mae: 0.363611, mean_q: 0.615699, mean_eps: 0.000000
 4303/5000: episode: 177, duration: 0.342s, episode steps:  24, steps per second:  70, episode reward: -32.240, mean reward: -1.343 [-32.080,  2.621], mean action: 4.292 [0.000, 16.000],  loss: 0.019351, mae: 0.351738, mean_q: 0.572928, mean_eps: 0.000000
 4328/5000: episode: 178, duration: 0.355s, episode steps:  25, steps per second:  70, episode reward: 35.547, mean reward:  1.422 [-2.593, 32.170], mean action: 6.040 [0.000, 19.000],  loss: 0.020785, mae: 0.370611, mean_q: 0.517103, mean_eps: 0.000000
 4350/5000: episode: 179, duration: 0.316s, episode steps:  22, steps per second:  70, episode reward: 35.581, mean reward:  1.617 [-2.586, 31.681], mean action: 4.000 [0.000, 15.000],  loss: 0.018582, mae: 0.354804, mean_q: 0.503078, mean_eps: 0.000000
 4369/5000: episode: 180, duration: 0.277s, episode steps:  19, steps per second:  69, episode reward: -35.790, mean reward: -1.884 [-32.008,  3.000], mean action: 7.368 [0.000, 15.000],  loss: 0.018217, mae: 0.344967, mean_q: 0.497872, mean_eps: 0.000000
 4390/5000: episode: 181, duration: 0.307s, episode steps:  21, steps per second:  68, episode reward: 35.254, mean reward:  1.679 [-2.789, 31.821], mean action: 3.381 [0.000, 16.000],  loss: 0.020403, mae: 0.353106, mean_q: 0.570066, mean_eps: 0.000000
 4414/5000: episode: 182, duration: 0.346s, episode steps:  24, steps per second:  69, episode reward: 36.000, mean reward:  1.500 [-2.398, 32.590], mean action: 2.875 [0.000, 16.000],  loss: 0.018713, mae: 0.343527, mean_q: 0.518100, mean_eps: 0.000000
 4449/5000: episode: 183, duration: 0.492s, episode steps:  35, steps per second:  71, episode reward: 35.685, mean reward:  1.020 [-3.000, 32.760], mean action: 6.486 [0.000, 20.000],  loss: 0.018724, mae: 0.342442, mean_q: 0.531350, mean_eps: 0.000000
 4473/5000: episode: 184, duration: 0.342s, episode steps:  24, steps per second:  70, episode reward: 47.331, mean reward:  1.972 [-0.033, 32.152], mean action: 1.500 [1.000, 2.000],  loss: 0.022529, mae: 0.361967, mean_q: 0.581360, mean_eps: 0.000000
 4495/5000: episode: 185, duration: 0.312s, episode steps:  22, steps per second:  71, episode reward: 35.891, mean reward:  1.631 [-2.437, 33.000], mean action: 4.545 [0.000, 16.000],  loss: 0.021247, mae: 0.359700, mean_q: 0.502802, mean_eps: 0.000000
 4520/5000: episode: 186, duration: 0.360s, episode steps:  25, steps per second:  69, episode reward: 36.000, mean reward:  1.440 [-2.482, 32.330], mean action: 3.800 [0.000, 16.000],  loss: 0.019202, mae: 0.355489, mean_q: 0.513937, mean_eps: 0.000000
 4550/5000: episode: 187, duration: 0.423s, episode steps:  30, steps per second:  71, episode reward: 32.368, mean reward:  1.079 [-2.332, 32.368], mean action: 5.100 [0.000, 20.000],  loss: 0.017750, mae: 0.346402, mean_q: 0.558967, mean_eps: 0.000000
 4569/5000: episode: 188, duration: 0.276s, episode steps:  19, steps per second:  69, episode reward: -36.000, mean reward: -1.895 [-33.000,  2.300], mean action: 6.789 [0.000, 16.000],  loss: 0.017847, mae: 0.357142, mean_q: 0.537806, mean_eps: 0.000000
 4594/5000: episode: 189, duration: 0.349s, episode steps:  25, steps per second:  72, episode reward: 33.000, mean reward:  1.320 [-2.521, 32.310], mean action: 5.280 [0.000, 16.000],  loss: 0.023696, mae: 0.368488, mean_q: 0.503841, mean_eps: 0.000000
 4610/5000: episode: 190, duration: 0.235s, episode steps:  16, steps per second:  68, episode reward: 41.611, mean reward:  2.601 [-2.177, 32.310], mean action: 2.375 [0.000, 16.000],  loss: 0.019316, mae: 0.349491, mean_q: 0.517167, mean_eps: 0.000000
 4643/5000: episode: 191, duration: 0.457s, episode steps:  33, steps per second:  72, episode reward: -35.440, mean reward: -1.074 [-32.595,  2.576], mean action: 5.909 [0.000, 16.000],  loss: 0.019133, mae: 0.349509, mean_q: 0.501791, mean_eps: 0.000000
 4703/5000: episode: 192, duration: 0.814s, episode steps:  60, steps per second:  74, episode reward: -35.520, mean reward: -0.592 [-32.087,  2.400], mean action: 5.200 [0.000, 19.000],  loss: 0.020931, mae: 0.354432, mean_q: 0.538820, mean_eps: 0.000000
 4725/5000: episode: 193, duration: 0.320s, episode steps:  22, steps per second:  69, episode reward: 35.015, mean reward:  1.592 [-2.370, 32.150], mean action: 5.091 [0.000, 15.000],  loss: 0.016901, mae: 0.338011, mean_q: 0.611673, mean_eps: 0.000000
 4752/5000: episode: 194, duration: 0.383s, episode steps:  27, steps per second:  70, episode reward: 32.676, mean reward:  1.210 [-2.571, 31.943], mean action: 4.889 [0.000, 19.000],  loss: 0.018904, mae: 0.344755, mean_q: 0.531000, mean_eps: 0.000000
 4775/5000: episode: 195, duration: 0.332s, episode steps:  23, steps per second:  69, episode reward: -35.310, mean reward: -1.535 [-32.319,  2.783], mean action: 5.435 [0.000, 15.000],  loss: 0.018063, mae: 0.344906, mean_q: 0.528292, mean_eps: 0.000000
 4793/5000: episode: 196, duration: 0.271s, episode steps:  18, steps per second:  66, episode reward: 38.382, mean reward:  2.132 [-3.000, 32.379], mean action: 4.833 [0.000, 19.000],  loss: 0.022469, mae: 0.358878, mean_q: 0.540757, mean_eps: 0.000000
 4813/5000: episode: 197, duration: 0.292s, episode steps:  20, steps per second:  68, episode reward: 38.225, mean reward:  1.911 [-2.373, 31.690], mean action: 3.550 [0.000, 19.000],  loss: 0.017595, mae: 0.344771, mean_q: 0.506118, mean_eps: 0.000000
 4830/5000: episode: 198, duration: 0.246s, episode steps:  17, steps per second:  69, episode reward: 38.346, mean reward:  2.256 [-2.411, 32.013], mean action: 3.706 [0.000, 19.000],  loss: 0.018559, mae: 0.354916, mean_q: 0.493462, mean_eps: 0.000000
 4858/5000: episode: 199, duration: 0.397s, episode steps:  28, steps per second:  70, episode reward: 35.063, mean reward:  1.252 [-2.441, 32.010], mean action: 6.107 [1.000, 19.000],  loss: 0.022497, mae: 0.362983, mean_q: 0.528574, mean_eps: 0.000000
 4884/5000: episode: 200, duration: 0.361s, episode steps:  26, steps per second:  72, episode reward: 32.847, mean reward:  1.263 [-2.689, 33.000], mean action: 5.269 [0.000, 19.000],  loss: 0.020883, mae: 0.358447, mean_q: 0.547055, mean_eps: 0.000000
 4901/5000: episode: 201, duration: 0.248s, episode steps:  17, steps per second:  69, episode reward: 36.000, mean reward:  2.118 [-2.205, 30.000], mean action: 4.588 [1.000, 16.000],  loss: 0.020832, mae: 0.357113, mean_q: 0.556315, mean_eps: 0.000000
 4929/5000: episode: 202, duration: 0.390s, episode steps:  28, steps per second:  72, episode reward: 32.125, mean reward:  1.147 [-2.461, 32.050], mean action: 5.679 [0.000, 16.000],  loss: 0.022102, mae: 0.367150, mean_q: 0.548892, mean_eps: 0.000000
 4948/5000: episode: 203, duration: 0.277s, episode steps:  19, steps per second:  68, episode reward: 35.517, mean reward:  1.869 [-3.000, 32.026], mean action: 4.368 [0.000, 12.000],  loss: 0.016868, mae: 0.339852, mean_q: 0.550583, mean_eps: 0.000000
 4968/5000: episode: 204, duration: 0.284s, episode steps:  20, steps per second:  70, episode reward: 35.232, mean reward:  1.762 [-2.903, 32.220], mean action: 4.500 [0.000, 13.000],  loss: 0.018083, mae: 0.351113, mean_q: 0.513631, mean_eps: 0.000000
 4996/5000: episode: 205, duration: 0.396s, episode steps:  28, steps per second:  71, episode reward: 32.806, mean reward:  1.172 [-3.000, 32.466], mean action: 3.679 [0.000, 16.000],  loss: 0.019634, mae: 0.354362, mean_q: 0.499850, mean_eps: 0.000000
done, took 67.116 seconds
DQN Evaluation: 7822 victories out of 9191 episodes
Training for 5000 steps ...
   37/5000: episode: 1, duration: 0.288s, episode steps:  37, steps per second: 129, episode reward: 35.136, mean reward:  0.950 [-3.000, 31.206], mean action: 2.432 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   62/5000: episode: 2, duration: 0.170s, episode steps:  25, steps per second: 147, episode reward: 40.691, mean reward:  1.628 [-3.000, 32.590], mean action: 7.040 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   96/5000: episode: 3, duration: 0.208s, episode steps:  34, steps per second: 163, episode reward: 38.363, mean reward:  1.128 [-2.124, 31.991], mean action: 2.500 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  114/5000: episode: 4, duration: 0.133s, episode steps:  18, steps per second: 135, episode reward: 41.294, mean reward:  2.294 [-2.901, 32.140], mean action: 3.611 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  134/5000: episode: 5, duration: 0.145s, episode steps:  20, steps per second: 138, episode reward: 41.121, mean reward:  2.056 [-2.878, 32.123], mean action: 3.350 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  166/5000: episode: 6, duration: 0.208s, episode steps:  32, steps per second: 154, episode reward: 35.227, mean reward:  1.101 [-2.941, 31.804], mean action: 4.594 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  198/5000: episode: 7, duration: 0.210s, episode steps:  32, steps per second: 152, episode reward: 43.136, mean reward:  1.348 [-2.158, 32.373], mean action: 5.969 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  250/5000: episode: 8, duration: 0.311s, episode steps:  52, steps per second: 167, episode reward: 34.683, mean reward:  0.667 [-3.000, 31.983], mean action: 3.692 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  274/5000: episode: 9, duration: 0.173s, episode steps:  24, steps per second: 139, episode reward: 41.549, mean reward:  1.731 [-2.611, 31.769], mean action: 2.875 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  291/5000: episode: 10, duration: 0.127s, episode steps:  17, steps per second: 134, episode reward: 45.000, mean reward:  2.647 [-2.146, 32.210], mean action: 2.882 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  313/5000: episode: 11, duration: 0.153s, episode steps:  22, steps per second: 144, episode reward: 44.120, mean reward:  2.005 [-2.421, 32.120], mean action: 5.136 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  332/5000: episode: 12, duration: 0.138s, episode steps:  19, steps per second: 137, episode reward: 42.000, mean reward:  2.211 [-2.118, 32.440], mean action: 3.105 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  357/5000: episode: 13, duration: 0.173s, episode steps:  25, steps per second: 144, episode reward: 46.457, mean reward:  1.858 [-0.846, 32.806], mean action: 1.160 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  385/5000: episode: 14, duration: 0.181s, episode steps:  28, steps per second: 155, episode reward: 38.618, mean reward:  1.379 [-2.217, 32.070], mean action: 2.571 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  414/5000: episode: 15, duration: 0.190s, episode steps:  29, steps per second: 153, episode reward: 38.551, mean reward:  1.329 [-2.124, 32.388], mean action: 3.448 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  445/5000: episode: 16, duration: 0.202s, episode steps:  31, steps per second: 153, episode reward: 41.415, mean reward:  1.336 [-2.716, 32.500], mean action: 3.194 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  467/5000: episode: 17, duration: 0.175s, episode steps:  22, steps per second: 125, episode reward: 39.000, mean reward:  1.773 [-3.000, 32.390], mean action: 6.409 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  503/5000: episode: 18, duration: 0.233s, episode steps:  36, steps per second: 155, episode reward: 44.090, mean reward:  1.225 [-2.145, 32.041], mean action: 1.222 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  527/5000: episode: 19, duration: 0.176s, episode steps:  24, steps per second: 136, episode reward: 41.375, mean reward:  1.724 [-2.217, 31.999], mean action: 7.542 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  565/5000: episode: 20, duration: 0.240s, episode steps:  38, steps per second: 158, episode reward: 35.852, mean reward:  0.943 [-2.543, 31.962], mean action: 4.395 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  612/5000: episode: 21, duration: 0.288s, episode steps:  47, steps per second: 163, episode reward: 35.659, mean reward:  0.759 [-2.817, 32.940], mean action: 6.043 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  630/5000: episode: 22, duration: 0.128s, episode steps:  18, steps per second: 140, episode reward: 38.675, mean reward:  2.149 [-2.647, 32.380], mean action: 3.722 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  656/5000: episode: 23, duration: 0.169s, episode steps:  26, steps per second: 154, episode reward: 34.946, mean reward:  1.344 [-2.840, 31.310], mean action: 5.423 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  673/5000: episode: 24, duration: 0.128s, episode steps:  17, steps per second: 133, episode reward: 39.000, mean reward:  2.294 [-2.107, 30.938], mean action: 2.353 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  724/5000: episode: 25, duration: 0.438s, episode steps:  51, steps per second: 116, episode reward: 32.519, mean reward:  0.638 [-3.000, 31.640], mean action: 6.667 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  773/5000: episode: 26, duration: 0.298s, episode steps:  49, steps per second: 165, episode reward: 41.476, mean reward:  0.846 [-2.194, 32.550], mean action: 3.347 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  801/5000: episode: 27, duration: 0.181s, episode steps:  28, steps per second: 155, episode reward: 37.960, mean reward:  1.356 [-3.000, 32.001], mean action: 6.929 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  825/5000: episode: 28, duration: 0.169s, episode steps:  24, steps per second: 142, episode reward: 41.981, mean reward:  1.749 [-3.000, 32.140], mean action: 3.042 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  847/5000: episode: 29, duration: 0.149s, episode steps:  22, steps per second: 148, episode reward: 38.003, mean reward:  1.727 [-2.896, 32.230], mean action: 5.045 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  867/5000: episode: 30, duration: 0.148s, episode steps:  20, steps per second: 135, episode reward: 44.146, mean reward:  2.207 [-2.041, 32.447], mean action: 2.100 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  890/5000: episode: 31, duration: 0.162s, episode steps:  23, steps per second: 142, episode reward: 44.505, mean reward:  1.935 [-2.103, 31.976], mean action: 2.609 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  925/5000: episode: 32, duration: 0.222s, episode steps:  35, steps per second: 158, episode reward: 38.317, mean reward:  1.095 [-3.000, 32.090], mean action: 2.371 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  962/5000: episode: 33, duration: 0.252s, episode steps:  37, steps per second: 147, episode reward: 41.070, mean reward:  1.110 [-2.175, 32.330], mean action: 2.135 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  980/5000: episode: 34, duration: 0.139s, episode steps:  18, steps per second: 130, episode reward: 45.000, mean reward:  2.500 [-2.256, 32.670], mean action: 3.333 [1.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1022/5000: episode: 35, duration: 0.414s, episode steps:  42, steps per second: 102, episode reward: -35.850, mean reward: -0.854 [-32.323,  2.390], mean action: 7.476 [0.000, 20.000],  loss: 0.021291, mae: 0.367124, mean_q: 0.535342, mean_eps: 0.000000
 1046/5000: episode: 36, duration: 0.355s, episode steps:  24, steps per second:  68, episode reward: 42.000, mean reward:  1.750 [-2.871, 32.370], mean action: 2.583 [0.000, 16.000],  loss: 0.022451, mae: 0.368088, mean_q: 0.586506, mean_eps: 0.000000
 1065/5000: episode: 37, duration: 0.276s, episode steps:  19, steps per second:  69, episode reward: 44.202, mean reward:  2.326 [-2.380, 32.065], mean action: 3.895 [0.000, 16.000],  loss: 0.021041, mae: 0.359703, mean_q: 0.603333, mean_eps: 0.000000
 1086/5000: episode: 38, duration: 0.309s, episode steps:  21, steps per second:  68, episode reward: 41.900, mean reward:  1.995 [-2.225, 32.240], mean action: 0.952 [0.000, 15.000],  loss: 0.022636, mae: 0.375548, mean_q: 0.630578, mean_eps: 0.000000
 1105/5000: episode: 39, duration: 0.296s, episode steps:  19, steps per second:  64, episode reward: 44.972, mean reward:  2.367 [-2.522, 32.102], mean action: 2.526 [0.000, 15.000],  loss: 0.020553, mae: 0.354248, mean_q: 0.531754, mean_eps: 0.000000
 1128/5000: episode: 40, duration: 0.417s, episode steps:  23, steps per second:  55, episode reward: 40.928, mean reward:  1.779 [-2.625, 31.996], mean action: 3.870 [0.000, 15.000],  loss: 0.021737, mae: 0.362227, mean_q: 0.546393, mean_eps: 0.000000
 1152/5000: episode: 41, duration: 0.348s, episode steps:  24, steps per second:  69, episode reward: 41.121, mean reward:  1.713 [-2.393, 32.610], mean action: 4.250 [1.000, 15.000],  loss: 0.022515, mae: 0.363195, mean_q: 0.585161, mean_eps: 0.000000
 1176/5000: episode: 42, duration: 0.343s, episode steps:  24, steps per second:  70, episode reward: 44.210, mean reward:  1.842 [-2.109, 31.548], mean action: 2.167 [0.000, 15.000],  loss: 0.022600, mae: 0.365320, mean_q: 0.592387, mean_eps: 0.000000
 1202/5000: episode: 43, duration: 0.370s, episode steps:  26, steps per second:  70, episode reward: 41.370, mean reward:  1.591 [-2.194, 32.073], mean action: 2.846 [0.000, 15.000],  loss: 0.021664, mae: 0.363976, mean_q: 0.636867, mean_eps: 0.000000
 1245/5000: episode: 44, duration: 0.594s, episode steps:  43, steps per second:  72, episode reward: 38.449, mean reward:  0.894 [-3.000, 32.130], mean action: 2.930 [0.000, 15.000],  loss: 0.021081, mae: 0.360489, mean_q: 0.548609, mean_eps: 0.000000
 1266/5000: episode: 45, duration: 0.304s, episode steps:  21, steps per second:  69, episode reward: 47.778, mean reward:  2.275 [-0.188, 32.480], mean action: 4.667 [2.000, 15.000],  loss: 0.020159, mae: 0.348426, mean_q: 0.461071, mean_eps: 0.000000
 1286/5000: episode: 46, duration: 0.291s, episode steps:  20, steps per second:  69, episode reward: 44.109, mean reward:  2.205 [-2.502, 32.470], mean action: 2.000 [0.000, 19.000],  loss: 0.025962, mae: 0.381895, mean_q: 0.462451, mean_eps: 0.000000
 1298/5000: episode: 47, duration: 0.188s, episode steps:  12, steps per second:  64, episode reward: 47.133, mean reward:  3.928 [ 0.000, 32.904], mean action: 1.500 [0.000, 3.000],  loss: 0.019994, mae: 0.343072, mean_q: 0.543820, mean_eps: 0.000000
 1319/5000: episode: 48, duration: 0.306s, episode steps:  21, steps per second:  69, episode reward: 41.222, mean reward:  1.963 [-2.825, 32.070], mean action: 2.905 [0.000, 15.000],  loss: 0.019090, mae: 0.343690, mean_q: 0.510260, mean_eps: 0.000000
 1341/5000: episode: 49, duration: 0.321s, episode steps:  22, steps per second:  69, episode reward: 44.239, mean reward:  2.011 [-2.221, 32.110], mean action: 2.864 [0.000, 15.000],  loss: 0.016874, mae: 0.336443, mean_q: 0.505210, mean_eps: 0.000000
 1371/5000: episode: 50, duration: 0.482s, episode steps:  30, steps per second:  62, episode reward: 38.377, mean reward:  1.279 [-2.970, 32.233], mean action: 5.300 [0.000, 19.000],  loss: 0.018603, mae: 0.347778, mean_q: 0.523375, mean_eps: 0.000000
 1398/5000: episode: 51, duration: 0.525s, episode steps:  27, steps per second:  51, episode reward: 35.448, mean reward:  1.313 [-2.825, 32.390], mean action: 6.556 [0.000, 19.000],  loss: 0.021778, mae: 0.364993, mean_q: 0.530553, mean_eps: 0.000000
 1438/5000: episode: 52, duration: 0.609s, episode steps:  40, steps per second:  66, episode reward: 35.668, mean reward:  0.892 [-2.885, 29.790], mean action: 3.725 [0.000, 20.000],  loss: 0.021996, mae: 0.363939, mean_q: 0.539690, mean_eps: 0.000000
 1470/5000: episode: 53, duration: 0.452s, episode steps:  32, steps per second:  71, episode reward: 38.721, mean reward:  1.210 [-2.182, 32.340], mean action: 3.531 [0.000, 15.000],  loss: 0.022949, mae: 0.367050, mean_q: 0.540931, mean_eps: 0.000000
 1494/5000: episode: 54, duration: 0.341s, episode steps:  24, steps per second:  70, episode reward: 38.737, mean reward:  1.614 [-3.000, 32.490], mean action: 4.125 [1.000, 19.000],  loss: 0.020858, mae: 0.363006, mean_q: 0.583651, mean_eps: 0.000000
 1511/5000: episode: 55, duration: 0.253s, episode steps:  17, steps per second:  67, episode reward: 41.227, mean reward:  2.425 [-2.494, 32.330], mean action: 4.941 [1.000, 19.000],  loss: 0.018292, mae: 0.349795, mean_q: 0.582506, mean_eps: 0.000000
 1534/5000: episode: 56, duration: 0.332s, episode steps:  23, steps per second:  69, episode reward: 43.632, mean reward:  1.897 [-3.000, 31.943], mean action: 4.000 [0.000, 19.000],  loss: 0.020055, mae: 0.354908, mean_q: 0.506392, mean_eps: 0.000000
 1562/5000: episode: 57, duration: 0.404s, episode steps:  28, steps per second:  69, episode reward: 43.463, mean reward:  1.552 [-2.714, 31.990], mean action: 2.464 [0.000, 8.000],  loss: 0.019116, mae: 0.349415, mean_q: 0.465234, mean_eps: 0.000000
 1595/5000: episode: 58, duration: 0.464s, episode steps:  33, steps per second:  71, episode reward: 35.460, mean reward:  1.075 [-3.000, 32.280], mean action: 3.030 [0.000, 15.000],  loss: 0.018200, mae: 0.342777, mean_q: 0.483207, mean_eps: 0.000000
 1623/5000: episode: 59, duration: 0.397s, episode steps:  28, steps per second:  71, episode reward: 36.806, mean reward:  1.315 [-2.188, 32.007], mean action: 3.821 [0.000, 19.000],  loss: 0.018537, mae: 0.347212, mean_q: 0.558949, mean_eps: 0.000000
 1642/5000: episode: 60, duration: 0.283s, episode steps:  19, steps per second:  67, episode reward: 38.532, mean reward:  2.028 [-3.000, 32.790], mean action: 3.000 [0.000, 16.000],  loss: 0.019739, mae: 0.347970, mean_q: 0.543555, mean_eps: 0.000000
 1663/5000: episode: 61, duration: 0.304s, episode steps:  21, steps per second:  69, episode reward: 41.389, mean reward:  1.971 [-2.012, 32.050], mean action: 1.190 [0.000, 10.000],  loss: 0.022022, mae: 0.353651, mean_q: 0.504425, mean_eps: 0.000000
 1699/5000: episode: 62, duration: 0.496s, episode steps:  36, steps per second:  73, episode reward: 35.181, mean reward:  0.977 [-2.425, 32.380], mean action: 5.306 [0.000, 14.000],  loss: 0.022304, mae: 0.355899, mean_q: 0.501155, mean_eps: 0.000000
 1733/5000: episode: 63, duration: 0.491s, episode steps:  34, steps per second:  69, episode reward: 41.696, mean reward:  1.226 [-2.340, 32.220], mean action: 3.324 [0.000, 13.000],  loss: 0.022581, mae: 0.358337, mean_q: 0.565153, mean_eps: 0.000000
 1768/5000: episode: 64, duration: 0.479s, episode steps:  35, steps per second:  73, episode reward: -32.760, mean reward: -0.936 [-32.400,  2.760], mean action: 7.400 [0.000, 18.000],  loss: 0.021452, mae: 0.352126, mean_q: 0.548100, mean_eps: 0.000000
 1802/5000: episode: 65, duration: 0.499s, episode steps:  34, steps per second:  68, episode reward: 40.745, mean reward:  1.198 [-3.000, 31.934], mean action: 3.294 [0.000, 16.000],  loss: 0.017638, mae: 0.335552, mean_q: 0.564270, mean_eps: 0.000000
 1829/5000: episode: 66, duration: 0.378s, episode steps:  27, steps per second:  72, episode reward: 37.712, mean reward:  1.397 [-2.667, 31.286], mean action: 4.074 [0.000, 16.000],  loss: 0.021207, mae: 0.354685, mean_q: 0.578862, mean_eps: 0.000000
 1864/5000: episode: 67, duration: 0.499s, episode steps:  35, steps per second:  70, episode reward: 38.788, mean reward:  1.108 [-2.638, 32.120], mean action: 2.971 [0.000, 16.000],  loss: 0.022412, mae: 0.354365, mean_q: 0.588584, mean_eps: 0.000000
 1892/5000: episode: 68, duration: 0.395s, episode steps:  28, steps per second:  71, episode reward: 38.768, mean reward:  1.385 [-2.956, 32.080], mean action: 2.750 [0.000, 16.000],  loss: 0.020017, mae: 0.345286, mean_q: 0.563363, mean_eps: 0.000000
 1920/5000: episode: 69, duration: 0.388s, episode steps:  28, steps per second:  72, episode reward: -32.950, mean reward: -1.177 [-32.596,  2.962], mean action: 7.429 [1.000, 19.000],  loss: 0.023199, mae: 0.356619, mean_q: 0.551103, mean_eps: 0.000000
 1937/5000: episode: 70, duration: 0.247s, episode steps:  17, steps per second:  69, episode reward: 47.200, mean reward:  2.776 [-0.060, 32.540], mean action: 2.059 [0.000, 3.000],  loss: 0.018595, mae: 0.335677, mean_q: 0.605483, mean_eps: 0.000000
 1973/5000: episode: 71, duration: 0.507s, episode steps:  36, steps per second:  71, episode reward: 34.862, mean reward:  0.968 [-2.897, 31.954], mean action: 4.861 [0.000, 19.000],  loss: 0.019684, mae: 0.349922, mean_q: 0.575346, mean_eps: 0.000000
 1993/5000: episode: 72, duration: 0.291s, episode steps:  20, steps per second:  69, episode reward: 47.498, mean reward:  2.375 [ 0.001, 32.180], mean action: 2.400 [0.000, 3.000],  loss: 0.019910, mae: 0.349705, mean_q: 0.532897, mean_eps: 0.000000
 2011/5000: episode: 73, duration: 0.276s, episode steps:  18, steps per second:  65, episode reward: 41.943, mean reward:  2.330 [-2.498, 32.173], mean action: 2.722 [0.000, 19.000],  loss: 0.021586, mae: 0.349023, mean_q: 0.584281, mean_eps: 0.000000
 2044/5000: episode: 74, duration: 0.460s, episode steps:  33, steps per second:  72, episode reward: 35.646, mean reward:  1.080 [-2.466, 31.884], mean action: 2.667 [0.000, 9.000],  loss: 0.020718, mae: 0.348344, mean_q: 0.589072, mean_eps: 0.000000
 2066/5000: episode: 75, duration: 0.315s, episode steps:  22, steps per second:  70, episode reward: 41.158, mean reward:  1.871 [-3.000, 32.250], mean action: 2.909 [0.000, 12.000],  loss: 0.022897, mae: 0.357190, mean_q: 0.536829, mean_eps: 0.000000
 2090/5000: episode: 76, duration: 0.348s, episode steps:  24, steps per second:  69, episode reward: 44.168, mean reward:  1.840 [-2.198, 32.159], mean action: 1.625 [1.000, 9.000],  loss: 0.018203, mae: 0.342302, mean_q: 0.521329, mean_eps: 0.000000
 2124/5000: episode: 77, duration: 0.475s, episode steps:  34, steps per second:  72, episode reward: 41.617, mean reward:  1.224 [-2.324, 31.881], mean action: 3.676 [0.000, 19.000],  loss: 0.020249, mae: 0.353504, mean_q: 0.556818, mean_eps: 0.000000
 2161/5000: episode: 78, duration: 0.519s, episode steps:  37, steps per second:  71, episode reward: 38.153, mean reward:  1.031 [-2.878, 32.152], mean action: 3.297 [0.000, 19.000],  loss: 0.019753, mae: 0.361718, mean_q: 0.572176, mean_eps: 0.000000
 2202/5000: episode: 79, duration: 0.560s, episode steps:  41, steps per second:  73, episode reward: 35.377, mean reward:  0.863 [-2.725, 32.040], mean action: 4.463 [0.000, 19.000],  loss: 0.018453, mae: 0.342778, mean_q: 0.549573, mean_eps: 0.000000
 2225/5000: episode: 80, duration: 0.328s, episode steps:  23, steps per second:  70, episode reward: 41.841, mean reward:  1.819 [-2.841, 32.101], mean action: 2.217 [0.000, 9.000],  loss: 0.018464, mae: 0.344661, mean_q: 0.582375, mean_eps: 0.000000
 2254/5000: episode: 81, duration: 0.407s, episode steps:  29, steps per second:  71, episode reward: 38.722, mean reward:  1.335 [-2.468, 32.130], mean action: 2.828 [0.000, 12.000],  loss: 0.018729, mae: 0.353962, mean_q: 0.558725, mean_eps: 0.000000
 2279/5000: episode: 82, duration: 0.356s, episode steps:  25, steps per second:  70, episode reward: 44.304, mean reward:  1.772 [-2.022, 32.550], mean action: 3.760 [0.000, 12.000],  loss: 0.020738, mae: 0.363389, mean_q: 0.496025, mean_eps: 0.000000
 2294/5000: episode: 83, duration: 0.223s, episode steps:  15, steps per second:  67, episode reward: 47.151, mean reward:  3.143 [-0.228, 33.000], mean action: 3.333 [1.000, 14.000],  loss: 0.024770, mae: 0.380541, mean_q: 0.559065, mean_eps: 0.000000
 2318/5000: episode: 84, duration: 0.354s, episode steps:  24, steps per second:  68, episode reward: 38.900, mean reward:  1.621 [-2.464, 32.900], mean action: 1.708 [0.000, 11.000],  loss: 0.019245, mae: 0.352961, mean_q: 0.556827, mean_eps: 0.000000
 2348/5000: episode: 85, duration: 0.427s, episode steps:  30, steps per second:  70, episode reward: 38.750, mean reward:  1.292 [-2.387, 32.130], mean action: 2.000 [0.000, 16.000],  loss: 0.019124, mae: 0.357817, mean_q: 0.571854, mean_eps: 0.000000
 2378/5000: episode: 86, duration: 0.426s, episode steps:  30, steps per second:  70, episode reward: 42.000, mean reward:  1.400 [-2.696, 29.931], mean action: 2.667 [1.000, 11.000],  loss: 0.020913, mae: 0.364070, mean_q: 0.519137, mean_eps: 0.000000
 2404/5000: episode: 87, duration: 0.419s, episode steps:  26, steps per second:  62, episode reward: 42.000, mean reward:  1.615 [-2.238, 30.106], mean action: 2.769 [1.000, 11.000],  loss: 0.018916, mae: 0.354010, mean_q: 0.525671, mean_eps: 0.000000
 2439/5000: episode: 88, duration: 0.606s, episode steps:  35, steps per second:  58, episode reward: 35.875, mean reward:  1.025 [-3.000, 32.040], mean action: 2.629 [0.000, 11.000],  loss: 0.018000, mae: 0.344717, mean_q: 0.539526, mean_eps: 0.000000
 2464/5000: episode: 89, duration: 0.434s, episode steps:  25, steps per second:  58, episode reward: 41.736, mean reward:  1.669 [-2.594, 32.740], mean action: 4.120 [2.000, 20.000],  loss: 0.017297, mae: 0.339638, mean_q: 0.546240, mean_eps: 0.000000
 2496/5000: episode: 90, duration: 0.459s, episode steps:  32, steps per second:  70, episode reward: 35.215, mean reward:  1.100 [-3.000, 32.080], mean action: 3.469 [0.000, 20.000],  loss: 0.019992, mae: 0.349883, mean_q: 0.522421, mean_eps: 0.000000
 2518/5000: episode: 91, duration: 0.330s, episode steps:  22, steps per second:  67, episode reward: 38.639, mean reward:  1.756 [-2.900, 31.909], mean action: 2.045 [0.000, 12.000],  loss: 0.019045, mae: 0.346249, mean_q: 0.604839, mean_eps: 0.000000
 2540/5000: episode: 92, duration: 0.326s, episode steps:  22, steps per second:  67, episode reward: 40.528, mean reward:  1.842 [-2.461, 32.042], mean action: 3.000 [0.000, 19.000],  loss: 0.018622, mae: 0.347815, mean_q: 0.522215, mean_eps: 0.000000
 2558/5000: episode: 93, duration: 0.276s, episode steps:  18, steps per second:  65, episode reward: 47.057, mean reward:  2.614 [-0.355, 33.213], mean action: 1.889 [0.000, 20.000],  loss: 0.017890, mae: 0.344457, mean_q: 0.533101, mean_eps: 0.000000
 2582/5000: episode: 94, duration: 0.369s, episode steps:  24, steps per second:  65, episode reward: 38.558, mean reward:  1.607 [-2.498, 32.175], mean action: 2.375 [0.000, 11.000],  loss: 0.018164, mae: 0.356419, mean_q: 0.476311, mean_eps: 0.000000
 2614/5000: episode: 95, duration: 0.549s, episode steps:  32, steps per second:  58, episode reward: 44.342, mean reward:  1.386 [-2.031, 32.220], mean action: 2.500 [1.000, 14.000],  loss: 0.017917, mae: 0.350876, mean_q: 0.505999, mean_eps: 0.000000
 2643/5000: episode: 96, duration: 0.470s, episode steps:  29, steps per second:  62, episode reward: 35.140, mean reward:  1.212 [-2.518, 32.073], mean action: 5.310 [0.000, 21.000],  loss: 0.019505, mae: 0.352868, mean_q: 0.520559, mean_eps: 0.000000
 2678/5000: episode: 97, duration: 0.515s, episode steps:  35, steps per second:  68, episode reward: 40.625, mean reward:  1.161 [-2.253, 31.746], mean action: 5.857 [0.000, 20.000],  loss: 0.020004, mae: 0.353036, mean_q: 0.478373, mean_eps: 0.000000
 2698/5000: episode: 98, duration: 0.291s, episode steps:  20, steps per second:  69, episode reward: 44.219, mean reward:  2.211 [-2.378, 32.041], mean action: 1.200 [0.000, 3.000],  loss: 0.023271, mae: 0.362954, mean_q: 0.467551, mean_eps: 0.000000
 2729/5000: episode: 99, duration: 0.442s, episode steps:  31, steps per second:  70, episode reward: 46.505, mean reward:  1.500 [-0.684, 32.470], mean action: 4.065 [0.000, 19.000],  loss: 0.018973, mae: 0.346355, mean_q: 0.540861, mean_eps: 0.000000
 2765/5000: episode: 100, duration: 0.510s, episode steps:  36, steps per second:  71, episode reward: 43.544, mean reward:  1.210 [-2.154, 32.100], mean action: 2.528 [0.000, 14.000],  loss: 0.019137, mae: 0.347653, mean_q: 0.517625, mean_eps: 0.000000
 2793/5000: episode: 101, duration: 0.408s, episode steps:  28, steps per second:  69, episode reward: 41.416, mean reward:  1.479 [-2.146, 31.889], mean action: 3.321 [0.000, 12.000],  loss: 0.019133, mae: 0.342280, mean_q: 0.490214, mean_eps: 0.000000
 2821/5000: episode: 102, duration: 0.399s, episode steps:  28, steps per second:  70, episode reward: 43.446, mean reward:  1.552 [-2.241, 32.310], mean action: 4.464 [0.000, 20.000],  loss: 0.022427, mae: 0.364163, mean_q: 0.524217, mean_eps: 0.000000
 2870/5000: episode: 103, duration: 0.740s, episode steps:  49, steps per second:  66, episode reward: 41.494, mean reward:  0.847 [-2.202, 32.120], mean action: 2.714 [0.000, 12.000],  loss: 0.018729, mae: 0.340617, mean_q: 0.538950, mean_eps: 0.000000
 2914/5000: episode: 104, duration: 0.608s, episode steps:  44, steps per second:  72, episode reward: 41.056, mean reward:  0.933 [-2.939, 32.100], mean action: 2.682 [0.000, 19.000],  loss: 0.016681, mae: 0.336072, mean_q: 0.502746, mean_eps: 0.000000
 2938/5000: episode: 105, duration: 0.349s, episode steps:  24, steps per second:  69, episode reward: 41.112, mean reward:  1.713 [-2.109, 31.854], mean action: 3.167 [0.000, 19.000],  loss: 0.018038, mae: 0.340821, mean_q: 0.540323, mean_eps: 0.000000
 2992/5000: episode: 106, duration: 0.818s, episode steps:  54, steps per second:  66, episode reward: 37.973, mean reward:  0.703 [-2.301, 32.150], mean action: 2.963 [0.000, 19.000],  loss: 0.019070, mae: 0.339831, mean_q: 0.557874, mean_eps: 0.000000
 3016/5000: episode: 107, duration: 0.345s, episode steps:  24, steps per second:  70, episode reward: 44.008, mean reward:  1.834 [-2.170, 31.788], mean action: 2.417 [0.000, 19.000],  loss: 0.018588, mae: 0.342298, mean_q: 0.556528, mean_eps: 0.000000
 3038/5000: episode: 108, duration: 0.327s, episode steps:  22, steps per second:  67, episode reward: 41.292, mean reward:  1.877 [-2.314, 32.080], mean action: 3.318 [0.000, 19.000],  loss: 0.018572, mae: 0.342964, mean_q: 0.531490, mean_eps: 0.000000
 3061/5000: episode: 109, duration: 0.331s, episode steps:  23, steps per second:  69, episode reward: 35.678, mean reward:  1.551 [-3.000, 32.213], mean action: 4.826 [0.000, 19.000],  loss: 0.020983, mae: 0.357546, mean_q: 0.528572, mean_eps: 0.000000
 3104/5000: episode: 110, duration: 0.592s, episode steps:  43, steps per second:  73, episode reward: 35.235, mean reward:  0.819 [-3.000, 32.220], mean action: 4.860 [0.000, 19.000],  loss: 0.023203, mae: 0.370527, mean_q: 0.567670, mean_eps: 0.000000
 3129/5000: episode: 111, duration: 0.351s, episode steps:  25, steps per second:  71, episode reward: 34.934, mean reward:  1.397 [-2.438, 32.140], mean action: 4.080 [0.000, 19.000],  loss: 0.018296, mae: 0.346109, mean_q: 0.559494, mean_eps: 0.000000
 3167/5000: episode: 112, duration: 0.526s, episode steps:  38, steps per second:  72, episode reward: 38.711, mean reward:  1.019 [-2.095, 32.650], mean action: 2.368 [0.000, 16.000],  loss: 0.020104, mae: 0.347642, mean_q: 0.565590, mean_eps: 0.000000
 3189/5000: episode: 113, duration: 0.314s, episode steps:  22, steps per second:  70, episode reward: 38.699, mean reward:  1.759 [-2.375, 32.210], mean action: 5.045 [0.000, 16.000],  loss: 0.022906, mae: 0.363441, mean_q: 0.546721, mean_eps: 0.000000
 3211/5000: episode: 114, duration: 0.312s, episode steps:  22, steps per second:  70, episode reward: 38.901, mean reward:  1.768 [-2.178, 32.221], mean action: 3.409 [0.000, 12.000],  loss: 0.020434, mae: 0.355010, mean_q: 0.555974, mean_eps: 0.000000
 3236/5000: episode: 115, duration: 0.359s, episode steps:  25, steps per second:  70, episode reward: 47.367, mean reward:  1.895 [-0.194, 32.442], mean action: 1.480 [1.000, 7.000],  loss: 0.019942, mae: 0.344179, mean_q: 0.547477, mean_eps: 0.000000
 3255/5000: episode: 116, duration: 0.374s, episode steps:  19, steps per second:  51, episode reward: 39.000, mean reward:  2.053 [-2.420, 32.180], mean action: 4.526 [0.000, 16.000],  loss: 0.014394, mae: 0.324594, mean_q: 0.568871, mean_eps: 0.000000
 3295/5000: episode: 117, duration: 0.542s, episode steps:  40, steps per second:  74, episode reward: 35.664, mean reward:  0.892 [-2.354, 32.434], mean action: 2.975 [0.000, 16.000],  loss: 0.019531, mae: 0.346937, mean_q: 0.613136, mean_eps: 0.000000
 3314/5000: episode: 118, duration: 0.275s, episode steps:  19, steps per second:  69, episode reward: 42.000, mean reward:  2.211 [-2.788, 32.170], mean action: 2.632 [0.000, 16.000],  loss: 0.023804, mae: 0.369653, mean_q: 0.601994, mean_eps: 0.000000
 3353/5000: episode: 119, duration: 0.539s, episode steps:  39, steps per second:  72, episode reward: -33.000, mean reward: -0.846 [-32.219,  2.710], mean action: 9.821 [0.000, 18.000],  loss: 0.021847, mae: 0.361663, mean_q: 0.586253, mean_eps: 0.000000
 3373/5000: episode: 120, duration: 0.432s, episode steps:  20, steps per second:  46, episode reward: 44.290, mean reward:  2.215 [-2.619, 31.485], mean action: 5.400 [1.000, 14.000],  loss: 0.026369, mae: 0.380723, mean_q: 0.634182, mean_eps: 0.000000
 3409/5000: episode: 121, duration: 0.588s, episode steps:  36, steps per second:  61, episode reward: 41.309, mean reward:  1.147 [-2.324, 31.788], mean action: 4.194 [0.000, 20.000],  loss: 0.018593, mae: 0.346716, mean_q: 0.524756, mean_eps: 0.000000
 3443/5000: episode: 122, duration: 0.480s, episode steps:  34, steps per second:  71, episode reward: 41.450, mean reward:  1.219 [-2.224, 32.092], mean action: 2.529 [0.000, 16.000],  loss: 0.017975, mae: 0.352331, mean_q: 0.512591, mean_eps: 0.000000
 3478/5000: episode: 123, duration: 0.485s, episode steps:  35, steps per second:  72, episode reward: 43.472, mean reward:  1.242 [-2.500, 32.140], mean action: 2.771 [0.000, 12.000],  loss: 0.020399, mae: 0.352128, mean_q: 0.509515, mean_eps: 0.000000
 3502/5000: episode: 124, duration: 0.344s, episode steps:  24, steps per second:  70, episode reward: 38.614, mean reward:  1.609 [-3.000, 32.004], mean action: 2.833 [0.000, 19.000],  loss: 0.017989, mae: 0.339599, mean_q: 0.547478, mean_eps: 0.000000
 3529/5000: episode: 125, duration: 0.373s, episode steps:  27, steps per second:  72, episode reward: 38.903, mean reward:  1.441 [-2.701, 32.003], mean action: 2.333 [0.000, 19.000],  loss: 0.024247, mae: 0.378284, mean_q: 0.518176, mean_eps: 0.000000
 3572/5000: episode: 126, duration: 0.587s, episode steps:  43, steps per second:  73, episode reward: 38.509, mean reward:  0.896 [-3.000, 32.071], mean action: 5.581 [0.000, 20.000],  loss: 0.023006, mae: 0.365503, mean_q: 0.545471, mean_eps: 0.000000
 3594/5000: episode: 127, duration: 0.317s, episode steps:  22, steps per second:  69, episode reward: 42.000, mean reward:  1.909 [-2.399, 32.160], mean action: 2.682 [0.000, 19.000],  loss: 0.021627, mae: 0.357977, mean_q: 0.574011, mean_eps: 0.000000
 3630/5000: episode: 128, duration: 0.514s, episode steps:  36, steps per second:  70, episode reward: 40.772, mean reward:  1.133 [-2.227, 32.100], mean action: 3.389 [0.000, 15.000],  loss: 0.021833, mae: 0.360771, mean_q: 0.513645, mean_eps: 0.000000
 3660/5000: episode: 129, duration: 0.415s, episode steps:  30, steps per second:  72, episode reward: 36.000, mean reward:  1.200 [-2.805, 32.350], mean action: 3.300 [0.000, 19.000],  loss: 0.021794, mae: 0.361878, mean_q: 0.543695, mean_eps: 0.000000
 3679/5000: episode: 130, duration: 0.278s, episode steps:  19, steps per second:  68, episode reward: 47.245, mean reward:  2.487 [-0.060, 32.641], mean action: 2.684 [0.000, 14.000],  loss: 0.017592, mae: 0.333915, mean_q: 0.518944, mean_eps: 0.000000
 3699/5000: episode: 131, duration: 0.299s, episode steps:  20, steps per second:  67, episode reward: 38.455, mean reward:  1.923 [-3.000, 32.646], mean action: 5.550 [1.000, 19.000],  loss: 0.022079, mae: 0.356940, mean_q: 0.535727, mean_eps: 0.000000
 3728/5000: episode: 132, duration: 0.408s, episode steps:  29, steps per second:  71, episode reward: 38.137, mean reward:  1.315 [-3.000, 32.190], mean action: 5.414 [0.000, 19.000],  loss: 0.020986, mae: 0.348441, mean_q: 0.525198, mean_eps: 0.000000
 3744/5000: episode: 133, duration: 0.237s, episode steps:  16, steps per second:  67, episode reward: 41.903, mean reward:  2.619 [-3.000, 32.593], mean action: 3.500 [0.000, 19.000],  loss: 0.023834, mae: 0.367289, mean_q: 0.544736, mean_eps: 0.000000
 3762/5000: episode: 134, duration: 0.264s, episode steps:  18, steps per second:  68, episode reward: 41.112, mean reward:  2.284 [-3.000, 32.404], mean action: 2.778 [0.000, 19.000],  loss: 0.021465, mae: 0.355579, mean_q: 0.548491, mean_eps: 0.000000
 3784/5000: episode: 135, duration: 0.320s, episode steps:  22, steps per second:  69, episode reward: 38.943, mean reward:  1.770 [-2.722, 32.283], mean action: 4.273 [0.000, 19.000],  loss: 0.021018, mae: 0.350582, mean_q: 0.519688, mean_eps: 0.000000
 3806/5000: episode: 136, duration: 0.309s, episode steps:  22, steps per second:  71, episode reward: 41.909, mean reward:  1.905 [-2.663, 32.110], mean action: 2.045 [0.000, 11.000],  loss: 0.018118, mae: 0.336699, mean_q: 0.480357, mean_eps: 0.000000
 3834/5000: episode: 137, duration: 0.396s, episode steps:  28, steps per second:  71, episode reward: 35.437, mean reward:  1.266 [-2.584, 32.027], mean action: 4.786 [0.000, 20.000],  loss: 0.019778, mae: 0.346938, mean_q: 0.552343, mean_eps: 0.000000
 3856/5000: episode: 138, duration: 0.319s, episode steps:  22, steps per second:  69, episode reward: 44.038, mean reward:  2.002 [-2.161, 32.200], mean action: 3.136 [0.000, 14.000],  loss: 0.020951, mae: 0.353792, mean_q: 0.514538, mean_eps: 0.000000
 3878/5000: episode: 139, duration: 0.318s, episode steps:  22, steps per second:  69, episode reward: 41.534, mean reward:  1.888 [-3.000, 31.909], mean action: 4.545 [0.000, 20.000],  loss: 0.021294, mae: 0.357768, mean_q: 0.507885, mean_eps: 0.000000
 3897/5000: episode: 140, duration: 0.344s, episode steps:  19, steps per second:  55, episode reward: 44.213, mean reward:  2.327 [-2.202, 32.306], mean action: 3.579 [2.000, 15.000],  loss: 0.021836, mae: 0.359558, mean_q: 0.533360, mean_eps: 0.000000
 3933/5000: episode: 141, duration: 0.505s, episode steps:  36, steps per second:  71, episode reward: 36.000, mean reward:  1.000 [-2.379, 32.200], mean action: 4.222 [0.000, 20.000],  loss: 0.022156, mae: 0.356969, mean_q: 0.554646, mean_eps: 0.000000
 3957/5000: episode: 142, duration: 0.349s, episode steps:  24, steps per second:  69, episode reward: 41.227, mean reward:  1.718 [-2.903, 32.681], mean action: 3.042 [0.000, 15.000],  loss: 0.020275, mae: 0.338510, mean_q: 0.540110, mean_eps: 0.000000
 3976/5000: episode: 143, duration: 0.283s, episode steps:  19, steps per second:  67, episode reward: 45.000, mean reward:  2.368 [-2.003, 32.070], mean action: 1.368 [0.000, 11.000],  loss: 0.019179, mae: 0.335107, mean_q: 0.554469, mean_eps: 0.000000
 4037/5000: episode: 144, duration: 0.832s, episode steps:  61, steps per second:  73, episode reward: 32.890, mean reward:  0.539 [-3.000, 32.111], mean action: 8.262 [0.000, 20.000],  loss: 0.020794, mae: 0.343374, mean_q: 0.551499, mean_eps: 0.000000
 4059/5000: episode: 145, duration: 0.318s, episode steps:  22, steps per second:  69, episode reward: 41.353, mean reward:  1.880 [-3.000, 32.450], mean action: 2.227 [0.000, 11.000],  loss: 0.018742, mae: 0.335737, mean_q: 0.542596, mean_eps: 0.000000
 4090/5000: episode: 146, duration: 0.439s, episode steps:  31, steps per second:  71, episode reward: 38.307, mean reward:  1.236 [-2.709, 32.120], mean action: 3.806 [0.000, 12.000],  loss: 0.017695, mae: 0.326291, mean_q: 0.562211, mean_eps: 0.000000
 4105/5000: episode: 147, duration: 0.230s, episode steps:  15, steps per second:  65, episode reward: 44.745, mean reward:  2.983 [-2.229, 32.730], mean action: 3.467 [0.000, 12.000],  loss: 0.019186, mae: 0.339287, mean_q: 0.575644, mean_eps: 0.000000
 4123/5000: episode: 148, duration: 0.269s, episode steps:  18, steps per second:  67, episode reward: 42.522, mean reward:  2.362 [-3.000, 32.020], mean action: 5.444 [0.000, 15.000],  loss: 0.019639, mae: 0.347141, mean_q: 0.515162, mean_eps: 0.000000
 4169/5000: episode: 149, duration: 0.633s, episode steps:  46, steps per second:  73, episode reward: 32.486, mean reward:  0.706 [-3.000, 31.566], mean action: 3.478 [0.000, 12.000],  loss: 0.020632, mae: 0.357059, mean_q: 0.513024, mean_eps: 0.000000
 4188/5000: episode: 150, duration: 0.276s, episode steps:  19, steps per second:  69, episode reward: 43.173, mean reward:  2.272 [-2.150, 31.831], mean action: 3.684 [0.000, 16.000],  loss: 0.022262, mae: 0.356750, mean_q: 0.525191, mean_eps: 0.000000
 4223/5000: episode: 151, duration: 0.485s, episode steps:  35, steps per second:  72, episode reward: 32.953, mean reward:  0.942 [-3.000, 32.183], mean action: 3.800 [0.000, 19.000],  loss: 0.022838, mae: 0.375896, mean_q: 0.506423, mean_eps: 0.000000
 4250/5000: episode: 152, duration: 0.379s, episode steps:  27, steps per second:  71, episode reward: 33.000, mean reward:  1.222 [-3.000, 32.290], mean action: 5.519 [0.000, 19.000],  loss: 0.019379, mae: 0.352356, mean_q: 0.548185, mean_eps: 0.000000
 4273/5000: episode: 153, duration: 0.329s, episode steps:  23, steps per second:  70, episode reward: 35.415, mean reward:  1.540 [-2.799, 31.855], mean action: 3.696 [0.000, 19.000],  loss: 0.019907, mae: 0.353672, mean_q: 0.605530, mean_eps: 0.000000
 4327/5000: episode: 154, duration: 0.740s, episode steps:  54, steps per second:  73, episode reward: 38.939, mean reward:  0.721 [-3.000, 32.420], mean action: 3.426 [1.000, 13.000],  loss: 0.022326, mae: 0.360474, mean_q: 0.574822, mean_eps: 0.000000
 4394/5000: episode: 155, duration: 0.917s, episode steps:  67, steps per second:  73, episode reward: 35.376, mean reward:  0.528 [-2.113, 32.040], mean action: 3.836 [0.000, 12.000],  loss: 0.020888, mae: 0.354538, mean_q: 0.580238, mean_eps: 0.000000
 4424/5000: episode: 156, duration: 0.411s, episode steps:  30, steps per second:  73, episode reward: -34.440, mean reward: -1.148 [-32.207,  2.900], mean action: 4.367 [0.000, 12.000],  loss: 0.022261, mae: 0.357471, mean_q: 0.552428, mean_eps: 0.000000
 4443/5000: episode: 157, duration: 0.272s, episode steps:  19, steps per second:  70, episode reward: 38.088, mean reward:  2.005 [-2.901, 32.280], mean action: 4.053 [0.000, 16.000],  loss: 0.018909, mae: 0.335202, mean_q: 0.596247, mean_eps: 0.000000
 4468/5000: episode: 158, duration: 0.354s, episode steps:  25, steps per second:  71, episode reward: 44.162, mean reward:  1.766 [-2.709, 32.240], mean action: 2.480 [0.000, 16.000],  loss: 0.021251, mae: 0.350774, mean_q: 0.532218, mean_eps: 0.000000
 4489/5000: episode: 159, duration: 0.298s, episode steps:  21, steps per second:  70, episode reward: 41.971, mean reward:  1.999 [-2.684, 32.750], mean action: 4.524 [0.000, 19.000],  loss: 0.019563, mae: 0.340899, mean_q: 0.484961, mean_eps: 0.000000
 4517/5000: episode: 160, duration: 0.391s, episode steps:  28, steps per second:  72, episode reward: 37.638, mean reward:  1.344 [-3.000, 32.110], mean action: 6.429 [0.000, 20.000],  loss: 0.021774, mae: 0.359191, mean_q: 0.569012, mean_eps: 0.000000
 4551/5000: episode: 161, duration: 0.468s, episode steps:  34, steps per second:  73, episode reward: 40.895, mean reward:  1.203 [-2.160, 32.120], mean action: 2.265 [0.000, 11.000],  loss: 0.019776, mae: 0.350998, mean_q: 0.581547, mean_eps: 0.000000
 4570/5000: episode: 162, duration: 0.283s, episode steps:  19, steps per second:  67, episode reward: 38.903, mean reward:  2.048 [-2.471, 32.404], mean action: 3.263 [1.000, 11.000],  loss: 0.021211, mae: 0.353739, mean_q: 0.559102, mean_eps: 0.000000
 4589/5000: episode: 163, duration: 0.281s, episode steps:  19, steps per second:  68, episode reward: 38.274, mean reward:  2.014 [-3.000, 31.721], mean action: 3.474 [0.000, 12.000],  loss: 0.020473, mae: 0.364771, mean_q: 0.518991, mean_eps: 0.000000
 4620/5000: episode: 164, duration: 0.435s, episode steps:  31, steps per second:  71, episode reward: 40.270, mean reward:  1.299 [-2.705, 31.978], mean action: 3.806 [0.000, 20.000],  loss: 0.021286, mae: 0.364213, mean_q: 0.497007, mean_eps: 0.000000
 4650/5000: episode: 165, duration: 0.415s, episode steps:  30, steps per second:  72, episode reward: -32.590, mean reward: -1.086 [-29.945,  2.260], mean action: 6.600 [0.000, 19.000],  loss: 0.019791, mae: 0.354880, mean_q: 0.485212, mean_eps: 0.000000
 4694/5000: episode: 166, duration: 0.665s, episode steps:  44, steps per second:  66, episode reward: -32.520, mean reward: -0.739 [-32.095,  2.350], mean action: 8.068 [0.000, 19.000],  loss: 0.022985, mae: 0.357214, mean_q: 0.519843, mean_eps: 0.000000
 4722/5000: episode: 167, duration: 11.834s, episode steps:  28, steps per second:   2, episode reward: 38.446, mean reward:  1.373 [-3.000, 32.221], mean action: 4.393 [0.000, 16.000],  loss: 0.021650, mae: 0.352038, mean_q: 0.521629, mean_eps: 0.000000
 4743/5000: episode: 168, duration: 0.303s, episode steps:  21, steps per second:  69, episode reward: 38.901, mean reward:  1.852 [-2.782, 32.311], mean action: 3.333 [0.000, 14.000],  loss: 0.020677, mae: 0.345745, mean_q: 0.523247, mean_eps: 0.000000
 4774/5000: episode: 169, duration: 0.438s, episode steps:  31, steps per second:  71, episode reward: 41.189, mean reward:  1.329 [-2.377, 32.190], mean action: 3.226 [0.000, 15.000],  loss: 0.020314, mae: 0.352829, mean_q: 0.506567, mean_eps: 0.000000
 4816/5000: episode: 170, duration: 0.592s, episode steps:  42, steps per second:  71, episode reward: 38.381, mean reward:  0.914 [-2.599, 32.160], mean action: 3.143 [0.000, 15.000],  loss: 0.020020, mae: 0.353056, mean_q: 0.509330, mean_eps: 0.000000
 4832/5000: episode: 171, duration: 0.237s, episode steps:  16, steps per second:  68, episode reward: 41.073, mean reward:  2.567 [-3.000, 33.000], mean action: 3.688 [1.000, 15.000],  loss: 0.020388, mae: 0.352429, mean_q: 0.519848, mean_eps: 0.000000
 4852/5000: episode: 172, duration: 0.296s, episode steps:  20, steps per second:  68, episode reward: 44.849, mean reward:  2.242 [-2.286, 32.630], mean action: 2.800 [0.000, 15.000],  loss: 0.021528, mae: 0.349627, mean_q: 0.556441, mean_eps: 0.000000
 4878/5000: episode: 173, duration: 0.370s, episode steps:  26, steps per second:  70, episode reward: 41.806, mean reward:  1.608 [-2.380, 32.053], mean action: 3.115 [0.000, 15.000],  loss: 0.022809, mae: 0.361744, mean_q: 0.537607, mean_eps: 0.000000
 4896/5000: episode: 174, duration: 0.266s, episode steps:  18, steps per second:  68, episode reward: 43.885, mean reward:  2.438 [-2.170, 32.110], mean action: 3.111 [0.000, 15.000],  loss: 0.021033, mae: 0.347483, mean_q: 0.546059, mean_eps: 0.000000
 4946/5000: episode: 175, duration: 0.706s, episode steps:  50, steps per second:  71, episode reward: 43.840, mean reward:  0.877 [-2.268, 32.430], mean action: 2.920 [0.000, 15.000],  loss: 0.019538, mae: 0.341922, mean_q: 0.575146, mean_eps: 0.000000
 4981/5000: episode: 176, duration: 0.483s, episode steps:  35, steps per second:  72, episode reward: 35.533, mean reward:  1.015 [-2.320, 32.230], mean action: 4.314 [0.000, 18.000],  loss: 0.023589, mae: 0.370845, mean_q: 0.563709, mean_eps: 0.000000
done, took 76.508 seconds
DQN Evaluation: 7992 victories out of 9368 episodes
Training for 5000 steps ...
   19/5000: episode: 1, duration: 0.164s, episode steps:  19, steps per second: 116, episode reward: 38.163, mean reward:  2.009 [-2.541, 32.293], mean action: 3.526 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   39/5000: episode: 2, duration: 0.147s, episode steps:  20, steps per second: 136, episode reward: 35.102, mean reward:  1.755 [-3.000, 32.439], mean action: 3.650 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   62/5000: episode: 3, duration: 0.164s, episode steps:  23, steps per second: 140, episode reward: -32.910, mean reward: -1.431 [-32.143,  3.000], mean action: 4.478 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   86/5000: episode: 4, duration: 0.162s, episode steps:  24, steps per second: 148, episode reward: -38.600, mean reward: -1.608 [-31.720,  2.130], mean action: 3.958 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  110/5000: episode: 5, duration: 0.162s, episode steps:  24, steps per second: 148, episode reward: 36.000, mean reward:  1.500 [-3.000, 32.050], mean action: 3.708 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  143/5000: episode: 6, duration: 0.211s, episode steps:  33, steps per second: 156, episode reward: 32.655, mean reward:  0.990 [-2.493, 32.520], mean action: 2.576 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  168/5000: episode: 7, duration: 0.169s, episode steps:  25, steps per second: 148, episode reward: 35.801, mean reward:  1.432 [-2.368, 31.996], mean action: 6.240 [1.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  191/5000: episode: 8, duration: 0.157s, episode steps:  23, steps per second: 147, episode reward: 39.000, mean reward:  1.696 [-2.612, 32.330], mean action: 4.174 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  216/5000: episode: 9, duration: 0.174s, episode steps:  25, steps per second: 144, episode reward: 34.060, mean reward:  1.362 [-3.000, 31.700], mean action: 6.440 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  237/5000: episode: 10, duration: 0.159s, episode steps:  21, steps per second: 132, episode reward: 35.916, mean reward:  1.710 [-2.460, 32.520], mean action: 3.857 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  264/5000: episode: 11, duration: 0.191s, episode steps:  27, steps per second: 141, episode reward: -35.910, mean reward: -1.330 [-32.663,  2.316], mean action: 5.370 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  286/5000: episode: 12, duration: 0.155s, episode steps:  22, steps per second: 142, episode reward: 35.544, mean reward:  1.616 [-2.619, 32.004], mean action: 3.773 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  308/5000: episode: 13, duration: 0.156s, episode steps:  22, steps per second: 141, episode reward: 39.000, mean reward:  1.773 [-2.220, 33.000], mean action: 3.545 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  346/5000: episode: 14, duration: 0.237s, episode steps:  38, steps per second: 160, episode reward: 35.104, mean reward:  0.924 [-2.571, 32.300], mean action: 8.237 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  372/5000: episode: 15, duration: 0.172s, episode steps:  26, steps per second: 151, episode reward: -32.440, mean reward: -1.248 [-32.096,  2.179], mean action: 3.654 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  392/5000: episode: 16, duration: 0.133s, episode steps:  20, steps per second: 151, episode reward: -35.150, mean reward: -1.757 [-32.425,  2.120], mean action: 4.750 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  411/5000: episode: 17, duration: 0.134s, episode steps:  19, steps per second: 142, episode reward: 35.381, mean reward:  1.862 [-2.333, 32.610], mean action: 3.789 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  435/5000: episode: 18, duration: 0.161s, episode steps:  24, steps per second: 149, episode reward: 35.031, mean reward:  1.460 [-2.775, 32.064], mean action: 4.708 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  455/5000: episode: 19, duration: 0.138s, episode steps:  20, steps per second: 145, episode reward: 33.000, mean reward:  1.650 [-3.000, 32.630], mean action: 4.050 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  479/5000: episode: 20, duration: 0.167s, episode steps:  24, steps per second: 144, episode reward: 38.717, mean reward:  1.613 [-2.202, 32.040], mean action: 4.417 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  506/5000: episode: 21, duration: 0.176s, episode steps:  27, steps per second: 154, episode reward: 38.667, mean reward:  1.432 [-3.000, 32.200], mean action: 3.111 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  527/5000: episode: 22, duration: 0.141s, episode steps:  21, steps per second: 149, episode reward: 35.341, mean reward:  1.683 [-2.605, 31.786], mean action: 4.571 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  549/5000: episode: 23, duration: 0.149s, episode steps:  22, steps per second: 147, episode reward: 32.259, mean reward:  1.466 [-3.000, 32.123], mean action: 3.500 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  573/5000: episode: 24, duration: 0.157s, episode steps:  24, steps per second: 153, episode reward: 33.000, mean reward:  1.375 [-3.000, 32.300], mean action: 4.250 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  599/5000: episode: 25, duration: 0.168s, episode steps:  26, steps per second: 155, episode reward: 32.416, mean reward:  1.247 [-3.000, 32.299], mean action: 4.615 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  614/5000: episode: 26, duration: 0.110s, episode steps:  15, steps per second: 136, episode reward: 44.530, mean reward:  2.969 [-0.152, 29.482], mean action: 5.133 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  638/5000: episode: 27, duration: 0.159s, episode steps:  24, steps per second: 151, episode reward: 35.643, mean reward:  1.485 [-2.252, 31.973], mean action: 3.083 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  661/5000: episode: 28, duration: 0.160s, episode steps:  23, steps per second: 144, episode reward: 38.027, mean reward:  1.653 [-2.397, 32.150], mean action: 5.478 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  688/5000: episode: 29, duration: 0.177s, episode steps:  27, steps per second: 152, episode reward: -35.060, mean reward: -1.299 [-32.150,  2.649], mean action: 3.519 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  712/5000: episode: 30, duration: 0.161s, episode steps:  24, steps per second: 149, episode reward: 33.000, mean reward:  1.375 [-2.398, 33.000], mean action: 4.125 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  732/5000: episode: 31, duration: 0.140s, episode steps:  20, steps per second: 143, episode reward: 41.042, mean reward:  2.052 [-2.254, 32.120], mean action: 4.300 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  752/5000: episode: 32, duration: 0.138s, episode steps:  20, steps per second: 145, episode reward: 35.525, mean reward:  1.776 [-2.392, 32.171], mean action: 4.250 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  771/5000: episode: 33, duration: 0.129s, episode steps:  19, steps per second: 147, episode reward: 35.370, mean reward:  1.862 [-3.000, 32.900], mean action: 3.632 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  791/5000: episode: 34, duration: 0.209s, episode steps:  20, steps per second:  96, episode reward: 33.000, mean reward:  1.650 [-2.463, 29.610], mean action: 4.000 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  837/5000: episode: 35, duration: 0.469s, episode steps:  46, steps per second:  98, episode reward: -32.040, mean reward: -0.697 [-31.506,  2.350], mean action: 3.935 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  866/5000: episode: 36, duration: 0.190s, episode steps:  29, steps per second: 153, episode reward: 33.000, mean reward:  1.138 [-2.552, 33.000], mean action: 4.793 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  894/5000: episode: 37, duration: 0.181s, episode steps:  28, steps per second: 155, episode reward: 41.434, mean reward:  1.480 [-3.000, 32.360], mean action: 2.643 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  920/5000: episode: 38, duration: 0.175s, episode steps:  26, steps per second: 148, episode reward: 35.335, mean reward:  1.359 [-2.420, 32.440], mean action: 3.192 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  942/5000: episode: 39, duration: 0.145s, episode steps:  22, steps per second: 152, episode reward: -38.500, mean reward: -1.750 [-31.671,  2.713], mean action: 6.500 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  963/5000: episode: 40, duration: 0.142s, episode steps:  21, steps per second: 148, episode reward: 32.346, mean reward:  1.540 [-2.605, 32.126], mean action: 4.381 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  995/5000: episode: 41, duration: 0.203s, episode steps:  32, steps per second: 158, episode reward: 32.302, mean reward:  1.009 [-3.000, 32.066], mean action: 4.312 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1016/5000: episode: 42, duration: 0.272s, episode steps:  21, steps per second:  77, episode reward: 35.591, mean reward:  1.695 [-2.901, 31.861], mean action: 4.667 [0.000, 15.000],  loss: 0.020534, mae: 0.346963, mean_q: 0.584637, mean_eps: 0.000000
 1038/5000: episode: 43, duration: 0.317s, episode steps:  22, steps per second:  69, episode reward: 40.817, mean reward:  1.855 [-2.137, 32.170], mean action: 5.455 [0.000, 19.000],  loss: 0.019474, mae: 0.338386, mean_q: 0.539032, mean_eps: 0.000000
 1067/5000: episode: 44, duration: 0.403s, episode steps:  29, steps per second:  72, episode reward: -38.950, mean reward: -1.343 [-32.209,  2.747], mean action: 11.379 [0.000, 20.000],  loss: 0.019893, mae: 0.346744, mean_q: 0.528804, mean_eps: 0.000000
 1087/5000: episode: 45, duration: 0.288s, episode steps:  20, steps per second:  69, episode reward: 35.414, mean reward:  1.771 [-2.438, 31.731], mean action: 3.400 [0.000, 19.000],  loss: 0.018388, mae: 0.343008, mean_q: 0.520043, mean_eps: 0.000000
 1107/5000: episode: 46, duration: 0.282s, episode steps:  20, steps per second:  71, episode reward: 33.000, mean reward:  1.650 [-3.000, 32.090], mean action: 6.200 [0.000, 20.000],  loss: 0.020016, mae: 0.354614, mean_q: 0.519312, mean_eps: 0.000000
 1126/5000: episode: 47, duration: 0.275s, episode steps:  19, steps per second:  69, episode reward: 39.000, mean reward:  2.053 [-2.158, 32.150], mean action: 3.632 [0.000, 19.000],  loss: 0.019338, mae: 0.349397, mean_q: 0.499421, mean_eps: 0.000000
 1163/5000: episode: 48, duration: 0.506s, episode steps:  37, steps per second:  73, episode reward: 33.000, mean reward:  0.892 [-3.000, 32.250], mean action: 4.514 [0.000, 19.000],  loss: 0.016734, mae: 0.338346, mean_q: 0.489932, mean_eps: 0.000000
 1186/5000: episode: 49, duration: 0.334s, episode steps:  23, steps per second:  69, episode reward: 38.582, mean reward:  1.677 [-2.206, 32.223], mean action: 4.348 [0.000, 19.000],  loss: 0.022457, mae: 0.362596, mean_q: 0.536254, mean_eps: 0.000000
 1203/5000: episode: 50, duration: 0.247s, episode steps:  17, steps per second:  69, episode reward: 41.485, mean reward:  2.440 [-2.350, 32.570], mean action: 3.235 [0.000, 19.000],  loss: 0.020005, mae: 0.359113, mean_q: 0.567053, mean_eps: 0.000000
 1237/5000: episode: 51, duration: 0.473s, episode steps:  34, steps per second:  72, episode reward: 35.622, mean reward:  1.048 [-2.341, 30.888], mean action: 2.559 [0.000, 19.000],  loss: 0.021842, mae: 0.363395, mean_q: 0.562704, mean_eps: 0.000000
 1254/5000: episode: 52, duration: 0.246s, episode steps:  17, steps per second:  69, episode reward: 41.687, mean reward:  2.452 [-3.000, 32.410], mean action: 2.294 [0.000, 15.000],  loss: 0.022421, mae: 0.361842, mean_q: 0.549151, mean_eps: 0.000000
 1294/5000: episode: 53, duration: 0.565s, episode steps:  40, steps per second:  71, episode reward: -35.590, mean reward: -0.890 [-32.710,  3.000], mean action: 4.325 [0.000, 16.000],  loss: 0.020238, mae: 0.351773, mean_q: 0.539764, mean_eps: 0.000000
 1318/5000: episode: 54, duration: 0.347s, episode steps:  24, steps per second:  69, episode reward: 41.469, mean reward:  1.728 [-2.367, 32.360], mean action: 3.708 [0.000, 15.000],  loss: 0.021556, mae: 0.363279, mean_q: 0.502791, mean_eps: 0.000000
 1357/5000: episode: 55, duration: 0.542s, episode steps:  39, steps per second:  72, episode reward: 35.276, mean reward:  0.905 [-3.000, 32.625], mean action: 3.974 [0.000, 15.000],  loss: 0.019209, mae: 0.345765, mean_q: 0.532786, mean_eps: 0.000000
 1370/5000: episode: 56, duration: 0.193s, episode steps:  13, steps per second:  67, episode reward: 39.000, mean reward:  3.000 [-2.314, 30.865], mean action: 3.000 [0.000, 11.000],  loss: 0.021410, mae: 0.348718, mean_q: 0.519628, mean_eps: 0.000000
 1392/5000: episode: 57, duration: 0.315s, episode steps:  22, steps per second:  70, episode reward: 35.070, mean reward:  1.594 [-2.625, 31.723], mean action: 4.500 [0.000, 15.000],  loss: 0.024075, mae: 0.371065, mean_q: 0.505705, mean_eps: 0.000000
 1430/5000: episode: 58, duration: 0.529s, episode steps:  38, steps per second:  72, episode reward: 32.266, mean reward:  0.849 [-3.000, 32.030], mean action: 8.079 [0.000, 16.000],  loss: 0.021864, mae: 0.357908, mean_q: 0.531712, mean_eps: 0.000000
 1447/5000: episode: 59, duration: 0.248s, episode steps:  17, steps per second:  69, episode reward: 41.269, mean reward:  2.428 [-2.804, 32.470], mean action: 2.941 [0.000, 15.000],  loss: 0.019029, mae: 0.349803, mean_q: 0.531977, mean_eps: 0.000000
 1467/5000: episode: 60, duration: 0.296s, episode steps:  20, steps per second:  68, episode reward: 30.000, mean reward:  1.500 [-3.000, 30.097], mean action: 4.900 [0.000, 15.000],  loss: 0.020287, mae: 0.354670, mean_q: 0.552298, mean_eps: 0.000000
 1497/5000: episode: 61, duration: 0.422s, episode steps:  30, steps per second:  71, episode reward: -32.910, mean reward: -1.097 [-32.121,  2.459], mean action: 3.500 [0.000, 19.000],  loss: 0.022707, mae: 0.369740, mean_q: 0.569054, mean_eps: 0.000000
 1518/5000: episode: 62, duration: 0.305s, episode steps:  21, steps per second:  69, episode reward: 37.432, mean reward:  1.782 [-3.000, 32.330], mean action: 6.952 [0.000, 15.000],  loss: 0.019155, mae: 0.354356, mean_q: 0.601106, mean_eps: 0.000000
 1548/5000: episode: 63, duration: 0.452s, episode steps:  30, steps per second:  66, episode reward: 41.693, mean reward:  1.390 [-2.998, 32.190], mean action: 4.233 [1.000, 15.000],  loss: 0.019366, mae: 0.354830, mean_q: 0.605281, mean_eps: 0.000000
 1582/5000: episode: 64, duration: 0.479s, episode steps:  34, steps per second:  71, episode reward: -32.040, mean reward: -0.942 [-33.000,  2.387], mean action: 3.912 [0.000, 20.000],  loss: 0.019245, mae: 0.356888, mean_q: 0.546238, mean_eps: 0.000000
 1602/5000: episode: 65, duration: 0.288s, episode steps:  20, steps per second:  69, episode reward: 38.574, mean reward:  1.929 [-2.576, 32.230], mean action: 3.450 [0.000, 12.000],  loss: 0.018011, mae: 0.348728, mean_q: 0.514407, mean_eps: 0.000000
 1626/5000: episode: 66, duration: 0.342s, episode steps:  24, steps per second:  70, episode reward: 32.587, mean reward:  1.358 [-3.000, 32.103], mean action: 4.167 [0.000, 15.000],  loss: 0.019833, mae: 0.368097, mean_q: 0.504963, mean_eps: 0.000000
 1652/5000: episode: 67, duration: 0.364s, episode steps:  26, steps per second:  71, episode reward: 35.377, mean reward:  1.361 [-2.332, 32.380], mean action: 3.615 [0.000, 16.000],  loss: 0.020004, mae: 0.359241, mean_q: 0.486452, mean_eps: 0.000000
 1682/5000: episode: 68, duration: 0.422s, episode steps:  30, steps per second:  71, episode reward: 35.216, mean reward:  1.174 [-2.455, 32.900], mean action: 2.833 [0.000, 11.000],  loss: 0.021397, mae: 0.367667, mean_q: 0.492930, mean_eps: 0.000000
 1702/5000: episode: 69, duration: 0.285s, episode steps:  20, steps per second:  70, episode reward: 35.760, mean reward:  1.788 [-2.289, 33.000], mean action: 6.050 [0.000, 14.000],  loss: 0.021633, mae: 0.370569, mean_q: 0.527646, mean_eps: 0.000000
 1724/5000: episode: 70, duration: 0.314s, episode steps:  22, steps per second:  70, episode reward: 36.000, mean reward:  1.636 [-2.453, 32.150], mean action: 2.591 [0.000, 11.000],  loss: 0.025095, mae: 0.381962, mean_q: 0.578543, mean_eps: 0.000000
 1743/5000: episode: 71, duration: 0.270s, episode steps:  19, steps per second:  70, episode reward: 35.446, mean reward:  1.866 [-2.903, 32.270], mean action: 3.526 [0.000, 11.000],  loss: 0.025454, mae: 0.379666, mean_q: 0.559339, mean_eps: 0.000000
 1759/5000: episode: 72, duration: 0.237s, episode steps:  16, steps per second:  68, episode reward: 38.661, mean reward:  2.416 [-2.900, 32.820], mean action: 4.625 [0.000, 13.000],  loss: 0.016417, mae: 0.332645, mean_q: 0.515458, mean_eps: 0.000000
 1779/5000: episode: 73, duration: 0.285s, episode steps:  20, steps per second:  70, episode reward: -35.050, mean reward: -1.752 [-32.132,  2.460], mean action: 5.300 [0.000, 14.000],  loss: 0.019612, mae: 0.356807, mean_q: 0.535552, mean_eps: 0.000000
 1801/5000: episode: 74, duration: 0.312s, episode steps:  22, steps per second:  70, episode reward: -35.150, mean reward: -1.598 [-32.002,  2.750], mean action: 5.136 [0.000, 14.000],  loss: 0.021825, mae: 0.362336, mean_q: 0.568742, mean_eps: 0.000000
 1820/5000: episode: 75, duration: 0.274s, episode steps:  19, steps per second:  69, episode reward: 33.000, mean reward:  1.737 [-2.900, 29.798], mean action: 4.368 [0.000, 12.000],  loss: 0.018083, mae: 0.344155, mean_q: 0.628147, mean_eps: 0.000000
 1839/5000: episode: 76, duration: 0.281s, episode steps:  19, steps per second:  68, episode reward: 38.587, mean reward:  2.031 [-2.680, 32.064], mean action: 3.368 [0.000, 12.000],  loss: 0.025369, mae: 0.378395, mean_q: 0.659020, mean_eps: 0.000000
 1862/5000: episode: 77, duration: 0.325s, episode steps:  23, steps per second:  71, episode reward: -32.680, mean reward: -1.421 [-32.395,  3.000], mean action: 3.609 [0.000, 12.000],  loss: 0.022686, mae: 0.369965, mean_q: 0.554188, mean_eps: 0.000000
 1878/5000: episode: 78, duration: 0.240s, episode steps:  16, steps per second:  67, episode reward: 44.115, mean reward:  2.757 [-2.099, 32.229], mean action: 3.438 [0.000, 15.000],  loss: 0.017621, mae: 0.352344, mean_q: 0.579797, mean_eps: 0.000000
 1901/5000: episode: 79, duration: 0.329s, episode steps:  23, steps per second:  70, episode reward: 38.859, mean reward:  1.690 [-2.266, 32.390], mean action: 2.870 [0.000, 15.000],  loss: 0.021636, mae: 0.368524, mean_q: 0.528877, mean_eps: 0.000000
 1935/5000: episode: 80, duration: 0.467s, episode steps:  34, steps per second:  73, episode reward: 34.482, mean reward:  1.014 [-3.000, 31.425], mean action: 3.971 [0.000, 12.000],  loss: 0.020498, mae: 0.372405, mean_q: 0.521863, mean_eps: 0.000000
 1955/5000: episode: 81, duration: 0.290s, episode steps:  20, steps per second:  69, episode reward: 38.900, mean reward:  1.945 [-2.562, 32.800], mean action: 3.200 [1.000, 15.000],  loss: 0.023143, mae: 0.382679, mean_q: 0.539385, mean_eps: 0.000000
 1968/5000: episode: 82, duration: 0.209s, episode steps:  13, steps per second:  62, episode reward: 43.619, mean reward:  3.355 [-2.115, 32.360], mean action: 2.923 [0.000, 11.000],  loss: 0.029154, mae: 0.404929, mean_q: 0.552888, mean_eps: 0.000000
 1983/5000: episode: 83, duration: 0.227s, episode steps:  15, steps per second:  66, episode reward: 40.469, mean reward:  2.698 [-2.343, 32.092], mean action: 3.800 [0.000, 15.000],  loss: 0.021771, mae: 0.378070, mean_q: 0.570718, mean_eps: 0.000000
 2004/5000: episode: 84, duration: 0.309s, episode steps:  21, steps per second:  68, episode reward: 35.075, mean reward:  1.670 [-3.000, 32.001], mean action: 6.143 [1.000, 15.000],  loss: 0.022797, mae: 0.379200, mean_q: 0.551262, mean_eps: 0.000000
 2020/5000: episode: 85, duration: 0.232s, episode steps:  16, steps per second:  69, episode reward: 38.711, mean reward:  2.419 [-3.000, 32.450], mean action: 3.812 [0.000, 9.000],  loss: 0.017810, mae: 0.352832, mean_q: 0.546867, mean_eps: 0.000000
 2048/5000: episode: 86, duration: 0.394s, episode steps:  28, steps per second:  71, episode reward: -32.570, mean reward: -1.163 [-31.952,  2.470], mean action: 6.464 [0.000, 19.000],  loss: 0.018398, mae: 0.354800, mean_q: 0.506569, mean_eps: 0.000000
 2063/5000: episode: 87, duration: 0.220s, episode steps:  15, steps per second:  68, episode reward: 35.478, mean reward:  2.365 [-3.000, 32.188], mean action: 6.400 [1.000, 16.000],  loss: 0.023701, mae: 0.380855, mean_q: 0.520483, mean_eps: 0.000000
 2088/5000: episode: 88, duration: 0.408s, episode steps:  25, steps per second:  61, episode reward: 37.390, mean reward:  1.496 [-2.452, 32.140], mean action: 5.800 [0.000, 16.000],  loss: 0.018388, mae: 0.354407, mean_q: 0.495107, mean_eps: 0.000000
 2118/5000: episode: 89, duration: 0.424s, episode steps:  30, steps per second:  71, episode reward: 33.000, mean reward:  1.100 [-3.000, 32.120], mean action: 7.100 [1.000, 16.000],  loss: 0.018335, mae: 0.349294, mean_q: 0.553661, mean_eps: 0.000000
 2158/5000: episode: 90, duration: 0.560s, episode steps:  40, steps per second:  71, episode reward: 33.000, mean reward:  0.825 [-3.000, 32.130], mean action: 3.700 [0.000, 16.000],  loss: 0.022556, mae: 0.368312, mean_q: 0.577859, mean_eps: 0.000000
 2181/5000: episode: 91, duration: 0.339s, episode steps:  23, steps per second:  68, episode reward: 39.000, mean reward:  1.696 [-2.545, 32.290], mean action: 2.391 [0.000, 16.000],  loss: 0.021275, mae: 0.368077, mean_q: 0.664710, mean_eps: 0.000000
 2198/5000: episode: 92, duration: 0.255s, episode steps:  17, steps per second:  67, episode reward: 41.909, mean reward:  2.465 [-2.700, 32.190], mean action: 2.471 [0.000, 16.000],  loss: 0.018598, mae: 0.346094, mean_q: 0.606100, mean_eps: 0.000000
 2218/5000: episode: 93, duration: 0.291s, episode steps:  20, steps per second:  69, episode reward: 41.416, mean reward:  2.071 [-2.171, 32.180], mean action: 3.550 [0.000, 20.000],  loss: 0.018295, mae: 0.343516, mean_q: 0.571845, mean_eps: 0.000000
 2239/5000: episode: 94, duration: 0.299s, episode steps:  21, steps per second:  70, episode reward: 35.903, mean reward:  1.710 [-2.728, 32.293], mean action: 3.476 [0.000, 13.000],  loss: 0.019082, mae: 0.350645, mean_q: 0.549347, mean_eps: 0.000000
 2258/5000: episode: 95, duration: 0.276s, episode steps:  19, steps per second:  69, episode reward: 35.641, mean reward:  1.876 [-2.807, 31.912], mean action: 4.632 [0.000, 15.000],  loss: 0.022809, mae: 0.364926, mean_q: 0.573331, mean_eps: 0.000000
 2295/5000: episode: 96, duration: 0.511s, episode steps:  37, steps per second:  72, episode reward: 32.696, mean reward:  0.884 [-3.000, 32.600], mean action: 2.676 [0.000, 9.000],  loss: 0.017444, mae: 0.335200, mean_q: 0.561246, mean_eps: 0.000000
 2318/5000: episode: 97, duration: 0.334s, episode steps:  23, steps per second:  69, episode reward: 35.368, mean reward:  1.538 [-2.523, 32.420], mean action: 3.217 [0.000, 9.000],  loss: 0.021542, mae: 0.359229, mean_q: 0.559821, mean_eps: 0.000000
 2351/5000: episode: 98, duration: 0.469s, episode steps:  33, steps per second:  70, episode reward: 32.735, mean reward:  0.992 [-2.903, 32.073], mean action: 7.000 [0.000, 14.000],  loss: 0.019879, mae: 0.345833, mean_q: 0.565572, mean_eps: 0.000000
 2373/5000: episode: 99, duration: 0.316s, episode steps:  22, steps per second:  70, episode reward: 32.760, mean reward:  1.489 [-3.000, 32.290], mean action: 5.591 [0.000, 15.000],  loss: 0.019415, mae: 0.344280, mean_q: 0.559690, mean_eps: 0.000000
 2393/5000: episode: 100, duration: 0.286s, episode steps:  20, steps per second:  70, episode reward: 39.000, mean reward:  1.950 [-2.485, 32.474], mean action: 2.500 [0.000, 15.000],  loss: 0.021895, mae: 0.361573, mean_q: 0.494947, mean_eps: 0.000000
 2419/5000: episode: 101, duration: 0.364s, episode steps:  26, steps per second:  72, episode reward: 32.676, mean reward:  1.257 [-3.000, 32.220], mean action: 5.692 [0.000, 15.000],  loss: 0.023772, mae: 0.376503, mean_q: 0.490360, mean_eps: 0.000000
 2447/5000: episode: 102, duration: 0.423s, episode steps:  28, steps per second:  66, episode reward: 38.012, mean reward:  1.358 [-2.269, 32.636], mean action: 5.357 [0.000, 15.000],  loss: 0.021259, mae: 0.357666, mean_q: 0.516160, mean_eps: 0.000000
 2469/5000: episode: 103, duration: 0.324s, episode steps:  22, steps per second:  68, episode reward: 38.004, mean reward:  1.727 [-2.578, 32.707], mean action: 2.909 [0.000, 12.000],  loss: 0.021125, mae: 0.355122, mean_q: 0.491340, mean_eps: 0.000000
 2490/5000: episode: 104, duration: 0.301s, episode steps:  21, steps per second:  70, episode reward: 38.094, mean reward:  1.814 [-2.213, 32.540], mean action: 3.714 [0.000, 11.000],  loss: 0.021882, mae: 0.351895, mean_q: 0.500655, mean_eps: 0.000000
 2513/5000: episode: 105, duration: 0.391s, episode steps:  23, steps per second:  59, episode reward: 32.347, mean reward:  1.406 [-2.615, 32.990], mean action: 5.087 [0.000, 15.000],  loss: 0.020024, mae: 0.350354, mean_q: 0.538910, mean_eps: 0.000000
 2535/5000: episode: 106, duration: 0.352s, episode steps:  22, steps per second:  62, episode reward: -32.740, mean reward: -1.488 [-32.087,  2.260], mean action: 4.955 [0.000, 20.000],  loss: 0.018185, mae: 0.347157, mean_q: 0.523805, mean_eps: 0.000000
 2549/5000: episode: 107, duration: 0.242s, episode steps:  14, steps per second:  58, episode reward: 41.366, mean reward:  2.955 [-2.198, 32.322], mean action: 4.714 [0.000, 15.000],  loss: 0.027816, mae: 0.390979, mean_q: 0.508744, mean_eps: 0.000000
 2571/5000: episode: 108, duration: 0.313s, episode steps:  22, steps per second:  70, episode reward: 37.445, mean reward:  1.702 [-3.000, 32.280], mean action: 2.364 [0.000, 15.000],  loss: 0.019571, mae: 0.348493, mean_q: 0.529936, mean_eps: 0.000000
 2592/5000: episode: 109, duration: 0.303s, episode steps:  21, steps per second:  69, episode reward: 32.678, mean reward:  1.556 [-3.000, 32.678], mean action: 6.143 [1.000, 19.000],  loss: 0.020931, mae: 0.355005, mean_q: 0.523341, mean_eps: 0.000000
 2609/5000: episode: 110, duration: 0.250s, episode steps:  17, steps per second:  68, episode reward: 38.570, mean reward:  2.269 [-2.391, 33.000], mean action: 4.765 [0.000, 15.000],  loss: 0.020255, mae: 0.355548, mean_q: 0.528629, mean_eps: 0.000000
 2635/5000: episode: 111, duration: 0.365s, episode steps:  26, steps per second:  71, episode reward: 32.473, mean reward:  1.249 [-3.000, 32.243], mean action: 4.385 [0.000, 21.000],  loss: 0.022677, mae: 0.372052, mean_q: 0.521179, mean_eps: 0.000000
 2656/5000: episode: 112, duration: 0.309s, episode steps:  21, steps per second:  68, episode reward: 35.902, mean reward:  1.710 [-2.413, 32.762], mean action: 3.429 [0.000, 15.000],  loss: 0.022418, mae: 0.371642, mean_q: 0.539725, mean_eps: 0.000000
 2678/5000: episode: 113, duration: 0.313s, episode steps:  22, steps per second:  70, episode reward: 36.000, mean reward:  1.636 [-2.309, 32.140], mean action: 3.818 [1.000, 12.000],  loss: 0.021525, mae: 0.359232, mean_q: 0.584294, mean_eps: 0.000000
 2711/5000: episode: 114, duration: 0.470s, episode steps:  33, steps per second:  70, episode reward: 32.346, mean reward:  0.980 [-3.000, 32.570], mean action: 5.818 [0.000, 19.000],  loss: 0.022171, mae: 0.367958, mean_q: 0.578153, mean_eps: 0.000000
 2736/5000: episode: 115, duration: 0.358s, episode steps:  25, steps per second:  70, episode reward: 35.129, mean reward:  1.405 [-2.314, 32.678], mean action: 5.840 [0.000, 14.000],  loss: 0.018255, mae: 0.353074, mean_q: 0.554881, mean_eps: 0.000000
 2760/5000: episode: 116, duration: 0.342s, episode steps:  24, steps per second:  70, episode reward: 32.590, mean reward:  1.358 [-3.000, 32.280], mean action: 4.917 [0.000, 19.000],  loss: 0.021765, mae: 0.365881, mean_q: 0.546294, mean_eps: 0.000000
 2786/5000: episode: 117, duration: 0.368s, episode steps:  26, steps per second:  71, episode reward: 32.321, mean reward:  1.243 [-3.000, 32.020], mean action: 3.077 [0.000, 16.000],  loss: 0.024243, mae: 0.371062, mean_q: 0.593307, mean_eps: 0.000000
 2814/5000: episode: 118, duration: 0.391s, episode steps:  28, steps per second:  72, episode reward: -35.020, mean reward: -1.251 [-31.884,  2.807], mean action: 6.500 [0.000, 16.000],  loss: 0.020188, mae: 0.348293, mean_q: 0.573317, mean_eps: 0.000000
 2835/5000: episode: 119, duration: 0.296s, episode steps:  21, steps per second:  71, episode reward: -38.660, mean reward: -1.841 [-32.176,  2.340], mean action: 5.429 [0.000, 12.000],  loss: 0.021310, mae: 0.358577, mean_q: 0.511973, mean_eps: 0.000000
 2857/5000: episode: 120, duration: 0.317s, episode steps:  22, steps per second:  69, episode reward: 35.515, mean reward:  1.614 [-2.365, 32.027], mean action: 3.864 [0.000, 11.000],  loss: 0.014774, mae: 0.321939, mean_q: 0.529127, mean_eps: 0.000000
 2882/5000: episode: 121, duration: 0.358s, episode steps:  25, steps per second:  70, episode reward: -32.550, mean reward: -1.302 [-31.978,  2.314], mean action: 6.600 [0.000, 20.000],  loss: 0.019680, mae: 0.348000, mean_q: 0.583922, mean_eps: 0.000000
 2906/5000: episode: 122, duration: 0.346s, episode steps:  24, steps per second:  69, episode reward: 44.016, mean reward:  1.834 [-2.276, 32.073], mean action: 3.167 [0.000, 19.000],  loss: 0.019650, mae: 0.347124, mean_q: 0.566586, mean_eps: 0.000000
 2941/5000: episode: 123, duration: 0.564s, episode steps:  35, steps per second:  62, episode reward: 36.000, mean reward:  1.029 [-2.420, 30.455], mean action: 5.800 [0.000, 20.000],  loss: 0.022365, mae: 0.359082, mean_q: 0.614702, mean_eps: 0.000000
 2968/5000: episode: 124, duration: 0.392s, episode steps:  27, steps per second:  69, episode reward: 32.475, mean reward:  1.203 [-2.431, 32.200], mean action: 5.889 [0.000, 16.000],  loss: 0.018829, mae: 0.347592, mean_q: 0.614191, mean_eps: 0.000000
 3013/5000: episode: 125, duration: 0.648s, episode steps:  45, steps per second:  69, episode reward: 44.940, mean reward:  0.999 [-2.179, 33.000], mean action: 3.111 [0.000, 20.000],  loss: 0.020755, mae: 0.367613, mean_q: 0.588799, mean_eps: 0.000000
 3048/5000: episode: 126, duration: 0.499s, episode steps:  35, steps per second:  70, episode reward: 33.000, mean reward:  0.943 [-2.345, 32.130], mean action: 4.943 [1.000, 19.000],  loss: 0.016573, mae: 0.339031, mean_q: 0.537673, mean_eps: 0.000000
 3070/5000: episode: 127, duration: 0.321s, episode steps:  22, steps per second:  68, episode reward: 35.259, mean reward:  1.603 [-3.000, 32.370], mean action: 5.545 [0.000, 19.000],  loss: 0.024820, mae: 0.374049, mean_q: 0.543275, mean_eps: 0.000000
 3090/5000: episode: 128, duration: 0.306s, episode steps:  20, steps per second:  65, episode reward: 35.436, mean reward:  1.772 [-2.443, 32.023], mean action: 3.200 [0.000, 12.000],  loss: 0.023859, mae: 0.374383, mean_q: 0.544744, mean_eps: 0.000000
 3109/5000: episode: 129, duration: 0.321s, episode steps:  19, steps per second:  59, episode reward: 32.333, mean reward:  1.702 [-3.000, 32.003], mean action: 3.895 [0.000, 12.000],  loss: 0.020691, mae: 0.356914, mean_q: 0.573322, mean_eps: 0.000000
 3119/5000: episode: 130, duration: 0.167s, episode steps:  10, steps per second:  60, episode reward: 45.000, mean reward:  4.500 [-2.422, 33.000], mean action: 1.400 [0.000, 9.000],  loss: 0.017263, mae: 0.341026, mean_q: 0.518979, mean_eps: 0.000000
 3134/5000: episode: 131, duration: 0.229s, episode steps:  15, steps per second:  66, episode reward: 41.552, mean reward:  2.770 [-2.539, 31.939], mean action: 2.867 [0.000, 12.000],  loss: 0.021906, mae: 0.366020, mean_q: 0.519297, mean_eps: 0.000000
 3157/5000: episode: 132, duration: 0.348s, episode steps:  23, steps per second:  66, episode reward: 35.064, mean reward:  1.525 [-3.000, 32.153], mean action: 5.696 [0.000, 19.000],  loss: 0.019335, mae: 0.363193, mean_q: 0.500465, mean_eps: 0.000000
 3180/5000: episode: 133, duration: 0.323s, episode steps:  23, steps per second:  71, episode reward: 32.485, mean reward:  1.412 [-2.765, 32.260], mean action: 4.957 [0.000, 19.000],  loss: 0.022859, mae: 0.368123, mean_q: 0.432524, mean_eps: 0.000000
 3202/5000: episode: 134, duration: 0.318s, episode steps:  22, steps per second:  69, episode reward: 35.804, mean reward:  1.627 [-3.000, 32.902], mean action: 4.591 [0.000, 13.000],  loss: 0.020610, mae: 0.361014, mean_q: 0.466533, mean_eps: 0.000000
 3220/5000: episode: 135, duration: 0.266s, episode steps:  18, steps per second:  68, episode reward: 41.919, mean reward:  2.329 [-2.549, 32.420], mean action: 1.333 [0.000, 12.000],  loss: 0.020516, mae: 0.349975, mean_q: 0.540520, mean_eps: 0.000000
 3235/5000: episode: 136, duration: 0.223s, episode steps:  15, steps per second:  67, episode reward: 38.816, mean reward:  2.588 [-2.903, 32.006], mean action: 5.133 [0.000, 13.000],  loss: 0.025062, mae: 0.375890, mean_q: 0.518908, mean_eps: 0.000000
 3258/5000: episode: 137, duration: 0.424s, episode steps:  23, steps per second:  54, episode reward: 34.610, mean reward:  1.505 [-2.544, 31.780], mean action: 5.826 [0.000, 16.000],  loss: 0.019365, mae: 0.349168, mean_q: 0.505203, mean_eps: 0.000000
 3288/5000: episode: 138, duration: 0.422s, episode steps:  30, steps per second:  71, episode reward: 38.181, mean reward:  1.273 [-2.326, 32.150], mean action: 4.967 [0.000, 20.000],  loss: 0.017914, mae: 0.345830, mean_q: 0.476274, mean_eps: 0.000000
 3311/5000: episode: 139, duration: 0.327s, episode steps:  23, steps per second:  70, episode reward: 38.727, mean reward:  1.684 [-2.694, 32.440], mean action: 5.696 [0.000, 15.000],  loss: 0.022125, mae: 0.363119, mean_q: 0.489223, mean_eps: 0.000000
 3333/5000: episode: 140, duration: 0.316s, episode steps:  22, steps per second:  70, episode reward: 41.115, mean reward:  1.869 [-2.263, 32.099], mean action: 2.455 [0.000, 15.000],  loss: 0.018425, mae: 0.345881, mean_q: 0.495614, mean_eps: 0.000000
 3354/5000: episode: 141, duration: 0.300s, episode steps:  21, steps per second:  70, episode reward: 35.365, mean reward:  1.684 [-2.414, 32.108], mean action: 4.714 [0.000, 14.000],  loss: 0.022670, mae: 0.370557, mean_q: 0.545255, mean_eps: 0.000000
 3381/5000: episode: 142, duration: 0.379s, episode steps:  27, steps per second:  71, episode reward: -32.510, mean reward: -1.204 [-31.773,  2.630], mean action: 3.444 [0.000, 19.000],  loss: 0.019269, mae: 0.354210, mean_q: 0.533311, mean_eps: 0.000000
 3397/5000: episode: 143, duration: 0.236s, episode steps:  16, steps per second:  68, episode reward: 38.488, mean reward:  2.406 [-2.435, 32.288], mean action: 3.562 [0.000, 12.000],  loss: 0.023581, mae: 0.372353, mean_q: 0.543454, mean_eps: 0.000000
 3427/5000: episode: 144, duration: 0.420s, episode steps:  30, steps per second:  71, episode reward: 32.398, mean reward:  1.080 [-3.000, 31.809], mean action: 5.933 [0.000, 15.000],  loss: 0.028904, mae: 0.396157, mean_q: 0.559209, mean_eps: 0.000000
 3451/5000: episode: 145, duration: 0.341s, episode steps:  24, steps per second:  70, episode reward: 35.168, mean reward:  1.465 [-2.696, 30.225], mean action: 5.083 [0.000, 14.000],  loss: 0.023444, mae: 0.365943, mean_q: 0.523602, mean_eps: 0.000000
 3477/5000: episode: 146, duration: 0.364s, episode steps:  26, steps per second:  71, episode reward: -32.850, mean reward: -1.263 [-32.211,  2.340], mean action: 3.385 [0.000, 16.000],  loss: 0.020040, mae: 0.353376, mean_q: 0.553656, mean_eps: 0.000000
 3497/5000: episode: 147, duration: 0.289s, episode steps:  20, steps per second:  69, episode reward: 34.997, mean reward:  1.750 [-3.000, 32.602], mean action: 5.150 [0.000, 16.000],  loss: 0.019799, mae: 0.344725, mean_q: 0.558666, mean_eps: 0.000000
 3517/5000: episode: 148, duration: 0.284s, episode steps:  20, steps per second:  71, episode reward: -38.460, mean reward: -1.923 [-32.093,  2.360], mean action: 9.200 [0.000, 20.000],  loss: 0.018721, mae: 0.341812, mean_q: 0.572405, mean_eps: 0.000000
 3540/5000: episode: 149, duration: 0.326s, episode steps:  23, steps per second:  71, episode reward: 35.658, mean reward:  1.550 [-2.523, 31.866], mean action: 3.826 [0.000, 12.000],  loss: 0.020175, mae: 0.346762, mean_q: 0.556234, mean_eps: 0.000000
 3561/5000: episode: 150, duration: 0.300s, episode steps:  21, steps per second:  70, episode reward: 35.693, mean reward:  1.700 [-2.802, 32.180], mean action: 3.524 [0.000, 12.000],  loss: 0.022981, mae: 0.363622, mean_q: 0.526957, mean_eps: 0.000000
 3588/5000: episode: 151, duration: 0.377s, episode steps:  27, steps per second:  72, episode reward: 35.643, mean reward:  1.320 [-2.332, 32.643], mean action: 3.259 [0.000, 20.000],  loss: 0.026921, mae: 0.381332, mean_q: 0.485276, mean_eps: 0.000000
 3614/5000: episode: 152, duration: 0.372s, episode steps:  26, steps per second:  70, episode reward: -32.100, mean reward: -1.235 [-32.030,  2.480], mean action: 3.692 [0.000, 12.000],  loss: 0.021630, mae: 0.361271, mean_q: 0.494595, mean_eps: 0.000000
 3660/5000: episode: 153, duration: 0.624s, episode steps:  46, steps per second:  74, episode reward: -32.820, mean reward: -0.713 [-32.197,  2.340], mean action: 4.522 [1.000, 12.000],  loss: 0.019941, mae: 0.351769, mean_q: 0.499572, mean_eps: 0.000000
 3690/5000: episode: 154, duration: 0.434s, episode steps:  30, steps per second:  69, episode reward: 41.591, mean reward:  1.386 [-2.244, 32.030], mean action: 3.733 [2.000, 15.000],  loss: 0.022000, mae: 0.357792, mean_q: 0.580751, mean_eps: 0.000000
 3714/5000: episode: 155, duration: 0.341s, episode steps:  24, steps per second:  70, episode reward: 38.802, mean reward:  1.617 [-2.213, 32.262], mean action: 4.000 [0.000, 16.000],  loss: 0.019544, mae: 0.352406, mean_q: 0.585699, mean_eps: 0.000000
 3740/5000: episode: 156, duration: 0.368s, episode steps:  26, steps per second:  71, episode reward: -32.800, mean reward: -1.262 [-32.318,  2.870], mean action: 5.269 [0.000, 16.000],  loss: 0.019411, mae: 0.351239, mean_q: 0.529231, mean_eps: 0.000000
 3759/5000: episode: 157, duration: 0.278s, episode steps:  19, steps per second:  68, episode reward: 44.701, mean reward:  2.353 [-2.035, 32.210], mean action: 3.474 [2.000, 15.000],  loss: 0.023687, mae: 0.372369, mean_q: 0.513727, mean_eps: 0.000000
 3789/5000: episode: 158, duration: 0.418s, episode steps:  30, steps per second:  72, episode reward: 32.504, mean reward:  1.083 [-3.000, 32.330], mean action: 6.000 [0.000, 15.000],  loss: 0.022948, mae: 0.360392, mean_q: 0.516446, mean_eps: 0.000000
 3811/5000: episode: 159, duration: 0.524s, episode steps:  22, steps per second:  42, episode reward: 35.431, mean reward:  1.610 [-2.468, 31.991], mean action: 4.364 [0.000, 19.000],  loss: 0.024587, mae: 0.370047, mean_q: 0.497564, mean_eps: 0.000000
 3848/5000: episode: 160, duration: 0.534s, episode steps:  37, steps per second:  69, episode reward: 37.660, mean reward:  1.018 [-2.387, 31.918], mean action: 3.946 [0.000, 15.000],  loss: 0.021026, mae: 0.366727, mean_q: 0.497955, mean_eps: 0.000000
 3875/5000: episode: 161, duration: 0.381s, episode steps:  27, steps per second:  71, episode reward: 35.220, mean reward:  1.304 [-2.467, 32.270], mean action: 4.370 [0.000, 15.000],  loss: 0.021560, mae: 0.370554, mean_q: 0.437081, mean_eps: 0.000000
 3901/5000: episode: 162, duration: 0.364s, episode steps:  26, steps per second:  71, episode reward: -35.650, mean reward: -1.371 [-31.903,  2.342], mean action: 4.577 [0.000, 11.000],  loss: 0.018288, mae: 0.347205, mean_q: 0.460980, mean_eps: 0.000000
 3919/5000: episode: 163, duration: 0.258s, episode steps:  18, steps per second:  70, episode reward: 38.663, mean reward:  2.148 [-2.558, 32.926], mean action: 4.722 [0.000, 14.000],  loss: 0.023445, mae: 0.364030, mean_q: 0.504575, mean_eps: 0.000000
 3933/5000: episode: 164, duration: 0.209s, episode steps:  14, steps per second:  67, episode reward: 41.040, mean reward:  2.931 [-2.282, 31.820], mean action: 3.929 [0.000, 20.000],  loss: 0.027136, mae: 0.388686, mean_q: 0.489789, mean_eps: 0.000000
 3953/5000: episode: 165, duration: 0.287s, episode steps:  20, steps per second:  70, episode reward: -38.590, mean reward: -1.930 [-32.050,  2.903], mean action: 8.300 [0.000, 20.000],  loss: 0.022314, mae: 0.365952, mean_q: 0.527802, mean_eps: 0.000000
 3976/5000: episode: 166, duration: 0.329s, episode steps:  23, steps per second:  70, episode reward: -32.320, mean reward: -1.405 [-32.097,  2.903], mean action: 5.609 [0.000, 14.000],  loss: 0.021303, mae: 0.361058, mean_q: 0.529746, mean_eps: 0.000000
 3998/5000: episode: 167, duration: 0.311s, episode steps:  22, steps per second:  71, episode reward: -35.200, mean reward: -1.600 [-32.462,  2.810], mean action: 4.682 [0.000, 20.000],  loss: 0.022085, mae: 0.365600, mean_q: 0.570880, mean_eps: 0.000000
 4032/5000: episode: 168, duration: 0.481s, episode steps:  34, steps per second:  71, episode reward: 32.305, mean reward:  0.950 [-2.215, 32.080], mean action: 3.353 [0.000, 15.000],  loss: 0.020398, mae: 0.363803, mean_q: 0.550175, mean_eps: 0.000000
 4050/5000: episode: 169, duration: 0.292s, episode steps:  18, steps per second:  62, episode reward: 44.202, mean reward:  2.456 [-2.204, 32.599], mean action: 0.944 [0.000, 8.000],  loss: 0.017216, mae: 0.349263, mean_q: 0.521598, mean_eps: 0.000000
 4077/5000: episode: 170, duration: 0.410s, episode steps:  27, steps per second:  66, episode reward: -32.780, mean reward: -1.214 [-32.171,  2.571], mean action: 2.926 [0.000, 12.000],  loss: 0.021825, mae: 0.366777, mean_q: 0.542378, mean_eps: 0.000000
 4092/5000: episode: 171, duration: 0.229s, episode steps:  15, steps per second:  65, episode reward: 39.000, mean reward:  2.600 [-2.629, 30.157], mean action: 1.400 [0.000, 15.000],  loss: 0.022470, mae: 0.371220, mean_q: 0.585486, mean_eps: 0.000000
 4117/5000: episode: 172, duration: 0.352s, episode steps:  25, steps per second:  71, episode reward: 32.357, mean reward:  1.294 [-2.551, 31.973], mean action: 3.640 [0.000, 12.000],  loss: 0.017827, mae: 0.350935, mean_q: 0.543972, mean_eps: 0.000000
 4138/5000: episode: 173, duration: 0.303s, episode steps:  21, steps per second:  69, episode reward: 37.562, mean reward:  1.789 [-2.233, 31.897], mean action: 5.190 [0.000, 16.000],  loss: 0.023154, mae: 0.376171, mean_q: 0.502464, mean_eps: 0.000000
 4159/5000: episode: 174, duration: 0.303s, episode steps:  21, steps per second:  69, episode reward: 33.000, mean reward:  1.571 [-2.817, 30.108], mean action: 2.048 [0.000, 16.000],  loss: 0.021460, mae: 0.361653, mean_q: 0.511285, mean_eps: 0.000000
 4177/5000: episode: 175, duration: 0.267s, episode steps:  18, steps per second:  67, episode reward: 43.972, mean reward:  2.443 [-2.357, 32.160], mean action: 3.167 [0.000, 12.000],  loss: 0.018194, mae: 0.362770, mean_q: 0.527622, mean_eps: 0.000000
 4203/5000: episode: 176, duration: 0.368s, episode steps:  26, steps per second:  71, episode reward: 38.096, mean reward:  1.465 [-2.998, 32.080], mean action: 4.423 [0.000, 19.000],  loss: 0.021178, mae: 0.371372, mean_q: 0.539695, mean_eps: 0.000000
 4237/5000: episode: 177, duration: 0.476s, episode steps:  34, steps per second:  71, episode reward: 41.323, mean reward:  1.215 [-2.443, 32.055], mean action: 2.676 [0.000, 19.000],  loss: 0.022609, mae: 0.366322, mean_q: 0.558343, mean_eps: 0.000000
 4258/5000: episode: 178, duration: 0.301s, episode steps:  21, steps per second:  70, episode reward: 36.000, mean reward:  1.714 [-2.601, 32.540], mean action: 4.476 [0.000, 19.000],  loss: 0.021140, mae: 0.361116, mean_q: 0.523680, mean_eps: 0.000000
 4282/5000: episode: 179, duration: 0.341s, episode steps:  24, steps per second:  70, episode reward: 32.458, mean reward:  1.352 [-3.000, 31.728], mean action: 3.333 [0.000, 15.000],  loss: 0.021309, mae: 0.360328, mean_q: 0.551407, mean_eps: 0.000000
 4308/5000: episode: 180, duration: 0.396s, episode steps:  26, steps per second:  66, episode reward: 41.078, mean reward:  1.580 [-2.260, 32.183], mean action: 2.500 [0.000, 9.000],  loss: 0.018274, mae: 0.344284, mean_q: 0.550455, mean_eps: 0.000000
 4339/5000: episode: 181, duration: 0.436s, episode steps:  31, steps per second:  71, episode reward: 34.742, mean reward:  1.121 [-2.523, 32.171], mean action: 5.355 [0.000, 14.000],  loss: 0.022955, mae: 0.372489, mean_q: 0.494095, mean_eps: 0.000000
 4362/5000: episode: 182, duration: 3.922s, episode steps:  23, steps per second:   6, episode reward: -38.520, mean reward: -1.675 [-31.646,  2.900], mean action: 4.739 [0.000, 12.000],  loss: 0.025010, mae: 0.378747, mean_q: 0.560736, mean_eps: 0.000000
 4383/5000: episode: 183, duration: 0.302s, episode steps:  21, steps per second:  70, episode reward: 41.740, mean reward:  1.988 [-2.541, 32.280], mean action: 2.190 [0.000, 15.000],  loss: 0.025207, mae: 0.376806, mean_q: 0.654732, mean_eps: 0.000000
 4403/5000: episode: 184, duration: 0.284s, episode steps:  20, steps per second:  70, episode reward: -38.800, mean reward: -1.940 [-32.388,  2.280], mean action: 6.300 [0.000, 20.000],  loss: 0.022823, mae: 0.363548, mean_q: 0.615140, mean_eps: 0.000000
 4421/5000: episode: 185, duration: 0.260s, episode steps:  18, steps per second:  69, episode reward: 41.320, mean reward:  2.296 [-2.488, 31.907], mean action: 3.722 [0.000, 15.000],  loss: 0.018600, mae: 0.345831, mean_q: 0.531545, mean_eps: 0.000000
 4445/5000: episode: 186, duration: 0.346s, episode steps:  24, steps per second:  69, episode reward: 34.909, mean reward:  1.455 [-3.000, 32.210], mean action: 5.708 [0.000, 16.000],  loss: 0.018473, mae: 0.348275, mean_q: 0.537218, mean_eps: 0.000000
 4469/5000: episode: 187, duration: 0.354s, episode steps:  24, steps per second:  68, episode reward: 38.968, mean reward:  1.624 [-2.133, 32.180], mean action: 2.417 [0.000, 16.000],  loss: 0.022132, mae: 0.356654, mean_q: 0.564772, mean_eps: 0.000000
 4496/5000: episode: 188, duration: 0.375s, episode steps:  27, steps per second:  72, episode reward: -32.710, mean reward: -1.211 [-32.164,  3.000], mean action: 7.444 [0.000, 16.000],  loss: 0.019434, mae: 0.348803, mean_q: 0.570808, mean_eps: 0.000000
 4518/5000: episode: 189, duration: 0.316s, episode steps:  22, steps per second:  70, episode reward: 35.719, mean reward:  1.624 [-2.584, 32.081], mean action: 6.364 [0.000, 16.000],  loss: 0.020823, mae: 0.362021, mean_q: 0.487656, mean_eps: 0.000000
 4540/5000: episode: 190, duration: 0.318s, episode steps:  22, steps per second:  69, episode reward: -32.740, mean reward: -1.488 [-32.647,  3.062], mean action: 10.182 [0.000, 16.000],  loss: 0.021269, mae: 0.362249, mean_q: 0.506792, mean_eps: 0.000000
 4559/5000: episode: 191, duration: 0.278s, episode steps:  19, steps per second:  68, episode reward: 35.730, mean reward:  1.881 [-2.342, 32.730], mean action: 5.316 [0.000, 16.000],  loss: 0.020566, mae: 0.353399, mean_q: 0.560394, mean_eps: 0.000000
 4588/5000: episode: 192, duration: 0.409s, episode steps:  29, steps per second:  71, episode reward: 35.638, mean reward:  1.229 [-2.450, 32.140], mean action: 3.414 [0.000, 16.000],  loss: 0.025224, mae: 0.372999, mean_q: 0.566108, mean_eps: 0.000000
 4605/5000: episode: 193, duration: 0.244s, episode steps:  17, steps per second:  70, episode reward: 39.000, mean reward:  2.294 [-2.396, 32.150], mean action: 5.235 [0.000, 15.000],  loss: 0.020581, mae: 0.356894, mean_q: 0.571477, mean_eps: 0.000000
 4631/5000: episode: 194, duration: 0.367s, episode steps:  26, steps per second:  71, episode reward: 38.070, mean reward:  1.464 [-2.529, 32.760], mean action: 6.115 [0.000, 20.000],  loss: 0.019947, mae: 0.356606, mean_q: 0.575262, mean_eps: 0.000000
 4652/5000: episode: 195, duration: 0.298s, episode steps:  21, steps per second:  70, episode reward: 32.654, mean reward:  1.555 [-3.000, 29.570], mean action: 3.714 [0.000, 9.000],  loss: 0.022011, mae: 0.366535, mean_q: 0.533995, mean_eps: 0.000000
 4676/5000: episode: 196, duration: 0.340s, episode steps:  24, steps per second:  71, episode reward: -35.620, mean reward: -1.484 [-32.089,  2.050], mean action: 7.750 [0.000, 20.000],  loss: 0.023798, mae: 0.373349, mean_q: 0.519460, mean_eps: 0.000000
 4707/5000: episode: 197, duration: 0.437s, episode steps:  31, steps per second:  71, episode reward: -32.090, mean reward: -1.035 [-31.822,  2.590], mean action: 4.097 [0.000, 16.000],  loss: 0.017675, mae: 0.342501, mean_q: 0.516966, mean_eps: 0.000000
 4729/5000: episode: 198, duration: 0.313s, episode steps:  22, steps per second:  70, episode reward: -32.630, mean reward: -1.483 [-31.837,  2.350], mean action: 4.000 [0.000, 16.000],  loss: 0.021842, mae: 0.360233, mean_q: 0.531985, mean_eps: 0.000000
 4757/5000: episode: 199, duration: 0.391s, episode steps:  28, steps per second:  72, episode reward: 33.000, mean reward:  1.179 [-3.000, 33.240], mean action: 8.857 [1.000, 20.000],  loss: 0.019979, mae: 0.346265, mean_q: 0.490814, mean_eps: 0.000000
 4785/5000: episode: 200, duration: 0.396s, episode steps:  28, steps per second:  71, episode reward: 35.113, mean reward:  1.254 [-2.596, 31.936], mean action: 3.929 [0.000, 16.000],  loss: 0.020008, mae: 0.348904, mean_q: 0.509739, mean_eps: 0.000000
 4806/5000: episode: 201, duration: 0.303s, episode steps:  21, steps per second:  69, episode reward: 38.518, mean reward:  1.834 [-2.429, 32.100], mean action: 3.714 [0.000, 15.000],  loss: 0.020023, mae: 0.356879, mean_q: 0.481824, mean_eps: 0.000000
 4832/5000: episode: 202, duration: 0.365s, episode steps:  26, steps per second:  71, episode reward: 35.593, mean reward:  1.369 [-2.634, 32.593], mean action: 2.769 [0.000, 12.000],  loss: 0.023650, mae: 0.372360, mean_q: 0.504512, mean_eps: 0.000000
 4858/5000: episode: 203, duration: 0.365s, episode steps:  26, steps per second:  71, episode reward: 44.201, mean reward:  1.700 [-2.175, 32.910], mean action: 4.769 [0.000, 15.000],  loss: 0.021648, mae: 0.357410, mean_q: 0.485610, mean_eps: 0.000000
 4878/5000: episode: 204, duration: 0.319s, episode steps:  20, steps per second:  63, episode reward: 38.390, mean reward:  1.920 [-2.550, 33.000], mean action: 6.450 [0.000, 20.000],  loss: 0.022142, mae: 0.361820, mean_q: 0.500944, mean_eps: 0.000000
 4906/5000: episode: 205, duration: 0.393s, episode steps:  28, steps per second:  71, episode reward: -36.000, mean reward: -1.286 [-32.137,  2.350], mean action: 5.643 [0.000, 12.000],  loss: 0.021238, mae: 0.357881, mean_q: 0.513886, mean_eps: 0.000000
 4929/5000: episode: 206, duration: 0.325s, episode steps:  23, steps per second:  71, episode reward: -32.020, mean reward: -1.392 [-32.080,  2.600], mean action: 5.739 [0.000, 19.000],  loss: 0.023569, mae: 0.362189, mean_q: 0.539907, mean_eps: 0.000000
 4954/5000: episode: 207, duration: 0.357s, episode steps:  25, steps per second:  70, episode reward: 39.000, mean reward:  1.560 [-2.600, 32.180], mean action: 3.680 [0.000, 15.000],  loss: 0.022609, mae: 0.357653, mean_q: 0.581627, mean_eps: 0.000000
 4967/5000: episode: 208, duration: 0.196s, episode steps:  13, steps per second:  66, episode reward: 41.759, mean reward:  3.212 [-2.605, 33.000], mean action: 4.385 [0.000, 19.000],  loss: 0.017851, mae: 0.348835, mean_q: 0.525635, mean_eps: 0.000000
 4986/5000: episode: 209, duration: 0.270s, episode steps:  19, steps per second:  70, episode reward: 35.632, mean reward:  1.875 [-3.000, 31.732], mean action: 2.895 [0.000, 12.000],  loss: 0.014917, mae: 0.338527, mean_q: 0.497600, mean_eps: 0.000000
done, took 68.660 seconds
DQN Evaluation: 8162 victories out of 9578 episodes
Training for 5000 steps ...
   20/5000: episode: 1, duration: 0.171s, episode steps:  20, steps per second: 117, episode reward: 38.816, mean reward:  1.941 [-2.876, 32.830], mean action: 3.950 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   51/5000: episode: 2, duration: 0.215s, episode steps:  31, steps per second: 145, episode reward: 44.335, mean reward:  1.430 [-2.007, 32.230], mean action: 4.387 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   78/5000: episode: 3, duration: 0.186s, episode steps:  27, steps per second: 145, episode reward: 39.000, mean reward:  1.444 [-3.000, 32.120], mean action: 3.667 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   97/5000: episode: 4, duration: 0.141s, episode steps:  19, steps per second: 135, episode reward: 38.877, mean reward:  2.046 [-2.170, 32.910], mean action: 3.737 [1.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  130/5000: episode: 5, duration: 0.209s, episode steps:  33, steps per second: 158, episode reward: 33.000, mean reward:  1.000 [-3.000, 32.050], mean action: 5.000 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  171/5000: episode: 6, duration: 0.261s, episode steps:  41, steps per second: 157, episode reward: 38.313, mean reward:  0.934 [-2.517, 32.020], mean action: 3.341 [1.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  196/5000: episode: 7, duration: 0.175s, episode steps:  25, steps per second: 143, episode reward: 44.937, mean reward:  1.797 [-2.021, 32.550], mean action: 2.960 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  226/5000: episode: 8, duration: 0.193s, episode steps:  30, steps per second: 155, episode reward: 35.183, mean reward:  1.173 [-2.712, 31.243], mean action: 4.600 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  248/5000: episode: 9, duration: 0.160s, episode steps:  22, steps per second: 137, episode reward: 41.604, mean reward:  1.891 [-2.272, 32.368], mean action: 3.318 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  273/5000: episode: 10, duration: 0.170s, episode steps:  25, steps per second: 147, episode reward: 38.151, mean reward:  1.526 [-3.000, 32.178], mean action: 4.160 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  293/5000: episode: 11, duration: 0.139s, episode steps:  20, steps per second: 144, episode reward: 38.614, mean reward:  1.931 [-3.000, 32.250], mean action: 3.600 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  322/5000: episode: 12, duration: 0.190s, episode steps:  29, steps per second: 153, episode reward: 41.623, mean reward:  1.435 [-2.628, 32.150], mean action: 3.103 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  348/5000: episode: 13, duration: 0.177s, episode steps:  26, steps per second: 147, episode reward: 41.314, mean reward:  1.589 [-2.121, 32.480], mean action: 2.808 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  375/5000: episode: 14, duration: 0.173s, episode steps:  27, steps per second: 156, episode reward: 41.281, mean reward:  1.529 [-2.356, 31.926], mean action: 4.370 [1.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  417/5000: episode: 15, duration: 0.249s, episode steps:  42, steps per second: 169, episode reward: 35.519, mean reward:  0.846 [-3.000, 31.963], mean action: 4.881 [1.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  432/5000: episode: 16, duration: 0.109s, episode steps:  15, steps per second: 137, episode reward: 43.971, mean reward:  2.931 [-2.316, 31.796], mean action: 2.400 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  452/5000: episode: 17, duration: 0.138s, episode steps:  20, steps per second: 145, episode reward: 44.639, mean reward:  2.232 [-2.200, 32.220], mean action: 3.050 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  473/5000: episode: 18, duration: 0.146s, episode steps:  21, steps per second: 143, episode reward: 46.685, mean reward:  2.223 [-0.125, 31.153], mean action: 3.143 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  487/5000: episode: 19, duration: 0.104s, episode steps:  14, steps per second: 134, episode reward: 41.806, mean reward:  2.986 [-2.620, 32.403], mean action: 6.357 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  529/5000: episode: 20, duration: 0.248s, episode steps:  42, steps per second: 169, episode reward: 41.202, mean reward:  0.981 [-2.525, 31.932], mean action: 2.000 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  555/5000: episode: 21, duration: 0.173s, episode steps:  26, steps per second: 150, episode reward: 38.741, mean reward:  1.490 [-2.690, 32.050], mean action: 3.846 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  594/5000: episode: 22, duration: 0.242s, episode steps:  39, steps per second: 161, episode reward: 36.000, mean reward:  0.923 [-2.352, 29.864], mean action: 3.872 [1.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  634/5000: episode: 23, duration: 0.257s, episode steps:  40, steps per second: 155, episode reward: 41.687, mean reward:  1.042 [-2.063, 32.380], mean action: 3.450 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  658/5000: episode: 24, duration: 0.164s, episode steps:  24, steps per second: 146, episode reward: 47.273, mean reward:  1.970 [-0.246, 32.069], mean action: 1.792 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  676/5000: episode: 25, duration: 0.131s, episode steps:  18, steps per second: 137, episode reward: 46.819, mean reward:  2.601 [-0.822, 32.730], mean action: 1.278 [0.000, 7.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  698/5000: episode: 26, duration: 0.151s, episode steps:  22, steps per second: 146, episode reward: 38.183, mean reward:  1.736 [-2.584, 32.075], mean action: 3.773 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  719/5000: episode: 27, duration: 0.146s, episode steps:  21, steps per second: 144, episode reward: 41.851, mean reward:  1.993 [-2.172, 32.100], mean action: 2.476 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  745/5000: episode: 28, duration: 0.168s, episode steps:  26, steps per second: 154, episode reward: 34.876, mean reward:  1.341 [-3.000, 31.829], mean action: 4.269 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  764/5000: episode: 29, duration: 0.129s, episode steps:  19, steps per second: 147, episode reward: 41.694, mean reward:  2.194 [-2.473, 32.329], mean action: 3.947 [1.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  795/5000: episode: 30, duration: 0.201s, episode steps:  31, steps per second: 154, episode reward: 35.515, mean reward:  1.146 [-2.183, 32.230], mean action: 3.323 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  815/5000: episode: 31, duration: 0.144s, episode steps:  20, steps per second: 139, episode reward: 39.000, mean reward:  1.950 [-2.798, 32.480], mean action: 3.500 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  844/5000: episode: 32, duration: 0.383s, episode steps:  29, steps per second:  76, episode reward: 39.000, mean reward:  1.345 [-3.000, 32.280], mean action: 3.103 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  865/5000: episode: 33, duration: 0.193s, episode steps:  21, steps per second: 109, episode reward: 39.000, mean reward:  1.857 [-2.925, 32.240], mean action: 3.476 [1.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  897/5000: episode: 34, duration: 0.211s, episode steps:  32, steps per second: 152, episode reward: 40.856, mean reward:  1.277 [-2.380, 32.091], mean action: 3.906 [1.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  920/5000: episode: 35, duration: 0.160s, episode steps:  23, steps per second: 144, episode reward: 44.474, mean reward:  1.934 [-2.431, 32.120], mean action: 2.174 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  939/5000: episode: 36, duration: 0.133s, episode steps:  19, steps per second: 143, episode reward: 46.200, mean reward:  2.432 [-0.454, 32.610], mean action: 3.474 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  980/5000: episode: 37, duration: 0.263s, episode steps:  41, steps per second: 156, episode reward: 41.354, mean reward:  1.009 [-2.558, 32.320], mean action: 3.463 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1003/5000: episode: 38, duration: 0.198s, episode steps:  23, steps per second: 116, episode reward: 47.267, mean reward:  2.055 [-0.268, 32.190], mean action: 2.087 [0.000, 3.000],  loss: 0.012962, mae: 0.313590, mean_q: 0.411806, mean_eps: 0.000000
 1023/5000: episode: 39, duration: 0.336s, episode steps:  20, steps per second:  59, episode reward: 41.904, mean reward:  2.095 [-2.406, 32.044], mean action: 2.450 [0.000, 19.000],  loss: 0.020446, mae: 0.358516, mean_q: 0.516045, mean_eps: 0.000000
 1047/5000: episode: 40, duration: 0.346s, episode steps:  24, steps per second:  69, episode reward: 41.347, mean reward:  1.723 [-2.644, 32.220], mean action: 3.542 [1.000, 19.000],  loss: 0.020204, mae: 0.359978, mean_q: 0.519949, mean_eps: 0.000000
 1063/5000: episode: 41, duration: 0.234s, episode steps:  16, steps per second:  68, episode reward: 44.378, mean reward:  2.774 [-2.299, 32.310], mean action: 3.625 [1.000, 9.000],  loss: 0.018232, mae: 0.357799, mean_q: 0.511187, mean_eps: 0.000000
 1088/5000: episode: 42, duration: 0.366s, episode steps:  25, steps per second:  68, episode reward: 43.977, mean reward:  1.759 [-2.155, 32.070], mean action: 4.200 [0.000, 19.000],  loss: 0.020530, mae: 0.367183, mean_q: 0.442945, mean_eps: 0.000000
 1110/5000: episode: 43, duration: 0.314s, episode steps:  22, steps per second:  70, episode reward: 41.336, mean reward:  1.879 [-2.424, 32.187], mean action: 2.409 [0.000, 19.000],  loss: 0.021528, mae: 0.364895, mean_q: 0.477036, mean_eps: 0.000000
 1138/5000: episode: 44, duration: 0.739s, episode steps:  28, steps per second:  38, episode reward: 38.005, mean reward:  1.357 [-2.156, 32.273], mean action: 2.607 [0.000, 14.000],  loss: 0.019828, mae: 0.350419, mean_q: 0.535712, mean_eps: 0.000000
 1163/5000: episode: 45, duration: 0.420s, episode steps:  25, steps per second:  59, episode reward: 38.603, mean reward:  1.544 [-2.491, 31.849], mean action: 2.680 [0.000, 12.000],  loss: 0.019488, mae: 0.343756, mean_q: 0.558634, mean_eps: 0.000000
 1206/5000: episode: 46, duration: 0.655s, episode steps:  43, steps per second:  66, episode reward: -42.300, mean reward: -0.984 [-32.167,  2.780], mean action: 5.047 [0.000, 14.000],  loss: 0.020698, mae: 0.347596, mean_q: 0.572624, mean_eps: 0.000000
 1235/5000: episode: 47, duration: 0.452s, episode steps:  29, steps per second:  64, episode reward: 44.483, mean reward:  1.534 [-2.798, 32.260], mean action: 2.207 [0.000, 13.000],  loss: 0.019583, mae: 0.342170, mean_q: 0.551941, mean_eps: 0.000000
 1263/5000: episode: 48, duration: 0.430s, episode steps:  28, steps per second:  65, episode reward: 39.000, mean reward:  1.393 [-3.000, 32.080], mean action: 5.821 [0.000, 14.000],  loss: 0.021422, mae: 0.350251, mean_q: 0.549285, mean_eps: 0.000000
 1289/5000: episode: 49, duration: 0.389s, episode steps:  26, steps per second:  67, episode reward: 33.000, mean reward:  1.269 [-3.000, 32.380], mean action: 5.154 [0.000, 21.000],  loss: 0.020377, mae: 0.350641, mean_q: 0.537709, mean_eps: 0.000000
 1308/5000: episode: 50, duration: 0.298s, episode steps:  19, steps per second:  64, episode reward: 38.496, mean reward:  2.026 [-2.903, 32.096], mean action: 2.895 [0.000, 16.000],  loss: 0.021799, mae: 0.356270, mean_q: 0.516318, mean_eps: 0.000000
 1339/5000: episode: 51, duration: 0.429s, episode steps:  31, steps per second:  72, episode reward: 36.000, mean reward:  1.161 [-2.481, 32.210], mean action: 4.452 [0.000, 20.000],  loss: 0.020228, mae: 0.357033, mean_q: 0.492647, mean_eps: 0.000000
 1361/5000: episode: 52, duration: 0.320s, episode steps:  22, steps per second:  69, episode reward: 45.000, mean reward:  2.045 [-3.000, 32.130], mean action: 2.364 [0.000, 19.000],  loss: 0.023943, mae: 0.372669, mean_q: 0.481819, mean_eps: 0.000000
 1410/5000: episode: 53, duration: 0.673s, episode steps:  49, steps per second:  73, episode reward: 39.000, mean reward:  0.796 [-2.017, 32.170], mean action: 2.735 [0.000, 19.000],  loss: 0.018010, mae: 0.345019, mean_q: 0.552847, mean_eps: 0.000000
 1430/5000: episode: 54, duration: 0.296s, episode steps:  20, steps per second:  68, episode reward: 37.883, mean reward:  1.894 [-2.415, 32.103], mean action: 6.650 [0.000, 15.000],  loss: 0.021875, mae: 0.354676, mean_q: 0.558448, mean_eps: 0.000000
 1444/5000: episode: 55, duration: 0.216s, episode steps:  14, steps per second:  65, episode reward: 44.288, mean reward:  3.163 [-2.729, 31.994], mean action: 1.071 [0.000, 9.000],  loss: 0.023186, mae: 0.367034, mean_q: 0.533685, mean_eps: 0.000000
 1481/5000: episode: 56, duration: 0.526s, episode steps:  37, steps per second:  70, episode reward: 32.717, mean reward:  0.884 [-2.998, 32.040], mean action: 7.027 [0.000, 19.000],  loss: 0.019811, mae: 0.349273, mean_q: 0.501327, mean_eps: 0.000000
 1496/5000: episode: 57, duration: 0.219s, episode steps:  15, steps per second:  68, episode reward: 42.000, mean reward:  2.800 [-2.141, 32.400], mean action: 2.467 [0.000, 9.000],  loss: 0.022782, mae: 0.361569, mean_q: 0.510305, mean_eps: 0.000000
 1520/5000: episode: 58, duration: 0.343s, episode steps:  24, steps per second:  70, episode reward: 44.840, mean reward:  1.868 [-2.104, 32.149], mean action: 3.208 [0.000, 12.000],  loss: 0.019657, mae: 0.357340, mean_q: 0.514947, mean_eps: 0.000000
 1541/5000: episode: 59, duration: 0.313s, episode steps:  21, steps per second:  67, episode reward: 41.253, mean reward:  1.964 [-2.805, 32.120], mean action: 1.905 [0.000, 12.000],  loss: 0.019156, mae: 0.360968, mean_q: 0.513285, mean_eps: 0.000000
 1562/5000: episode: 60, duration: 0.309s, episode steps:  21, steps per second:  68, episode reward: 35.585, mean reward:  1.695 [-3.000, 32.160], mean action: 3.571 [0.000, 15.000],  loss: 0.018508, mae: 0.348921, mean_q: 0.547660, mean_eps: 0.000000
 1586/5000: episode: 61, duration: 0.342s, episode steps:  24, steps per second:  70, episode reward: 41.299, mean reward:  1.721 [-2.269, 32.331], mean action: 3.208 [0.000, 15.000],  loss: 0.019168, mae: 0.345173, mean_q: 0.584492, mean_eps: 0.000000
 1620/5000: episode: 62, duration: 0.479s, episode steps:  34, steps per second:  71, episode reward: 41.369, mean reward:  1.217 [-2.165, 32.450], mean action: 3.000 [1.000, 15.000],  loss: 0.024711, mae: 0.374187, mean_q: 0.544358, mean_eps: 0.000000
 1637/5000: episode: 63, duration: 0.249s, episode steps:  17, steps per second:  68, episode reward: 39.000, mean reward:  2.294 [-2.335, 30.372], mean action: 2.588 [0.000, 15.000],  loss: 0.025966, mae: 0.378848, mean_q: 0.507162, mean_eps: 0.000000
 1668/5000: episode: 64, duration: 0.434s, episode steps:  31, steps per second:  71, episode reward: 42.000, mean reward:  1.355 [-2.646, 32.290], mean action: 1.387 [0.000, 15.000],  loss: 0.023763, mae: 0.370560, mean_q: 0.554201, mean_eps: 0.000000
 1688/5000: episode: 65, duration: 0.461s, episode steps:  20, steps per second:  43, episode reward: 38.902, mean reward:  1.945 [-3.000, 32.012], mean action: 3.850 [0.000, 16.000],  loss: 0.022969, mae: 0.367426, mean_q: 0.573807, mean_eps: 0.000000
 1710/5000: episode: 66, duration: 0.319s, episode steps:  22, steps per second:  69, episode reward: 44.085, mean reward:  2.004 [-2.268, 31.972], mean action: 2.455 [0.000, 15.000],  loss: 0.017894, mae: 0.340937, mean_q: 0.587006, mean_eps: 0.000000
 1742/5000: episode: 67, duration: 0.444s, episode steps:  32, steps per second:  72, episode reward: 35.017, mean reward:  1.094 [-3.000, 32.306], mean action: 4.750 [0.000, 19.000],  loss: 0.020007, mae: 0.357913, mean_q: 0.564247, mean_eps: 0.000000
 1773/5000: episode: 68, duration: 0.429s, episode steps:  31, steps per second:  72, episode reward: 37.600, mean reward:  1.213 [-2.498, 32.251], mean action: 4.935 [0.000, 19.000],  loss: 0.019872, mae: 0.352655, mean_q: 0.547219, mean_eps: 0.000000
 1810/5000: episode: 69, duration: 0.514s, episode steps:  37, steps per second:  72, episode reward: 39.000, mean reward:  1.054 [-2.643, 32.090], mean action: 2.973 [0.000, 16.000],  loss: 0.020239, mae: 0.350667, mean_q: 0.565025, mean_eps: 0.000000
 1835/5000: episode: 70, duration: 0.355s, episode steps:  25, steps per second:  70, episode reward: 39.000, mean reward:  1.560 [-2.402, 29.816], mean action: 2.600 [0.000, 15.000],  loss: 0.019450, mae: 0.347162, mean_q: 0.501262, mean_eps: 0.000000
 1857/5000: episode: 71, duration: 0.322s, episode steps:  22, steps per second:  68, episode reward: 43.928, mean reward:  1.997 [-2.196, 32.220], mean action: 4.136 [0.000, 14.000],  loss: 0.019439, mae: 0.349128, mean_q: 0.480795, mean_eps: 0.000000
 1900/5000: episode: 72, duration: 0.595s, episode steps:  43, steps per second:  72, episode reward: 34.175, mean reward:  0.795 [-3.000, 32.031], mean action: 4.233 [0.000, 16.000],  loss: 0.019657, mae: 0.361058, mean_q: 0.473017, mean_eps: 0.000000
 1925/5000: episode: 73, duration: 0.357s, episode steps:  25, steps per second:  70, episode reward: 32.372, mean reward:  1.295 [-3.000, 32.201], mean action: 3.840 [1.000, 16.000],  loss: 0.022911, mae: 0.370082, mean_q: 0.548876, mean_eps: 0.000000
 1948/5000: episode: 74, duration: 0.328s, episode steps:  23, steps per second:  70, episode reward: 43.858, mean reward:  1.907 [-2.676, 32.330], mean action: 2.652 [0.000, 12.000],  loss: 0.023771, mae: 0.376931, mean_q: 0.483716, mean_eps: 0.000000
 1970/5000: episode: 75, duration: 0.322s, episode steps:  22, steps per second:  68, episode reward: 44.254, mean reward:  2.012 [-2.376, 32.130], mean action: 2.727 [0.000, 13.000],  loss: 0.020512, mae: 0.355975, mean_q: 0.511989, mean_eps: 0.000000
 2000/5000: episode: 76, duration: 0.424s, episode steps:  30, steps per second:  71, episode reward: 35.644, mean reward:  1.188 [-2.766, 31.734], mean action: 4.267 [0.000, 14.000],  loss: 0.018064, mae: 0.344693, mean_q: 0.526140, mean_eps: 0.000000
 2019/5000: episode: 77, duration: 0.276s, episode steps:  19, steps per second:  69, episode reward: 41.109, mean reward:  2.164 [-2.064, 31.776], mean action: 4.474 [0.000, 14.000],  loss: 0.019928, mae: 0.355833, mean_q: 0.481219, mean_eps: 0.000000
 2041/5000: episode: 78, duration: 0.332s, episode steps:  22, steps per second:  66, episode reward: 44.586, mean reward:  2.027 [-2.094, 32.370], mean action: 1.000 [0.000, 3.000],  loss: 0.018964, mae: 0.348587, mean_q: 0.479441, mean_eps: 0.000000
 2075/5000: episode: 79, duration: 0.473s, episode steps:  34, steps per second:  72, episode reward: 41.316, mean reward:  1.215 [-2.052, 32.120], mean action: 2.912 [0.000, 15.000],  loss: 0.017429, mae: 0.339579, mean_q: 0.520428, mean_eps: 0.000000
 2103/5000: episode: 80, duration: 0.389s, episode steps:  28, steps per second:  72, episode reward: 41.262, mean reward:  1.474 [-3.000, 32.106], mean action: 2.607 [0.000, 11.000],  loss: 0.023868, mae: 0.368997, mean_q: 0.506814, mean_eps: 0.000000
 2126/5000: episode: 81, duration: 0.332s, episode steps:  23, steps per second:  69, episode reward: 43.245, mean reward:  1.880 [-2.469, 31.942], mean action: 3.652 [0.000, 20.000],  loss: 0.020885, mae: 0.360511, mean_q: 0.491012, mean_eps: 0.000000
 2153/5000: episode: 82, duration: 0.412s, episode steps:  27, steps per second:  66, episode reward: 41.939, mean reward:  1.553 [-2.257, 30.006], mean action: 3.444 [0.000, 20.000],  loss: 0.023417, mae: 0.363119, mean_q: 0.549704, mean_eps: 0.000000
 2170/5000: episode: 83, duration: 0.248s, episode steps:  17, steps per second:  68, episode reward: 44.737, mean reward:  2.632 [-3.000, 32.360], mean action: 2.882 [0.000, 19.000],  loss: 0.020857, mae: 0.356005, mean_q: 0.527542, mean_eps: 0.000000
 2194/5000: episode: 84, duration: 0.335s, episode steps:  24, steps per second:  72, episode reward: 38.313, mean reward:  1.596 [-3.000, 32.280], mean action: 5.125 [0.000, 19.000],  loss: 0.024908, mae: 0.378157, mean_q: 0.480513, mean_eps: 0.000000
 2218/5000: episode: 85, duration: 0.336s, episode steps:  24, steps per second:  71, episode reward: 42.035, mean reward:  1.751 [-2.326, 32.193], mean action: 5.083 [0.000, 15.000],  loss: 0.023268, mae: 0.363336, mean_q: 0.504615, mean_eps: 0.000000
 2257/5000: episode: 86, duration: 0.543s, episode steps:  39, steps per second:  72, episode reward: 40.425, mean reward:  1.037 [-2.318, 30.271], mean action: 3.179 [0.000, 12.000],  loss: 0.020032, mae: 0.341916, mean_q: 0.518005, mean_eps: 0.000000
 2284/5000: episode: 87, duration: 0.376s, episode steps:  27, steps per second:  72, episode reward: 39.000, mean reward:  1.444 [-3.000, 32.380], mean action: 4.519 [0.000, 14.000],  loss: 0.020196, mae: 0.341290, mean_q: 0.519504, mean_eps: 0.000000
 2315/5000: episode: 88, duration: 0.439s, episode steps:  31, steps per second:  71, episode reward: 35.290, mean reward:  1.138 [-3.000, 31.928], mean action: 3.871 [0.000, 20.000],  loss: 0.017670, mae: 0.331968, mean_q: 0.511551, mean_eps: 0.000000
 2329/5000: episode: 89, duration: 0.209s, episode steps:  14, steps per second:  67, episode reward: 45.000, mean reward:  3.214 [-2.559, 32.440], mean action: 1.429 [0.000, 11.000],  loss: 0.018838, mae: 0.332202, mean_q: 0.523843, mean_eps: 0.000000
 2347/5000: episode: 90, duration: 0.264s, episode steps:  18, steps per second:  68, episode reward: 44.878, mean reward:  2.493 [-2.222, 32.410], mean action: 3.167 [0.000, 15.000],  loss: 0.018613, mae: 0.340188, mean_q: 0.511988, mean_eps: 0.000000
 2371/5000: episode: 91, duration: 0.341s, episode steps:  24, steps per second:  70, episode reward: 39.000, mean reward:  1.625 [-2.286, 30.560], mean action: 2.625 [0.000, 16.000],  loss: 0.019839, mae: 0.341917, mean_q: 0.453916, mean_eps: 0.000000
 2427/5000: episode: 92, duration: 0.770s, episode steps:  56, steps per second:  73, episode reward: -35.440, mean reward: -0.633 [-32.556,  2.051], mean action: 5.179 [0.000, 15.000],  loss: 0.020359, mae: 0.345874, mean_q: 0.487202, mean_eps: 0.000000
 2449/5000: episode: 93, duration: 0.314s, episode steps:  22, steps per second:  70, episode reward: 38.580, mean reward:  1.754 [-2.538, 30.379], mean action: 4.955 [0.000, 19.000],  loss: 0.019955, mae: 0.343656, mean_q: 0.511983, mean_eps: 0.000000
 2473/5000: episode: 94, duration: 0.350s, episode steps:  24, steps per second:  69, episode reward: 44.144, mean reward:  1.839 [-2.031, 32.273], mean action: 2.458 [0.000, 13.000],  loss: 0.019500, mae: 0.338602, mean_q: 0.509498, mean_eps: 0.000000
 2494/5000: episode: 95, duration: 0.327s, episode steps:  21, steps per second:  64, episode reward: 47.142, mean reward:  2.245 [-0.252, 32.010], mean action: 1.143 [0.000, 3.000],  loss: 0.020656, mae: 0.344440, mean_q: 0.551034, mean_eps: 0.000000
 2537/5000: episode: 96, duration: 0.589s, episode steps:  43, steps per second:  73, episode reward: -34.850, mean reward: -0.810 [-32.189,  3.000], mean action: 5.000 [0.000, 18.000],  loss: 0.020419, mae: 0.348411, mean_q: 0.520440, mean_eps: 0.000000
 2564/5000: episode: 97, duration: 0.389s, episode steps:  27, steps per second:  69, episode reward: 38.007, mean reward:  1.408 [-2.105, 31.918], mean action: 2.815 [0.000, 12.000],  loss: 0.018359, mae: 0.340578, mean_q: 0.481659, mean_eps: 0.000000
 2589/5000: episode: 98, duration: 0.407s, episode steps:  25, steps per second:  61, episode reward: 39.000, mean reward:  1.560 [-2.197, 32.220], mean action: 4.320 [0.000, 15.000],  loss: 0.021508, mae: 0.351117, mean_q: 0.555632, mean_eps: 0.000000
 2619/5000: episode: 99, duration: 0.427s, episode steps:  30, steps per second:  70, episode reward: 46.645, mean reward:  1.555 [-0.259, 32.494], mean action: 4.233 [0.000, 20.000],  loss: 0.020291, mae: 0.344665, mean_q: 0.525890, mean_eps: 0.000000
 2638/5000: episode: 100, duration: 0.276s, episode steps:  19, steps per second:  69, episode reward: 41.760, mean reward:  2.198 [-2.498, 32.360], mean action: 3.842 [0.000, 15.000],  loss: 0.020553, mae: 0.350510, mean_q: 0.487591, mean_eps: 0.000000
 2662/5000: episode: 101, duration: 0.341s, episode steps:  24, steps per second:  70, episode reward: 38.367, mean reward:  1.599 [-2.073, 31.759], mean action: 3.042 [0.000, 16.000],  loss: 0.020593, mae: 0.347457, mean_q: 0.518942, mean_eps: 0.000000
 2683/5000: episode: 102, duration: 0.307s, episode steps:  21, steps per second:  68, episode reward: 38.876, mean reward:  1.851 [-3.000, 32.091], mean action: 7.095 [0.000, 15.000],  loss: 0.020300, mae: 0.346278, mean_q: 0.512803, mean_eps: 0.000000
 2710/5000: episode: 103, duration: 0.386s, episode steps:  27, steps per second:  70, episode reward: 38.817, mean reward:  1.438 [-3.000, 32.170], mean action: 3.556 [0.000, 19.000],  loss: 0.020437, mae: 0.352095, mean_q: 0.501615, mean_eps: 0.000000
 2734/5000: episode: 104, duration: 0.343s, episode steps:  24, steps per second:  70, episode reward: 44.374, mean reward:  1.849 [-2.158, 32.120], mean action: 5.000 [0.000, 14.000],  loss: 0.024153, mae: 0.359970, mean_q: 0.515420, mean_eps: 0.000000
 2755/5000: episode: 105, duration: 0.306s, episode steps:  21, steps per second:  69, episode reward: 41.082, mean reward:  1.956 [-2.574, 31.660], mean action: 2.810 [0.000, 16.000],  loss: 0.026577, mae: 0.376968, mean_q: 0.567025, mean_eps: 0.000000
 2780/5000: episode: 106, duration: 0.356s, episode steps:  25, steps per second:  70, episode reward: 38.795, mean reward:  1.552 [-2.533, 32.025], mean action: 2.720 [0.000, 16.000],  loss: 0.018778, mae: 0.343311, mean_q: 0.561343, mean_eps: 0.000000
 2815/5000: episode: 107, duration: 0.482s, episode steps:  35, steps per second:  73, episode reward: 40.512, mean reward:  1.157 [-2.491, 32.371], mean action: 3.486 [0.000, 15.000],  loss: 0.019215, mae: 0.342336, mean_q: 0.553627, mean_eps: 0.000000
 2838/5000: episode: 108, duration: 0.366s, episode steps:  23, steps per second:  63, episode reward: 44.322, mean reward:  1.927 [-2.420, 32.100], mean action: 1.391 [0.000, 3.000],  loss: 0.022592, mae: 0.359560, mean_q: 0.610066, mean_eps: 0.000000
 2886/5000: episode: 109, duration: 0.680s, episode steps:  48, steps per second:  71, episode reward: 37.934, mean reward:  0.790 [-3.000, 32.130], mean action: 4.125 [1.000, 15.000],  loss: 0.023538, mae: 0.363923, mean_q: 0.567363, mean_eps: 0.000000
 2905/5000: episode: 110, duration: 0.271s, episode steps:  19, steps per second:  70, episode reward: 36.000, mean reward:  1.895 [-3.000, 32.170], mean action: 3.474 [0.000, 15.000],  loss: 0.018534, mae: 0.343481, mean_q: 0.573039, mean_eps: 0.000000
 2927/5000: episode: 111, duration: 0.313s, episode steps:  22, steps per second:  70, episode reward: 39.000, mean reward:  1.773 [-2.555, 32.220], mean action: 3.545 [0.000, 20.000],  loss: 0.020549, mae: 0.354850, mean_q: 0.557658, mean_eps: 0.000000
 2960/5000: episode: 112, duration: 0.469s, episode steps:  33, steps per second:  70, episode reward: 38.770, mean reward:  1.175 [-2.257, 32.280], mean action: 2.697 [0.000, 19.000],  loss: 0.019943, mae: 0.347270, mean_q: 0.569461, mean_eps: 0.000000
 3003/5000: episode: 113, duration: 0.598s, episode steps:  43, steps per second:  72, episode reward: 38.212, mean reward:  0.889 [-2.663, 32.222], mean action: 5.698 [0.000, 19.000],  loss: 0.021626, mae: 0.357221, mean_q: 0.512720, mean_eps: 0.000000
 3020/5000: episode: 114, duration: 0.271s, episode steps:  17, steps per second:  63, episode reward: 47.363, mean reward:  2.786 [-0.572, 32.510], mean action: 1.235 [1.000, 2.000],  loss: 0.023416, mae: 0.362623, mean_q: 0.550609, mean_eps: 0.000000
 3066/5000: episode: 115, duration: 0.635s, episode steps:  46, steps per second:  72, episode reward: 37.581, mean reward:  0.817 [-2.753, 31.292], mean action: 7.152 [0.000, 20.000],  loss: 0.023986, mae: 0.361161, mean_q: 0.521087, mean_eps: 0.000000
 3093/5000: episode: 116, duration: 0.389s, episode steps:  27, steps per second:  69, episode reward: 41.790, mean reward:  1.548 [-2.639, 32.580], mean action: 5.000 [0.000, 19.000],  loss: 0.022748, mae: 0.356878, mean_q: 0.533837, mean_eps: 0.000000
 3111/5000: episode: 117, duration: 0.263s, episode steps:  18, steps per second:  68, episode reward: 44.711, mean reward:  2.484 [-2.373, 32.680], mean action: 3.389 [0.000, 19.000],  loss: 0.018379, mae: 0.342326, mean_q: 0.557883, mean_eps: 0.000000
 3146/5000: episode: 118, duration: 0.487s, episode steps:  35, steps per second:  72, episode reward: -32.220, mean reward: -0.921 [-31.712,  2.901], mean action: 3.371 [0.000, 16.000],  loss: 0.020642, mae: 0.349234, mean_q: 0.533696, mean_eps: 0.000000
 3170/5000: episode: 119, duration: 0.353s, episode steps:  24, steps per second:  68, episode reward: 38.098, mean reward:  1.587 [-2.879, 32.080], mean action: 2.292 [0.000, 9.000],  loss: 0.021626, mae: 0.357074, mean_q: 0.534869, mean_eps: 0.000000
 3185/5000: episode: 120, duration: 0.224s, episode steps:  15, steps per second:  67, episode reward: 41.071, mean reward:  2.738 [-2.700, 31.301], mean action: 2.133 [0.000, 9.000],  loss: 0.020128, mae: 0.347697, mean_q: 0.508705, mean_eps: 0.000000
 3208/5000: episode: 121, duration: 0.458s, episode steps:  23, steps per second:  50, episode reward: 39.000, mean reward:  1.696 [-2.618, 32.320], mean action: 2.957 [0.000, 16.000],  loss: 0.021287, mae: 0.352765, mean_q: 0.541925, mean_eps: 0.000000
 3228/5000: episode: 122, duration: 0.294s, episode steps:  20, steps per second:  68, episode reward: 44.549, mean reward:  2.227 [-2.041, 32.120], mean action: 2.200 [0.000, 16.000],  loss: 0.015830, mae: 0.330166, mean_q: 0.570885, mean_eps: 0.000000
 3250/5000: episode: 123, duration: 0.318s, episode steps:  22, steps per second:  69, episode reward: 36.000, mean reward:  1.636 [-3.000, 32.200], mean action: 3.773 [0.000, 16.000],  loss: 0.022091, mae: 0.358602, mean_q: 0.598124, mean_eps: 0.000000
 3283/5000: episode: 124, duration: 0.452s, episode steps:  33, steps per second:  73, episode reward: 32.879, mean reward:  0.996 [-3.000, 32.100], mean action: 4.424 [0.000, 16.000],  loss: 0.020654, mae: 0.350749, mean_q: 0.569901, mean_eps: 0.000000
 3320/5000: episode: 125, duration: 0.510s, episode steps:  37, steps per second:  73, episode reward: 40.865, mean reward:  1.104 [-3.000, 32.333], mean action: 4.351 [0.000, 14.000],  loss: 0.018317, mae: 0.343942, mean_q: 0.576301, mean_eps: 0.000000
 3341/5000: episode: 126, duration: 0.312s, episode steps:  21, steps per second:  67, episode reward: 41.071, mean reward:  1.956 [-2.041, 32.389], mean action: 3.143 [0.000, 19.000],  loss: 0.017892, mae: 0.331840, mean_q: 0.532179, mean_eps: 0.000000
 3362/5000: episode: 127, duration: 0.308s, episode steps:  21, steps per second:  68, episode reward: 38.128, mean reward:  1.816 [-2.530, 32.041], mean action: 2.714 [0.000, 16.000],  loss: 0.017166, mae: 0.331386, mean_q: 0.501351, mean_eps: 0.000000
 3385/5000: episode: 128, duration: 0.337s, episode steps:  23, steps per second:  68, episode reward: 43.737, mean reward:  1.902 [-2.209, 32.160], mean action: 2.826 [0.000, 12.000],  loss: 0.018861, mae: 0.327197, mean_q: 0.471518, mean_eps: 0.000000
 3417/5000: episode: 129, duration: 0.451s, episode steps:  32, steps per second:  71, episode reward: 44.152, mean reward:  1.380 [-2.275, 32.320], mean action: 1.062 [0.000, 3.000],  loss: 0.019049, mae: 0.326640, mean_q: 0.511572, mean_eps: 0.000000
 3437/5000: episode: 130, duration: 0.301s, episode steps:  20, steps per second:  66, episode reward: 44.401, mean reward:  2.220 [-2.359, 32.280], mean action: 0.900 [0.000, 2.000],  loss: 0.018432, mae: 0.334064, mean_q: 0.499032, mean_eps: 0.000000
 3452/5000: episode: 131, duration: 0.223s, episode steps:  15, steps per second:  67, episode reward: 43.477, mean reward:  2.898 [-2.235, 32.233], mean action: 2.467 [0.000, 8.000],  loss: 0.016436, mae: 0.318773, mean_q: 0.470967, mean_eps: 0.000000
 3481/5000: episode: 132, duration: 0.400s, episode steps:  29, steps per second:  73, episode reward: 37.755, mean reward:  1.302 [-3.000, 32.280], mean action: 4.724 [1.000, 12.000],  loss: 0.022464, mae: 0.351931, mean_q: 0.453075, mean_eps: 0.000000
 3504/5000: episode: 133, duration: 0.328s, episode steps:  23, steps per second:  70, episode reward: 36.000, mean reward:  1.565 [-3.000, 32.090], mean action: 3.435 [0.000, 12.000],  loss: 0.025420, mae: 0.362257, mean_q: 0.545251, mean_eps: 0.000000
 3555/5000: episode: 134, duration: 0.698s, episode steps:  51, steps per second:  73, episode reward: 39.831, mean reward:  0.781 [-2.914, 32.820], mean action: 3.490 [0.000, 14.000],  loss: 0.019426, mae: 0.335634, mean_q: 0.549661, mean_eps: 0.000000
 3571/5000: episode: 135, duration: 0.239s, episode steps:  16, steps per second:  67, episode reward: 41.657, mean reward:  2.604 [-2.385, 31.991], mean action: 4.750 [1.000, 14.000],  loss: 0.019827, mae: 0.355566, mean_q: 0.456727, mean_eps: 0.000000
 3605/5000: episode: 136, duration: 0.475s, episode steps:  34, steps per second:  72, episode reward: 41.942, mean reward:  1.234 [-2.515, 32.045], mean action: 2.882 [0.000, 12.000],  loss: 0.023538, mae: 0.359151, mean_q: 0.554400, mean_eps: 0.000000
 3640/5000: episode: 137, duration: 0.482s, episode steps:  35, steps per second:  73, episode reward: 35.181, mean reward:  1.005 [-2.522, 31.993], mean action: 2.229 [0.000, 12.000],  loss: 0.020004, mae: 0.339987, mean_q: 0.583466, mean_eps: 0.000000
 3667/5000: episode: 138, duration: 0.381s, episode steps:  27, steps per second:  71, episode reward: 41.239, mean reward:  1.527 [-3.000, 32.130], mean action: 3.741 [0.000, 12.000],  loss: 0.019047, mae: 0.342830, mean_q: 0.577160, mean_eps: 0.000000
 3691/5000: episode: 139, duration: 0.341s, episode steps:  24, steps per second:  70, episode reward: 38.711, mean reward:  1.613 [-2.387, 32.504], mean action: 1.917 [0.000, 9.000],  loss: 0.020573, mae: 0.344166, mean_q: 0.579292, mean_eps: 0.000000
 3715/5000: episode: 140, duration: 0.347s, episode steps:  24, steps per second:  69, episode reward: 44.721, mean reward:  1.863 [-2.067, 32.390], mean action: 0.583 [0.000, 3.000],  loss: 0.020185, mae: 0.343655, mean_q: 0.610844, mean_eps: 0.000000
 3739/5000: episode: 141, duration: 0.347s, episode steps:  24, steps per second:  69, episode reward: 39.000, mean reward:  1.625 [-2.591, 32.250], mean action: 1.542 [0.000, 12.000],  loss: 0.022511, mae: 0.356064, mean_q: 0.545325, mean_eps: 0.000000
 3767/5000: episode: 142, duration: 0.394s, episode steps:  28, steps per second:  71, episode reward: 42.000, mean reward:  1.500 [-2.263, 32.370], mean action: 1.286 [0.000, 3.000],  loss: 0.020824, mae: 0.348077, mean_q: 0.530658, mean_eps: 0.000000
 3810/5000: episode: 143, duration: 0.608s, episode steps:  43, steps per second:  71, episode reward: 36.000, mean reward:  0.837 [-2.139, 32.200], mean action: 3.605 [0.000, 19.000],  loss: 0.021090, mae: 0.344334, mean_q: 0.528514, mean_eps: 0.000000
 3848/5000: episode: 144, duration: 0.526s, episode steps:  38, steps per second:  72, episode reward: 38.434, mean reward:  1.011 [-2.713, 31.741], mean action: 5.553 [0.000, 16.000],  loss: 0.020552, mae: 0.350162, mean_q: 0.546965, mean_eps: 0.000000
 3875/5000: episode: 145, duration: 0.378s, episode steps:  27, steps per second:  71, episode reward: 35.241, mean reward:  1.305 [-2.575, 32.130], mean action: 5.370 [1.000, 16.000],  loss: 0.019615, mae: 0.344789, mean_q: 0.559305, mean_eps: 0.000000
 3922/5000: episode: 146, duration: 0.645s, episode steps:  47, steps per second:  73, episode reward: 41.767, mean reward:  0.889 [-3.000, 32.360], mean action: 3.894 [0.000, 16.000],  loss: 0.023324, mae: 0.366081, mean_q: 0.554298, mean_eps: 0.000000
 3989/5000: episode: 147, duration: 0.894s, episode steps:  67, steps per second:  75, episode reward: 32.902, mean reward:  0.491 [-3.000, 32.512], mean action: 1.731 [0.000, 13.000],  loss: 0.022106, mae: 0.369562, mean_q: 0.555811, mean_eps: 0.000000
 4019/5000: episode: 148, duration: 0.470s, episode steps:  30, steps per second:  64, episode reward: 38.948, mean reward:  1.298 [-2.342, 32.380], mean action: 2.600 [0.000, 16.000],  loss: 0.022434, mae: 0.373698, mean_q: 0.543370, mean_eps: 0.000000
 4045/5000: episode: 149, duration: 0.369s, episode steps:  26, steps per second:  70, episode reward: 44.132, mean reward:  1.697 [-2.285, 32.330], mean action: 2.423 [1.000, 3.000],  loss: 0.021211, mae: 0.359113, mean_q: 0.571250, mean_eps: 0.000000
 4092/5000: episode: 150, duration: 0.640s, episode steps:  47, steps per second:  73, episode reward: 36.000, mean reward:  0.766 [-3.000, 32.470], mean action: 2.745 [0.000, 16.000],  loss: 0.016456, mae: 0.337044, mean_q: 0.519689, mean_eps: 0.000000
 4116/5000: episode: 151, duration: 0.339s, episode steps:  24, steps per second:  71, episode reward: 44.146, mean reward:  1.839 [-2.699, 31.953], mean action: 3.000 [0.000, 13.000],  loss: 0.019436, mae: 0.345478, mean_q: 0.498199, mean_eps: 0.000000
 4139/5000: episode: 152, duration: 0.331s, episode steps:  23, steps per second:  70, episode reward: 44.584, mean reward:  1.938 [-2.017, 32.290], mean action: 2.783 [0.000, 19.000],  loss: 0.020952, mae: 0.356144, mean_q: 0.485322, mean_eps: 0.000000
 4162/5000: episode: 153, duration: 0.327s, episode steps:  23, steps per second:  70, episode reward: 40.561, mean reward:  1.764 [-2.441, 32.053], mean action: 4.043 [0.000, 20.000],  loss: 0.018344, mae: 0.341836, mean_q: 0.522755, mean_eps: 0.000000
 4192/5000: episode: 154, duration: 0.424s, episode steps:  30, steps per second:  71, episode reward: 43.800, mean reward:  1.460 [-2.111, 32.210], mean action: 3.167 [0.000, 19.000],  loss: 0.019629, mae: 0.348431, mean_q: 0.462932, mean_eps: 0.000000
 4213/5000: episode: 155, duration: 0.308s, episode steps:  21, steps per second:  68, episode reward: 44.337, mean reward:  2.111 [-2.667, 32.150], mean action: 2.762 [1.000, 19.000],  loss: 0.020935, mae: 0.355266, mean_q: 0.466639, mean_eps: 0.000000
 4247/5000: episode: 156, duration: 0.484s, episode steps:  34, steps per second:  70, episode reward: 41.522, mean reward:  1.221 [-2.163, 32.210], mean action: 1.824 [0.000, 20.000],  loss: 0.018707, mae: 0.340441, mean_q: 0.547181, mean_eps: 0.000000
 4268/5000: episode: 157, duration: 0.299s, episode steps:  21, steps per second:  70, episode reward: 38.258, mean reward:  1.822 [-3.000, 32.514], mean action: 2.000 [0.000, 12.000],  loss: 0.017399, mae: 0.341710, mean_q: 0.528609, mean_eps: 0.000000
 4296/5000: episode: 158, duration: 0.594s, episode steps:  28, steps per second:  47, episode reward: 44.220, mean reward:  1.579 [-2.375, 31.941], mean action: 2.571 [1.000, 3.000],  loss: 0.019948, mae: 0.353090, mean_q: 0.543566, mean_eps: 0.000000
 4330/5000: episode: 159, duration: 0.472s, episode steps:  34, steps per second:  72, episode reward: 41.231, mean reward:  1.213 [-2.447, 33.000], mean action: 2.235 [0.000, 16.000],  loss: 0.021028, mae: 0.357358, mean_q: 0.488230, mean_eps: 0.000000
 4354/5000: episode: 160, duration: 0.352s, episode steps:  24, steps per second:  68, episode reward: 39.000, mean reward:  1.625 [-2.671, 32.080], mean action: 3.417 [0.000, 16.000],  loss: 0.021628, mae: 0.358041, mean_q: 0.528318, mean_eps: 0.000000
 4372/5000: episode: 161, duration: 0.260s, episode steps:  18, steps per second:  69, episode reward: 44.077, mean reward:  2.449 [-2.022, 31.127], mean action: 2.556 [0.000, 9.000],  loss: 0.019306, mae: 0.350643, mean_q: 0.497906, mean_eps: 0.000000
 4404/5000: episode: 162, duration: 0.450s, episode steps:  32, steps per second:  71, episode reward: 35.872, mean reward:  1.121 [-2.385, 32.152], mean action: 3.094 [0.000, 12.000],  loss: 0.019742, mae: 0.356721, mean_q: 0.506116, mean_eps: 0.000000
 4428/5000: episode: 163, duration: 0.352s, episode steps:  24, steps per second:  68, episode reward: 41.608, mean reward:  1.734 [-2.583, 33.000], mean action: 3.625 [0.000, 16.000],  loss: 0.020303, mae: 0.362413, mean_q: 0.546076, mean_eps: 0.000000
 4450/5000: episode: 164, duration: 0.311s, episode steps:  22, steps per second:  71, episode reward: 35.667, mean reward:  1.621 [-3.000, 32.417], mean action: 4.455 [0.000, 16.000],  loss: 0.024723, mae: 0.379293, mean_q: 0.566149, mean_eps: 0.000000
 4479/5000: episode: 165, duration: 0.467s, episode steps:  29, steps per second:  62, episode reward: 42.000, mean reward:  1.448 [-2.693, 32.160], mean action: 5.345 [0.000, 16.000],  loss: 0.018932, mae: 0.343266, mean_q: 0.549183, mean_eps: 0.000000
 4507/5000: episode: 166, duration: 0.407s, episode steps:  28, steps per second:  69, episode reward: 38.220, mean reward:  1.365 [-3.000, 31.956], mean action: 4.571 [0.000, 20.000],  loss: 0.020214, mae: 0.364984, mean_q: 0.517815, mean_eps: 0.000000
 4538/5000: episode: 167, duration: 0.441s, episode steps:  31, steps per second:  70, episode reward: 38.158, mean reward:  1.231 [-3.000, 32.203], mean action: 4.903 [1.000, 16.000],  loss: 0.021128, mae: 0.366192, mean_q: 0.507715, mean_eps: 0.000000
 4573/5000: episode: 168, duration: 0.495s, episode steps:  35, steps per second:  71, episode reward: 40.566, mean reward:  1.159 [-2.582, 32.120], mean action: 4.800 [0.000, 20.000],  loss: 0.019870, mae: 0.358639, mean_q: 0.482872, mean_eps: 0.000000
 4599/5000: episode: 169, duration: 0.379s, episode steps:  26, steps per second:  69, episode reward: 41.410, mean reward:  1.593 [-2.396, 32.298], mean action: 3.692 [0.000, 15.000],  loss: 0.018840, mae: 0.350975, mean_q: 0.490426, mean_eps: 0.000000
 4650/5000: episode: 170, duration: 0.744s, episode steps:  51, steps per second:  69, episode reward: 46.701, mean reward:  0.916 [-0.970, 32.190], mean action: 1.882 [1.000, 4.000],  loss: 0.020545, mae: 0.351024, mean_q: 0.502606, mean_eps: 0.000000
 4679/5000: episode: 171, duration: 0.408s, episode steps:  29, steps per second:  71, episode reward: 35.124, mean reward:  1.211 [-2.787, 31.992], mean action: 4.621 [0.000, 20.000],  loss: 0.022839, mae: 0.356357, mean_q: 0.531900, mean_eps: 0.000000
 4710/5000: episode: 172, duration: 0.471s, episode steps:  31, steps per second:  66, episode reward: 38.007, mean reward:  1.226 [-2.534, 32.030], mean action: 2.968 [0.000, 16.000],  loss: 0.020424, mae: 0.350163, mean_q: 0.634851, mean_eps: 0.000000
 4744/5000: episode: 173, duration: 0.467s, episode steps:  34, steps per second:  73, episode reward: 41.655, mean reward:  1.225 [-2.543, 32.041], mean action: 4.471 [0.000, 16.000],  loss: 0.021362, mae: 0.350547, mean_q: 0.606701, mean_eps: 0.000000
 4772/5000: episode: 174, duration: 0.403s, episode steps:  28, steps per second:  69, episode reward: 38.007, mean reward:  1.357 [-2.903, 31.147], mean action: 3.143 [0.000, 20.000],  loss: 0.021670, mae: 0.356043, mean_q: 0.579268, mean_eps: 0.000000
 4785/5000: episode: 175, duration: 0.197s, episode steps:  13, steps per second:  66, episode reward: 44.320, mean reward:  3.409 [-2.058, 31.892], mean action: 2.000 [1.000, 9.000],  loss: 0.016545, mae: 0.335714, mean_q: 0.535994, mean_eps: 0.000000
 4803/5000: episode: 176, duration: 0.259s, episode steps:  18, steps per second:  69, episode reward: 45.000, mean reward:  2.500 [-2.037, 32.130], mean action: 1.000 [0.000, 9.000],  loss: 0.020774, mae: 0.361857, mean_q: 0.512229, mean_eps: 0.000000
 4824/5000: episode: 177, duration: 0.302s, episode steps:  21, steps per second:  70, episode reward: 42.000, mean reward:  2.000 [-2.251, 32.130], mean action: 4.048 [0.000, 12.000],  loss: 0.022495, mae: 0.360342, mean_q: 0.494345, mean_eps: 0.000000
 4850/5000: episode: 178, duration: 0.369s, episode steps:  26, steps per second:  71, episode reward: 41.531, mean reward:  1.597 [-2.203, 32.170], mean action: 2.308 [0.000, 9.000],  loss: 0.019878, mae: 0.348387, mean_q: 0.516319, mean_eps: 0.000000
 4880/5000: episode: 179, duration: 0.427s, episode steps:  30, steps per second:  70, episode reward: 43.634, mean reward:  1.454 [-2.626, 32.280], mean action: 3.767 [0.000, 16.000],  loss: 0.021162, mae: 0.355226, mean_q: 0.575907, mean_eps: 0.000000
 4901/5000: episode: 180, duration: 0.308s, episode steps:  21, steps per second:  68, episode reward: 39.000, mean reward:  1.857 [-3.000, 32.050], mean action: 4.810 [0.000, 13.000],  loss: 0.021308, mae: 0.353231, mean_q: 0.556423, mean_eps: 0.000000
 4935/5000: episode: 181, duration: 0.469s, episode steps:  34, steps per second:  72, episode reward: 43.844, mean reward:  1.290 [-2.902, 32.270], mean action: 4.206 [0.000, 19.000],  loss: 0.022454, mae: 0.363698, mean_q: 0.517887, mean_eps: 0.000000
 4956/5000: episode: 182, duration: 0.298s, episode steps:  21, steps per second:  71, episode reward: 38.596, mean reward:  1.838 [-3.000, 32.430], mean action: 4.333 [0.000, 19.000],  loss: 0.024251, mae: 0.374694, mean_q: 0.525333, mean_eps: 0.000000
 4985/5000: episode: 183, duration: 0.405s, episode steps:  29, steps per second:  72, episode reward: 35.645, mean reward:  1.229 [-2.465, 32.390], mean action: 4.517 [0.000, 19.000],  loss: 0.023158, mae: 0.367357, mean_q: 0.593925, mean_eps: 0.000000
done, took 65.235 seconds
DQN Evaluation: 8341 victories out of 9762 episodes
Training for 5000 steps ...
   36/5000: episode: 1, duration: 0.277s, episode steps:  36, steps per second: 130, episode reward: -32.680, mean reward: -0.908 [-31.995,  2.238], mean action: 5.889 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   62/5000: episode: 2, duration: 0.185s, episode steps:  26, steps per second: 141, episode reward: -32.470, mean reward: -1.249 [-32.404,  2.380], mean action: 3.269 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   88/5000: episode: 3, duration: 0.177s, episode steps:  26, steps per second: 147, episode reward: -32.410, mean reward: -1.247 [-32.207,  2.910], mean action: 4.423 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  107/5000: episode: 4, duration: 0.143s, episode steps:  19, steps per second: 133, episode reward: 35.045, mean reward:  1.844 [-2.900, 32.440], mean action: 4.842 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  131/5000: episode: 5, duration: 0.179s, episode steps:  24, steps per second: 134, episode reward: 37.963, mean reward:  1.582 [-2.267, 32.910], mean action: 5.667 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  158/5000: episode: 6, duration: 0.173s, episode steps:  27, steps per second: 156, episode reward: 32.662, mean reward:  1.210 [-2.806, 31.980], mean action: 3.741 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  184/5000: episode: 7, duration: 0.187s, episode steps:  26, steps per second: 139, episode reward: 38.229, mean reward:  1.470 [-2.838, 32.011], mean action: 3.038 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  210/5000: episode: 8, duration: 0.204s, episode steps:  26, steps per second: 127, episode reward: 32.139, mean reward:  1.236 [-3.000, 32.590], mean action: 4.077 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  232/5000: episode: 9, duration: 0.154s, episode steps:  22, steps per second: 143, episode reward: 35.853, mean reward:  1.630 [-2.405, 32.853], mean action: 3.136 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  267/5000: episode: 10, duration: 0.212s, episode steps:  35, steps per second: 165, episode reward: -35.340, mean reward: -1.010 [-32.071,  2.333], mean action: 3.629 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  294/5000: episode: 11, duration: 0.192s, episode steps:  27, steps per second: 141, episode reward: 35.028, mean reward:  1.297 [-2.215, 31.920], mean action: 4.185 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  315/5000: episode: 12, duration: 0.142s, episode steps:  21, steps per second: 148, episode reward: 35.730, mean reward:  1.701 [-3.000, 32.730], mean action: 4.429 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  331/5000: episode: 13, duration: 0.111s, episode steps:  16, steps per second: 144, episode reward: 36.000, mean reward:  2.250 [-2.366, 32.510], mean action: 4.312 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  358/5000: episode: 14, duration: 0.171s, episode steps:  27, steps per second: 158, episode reward: 33.000, mean reward:  1.222 [-2.941, 32.260], mean action: 5.852 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  394/5000: episode: 15, duration: 0.232s, episode steps:  36, steps per second: 155, episode reward: 38.593, mean reward:  1.072 [-2.146, 31.922], mean action: 2.806 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  420/5000: episode: 16, duration: 0.168s, episode steps:  26, steps per second: 155, episode reward: 33.000, mean reward:  1.269 [-2.488, 32.650], mean action: 4.346 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  443/5000: episode: 17, duration: 0.156s, episode steps:  23, steps per second: 147, episode reward: 35.532, mean reward:  1.545 [-2.332, 31.582], mean action: 3.696 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  456/5000: episode: 18, duration: 0.099s, episode steps:  13, steps per second: 131, episode reward: 42.000, mean reward:  3.231 [-2.502, 32.310], mean action: 4.154 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  481/5000: episode: 19, duration: 0.166s, episode steps:  25, steps per second: 150, episode reward: -30.000, mean reward: -1.200 [-30.480,  3.000], mean action: 3.640 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  506/5000: episode: 20, duration: 0.159s, episode steps:  25, steps per second: 157, episode reward: 33.000, mean reward:  1.320 [-2.490, 32.040], mean action: 3.600 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  522/5000: episode: 21, duration: 0.113s, episode steps:  16, steps per second: 142, episode reward: 38.715, mean reward:  2.420 [-3.000, 32.610], mean action: 3.375 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  544/5000: episode: 22, duration: 0.176s, episode steps:  22, steps per second: 125, episode reward: 38.242, mean reward:  1.738 [-2.319, 32.099], mean action: 3.773 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  572/5000: episode: 23, duration: 0.181s, episode steps:  28, steps per second: 155, episode reward: 33.000, mean reward:  1.179 [-2.462, 32.280], mean action: 3.250 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  593/5000: episode: 24, duration: 0.139s, episode steps:  21, steps per second: 151, episode reward: -35.140, mean reward: -1.673 [-32.325,  2.442], mean action: 5.095 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  618/5000: episode: 25, duration: 0.161s, episode steps:  25, steps per second: 155, episode reward: 35.284, mean reward:  1.411 [-2.619, 32.260], mean action: 4.080 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  633/5000: episode: 26, duration: 0.113s, episode steps:  15, steps per second: 133, episode reward: 41.131, mean reward:  2.742 [-2.029, 31.886], mean action: 4.133 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  653/5000: episode: 27, duration: 0.135s, episode steps:  20, steps per second: 149, episode reward: 35.383, mean reward:  1.769 [-2.806, 32.140], mean action: 4.500 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  680/5000: episode: 28, duration: 0.173s, episode steps:  27, steps per second: 156, episode reward: -32.240, mean reward: -1.194 [-32.680,  2.396], mean action: 4.444 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  702/5000: episode: 29, duration: 0.157s, episode steps:  22, steps per second: 140, episode reward: 38.057, mean reward:  1.730 [-2.386, 32.199], mean action: 5.773 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  724/5000: episode: 30, duration: 0.147s, episode steps:  22, steps per second: 150, episode reward: 32.604, mean reward:  1.482 [-2.546, 32.080], mean action: 4.318 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  748/5000: episode: 31, duration: 0.162s, episode steps:  24, steps per second: 148, episode reward: 32.413, mean reward:  1.351 [-2.389, 32.140], mean action: 4.083 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  771/5000: episode: 32, duration: 0.155s, episode steps:  23, steps per second: 148, episode reward: -32.360, mean reward: -1.407 [-32.307,  2.578], mean action: 5.783 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  789/5000: episode: 33, duration: 0.123s, episode steps:  18, steps per second: 147, episode reward: 39.000, mean reward:  2.167 [-3.000, 33.000], mean action: 4.778 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  812/5000: episode: 34, duration: 0.158s, episode steps:  23, steps per second: 146, episode reward: 41.313, mean reward:  1.796 [-2.224, 32.240], mean action: 3.652 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  834/5000: episode: 35, duration: 0.161s, episode steps:  22, steps per second: 137, episode reward: 41.188, mean reward:  1.872 [-2.534, 31.971], mean action: 2.682 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  850/5000: episode: 36, duration: 0.107s, episode steps:  16, steps per second: 150, episode reward: 39.000, mean reward:  2.438 [-3.000, 33.000], mean action: 4.250 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  874/5000: episode: 37, duration: 0.157s, episode steps:  24, steps per second: 153, episode reward: 38.903, mean reward:  1.621 [-3.000, 32.903], mean action: 3.375 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  895/5000: episode: 38, duration: 0.137s, episode steps:  21, steps per second: 153, episode reward: -32.450, mean reward: -1.545 [-31.610,  2.104], mean action: 5.286 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  920/5000: episode: 39, duration: 0.164s, episode steps:  25, steps per second: 152, episode reward: 38.885, mean reward:  1.555 [-2.458, 32.290], mean action: 3.360 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  946/5000: episode: 40, duration: 0.168s, episode steps:  26, steps per second: 154, episode reward: -32.480, mean reward: -1.249 [-31.927,  2.300], mean action: 3.308 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  965/5000: episode: 41, duration: 0.128s, episode steps:  19, steps per second: 149, episode reward: 33.000, mean reward:  1.737 [-2.900, 30.453], mean action: 4.684 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  984/5000: episode: 42, duration: 0.126s, episode steps:  19, steps per second: 151, episode reward: 38.770, mean reward:  2.041 [-2.174, 32.260], mean action: 3.000 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  997/5000: episode: 43, duration: 0.098s, episode steps:  13, steps per second: 133, episode reward: 38.888, mean reward:  2.991 [-3.000, 32.538], mean action: 4.077 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1019/5000: episode: 44, duration: 0.315s, episode steps:  22, steps per second:  70, episode reward: 39.000, mean reward:  1.773 [-2.459, 33.000], mean action: 3.909 [0.000, 19.000],  loss: 0.022825, mae: 0.365042, mean_q: 0.572837, mean_eps: 0.000000
 1050/5000: episode: 45, duration: 0.435s, episode steps:  31, steps per second:  71, episode reward: 32.630, mean reward:  1.053 [-3.000, 32.030], mean action: 6.742 [0.000, 20.000],  loss: 0.022840, mae: 0.369028, mean_q: 0.586929, mean_eps: 0.000000
 1085/5000: episode: 46, duration: 0.496s, episode steps:  35, steps per second:  71, episode reward: -32.910, mean reward: -0.940 [-32.062,  2.140], mean action: 4.743 [0.000, 19.000],  loss: 0.020609, mae: 0.354064, mean_q: 0.621567, mean_eps: 0.000000
 1099/5000: episode: 47, duration: 0.210s, episode steps:  14, steps per second:  67, episode reward: 41.323, mean reward:  2.952 [-2.597, 32.323], mean action: 3.000 [1.000, 9.000],  loss: 0.018629, mae: 0.345138, mean_q: 0.631790, mean_eps: 0.000000
 1116/5000: episode: 48, duration: 0.249s, episode steps:  17, steps per second:  68, episode reward: 38.901, mean reward:  2.288 [-3.000, 32.321], mean action: 3.941 [0.000, 19.000],  loss: 0.019141, mae: 0.358205, mean_q: 0.614528, mean_eps: 0.000000
 1140/5000: episode: 49, duration: 0.343s, episode steps:  24, steps per second:  70, episode reward: 35.124, mean reward:  1.463 [-3.000, 32.032], mean action: 1.875 [0.000, 16.000],  loss: 0.021325, mae: 0.372455, mean_q: 0.603250, mean_eps: 0.000000
 1162/5000: episode: 50, duration: 0.312s, episode steps:  22, steps per second:  71, episode reward: 33.000, mean reward:  1.500 [-3.000, 32.070], mean action: 2.909 [0.000, 12.000],  loss: 0.018097, mae: 0.354553, mean_q: 0.579631, mean_eps: 0.000000
 1185/5000: episode: 51, duration: 0.327s, episode steps:  23, steps per second:  70, episode reward: 38.591, mean reward:  1.678 [-2.680, 33.000], mean action: 3.696 [0.000, 14.000],  loss: 0.020985, mae: 0.371491, mean_q: 0.559076, mean_eps: 0.000000
 1208/5000: episode: 52, duration: 0.332s, episode steps:  23, steps per second:  69, episode reward: 36.000, mean reward:  1.565 [-2.787, 33.000], mean action: 2.696 [0.000, 9.000],  loss: 0.018393, mae: 0.357054, mean_q: 0.559415, mean_eps: 0.000000
 1230/5000: episode: 53, duration: 0.309s, episode steps:  22, steps per second:  71, episode reward: -33.000, mean reward: -1.500 [-32.004,  2.540], mean action: 3.455 [0.000, 12.000],  loss: 0.020658, mae: 0.364399, mean_q: 0.577805, mean_eps: 0.000000
 1246/5000: episode: 54, duration: 0.240s, episode steps:  16, steps per second:  67, episode reward: 37.717, mean reward:  2.357 [-2.434, 32.480], mean action: 4.000 [0.000, 14.000],  loss: 0.018782, mae: 0.354336, mean_q: 0.581935, mean_eps: 0.000000
 1264/5000: episode: 55, duration: 0.265s, episode steps:  18, steps per second:  68, episode reward: 38.904, mean reward:  2.161 [-2.371, 33.715], mean action: 2.056 [0.000, 12.000],  loss: 0.018147, mae: 0.355038, mean_q: 0.608160, mean_eps: 0.000000
 1279/5000: episode: 56, duration: 0.227s, episode steps:  15, steps per second:  66, episode reward: 44.498, mean reward:  2.967 [-2.357, 32.170], mean action: 1.267 [0.000, 8.000],  loss: 0.016915, mae: 0.354845, mean_q: 0.537943, mean_eps: 0.000000
 1295/5000: episode: 57, duration: 0.275s, episode steps:  16, steps per second:  58, episode reward: 41.895, mean reward:  2.618 [-2.504, 32.345], mean action: 0.438 [0.000, 2.000],  loss: 0.019745, mae: 0.364452, mean_q: 0.531444, mean_eps: 0.000000
 1315/5000: episode: 58, duration: 0.509s, episode steps:  20, steps per second:  39, episode reward: 41.384, mean reward:  2.069 [-2.615, 32.100], mean action: 9.600 [0.000, 20.000],  loss: 0.016177, mae: 0.339362, mean_q: 0.543191, mean_eps: 0.000000
 1336/5000: episode: 59, duration: 0.306s, episode steps:  21, steps per second:  69, episode reward: 35.900, mean reward:  1.710 [-2.380, 32.150], mean action: 2.619 [0.000, 19.000],  loss: 0.019760, mae: 0.361273, mean_q: 0.521008, mean_eps: 0.000000
 1360/5000: episode: 60, duration: 0.344s, episode steps:  24, steps per second:  70, episode reward: 41.418, mean reward:  1.726 [-2.078, 32.270], mean action: 3.250 [0.000, 13.000],  loss: 0.017743, mae: 0.350529, mean_q: 0.493262, mean_eps: 0.000000
 1387/5000: episode: 61, duration: 0.381s, episode steps:  27, steps per second:  71, episode reward: 34.431, mean reward:  1.275 [-3.000, 32.671], mean action: 7.556 [0.000, 21.000],  loss: 0.015701, mae: 0.337052, mean_q: 0.523203, mean_eps: 0.000000
 1407/5000: episode: 62, duration: 0.285s, episode steps:  20, steps per second:  70, episode reward: -32.640, mean reward: -1.632 [-32.228,  2.816], mean action: 4.600 [0.000, 15.000],  loss: 0.020202, mae: 0.365109, mean_q: 0.514337, mean_eps: 0.000000
 1422/5000: episode: 63, duration: 0.229s, episode steps:  15, steps per second:  65, episode reward: 41.204, mean reward:  2.747 [-2.164, 32.313], mean action: 2.467 [0.000, 16.000],  loss: 0.019804, mae: 0.354946, mean_q: 0.554677, mean_eps: 0.000000
 1445/5000: episode: 64, duration: 0.325s, episode steps:  23, steps per second:  71, episode reward: 35.355, mean reward:  1.537 [-2.601, 32.044], mean action: 5.348 [0.000, 16.000],  loss: 0.021750, mae: 0.360151, mean_q: 0.512734, mean_eps: 0.000000
 1468/5000: episode: 65, duration: 0.318s, episode steps:  23, steps per second:  72, episode reward: 32.717, mean reward:  1.422 [-2.806, 32.330], mean action: 5.478 [0.000, 16.000],  loss: 0.022894, mae: 0.367402, mean_q: 0.509623, mean_eps: 0.000000
 1495/5000: episode: 66, duration: 0.378s, episode steps:  27, steps per second:  71, episode reward: 32.173, mean reward:  1.192 [-2.560, 32.240], mean action: 6.370 [0.000, 16.000],  loss: 0.019102, mae: 0.351955, mean_q: 0.554348, mean_eps: 0.000000
 1515/5000: episode: 67, duration: 0.291s, episode steps:  20, steps per second:  69, episode reward: 38.867, mean reward:  1.943 [-2.086, 32.867], mean action: 3.450 [0.000, 15.000],  loss: 0.020899, mae: 0.371674, mean_q: 0.485930, mean_eps: 0.000000
 1538/5000: episode: 68, duration: 0.324s, episode steps:  23, steps per second:  71, episode reward: 32.749, mean reward:  1.424 [-2.243, 32.659], mean action: 6.478 [0.000, 16.000],  loss: 0.021455, mae: 0.357910, mean_q: 0.539557, mean_eps: 0.000000
 1557/5000: episode: 69, duration: 0.272s, episode steps:  19, steps per second:  70, episode reward: 33.000, mean reward:  1.737 [-3.000, 33.000], mean action: 5.632 [0.000, 16.000],  loss: 0.020776, mae: 0.357573, mean_q: 0.555183, mean_eps: 0.000000
 1576/5000: episode: 70, duration: 0.271s, episode steps:  19, steps per second:  70, episode reward: -39.000, mean reward: -2.053 [-32.398,  2.737], mean action: 7.368 [0.000, 20.000],  loss: 0.024654, mae: 0.371727, mean_q: 0.497978, mean_eps: 0.000000
 1598/5000: episode: 71, duration: 0.312s, episode steps:  22, steps per second:  71, episode reward: 32.490, mean reward:  1.477 [-3.000, 32.289], mean action: 6.636 [0.000, 19.000],  loss: 0.022509, mae: 0.365400, mean_q: 0.530534, mean_eps: 0.000000
 1621/5000: episode: 72, duration: 0.370s, episode steps:  23, steps per second:  62, episode reward: 38.401, mean reward:  1.670 [-2.164, 32.903], mean action: 6.565 [0.000, 19.000],  loss: 0.020347, mae: 0.361102, mean_q: 0.558747, mean_eps: 0.000000
 1638/5000: episode: 73, duration: 0.258s, episode steps:  17, steps per second:  66, episode reward: 35.815, mean reward:  2.107 [-3.000, 32.595], mean action: 6.412 [0.000, 20.000],  loss: 0.025765, mae: 0.382590, mean_q: 0.566819, mean_eps: 0.000000
 1663/5000: episode: 74, duration: 0.353s, episode steps:  25, steps per second:  71, episode reward: 32.249, mean reward:  1.290 [-3.000, 32.320], mean action: 6.800 [0.000, 19.000],  loss: 0.019681, mae: 0.354213, mean_q: 0.566162, mean_eps: 0.000000
 1692/5000: episode: 75, duration: 0.408s, episode steps:  29, steps per second:  71, episode reward: 32.713, mean reward:  1.128 [-3.000, 33.000], mean action: 6.379 [0.000, 19.000],  loss: 0.020907, mae: 0.360315, mean_q: 0.529592, mean_eps: 0.000000
 1720/5000: episode: 76, duration: 0.397s, episode steps:  28, steps per second:  71, episode reward: 37.627, mean reward:  1.344 [-2.568, 32.270], mean action: 7.607 [0.000, 19.000],  loss: 0.024607, mae: 0.380509, mean_q: 0.525397, mean_eps: 0.000000
 1735/5000: episode: 77, duration: 0.228s, episode steps:  15, steps per second:  66, episode reward: 41.460, mean reward:  2.764 [-2.109, 32.940], mean action: 4.600 [0.000, 16.000],  loss: 0.023617, mae: 0.366633, mean_q: 0.533226, mean_eps: 0.000000
 1766/5000: episode: 78, duration: 0.435s, episode steps:  31, steps per second:  71, episode reward: 32.234, mean reward:  1.040 [-3.000, 32.487], mean action: 5.645 [0.000, 18.000],  loss: 0.021467, mae: 0.361117, mean_q: 0.534149, mean_eps: 0.000000
 1788/5000: episode: 79, duration: 0.312s, episode steps:  22, steps per second:  71, episode reward: 37.116, mean reward:  1.687 [-2.363, 32.025], mean action: 3.455 [0.000, 15.000],  loss: 0.020034, mae: 0.354493, mean_q: 0.559161, mean_eps: 0.000000
 1809/5000: episode: 80, duration: 0.308s, episode steps:  21, steps per second:  68, episode reward: 32.467, mean reward:  1.546 [-3.000, 31.727], mean action: 3.714 [0.000, 15.000],  loss: 0.020042, mae: 0.348558, mean_q: 0.525416, mean_eps: 0.000000
 1840/5000: episode: 81, duration: 0.439s, episode steps:  31, steps per second:  71, episode reward: 35.091, mean reward:  1.132 [-2.130, 33.000], mean action: 2.710 [0.000, 14.000],  loss: 0.021432, mae: 0.361860, mean_q: 0.585995, mean_eps: 0.000000
 1876/5000: episode: 82, duration: 0.496s, episode steps:  36, steps per second:  73, episode reward: 32.984, mean reward:  0.916 [-2.621, 32.343], mean action: 7.583 [0.000, 18.000],  loss: 0.020085, mae: 0.364457, mean_q: 0.523465, mean_eps: 0.000000
 1889/5000: episode: 83, duration: 0.198s, episode steps:  13, steps per second:  66, episode reward: 44.049, mean reward:  3.388 [-2.158, 32.431], mean action: 2.462 [0.000, 11.000],  loss: 0.027685, mae: 0.398065, mean_q: 0.552740, mean_eps: 0.000000
 1915/5000: episode: 84, duration: 0.375s, episode steps:  26, steps per second:  69, episode reward: 35.138, mean reward:  1.351 [-2.184, 32.468], mean action: 4.077 [1.000, 13.000],  loss: 0.019809, mae: 0.356940, mean_q: 0.540200, mean_eps: 0.000000
 1940/5000: episode: 85, duration: 0.353s, episode steps:  25, steps per second:  71, episode reward: -33.000, mean reward: -1.320 [-32.013,  2.388], mean action: 6.640 [0.000, 14.000],  loss: 0.022443, mae: 0.367720, mean_q: 0.576922, mean_eps: 0.000000
 1959/5000: episode: 86, duration: 0.278s, episode steps:  19, steps per second:  68, episode reward: 38.698, mean reward:  2.037 [-2.401, 32.067], mean action: 3.789 [0.000, 21.000],  loss: 0.020250, mae: 0.361959, mean_q: 0.549704, mean_eps: 0.000000
 1994/5000: episode: 87, duration: 0.489s, episode steps:  35, steps per second:  72, episode reward: 35.743, mean reward:  1.021 [-2.556, 32.550], mean action: 2.514 [0.000, 11.000],  loss: 0.020900, mae: 0.365018, mean_q: 0.546397, mean_eps: 0.000000
 2012/5000: episode: 88, duration: 0.264s, episode steps:  18, steps per second:  68, episode reward: 35.448, mean reward:  1.969 [-2.482, 31.989], mean action: 3.944 [0.000, 12.000],  loss: 0.017313, mae: 0.345094, mean_q: 0.598353, mean_eps: 0.000000
 2039/5000: episode: 89, duration: 0.379s, episode steps:  27, steps per second:  71, episode reward: 34.095, mean reward:  1.263 [-3.000, 31.836], mean action: 6.593 [0.000, 18.000],  loss: 0.021803, mae: 0.363060, mean_q: 0.570009, mean_eps: 0.000000
 2065/5000: episode: 90, duration: 0.364s, episode steps:  26, steps per second:  71, episode reward: -36.000, mean reward: -1.385 [-32.410,  2.324], mean action: 5.731 [0.000, 20.000],  loss: 0.019178, mae: 0.364566, mean_q: 0.501928, mean_eps: 0.000000
 2095/5000: episode: 91, duration: 0.479s, episode steps:  30, steps per second:  63, episode reward: 36.000, mean reward:  1.200 [-3.000, 32.360], mean action: 6.133 [0.000, 15.000],  loss: 0.020476, mae: 0.369461, mean_q: 0.508860, mean_eps: 0.000000
 2114/5000: episode: 92, duration: 0.274s, episode steps:  19, steps per second:  69, episode reward: 35.217, mean reward:  1.854 [-3.000, 32.910], mean action: 3.947 [0.000, 19.000],  loss: 0.021351, mae: 0.370362, mean_q: 0.481455, mean_eps: 0.000000
 2137/5000: episode: 93, duration: 0.321s, episode steps:  23, steps per second:  72, episode reward: -35.520, mean reward: -1.544 [-31.543,  3.000], mean action: 6.652 [0.000, 19.000],  loss: 0.021581, mae: 0.369358, mean_q: 0.465831, mean_eps: 0.000000
 2155/5000: episode: 94, duration: 0.261s, episode steps:  18, steps per second:  69, episode reward: 35.823, mean reward:  1.990 [-3.000, 32.270], mean action: 5.389 [1.000, 18.000],  loss: 0.015893, mae: 0.344384, mean_q: 0.479911, mean_eps: 0.000000
 2177/5000: episode: 95, duration: 0.315s, episode steps:  22, steps per second:  70, episode reward: 41.386, mean reward:  1.881 [-2.364, 32.350], mean action: 0.636 [0.000, 3.000],  loss: 0.021522, mae: 0.368233, mean_q: 0.483286, mean_eps: 0.000000
 2202/5000: episode: 96, duration: 0.359s, episode steps:  25, steps per second:  70, episode reward: 35.891, mean reward:  1.436 [-2.405, 30.464], mean action: 7.080 [0.000, 20.000],  loss: 0.018667, mae: 0.354267, mean_q: 0.559365, mean_eps: 0.000000
 2227/5000: episode: 97, duration: 0.354s, episode steps:  25, steps per second:  71, episode reward: 32.330, mean reward:  1.293 [-2.738, 32.100], mean action: 4.800 [0.000, 12.000],  loss: 0.018708, mae: 0.355265, mean_q: 0.533032, mean_eps: 0.000000
 2252/5000: episode: 98, duration: 0.353s, episode steps:  25, steps per second:  71, episode reward: 32.024, mean reward:  1.281 [-2.680, 32.142], mean action: 5.520 [0.000, 14.000],  loss: 0.019288, mae: 0.358407, mean_q: 0.524301, mean_eps: 0.000000
 2275/5000: episode: 99, duration: 0.326s, episode steps:  23, steps per second:  70, episode reward: 35.549, mean reward:  1.546 [-2.751, 32.903], mean action: 5.174 [0.000, 14.000],  loss: 0.021520, mae: 0.364410, mean_q: 0.532952, mean_eps: 0.000000
 2294/5000: episode: 100, duration: 0.287s, episode steps:  19, steps per second:  66, episode reward: 41.646, mean reward:  2.192 [-2.255, 32.500], mean action: 3.211 [0.000, 19.000],  loss: 0.019895, mae: 0.344046, mean_q: 0.550020, mean_eps: 0.000000
 2323/5000: episode: 101, duration: 0.405s, episode steps:  29, steps per second:  72, episode reward: 32.520, mean reward:  1.121 [-2.753, 32.480], mean action: 5.414 [0.000, 20.000],  loss: 0.022473, mae: 0.360859, mean_q: 0.538385, mean_eps: 0.000000
 2345/5000: episode: 102, duration: 0.309s, episode steps:  22, steps per second:  71, episode reward: -38.080, mean reward: -1.731 [-31.997,  1.929], mean action: 3.545 [0.000, 12.000],  loss: 0.017173, mae: 0.341963, mean_q: 0.518550, mean_eps: 0.000000
 2370/5000: episode: 103, duration: 0.349s, episode steps:  25, steps per second:  72, episode reward: 35.594, mean reward:  1.424 [-3.000, 32.320], mean action: 3.840 [0.000, 12.000],  loss: 0.020583, mae: 0.361042, mean_q: 0.511210, mean_eps: 0.000000
 2390/5000: episode: 104, duration: 0.298s, episode steps:  20, steps per second:  67, episode reward: 44.285, mean reward:  2.214 [-2.023, 33.000], mean action: 1.750 [0.000, 11.000],  loss: 0.017137, mae: 0.340435, mean_q: 0.543808, mean_eps: 0.000000
 2409/5000: episode: 105, duration: 0.278s, episode steps:  19, steps per second:  68, episode reward: 36.000, mean reward:  1.895 [-2.752, 29.925], mean action: 2.632 [0.000, 11.000],  loss: 0.020187, mae: 0.354123, mean_q: 0.475459, mean_eps: 0.000000
 2431/5000: episode: 106, duration: 0.316s, episode steps:  22, steps per second:  70, episode reward: 41.569, mean reward:  1.890 [-2.301, 32.082], mean action: 4.591 [0.000, 16.000],  loss: 0.019698, mae: 0.360261, mean_q: 0.475290, mean_eps: 0.000000
 2452/5000: episode: 107, duration: 0.297s, episode steps:  21, steps per second:  71, episode reward: 33.000, mean reward:  1.571 [-2.619, 30.574], mean action: 4.667 [0.000, 16.000],  loss: 0.023992, mae: 0.373439, mean_q: 0.547601, mean_eps: 0.000000
 2474/5000: episode: 108, duration: 0.315s, episode steps:  22, steps per second:  70, episode reward: 32.453, mean reward:  1.475 [-2.903, 32.453], mean action: 4.682 [0.000, 19.000],  loss: 0.018821, mae: 0.340376, mean_q: 0.563284, mean_eps: 0.000000
 2495/5000: episode: 109, duration: 0.302s, episode steps:  21, steps per second:  70, episode reward: -32.450, mean reward: -1.545 [-31.630,  3.000], mean action: 4.905 [0.000, 19.000],  loss: 0.022144, mae: 0.360096, mean_q: 0.500472, mean_eps: 0.000000
 2522/5000: episode: 110, duration: 0.375s, episode steps:  27, steps per second:  72, episode reward: 34.442, mean reward:  1.276 [-2.326, 32.651], mean action: 6.148 [0.000, 20.000],  loss: 0.021349, mae: 0.352788, mean_q: 0.493359, mean_eps: 0.000000
 2536/5000: episode: 111, duration: 0.218s, episode steps:  14, steps per second:  64, episode reward: 38.737, mean reward:  2.767 [-3.000, 32.796], mean action: 4.214 [0.000, 16.000],  loss: 0.023794, mae: 0.365738, mean_q: 0.497817, mean_eps: 0.000000
 2552/5000: episode: 112, duration: 0.237s, episode steps:  16, steps per second:  67, episode reward: 38.272, mean reward:  2.392 [-3.000, 32.052], mean action: 4.125 [0.000, 12.000],  loss: 0.021726, mae: 0.355642, mean_q: 0.514524, mean_eps: 0.000000
 2573/5000: episode: 113, duration: 0.302s, episode steps:  21, steps per second:  70, episode reward: 39.000, mean reward:  1.857 [-2.476, 32.339], mean action: 6.000 [0.000, 16.000],  loss: 0.022559, mae: 0.362801, mean_q: 0.547672, mean_eps: 0.000000
 2594/5000: episode: 114, duration: 0.302s, episode steps:  21, steps per second:  69, episode reward: 41.134, mean reward:  1.959 [-2.290, 32.280], mean action: 3.333 [0.000, 16.000],  loss: 0.021660, mae: 0.360803, mean_q: 0.480756, mean_eps: 0.000000
 2616/5000: episode: 115, duration: 0.314s, episode steps:  22, steps per second:  70, episode reward: 35.903, mean reward:  1.632 [-2.274, 32.673], mean action: 4.136 [0.000, 16.000],  loss: 0.020964, mae: 0.366967, mean_q: 0.486741, mean_eps: 0.000000
 2635/5000: episode: 116, duration: 0.271s, episode steps:  19, steps per second:  70, episode reward: -35.370, mean reward: -1.862 [-31.395,  2.903], mean action: 5.211 [0.000, 19.000],  loss: 0.018445, mae: 0.348611, mean_q: 0.467579, mean_eps: 0.000000
 2650/5000: episode: 117, duration: 0.219s, episode steps:  15, steps per second:  69, episode reward: 38.041, mean reward:  2.536 [-3.000, 32.469], mean action: 2.933 [0.000, 15.000],  loss: 0.020375, mae: 0.352290, mean_q: 0.486648, mean_eps: 0.000000
 2676/5000: episode: 118, duration: 0.368s, episode steps:  26, steps per second:  71, episode reward: -35.010, mean reward: -1.347 [-32.415,  2.170], mean action: 4.769 [0.000, 15.000],  loss: 0.018803, mae: 0.340834, mean_q: 0.525480, mean_eps: 0.000000
 2698/5000: episode: 119, duration: 0.311s, episode steps:  22, steps per second:  71, episode reward: 38.015, mean reward:  1.728 [-2.538, 29.990], mean action: 5.136 [0.000, 15.000],  loss: 0.017791, mae: 0.334154, mean_q: 0.530401, mean_eps: 0.000000
 2720/5000: episode: 120, duration: 0.311s, episode steps:  22, steps per second:  71, episode reward: 38.446, mean reward:  1.748 [-2.340, 32.080], mean action: 3.182 [0.000, 15.000],  loss: 0.024094, mae: 0.366609, mean_q: 0.496451, mean_eps: 0.000000
 2739/5000: episode: 121, duration: 0.272s, episode steps:  19, steps per second:  70, episode reward: 35.900, mean reward:  1.889 [-2.903, 32.160], mean action: 3.053 [0.000, 15.000],  loss: 0.024426, mae: 0.358083, mean_q: 0.527421, mean_eps: 0.000000
 2755/5000: episode: 122, duration: 0.238s, episode steps:  16, steps per second:  67, episode reward: 41.367, mean reward:  2.585 [-2.359, 32.008], mean action: 2.312 [0.000, 13.000],  loss: 0.017616, mae: 0.328307, mean_q: 0.582602, mean_eps: 0.000000
 2778/5000: episode: 123, duration: 0.324s, episode steps:  23, steps per second:  71, episode reward: 34.519, mean reward:  1.501 [-2.433, 32.672], mean action: 2.609 [0.000, 15.000],  loss: 0.018918, mae: 0.331987, mean_q: 0.512746, mean_eps: 0.000000
 2795/5000: episode: 124, duration: 0.248s, episode steps:  17, steps per second:  69, episode reward: 38.055, mean reward:  2.239 [-2.249, 31.778], mean action: 4.118 [0.000, 19.000],  loss: 0.018036, mae: 0.335050, mean_q: 0.475159, mean_eps: 0.000000
 2815/5000: episode: 125, duration: 0.295s, episode steps:  20, steps per second:  68, episode reward: 32.746, mean reward:  1.637 [-2.713, 32.470], mean action: 3.450 [0.000, 11.000],  loss: 0.025061, mae: 0.361442, mean_q: 0.481912, mean_eps: 0.000000
 2835/5000: episode: 126, duration: 0.292s, episode steps:  20, steps per second:  68, episode reward: 36.000, mean reward:  1.800 [-2.463, 32.410], mean action: 5.100 [0.000, 15.000],  loss: 0.015062, mae: 0.318200, mean_q: 0.535823, mean_eps: 0.000000
 2872/5000: episode: 127, duration: 0.543s, episode steps:  37, steps per second:  68, episode reward: -32.570, mean reward: -0.880 [-32.045,  2.220], mean action: 6.324 [0.000, 15.000],  loss: 0.021771, mae: 0.345194, mean_q: 0.528749, mean_eps: 0.000000
 2900/5000: episode: 128, duration: 0.392s, episode steps:  28, steps per second:  71, episode reward: -33.000, mean reward: -1.179 [-32.116,  2.370], mean action: 4.321 [0.000, 16.000],  loss: 0.019627, mae: 0.348872, mean_q: 0.552701, mean_eps: 0.000000
 2933/5000: episode: 129, duration: 0.463s, episode steps:  33, steps per second:  71, episode reward: 36.000, mean reward:  1.091 [-2.306, 32.570], mean action: 4.758 [0.000, 16.000],  loss: 0.015851, mae: 0.325156, mean_q: 0.577827, mean_eps: 0.000000
 2954/5000: episode: 130, duration: 0.302s, episode steps:  21, steps per second:  69, episode reward: 38.236, mean reward:  1.821 [-2.588, 32.271], mean action: 2.524 [0.000, 16.000],  loss: 0.021586, mae: 0.351205, mean_q: 0.542901, mean_eps: 0.000000
 2982/5000: episode: 131, duration: 0.398s, episode steps:  28, steps per second:  70, episode reward: -33.000, mean reward: -1.179 [-33.000,  3.000], mean action: 3.393 [0.000, 16.000],  loss: 0.020930, mae: 0.351527, mean_q: 0.524402, mean_eps: 0.000000
 3002/5000: episode: 132, duration: 0.290s, episode steps:  20, steps per second:  69, episode reward: 36.000, mean reward:  1.800 [-2.396, 32.580], mean action: 3.850 [0.000, 16.000],  loss: 0.026462, mae: 0.376782, mean_q: 0.531387, mean_eps: 0.000000
 3024/5000: episode: 133, duration: 0.313s, episode steps:  22, steps per second:  70, episode reward: -35.410, mean reward: -1.610 [-32.710,  2.532], mean action: 4.455 [0.000, 16.000],  loss: 0.020904, mae: 0.348531, mean_q: 0.571667, mean_eps: 0.000000
 3056/5000: episode: 134, duration: 0.441s, episode steps:  32, steps per second:  73, episode reward: -32.780, mean reward: -1.024 [-32.110,  2.160], mean action: 4.719 [0.000, 14.000],  loss: 0.021462, mae: 0.356509, mean_q: 0.491085, mean_eps: 0.000000
 3079/5000: episode: 135, duration: 0.515s, episode steps:  23, steps per second:  45, episode reward: -38.110, mean reward: -1.657 [-31.846,  2.350], mean action: 4.522 [0.000, 19.000],  loss: 0.020887, mae: 0.346713, mean_q: 0.511917, mean_eps: 0.000000
 3103/5000: episode: 136, duration: 0.355s, episode steps:  24, steps per second:  68, episode reward: 35.965, mean reward:  1.499 [-3.000, 32.950], mean action: 3.833 [0.000, 12.000],  loss: 0.019828, mae: 0.339705, mean_q: 0.536641, mean_eps: 0.000000
 3138/5000: episode: 137, duration: 0.486s, episode steps:  35, steps per second:  72, episode reward: 41.653, mean reward:  1.190 [-2.583, 33.000], mean action: 3.829 [0.000, 15.000],  loss: 0.019596, mae: 0.336912, mean_q: 0.549082, mean_eps: 0.000000
 3154/5000: episode: 138, duration: 0.237s, episode steps:  16, steps per second:  67, episode reward: 39.000, mean reward:  2.438 [-2.275, 33.000], mean action: 5.125 [0.000, 15.000],  loss: 0.025363, mae: 0.361723, mean_q: 0.501825, mean_eps: 0.000000
 3204/5000: episode: 139, duration: 0.756s, episode steps:  50, steps per second:  66, episode reward: 35.180, mean reward:  0.704 [-3.000, 31.957], mean action: 5.360 [0.000, 15.000],  loss: 0.022788, mae: 0.351176, mean_q: 0.540673, mean_eps: 0.000000
 3220/5000: episode: 140, duration: 0.244s, episode steps:  16, steps per second:  65, episode reward: 38.057, mean reward:  2.379 [-2.807, 32.057], mean action: 4.125 [0.000, 19.000],  loss: 0.020704, mae: 0.339720, mean_q: 0.529225, mean_eps: 0.000000
 3256/5000: episode: 141, duration: 0.499s, episode steps:  36, steps per second:  72, episode reward: -30.000, mean reward: -0.833 [-30.061,  2.750], mean action: 6.361 [0.000, 19.000],  loss: 0.021258, mae: 0.344652, mean_q: 0.544003, mean_eps: 0.000000
 3277/5000: episode: 142, duration: 0.315s, episode steps:  21, steps per second:  67, episode reward: 37.455, mean reward:  1.784 [-2.405, 32.385], mean action: 6.810 [0.000, 16.000],  loss: 0.020464, mae: 0.345004, mean_q: 0.503286, mean_eps: 0.000000
 3295/5000: episode: 143, duration: 0.262s, episode steps:  18, steps per second:  69, episode reward: 39.000, mean reward:  2.167 [-2.273, 32.130], mean action: 3.778 [0.000, 16.000],  loss: 0.019706, mae: 0.339369, mean_q: 0.526555, mean_eps: 0.000000
 3317/5000: episode: 144, duration: 0.311s, episode steps:  22, steps per second:  71, episode reward: 35.314, mean reward:  1.605 [-2.939, 32.210], mean action: 4.500 [0.000, 19.000],  loss: 0.018167, mae: 0.333030, mean_q: 0.522597, mean_eps: 0.000000
 3332/5000: episode: 145, duration: 0.223s, episode steps:  15, steps per second:  67, episode reward: 39.000, mean reward:  2.600 [-2.287, 32.450], mean action: 3.133 [0.000, 15.000],  loss: 0.022400, mae: 0.356094, mean_q: 0.480712, mean_eps: 0.000000
 3369/5000: episode: 146, duration: 0.514s, episode steps:  37, steps per second:  72, episode reward: 32.808, mean reward:  0.887 [-2.706, 32.164], mean action: 6.784 [0.000, 17.000],  loss: 0.021301, mae: 0.360829, mean_q: 0.484191, mean_eps: 0.000000
 3390/5000: episode: 147, duration: 0.302s, episode steps:  21, steps per second:  70, episode reward: 35.200, mean reward:  1.676 [-2.440, 32.340], mean action: 6.905 [0.000, 18.000],  loss: 0.023352, mae: 0.359999, mean_q: 0.524012, mean_eps: 0.000000
 3412/5000: episode: 148, duration: 0.311s, episode steps:  22, steps per second:  71, episode reward: 34.981, mean reward:  1.590 [-3.000, 32.813], mean action: 4.273 [0.000, 15.000],  loss: 0.021561, mae: 0.353888, mean_q: 0.573247, mean_eps: 0.000000
 3428/5000: episode: 149, duration: 0.237s, episode steps:  16, steps per second:  67, episode reward: 38.805, mean reward:  2.425 [-2.277, 32.902], mean action: 3.250 [0.000, 15.000],  loss: 0.022929, mae: 0.359329, mean_q: 0.531507, mean_eps: 0.000000
 3451/5000: episode: 150, duration: 0.321s, episode steps:  23, steps per second:  72, episode reward: -35.370, mean reward: -1.538 [-29.902,  2.490], mean action: 3.522 [0.000, 12.000],  loss: 0.022246, mae: 0.357876, mean_q: 0.494174, mean_eps: 0.000000
 3471/5000: episode: 151, duration: 0.296s, episode steps:  20, steps per second:  68, episode reward: 37.959, mean reward:  1.898 [-2.476, 32.480], mean action: 6.700 [0.000, 18.000],  loss: 0.018713, mae: 0.341384, mean_q: 0.538366, mean_eps: 0.000000
 3495/5000: episode: 152, duration: 0.339s, episode steps:  24, steps per second:  71, episode reward: -35.500, mean reward: -1.479 [-31.822,  2.362], mean action: 6.542 [0.000, 15.000],  loss: 0.019271, mae: 0.349710, mean_q: 0.575287, mean_eps: 0.000000
 3517/5000: episode: 153, duration: 0.310s, episode steps:  22, steps per second:  71, episode reward: 35.597, mean reward:  1.618 [-3.000, 32.380], mean action: 6.591 [0.000, 20.000],  loss: 0.019561, mae: 0.351490, mean_q: 0.553617, mean_eps: 0.000000
 3539/5000: episode: 154, duration: 0.309s, episode steps:  22, steps per second:  71, episode reward: -35.910, mean reward: -1.632 [-31.916,  2.322], mean action: 4.591 [0.000, 19.000],  loss: 0.025053, mae: 0.387298, mean_q: 0.475380, mean_eps: 0.000000
 3557/5000: episode: 155, duration: 0.261s, episode steps:  18, steps per second:  69, episode reward: 35.333, mean reward:  1.963 [-3.000, 32.490], mean action: 5.333 [0.000, 19.000],  loss: 0.020933, mae: 0.364925, mean_q: 0.481698, mean_eps: 0.000000
 3581/5000: episode: 156, duration: 0.340s, episode steps:  24, steps per second:  71, episode reward: 38.413, mean reward:  1.601 [-3.000, 32.270], mean action: 3.667 [0.000, 19.000],  loss: 0.022623, mae: 0.368211, mean_q: 0.523676, mean_eps: 0.000000
 3605/5000: episode: 157, duration: 0.342s, episode steps:  24, steps per second:  70, episode reward: 35.601, mean reward:  1.483 [-3.000, 32.128], mean action: 3.917 [0.000, 15.000],  loss: 0.019306, mae: 0.347485, mean_q: 0.527209, mean_eps: 0.000000
 3628/5000: episode: 158, duration: 0.332s, episode steps:  23, steps per second:  69, episode reward: 32.755, mean reward:  1.424 [-2.456, 32.061], mean action: 6.826 [0.000, 15.000],  loss: 0.019336, mae: 0.349689, mean_q: 0.521135, mean_eps: 0.000000
 3642/5000: episode: 159, duration: 0.208s, episode steps:  14, steps per second:  67, episode reward: 43.757, mean reward:  3.125 [-2.537, 32.120], mean action: 4.143 [0.000, 15.000],  loss: 0.021226, mae: 0.363876, mean_q: 0.462140, mean_eps: 0.000000
 3667/5000: episode: 160, duration: 0.355s, episode steps:  25, steps per second:  70, episode reward: -33.000, mean reward: -1.320 [-32.401,  2.447], mean action: 4.480 [0.000, 16.000],  loss: 0.019266, mae: 0.349239, mean_q: 0.544705, mean_eps: 0.000000
 3692/5000: episode: 161, duration: 0.363s, episode steps:  25, steps per second:  69, episode reward: 35.903, mean reward:  1.436 [-2.298, 31.963], mean action: 3.960 [0.000, 16.000],  loss: 0.019087, mae: 0.343893, mean_q: 0.532537, mean_eps: 0.000000
 3713/5000: episode: 162, duration: 0.310s, episode steps:  21, steps per second:  68, episode reward: 35.079, mean reward:  1.670 [-3.000, 33.000], mean action: 4.429 [0.000, 16.000],  loss: 0.020828, mae: 0.358316, mean_q: 0.462216, mean_eps: 0.000000
 3741/5000: episode: 163, duration: 0.384s, episode steps:  28, steps per second:  73, episode reward: -36.000, mean reward: -1.286 [-32.502,  2.290], mean action: 4.107 [0.000, 16.000],  loss: 0.019599, mae: 0.339685, mean_q: 0.512190, mean_eps: 0.000000
 3761/5000: episode: 164, duration: 0.291s, episode steps:  20, steps per second:  69, episode reward: 39.000, mean reward:  1.950 [-2.838, 32.370], mean action: 3.850 [0.000, 20.000],  loss: 0.019588, mae: 0.346146, mean_q: 0.471379, mean_eps: 0.000000
 3784/5000: episode: 165, duration: 0.335s, episode steps:  23, steps per second:  69, episode reward: 38.775, mean reward:  1.686 [-2.661, 31.965], mean action: 1.957 [0.000, 11.000],  loss: 0.021099, mae: 0.351758, mean_q: 0.456106, mean_eps: 0.000000
 3806/5000: episode: 166, duration: 0.320s, episode steps:  22, steps per second:  69, episode reward: 41.398, mean reward:  1.882 [-2.463, 31.929], mean action: 2.955 [0.000, 19.000],  loss: 0.020076, mae: 0.355109, mean_q: 0.487869, mean_eps: 0.000000
 3832/5000: episode: 167, duration: 0.367s, episode steps:  26, steps per second:  71, episode reward: 35.134, mean reward:  1.351 [-2.615, 32.540], mean action: 4.923 [0.000, 11.000],  loss: 0.022637, mae: 0.359959, mean_q: 0.516602, mean_eps: 0.000000
 3855/5000: episode: 168, duration: 0.343s, episode steps:  23, steps per second:  67, episode reward: 38.619, mean reward:  1.679 [-2.460, 32.806], mean action: 2.348 [0.000, 11.000],  loss: 0.015740, mae: 0.334152, mean_q: 0.472806, mean_eps: 0.000000
 3881/5000: episode: 169, duration: 0.368s, episode steps:  26, steps per second:  71, episode reward: 32.079, mean reward:  1.234 [-2.649, 31.219], mean action: 4.769 [0.000, 19.000],  loss: 0.019948, mae: 0.348490, mean_q: 0.503698, mean_eps: 0.000000
 3903/5000: episode: 170, duration: 0.323s, episode steps:  22, steps per second:  68, episode reward: 41.924, mean reward:  1.906 [-2.154, 32.050], mean action: 3.182 [0.000, 19.000],  loss: 0.020216, mae: 0.345057, mean_q: 0.498699, mean_eps: 0.000000
 3918/5000: episode: 171, duration: 0.225s, episode steps:  15, steps per second:  67, episode reward: 38.452, mean reward:  2.563 [-2.802, 33.000], mean action: 3.733 [0.000, 19.000],  loss: 0.017109, mae: 0.331602, mean_q: 0.480396, mean_eps: 0.000000
 3945/5000: episode: 172, duration: 0.379s, episode steps:  27, steps per second:  71, episode reward: 32.570, mean reward:  1.206 [-2.657, 31.680], mean action: 4.852 [0.000, 17.000],  loss: 0.019599, mae: 0.354980, mean_q: 0.489184, mean_eps: 0.000000
 3962/5000: episode: 173, duration: 0.241s, episode steps:  17, steps per second:  71, episode reward: -39.000, mean reward: -2.294 [-33.000,  2.580], mean action: 4.529 [0.000, 16.000],  loss: 0.023474, mae: 0.366894, mean_q: 0.470183, mean_eps: 0.000000
 4003/5000: episode: 174, duration: 0.558s, episode steps:  41, steps per second:  73, episode reward: -36.000, mean reward: -0.878 [-32.438,  2.901], mean action: 5.488 [0.000, 17.000],  loss: 0.018092, mae: 0.332132, mean_q: 0.497350, mean_eps: 0.000000
 4020/5000: episode: 175, duration: 0.257s, episode steps:  17, steps per second:  66, episode reward: 44.099, mean reward:  2.594 [-2.346, 32.373], mean action: 0.471 [0.000, 2.000],  loss: 0.017901, mae: 0.331034, mean_q: 0.526275, mean_eps: 0.000000
 4055/5000: episode: 176, duration: 0.493s, episode steps:  35, steps per second:  71, episode reward: 41.106, mean reward:  1.174 [-2.400, 32.410], mean action: 2.057 [0.000, 9.000],  loss: 0.020505, mae: 0.350010, mean_q: 0.527994, mean_eps: 0.000000
 4078/5000: episode: 177, duration: 0.334s, episode steps:  23, steps per second:  69, episode reward: 41.963, mean reward:  1.824 [-2.125, 32.013], mean action: 2.304 [0.000, 16.000],  loss: 0.020886, mae: 0.363052, mean_q: 0.501895, mean_eps: 0.000000
 4110/5000: episode: 178, duration: 0.445s, episode steps:  32, steps per second:  72, episode reward: -32.690, mean reward: -1.022 [-32.140,  2.131], mean action: 2.531 [0.000, 11.000],  loss: 0.019221, mae: 0.353616, mean_q: 0.484122, mean_eps: 0.000000
 4146/5000: episode: 179, duration: 0.507s, episode steps:  36, steps per second:  71, episode reward: 32.378, mean reward:  0.899 [-2.878, 32.470], mean action: 2.806 [0.000, 16.000],  loss: 0.019694, mae: 0.350887, mean_q: 0.531760, mean_eps: 0.000000
 4167/5000: episode: 180, duration: 0.295s, episode steps:  21, steps per second:  71, episode reward: 32.796, mean reward:  1.562 [-3.000, 32.278], mean action: 4.095 [0.000, 15.000],  loss: 0.016046, mae: 0.329002, mean_q: 0.516704, mean_eps: 0.000000
 4188/5000: episode: 181, duration: 0.304s, episode steps:  21, steps per second:  69, episode reward: -32.760, mean reward: -1.560 [-32.288,  2.232], mean action: 5.429 [0.000, 15.000],  loss: 0.019157, mae: 0.343745, mean_q: 0.496756, mean_eps: 0.000000
 4211/5000: episode: 182, duration: 0.328s, episode steps:  23, steps per second:  70, episode reward: 38.066, mean reward:  1.655 [-2.711, 31.980], mean action: 3.391 [0.000, 14.000],  loss: 0.021441, mae: 0.358631, mean_q: 0.509502, mean_eps: 0.000000
 4234/5000: episode: 183, duration: 0.320s, episode steps:  23, steps per second:  72, episode reward: -33.000, mean reward: -1.435 [-33.000,  2.828], mean action: 8.826 [0.000, 20.000],  loss: 0.019484, mae: 0.344221, mean_q: 0.529365, mean_eps: 0.000000
 4252/5000: episode: 184, duration: 0.260s, episode steps:  18, steps per second:  69, episode reward: 38.493, mean reward:  2.139 [-2.491, 33.000], mean action: 2.944 [0.000, 15.000],  loss: 0.017892, mae: 0.332912, mean_q: 0.557959, mean_eps: 0.000000
 4273/5000: episode: 185, duration: 0.300s, episode steps:  21, steps per second:  70, episode reward: 35.806, mean reward:  1.705 [-2.508, 32.090], mean action: 5.762 [1.000, 19.000],  loss: 0.021624, mae: 0.350587, mean_q: 0.589292, mean_eps: 0.000000
 4299/5000: episode: 186, duration: 0.361s, episode steps:  26, steps per second:  72, episode reward: -35.360, mean reward: -1.360 [-32.073,  2.878], mean action: 5.615 [1.000, 15.000],  loss: 0.021594, mae: 0.348372, mean_q: 0.575017, mean_eps: 0.000000
 4321/5000: episode: 187, duration: 0.309s, episode steps:  22, steps per second:  71, episode reward: 39.000, mean reward:  1.773 [-2.444, 32.270], mean action: 3.500 [0.000, 11.000],  loss: 0.019812, mae: 0.340577, mean_q: 0.518355, mean_eps: 0.000000
 4347/5000: episode: 188, duration: 0.367s, episode steps:  26, steps per second:  71, episode reward: 38.759, mean reward:  1.491 [-2.677, 32.120], mean action: 3.846 [0.000, 12.000],  loss: 0.019531, mae: 0.343413, mean_q: 0.488499, mean_eps: 0.000000
 4374/5000: episode: 189, duration: 0.375s, episode steps:  27, steps per second:  72, episode reward: -32.620, mean reward: -1.208 [-31.826,  2.430], mean action: 4.407 [1.000, 12.000],  loss: 0.019867, mae: 0.331880, mean_q: 0.576242, mean_eps: 0.000000
 4394/5000: episode: 190, duration: 0.281s, episode steps:  20, steps per second:  71, episode reward: 38.106, mean reward:  1.905 [-2.475, 32.106], mean action: 4.500 [0.000, 19.000],  loss: 0.016972, mae: 0.324806, mean_q: 0.574258, mean_eps: 0.000000
 4419/5000: episode: 191, duration: 0.366s, episode steps:  25, steps per second:  68, episode reward: 35.658, mean reward:  1.426 [-2.525, 33.000], mean action: 4.000 [0.000, 19.000],  loss: 0.019777, mae: 0.338264, mean_q: 0.575008, mean_eps: 0.000000
 4441/5000: episode: 192, duration: 0.310s, episode steps:  22, steps per second:  71, episode reward: -32.810, mean reward: -1.491 [-32.900,  2.349], mean action: 4.636 [0.000, 15.000],  loss: 0.022169, mae: 0.355465, mean_q: 0.561526, mean_eps: 0.000000
 4465/5000: episode: 193, duration: 0.332s, episode steps:  24, steps per second:  72, episode reward: 32.479, mean reward:  1.353 [-2.466, 32.170], mean action: 3.958 [0.000, 9.000],  loss: 0.018497, mae: 0.338867, mean_q: 0.513902, mean_eps: 0.000000
 4488/5000: episode: 194, duration: 0.331s, episode steps:  23, steps per second:  69, episode reward: 33.000, mean reward:  1.435 [-3.000, 32.410], mean action: 3.739 [0.000, 12.000],  loss: 0.020951, mae: 0.346355, mean_q: 0.572705, mean_eps: 0.000000
 4510/5000: episode: 195, duration: 0.314s, episode steps:  22, steps per second:  70, episode reward: 30.000, mean reward:  1.364 [-3.000, 30.807], mean action: 4.500 [0.000, 12.000],  loss: 0.021951, mae: 0.361050, mean_q: 0.545027, mean_eps: 0.000000
 4537/5000: episode: 196, duration: 0.379s, episode steps:  27, steps per second:  71, episode reward: 32.706, mean reward:  1.211 [-2.598, 32.210], mean action: 3.407 [0.000, 21.000],  loss: 0.021315, mae: 0.360142, mean_q: 0.562080, mean_eps: 0.000000
 4553/5000: episode: 197, duration: 0.238s, episode steps:  16, steps per second:  67, episode reward: 41.247, mean reward:  2.578 [-2.500, 31.870], mean action: 3.938 [0.000, 14.000],  loss: 0.021447, mae: 0.356599, mean_q: 0.520340, mean_eps: 0.000000
 4577/5000: episode: 198, duration: 0.349s, episode steps:  24, steps per second:  69, episode reward: -35.440, mean reward: -1.477 [-31.925,  2.653], mean action: 4.375 [0.000, 12.000],  loss: 0.024071, mae: 0.367151, mean_q: 0.545266, mean_eps: 0.000000
 4592/5000: episode: 199, duration: 0.221s, episode steps:  15, steps per second:  68, episode reward: 41.024, mean reward:  2.735 [-3.000, 31.836], mean action: 4.067 [0.000, 16.000],  loss: 0.018218, mae: 0.345722, mean_q: 0.584805, mean_eps: 0.000000
 4609/5000: episode: 200, duration: 0.246s, episode steps:  17, steps per second:  69, episode reward: 38.239, mean reward:  2.249 [-3.000, 32.494], mean action: 3.706 [0.000, 16.000],  loss: 0.027604, mae: 0.386522, mean_q: 0.564465, mean_eps: 0.000000
 4629/5000: episode: 201, duration: 0.287s, episode steps:  20, steps per second:  70, episode reward: 38.886, mean reward:  1.944 [-2.470, 32.220], mean action: 4.750 [0.000, 19.000],  loss: 0.022113, mae: 0.356588, mean_q: 0.536673, mean_eps: 0.000000
 4654/5000: episode: 202, duration: 0.351s, episode steps:  25, steps per second:  71, episode reward: -33.000, mean reward: -1.320 [-32.139,  2.795], mean action: 3.040 [0.000, 12.000],  loss: 0.023465, mae: 0.363014, mean_q: 0.604637, mean_eps: 0.000000
 4668/5000: episode: 203, duration: 0.217s, episode steps:  14, steps per second:  64, episode reward: 41.531, mean reward:  2.967 [-2.225, 31.871], mean action: 4.714 [0.000, 15.000],  loss: 0.018939, mae: 0.343329, mean_q: 0.533130, mean_eps: 0.000000
 4684/5000: episode: 204, duration: 0.243s, episode steps:  16, steps per second:  66, episode reward: 38.520, mean reward:  2.407 [-2.227, 32.160], mean action: 3.500 [0.000, 15.000],  loss: 0.020978, mae: 0.350557, mean_q: 0.535873, mean_eps: 0.000000
 4699/5000: episode: 205, duration: 0.219s, episode steps:  15, steps per second:  68, episode reward: 41.434, mean reward:  2.762 [-2.325, 32.264], mean action: 3.333 [0.000, 15.000],  loss: 0.021243, mae: 0.352567, mean_q: 0.552652, mean_eps: 0.000000
 4726/5000: episode: 206, duration: 0.378s, episode steps:  27, steps per second:  71, episode reward: 32.705, mean reward:  1.211 [-2.979, 32.352], mean action: 5.074 [0.000, 19.000],  loss: 0.016900, mae: 0.333285, mean_q: 0.504259, mean_eps: 0.000000
 4752/5000: episode: 207, duration: 0.364s, episode steps:  26, steps per second:  71, episode reward: 32.263, mean reward:  1.241 [-3.000, 31.736], mean action: 4.385 [0.000, 13.000],  loss: 0.021708, mae: 0.346489, mean_q: 0.567835, mean_eps: 0.000000
 4771/5000: episode: 208, duration: 0.275s, episode steps:  19, steps per second:  69, episode reward: 41.416, mean reward:  2.180 [-2.065, 31.994], mean action: 2.105 [0.000, 9.000],  loss: 0.021237, mae: 0.347194, mean_q: 0.549666, mean_eps: 0.000000
 4793/5000: episode: 209, duration: 0.314s, episode steps:  22, steps per second:  70, episode reward: 36.000, mean reward:  1.636 [-2.335, 30.246], mean action: 3.318 [1.000, 19.000],  loss: 0.018804, mae: 0.335555, mean_q: 0.552645, mean_eps: 0.000000
 4816/5000: episode: 210, duration: 0.331s, episode steps:  23, steps per second:  69, episode reward: 35.095, mean reward:  1.526 [-2.901, 31.865], mean action: 6.174 [0.000, 19.000],  loss: 0.017133, mae: 0.324302, mean_q: 0.527326, mean_eps: 0.000000
 4835/5000: episode: 211, duration: 0.279s, episode steps:  19, steps per second:  68, episode reward: 35.470, mean reward:  1.867 [-3.000, 31.500], mean action: 5.368 [0.000, 19.000],  loss: 0.027341, mae: 0.370720, mean_q: 0.519560, mean_eps: 0.000000
 4848/5000: episode: 212, duration: 0.208s, episode steps:  13, steps per second:  63, episode reward: 46.502, mean reward:  3.577 [-0.106, 32.041], mean action: 3.692 [0.000, 12.000],  loss: 0.023568, mae: 0.355872, mean_q: 0.592482, mean_eps: 0.000000
 4868/5000: episode: 213, duration: 0.280s, episode steps:  20, steps per second:  71, episode reward: -33.000, mean reward: -1.650 [-32.835,  2.559], mean action: 5.050 [0.000, 19.000],  loss: 0.022778, mae: 0.357429, mean_q: 0.535554, mean_eps: 0.000000
 4891/5000: episode: 214, duration: 0.326s, episode steps:  23, steps per second:  70, episode reward: 35.691, mean reward:  1.552 [-2.515, 32.278], mean action: 5.652 [0.000, 19.000],  loss: 0.018271, mae: 0.336398, mean_q: 0.528188, mean_eps: 0.000000
 4905/5000: episode: 215, duration: 0.212s, episode steps:  14, steps per second:  66, episode reward: 43.675, mean reward:  3.120 [-2.004, 31.911], mean action: 3.429 [0.000, 16.000],  loss: 0.019591, mae: 0.340716, mean_q: 0.530159, mean_eps: 0.000000
 4927/5000: episode: 216, duration: 0.308s, episode steps:  22, steps per second:  72, episode reward: -35.400, mean reward: -1.609 [-31.560,  2.810], mean action: 4.091 [0.000, 16.000],  loss: 0.021005, mae: 0.354342, mean_q: 0.546580, mean_eps: 0.000000
 4940/5000: episode: 217, duration: 0.197s, episode steps:  13, steps per second:  66, episode reward: 41.431, mean reward:  3.187 [-2.236, 32.098], mean action: 2.923 [0.000, 16.000],  loss: 0.023708, mae: 0.359598, mean_q: 0.488867, mean_eps: 0.000000
 4958/5000: episode: 218, duration: 0.263s, episode steps:  18, steps per second:  68, episode reward: 38.264, mean reward:  2.126 [-2.807, 32.120], mean action: 3.167 [0.000, 16.000],  loss: 0.019939, mae: 0.347063, mean_q: 0.502338, mean_eps: 0.000000
 4978/5000: episode: 219, duration: 0.286s, episode steps:  20, steps per second:  70, episode reward: -35.190, mean reward: -1.759 [-33.000,  3.000], mean action: 4.750 [0.000, 15.000],  loss: 0.025228, mae: 0.362799, mean_q: 0.546680, mean_eps: 0.000000
done, took 64.790 seconds
DQN Evaluation: 8514 victories out of 9982 episodes
Training for 5000 steps ...
   13/5000: episode: 1, duration: 0.118s, episode steps:  13, steps per second: 110, episode reward: 41.154, mean reward:  3.166 [-2.309, 32.054], mean action: 3.615 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   55/5000: episode: 2, duration: 0.298s, episode steps:  42, steps per second: 141, episode reward: -32.110, mean reward: -0.765 [-32.083,  2.389], mean action: 4.476 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   71/5000: episode: 3, duration: 0.126s, episode steps:  16, steps per second: 127, episode reward: 43.855, mean reward:  2.741 [-2.677, 32.160], mean action: 1.875 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   88/5000: episode: 4, duration: 0.130s, episode steps:  17, steps per second: 130, episode reward: 42.000, mean reward:  2.471 [-2.847, 32.140], mean action: 4.471 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  118/5000: episode: 5, duration: 0.218s, episode steps:  30, steps per second: 138, episode reward: 40.360, mean reward:  1.345 [-3.000, 32.040], mean action: 3.900 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  138/5000: episode: 6, duration: 0.140s, episode steps:  20, steps per second: 143, episode reward: 43.584, mean reward:  2.179 [-2.031, 32.350], mean action: 3.350 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  154/5000: episode: 7, duration: 0.121s, episode steps:  16, steps per second: 132, episode reward: 44.904, mean reward:  2.807 [-2.464, 32.030], mean action: 2.375 [1.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  175/5000: episode: 8, duration: 0.144s, episode steps:  21, steps per second: 146, episode reward: 47.019, mean reward:  2.239 [-0.129, 32.420], mean action: 3.381 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  221/5000: episode: 9, duration: 0.286s, episode steps:  46, steps per second: 161, episode reward: -38.350, mean reward: -0.834 [-32.358,  2.270], mean action: 6.804 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  248/5000: episode: 10, duration: 0.189s, episode steps:  27, steps per second: 143, episode reward: 37.871, mean reward:  1.403 [-2.297, 32.530], mean action: 4.593 [1.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  276/5000: episode: 11, duration: 0.190s, episode steps:  28, steps per second: 148, episode reward: 40.250, mean reward:  1.438 [-2.511, 32.038], mean action: 4.107 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  309/5000: episode: 12, duration: 0.210s, episode steps:  33, steps per second: 157, episode reward: 44.368, mean reward:  1.344 [-2.509, 32.190], mean action: 2.818 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  336/5000: episode: 13, duration: 0.177s, episode steps:  27, steps per second: 153, episode reward: 36.000, mean reward:  1.333 [-3.000, 32.160], mean action: 5.556 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  361/5000: episode: 14, duration: 0.166s, episode steps:  25, steps per second: 151, episode reward: 46.942, mean reward:  1.878 [-0.062, 32.023], mean action: 3.320 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  400/5000: episode: 15, duration: 0.260s, episode steps:  39, steps per second: 150, episode reward: 41.599, mean reward:  1.067 [-2.199, 32.510], mean action: 3.308 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  446/5000: episode: 16, duration: 0.294s, episode steps:  46, steps per second: 156, episode reward: 44.695, mean reward:  0.972 [-2.082, 32.100], mean action: 1.696 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  472/5000: episode: 17, duration: 0.181s, episode steps:  26, steps per second: 144, episode reward: 43.773, mean reward:  1.684 [-2.121, 32.250], mean action: 3.154 [2.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  534/5000: episode: 18, duration: 0.379s, episode steps:  62, steps per second: 164, episode reward: -32.950, mean reward: -0.531 [-32.115,  2.960], mean action: 8.468 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  557/5000: episode: 19, duration: 0.161s, episode steps:  23, steps per second: 143, episode reward: 38.556, mean reward:  1.676 [-2.712, 32.860], mean action: 4.739 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  586/5000: episode: 20, duration: 0.186s, episode steps:  29, steps per second: 156, episode reward: 44.718, mean reward:  1.542 [-2.345, 32.300], mean action: 2.552 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  620/5000: episode: 21, duration: 0.221s, episode steps:  34, steps per second: 154, episode reward: 38.195, mean reward:  1.123 [-3.000, 32.180], mean action: 3.971 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  656/5000: episode: 22, duration: 0.244s, episode steps:  36, steps per second: 147, episode reward: 36.000, mean reward:  1.000 [-2.757, 32.470], mean action: 6.583 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  695/5000: episode: 23, duration: 0.507s, episode steps:  39, steps per second:  77, episode reward: 41.275, mean reward:  1.058 [-2.335, 32.350], mean action: 2.846 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  717/5000: episode: 24, duration: 0.154s, episode steps:  22, steps per second: 143, episode reward: 41.448, mean reward:  1.884 [-2.902, 32.110], mean action: 3.682 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  743/5000: episode: 25, duration: 0.185s, episode steps:  26, steps per second: 141, episode reward: 39.000, mean reward:  1.500 [-2.801, 32.100], mean action: 4.808 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  759/5000: episode: 26, duration: 0.121s, episode steps:  16, steps per second: 133, episode reward: 44.301, mean reward:  2.769 [-3.000, 33.000], mean action: 2.875 [1.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  780/5000: episode: 27, duration: 0.149s, episode steps:  21, steps per second: 141, episode reward: 42.000, mean reward:  2.000 [-2.180, 32.620], mean action: 2.810 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  803/5000: episode: 28, duration: 0.161s, episode steps:  23, steps per second: 143, episode reward: 39.000, mean reward:  1.696 [-2.553, 32.070], mean action: 4.826 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  824/5000: episode: 29, duration: 0.158s, episode steps:  21, steps per second: 133, episode reward: 41.901, mean reward:  1.995 [-2.482, 32.441], mean action: 3.714 [1.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  865/5000: episode: 30, duration: 0.261s, episode steps:  41, steps per second: 157, episode reward: 40.651, mean reward:  0.991 [-2.167, 32.830], mean action: 6.195 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  903/5000: episode: 31, duration: 0.243s, episode steps:  38, steps per second: 156, episode reward: 44.114, mean reward:  1.161 [-3.000, 32.020], mean action: 5.395 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  925/5000: episode: 32, duration: 0.144s, episode steps:  22, steps per second: 153, episode reward: 44.481, mean reward:  2.022 [-2.588, 32.320], mean action: 2.273 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  952/5000: episode: 33, duration: 0.189s, episode steps:  27, steps per second: 143, episode reward: 41.602, mean reward:  1.541 [-2.855, 31.901], mean action: 3.370 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  973/5000: episode: 34, duration: 0.146s, episode steps:  21, steps per second: 144, episode reward: 40.636, mean reward:  1.935 [-2.878, 32.051], mean action: 3.476 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  991/5000: episode: 35, duration: 0.134s, episode steps:  18, steps per second: 134, episode reward: 45.000, mean reward:  2.500 [-2.223, 32.300], mean action: 3.000 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1032/5000: episode: 36, duration: 0.527s, episode steps:  41, steps per second:  78, episode reward: 35.781, mean reward:  0.873 [-2.383, 32.264], mean action: 4.829 [0.000, 16.000],  loss: 0.020672, mae: 0.346433, mean_q: 0.495456, mean_eps: 0.000000
 1067/5000: episode: 37, duration: 0.506s, episode steps:  35, steps per second:  69, episode reward: 43.748, mean reward:  1.250 [-2.053, 32.270], mean action: 2.457 [0.000, 16.000],  loss: 0.019144, mae: 0.340436, mean_q: 0.526300, mean_eps: 0.000000
 1090/5000: episode: 38, duration: 0.337s, episode steps:  23, steps per second:  68, episode reward: 40.919, mean reward:  1.779 [-2.588, 30.261], mean action: 4.478 [0.000, 16.000],  loss: 0.018576, mae: 0.343542, mean_q: 0.482987, mean_eps: 0.000000
 1123/5000: episode: 39, duration: 0.479s, episode steps:  33, steps per second:  69, episode reward: 35.624, mean reward:  1.080 [-3.000, 31.944], mean action: 5.758 [0.000, 19.000],  loss: 0.018720, mae: 0.340323, mean_q: 0.479697, mean_eps: 0.000000
 1140/5000: episode: 40, duration: 0.258s, episode steps:  17, steps per second:  66, episode reward: 47.486, mean reward:  2.793 [-0.095, 32.140], mean action: 1.235 [1.000, 3.000],  loss: 0.017900, mae: 0.338739, mean_q: 0.515305, mean_eps: 0.000000
 1170/5000: episode: 41, duration: 0.440s, episode steps:  30, steps per second:  68, episode reward: 41.851, mean reward:  1.395 [-2.102, 32.130], mean action: 2.200 [0.000, 14.000],  loss: 0.021164, mae: 0.348483, mean_q: 0.547961, mean_eps: 0.000000
 1201/5000: episode: 42, duration: 0.454s, episode steps:  31, steps per second:  68, episode reward: 38.690, mean reward:  1.248 [-3.000, 31.780], mean action: 1.548 [0.000, 9.000],  loss: 0.021204, mae: 0.353139, mean_q: 0.526348, mean_eps: 0.000000
 1227/5000: episode: 43, duration: 0.375s, episode steps:  26, steps per second:  69, episode reward: 38.620, mean reward:  1.485 [-2.539, 30.600], mean action: 4.000 [0.000, 14.000],  loss: 0.023732, mae: 0.361534, mean_q: 0.535132, mean_eps: 0.000000
 1240/5000: episode: 44, duration: 0.208s, episode steps:  13, steps per second:  62, episode reward: 47.148, mean reward:  3.627 [ 0.187, 31.539], mean action: 1.385 [1.000, 2.000],  loss: 0.018180, mae: 0.337991, mean_q: 0.572105, mean_eps: 0.000000
 1275/5000: episode: 45, duration: 0.730s, episode steps:  35, steps per second:  48, episode reward: 37.913, mean reward:  1.083 [-3.000, 32.100], mean action: 1.857 [0.000, 12.000],  loss: 0.020169, mae: 0.346363, mean_q: 0.620379, mean_eps: 0.000000
 1310/5000: episode: 46, duration: 0.601s, episode steps:  35, steps per second:  58, episode reward: 38.203, mean reward:  1.092 [-3.000, 32.050], mean action: 3.829 [0.000, 14.000],  loss: 0.019746, mae: 0.348111, mean_q: 0.566062, mean_eps: 0.000000
 1337/5000: episode: 47, duration: 0.390s, episode steps:  27, steps per second:  69, episode reward: 44.127, mean reward:  1.634 [-2.211, 32.200], mean action: 2.963 [0.000, 14.000],  loss: 0.020788, mae: 0.346318, mean_q: 0.557271, mean_eps: 0.000000
 1356/5000: episode: 48, duration: 0.283s, episode steps:  19, steps per second:  67, episode reward: 45.000, mean reward:  2.368 [-2.187, 33.000], mean action: 5.368 [1.000, 20.000],  loss: 0.019826, mae: 0.345469, mean_q: 0.614720, mean_eps: 0.000000
 1387/5000: episode: 49, duration: 0.451s, episode steps:  31, steps per second:  69, episode reward: 44.731, mean reward:  1.443 [-2.073, 32.260], mean action: 2.677 [0.000, 12.000],  loss: 0.021189, mae: 0.358159, mean_q: 0.522268, mean_eps: 0.000000
 1407/5000: episode: 50, duration: 0.296s, episode steps:  20, steps per second:  68, episode reward: 39.000, mean reward:  1.950 [-3.000, 32.640], mean action: 3.900 [0.000, 16.000],  loss: 0.020113, mae: 0.351375, mean_q: 0.568644, mean_eps: 0.000000
 1434/5000: episode: 51, duration: 0.400s, episode steps:  27, steps per second:  68, episode reward: 40.880, mean reward:  1.514 [-3.000, 32.260], mean action: 4.519 [0.000, 16.000],  loss: 0.019131, mae: 0.356401, mean_q: 0.572585, mean_eps: 0.000000
 1455/5000: episode: 52, duration: 0.310s, episode steps:  21, steps per second:  68, episode reward: 32.746, mean reward:  1.559 [-3.000, 29.850], mean action: 4.286 [0.000, 16.000],  loss: 0.022054, mae: 0.371118, mean_q: 0.550297, mean_eps: 0.000000
 1469/5000: episode: 53, duration: 0.219s, episode steps:  14, steps per second:  64, episode reward: 45.000, mean reward:  3.214 [ 0.000, 30.297], mean action: 1.786 [1.000, 3.000],  loss: 0.023419, mae: 0.367804, mean_q: 0.533497, mean_eps: 0.000000
 1509/5000: episode: 54, duration: 0.580s, episode steps:  40, steps per second:  69, episode reward: 38.813, mean reward:  0.970 [-2.381, 32.210], mean action: 3.850 [0.000, 16.000],  loss: 0.019650, mae: 0.359626, mean_q: 0.574245, mean_eps: 0.000000
 1546/5000: episode: 55, duration: 0.524s, episode steps:  37, steps per second:  71, episode reward: 40.852, mean reward:  1.104 [-2.402, 31.970], mean action: 2.000 [0.000, 11.000],  loss: 0.019355, mae: 0.346398, mean_q: 0.576375, mean_eps: 0.000000
 1598/5000: episode: 56, duration: 0.762s, episode steps:  52, steps per second:  68, episode reward: 35.730, mean reward:  0.687 [-2.316, 32.150], mean action: 6.500 [0.000, 21.000],  loss: 0.019929, mae: 0.353753, mean_q: 0.528584, mean_eps: 0.000000
 1641/5000: episode: 57, duration: 0.613s, episode steps:  43, steps per second:  70, episode reward: 41.579, mean reward:  0.967 [-2.112, 32.031], mean action: 2.349 [0.000, 11.000],  loss: 0.018384, mae: 0.353931, mean_q: 0.470994, mean_eps: 0.000000
 1686/5000: episode: 58, duration: 0.620s, episode steps:  45, steps per second:  73, episode reward: 38.032, mean reward:  0.845 [-2.317, 32.400], mean action: 4.267 [0.000, 19.000],  loss: 0.020021, mae: 0.356215, mean_q: 0.544698, mean_eps: 0.000000
 1712/5000: episode: 59, duration: 0.377s, episode steps:  26, steps per second:  69, episode reward: 41.881, mean reward:  1.611 [-2.744, 32.310], mean action: 3.538 [0.000, 16.000],  loss: 0.021664, mae: 0.362743, mean_q: 0.502761, mean_eps: 0.000000
 1744/5000: episode: 60, duration: 0.463s, episode steps:  32, steps per second:  69, episode reward: 41.755, mean reward:  1.305 [-2.125, 32.060], mean action: 2.531 [0.000, 15.000],  loss: 0.023183, mae: 0.370579, mean_q: 0.498962, mean_eps: 0.000000
 1765/5000: episode: 61, duration: 0.302s, episode steps:  21, steps per second:  69, episode reward: 43.582, mean reward:  2.075 [-2.345, 32.457], mean action: 4.095 [0.000, 19.000],  loss: 0.023106, mae: 0.369879, mean_q: 0.480804, mean_eps: 0.000000
 1788/5000: episode: 62, duration: 0.335s, episode steps:  23, steps per second:  69, episode reward: 38.564, mean reward:  1.677 [-3.000, 31.744], mean action: 2.826 [0.000, 19.000],  loss: 0.020324, mae: 0.342245, mean_q: 0.527129, mean_eps: 0.000000
 1828/5000: episode: 63, duration: 0.564s, episode steps:  40, steps per second:  71, episode reward: -32.100, mean reward: -0.802 [-32.161,  2.392], mean action: 3.375 [0.000, 19.000],  loss: 0.021223, mae: 0.352047, mean_q: 0.541678, mean_eps: 0.000000
 1850/5000: episode: 64, duration: 0.319s, episode steps:  22, steps per second:  69, episode reward: 43.676, mean reward:  1.985 [-2.099, 32.102], mean action: 5.636 [0.000, 13.000],  loss: 0.020175, mae: 0.346873, mean_q: 0.559219, mean_eps: 0.000000
 1870/5000: episode: 65, duration: 0.303s, episode steps:  20, steps per second:  66, episode reward: 41.523, mean reward:  2.076 [-2.387, 32.180], mean action: 4.400 [0.000, 19.000],  loss: 0.018951, mae: 0.341102, mean_q: 0.555086, mean_eps: 0.000000
 1889/5000: episode: 66, duration: 0.278s, episode steps:  19, steps per second:  68, episode reward: 35.345, mean reward:  1.860 [-3.000, 32.010], mean action: 2.895 [0.000, 19.000],  loss: 0.019748, mae: 0.345986, mean_q: 0.560408, mean_eps: 0.000000
 1922/5000: episode: 67, duration: 0.465s, episode steps:  33, steps per second:  71, episode reward: 32.756, mean reward:  0.993 [-2.838, 32.190], mean action: 7.455 [1.000, 19.000],  loss: 0.021818, mae: 0.363846, mean_q: 0.497637, mean_eps: 0.000000
 1943/5000: episode: 68, duration: 0.301s, episode steps:  21, steps per second:  70, episode reward: 41.169, mean reward:  1.960 [-2.403, 32.480], mean action: 5.905 [0.000, 19.000],  loss: 0.018458, mae: 0.351917, mean_q: 0.449742, mean_eps: 0.000000
 1975/5000: episode: 69, duration: 0.454s, episode steps:  32, steps per second:  70, episode reward: 38.575, mean reward:  1.205 [-2.704, 32.310], mean action: 6.219 [0.000, 20.000],  loss: 0.019019, mae: 0.340394, mean_q: 0.487088, mean_eps: 0.000000
 2007/5000: episode: 70, duration: 0.469s, episode steps:  32, steps per second:  68, episode reward: 41.080, mean reward:  1.284 [-2.527, 32.080], mean action: 3.281 [0.000, 20.000],  loss: 0.020466, mae: 0.347262, mean_q: 0.523319, mean_eps: 0.000000
 2035/5000: episode: 71, duration: 0.407s, episode steps:  28, steps per second:  69, episode reward: 35.461, mean reward:  1.266 [-3.000, 32.023], mean action: 4.571 [0.000, 15.000],  loss: 0.020713, mae: 0.354933, mean_q: 0.454058, mean_eps: 0.000000
 2074/5000: episode: 72, duration: 0.589s, episode steps:  39, steps per second:  66, episode reward: 37.163, mean reward:  0.953 [-2.243, 32.020], mean action: 5.410 [0.000, 16.000],  loss: 0.019890, mae: 0.345550, mean_q: 0.462650, mean_eps: 0.000000
 2091/5000: episode: 73, duration: 0.257s, episode steps:  17, steps per second:  66, episode reward: 42.000, mean reward:  2.471 [-2.212, 30.301], mean action: 1.941 [0.000, 4.000],  loss: 0.020848, mae: 0.340207, mean_q: 0.610782, mean_eps: 0.000000
 2134/5000: episode: 74, duration: 0.616s, episode steps:  43, steps per second:  70, episode reward: 43.949, mean reward:  1.022 [-2.125, 32.170], mean action: 2.442 [0.000, 19.000],  loss: 0.017844, mae: 0.329766, mean_q: 0.557356, mean_eps: 0.000000
 2167/5000: episode: 75, duration: 0.468s, episode steps:  33, steps per second:  71, episode reward: 41.903, mean reward:  1.270 [-2.384, 32.343], mean action: 2.212 [0.000, 19.000],  loss: 0.019199, mae: 0.344341, mean_q: 0.559059, mean_eps: 0.000000
 2192/5000: episode: 76, duration: 0.362s, episode steps:  25, steps per second:  69, episode reward: 38.498, mean reward:  1.540 [-3.000, 31.887], mean action: 5.880 [1.000, 19.000],  loss: 0.018090, mae: 0.349320, mean_q: 0.469810, mean_eps: 0.000000
 2220/5000: episode: 77, duration: 0.394s, episode steps:  28, steps per second:  71, episode reward: 38.495, mean reward:  1.375 [-3.000, 32.770], mean action: 5.571 [0.000, 20.000],  loss: 0.021232, mae: 0.353789, mean_q: 0.503524, mean_eps: 0.000000
 2247/5000: episode: 78, duration: 0.399s, episode steps:  27, steps per second:  68, episode reward: 41.154, mean reward:  1.524 [-2.492, 32.420], mean action: 1.926 [0.000, 19.000],  loss: 0.019563, mae: 0.337318, mean_q: 0.513273, mean_eps: 0.000000
 2272/5000: episode: 79, duration: 0.362s, episode steps:  25, steps per second:  69, episode reward: 43.883, mean reward:  1.755 [-3.000, 32.050], mean action: 3.200 [0.000, 14.000],  loss: 0.025524, mae: 0.363215, mean_q: 0.539586, mean_eps: 0.000000
 2302/5000: episode: 80, duration: 0.436s, episode steps:  30, steps per second:  69, episode reward: 40.519, mean reward:  1.351 [-2.629, 32.050], mean action: 4.000 [0.000, 19.000],  loss: 0.020670, mae: 0.348628, mean_q: 0.543192, mean_eps: 0.000000
 2322/5000: episode: 81, duration: 0.286s, episode steps:  20, steps per second:  70, episode reward: 41.383, mean reward:  2.069 [-2.577, 31.733], mean action: 3.150 [1.000, 16.000],  loss: 0.018006, mae: 0.334899, mean_q: 0.552219, mean_eps: 0.000000
 2336/5000: episode: 82, duration: 0.217s, episode steps:  14, steps per second:  65, episode reward: 47.931, mean reward:  3.424 [-0.049, 32.030], mean action: 0.500 [0.000, 5.000],  loss: 0.021142, mae: 0.350588, mean_q: 0.563751, mean_eps: 0.000000
 2359/5000: episode: 83, duration: 0.331s, episode steps:  23, steps per second:  69, episode reward: 42.000, mean reward:  1.826 [-3.000, 32.130], mean action: 3.304 [0.000, 16.000],  loss: 0.022945, mae: 0.360637, mean_q: 0.543592, mean_eps: 0.000000
 2382/5000: episode: 84, duration: 0.347s, episode steps:  23, steps per second:  66, episode reward: 39.000, mean reward:  1.696 [-2.236, 32.190], mean action: 2.783 [0.000, 15.000],  loss: 0.019189, mae: 0.348136, mean_q: 0.553602, mean_eps: 0.000000
 2403/5000: episode: 85, duration: 0.311s, episode steps:  21, steps per second:  67, episode reward: 36.000, mean reward:  1.714 [-3.000, 32.480], mean action: 3.619 [0.000, 15.000],  loss: 0.016679, mae: 0.341500, mean_q: 0.487383, mean_eps: 0.000000
 2428/5000: episode: 86, duration: 0.354s, episode steps:  25, steps per second:  71, episode reward: 41.506, mean reward:  1.660 [-2.676, 31.706], mean action: 2.400 [0.000, 15.000],  loss: 0.021120, mae: 0.352029, mean_q: 0.505146, mean_eps: 0.000000
 2442/5000: episode: 87, duration: 0.220s, episode steps:  14, steps per second:  64, episode reward: 44.090, mean reward:  3.149 [-2.489, 32.175], mean action: 3.143 [0.000, 12.000],  loss: 0.019454, mae: 0.343697, mean_q: 0.581689, mean_eps: 0.000000
 2485/5000: episode: 88, duration: 0.596s, episode steps:  43, steps per second:  72, episode reward: 38.734, mean reward:  0.901 [-2.512, 32.030], mean action: 3.860 [0.000, 13.000],  loss: 0.022397, mae: 0.361035, mean_q: 0.567833, mean_eps: 0.000000
 2507/5000: episode: 89, duration: 0.319s, episode steps:  22, steps per second:  69, episode reward: 41.140, mean reward:  1.870 [-3.000, 31.738], mean action: 2.500 [0.000, 11.000],  loss: 0.021189, mae: 0.353742, mean_q: 0.590390, mean_eps: 0.000000
 2553/5000: episode: 90, duration: 0.635s, episode steps:  46, steps per second:  72, episode reward: 44.480, mean reward:  0.967 [-2.004, 32.130], mean action: 3.761 [0.000, 13.000],  loss: 0.020851, mae: 0.350225, mean_q: 0.574827, mean_eps: 0.000000
 2580/5000: episode: 91, duration: 0.389s, episode steps:  27, steps per second:  69, episode reward: 38.434, mean reward:  1.423 [-2.350, 32.160], mean action: 2.630 [0.000, 12.000],  loss: 0.023517, mae: 0.361287, mean_q: 0.570402, mean_eps: 0.000000
 2596/5000: episode: 92, duration: 0.256s, episode steps:  16, steps per second:  62, episode reward: 47.665, mean reward:  2.979 [ 0.000, 32.090], mean action: 5.250 [0.000, 15.000],  loss: 0.018878, mae: 0.337873, mean_q: 0.522983, mean_eps: 0.000000
 2633/5000: episode: 93, duration: 0.508s, episode steps:  37, steps per second:  73, episode reward: 37.705, mean reward:  1.019 [-2.669, 32.051], mean action: 3.189 [0.000, 16.000],  loss: 0.022586, mae: 0.356561, mean_q: 0.494648, mean_eps: 0.000000
 2668/5000: episode: 94, duration: 0.496s, episode steps:  35, steps per second:  71, episode reward: -34.390, mean reward: -0.983 [-32.183,  2.320], mean action: 5.057 [1.000, 12.000],  loss: 0.019432, mae: 0.344088, mean_q: 0.503464, mean_eps: 0.000000
 2696/5000: episode: 95, duration: 0.402s, episode steps:  28, steps per second:  70, episode reward: 35.164, mean reward:  1.256 [-2.272, 32.240], mean action: 4.500 [0.000, 13.000],  loss: 0.021897, mae: 0.347836, mean_q: 0.545342, mean_eps: 0.000000
 2714/5000: episode: 96, duration: 0.273s, episode steps:  18, steps per second:  66, episode reward: 38.128, mean reward:  2.118 [-3.000, 32.374], mean action: 6.278 [2.000, 16.000],  loss: 0.021763, mae: 0.351347, mean_q: 0.510984, mean_eps: 0.000000
 2744/5000: episode: 97, duration: 0.427s, episode steps:  30, steps per second:  70, episode reward: 46.427, mean reward:  1.548 [-0.327, 32.130], mean action: 3.733 [0.000, 14.000],  loss: 0.019852, mae: 0.348043, mean_q: 0.530125, mean_eps: 0.000000
 2780/5000: episode: 98, duration: 0.499s, episode steps:  36, steps per second:  72, episode reward: 35.811, mean reward:  0.995 [-2.654, 30.194], mean action: 4.472 [0.000, 12.000],  loss: 0.021931, mae: 0.356066, mean_q: 0.499616, mean_eps: 0.000000
 2798/5000: episode: 99, duration: 0.271s, episode steps:  18, steps per second:  66, episode reward: 46.869, mean reward:  2.604 [-0.312, 32.160], mean action: 2.722 [0.000, 14.000],  loss: 0.019487, mae: 0.346133, mean_q: 0.544944, mean_eps: 0.000000
 2818/5000: episode: 100, duration: 0.310s, episode steps:  20, steps per second:  64, episode reward: 41.770, mean reward:  2.088 [-2.508, 32.010], mean action: 2.750 [0.000, 12.000],  loss: 0.020009, mae: 0.349856, mean_q: 0.603870, mean_eps: 0.000000
 2846/5000: episode: 101, duration: 0.406s, episode steps:  28, steps per second:  69, episode reward: 32.901, mean reward:  1.175 [-2.496, 31.941], mean action: 4.143 [0.000, 14.000],  loss: 0.018554, mae: 0.334818, mean_q: 0.532749, mean_eps: 0.000000
 2867/5000: episode: 102, duration: 0.305s, episode steps:  21, steps per second:  69, episode reward: 46.991, mean reward:  2.238 [-0.370, 32.770], mean action: 2.238 [1.000, 5.000],  loss: 0.023128, mae: 0.363906, mean_q: 0.570138, mean_eps: 0.000000
 2884/5000: episode: 103, duration: 0.263s, episode steps:  17, steps per second:  65, episode reward: 42.000, mean reward:  2.471 [-2.287, 29.993], mean action: 2.353 [0.000, 12.000],  loss: 0.017933, mae: 0.330489, mean_q: 0.523673, mean_eps: 0.000000
 2908/5000: episode: 104, duration: 0.340s, episode steps:  24, steps per second:  71, episode reward: 39.000, mean reward:  1.625 [-3.000, 32.190], mean action: 2.458 [0.000, 11.000],  loss: 0.021717, mae: 0.347714, mean_q: 0.587482, mean_eps: 0.000000
 2934/5000: episode: 105, duration: 0.376s, episode steps:  26, steps per second:  69, episode reward: 35.781, mean reward:  1.376 [-2.740, 33.000], mean action: 3.385 [0.000, 12.000],  loss: 0.021089, mae: 0.346526, mean_q: 0.578517, mean_eps: 0.000000
 2955/5000: episode: 106, duration: 0.327s, episode steps:  21, steps per second:  64, episode reward: 38.546, mean reward:  1.836 [-3.000, 32.020], mean action: 3.810 [0.000, 12.000],  loss: 0.020552, mae: 0.346908, mean_q: 0.549164, mean_eps: 0.000000
 2984/5000: episode: 107, duration: 0.428s, episode steps:  29, steps per second:  68, episode reward: 44.939, mean reward:  1.550 [-2.073, 32.500], mean action: 2.069 [0.000, 11.000],  loss: 0.021903, mae: 0.355077, mean_q: 0.585316, mean_eps: 0.000000
 3017/5000: episode: 108, duration: 0.475s, episode steps:  33, steps per second:  69, episode reward: 38.708, mean reward:  1.173 [-3.000, 31.948], mean action: 1.939 [0.000, 11.000],  loss: 0.018417, mae: 0.339880, mean_q: 0.549651, mean_eps: 0.000000
 3033/5000: episode: 109, duration: 0.244s, episode steps:  16, steps per second:  66, episode reward: 47.179, mean reward:  2.949 [-0.560, 32.580], mean action: 0.750 [0.000, 12.000],  loss: 0.021462, mae: 0.348389, mean_q: 0.548557, mean_eps: 0.000000
 3061/5000: episode: 110, duration: 0.393s, episode steps:  28, steps per second:  71, episode reward: 40.742, mean reward:  1.455 [-2.696, 31.951], mean action: 4.821 [0.000, 19.000],  loss: 0.020279, mae: 0.339824, mean_q: 0.500592, mean_eps: 0.000000
 3097/5000: episode: 111, duration: 0.558s, episode steps:  36, steps per second:  64, episode reward: 36.000, mean reward:  1.000 [-2.412, 32.212], mean action: 8.972 [0.000, 20.000],  loss: 0.020745, mae: 0.347209, mean_q: 0.512085, mean_eps: 0.000000
 3124/5000: episode: 112, duration: 0.387s, episode steps:  27, steps per second:  70, episode reward: 37.796, mean reward:  1.400 [-3.000, 32.842], mean action: 3.963 [0.000, 19.000],  loss: 0.020944, mae: 0.355476, mean_q: 0.501562, mean_eps: 0.000000
 3181/5000: episode: 113, duration: 0.796s, episode steps:  57, steps per second:  72, episode reward: 35.245, mean reward:  0.618 [-2.442, 32.320], mean action: 5.053 [0.000, 19.000],  loss: 0.019837, mae: 0.342680, mean_q: 0.499265, mean_eps: 0.000000
 3201/5000: episode: 114, duration: 0.505s, episode steps:  20, steps per second:  40, episode reward: 35.738, mean reward:  1.787 [-3.000, 32.339], mean action: 5.050 [0.000, 19.000],  loss: 0.015956, mae: 0.309863, mean_q: 0.522027, mean_eps: 0.000000
 3213/5000: episode: 115, duration: 0.205s, episode steps:  12, steps per second:  59, episode reward: 41.145, mean reward:  3.429 [-2.296, 32.779], mean action: 4.833 [0.000, 16.000],  loss: 0.024598, mae: 0.350333, mean_q: 0.527934, mean_eps: 0.000000
 3241/5000: episode: 116, duration: 0.399s, episode steps:  28, steps per second:  70, episode reward: 38.517, mean reward:  1.376 [-2.732, 32.343], mean action: 6.964 [0.000, 16.000],  loss: 0.022198, mae: 0.347351, mean_q: 0.553404, mean_eps: 0.000000
 3265/5000: episode: 117, duration: 0.392s, episode steps:  24, steps per second:  61, episode reward: 38.661, mean reward:  1.611 [-2.752, 31.923], mean action: 2.208 [0.000, 12.000],  loss: 0.023226, mae: 0.352095, mean_q: 0.564777, mean_eps: 0.000000
 3309/5000: episode: 118, duration: 0.761s, episode steps:  44, steps per second:  58, episode reward: 41.799, mean reward:  0.950 [-2.361, 32.080], mean action: 4.591 [0.000, 14.000],  loss: 0.020202, mae: 0.342830, mean_q: 0.517286, mean_eps: 0.000000
 3346/5000: episode: 119, duration: 0.601s, episode steps:  37, steps per second:  62, episode reward: 43.652, mean reward:  1.180 [-2.125, 32.456], mean action: 4.541 [0.000, 15.000],  loss: 0.021908, mae: 0.346532, mean_q: 0.506423, mean_eps: 0.000000
 3381/5000: episode: 120, duration: 0.515s, episode steps:  35, steps per second:  68, episode reward: 41.481, mean reward:  1.185 [-2.459, 32.341], mean action: 2.286 [0.000, 19.000],  loss: 0.018163, mae: 0.319064, mean_q: 0.486770, mean_eps: 0.000000
 3401/5000: episode: 121, duration: 0.338s, episode steps:  20, steps per second:  59, episode reward: 38.133, mean reward:  1.907 [-2.472, 32.150], mean action: 3.550 [1.000, 16.000],  loss: 0.020987, mae: 0.336330, mean_q: 0.519750, mean_eps: 0.000000
 3421/5000: episode: 122, duration: 0.315s, episode steps:  20, steps per second:  64, episode reward: 41.700, mean reward:  2.085 [-2.448, 32.903], mean action: 2.750 [0.000, 12.000],  loss: 0.021541, mae: 0.336463, mean_q: 0.552424, mean_eps: 0.000000
 3446/5000: episode: 123, duration: 0.431s, episode steps:  25, steps per second:  58, episode reward: 38.126, mean reward:  1.525 [-2.911, 32.122], mean action: 4.600 [0.000, 19.000],  loss: 0.023580, mae: 0.351983, mean_q: 0.584547, mean_eps: 0.000000
 3466/5000: episode: 124, duration: 0.306s, episode steps:  20, steps per second:  65, episode reward: 47.159, mean reward:  2.358 [-0.070, 32.203], mean action: 0.900 [0.000, 3.000],  loss: 0.015983, mae: 0.314308, mean_q: 0.546947, mean_eps: 0.000000
 3480/5000: episode: 125, duration: 0.219s, episode steps:  14, steps per second:  64, episode reward: 44.786, mean reward:  3.199 [-2.382, 32.186], mean action: 1.857 [0.000, 3.000],  loss: 0.019892, mae: 0.340155, mean_q: 0.570582, mean_eps: 0.000000
 3515/5000: episode: 126, duration: 0.488s, episode steps:  35, steps per second:  72, episode reward: 38.016, mean reward:  1.086 [-2.653, 31.982], mean action: 3.114 [0.000, 15.000],  loss: 0.020830, mae: 0.343314, mean_q: 0.528760, mean_eps: 0.000000
 3539/5000: episode: 127, duration: 0.346s, episode steps:  24, steps per second:  69, episode reward: 41.070, mean reward:  1.711 [-2.464, 32.370], mean action: 2.625 [0.000, 12.000],  loss: 0.019506, mae: 0.331478, mean_q: 0.528044, mean_eps: 0.000000
 3566/5000: episode: 128, duration: 0.375s, episode steps:  27, steps per second:  72, episode reward: 38.814, mean reward:  1.438 [-3.000, 33.000], mean action: 2.148 [0.000, 12.000],  loss: 0.023910, mae: 0.347567, mean_q: 0.529738, mean_eps: 0.000000
 3589/5000: episode: 129, duration: 0.330s, episode steps:  23, steps per second:  70, episode reward: 41.695, mean reward:  1.813 [-2.785, 32.140], mean action: 3.391 [0.000, 12.000],  loss: 0.019132, mae: 0.322828, mean_q: 0.562139, mean_eps: 0.000000
 3606/5000: episode: 130, duration: 0.263s, episode steps:  17, steps per second:  65, episode reward: 44.071, mean reward:  2.592 [-2.596, 32.280], mean action: 2.118 [0.000, 11.000],  loss: 0.022570, mae: 0.345621, mean_q: 0.550544, mean_eps: 0.000000
 3630/5000: episode: 131, duration: 0.352s, episode steps:  24, steps per second:  68, episode reward: 43.746, mean reward:  1.823 [-2.260, 32.010], mean action: 4.125 [1.000, 15.000],  loss: 0.019646, mae: 0.335337, mean_q: 0.494990, mean_eps: 0.000000
 3655/5000: episode: 132, duration: 0.379s, episode steps:  25, steps per second:  66, episode reward: 44.151, mean reward:  1.766 [-2.037, 32.037], mean action: 2.840 [0.000, 14.000],  loss: 0.019308, mae: 0.335677, mean_q: 0.504415, mean_eps: 0.000000
 3684/5000: episode: 133, duration: 0.416s, episode steps:  29, steps per second:  70, episode reward: 41.279, mean reward:  1.423 [-3.000, 32.080], mean action: 1.724 [0.000, 16.000],  loss: 0.022832, mae: 0.344407, mean_q: 0.518329, mean_eps: 0.000000
 3698/5000: episode: 134, duration: 0.215s, episode steps:  14, steps per second:  65, episode reward: 44.621, mean reward:  3.187 [-2.377, 32.170], mean action: 1.357 [0.000, 15.000],  loss: 0.018722, mae: 0.327520, mean_q: 0.511508, mean_eps: 0.000000
 3715/5000: episode: 135, duration: 0.254s, episode steps:  17, steps per second:  67, episode reward: 44.594, mean reward:  2.623 [-2.238, 32.760], mean action: 2.353 [0.000, 15.000],  loss: 0.017497, mae: 0.321985, mean_q: 0.470517, mean_eps: 0.000000
 3746/5000: episode: 136, duration: 0.443s, episode steps:  31, steps per second:  70, episode reward: 32.746, mean reward:  1.056 [-3.000, 32.220], mean action: 6.968 [0.000, 15.000],  loss: 0.019598, mae: 0.334753, mean_q: 0.490642, mean_eps: 0.000000
 3790/5000: episode: 137, duration: 0.616s, episode steps:  44, steps per second:  71, episode reward: -32.910, mean reward: -0.748 [-32.861,  2.470], mean action: 5.364 [0.000, 19.000],  loss: 0.019471, mae: 0.333588, mean_q: 0.533847, mean_eps: 0.000000
 3818/5000: episode: 138, duration: 0.445s, episode steps:  28, steps per second:  63, episode reward: 41.210, mean reward:  1.472 [-2.590, 32.362], mean action: 9.321 [0.000, 20.000],  loss: 0.020503, mae: 0.346084, mean_q: 0.508807, mean_eps: 0.000000
 3851/5000: episode: 139, duration: 0.502s, episode steps:  33, steps per second:  66, episode reward: 41.466, mean reward:  1.257 [-2.505, 32.230], mean action: 3.758 [0.000, 19.000],  loss: 0.020946, mae: 0.352686, mean_q: 0.550466, mean_eps: 0.000000
 3874/5000: episode: 140, duration: 0.371s, episode steps:  23, steps per second:  62, episode reward: 44.684, mean reward:  1.943 [-2.475, 32.050], mean action: 1.783 [0.000, 19.000],  loss: 0.024255, mae: 0.358463, mean_q: 0.599111, mean_eps: 0.000000
 3899/5000: episode: 141, duration: 0.379s, episode steps:  25, steps per second:  66, episode reward: 41.142, mean reward:  1.646 [-2.718, 31.422], mean action: 2.120 [0.000, 19.000],  loss: 0.021321, mae: 0.342398, mean_q: 0.591030, mean_eps: 0.000000
 3927/5000: episode: 142, duration: 0.400s, episode steps:  28, steps per second:  70, episode reward: 41.332, mean reward:  1.476 [-2.115, 31.755], mean action: 4.536 [1.000, 19.000],  loss: 0.021787, mae: 0.345012, mean_q: 0.578500, mean_eps: 0.000000
 3952/5000: episode: 143, duration: 0.356s, episode steps:  25, steps per second:  70, episode reward: 47.032, mean reward:  1.881 [-0.160, 32.340], mean action: 2.200 [0.000, 14.000],  loss: 0.021380, mae: 0.353424, mean_q: 0.560240, mean_eps: 0.000000
 3989/5000: episode: 144, duration: 0.548s, episode steps:  37, steps per second:  68, episode reward: 36.000, mean reward:  0.973 [-3.000, 32.030], mean action: 4.595 [0.000, 14.000],  loss: 0.021895, mae: 0.358757, mean_q: 0.553709, mean_eps: 0.000000
 4013/5000: episode: 145, duration: 0.343s, episode steps:  24, steps per second:  70, episode reward: 38.184, mean reward:  1.591 [-2.930, 32.044], mean action: 3.750 [0.000, 12.000],  loss: 0.022090, mae: 0.361252, mean_q: 0.537006, mean_eps: 0.000000
 4047/5000: episode: 146, duration: 0.489s, episode steps:  34, steps per second:  69, episode reward: 35.297, mean reward:  1.038 [-2.423, 32.263], mean action: 3.265 [0.000, 12.000],  loss: 0.017588, mae: 0.328446, mean_q: 0.556541, mean_eps: 0.000000
 4069/5000: episode: 147, duration: 0.321s, episode steps:  22, steps per second:  69, episode reward: 44.098, mean reward:  2.004 [-2.113, 32.210], mean action: 0.773 [0.000, 3.000],  loss: 0.023549, mae: 0.365399, mean_q: 0.598985, mean_eps: 0.000000
 4107/5000: episode: 148, duration: 0.552s, episode steps:  38, steps per second:  69, episode reward: 41.354, mean reward:  1.088 [-2.139, 32.020], mean action: 3.526 [0.000, 19.000],  loss: 0.017992, mae: 0.332417, mean_q: 0.518932, mean_eps: 0.000000
 4127/5000: episode: 149, duration: 0.301s, episode steps:  20, steps per second:  66, episode reward: 41.490, mean reward:  2.075 [-2.864, 32.340], mean action: 4.150 [0.000, 19.000],  loss: 0.021591, mae: 0.357589, mean_q: 0.421032, mean_eps: 0.000000
 4156/5000: episode: 150, duration: 0.417s, episode steps:  29, steps per second:  69, episode reward: 38.790, mean reward:  1.338 [-2.258, 32.270], mean action: 3.828 [0.000, 19.000],  loss: 0.019699, mae: 0.346192, mean_q: 0.421385, mean_eps: 0.000000
 4186/5000: episode: 151, duration: 0.418s, episode steps:  30, steps per second:  72, episode reward: 35.704, mean reward:  1.190 [-2.984, 32.220], mean action: 4.233 [0.000, 19.000],  loss: 0.024910, mae: 0.361843, mean_q: 0.505538, mean_eps: 0.000000
 4206/5000: episode: 152, duration: 0.288s, episode steps:  20, steps per second:  70, episode reward: 38.717, mean reward:  1.936 [-3.000, 32.620], mean action: 5.650 [0.000, 19.000],  loss: 0.021650, mae: 0.347501, mean_q: 0.530473, mean_eps: 0.000000
 4220/5000: episode: 153, duration: 0.221s, episode steps:  14, steps per second:  63, episode reward: 41.584, mean reward:  2.970 [-2.391, 32.494], mean action: 1.571 [0.000, 14.000],  loss: 0.020272, mae: 0.341691, mean_q: 0.566336, mean_eps: 0.000000
 4249/5000: episode: 154, duration: 0.415s, episode steps:  29, steps per second:  70, episode reward: 41.696, mean reward:  1.438 [-2.556, 31.806], mean action: 2.345 [0.000, 12.000],  loss: 0.022472, mae: 0.350279, mean_q: 0.574161, mean_eps: 0.000000
 4272/5000: episode: 155, duration: 0.326s, episode steps:  23, steps per second:  70, episode reward: 44.167, mean reward:  1.920 [-2.046, 32.353], mean action: 2.391 [0.000, 3.000],  loss: 0.018475, mae: 0.332196, mean_q: 0.618100, mean_eps: 0.000000
 4296/5000: episode: 156, duration: 0.348s, episode steps:  24, steps per second:  69, episode reward: 48.000, mean reward:  2.000 [-0.060, 32.130], mean action: 1.292 [1.000, 2.000],  loss: 0.021557, mae: 0.345690, mean_q: 0.594672, mean_eps: 0.000000
 4320/5000: episode: 157, duration: 0.346s, episode steps:  24, steps per second:  69, episode reward: 38.418, mean reward:  1.601 [-3.000, 31.860], mean action: 4.958 [0.000, 19.000],  loss: 0.017410, mae: 0.333475, mean_q: 0.574274, mean_eps: 0.000000
 4343/5000: episode: 158, duration: 0.340s, episode steps:  23, steps per second:  68, episode reward: 40.542, mean reward:  1.763 [-2.352, 32.400], mean action: 5.348 [0.000, 15.000],  loss: 0.021764, mae: 0.349088, mean_q: 0.551470, mean_eps: 0.000000
 4361/5000: episode: 159, duration: 0.268s, episode steps:  18, steps per second:  67, episode reward: 39.000, mean reward:  2.167 [-2.700, 32.760], mean action: 2.722 [0.000, 12.000],  loss: 0.023547, mae: 0.364028, mean_q: 0.564180, mean_eps: 0.000000
 4397/5000: episode: 160, duration: 0.508s, episode steps:  36, steps per second:  71, episode reward: 41.046, mean reward:  1.140 [-2.498, 32.340], mean action: 3.444 [0.000, 15.000],  loss: 0.021599, mae: 0.358797, mean_q: 0.461337, mean_eps: 0.000000
 4421/5000: episode: 161, duration: 0.351s, episode steps:  24, steps per second:  68, episode reward: 39.000, mean reward:  1.625 [-3.000, 32.200], mean action: 2.875 [0.000, 12.000],  loss: 0.020895, mae: 0.358064, mean_q: 0.482583, mean_eps: 0.000000
 4446/5000: episode: 162, duration: 0.357s, episode steps:  25, steps per second:  70, episode reward: 44.011, mean reward:  1.760 [-2.146, 32.460], mean action: 3.800 [0.000, 15.000],  loss: 0.019609, mae: 0.338387, mean_q: 0.529708, mean_eps: 0.000000
 4467/5000: episode: 163, duration: 0.316s, episode steps:  21, steps per second:  66, episode reward: 41.433, mean reward:  1.973 [-3.000, 32.113], mean action: 2.381 [0.000, 13.000],  loss: 0.026262, mae: 0.361958, mean_q: 0.615959, mean_eps: 0.000000
 4505/5000: episode: 164, duration: 0.525s, episode steps:  38, steps per second:  72, episode reward: 35.556, mean reward:  0.936 [-3.000, 32.043], mean action: 6.763 [0.000, 16.000],  loss: 0.020401, mae: 0.345005, mean_q: 0.521076, mean_eps: 0.000000
 4524/5000: episode: 165, duration: 0.278s, episode steps:  19, steps per second:  68, episode reward: 41.473, mean reward:  2.183 [-2.322, 31.795], mean action: 2.474 [0.000, 9.000],  loss: 0.024463, mae: 0.367599, mean_q: 0.498158, mean_eps: 0.000000
 4547/5000: episode: 166, duration: 0.359s, episode steps:  23, steps per second:  64, episode reward: 42.000, mean reward:  1.826 [-2.286, 32.250], mean action: 2.130 [0.000, 9.000],  loss: 0.022136, mae: 0.343136, mean_q: 0.527958, mean_eps: 0.000000
 4586/5000: episode: 167, duration: 0.553s, episode steps:  39, steps per second:  71, episode reward: 38.504, mean reward:  0.987 [-3.000, 32.004], mean action: 2.410 [0.000, 16.000],  loss: 0.020920, mae: 0.340827, mean_q: 0.519527, mean_eps: 0.000000
 4613/5000: episode: 168, duration: 0.384s, episode steps:  27, steps per second:  70, episode reward: 38.317, mean reward:  1.419 [-2.175, 31.642], mean action: 3.630 [0.000, 19.000],  loss: 0.021948, mae: 0.347589, mean_q: 0.517300, mean_eps: 0.000000
 4633/5000: episode: 169, duration: 0.293s, episode steps:  20, steps per second:  68, episode reward: 41.370, mean reward:  2.069 [-2.229, 31.749], mean action: 3.550 [0.000, 19.000],  loss: 0.020259, mae: 0.342107, mean_q: 0.527926, mean_eps: 0.000000
 4653/5000: episode: 170, duration: 0.319s, episode steps:  20, steps per second:  63, episode reward: 40.967, mean reward:  2.048 [-2.344, 31.405], mean action: 4.450 [1.000, 16.000],  loss: 0.021869, mae: 0.348551, mean_q: 0.510958, mean_eps: 0.000000
 4678/5000: episode: 171, duration: 0.377s, episode steps:  25, steps per second:  66, episode reward: 44.416, mean reward:  1.777 [-2.187, 32.040], mean action: 1.880 [0.000, 16.000],  loss: 0.022252, mae: 0.352863, mean_q: 0.577457, mean_eps: 0.000000
 4698/5000: episode: 172, duration: 0.290s, episode steps:  20, steps per second:  69, episode reward: 41.175, mean reward:  2.059 [-2.093, 32.301], mean action: 2.400 [0.000, 16.000],  loss: 0.018336, mae: 0.330591, mean_q: 0.605747, mean_eps: 0.000000
 4721/5000: episode: 173, duration: 0.339s, episode steps:  23, steps per second:  68, episode reward: 44.002, mean reward:  1.913 [-2.037, 32.450], mean action: 2.783 [0.000, 13.000],  loss: 0.020708, mae: 0.345725, mean_q: 0.591356, mean_eps: 0.000000
 4752/5000: episode: 174, duration: 0.430s, episode steps:  31, steps per second:  72, episode reward: 37.872, mean reward:  1.222 [-3.000, 32.130], mean action: 3.194 [0.000, 13.000],  loss: 0.022182, mae: 0.360764, mean_q: 0.562022, mean_eps: 0.000000
 4770/5000: episode: 175, duration: 0.270s, episode steps:  18, steps per second:  67, episode reward: 41.483, mean reward:  2.305 [-2.563, 32.208], mean action: 3.389 [0.000, 11.000],  loss: 0.023253, mae: 0.359280, mean_q: 0.518488, mean_eps: 0.000000
 4801/5000: episode: 176, duration: 0.437s, episode steps:  31, steps per second:  71, episode reward: 38.938, mean reward:  1.256 [-3.000, 32.090], mean action: 3.161 [1.000, 11.000],  loss: 0.018917, mae: 0.336688, mean_q: 0.514416, mean_eps: 0.000000
 4840/5000: episode: 177, duration: 0.543s, episode steps:  39, steps per second:  72, episode reward: 41.400, mean reward:  1.062 [-2.342, 32.070], mean action: 2.846 [0.000, 14.000],  loss: 0.020504, mae: 0.347168, mean_q: 0.513056, mean_eps: 0.000000
 4856/5000: episode: 178, duration: 0.240s, episode steps:  16, steps per second:  67, episode reward: 41.183, mean reward:  2.574 [-3.000, 32.260], mean action: 2.312 [0.000, 11.000],  loss: 0.016801, mae: 0.338669, mean_q: 0.470158, mean_eps: 0.000000
 4893/5000: episode: 179, duration: 0.742s, episode steps:  37, steps per second:  50, episode reward: 35.606, mean reward:  0.962 [-2.382, 31.976], mean action: 4.919 [0.000, 20.000],  loss: 0.020275, mae: 0.349920, mean_q: 0.498973, mean_eps: 0.000000
 4927/5000: episode: 180, duration: 0.486s, episode steps:  34, steps per second:  70, episode reward: 41.663, mean reward:  1.225 [-2.284, 32.901], mean action: 5.029 [0.000, 15.000],  loss: 0.019029, mae: 0.337636, mean_q: 0.504706, mean_eps: 0.000000
 4960/5000: episode: 181, duration: 0.468s, episode steps:  33, steps per second:  70, episode reward: 38.468, mean reward:  1.166 [-2.303, 32.100], mean action: 3.879 [0.000, 14.000],  loss: 0.020829, mae: 0.345997, mean_q: 0.565339, mean_eps: 0.000000
 4981/5000: episode: 182, duration: 0.307s, episode steps:  21, steps per second:  68, episode reward: 47.603, mean reward:  2.267 [-0.188, 32.080], mean action: 3.429 [0.000, 15.000],  loss: 0.016489, mae: 0.319805, mean_q: 0.555919, mean_eps: 0.000000
done, took 66.417 seconds
DQN Evaluation: 8691 victories out of 10165 episodes
Training for 5000 steps ...
   25/5000: episode: 1, duration: 0.206s, episode steps:  25, steps per second: 122, episode reward: 35.598, mean reward:  1.424 [-2.321, 32.760], mean action: 4.720 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   45/5000: episode: 2, duration: 0.145s, episode steps:  20, steps per second: 138, episode reward: 38.539, mean reward:  1.927 [-2.733, 32.120], mean action: 3.500 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   61/5000: episode: 3, duration: 0.121s, episode steps:  16, steps per second: 132, episode reward: 38.622, mean reward:  2.414 [-3.000, 32.312], mean action: 4.062 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   75/5000: episode: 4, duration: 0.103s, episode steps:  14, steps per second: 136, episode reward: 41.611, mean reward:  2.972 [-2.679, 32.611], mean action: 3.071 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   98/5000: episode: 5, duration: 0.164s, episode steps:  23, steps per second: 141, episode reward: 33.000, mean reward:  1.435 [-2.347, 30.252], mean action: 4.435 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  116/5000: episode: 6, duration: 0.131s, episode steps:  18, steps per second: 138, episode reward: 38.411, mean reward:  2.134 [-2.549, 32.904], mean action: 4.944 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  161/5000: episode: 7, duration: 0.295s, episode steps:  45, steps per second: 153, episode reward: 43.510, mean reward:  0.967 [-3.000, 32.450], mean action: 2.311 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  180/5000: episode: 8, duration: 0.133s, episode steps:  19, steps per second: 142, episode reward: 35.454, mean reward:  1.866 [-3.000, 32.020], mean action: 4.263 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  187/5000: episode: 9, duration: 0.069s, episode steps:   7, steps per second: 101, episode reward: 48.000, mean reward:  6.857 [ 0.590, 33.000], mean action: 1.571 [1.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  209/5000: episode: 10, duration: 0.163s, episode steps:  22, steps per second: 135, episode reward: 37.602, mean reward:  1.709 [-2.757, 32.317], mean action: 6.591 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  234/5000: episode: 11, duration: 0.173s, episode steps:  25, steps per second: 145, episode reward: 35.114, mean reward:  1.405 [-2.185, 31.745], mean action: 3.440 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  257/5000: episode: 12, duration: 0.155s, episode steps:  23, steps per second: 149, episode reward: 32.701, mean reward:  1.422 [-2.508, 32.091], mean action: 4.913 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  277/5000: episode: 13, duration: 0.138s, episode steps:  20, steps per second: 145, episode reward: 36.000, mean reward:  1.800 [-3.000, 33.000], mean action: 4.900 [1.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  301/5000: episode: 14, duration: 0.162s, episode steps:  24, steps per second: 148, episode reward: 33.000, mean reward:  1.375 [-2.300, 29.170], mean action: 3.625 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  325/5000: episode: 15, duration: 0.162s, episode steps:  24, steps per second: 149, episode reward: 32.338, mean reward:  1.347 [-3.000, 32.640], mean action: 5.125 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  340/5000: episode: 16, duration: 0.115s, episode steps:  15, steps per second: 131, episode reward: 35.424, mean reward:  2.362 [-3.000, 32.530], mean action: 5.400 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  362/5000: episode: 17, duration: 0.151s, episode steps:  22, steps per second: 146, episode reward: 32.906, mean reward:  1.496 [-2.715, 33.000], mean action: 4.818 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  376/5000: episode: 18, duration: 0.110s, episode steps:  14, steps per second: 128, episode reward: 41.552, mean reward:  2.968 [-2.381, 32.170], mean action: 4.286 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  397/5000: episode: 19, duration: 0.152s, episode steps:  21, steps per second: 139, episode reward: 38.735, mean reward:  1.845 [-2.352, 32.065], mean action: 3.952 [1.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  425/5000: episode: 20, duration: 0.181s, episode steps:  28, steps per second: 155, episode reward: -32.680, mean reward: -1.167 [-32.302,  3.000], mean action: 6.679 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  445/5000: episode: 21, duration: 0.136s, episode steps:  20, steps per second: 147, episode reward: 33.000, mean reward:  1.650 [-3.000, 32.480], mean action: 4.800 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  479/5000: episode: 22, duration: 0.214s, episode steps:  34, steps per second: 159, episode reward: 35.060, mean reward:  1.031 [-3.000, 31.654], mean action: 6.471 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  502/5000: episode: 23, duration: 0.163s, episode steps:  23, steps per second: 141, episode reward: 37.677, mean reward:  1.638 [-2.268, 32.023], mean action: 6.652 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  525/5000: episode: 24, duration: 0.155s, episode steps:  23, steps per second: 148, episode reward: 35.144, mean reward:  1.528 [-2.281, 33.000], mean action: 3.783 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  549/5000: episode: 25, duration: 0.154s, episode steps:  24, steps per second: 156, episode reward: -38.720, mean reward: -1.613 [-32.910,  2.253], mean action: 5.375 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  575/5000: episode: 26, duration: 0.169s, episode steps:  26, steps per second: 154, episode reward: 32.985, mean reward:  1.269 [-2.606, 32.035], mean action: 6.385 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  592/5000: episode: 27, duration: 0.143s, episode steps:  17, steps per second: 119, episode reward: 38.301, mean reward:  2.253 [-2.373, 32.301], mean action: 3.235 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  616/5000: episode: 28, duration: 0.169s, episode steps:  24, steps per second: 142, episode reward: 35.411, mean reward:  1.475 [-2.434, 32.140], mean action: 4.542 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  642/5000: episode: 29, duration: 0.173s, episode steps:  26, steps per second: 151, episode reward: 32.022, mean reward:  1.232 [-2.463, 31.643], mean action: 6.538 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  659/5000: episode: 30, duration: 0.130s, episode steps:  17, steps per second: 131, episode reward: 41.053, mean reward:  2.415 [-3.000, 31.923], mean action: 2.765 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  677/5000: episode: 31, duration: 0.118s, episode steps:  18, steps per second: 152, episode reward: -41.550, mean reward: -2.308 [-32.195,  2.139], mean action: 6.833 [1.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  702/5000: episode: 32, duration: 0.171s, episode steps:  25, steps per second: 146, episode reward: -33.000, mean reward: -1.320 [-32.422,  2.373], mean action: 7.080 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  725/5000: episode: 33, duration: 0.162s, episode steps:  23, steps per second: 142, episode reward: -38.300, mean reward: -1.665 [-32.574,  2.351], mean action: 6.000 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  742/5000: episode: 34, duration: 0.126s, episode steps:  17, steps per second: 135, episode reward: 38.429, mean reward:  2.261 [-3.000, 32.627], mean action: 4.882 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  759/5000: episode: 35, duration: 0.125s, episode steps:  17, steps per second: 136, episode reward: 35.500, mean reward:  2.088 [-2.588, 32.400], mean action: 5.176 [1.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  777/5000: episode: 36, duration: 0.139s, episode steps:  18, steps per second: 130, episode reward: 45.000, mean reward:  2.500 [-2.211, 32.140], mean action: 2.500 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  806/5000: episode: 37, duration: 0.197s, episode steps:  29, steps per second: 147, episode reward: 38.567, mean reward:  1.330 [-2.305, 32.127], mean action: 2.621 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  828/5000: episode: 38, duration: 0.141s, episode steps:  22, steps per second: 156, episode reward: 34.815, mean reward:  1.582 [-3.000, 32.603], mean action: 4.818 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  851/5000: episode: 39, duration: 0.146s, episode steps:  23, steps per second: 158, episode reward: 30.000, mean reward:  1.304 [-2.273, 30.000], mean action: 4.522 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  873/5000: episode: 40, duration: 0.147s, episode steps:  22, steps per second: 149, episode reward: 33.000, mean reward:  1.500 [-2.508, 32.050], mean action: 4.727 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  895/5000: episode: 41, duration: 0.160s, episode steps:  22, steps per second: 137, episode reward: 41.015, mean reward:  1.864 [-2.301, 32.014], mean action: 1.955 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  924/5000: episode: 42, duration: 0.187s, episode steps:  29, steps per second: 155, episode reward: 38.523, mean reward:  1.328 [-2.374, 32.304], mean action: 2.966 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  940/5000: episode: 43, duration: 0.115s, episode steps:  16, steps per second: 140, episode reward: 38.886, mean reward:  2.430 [-3.000, 33.000], mean action: 3.875 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  951/5000: episode: 44, duration: 0.090s, episode steps:  11, steps per second: 123, episode reward: 47.013, mean reward:  4.274 [ 0.127, 33.000], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  975/5000: episode: 45, duration: 0.170s, episode steps:  24, steps per second: 141, episode reward: 35.971, mean reward:  1.499 [-2.441, 32.530], mean action: 5.333 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  990/5000: episode: 46, duration: 0.110s, episode steps:  15, steps per second: 136, episode reward: 42.000, mean reward:  2.800 [-2.140, 32.910], mean action: 3.600 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1009/5000: episode: 47, duration: 0.208s, episode steps:  19, steps per second:  91, episode reward: 39.000, mean reward:  2.053 [-2.374, 32.340], mean action: 4.105 [0.000, 16.000],  loss: 0.023600, mae: 0.358493, mean_q: 0.506060, mean_eps: 0.000000
 1049/5000: episode: 48, duration: 0.568s, episode steps:  40, steps per second:  70, episode reward: -32.230, mean reward: -0.806 [-31.791,  3.000], mean action: 3.800 [0.000, 16.000],  loss: 0.021114, mae: 0.352334, mean_q: 0.512536, mean_eps: 0.000000
 1078/5000: episode: 49, duration: 0.410s, episode steps:  29, steps per second:  71, episode reward: 34.725, mean reward:  1.197 [-2.479, 31.645], mean action: 2.448 [0.000, 12.000],  loss: 0.019748, mae: 0.343094, mean_q: 0.570607, mean_eps: 0.000000
 1106/5000: episode: 50, duration: 0.390s, episode steps:  28, steps per second:  72, episode reward: 32.096, mean reward:  1.146 [-2.332, 32.140], mean action: 4.464 [0.000, 16.000],  loss: 0.021692, mae: 0.348790, mean_q: 0.563183, mean_eps: 0.000000
 1126/5000: episode: 51, duration: 0.283s, episode steps:  20, steps per second:  71, episode reward: -35.270, mean reward: -1.763 [-32.298,  2.754], mean action: 7.700 [0.000, 16.000],  loss: 0.021890, mae: 0.354276, mean_q: 0.530510, mean_eps: 0.000000
 1145/5000: episode: 52, duration: 0.277s, episode steps:  19, steps per second:  69, episode reward: 35.900, mean reward:  1.889 [-3.000, 32.170], mean action: 4.316 [0.000, 16.000],  loss: 0.023575, mae: 0.370780, mean_q: 0.559287, mean_eps: 0.000000
 1166/5000: episode: 53, duration: 0.304s, episode steps:  21, steps per second:  69, episode reward: 35.742, mean reward:  1.702 [-3.000, 32.742], mean action: 3.857 [0.000, 16.000],  loss: 0.022174, mae: 0.355389, mean_q: 0.510356, mean_eps: 0.000000
 1196/5000: episode: 54, duration: 0.427s, episode steps:  30, steps per second:  70, episode reward: 37.581, mean reward:  1.253 [-2.658, 32.180], mean action: 6.467 [0.000, 20.000],  loss: 0.019986, mae: 0.343830, mean_q: 0.568845, mean_eps: 0.000000
 1216/5000: episode: 55, duration: 0.296s, episode steps:  20, steps per second:  67, episode reward: 41.328, mean reward:  2.066 [-2.601, 32.080], mean action: 1.950 [0.000, 9.000],  loss: 0.020245, mae: 0.348458, mean_q: 0.556549, mean_eps: 0.000000
 1243/5000: episode: 56, duration: 0.563s, episode steps:  27, steps per second:  48, episode reward: 39.000, mean reward:  1.444 [-2.319, 32.050], mean action: 4.148 [0.000, 15.000],  loss: 0.018148, mae: 0.344058, mean_q: 0.536392, mean_eps: 0.000000
 1281/5000: episode: 57, duration: 0.542s, episode steps:  38, steps per second:  70, episode reward: -32.110, mean reward: -0.845 [-32.268,  2.902], mean action: 3.763 [0.000, 15.000],  loss: 0.020798, mae: 0.361399, mean_q: 0.501493, mean_eps: 0.000000
 1308/5000: episode: 58, duration: 0.385s, episode steps:  27, steps per second:  70, episode reward: 35.114, mean reward:  1.301 [-3.000, 32.170], mean action: 7.185 [0.000, 20.000],  loss: 0.022139, mae: 0.352972, mean_q: 0.560350, mean_eps: 0.000000
 1329/5000: episode: 59, duration: 0.305s, episode steps:  21, steps per second:  69, episode reward: 34.433, mean reward:  1.640 [-3.000, 31.516], mean action: 3.190 [0.000, 12.000],  loss: 0.023223, mae: 0.363160, mean_q: 0.537253, mean_eps: 0.000000
 1350/5000: episode: 60, duration: 0.298s, episode steps:  21, steps per second:  70, episode reward: 32.807, mean reward:  1.562 [-3.000, 32.477], mean action: 6.095 [0.000, 15.000],  loss: 0.019293, mae: 0.344040, mean_q: 0.551080, mean_eps: 0.000000
 1372/5000: episode: 61, duration: 0.350s, episode steps:  22, steps per second:  63, episode reward: 35.489, mean reward:  1.613 [-2.434, 32.489], mean action: 3.818 [0.000, 15.000],  loss: 0.021209, mae: 0.359066, mean_q: 0.565039, mean_eps: 0.000000
 1391/5000: episode: 62, duration: 0.304s, episode steps:  19, steps per second:  63, episode reward: 35.400, mean reward:  1.863 [-2.740, 32.900], mean action: 7.105 [0.000, 20.000],  loss: 0.022790, mae: 0.366249, mean_q: 0.583764, mean_eps: 0.000000
 1414/5000: episode: 63, duration: 0.330s, episode steps:  23, steps per second:  70, episode reward: 38.335, mean reward:  1.667 [-2.108, 33.000], mean action: 2.696 [0.000, 12.000],  loss: 0.022374, mae: 0.359234, mean_q: 0.592120, mean_eps: 0.000000
 1444/5000: episode: 64, duration: 0.423s, episode steps:  30, steps per second:  71, episode reward: 32.708, mean reward:  1.090 [-3.000, 32.510], mean action: 6.467 [2.000, 18.000],  loss: 0.022104, mae: 0.361519, mean_q: 0.555101, mean_eps: 0.000000
 1468/5000: episode: 65, duration: 0.341s, episode steps:  24, steps per second:  70, episode reward: 34.357, mean reward:  1.432 [-2.494, 32.015], mean action: 4.917 [0.000, 14.000],  loss: 0.020909, mae: 0.352901, mean_q: 0.499823, mean_eps: 0.000000
 1490/5000: episode: 66, duration: 0.312s, episode steps:  22, steps per second:  70, episode reward: 35.904, mean reward:  1.632 [-2.406, 32.224], mean action: 3.227 [0.000, 15.000],  loss: 0.018342, mae: 0.344742, mean_q: 0.493167, mean_eps: 0.000000
 1513/5000: episode: 67, duration: 0.333s, episode steps:  23, steps per second:  69, episode reward: -32.330, mean reward: -1.406 [-32.142,  2.941], mean action: 9.478 [0.000, 19.000],  loss: 0.020997, mae: 0.358206, mean_q: 0.508669, mean_eps: 0.000000
 1531/5000: episode: 68, duration: 0.270s, episode steps:  18, steps per second:  67, episode reward: 41.335, mean reward:  2.296 [-2.492, 32.150], mean action: 2.889 [0.000, 11.000],  loss: 0.020242, mae: 0.358563, mean_q: 0.544602, mean_eps: 0.000000
 1545/5000: episode: 69, duration: 0.211s, episode steps:  14, steps per second:  66, episode reward: 41.856, mean reward:  2.990 [-2.291, 32.490], mean action: 3.429 [0.000, 15.000],  loss: 0.016437, mae: 0.341806, mean_q: 0.469899, mean_eps: 0.000000
 1574/5000: episode: 70, duration: 0.426s, episode steps:  29, steps per second:  68, episode reward: -35.080, mean reward: -1.210 [-32.183,  3.061], mean action: 8.621 [0.000, 14.000],  loss: 0.020677, mae: 0.365599, mean_q: 0.530261, mean_eps: 0.000000
 1597/5000: episode: 71, duration: 0.347s, episode steps:  23, steps per second:  66, episode reward: 40.895, mean reward:  1.778 [-2.259, 32.517], mean action: 2.565 [0.000, 16.000],  loss: 0.017240, mae: 0.347340, mean_q: 0.513533, mean_eps: 0.000000
 1625/5000: episode: 72, duration: 0.400s, episode steps:  28, steps per second:  70, episode reward: 33.000, mean reward:  1.179 [-2.435, 32.743], mean action: 7.821 [0.000, 16.000],  loss: 0.024266, mae: 0.381524, mean_q: 0.484952, mean_eps: 0.000000
 1650/5000: episode: 73, duration: 0.369s, episode steps:  25, steps per second:  68, episode reward: 35.421, mean reward:  1.417 [-2.394, 32.051], mean action: 2.800 [0.000, 15.000],  loss: 0.019107, mae: 0.359177, mean_q: 0.474111, mean_eps: 0.000000
 1672/5000: episode: 74, duration: 0.322s, episode steps:  22, steps per second:  68, episode reward: 32.537, mean reward:  1.479 [-3.000, 32.250], mean action: 3.909 [0.000, 15.000],  loss: 0.020372, mae: 0.357848, mean_q: 0.496305, mean_eps: 0.000000
 1701/5000: episode: 75, duration: 0.407s, episode steps:  29, steps per second:  71, episode reward: 36.000, mean reward:  1.241 [-2.463, 32.210], mean action: 2.517 [0.000, 13.000],  loss: 0.021773, mae: 0.359647, mean_q: 0.592199, mean_eps: 0.000000
 1723/5000: episode: 76, duration: 0.328s, episode steps:  22, steps per second:  67, episode reward: 37.508, mean reward:  1.705 [-3.000, 33.000], mean action: 3.273 [0.000, 13.000],  loss: 0.019768, mae: 0.343684, mean_q: 0.600068, mean_eps: 0.000000
 1735/5000: episode: 77, duration: 0.200s, episode steps:  12, steps per second:  60, episode reward: 41.224, mean reward:  3.435 [-3.000, 33.000], mean action: 4.750 [0.000, 19.000],  loss: 0.018646, mae: 0.344372, mean_q: 0.530478, mean_eps: 0.000000
 1752/5000: episode: 78, duration: 0.246s, episode steps:  17, steps per second:  69, episode reward: 42.000, mean reward:  2.471 [-2.528, 32.220], mean action: 2.765 [0.000, 19.000],  loss: 0.020106, mae: 0.359314, mean_q: 0.506280, mean_eps: 0.000000
 1777/5000: episode: 79, duration: 0.365s, episode steps:  25, steps per second:  69, episode reward: 41.578, mean reward:  1.663 [-2.142, 30.014], mean action: 2.720 [0.000, 16.000],  loss: 0.023253, mae: 0.370655, mean_q: 0.534798, mean_eps: 0.000000
 1809/5000: episode: 80, duration: 0.459s, episode steps:  32, steps per second:  70, episode reward: -32.380, mean reward: -1.012 [-32.949,  3.000], mean action: 7.344 [0.000, 15.000],  loss: 0.019826, mae: 0.370628, mean_q: 0.467210, mean_eps: 0.000000
 1826/5000: episode: 81, duration: 0.257s, episode steps:  17, steps per second:  66, episode reward: 41.230, mean reward:  2.425 [-2.764, 32.894], mean action: 2.118 [0.000, 12.000],  loss: 0.021555, mae: 0.363322, mean_q: 0.458672, mean_eps: 0.000000
 1854/5000: episode: 82, duration: 0.400s, episode steps:  28, steps per second:  70, episode reward: -35.270, mean reward: -1.260 [-32.048,  3.059], mean action: 9.000 [0.000, 20.000],  loss: 0.023697, mae: 0.368222, mean_q: 0.544596, mean_eps: 0.000000
 1880/5000: episode: 83, duration: 0.367s, episode steps:  26, steps per second:  71, episode reward: 33.000, mean reward:  1.269 [-3.000, 32.210], mean action: 8.846 [0.000, 19.000],  loss: 0.020875, mae: 0.350494, mean_q: 0.505815, mean_eps: 0.000000
 1891/5000: episode: 84, duration: 0.172s, episode steps:  11, steps per second:  64, episode reward: 44.175, mean reward:  4.016 [-2.248, 32.345], mean action: 2.182 [0.000, 16.000],  loss: 0.019999, mae: 0.353768, mean_q: 0.496369, mean_eps: 0.000000
 1912/5000: episode: 85, duration: 0.313s, episode steps:  21, steps per second:  67, episode reward: 38.650, mean reward:  1.840 [-2.700, 32.560], mean action: 4.000 [0.000, 16.000],  loss: 0.020152, mae: 0.352208, mean_q: 0.517874, mean_eps: 0.000000
 1946/5000: episode: 86, duration: 0.488s, episode steps:  34, steps per second:  70, episode reward: -33.000, mean reward: -0.971 [-32.020,  2.883], mean action: 9.294 [0.000, 20.000],  loss: 0.023250, mae: 0.364451, mean_q: 0.568301, mean_eps: 0.000000
 1971/5000: episode: 87, duration: 0.368s, episode steps:  25, steps per second:  68, episode reward: 38.222, mean reward:  1.529 [-2.394, 32.077], mean action: 3.960 [0.000, 16.000],  loss: 0.022611, mae: 0.360198, mean_q: 0.586776, mean_eps: 0.000000
 1995/5000: episode: 88, duration: 0.340s, episode steps:  24, steps per second:  71, episode reward: -35.350, mean reward: -1.473 [-31.847,  1.994], mean action: 9.125 [0.000, 18.000],  loss: 0.020178, mae: 0.349958, mean_q: 0.541124, mean_eps: 0.000000
 2019/5000: episode: 89, duration: 0.352s, episode steps:  24, steps per second:  68, episode reward: 32.403, mean reward:  1.350 [-3.000, 31.964], mean action: 3.333 [0.000, 12.000],  loss: 0.018642, mae: 0.350734, mean_q: 0.538145, mean_eps: 0.000000
 2044/5000: episode: 90, duration: 0.361s, episode steps:  25, steps per second:  69, episode reward: 32.141, mean reward:  1.286 [-3.000, 32.060], mean action: 4.240 [0.000, 16.000],  loss: 0.021795, mae: 0.361085, mean_q: 0.534177, mean_eps: 0.000000
 2062/5000: episode: 91, duration: 0.268s, episode steps:  18, steps per second:  67, episode reward: 33.000, mean reward:  1.833 [-2.616, 30.083], mean action: 4.556 [0.000, 20.000],  loss: 0.019616, mae: 0.350853, mean_q: 0.506039, mean_eps: 0.000000
 2080/5000: episode: 92, duration: 0.282s, episode steps:  18, steps per second:  64, episode reward: 41.004, mean reward:  2.278 [-2.442, 32.260], mean action: 4.111 [0.000, 19.000],  loss: 0.022040, mae: 0.361344, mean_q: 0.489239, mean_eps: 0.000000
 2098/5000: episode: 93, duration: 0.279s, episode steps:  18, steps per second:  65, episode reward: 44.335, mean reward:  2.463 [-2.573, 33.000], mean action: 1.944 [1.000, 3.000],  loss: 0.021846, mae: 0.356949, mean_q: 0.516949, mean_eps: 0.000000
 2119/5000: episode: 94, duration: 0.308s, episode steps:  21, steps per second:  68, episode reward: 37.853, mean reward:  1.803 [-2.772, 32.330], mean action: 3.762 [0.000, 16.000],  loss: 0.020399, mae: 0.354905, mean_q: 0.555117, mean_eps: 0.000000
 2147/5000: episode: 95, duration: 0.417s, episode steps:  28, steps per second:  67, episode reward: -32.320, mean reward: -1.154 [-32.110,  2.232], mean action: 5.000 [0.000, 15.000],  loss: 0.022350, mae: 0.355101, mean_q: 0.576230, mean_eps: 0.000000
 2162/5000: episode: 96, duration: 0.224s, episode steps:  15, steps per second:  67, episode reward: 38.821, mean reward:  2.588 [-3.000, 31.851], mean action: 2.933 [0.000, 15.000],  loss: 0.017007, mae: 0.334319, mean_q: 0.514000, mean_eps: 0.000000
 2171/5000: episode: 97, duration: 0.154s, episode steps:   9, steps per second:  58, episode reward: 47.823, mean reward:  5.314 [ 0.183, 33.000], mean action: 3.222 [0.000, 13.000],  loss: 0.027641, mae: 0.380775, mean_q: 0.564311, mean_eps: 0.000000
 2187/5000: episode: 98, duration: 0.240s, episode steps:  16, steps per second:  67, episode reward: 44.754, mean reward:  2.797 [-2.067, 32.384], mean action: 1.000 [0.000, 2.000],  loss: 0.020995, mae: 0.349798, mean_q: 0.551909, mean_eps: 0.000000
 2211/5000: episode: 99, duration: 0.354s, episode steps:  24, steps per second:  68, episode reward: 33.000, mean reward:  1.375 [-2.443, 32.350], mean action: 2.917 [0.000, 12.000],  loss: 0.022102, mae: 0.353677, mean_q: 0.572770, mean_eps: 0.000000
 2228/5000: episode: 100, duration: 0.246s, episode steps:  17, steps per second:  69, episode reward: 35.904, mean reward:  2.112 [-3.000, 32.904], mean action: 5.000 [0.000, 20.000],  loss: 0.020609, mae: 0.348763, mean_q: 0.555759, mean_eps: 0.000000
 2257/5000: episode: 101, duration: 0.411s, episode steps:  29, steps per second:  70, episode reward: 35.025, mean reward:  1.208 [-2.826, 32.249], mean action: 3.931 [0.000, 12.000],  loss: 0.019127, mae: 0.349537, mean_q: 0.492812, mean_eps: 0.000000
 2273/5000: episode: 102, duration: 0.237s, episode steps:  16, steps per second:  68, episode reward: 41.186, mean reward:  2.574 [-2.293, 32.783], mean action: 2.875 [0.000, 9.000],  loss: 0.019368, mae: 0.356120, mean_q: 0.464587, mean_eps: 0.000000
 2298/5000: episode: 103, duration: 0.365s, episode steps:  25, steps per second:  68, episode reward: 35.788, mean reward:  1.432 [-2.444, 32.018], mean action: 4.680 [0.000, 14.000],  loss: 0.016707, mae: 0.341593, mean_q: 0.495856, mean_eps: 0.000000
 2318/5000: episode: 104, duration: 0.284s, episode steps:  20, steps per second:  70, episode reward: 35.402, mean reward:  1.770 [-2.406, 32.420], mean action: 4.000 [0.000, 12.000],  loss: 0.020072, mae: 0.347830, mean_q: 0.528220, mean_eps: 0.000000
 2335/5000: episode: 105, duration: 0.252s, episode steps:  17, steps per second:  67, episode reward: 41.387, mean reward:  2.435 [-2.422, 31.767], mean action: 2.118 [0.000, 16.000],  loss: 0.021394, mae: 0.354200, mean_q: 0.519387, mean_eps: 0.000000
 2358/5000: episode: 106, duration: 0.336s, episode steps:  23, steps per second:  68, episode reward: 36.000, mean reward:  1.565 [-2.396, 32.180], mean action: 6.087 [0.000, 16.000],  loss: 0.022318, mae: 0.360803, mean_q: 0.512450, mean_eps: 0.000000
 2378/5000: episode: 107, duration: 0.296s, episode steps:  20, steps per second:  68, episode reward: 37.222, mean reward:  1.861 [-2.614, 32.090], mean action: 3.450 [0.000, 16.000],  loss: 0.020208, mae: 0.356112, mean_q: 0.559209, mean_eps: 0.000000
 2396/5000: episode: 108, duration: 0.272s, episode steps:  18, steps per second:  66, episode reward: 41.955, mean reward:  2.331 [-3.000, 32.230], mean action: 3.833 [0.000, 16.000],  loss: 0.020216, mae: 0.357272, mean_q: 0.518658, mean_eps: 0.000000
 2419/5000: episode: 109, duration: 0.333s, episode steps:  23, steps per second:  69, episode reward: 33.000, mean reward:  1.435 [-3.000, 29.781], mean action: 3.391 [0.000, 16.000],  loss: 0.019487, mae: 0.348227, mean_q: 0.526372, mean_eps: 0.000000
 2447/5000: episode: 110, duration: 0.411s, episode steps:  28, steps per second:  68, episode reward: 33.000, mean reward:  1.179 [-3.000, 32.120], mean action: 4.321 [0.000, 16.000],  loss: 0.022576, mae: 0.359584, mean_q: 0.543565, mean_eps: 0.000000
 2468/5000: episode: 111, duration: 0.310s, episode steps:  21, steps per second:  68, episode reward: 38.404, mean reward:  1.829 [-3.000, 32.350], mean action: 3.476 [0.000, 16.000],  loss: 0.021151, mae: 0.356440, mean_q: 0.508491, mean_eps: 0.000000
 2492/5000: episode: 112, duration: 0.345s, episode steps:  24, steps per second:  70, episode reward: 38.492, mean reward:  1.604 [-2.311, 31.864], mean action: 2.625 [0.000, 16.000],  loss: 0.021814, mae: 0.356090, mean_q: 0.570970, mean_eps: 0.000000
 2529/5000: episode: 113, duration: 0.767s, episode steps:  37, steps per second:  48, episode reward: 32.612, mean reward:  0.881 [-2.709, 32.033], mean action: 9.622 [0.000, 20.000],  loss: 0.022001, mae: 0.357166, mean_q: 0.579648, mean_eps: 0.000000
 2555/5000: episode: 114, duration: 0.377s, episode steps:  26, steps per second:  69, episode reward: -33.000, mean reward: -1.269 [-32.451,  3.000], mean action: 7.846 [0.000, 19.000],  loss: 0.020765, mae: 0.354954, mean_q: 0.607998, mean_eps: 0.000000
 2576/5000: episode: 115, duration: 0.333s, episode steps:  21, steps per second:  63, episode reward: 38.888, mean reward:  1.852 [-2.322, 32.180], mean action: 2.952 [0.000, 12.000],  loss: 0.021529, mae: 0.365478, mean_q: 0.551871, mean_eps: 0.000000
 2605/5000: episode: 116, duration: 0.418s, episode steps:  29, steps per second:  69, episode reward: -32.080, mean reward: -1.106 [-32.690,  2.460], mean action: 4.586 [0.000, 19.000],  loss: 0.019982, mae: 0.354294, mean_q: 0.558271, mean_eps: 0.000000
 2621/5000: episode: 117, duration: 0.232s, episode steps:  16, steps per second:  69, episode reward: 36.000, mean reward:  2.250 [-2.611, 32.340], mean action: 4.688 [0.000, 16.000],  loss: 0.020010, mae: 0.343692, mean_q: 0.552044, mean_eps: 0.000000
 2645/5000: episode: 118, duration: 0.353s, episode steps:  24, steps per second:  68, episode reward: 39.775, mean reward:  1.657 [-2.047, 31.662], mean action: 4.125 [0.000, 12.000],  loss: 0.019521, mae: 0.344738, mean_q: 0.557421, mean_eps: 0.000000
 2663/5000: episode: 119, duration: 0.269s, episode steps:  18, steps per second:  67, episode reward: 38.361, mean reward:  2.131 [-2.369, 32.910], mean action: 4.944 [0.000, 19.000],  loss: 0.019594, mae: 0.355512, mean_q: 0.520031, mean_eps: 0.000000
 2682/5000: episode: 120, duration: 0.274s, episode steps:  19, steps per second:  69, episode reward: 35.383, mean reward:  1.862 [-2.540, 32.440], mean action: 5.105 [0.000, 16.000],  loss: 0.018956, mae: 0.343455, mean_q: 0.511946, mean_eps: 0.000000
 2695/5000: episode: 121, duration: 0.201s, episode steps:  13, steps per second:  65, episode reward: 43.996, mean reward:  3.384 [-2.511, 33.000], mean action: 4.077 [0.000, 19.000],  loss: 0.028633, mae: 0.391195, mean_q: 0.551320, mean_eps: 0.000000
 2725/5000: episode: 122, duration: 0.425s, episode steps:  30, steps per second:  71, episode reward: 35.799, mean reward:  1.193 [-2.160, 32.120], mean action: 4.233 [0.000, 19.000],  loss: 0.023491, mae: 0.369882, mean_q: 0.578448, mean_eps: 0.000000
 2753/5000: episode: 123, duration: 0.400s, episode steps:  28, steps per second:  70, episode reward: -32.170, mean reward: -1.149 [-32.048,  3.000], mean action: 4.571 [0.000, 15.000],  loss: 0.020499, mae: 0.363725, mean_q: 0.546637, mean_eps: 0.000000
 2779/5000: episode: 124, duration: 0.376s, episode steps:  26, steps per second:  69, episode reward: 32.737, mean reward:  1.259 [-2.528, 32.180], mean action: 5.846 [0.000, 19.000],  loss: 0.021140, mae: 0.359889, mean_q: 0.547373, mean_eps: 0.000000
 2800/5000: episode: 125, duration: 0.311s, episode steps:  21, steps per second:  68, episode reward: 33.000, mean reward:  1.571 [-3.000, 29.474], mean action: 7.476 [1.000, 19.000],  loss: 0.021923, mae: 0.370353, mean_q: 0.538092, mean_eps: 0.000000
 2829/5000: episode: 126, duration: 0.413s, episode steps:  29, steps per second:  70, episode reward: -32.030, mean reward: -1.104 [-32.227,  2.410], mean action: 4.586 [0.000, 19.000],  loss: 0.020433, mae: 0.372450, mean_q: 0.538895, mean_eps: 0.000000
 2851/5000: episode: 127, duration: 0.321s, episode steps:  22, steps per second:  69, episode reward: 35.800, mean reward:  1.627 [-2.182, 32.020], mean action: 4.409 [0.000, 19.000],  loss: 0.021483, mae: 0.388369, mean_q: 0.516009, mean_eps: 0.000000
 2861/5000: episode: 128, duration: 0.173s, episode steps:  10, steps per second:  58, episode reward: 47.135, mean reward:  4.714 [-0.153, 32.210], mean action: 3.800 [0.000, 13.000],  loss: 0.021360, mae: 0.382284, mean_q: 0.518936, mean_eps: 0.000000
 2889/5000: episode: 129, duration: 0.415s, episode steps:  28, steps per second:  67, episode reward: 41.154, mean reward:  1.470 [-2.350, 33.000], mean action: 3.000 [0.000, 16.000],  loss: 0.018727, mae: 0.360734, mean_q: 0.524760, mean_eps: 0.000000
 2916/5000: episode: 130, duration: 0.384s, episode steps:  27, steps per second:  70, episode reward: 32.286, mean reward:  1.196 [-2.652, 32.150], mean action: 8.037 [0.000, 19.000],  loss: 0.018623, mae: 0.365570, mean_q: 0.551715, mean_eps: 0.000000
 2936/5000: episode: 131, duration: 0.306s, episode steps:  20, steps per second:  65, episode reward: -35.660, mean reward: -1.783 [-31.920,  2.510], mean action: 5.500 [0.000, 16.000],  loss: 0.025361, mae: 0.400200, mean_q: 0.484814, mean_eps: 0.000000
 2956/5000: episode: 132, duration: 0.298s, episode steps:  20, steps per second:  67, episode reward: 37.631, mean reward:  1.882 [-2.251, 32.901], mean action: 4.600 [0.000, 16.000],  loss: 0.021093, mae: 0.376961, mean_q: 0.552071, mean_eps: 0.000000
 2977/5000: episode: 133, duration: 0.306s, episode steps:  21, steps per second:  69, episode reward: 38.855, mean reward:  1.850 [-2.902, 31.946], mean action: 4.190 [0.000, 14.000],  loss: 0.019804, mae: 0.366793, mean_q: 0.577802, mean_eps: 0.000000
 2999/5000: episode: 134, duration: 0.346s, episode steps:  22, steps per second:  64, episode reward: 38.087, mean reward:  1.731 [-2.630, 32.400], mean action: 2.727 [0.000, 12.000],  loss: 0.019941, mae: 0.353127, mean_q: 0.556162, mean_eps: 0.000000
 3014/5000: episode: 135, duration: 0.228s, episode steps:  15, steps per second:  66, episode reward: 38.210, mean reward:  2.547 [-2.464, 31.999], mean action: 2.933 [0.000, 12.000],  loss: 0.017135, mae: 0.344751, mean_q: 0.570707, mean_eps: 0.000000
 3048/5000: episode: 136, duration: 0.475s, episode steps:  34, steps per second:  72, episode reward: 38.836, mean reward:  1.142 [-2.174, 32.250], mean action: 2.559 [0.000, 12.000],  loss: 0.019568, mae: 0.357300, mean_q: 0.516886, mean_eps: 0.000000
 3072/5000: episode: 137, duration: 0.353s, episode steps:  24, steps per second:  68, episode reward: 37.334, mean reward:  1.556 [-2.913, 32.053], mean action: 2.000 [0.000, 9.000],  loss: 0.026442, mae: 0.393496, mean_q: 0.560979, mean_eps: 0.000000
 3098/5000: episode: 138, duration: 0.369s, episode steps:  26, steps per second:  71, episode reward: 30.000, mean reward:  1.154 [-2.517, 29.941], mean action: 3.500 [0.000, 12.000],  loss: 0.016482, mae: 0.341712, mean_q: 0.556869, mean_eps: 0.000000
 3127/5000: episode: 139, duration: 0.476s, episode steps:  29, steps per second:  61, episode reward: 35.891, mean reward:  1.238 [-2.183, 32.064], mean action: 3.379 [0.000, 12.000],  loss: 0.024377, mae: 0.373103, mean_q: 0.577838, mean_eps: 0.000000
 3150/5000: episode: 140, duration: 0.339s, episode steps:  23, steps per second:  68, episode reward: 35.594, mean reward:  1.548 [-2.588, 32.544], mean action: 3.087 [0.000, 12.000],  loss: 0.022769, mae: 0.359293, mean_q: 0.643975, mean_eps: 0.000000
 3177/5000: episode: 141, duration: 0.390s, episode steps:  27, steps per second:  69, episode reward: 38.647, mean reward:  1.431 [-2.433, 32.029], mean action: 1.630 [0.000, 12.000],  loss: 0.020127, mae: 0.346904, mean_q: 0.601553, mean_eps: 0.000000
 3200/5000: episode: 142, duration: 0.329s, episode steps:  23, steps per second:  70, episode reward: 35.033, mean reward:  1.523 [-3.000, 32.083], mean action: 3.130 [0.000, 16.000],  loss: 0.021438, mae: 0.359860, mean_q: 0.562585, mean_eps: 0.000000
 3218/5000: episode: 143, duration: 0.268s, episode steps:  18, steps per second:  67, episode reward: 37.732, mean reward:  2.096 [-3.000, 32.191], mean action: 2.667 [0.000, 12.000],  loss: 0.023496, mae: 0.371505, mean_q: 0.524240, mean_eps: 0.000000
 3253/5000: episode: 144, duration: 4.290s, episode steps:  35, steps per second:   8, episode reward: 32.493, mean reward:  0.928 [-3.000, 32.095], mean action: 5.171 [0.000, 16.000],  loss: 0.020588, mae: 0.366065, mean_q: 0.546060, mean_eps: 0.000000
 3277/5000: episode: 145, duration: 0.350s, episode steps:  24, steps per second:  68, episode reward: 35.042, mean reward:  1.460 [-2.298, 32.143], mean action: 1.875 [0.000, 9.000],  loss: 0.022468, mae: 0.367946, mean_q: 0.524866, mean_eps: 0.000000
 3293/5000: episode: 146, duration: 0.301s, episode steps:  16, steps per second:  53, episode reward: 39.000, mean reward:  2.438 [-2.189, 32.170], mean action: 2.250 [0.000, 3.000],  loss: 0.022149, mae: 0.365930, mean_q: 0.531046, mean_eps: 0.000000
 3319/5000: episode: 147, duration: 0.427s, episode steps:  26, steps per second:  61, episode reward: -32.730, mean reward: -1.259 [-32.128,  2.480], mean action: 4.231 [0.000, 16.000],  loss: 0.018028, mae: 0.346945, mean_q: 0.516104, mean_eps: 0.000000
 3333/5000: episode: 148, duration: 0.238s, episode steps:  14, steps per second:  59, episode reward: 44.140, mean reward:  3.153 [-2.230, 33.000], mean action: 3.857 [0.000, 15.000],  loss: 0.020030, mae: 0.347415, mean_q: 0.516328, mean_eps: 0.000000
 3356/5000: episode: 149, duration: 0.351s, episode steps:  23, steps per second:  66, episode reward: 44.612, mean reward:  1.940 [-2.032, 32.191], mean action: 0.348 [0.000, 2.000],  loss: 0.021775, mae: 0.355328, mean_q: 0.552735, mean_eps: 0.000000
 3381/5000: episode: 150, duration: 0.383s, episode steps:  25, steps per second:  65, episode reward: 30.000, mean reward:  1.200 [-2.668, 30.806], mean action: 3.200 [0.000, 16.000],  loss: 0.020995, mae: 0.357370, mean_q: 0.585688, mean_eps: 0.000000
 3401/5000: episode: 151, duration: 0.299s, episode steps:  20, steps per second:  67, episode reward: -32.860, mean reward: -1.643 [-32.232,  2.790], mean action: 4.850 [0.000, 16.000],  loss: 0.015004, mae: 0.332589, mean_q: 0.533608, mean_eps: 0.000000
 3450/5000: episode: 152, duration: 0.675s, episode steps:  49, steps per second:  73, episode reward: -35.620, mean reward: -0.727 [-31.764,  2.520], mean action: 2.122 [0.000, 16.000],  loss: 0.021517, mae: 0.359801, mean_q: 0.577490, mean_eps: 0.000000
 3470/5000: episode: 153, duration: 0.313s, episode steps:  20, steps per second:  64, episode reward: 38.644, mean reward:  1.932 [-2.631, 32.080], mean action: 5.600 [0.000, 20.000],  loss: 0.024564, mae: 0.374632, mean_q: 0.604576, mean_eps: 0.000000
 3497/5000: episode: 154, duration: 0.396s, episode steps:  27, steps per second:  68, episode reward: 41.192, mean reward:  1.526 [-2.424, 32.556], mean action: 3.889 [0.000, 19.000],  loss: 0.017411, mae: 0.343264, mean_q: 0.557221, mean_eps: 0.000000
 3520/5000: episode: 155, duration: 0.338s, episode steps:  23, steps per second:  68, episode reward: 38.009, mean reward:  1.653 [-2.383, 32.040], mean action: 4.609 [0.000, 16.000],  loss: 0.022990, mae: 0.368442, mean_q: 0.544302, mean_eps: 0.000000
 3554/5000: episode: 156, duration: 0.479s, episode steps:  34, steps per second:  71, episode reward: -32.160, mean reward: -0.946 [-31.904,  2.342], mean action: 7.294 [0.000, 19.000],  loss: 0.020043, mae: 0.349310, mean_q: 0.549911, mean_eps: 0.000000
 3595/5000: episode: 157, duration: 0.576s, episode steps:  41, steps per second:  71, episode reward: 46.366, mean reward:  1.131 [-0.517, 33.000], mean action: 4.244 [0.000, 19.000],  loss: 0.021341, mae: 0.354776, mean_q: 0.527421, mean_eps: 0.000000
 3618/5000: episode: 158, duration: 0.337s, episode steps:  23, steps per second:  68, episode reward: 38.178, mean reward:  1.660 [-2.226, 32.340], mean action: 4.435 [0.000, 19.000],  loss: 0.021924, mae: 0.360030, mean_q: 0.539656, mean_eps: 0.000000
 3632/5000: episode: 159, duration: 0.226s, episode steps:  14, steps per second:  62, episode reward: 41.420, mean reward:  2.959 [-2.781, 32.499], mean action: 3.071 [0.000, 19.000],  loss: 0.023747, mae: 0.358789, mean_q: 0.502841, mean_eps: 0.000000
 3654/5000: episode: 160, duration: 0.330s, episode steps:  22, steps per second:  67, episode reward: 35.233, mean reward:  1.601 [-2.876, 32.123], mean action: 5.182 [0.000, 19.000],  loss: 0.018094, mae: 0.329675, mean_q: 0.526573, mean_eps: 0.000000
 3674/5000: episode: 161, duration: 0.298s, episode steps:  20, steps per second:  67, episode reward: 38.203, mean reward:  1.910 [-2.211, 31.939], mean action: 5.350 [0.000, 19.000],  loss: 0.021764, mae: 0.347019, mean_q: 0.583209, mean_eps: 0.000000
 3692/5000: episode: 162, duration: 0.269s, episode steps:  18, steps per second:  67, episode reward: 41.691, mean reward:  2.316 [-2.311, 32.601], mean action: 1.889 [0.000, 3.000],  loss: 0.023277, mae: 0.355024, mean_q: 0.520464, mean_eps: 0.000000
 3713/5000: episode: 163, duration: 0.300s, episode steps:  21, steps per second:  70, episode reward: -35.170, mean reward: -1.675 [-32.043,  2.940], mean action: 5.810 [0.000, 20.000],  loss: 0.021514, mae: 0.349695, mean_q: 0.494498, mean_eps: 0.000000
 3739/5000: episode: 164, duration: 0.368s, episode steps:  26, steps per second:  71, episode reward: 32.578, mean reward:  1.253 [-3.000, 32.050], mean action: 4.731 [0.000, 12.000],  loss: 0.024067, mae: 0.363549, mean_q: 0.514187, mean_eps: 0.000000
 3752/5000: episode: 165, duration: 0.200s, episode steps:  13, steps per second:  65, episode reward: 39.000, mean reward:  3.000 [-2.265, 30.993], mean action: 1.231 [0.000, 3.000],  loss: 0.024622, mae: 0.356478, mean_q: 0.484845, mean_eps: 0.000000
 3781/5000: episode: 166, duration: 0.413s, episode steps:  29, steps per second:  70, episode reward: -32.370, mean reward: -1.116 [-32.228,  2.560], mean action: 5.862 [0.000, 15.000],  loss: 0.019245, mae: 0.334823, mean_q: 0.485707, mean_eps: 0.000000
 3807/5000: episode: 167, duration: 0.371s, episode steps:  26, steps per second:  70, episode reward: 33.000, mean reward:  1.269 [-3.000, 32.140], mean action: 5.269 [2.000, 15.000],  loss: 0.018262, mae: 0.328315, mean_q: 0.523529, mean_eps: 0.000000
 3850/5000: episode: 168, duration: 0.597s, episode steps:  43, steps per second:  72, episode reward: -32.520, mean reward: -0.756 [-32.127,  3.000], mean action: 6.233 [0.000, 20.000],  loss: 0.020526, mae: 0.336109, mean_q: 0.604472, mean_eps: 0.000000
 3878/5000: episode: 169, duration: 0.397s, episode steps:  28, steps per second:  70, episode reward: 35.127, mean reward:  1.255 [-2.486, 32.310], mean action: 5.286 [0.000, 16.000],  loss: 0.020738, mae: 0.339528, mean_q: 0.511677, mean_eps: 0.000000
 3895/5000: episode: 170, duration: 0.254s, episode steps:  17, steps per second:  67, episode reward: 36.000, mean reward:  2.118 [-2.446, 32.360], mean action: 5.000 [0.000, 16.000],  loss: 0.019114, mae: 0.326794, mean_q: 0.524852, mean_eps: 0.000000
 3918/5000: episode: 171, duration: 0.334s, episode steps:  23, steps per second:  69, episode reward: 35.498, mean reward:  1.543 [-2.276, 32.640], mean action: 4.130 [0.000, 16.000],  loss: 0.019826, mae: 0.336390, mean_q: 0.507074, mean_eps: 0.000000
 3944/5000: episode: 172, duration: 0.374s, episode steps:  26, steps per second:  70, episode reward: -34.790, mean reward: -1.338 [-32.181,  2.430], mean action: 5.962 [0.000, 19.000],  loss: 0.017868, mae: 0.317957, mean_q: 0.525852, mean_eps: 0.000000
 3966/5000: episode: 173, duration: 0.327s, episode steps:  22, steps per second:  67, episode reward: 41.503, mean reward:  1.886 [-2.260, 32.100], mean action: 3.727 [0.000, 14.000],  loss: 0.024310, mae: 0.353819, mean_q: 0.590636, mean_eps: 0.000000
 3982/5000: episode: 174, duration: 0.233s, episode steps:  16, steps per second:  69, episode reward: 35.736, mean reward:  2.233 [-3.000, 32.853], mean action: 5.438 [0.000, 19.000],  loss: 0.022405, mae: 0.350205, mean_q: 0.576529, mean_eps: 0.000000
 4001/5000: episode: 175, duration: 0.274s, episode steps:  19, steps per second:  69, episode reward: -38.080, mean reward: -2.004 [-30.171,  2.077], mean action: 4.421 [0.000, 19.000],  loss: 0.021323, mae: 0.348760, mean_q: 0.574673, mean_eps: 0.000000
 4021/5000: episode: 176, duration: 0.295s, episode steps:  20, steps per second:  68, episode reward: 35.901, mean reward:  1.795 [-2.347, 32.391], mean action: 3.850 [0.000, 16.000],  loss: 0.020431, mae: 0.341226, mean_q: 0.617347, mean_eps: 0.000000
 4043/5000: episode: 177, duration: 0.322s, episode steps:  22, steps per second:  68, episode reward: -32.680, mean reward: -1.485 [-32.223,  3.000], mean action: 6.636 [1.000, 19.000],  loss: 0.021832, mae: 0.349808, mean_q: 0.556748, mean_eps: 0.000000
 4068/5000: episode: 178, duration: 0.363s, episode steps:  25, steps per second:  69, episode reward: 32.070, mean reward:  1.283 [-2.252, 31.660], mean action: 4.320 [0.000, 19.000],  loss: 0.020530, mae: 0.337742, mean_q: 0.610568, mean_eps: 0.000000
 4093/5000: episode: 179, duration: 0.365s, episode steps:  25, steps per second:  69, episode reward: -32.540, mean reward: -1.302 [-32.153,  2.663], mean action: 7.160 [0.000, 19.000],  loss: 0.018645, mae: 0.328202, mean_q: 0.589755, mean_eps: 0.000000
 4120/5000: episode: 180, duration: 0.390s, episode steps:  27, steps per second:  69, episode reward: -32.610, mean reward: -1.208 [-32.610,  2.590], mean action: 4.185 [0.000, 19.000],  loss: 0.020242, mae: 0.339170, mean_q: 0.574840, mean_eps: 0.000000
 4135/5000: episode: 181, duration: 0.228s, episode steps:  15, steps per second:  66, episode reward: 38.340, mean reward:  2.556 [-2.964, 33.000], mean action: 5.267 [0.000, 15.000],  loss: 0.019879, mae: 0.336956, mean_q: 0.542858, mean_eps: 0.000000
 4159/5000: episode: 182, duration: 0.340s, episode steps:  24, steps per second:  71, episode reward: 32.173, mean reward:  1.341 [-3.000, 32.076], mean action: 4.875 [0.000, 15.000],  loss: 0.025953, mae: 0.373775, mean_q: 0.532699, mean_eps: 0.000000
 4185/5000: episode: 183, duration: 0.373s, episode steps:  26, steps per second:  70, episode reward: 32.796, mean reward:  1.261 [-3.000, 33.000], mean action: 4.962 [1.000, 15.000],  loss: 0.023400, mae: 0.370878, mean_q: 0.548211, mean_eps: 0.000000
 4209/5000: episode: 184, duration: 0.534s, episode steps:  24, steps per second:  45, episode reward: 32.154, mean reward:  1.340 [-3.000, 32.064], mean action: 5.375 [0.000, 15.000],  loss: 0.022802, mae: 0.367344, mean_q: 0.560418, mean_eps: 0.000000
 4231/5000: episode: 185, duration: 0.376s, episode steps:  22, steps per second:  59, episode reward: -32.130, mean reward: -1.460 [-32.250,  2.340], mean action: 3.909 [0.000, 15.000],  loss: 0.021106, mae: 0.346728, mean_q: 0.558322, mean_eps: 0.000000
 4260/5000: episode: 186, duration: 0.412s, episode steps:  29, steps per second:  70, episode reward: -32.620, mean reward: -1.125 [-31.928,  2.688], mean action: 2.724 [0.000, 16.000],  loss: 0.020786, mae: 0.344723, mean_q: 0.557964, mean_eps: 0.000000
 4279/5000: episode: 187, duration: 0.286s, episode steps:  19, steps per second:  66, episode reward: 35.161, mean reward:  1.851 [-3.000, 32.300], mean action: 4.947 [0.000, 16.000],  loss: 0.019826, mae: 0.343678, mean_q: 0.542739, mean_eps: 0.000000
 4299/5000: episode: 188, duration: 0.302s, episode steps:  20, steps per second:  66, episode reward: 35.569, mean reward:  1.778 [-2.801, 32.280], mean action: 4.500 [0.000, 16.000],  loss: 0.019451, mae: 0.343426, mean_q: 0.554939, mean_eps: 0.000000
 4321/5000: episode: 189, duration: 0.320s, episode steps:  22, steps per second:  69, episode reward: 35.381, mean reward:  1.608 [-2.590, 33.000], mean action: 5.364 [0.000, 16.000],  loss: 0.021378, mae: 0.350790, mean_q: 0.547157, mean_eps: 0.000000
 4348/5000: episode: 190, duration: 0.383s, episode steps:  27, steps per second:  70, episode reward: 38.347, mean reward:  1.420 [-2.424, 32.400], mean action: 4.926 [0.000, 15.000],  loss: 0.018482, mae: 0.337061, mean_q: 0.505440, mean_eps: 0.000000
 4374/5000: episode: 191, duration: 0.376s, episode steps:  26, steps per second:  69, episode reward: 35.824, mean reward:  1.378 [-2.381, 32.850], mean action: 4.192 [0.000, 15.000],  loss: 0.019542, mae: 0.350866, mean_q: 0.517319, mean_eps: 0.000000
 4398/5000: episode: 192, duration: 0.342s, episode steps:  24, steps per second:  70, episode reward: 34.944, mean reward:  1.456 [-3.000, 32.903], mean action: 5.500 [0.000, 14.000],  loss: 0.014595, mae: 0.320601, mean_q: 0.502745, mean_eps: 0.000000
 4417/5000: episode: 193, duration: 0.280s, episode steps:  19, steps per second:  68, episode reward: 37.141, mean reward:  1.955 [-2.500, 32.340], mean action: 4.158 [0.000, 12.000],  loss: 0.020171, mae: 0.341509, mean_q: 0.476746, mean_eps: 0.000000
 4435/5000: episode: 194, duration: 0.273s, episode steps:  18, steps per second:  66, episode reward: 37.917, mean reward:  2.106 [-2.411, 32.052], mean action: 5.556 [0.000, 15.000],  loss: 0.019195, mae: 0.341479, mean_q: 0.492717, mean_eps: 0.000000
 4461/5000: episode: 195, duration: 0.379s, episode steps:  26, steps per second:  69, episode reward: -33.000, mean reward: -1.269 [-32.226,  2.490], mean action: 5.808 [0.000, 15.000],  loss: 0.019326, mae: 0.335022, mean_q: 0.471071, mean_eps: 0.000000
 4484/5000: episode: 196, duration: 0.374s, episode steps:  23, steps per second:  62, episode reward: 32.795, mean reward:  1.426 [-3.000, 32.380], mean action: 5.870 [0.000, 20.000],  loss: 0.020758, mae: 0.334867, mean_q: 0.490829, mean_eps: 0.000000
 4504/5000: episode: 197, duration: 0.297s, episode steps:  20, steps per second:  67, episode reward: 35.712, mean reward:  1.786 [-2.401, 32.851], mean action: 5.600 [0.000, 16.000],  loss: 0.018950, mae: 0.325437, mean_q: 0.565841, mean_eps: 0.000000
 4524/5000: episode: 198, duration: 0.295s, episode steps:  20, steps per second:  68, episode reward: 35.916, mean reward:  1.796 [-3.000, 31.936], mean action: 4.350 [0.000, 19.000],  loss: 0.023734, mae: 0.352544, mean_q: 0.507063, mean_eps: 0.000000
 4555/5000: episode: 199, duration: 0.444s, episode steps:  31, steps per second:  70, episode reward: -32.530, mean reward: -1.049 [-32.423,  2.580], mean action: 6.806 [0.000, 16.000],  loss: 0.020771, mae: 0.345783, mean_q: 0.575870, mean_eps: 0.000000
 4588/5000: episode: 200, duration: 0.479s, episode steps:  33, steps per second:  69, episode reward: 32.622, mean reward:  0.989 [-2.783, 32.690], mean action: 5.242 [0.000, 16.000],  loss: 0.025505, mae: 0.370239, mean_q: 0.557483, mean_eps: 0.000000
 4609/5000: episode: 201, duration: 0.311s, episode steps:  21, steps per second:  68, episode reward: -35.430, mean reward: -1.687 [-32.336,  2.380], mean action: 6.095 [0.000, 19.000],  loss: 0.025501, mae: 0.368709, mean_q: 0.634683, mean_eps: 0.000000
 4633/5000: episode: 202, duration: 0.348s, episode steps:  24, steps per second:  69, episode reward: 36.000, mean reward:  1.500 [-2.371, 32.430], mean action: 4.417 [0.000, 15.000],  loss: 0.018664, mae: 0.343233, mean_q: 0.562698, mean_eps: 0.000000
 4657/5000: episode: 203, duration: 0.346s, episode steps:  24, steps per second:  69, episode reward: 35.903, mean reward:  1.496 [-2.312, 32.013], mean action: 6.500 [0.000, 15.000],  loss: 0.019474, mae: 0.345893, mean_q: 0.597817, mean_eps: 0.000000
 4679/5000: episode: 204, duration: 0.370s, episode steps:  22, steps per second:  59, episode reward: 41.497, mean reward:  1.886 [-2.309, 32.896], mean action: 4.727 [0.000, 14.000],  loss: 0.021262, mae: 0.352413, mean_q: 0.629236, mean_eps: 0.000000
 4698/5000: episode: 205, duration: 0.284s, episode steps:  19, steps per second:  67, episode reward: 38.312, mean reward:  2.016 [-2.534, 32.321], mean action: 2.947 [0.000, 13.000],  loss: 0.021706, mae: 0.360190, mean_q: 0.558557, mean_eps: 0.000000
 4716/5000: episode: 206, duration: 0.265s, episode steps:  18, steps per second:  68, episode reward: 36.000, mean reward:  2.000 [-2.877, 32.260], mean action: 4.000 [0.000, 14.000],  loss: 0.017916, mae: 0.337472, mean_q: 0.561661, mean_eps: 0.000000
 4744/5000: episode: 207, duration: 0.407s, episode steps:  28, steps per second:  69, episode reward: 32.194, mean reward:  1.150 [-3.000, 32.460], mean action: 3.286 [0.000, 14.000],  loss: 0.021771, mae: 0.353067, mean_q: 0.569392, mean_eps: 0.000000
 4770/5000: episode: 208, duration: 0.397s, episode steps:  26, steps per second:  65, episode reward: 32.867, mean reward:  1.264 [-2.708, 32.267], mean action: 2.962 [0.000, 9.000],  loss: 0.020619, mae: 0.351373, mean_q: 0.571026, mean_eps: 0.000000
 4799/5000: episode: 209, duration: 0.526s, episode steps:  29, steps per second:  55, episode reward: 32.903, mean reward:  1.135 [-3.000, 32.363], mean action: 2.828 [0.000, 16.000],  loss: 0.023016, mae: 0.357705, mean_q: 0.498465, mean_eps: 0.000000
 4829/5000: episode: 210, duration: 0.462s, episode steps:  30, steps per second:  65, episode reward: -35.250, mean reward: -1.175 [-32.367,  3.000], mean action: 2.167 [0.000, 9.000],  loss: 0.019052, mae: 0.351184, mean_q: 0.515169, mean_eps: 0.000000
 4844/5000: episode: 211, duration: 0.229s, episode steps:  15, steps per second:  66, episode reward: 41.616, mean reward:  2.774 [-2.423, 32.904], mean action: 1.333 [0.000, 3.000],  loss: 0.025389, mae: 0.375340, mean_q: 0.494250, mean_eps: 0.000000
 4867/5000: episode: 212, duration: 0.340s, episode steps:  23, steps per second:  68, episode reward: 38.704, mean reward:  1.683 [-2.248, 32.111], mean action: 1.739 [0.000, 11.000],  loss: 0.020519, mae: 0.338830, mean_q: 0.564939, mean_eps: 0.000000
 4895/5000: episode: 213, duration: 0.390s, episode steps:  28, steps per second:  72, episode reward: -35.130, mean reward: -1.255 [-32.268,  2.738], mean action: 5.214 [0.000, 14.000],  loss: 0.018225, mae: 0.330390, mean_q: 0.575783, mean_eps: 0.000000
 4913/5000: episode: 214, duration: 0.269s, episode steps:  18, steps per second:  67, episode reward: 41.016, mean reward:  2.279 [-2.235, 32.910], mean action: 4.556 [0.000, 14.000],  loss: 0.021770, mae: 0.352528, mean_q: 0.534736, mean_eps: 0.000000
 4952/5000: episode: 215, duration: 0.552s, episode steps:  39, steps per second:  71, episode reward: -32.700, mean reward: -0.838 [-32.240,  2.210], mean action: 5.718 [0.000, 15.000],  loss: 0.019478, mae: 0.350923, mean_q: 0.529016, mean_eps: 0.000000
 4989/5000: episode: 216, duration: 0.520s, episode steps:  37, steps per second:  71, episode reward: 38.506, mean reward:  1.041 [-2.283, 32.143], mean action: 3.432 [0.000, 13.000],  loss: 0.019662, mae: 0.348280, mean_q: 0.541572, mean_eps: 0.000000
done, took 70.341 seconds
DQN Evaluation: 8868 victories out of 10382 episodes
Training for 5000 steps ...
   17/5000: episode: 1, duration: 0.140s, episode steps:  17, steps per second: 121, episode reward: 44.259, mean reward:  2.603 [-2.752, 32.090], mean action: 1.941 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   39/5000: episode: 2, duration: 0.163s, episode steps:  22, steps per second: 135, episode reward: 43.076, mean reward:  1.958 [-2.155, 32.120], mean action: 3.864 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   69/5000: episode: 3, duration: 0.206s, episode steps:  30, steps per second: 146, episode reward: 44.512, mean reward:  1.484 [-2.675, 32.050], mean action: 2.433 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   90/5000: episode: 4, duration: 0.198s, episode steps:  21, steps per second: 106, episode reward: 43.132, mean reward:  2.054 [-2.972, 32.330], mean action: 3.333 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  113/5000: episode: 5, duration: 0.168s, episode steps:  23, steps per second: 137, episode reward: 41.501, mean reward:  1.804 [-2.105, 31.780], mean action: 3.087 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  162/5000: episode: 6, duration: 0.309s, episode steps:  49, steps per second: 158, episode reward: 32.951, mean reward:  0.672 [-2.634, 32.201], mean action: 10.163 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  227/5000: episode: 7, duration: 0.396s, episode steps:  65, steps per second: 164, episode reward: -32.940, mean reward: -0.507 [-32.128,  2.362], mean action: 8.769 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  263/5000: episode: 8, duration: 0.234s, episode steps:  36, steps per second: 154, episode reward: 41.087, mean reward:  1.141 [-2.201, 32.290], mean action: 2.222 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  285/5000: episode: 9, duration: 0.147s, episode steps:  22, steps per second: 149, episode reward: 35.495, mean reward:  1.613 [-2.392, 31.595], mean action: 3.500 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  314/5000: episode: 10, duration: 0.197s, episode steps:  29, steps per second: 147, episode reward: 46.810, mean reward:  1.614 [-0.845, 32.390], mean action: 2.069 [0.000, 8.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  342/5000: episode: 11, duration: 0.189s, episode steps:  28, steps per second: 148, episode reward: 37.662, mean reward:  1.345 [-2.781, 32.400], mean action: 3.071 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  356/5000: episode: 12, duration: 0.110s, episode steps:  14, steps per second: 127, episode reward: 44.286, mean reward:  3.163 [-2.037, 31.951], mean action: 2.786 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  381/5000: episode: 13, duration: 0.181s, episode steps:  25, steps per second: 138, episode reward: 44.390, mean reward:  1.776 [-3.000, 31.954], mean action: 3.360 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  399/5000: episode: 14, duration: 0.127s, episode steps:  18, steps per second: 141, episode reward: 44.559, mean reward:  2.475 [-2.004, 32.040], mean action: 4.500 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  430/5000: episode: 15, duration: 0.199s, episode steps:  31, steps per second: 156, episode reward: 35.210, mean reward:  1.136 [-2.882, 32.120], mean action: 4.097 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  488/5000: episode: 16, duration: 0.352s, episode steps:  58, steps per second: 165, episode reward: 35.617, mean reward:  0.614 [-2.423, 32.040], mean action: 4.517 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  506/5000: episode: 17, duration: 0.130s, episode steps:  18, steps per second: 139, episode reward: 44.177, mean reward:  2.454 [-2.173, 32.180], mean action: 4.167 [1.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  522/5000: episode: 18, duration: 0.124s, episode steps:  16, steps per second: 129, episode reward: 41.813, mean reward:  2.613 [-3.000, 32.030], mean action: 3.125 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  550/5000: episode: 19, duration: 0.248s, episode steps:  28, steps per second: 113, episode reward: 38.878, mean reward:  1.389 [-2.940, 32.760], mean action: 3.821 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  570/5000: episode: 20, duration: 0.148s, episode steps:  20, steps per second: 135, episode reward: 45.000, mean reward:  2.250 [-2.098, 32.780], mean action: 2.300 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  585/5000: episode: 21, duration: 0.123s, episode steps:  15, steps per second: 122, episode reward: 44.343, mean reward:  2.956 [-2.061, 32.449], mean action: 2.733 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  611/5000: episode: 22, duration: 0.176s, episode steps:  26, steps per second: 148, episode reward: 41.527, mean reward:  1.597 [-2.417, 32.260], mean action: 2.423 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  643/5000: episode: 23, duration: 0.219s, episode steps:  32, steps per second: 146, episode reward: 44.034, mean reward:  1.376 [-2.330, 32.293], mean action: 1.719 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  672/5000: episode: 24, duration: 0.185s, episode steps:  29, steps per second: 157, episode reward: -32.020, mean reward: -1.104 [-32.074,  3.000], mean action: 4.069 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  696/5000: episode: 25, duration: 0.156s, episode steps:  24, steps per second: 154, episode reward: 44.952, mean reward:  1.873 [-2.169, 32.140], mean action: 3.125 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  735/5000: episode: 26, duration: 0.247s, episode steps:  39, steps per second: 158, episode reward: 40.141, mean reward:  1.029 [-2.093, 32.140], mean action: 2.564 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  768/5000: episode: 27, duration: 0.215s, episode steps:  33, steps per second: 153, episode reward: 47.717, mean reward:  1.446 [-0.217, 32.050], mean action: 3.242 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  801/5000: episode: 28, duration: 0.218s, episode steps:  33, steps per second: 152, episode reward: 41.162, mean reward:  1.247 [-3.000, 32.410], mean action: 3.000 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  835/5000: episode: 29, duration: 0.225s, episode steps:  34, steps per second: 151, episode reward: 43.500, mean reward:  1.279 [-2.086, 32.360], mean action: 2.618 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  864/5000: episode: 30, duration: 0.189s, episode steps:  29, steps per second: 153, episode reward: 38.392, mean reward:  1.324 [-2.614, 31.971], mean action: 3.621 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  887/5000: episode: 31, duration: 0.158s, episode steps:  23, steps per second: 146, episode reward: 41.245, mean reward:  1.793 [-2.903, 32.013], mean action: 3.522 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  920/5000: episode: 32, duration: 0.211s, episode steps:  33, steps per second: 156, episode reward: -38.910, mean reward: -1.179 [-32.234,  3.000], mean action: 8.000 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  952/5000: episode: 33, duration: 0.202s, episode steps:  32, steps per second: 158, episode reward: 38.341, mean reward:  1.198 [-2.335, 32.400], mean action: 4.188 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  990/5000: episode: 34, duration: 0.245s, episode steps:  38, steps per second: 155, episode reward: 40.753, mean reward:  1.072 [-2.940, 32.130], mean action: 3.632 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1030/5000: episode: 35, duration: 0.490s, episode steps:  40, steps per second:  82, episode reward: 34.924, mean reward:  0.873 [-3.000, 32.220], mean action: 2.900 [0.000, 12.000],  loss: 0.018703, mae: 0.341913, mean_q: 0.564365, mean_eps: 0.000000
 1053/5000: episode: 36, duration: 0.333s, episode steps:  23, steps per second:  69, episode reward: 35.476, mean reward:  1.542 [-2.337, 32.597], mean action: 3.478 [0.000, 16.000],  loss: 0.021939, mae: 0.357633, mean_q: 0.587765, mean_eps: 0.000000
 1073/5000: episode: 37, duration: 0.299s, episode steps:  20, steps per second:  67, episode reward: 41.877, mean reward:  2.094 [-2.200, 32.030], mean action: 2.900 [0.000, 16.000],  loss: 0.020639, mae: 0.347018, mean_q: 0.580718, mean_eps: 0.000000
 1100/5000: episode: 38, duration: 0.423s, episode steps:  27, steps per second:  64, episode reward: 41.759, mean reward:  1.547 [-2.260, 32.140], mean action: 2.556 [0.000, 12.000],  loss: 0.017272, mae: 0.333544, mean_q: 0.525030, mean_eps: 0.000000
 1133/5000: episode: 39, duration: 0.672s, episode steps:  33, steps per second:  49, episode reward: 35.595, mean reward:  1.079 [-3.000, 32.210], mean action: 4.394 [0.000, 15.000],  loss: 0.022400, mae: 0.361915, mean_q: 0.551656, mean_eps: 0.000000
 1166/5000: episode: 40, duration: 0.485s, episode steps:  33, steps per second:  68, episode reward: 41.151, mean reward:  1.247 [-3.000, 32.054], mean action: 3.636 [0.000, 14.000],  loss: 0.019115, mae: 0.343362, mean_q: 0.523768, mean_eps: 0.000000
 1186/5000: episode: 41, duration: 0.294s, episode steps:  20, steps per second:  68, episode reward: 44.302, mean reward:  2.215 [-2.329, 32.270], mean action: 2.750 [0.000, 13.000],  loss: 0.018571, mae: 0.339684, mean_q: 0.528989, mean_eps: 0.000000
 1217/5000: episode: 42, duration: 0.441s, episode steps:  31, steps per second:  70, episode reward: 38.653, mean reward:  1.247 [-2.632, 32.170], mean action: 2.065 [0.000, 11.000],  loss: 0.021158, mae: 0.348896, mean_q: 0.538025, mean_eps: 0.000000
 1239/5000: episode: 43, duration: 0.331s, episode steps:  22, steps per second:  66, episode reward: 44.598, mean reward:  2.027 [-2.048, 32.290], mean action: 1.091 [0.000, 11.000],  loss: 0.019285, mae: 0.340880, mean_q: 0.626902, mean_eps: 0.000000
 1262/5000: episode: 44, duration: 0.340s, episode steps:  23, steps per second:  68, episode reward: 38.407, mean reward:  1.670 [-2.446, 31.858], mean action: 4.130 [0.000, 20.000],  loss: 0.021488, mae: 0.353873, mean_q: 0.589338, mean_eps: 0.000000
 1282/5000: episode: 45, duration: 0.304s, episode steps:  20, steps per second:  66, episode reward: 41.232, mean reward:  2.062 [-3.000, 33.000], mean action: 2.900 [0.000, 19.000],  loss: 0.022605, mae: 0.365731, mean_q: 0.632022, mean_eps: 0.000000
 1315/5000: episode: 46, duration: 0.468s, episode steps:  33, steps per second:  71, episode reward: 38.723, mean reward:  1.173 [-2.091, 31.930], mean action: 3.545 [0.000, 19.000],  loss: 0.021612, mae: 0.360253, mean_q: 0.585754, mean_eps: 0.000000
 1339/5000: episode: 47, duration: 0.366s, episode steps:  24, steps per second:  66, episode reward: 41.303, mean reward:  1.721 [-2.564, 32.428], mean action: 3.000 [1.000, 19.000],  loss: 0.019536, mae: 0.360229, mean_q: 0.558090, mean_eps: 0.000000
 1367/5000: episode: 48, duration: 0.417s, episode steps:  28, steps per second:  67, episode reward: 41.554, mean reward:  1.484 [-2.353, 32.160], mean action: 2.571 [0.000, 14.000],  loss: 0.019242, mae: 0.342068, mean_q: 0.563968, mean_eps: 0.000000
 1394/5000: episode: 49, duration: 0.391s, episode steps:  27, steps per second:  69, episode reward: 38.215, mean reward:  1.415 [-2.077, 32.060], mean action: 2.519 [0.000, 19.000],  loss: 0.021461, mae: 0.352473, mean_q: 0.550972, mean_eps: 0.000000
 1414/5000: episode: 50, duration: 0.299s, episode steps:  20, steps per second:  67, episode reward: 44.218, mean reward:  2.211 [-2.109, 32.120], mean action: 4.250 [1.000, 14.000],  loss: 0.017944, mae: 0.342967, mean_q: 0.554127, mean_eps: 0.000000
 1440/5000: episode: 51, duration: 0.372s, episode steps:  26, steps per second:  70, episode reward: 36.000, mean reward:  1.385 [-3.000, 32.060], mean action: 3.000 [0.000, 15.000],  loss: 0.020938, mae: 0.351411, mean_q: 0.532299, mean_eps: 0.000000
 1470/5000: episode: 52, duration: 0.430s, episode steps:  30, steps per second:  70, episode reward: 42.000, mean reward:  1.400 [-2.579, 32.080], mean action: 3.233 [0.000, 14.000],  loss: 0.018281, mae: 0.340047, mean_q: 0.572439, mean_eps: 0.000000
 1499/5000: episode: 53, duration: 0.430s, episode steps:  29, steps per second:  67, episode reward: 35.308, mean reward:  1.218 [-2.763, 32.138], mean action: 5.759 [2.000, 15.000],  loss: 0.020187, mae: 0.341575, mean_q: 0.525073, mean_eps: 0.000000
 1530/5000: episode: 54, duration: 0.452s, episode steps:  31, steps per second:  69, episode reward: 38.320, mean reward:  1.236 [-3.000, 31.793], mean action: 2.742 [0.000, 15.000],  loss: 0.022843, mae: 0.362422, mean_q: 0.506579, mean_eps: 0.000000
 1556/5000: episode: 55, duration: 0.371s, episode steps:  26, steps per second:  70, episode reward: 33.000, mean reward:  1.269 [-3.000, 32.060], mean action: 5.077 [0.000, 15.000],  loss: 0.024860, mae: 0.370890, mean_q: 0.533983, mean_eps: 0.000000
 1591/5000: episode: 56, duration: 0.526s, episode steps:  35, steps per second:  67, episode reward: 38.440, mean reward:  1.098 [-2.308, 32.040], mean action: 3.629 [0.000, 15.000],  loss: 0.019932, mae: 0.350768, mean_q: 0.532602, mean_eps: 0.000000
 1627/5000: episode: 57, duration: 0.519s, episode steps:  36, steps per second:  69, episode reward: 35.256, mean reward:  0.979 [-2.251, 31.923], mean action: 4.444 [0.000, 16.000],  loss: 0.019547, mae: 0.349924, mean_q: 0.562830, mean_eps: 0.000000
 1673/5000: episode: 58, duration: 0.661s, episode steps:  46, steps per second:  70, episode reward: 47.334, mean reward:  1.029 [-0.084, 32.180], mean action: 4.913 [0.000, 15.000],  loss: 0.021136, mae: 0.355666, mean_q: 0.534796, mean_eps: 0.000000
 1691/5000: episode: 59, duration: 0.269s, episode steps:  18, steps per second:  67, episode reward: 45.000, mean reward:  2.500 [-3.000, 33.000], mean action: 1.611 [0.000, 16.000],  loss: 0.020425, mae: 0.358311, mean_q: 0.554189, mean_eps: 0.000000
 1721/5000: episode: 60, duration: 0.429s, episode steps:  30, steps per second:  70, episode reward: 43.497, mean reward:  1.450 [-2.871, 31.930], mean action: 3.133 [0.000, 9.000],  loss: 0.023227, mae: 0.364397, mean_q: 0.542085, mean_eps: 0.000000
 1753/5000: episode: 61, duration: 0.458s, episode steps:  32, steps per second:  70, episode reward: 44.014, mean reward:  1.375 [-2.141, 32.146], mean action: 4.062 [0.000, 12.000],  loss: 0.020675, mae: 0.348877, mean_q: 0.566941, mean_eps: 0.000000
 1769/5000: episode: 62, duration: 0.247s, episode steps:  16, steps per second:  65, episode reward: 44.329, mean reward:  2.771 [-2.176, 32.081], mean action: 3.312 [0.000, 19.000],  loss: 0.025672, mae: 0.368259, mean_q: 0.544126, mean_eps: 0.000000
 1789/5000: episode: 63, duration: 0.297s, episode steps:  20, steps per second:  67, episode reward: 36.000, mean reward:  1.800 [-3.000, 32.220], mean action: 3.650 [0.000, 16.000],  loss: 0.019465, mae: 0.342139, mean_q: 0.557508, mean_eps: 0.000000
 1825/5000: episode: 64, duration: 0.514s, episode steps:  36, steps per second:  70, episode reward: 37.834, mean reward:  1.051 [-2.790, 32.290], mean action: 3.944 [0.000, 15.000],  loss: 0.022978, mae: 0.354970, mean_q: 0.587357, mean_eps: 0.000000
 1851/5000: episode: 65, duration: 0.378s, episode steps:  26, steps per second:  69, episode reward: 41.287, mean reward:  1.588 [-2.313, 31.942], mean action: 2.923 [0.000, 15.000],  loss: 0.019165, mae: 0.337399, mean_q: 0.614702, mean_eps: 0.000000
 1871/5000: episode: 66, duration: 0.297s, episode steps:  20, steps per second:  67, episode reward: 42.000, mean reward:  2.100 [-3.000, 32.520], mean action: 3.100 [0.000, 15.000],  loss: 0.022503, mae: 0.355078, mean_q: 0.598311, mean_eps: 0.000000
 1896/5000: episode: 67, duration: 0.363s, episode steps:  25, steps per second:  69, episode reward: 38.556, mean reward:  1.542 [-3.000, 32.190], mean action: 4.440 [0.000, 15.000],  loss: 0.023235, mae: 0.366063, mean_q: 0.570633, mean_eps: 0.000000
 1923/5000: episode: 68, duration: 0.388s, episode steps:  27, steps per second:  70, episode reward: 41.674, mean reward:  1.543 [-2.261, 32.312], mean action: 3.000 [1.000, 14.000],  loss: 0.020797, mae: 0.343376, mean_q: 0.576953, mean_eps: 0.000000
 1943/5000: episode: 69, duration: 0.305s, episode steps:  20, steps per second:  66, episode reward: 44.401, mean reward:  2.220 [-2.080, 31.811], mean action: 2.700 [0.000, 16.000],  loss: 0.019441, mae: 0.341695, mean_q: 0.552004, mean_eps: 0.000000
 1961/5000: episode: 70, duration: 0.267s, episode steps:  18, steps per second:  68, episode reward: 44.476, mean reward:  2.471 [-2.952, 32.329], mean action: 3.333 [1.000, 16.000],  loss: 0.022399, mae: 0.350783, mean_q: 0.555606, mean_eps: 0.000000
 1981/5000: episode: 71, duration: 0.288s, episode steps:  20, steps per second:  69, episode reward: 42.000, mean reward:  2.100 [-2.274, 32.200], mean action: 2.650 [0.000, 16.000],  loss: 0.020245, mae: 0.346553, mean_q: 0.484141, mean_eps: 0.000000
 1998/5000: episode: 72, duration: 0.270s, episode steps:  17, steps per second:  63, episode reward: 47.919, mean reward:  2.819 [-0.111, 32.300], mean action: 1.706 [1.000, 3.000],  loss: 0.026974, mae: 0.381363, mean_q: 0.451265, mean_eps: 0.000000
 2021/5000: episode: 73, duration: 0.352s, episode steps:  23, steps per second:  65, episode reward: 38.399, mean reward:  1.670 [-2.713, 29.759], mean action: 2.739 [0.000, 9.000],  loss: 0.023400, mae: 0.363266, mean_q: 0.500435, mean_eps: 0.000000
 2037/5000: episode: 74, duration: 0.246s, episode steps:  16, steps per second:  65, episode reward: 40.871, mean reward:  2.554 [-3.000, 32.080], mean action: 4.062 [0.000, 15.000],  loss: 0.025264, mae: 0.373542, mean_q: 0.494967, mean_eps: 0.000000
 2047/5000: episode: 75, duration: 0.164s, episode steps:  10, steps per second:  61, episode reward: 47.448, mean reward:  4.745 [ 0.151, 32.561], mean action: 1.600 [0.000, 5.000],  loss: 0.023864, mae: 0.365937, mean_q: 0.520645, mean_eps: 0.000000
 2076/5000: episode: 76, duration: 0.423s, episode steps:  29, steps per second:  69, episode reward: 41.176, mean reward:  1.420 [-2.332, 32.220], mean action: 3.310 [0.000, 16.000],  loss: 0.022596, mae: 0.356331, mean_q: 0.530080, mean_eps: 0.000000
 2102/5000: episode: 77, duration: 0.378s, episode steps:  26, steps per second:  69, episode reward: 43.792, mean reward:  1.684 [-2.661, 32.270], mean action: 1.731 [0.000, 12.000],  loss: 0.021520, mae: 0.354261, mean_q: 0.496865, mean_eps: 0.000000
 2164/5000: episode: 78, duration: 0.913s, episode steps:  62, steps per second:  68, episode reward: 40.510, mean reward:  0.653 [-2.589, 32.150], mean action: 5.419 [0.000, 21.000],  loss: 0.018851, mae: 0.344286, mean_q: 0.538416, mean_eps: 0.000000
 2210/5000: episode: 79, duration: 0.680s, episode steps:  46, steps per second:  68, episode reward: 35.305, mean reward:  0.767 [-3.000, 32.074], mean action: 5.087 [0.000, 16.000],  loss: 0.019810, mae: 0.349861, mean_q: 0.552242, mean_eps: 0.000000
 2234/5000: episode: 80, duration: 0.363s, episode steps:  24, steps per second:  66, episode reward: 47.030, mean reward:  1.960 [-0.230, 32.146], mean action: 1.250 [0.000, 3.000],  loss: 0.022373, mae: 0.355945, mean_q: 0.500202, mean_eps: 0.000000
 2267/5000: episode: 81, duration: 0.480s, episode steps:  33, steps per second:  69, episode reward: 38.322, mean reward:  1.161 [-3.000, 32.070], mean action: 7.182 [0.000, 19.000],  loss: 0.020073, mae: 0.343469, mean_q: 0.507501, mean_eps: 0.000000
 2295/5000: episode: 82, duration: 0.419s, episode steps:  28, steps per second:  67, episode reward: 32.802, mean reward:  1.171 [-3.000, 32.161], mean action: 6.036 [0.000, 16.000],  loss: 0.021504, mae: 0.350744, mean_q: 0.528297, mean_eps: 0.000000
 2329/5000: episode: 83, duration: 0.508s, episode steps:  34, steps per second:  67, episode reward: -32.030, mean reward: -0.942 [-32.532,  2.370], mean action: 4.794 [0.000, 19.000],  loss: 0.019047, mae: 0.347649, mean_q: 0.533584, mean_eps: 0.000000
 2342/5000: episode: 84, duration: 0.213s, episode steps:  13, steps per second:  61, episode reward: 41.393, mean reward:  3.184 [-3.000, 32.170], mean action: 3.769 [1.000, 16.000],  loss: 0.018013, mae: 0.344190, mean_q: 0.510854, mean_eps: 0.000000
 2365/5000: episode: 85, duration: 0.372s, episode steps:  23, steps per second:  62, episode reward: 38.398, mean reward:  1.669 [-2.461, 32.037], mean action: 4.435 [0.000, 19.000],  loss: 0.017830, mae: 0.343155, mean_q: 0.501208, mean_eps: 0.000000
 2380/5000: episode: 86, duration: 0.241s, episode steps:  15, steps per second:  62, episode reward: 47.708, mean reward:  3.181 [-0.060, 32.350], mean action: 4.067 [0.000, 14.000],  loss: 0.021212, mae: 0.365696, mean_q: 0.543295, mean_eps: 0.000000
 2396/5000: episode: 87, duration: 0.251s, episode steps:  16, steps per second:  64, episode reward: 45.000, mean reward:  2.812 [-2.252, 33.000], mean action: 2.062 [0.000, 16.000],  loss: 0.019777, mae: 0.342859, mean_q: 0.588819, mean_eps: 0.000000
 2415/5000: episode: 88, duration: 0.300s, episode steps:  19, steps per second:  63, episode reward: 38.122, mean reward:  2.006 [-2.204, 32.260], mean action: 3.105 [0.000, 16.000],  loss: 0.020578, mae: 0.347381, mean_q: 0.572344, mean_eps: 0.000000
 2454/5000: episode: 89, duration: 0.565s, episode steps:  39, steps per second:  69, episode reward: 35.810, mean reward:  0.918 [-2.522, 32.240], mean action: 4.231 [0.000, 16.000],  loss: 0.023235, mae: 0.364037, mean_q: 0.600602, mean_eps: 0.000000
 2489/5000: episode: 90, duration: 0.550s, episode steps:  35, steps per second:  64, episode reward: 41.825, mean reward:  1.195 [-2.070, 32.841], mean action: 3.229 [0.000, 20.000],  loss: 0.020649, mae: 0.350046, mean_q: 0.614495, mean_eps: 0.000000
 2514/5000: episode: 91, duration: 0.379s, episode steps:  25, steps per second:  66, episode reward: 41.293, mean reward:  1.652 [-2.682, 31.809], mean action: 2.640 [0.000, 16.000],  loss: 0.021633, mae: 0.357080, mean_q: 0.602965, mean_eps: 0.000000
 2541/5000: episode: 92, duration: 0.441s, episode steps:  27, steps per second:  61, episode reward: 35.397, mean reward:  1.311 [-3.000, 31.973], mean action: 3.185 [0.000, 16.000],  loss: 0.018531, mae: 0.345370, mean_q: 0.549229, mean_eps: 0.000000
 2574/5000: episode: 93, duration: 0.656s, episode steps:  33, steps per second:  50, episode reward: 45.000, mean reward:  1.364 [-2.043, 32.160], mean action: 1.152 [0.000, 12.000],  loss: 0.023695, mae: 0.355924, mean_q: 0.569615, mean_eps: 0.000000
 2611/5000: episode: 94, duration: 0.546s, episode steps:  37, steps per second:  68, episode reward: 35.288, mean reward:  0.954 [-3.000, 31.740], mean action: 4.568 [0.000, 15.000],  loss: 0.021303, mae: 0.343499, mean_q: 0.546015, mean_eps: 0.000000
 2635/5000: episode: 95, duration: 0.347s, episode steps:  24, steps per second:  69, episode reward: 45.000, mean reward:  1.875 [-2.056, 32.220], mean action: 2.667 [0.000, 15.000],  loss: 0.020344, mae: 0.336951, mean_q: 0.534401, mean_eps: 0.000000
 2667/5000: episode: 96, duration: 0.467s, episode steps:  32, steps per second:  69, episode reward: 41.547, mean reward:  1.298 [-2.108, 32.318], mean action: 2.062 [0.000, 15.000],  loss: 0.020903, mae: 0.341437, mean_q: 0.504117, mean_eps: 0.000000
 2713/5000: episode: 97, duration: 0.636s, episode steps:  46, steps per second:  72, episode reward: -33.000, mean reward: -0.717 [-32.204,  3.000], mean action: 6.522 [0.000, 18.000],  loss: 0.022152, mae: 0.354277, mean_q: 0.570376, mean_eps: 0.000000
 2734/5000: episode: 98, duration: 0.312s, episode steps:  21, steps per second:  67, episode reward: 45.000, mean reward:  2.143 [-2.233, 32.260], mean action: 2.000 [0.000, 3.000],  loss: 0.021003, mae: 0.343819, mean_q: 0.553705, mean_eps: 0.000000
 2760/5000: episode: 99, duration: 0.378s, episode steps:  26, steps per second:  69, episode reward: 41.432, mean reward:  1.594 [-2.314, 32.260], mean action: 3.269 [0.000, 15.000],  loss: 0.024001, mae: 0.368434, mean_q: 0.530709, mean_eps: 0.000000
 2783/5000: episode: 100, duration: 0.344s, episode steps:  23, steps per second:  67, episode reward: 41.803, mean reward:  1.818 [-3.000, 32.300], mean action: 2.435 [0.000, 15.000],  loss: 0.021566, mae: 0.350142, mean_q: 0.554275, mean_eps: 0.000000
 2840/5000: episode: 101, duration: 0.793s, episode steps:  57, steps per second:  72, episode reward: 41.278, mean reward:  0.724 [-2.766, 32.440], mean action: 2.404 [0.000, 20.000],  loss: 0.019999, mae: 0.338657, mean_q: 0.527394, mean_eps: 0.000000
 2860/5000: episode: 102, duration: 0.300s, episode steps:  20, steps per second:  67, episode reward: 45.000, mean reward:  2.250 [-2.599, 32.450], mean action: 1.450 [1.000, 7.000],  loss: 0.025411, mae: 0.367711, mean_q: 0.545730, mean_eps: 0.000000
 2892/5000: episode: 103, duration: 0.478s, episode steps:  32, steps per second:  67, episode reward: 40.685, mean reward:  1.271 [-2.468, 31.441], mean action: 5.469 [0.000, 20.000],  loss: 0.021107, mae: 0.346237, mean_q: 0.573820, mean_eps: 0.000000
 2916/5000: episode: 104, duration: 0.344s, episode steps:  24, steps per second:  70, episode reward: 35.443, mean reward:  1.477 [-3.000, 32.590], mean action: 3.292 [0.000, 11.000],  loss: 0.019188, mae: 0.341409, mean_q: 0.550939, mean_eps: 0.000000
 2959/5000: episode: 105, duration: 0.595s, episode steps:  43, steps per second:  72, episode reward: -32.580, mean reward: -0.758 [-32.196,  2.524], mean action: 4.186 [0.000, 13.000],  loss: 0.022164, mae: 0.359117, mean_q: 0.489984, mean_eps: 0.000000
 2991/5000: episode: 106, duration: 0.447s, episode steps:  32, steps per second:  72, episode reward: 38.763, mean reward:  1.211 [-3.000, 32.160], mean action: 2.875 [0.000, 16.000],  loss: 0.023167, mae: 0.359122, mean_q: 0.472442, mean_eps: 0.000000
 3016/5000: episode: 107, duration: 0.360s, episode steps:  25, steps per second:  70, episode reward: 38.200, mean reward:  1.528 [-2.885, 32.440], mean action: 4.320 [1.000, 16.000],  loss: 0.024171, mae: 0.361186, mean_q: 0.485655, mean_eps: 0.000000
 3048/5000: episode: 108, duration: 0.449s, episode steps:  32, steps per second:  71, episode reward: 36.000, mean reward:  1.125 [-2.340, 32.250], mean action: 2.094 [0.000, 12.000],  loss: 0.022275, mae: 0.360727, mean_q: 0.502573, mean_eps: 0.000000
 3069/5000: episode: 109, duration: 0.321s, episode steps:  21, steps per second:  65, episode reward: 44.556, mean reward:  2.122 [-3.000, 32.760], mean action: 5.190 [0.000, 15.000],  loss: 0.015794, mae: 0.327812, mean_q: 0.515187, mean_eps: 0.000000
 3095/5000: episode: 110, duration: 0.383s, episode steps:  26, steps per second:  68, episode reward: 44.317, mean reward:  1.704 [-2.500, 32.140], mean action: 3.577 [1.000, 14.000],  loss: 0.021597, mae: 0.352959, mean_q: 0.503085, mean_eps: 0.000000
 3115/5000: episode: 111, duration: 0.293s, episode steps:  20, steps per second:  68, episode reward: 41.157, mean reward:  2.058 [-2.457, 32.544], mean action: 4.750 [0.000, 20.000],  loss: 0.023902, mae: 0.360313, mean_q: 0.543196, mean_eps: 0.000000
 3144/5000: episode: 112, duration: 0.424s, episode steps:  29, steps per second:  68, episode reward: 38.244, mean reward:  1.319 [-2.941, 32.360], mean action: 2.103 [0.000, 16.000],  loss: 0.022427, mae: 0.361485, mean_q: 0.541353, mean_eps: 0.000000
 3161/5000: episode: 113, duration: 0.249s, episode steps:  17, steps per second:  68, episode reward: 47.271, mean reward:  2.781 [-0.440, 31.911], mean action: 3.176 [0.000, 13.000],  loss: 0.017609, mae: 0.334949, mean_q: 0.519125, mean_eps: 0.000000
 3172/5000: episode: 114, duration: 0.179s, episode steps:  11, steps per second:  62, episode reward: 45.000, mean reward:  4.091 [ 0.000, 30.055], mean action: 2.182 [0.000, 3.000],  loss: 0.016897, mae: 0.334474, mean_q: 0.527422, mean_eps: 0.000000
 3211/5000: episode: 115, duration: 0.547s, episode steps:  39, steps per second:  71, episode reward: 37.880, mean reward:  0.971 [-2.926, 32.490], mean action: 3.846 [0.000, 20.000],  loss: 0.024547, mae: 0.366389, mean_q: 0.503339, mean_eps: 0.000000
 3252/5000: episode: 116, duration: 0.575s, episode steps:  41, steps per second:  71, episode reward: 35.552, mean reward:  0.867 [-2.806, 32.072], mean action: 5.585 [0.000, 15.000],  loss: 0.019912, mae: 0.336003, mean_q: 0.499946, mean_eps: 0.000000
 3283/5000: episode: 117, duration: 0.622s, episode steps:  31, steps per second:  50, episode reward: 32.903, mean reward:  1.061 [-3.000, 32.303], mean action: 4.000 [0.000, 19.000],  loss: 0.020540, mae: 0.343632, mean_q: 0.545256, mean_eps: 0.000000
 3299/5000: episode: 118, duration: 0.262s, episode steps:  16, steps per second:  61, episode reward: 44.013, mean reward:  2.751 [-2.083, 31.982], mean action: 3.375 [0.000, 16.000],  loss: 0.019094, mae: 0.337882, mean_q: 0.595414, mean_eps: 0.000000
 3354/5000: episode: 119, duration: 0.758s, episode steps:  55, steps per second:  73, episode reward: 35.693, mean reward:  0.649 [-3.000, 32.240], mean action: 8.855 [0.000, 21.000],  loss: 0.020557, mae: 0.342678, mean_q: 0.553507, mean_eps: 0.000000
 3370/5000: episode: 120, duration: 0.239s, episode steps:  16, steps per second:  67, episode reward: 38.494, mean reward:  2.406 [-3.000, 32.394], mean action: 2.875 [0.000, 19.000],  loss: 0.016986, mae: 0.332192, mean_q: 0.536572, mean_eps: 0.000000
 3387/5000: episode: 121, duration: 0.268s, episode steps:  17, steps per second:  63, episode reward: 44.519, mean reward:  2.619 [-3.000, 32.160], mean action: 2.824 [0.000, 14.000],  loss: 0.022946, mae: 0.353841, mean_q: 0.554389, mean_eps: 0.000000
 3412/5000: episode: 122, duration: 0.373s, episode steps:  25, steps per second:  67, episode reward: 43.887, mean reward:  1.755 [-3.000, 32.250], mean action: 2.720 [0.000, 12.000],  loss: 0.019990, mae: 0.341383, mean_q: 0.569975, mean_eps: 0.000000
 3445/5000: episode: 123, duration: 0.570s, episode steps:  33, steps per second:  58, episode reward: 33.000, mean reward:  1.000 [-3.000, 32.090], mean action: 4.697 [0.000, 19.000],  loss: 0.019416, mae: 0.336800, mean_q: 0.574041, mean_eps: 0.000000
 3481/5000: episode: 124, duration: 0.513s, episode steps:  36, steps per second:  70, episode reward: 32.601, mean reward:  0.906 [-3.000, 33.000], mean action: 7.111 [0.000, 19.000],  loss: 0.020351, mae: 0.344923, mean_q: 0.563603, mean_eps: 0.000000
 3513/5000: episode: 125, duration: 0.442s, episode steps:  32, steps per second:  72, episode reward: 35.604, mean reward:  1.113 [-3.000, 32.450], mean action: 3.344 [0.000, 16.000],  loss: 0.022098, mae: 0.349483, mean_q: 0.562469, mean_eps: 0.000000
 3544/5000: episode: 126, duration: 0.444s, episode steps:  31, steps per second:  70, episode reward: 38.378, mean reward:  1.238 [-2.208, 32.289], mean action: 2.548 [0.000, 13.000],  loss: 0.019953, mae: 0.342006, mean_q: 0.530917, mean_eps: 0.000000
 3557/5000: episode: 127, duration: 0.205s, episode steps:  13, steps per second:  63, episode reward: 45.000, mean reward:  3.462 [-2.181, 32.180], mean action: 1.692 [1.000, 2.000],  loss: 0.024055, mae: 0.362864, mean_q: 0.519321, mean_eps: 0.000000
 3596/5000: episode: 128, duration: 0.557s, episode steps:  39, steps per second:  70, episode reward: 39.525, mean reward:  1.013 [-2.217, 31.903], mean action: 4.615 [0.000, 16.000],  loss: 0.022907, mae: 0.358357, mean_q: 0.561999, mean_eps: 0.000000
 3617/5000: episode: 129, duration: 0.309s, episode steps:  21, steps per second:  68, episode reward: 44.080, mean reward:  2.099 [-2.040, 32.140], mean action: 2.810 [0.000, 20.000],  loss: 0.019542, mae: 0.346131, mean_q: 0.542947, mean_eps: 0.000000
 3641/5000: episode: 130, duration: 0.356s, episode steps:  24, steps per second:  67, episode reward: 47.544, mean reward:  1.981 [-0.116, 32.196], mean action: 1.000 [0.000, 8.000],  loss: 0.024829, mae: 0.365853, mean_q: 0.530592, mean_eps: 0.000000
 3659/5000: episode: 131, duration: 0.270s, episode steps:  18, steps per second:  67, episode reward: 41.629, mean reward:  2.313 [-2.191, 32.253], mean action: 3.611 [0.000, 12.000],  loss: 0.025151, mae: 0.365458, mean_q: 0.526562, mean_eps: 0.000000
 3681/5000: episode: 132, duration: 0.324s, episode steps:  22, steps per second:  68, episode reward: 41.583, mean reward:  1.890 [-2.847, 32.230], mean action: 3.364 [0.000, 14.000],  loss: 0.021461, mae: 0.350014, mean_q: 0.510225, mean_eps: 0.000000
 3703/5000: episode: 133, duration: 0.405s, episode steps:  22, steps per second:  54, episode reward: 41.276, mean reward:  1.876 [-2.755, 31.594], mean action: 2.545 [1.000, 3.000],  loss: 0.019574, mae: 0.344977, mean_q: 0.503699, mean_eps: 0.000000
 3729/5000: episode: 134, duration: 0.413s, episode steps:  26, steps per second:  63, episode reward: 40.048, mean reward:  1.540 [-3.000, 32.210], mean action: 3.115 [0.000, 12.000],  loss: 0.022985, mae: 0.353878, mean_q: 0.517795, mean_eps: 0.000000
 3744/5000: episode: 135, duration: 0.228s, episode steps:  15, steps per second:  66, episode reward: 47.038, mean reward:  3.136 [ 0.000, 31.723], mean action: 4.533 [1.000, 14.000],  loss: 0.017514, mae: 0.330947, mean_q: 0.508221, mean_eps: 0.000000
 3767/5000: episode: 136, duration: 0.345s, episode steps:  23, steps per second:  67, episode reward: 41.952, mean reward:  1.824 [-2.329, 32.030], mean action: 2.000 [0.000, 16.000],  loss: 0.020276, mae: 0.340869, mean_q: 0.570775, mean_eps: 0.000000
 3793/5000: episode: 137, duration: 0.384s, episode steps:  26, steps per second:  68, episode reward: 38.708, mean reward:  1.489 [-2.579, 32.250], mean action: 2.923 [0.000, 16.000],  loss: 0.020814, mae: 0.343865, mean_q: 0.644796, mean_eps: 0.000000
 3845/5000: episode: 138, duration: 0.740s, episode steps:  52, steps per second:  70, episode reward: 39.051, mean reward:  0.751 [-2.174, 32.320], mean action: 4.038 [0.000, 16.000],  loss: 0.019869, mae: 0.342604, mean_q: 0.564485, mean_eps: 0.000000
 3871/5000: episode: 139, duration: 0.386s, episode steps:  26, steps per second:  67, episode reward: 41.507, mean reward:  1.596 [-2.338, 32.019], mean action: 2.538 [0.000, 16.000],  loss: 0.021407, mae: 0.348856, mean_q: 0.487518, mean_eps: 0.000000
 3900/5000: episode: 140, duration: 0.427s, episode steps:  29, steps per second:  68, episode reward: 42.000, mean reward:  1.448 [-2.329, 32.610], mean action: 1.931 [0.000, 14.000],  loss: 0.020659, mae: 0.353046, mean_q: 0.510141, mean_eps: 0.000000
 3936/5000: episode: 141, duration: 0.508s, episode steps:  36, steps per second:  71, episode reward: 42.000, mean reward:  1.167 [-2.305, 30.141], mean action: 4.444 [0.000, 20.000],  loss: 0.022468, mae: 0.357979, mean_q: 0.560086, mean_eps: 0.000000
 3973/5000: episode: 142, duration: 0.540s, episode steps:  37, steps per second:  68, episode reward: 41.632, mean reward:  1.125 [-2.374, 31.712], mean action: 2.757 [0.000, 16.000],  loss: 0.022633, mae: 0.356624, mean_q: 0.569437, mean_eps: 0.000000
 3994/5000: episode: 143, duration: 0.317s, episode steps:  21, steps per second:  66, episode reward: 40.819, mean reward:  1.944 [-2.498, 32.610], mean action: 4.524 [0.000, 16.000],  loss: 0.021411, mae: 0.356116, mean_q: 0.529034, mean_eps: 0.000000
 4017/5000: episode: 144, duration: 0.333s, episode steps:  23, steps per second:  69, episode reward: 38.860, mean reward:  1.690 [-2.438, 32.540], mean action: 4.000 [1.000, 19.000],  loss: 0.020347, mae: 0.345477, mean_q: 0.534451, mean_eps: 0.000000
 4038/5000: episode: 145, duration: 0.342s, episode steps:  21, steps per second:  61, episode reward: 41.682, mean reward:  1.985 [-2.486, 32.028], mean action: 4.238 [0.000, 20.000],  loss: 0.019419, mae: 0.340039, mean_q: 0.553277, mean_eps: 0.000000
 4058/5000: episode: 146, duration: 0.295s, episode steps:  20, steps per second:  68, episode reward: 35.640, mean reward:  1.782 [-2.876, 31.900], mean action: 4.550 [0.000, 19.000],  loss: 0.018721, mae: 0.340838, mean_q: 0.559167, mean_eps: 0.000000
 4079/5000: episode: 147, duration: 0.306s, episode steps:  21, steps per second:  69, episode reward: 42.000, mean reward:  2.000 [-2.647, 32.160], mean action: 3.381 [0.000, 19.000],  loss: 0.018082, mae: 0.337800, mean_q: 0.557144, mean_eps: 0.000000
 4104/5000: episode: 148, duration: 0.360s, episode steps:  25, steps per second:  69, episode reward: 43.708, mean reward:  1.748 [-3.000, 32.197], mean action: 4.880 [0.000, 21.000],  loss: 0.023087, mae: 0.367281, mean_q: 0.547003, mean_eps: 0.000000
 4132/5000: episode: 149, duration: 0.397s, episode steps:  28, steps per second:  71, episode reward: 41.903, mean reward:  1.497 [-2.608, 32.113], mean action: 3.214 [1.000, 14.000],  loss: 0.021967, mae: 0.369770, mean_q: 0.459319, mean_eps: 0.000000
 4165/5000: episode: 150, duration: 0.466s, episode steps:  33, steps per second:  71, episode reward: 36.000, mean reward:  1.091 [-3.000, 30.073], mean action: 4.030 [1.000, 19.000],  loss: 0.018871, mae: 0.350810, mean_q: 0.485256, mean_eps: 0.000000
 4189/5000: episode: 151, duration: 0.355s, episode steps:  24, steps per second:  68, episode reward: 38.952, mean reward:  1.623 [-3.000, 32.142], mean action: 2.625 [1.000, 14.000],  loss: 0.021869, mae: 0.361477, mean_q: 0.471224, mean_eps: 0.000000
 4210/5000: episode: 152, duration: 0.337s, episode steps:  21, steps per second:  62, episode reward: 35.900, mean reward:  1.710 [-3.000, 32.810], mean action: 2.714 [0.000, 9.000],  loss: 0.021807, mae: 0.350280, mean_q: 0.492337, mean_eps: 0.000000
 4227/5000: episode: 153, duration: 0.261s, episode steps:  17, steps per second:  65, episode reward: 39.000, mean reward:  2.294 [-3.000, 32.050], mean action: 1.588 [0.000, 9.000],  loss: 0.021770, mae: 0.356449, mean_q: 0.516482, mean_eps: 0.000000
 4255/5000: episode: 154, duration: 0.395s, episode steps:  28, steps per second:  71, episode reward: 35.658, mean reward:  1.273 [-3.000, 32.328], mean action: 2.607 [0.000, 9.000],  loss: 0.022526, mae: 0.359703, mean_q: 0.496899, mean_eps: 0.000000
 4276/5000: episode: 155, duration: 0.322s, episode steps:  21, steps per second:  65, episode reward: 37.176, mean reward:  1.770 [-2.796, 32.190], mean action: 1.905 [0.000, 9.000],  loss: 0.020533, mae: 0.347994, mean_q: 0.517023, mean_eps: 0.000000
 4301/5000: episode: 156, duration: 0.598s, episode steps:  25, steps per second:  42, episode reward: 41.423, mean reward:  1.657 [-2.776, 32.340], mean action: 2.360 [0.000, 16.000],  loss: 0.020552, mae: 0.351193, mean_q: 0.586823, mean_eps: 0.000000
 4347/5000: episode: 157, duration: 0.641s, episode steps:  46, steps per second:  72, episode reward: 32.938, mean reward:  0.716 [-3.000, 32.090], mean action: 6.826 [0.000, 19.000],  loss: 0.019618, mae: 0.338481, mean_q: 0.584611, mean_eps: 0.000000
 4374/5000: episode: 158, duration: 0.396s, episode steps:  27, steps per second:  68, episode reward: 44.335, mean reward:  1.642 [-2.469, 32.340], mean action: 2.630 [0.000, 19.000],  loss: 0.020135, mae: 0.345440, mean_q: 0.521503, mean_eps: 0.000000
 4399/5000: episode: 159, duration: 0.362s, episode steps:  25, steps per second:  69, episode reward: 44.580, mean reward:  1.783 [-2.482, 32.354], mean action: 2.080 [0.000, 11.000],  loss: 0.019646, mae: 0.340065, mean_q: 0.573097, mean_eps: 0.000000
 4426/5000: episode: 160, duration: 0.404s, episode steps:  27, steps per second:  67, episode reward: 38.767, mean reward:  1.436 [-2.541, 32.107], mean action: 2.370 [0.000, 19.000],  loss: 0.020698, mae: 0.351371, mean_q: 0.585798, mean_eps: 0.000000
 4465/5000: episode: 161, duration: 0.554s, episode steps:  39, steps per second:  70, episode reward: 44.143, mean reward:  1.132 [-2.119, 32.070], mean action: 3.179 [0.000, 19.000],  loss: 0.022141, mae: 0.362324, mean_q: 0.497428, mean_eps: 0.000000
 4491/5000: episode: 162, duration: 0.389s, episode steps:  26, steps per second:  67, episode reward: 47.344, mean reward:  1.821 [-0.442, 32.030], mean action: 3.192 [1.000, 20.000],  loss: 0.019661, mae: 0.349592, mean_q: 0.473633, mean_eps: 0.000000
 4521/5000: episode: 163, duration: 0.462s, episode steps:  30, steps per second:  65, episode reward: 38.812, mean reward:  1.294 [-3.000, 32.190], mean action: 2.300 [0.000, 11.000],  loss: 0.020982, mae: 0.346821, mean_q: 0.515195, mean_eps: 0.000000
 4553/5000: episode: 164, duration: 0.460s, episode steps:  32, steps per second:  70, episode reward: 37.946, mean reward:  1.186 [-3.000, 32.410], mean action: 2.531 [0.000, 19.000],  loss: 0.025991, mae: 0.369848, mean_q: 0.567129, mean_eps: 0.000000
 4573/5000: episode: 165, duration: 0.294s, episode steps:  20, steps per second:  68, episode reward: 43.671, mean reward:  2.184 [-3.000, 32.330], mean action: 3.900 [0.000, 12.000],  loss: 0.022550, mae: 0.349193, mean_q: 0.570962, mean_eps: 0.000000
 4607/5000: episode: 166, duration: 0.477s, episode steps:  34, steps per second:  71, episode reward: -32.060, mean reward: -0.943 [-32.756,  3.000], mean action: 3.235 [0.000, 15.000],  loss: 0.018629, mae: 0.335345, mean_q: 0.553166, mean_eps: 0.000000
 4652/5000: episode: 167, duration: 0.626s, episode steps:  45, steps per second:  72, episode reward: 35.620, mean reward:  0.792 [-2.399, 32.590], mean action: 4.933 [0.000, 20.000],  loss: 0.020468, mae: 0.336957, mean_q: 0.489259, mean_eps: 0.000000
 4673/5000: episode: 168, duration: 0.309s, episode steps:  21, steps per second:  68, episode reward: 44.317, mean reward:  2.110 [-2.491, 32.120], mean action: 5.333 [0.000, 14.000],  loss: 0.020045, mae: 0.327220, mean_q: 0.535618, mean_eps: 0.000000
 4694/5000: episode: 169, duration: 0.334s, episode steps:  21, steps per second:  63, episode reward: 45.000, mean reward:  2.143 [-2.007, 32.400], mean action: 0.381 [0.000, 3.000],  loss: 0.020723, mae: 0.329954, mean_q: 0.554360, mean_eps: 0.000000
 4719/5000: episode: 170, duration: 0.360s, episode steps:  25, steps per second:  70, episode reward: 41.604, mean reward:  1.664 [-2.753, 32.400], mean action: 3.320 [0.000, 14.000],  loss: 0.018492, mae: 0.324392, mean_q: 0.513543, mean_eps: 0.000000
 4739/5000: episode: 171, duration: 0.289s, episode steps:  20, steps per second:  69, episode reward: 41.571, mean reward:  2.079 [-2.915, 32.160], mean action: 2.600 [0.000, 19.000],  loss: 0.016504, mae: 0.318008, mean_q: 0.438510, mean_eps: 0.000000
 4763/5000: episode: 172, duration: 0.348s, episode steps:  24, steps per second:  69, episode reward: 41.174, mean reward:  1.716 [-2.371, 32.137], mean action: 4.250 [0.000, 19.000],  loss: 0.021892, mae: 0.338864, mean_q: 0.443396, mean_eps: 0.000000
 4794/5000: episode: 173, duration: 0.448s, episode steps:  31, steps per second:  69, episode reward: 44.108, mean reward:  1.423 [-2.325, 32.590], mean action: 3.516 [0.000, 15.000],  loss: 0.021553, mae: 0.335163, mean_q: 0.560890, mean_eps: 0.000000
 4826/5000: episode: 174, duration: 0.461s, episode steps:  32, steps per second:  69, episode reward: 35.901, mean reward:  1.122 [-2.813, 32.131], mean action: 4.125 [0.000, 15.000],  loss: 0.021385, mae: 0.338081, mean_q: 0.622580, mean_eps: 0.000000
 4846/5000: episode: 175, duration: 0.294s, episode steps:  20, steps per second:  68, episode reward: 44.817, mean reward:  2.241 [-2.223, 32.270], mean action: 2.600 [0.000, 16.000],  loss: 0.017582, mae: 0.319131, mean_q: 0.565651, mean_eps: 0.000000
 4858/5000: episode: 176, duration: 0.183s, episode steps:  12, steps per second:  65, episode reward: 45.000, mean reward:  3.750 [-2.165, 32.490], mean action: 4.917 [2.000, 16.000],  loss: 0.024711, mae: 0.364105, mean_q: 0.575266, mean_eps: 0.000000
 4875/5000: episode: 177, duration: 0.267s, episode steps:  17, steps per second:  64, episode reward: 44.308, mean reward:  2.606 [-2.121, 32.370], mean action: 6.529 [1.000, 16.000],  loss: 0.020000, mae: 0.342547, mean_q: 0.518160, mean_eps: 0.000000
 4909/5000: episode: 178, duration: 0.513s, episode steps:  34, steps per second:  66, episode reward: 41.266, mean reward:  1.214 [-2.877, 32.320], mean action: 2.882 [0.000, 16.000],  loss: 0.018834, mae: 0.324566, mean_q: 0.524851, mean_eps: 0.000000
 4932/5000: episode: 179, duration: 0.331s, episode steps:  23, steps per second:  70, episode reward: 41.117, mean reward:  1.788 [-2.525, 32.376], mean action: 2.652 [0.000, 13.000],  loss: 0.021884, mae: 0.340187, mean_q: 0.519423, mean_eps: 0.000000
 4970/5000: episode: 180, duration: 0.544s, episode steps:  38, steps per second:  70, episode reward: 38.854, mean reward:  1.022 [-2.400, 32.720], mean action: 3.474 [0.000, 14.000],  loss: 0.019086, mae: 0.331462, mean_q: 0.517245, mean_eps: 0.000000
 4992/5000: episode: 181, duration: 0.315s, episode steps:  22, steps per second:  70, episode reward: 40.967, mean reward:  1.862 [-3.000, 32.468], mean action: 4.545 [0.000, 19.000],  loss: 0.018603, mae: 0.331367, mean_q: 0.481904, mean_eps: 0.000000
done, took 66.519 seconds
DQN Evaluation: 9043 victories out of 10564 episodes
Training for 5000 steps ...
   14/5000: episode: 1, duration: 0.129s, episode steps:  14, steps per second: 109, episode reward: 44.264, mean reward:  3.162 [-2.222, 32.789], mean action: 2.357 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   47/5000: episode: 2, duration: 0.228s, episode steps:  33, steps per second: 145, episode reward: 32.078, mean reward:  0.972 [-2.369, 32.240], mean action: 5.273 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   74/5000: episode: 3, duration: 0.196s, episode steps:  27, steps per second: 138, episode reward: -33.000, mean reward: -1.222 [-32.179,  3.000], mean action: 5.556 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   96/5000: episode: 4, duration: 0.161s, episode steps:  22, steps per second: 137, episode reward: 41.238, mean reward:  1.874 [-2.172, 33.000], mean action: 4.773 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  119/5000: episode: 5, duration: 0.161s, episode steps:  23, steps per second: 143, episode reward: -32.610, mean reward: -1.418 [-32.306,  3.000], mean action: 5.522 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  145/5000: episode: 6, duration: 0.175s, episode steps:  26, steps per second: 149, episode reward: -32.610, mean reward: -1.254 [-32.182,  2.350], mean action: 9.577 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  167/5000: episode: 7, duration: 0.155s, episode steps:  22, steps per second: 142, episode reward: 35.055, mean reward:  1.593 [-2.394, 31.901], mean action: 6.682 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  180/5000: episode: 8, duration: 0.101s, episode steps:  13, steps per second: 129, episode reward: 41.610, mean reward:  3.201 [-3.000, 33.000], mean action: 4.231 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  207/5000: episode: 9, duration: 0.183s, episode steps:  27, steps per second: 148, episode reward: 32.154, mean reward:  1.191 [-2.724, 32.230], mean action: 4.519 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  238/5000: episode: 10, duration: 0.194s, episode steps:  31, steps per second: 159, episode reward: 32.815, mean reward:  1.059 [-2.428, 32.871], mean action: 5.645 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  258/5000: episode: 11, duration: 0.152s, episode steps:  20, steps per second: 132, episode reward: 35.140, mean reward:  1.757 [-3.000, 32.568], mean action: 6.500 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  276/5000: episode: 12, duration: 0.127s, episode steps:  18, steps per second: 142, episode reward: 44.121, mean reward:  2.451 [-2.027, 32.178], mean action: 1.833 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  296/5000: episode: 13, duration: 0.144s, episode steps:  20, steps per second: 139, episode reward: 34.725, mean reward:  1.736 [-3.000, 32.497], mean action: 6.950 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  319/5000: episode: 14, duration: 0.162s, episode steps:  23, steps per second: 142, episode reward: 39.000, mean reward:  1.696 [-2.292, 32.050], mean action: 4.435 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  358/5000: episode: 15, duration: 0.241s, episode steps:  39, steps per second: 162, episode reward: -32.540, mean reward: -0.834 [-32.106,  2.211], mean action: 6.436 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  380/5000: episode: 16, duration: 0.153s, episode steps:  22, steps per second: 143, episode reward: 48.000, mean reward:  2.182 [ 0.029, 32.170], mean action: 0.864 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  402/5000: episode: 17, duration: 0.147s, episode steps:  22, steps per second: 150, episode reward: 45.000, mean reward:  2.045 [-2.890, 32.100], mean action: 2.909 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  415/5000: episode: 18, duration: 0.107s, episode steps:  13, steps per second: 121, episode reward: 44.426, mean reward:  3.417 [-2.365, 32.952], mean action: 3.077 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  441/5000: episode: 19, duration: 0.168s, episode steps:  26, steps per second: 155, episode reward: -38.050, mean reward: -1.463 [-32.107,  2.690], mean action: 10.538 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  484/5000: episode: 20, duration: 0.262s, episode steps:  43, steps per second: 164, episode reward: 32.310, mean reward:  0.751 [-3.000, 33.000], mean action: 4.488 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  507/5000: episode: 21, duration: 0.160s, episode steps:  23, steps per second: 143, episode reward: 35.384, mean reward:  1.538 [-2.317, 32.050], mean action: 4.870 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  525/5000: episode: 22, duration: 0.122s, episode steps:  18, steps per second: 147, episode reward: 42.000, mean reward:  2.333 [-2.100, 33.000], mean action: 2.667 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  550/5000: episode: 23, duration: 0.170s, episode steps:  25, steps per second: 147, episode reward: 35.795, mean reward:  1.432 [-3.000, 32.510], mean action: 4.880 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  576/5000: episode: 24, duration: 0.176s, episode steps:  26, steps per second: 148, episode reward: 41.286, mean reward:  1.588 [-3.000, 33.000], mean action: 2.038 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  594/5000: episode: 25, duration: 0.128s, episode steps:  18, steps per second: 140, episode reward: 38.696, mean reward:  2.150 [-2.819, 32.370], mean action: 5.333 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  610/5000: episode: 26, duration: 0.119s, episode steps:  16, steps per second: 134, episode reward: 38.419, mean reward:  2.401 [-3.000, 32.595], mean action: 6.250 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  639/5000: episode: 27, duration: 0.195s, episode steps:  29, steps per second: 149, episode reward: -36.000, mean reward: -1.241 [-32.139,  2.220], mean action: 8.207 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  663/5000: episode: 28, duration: 0.156s, episode steps:  24, steps per second: 154, episode reward: 34.391, mean reward:  1.433 [-2.758, 32.778], mean action: 8.292 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  680/5000: episode: 29, duration: 0.122s, episode steps:  17, steps per second: 139, episode reward: 38.013, mean reward:  2.236 [-2.566, 33.000], mean action: 3.647 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  723/5000: episode: 30, duration: 0.270s, episode steps:  43, steps per second: 159, episode reward: -32.630, mean reward: -0.759 [-32.064,  2.367], mean action: 4.605 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  739/5000: episode: 31, duration: 0.111s, episode steps:  16, steps per second: 144, episode reward: 35.823, mean reward:  2.239 [-3.000, 32.293], mean action: 5.750 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  764/5000: episode: 32, duration: 0.172s, episode steps:  25, steps per second: 146, episode reward: 38.322, mean reward:  1.533 [-2.178, 32.090], mean action: 4.720 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  786/5000: episode: 33, duration: 0.146s, episode steps:  22, steps per second: 151, episode reward: 32.615, mean reward:  1.482 [-3.000, 32.892], mean action: 6.000 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  834/5000: episode: 34, duration: 0.303s, episode steps:  48, steps per second: 158, episode reward: -39.000, mean reward: -0.812 [-32.003,  2.320], mean action: 10.083 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  882/5000: episode: 35, duration: 0.300s, episode steps:  48, steps per second: 160, episode reward: 43.247, mean reward:  0.901 [-2.172, 32.310], mean action: 1.667 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  899/5000: episode: 36, duration: 0.121s, episode steps:  17, steps per second: 141, episode reward: 38.324, mean reward:  2.254 [-2.514, 32.410], mean action: 5.118 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  916/5000: episode: 37, duration: 0.119s, episode steps:  17, steps per second: 142, episode reward: 36.000, mean reward:  2.118 [-2.143, 29.654], mean action: 4.882 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  941/5000: episode: 38, duration: 0.163s, episode steps:  25, steps per second: 153, episode reward: -33.000, mean reward: -1.320 [-32.178,  2.710], mean action: 10.280 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  967/5000: episode: 39, duration: 0.174s, episode steps:  26, steps per second: 149, episode reward: 35.885, mean reward:  1.380 [-3.000, 32.275], mean action: 5.538 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  986/5000: episode: 40, duration: 0.136s, episode steps:  19, steps per second: 140, episode reward: 35.253, mean reward:  1.855 [-2.376, 33.000], mean action: 7.947 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1009/5000: episode: 41, duration: 0.239s, episode steps:  23, steps per second:  96, episode reward: -33.000, mean reward: -1.435 [-32.249,  2.640], mean action: 7.087 [1.000, 19.000],  loss: 0.015750, mae: 0.315718, mean_q: 0.522687, mean_eps: 0.000000
 1030/5000: episode: 42, duration: 0.299s, episode steps:  21, steps per second:  70, episode reward: 38.469, mean reward:  1.832 [-2.375, 33.000], mean action: 6.190 [0.000, 19.000],  loss: 0.022364, mae: 0.341248, mean_q: 0.509708, mean_eps: 0.000000
 1047/5000: episode: 43, duration: 0.257s, episode steps:  17, steps per second:  66, episode reward: 38.530, mean reward:  2.266 [-3.000, 33.000], mean action: 6.471 [0.000, 19.000],  loss: 0.023552, mae: 0.348790, mean_q: 0.479836, mean_eps: 0.000000
 1066/5000: episode: 44, duration: 0.288s, episode steps:  19, steps per second:  66, episode reward: 41.300, mean reward:  2.174 [-2.086, 32.120], mean action: 3.474 [0.000, 16.000],  loss: 0.017332, mae: 0.317561, mean_q: 0.542653, mean_eps: 0.000000
 1091/5000: episode: 45, duration: 0.358s, episode steps:  25, steps per second:  70, episode reward: 38.907, mean reward:  1.556 [-2.611, 32.100], mean action: 5.240 [1.000, 20.000],  loss: 0.018094, mae: 0.320902, mean_q: 0.568818, mean_eps: 0.000000
 1113/5000: episode: 46, duration: 0.330s, episode steps:  22, steps per second:  67, episode reward: 38.773, mean reward:  1.762 [-2.904, 31.813], mean action: 2.727 [0.000, 16.000],  loss: 0.022449, mae: 0.349637, mean_q: 0.523588, mean_eps: 0.000000
 1146/5000: episode: 47, duration: 0.475s, episode steps:  33, steps per second:  69, episode reward: 32.893, mean reward:  0.997 [-2.389, 31.903], mean action: 3.606 [0.000, 16.000],  loss: 0.021902, mae: 0.341248, mean_q: 0.512687, mean_eps: 0.000000
 1173/5000: episode: 48, duration: 0.383s, episode steps:  27, steps per second:  71, episode reward: -38.720, mean reward: -1.434 [-32.213,  2.220], mean action: 5.296 [0.000, 19.000],  loss: 0.020481, mae: 0.331795, mean_q: 0.470255, mean_eps: 0.000000
 1199/5000: episode: 49, duration: 0.378s, episode steps:  26, steps per second:  69, episode reward: -32.220, mean reward: -1.239 [-32.312,  2.380], mean action: 5.885 [0.000, 19.000],  loss: 0.018449, mae: 0.325965, mean_q: 0.495490, mean_eps: 0.000000
 1212/5000: episode: 50, duration: 0.230s, episode steps:  13, steps per second:  57, episode reward: 41.519, mean reward:  3.194 [-2.408, 32.619], mean action: 2.769 [0.000, 19.000],  loss: 0.022194, mae: 0.343031, mean_q: 0.518946, mean_eps: 0.000000
 1237/5000: episode: 51, duration: 0.370s, episode steps:  25, steps per second:  68, episode reward: 32.123, mean reward:  1.285 [-3.000, 31.923], mean action: 5.080 [0.000, 19.000],  loss: 0.021313, mae: 0.340745, mean_q: 0.539479, mean_eps: 0.000000
 1286/5000: episode: 52, duration: 0.678s, episode steps:  49, steps per second:  72, episode reward: -32.530, mean reward: -0.664 [-31.783,  3.000], mean action: 4.796 [0.000, 13.000],  loss: 0.021200, mae: 0.350949, mean_q: 0.606018, mean_eps: 0.000000
 1303/5000: episode: 53, duration: 0.253s, episode steps:  17, steps per second:  67, episode reward: 38.292, mean reward:  2.252 [-2.900, 32.130], mean action: 3.765 [0.000, 11.000],  loss: 0.018534, mae: 0.324793, mean_q: 0.570100, mean_eps: 0.000000
 1319/5000: episode: 54, duration: 0.251s, episode steps:  16, steps per second:  64, episode reward: 38.025, mean reward:  2.377 [-3.000, 31.675], mean action: 3.938 [1.000, 11.000],  loss: 0.019785, mae: 0.345725, mean_q: 0.577406, mean_eps: 0.000000
 1335/5000: episode: 55, duration: 0.256s, episode steps:  16, steps per second:  63, episode reward: 47.318, mean reward:  2.957 [ 0.108, 33.000], mean action: 1.438 [0.000, 3.000],  loss: 0.017974, mae: 0.328817, mean_q: 0.575441, mean_eps: 0.000000
 1362/5000: episode: 56, duration: 0.385s, episode steps:  27, steps per second:  70, episode reward: 38.402, mean reward:  1.422 [-2.253, 32.235], mean action: 2.926 [0.000, 16.000],  loss: 0.021760, mae: 0.342101, mean_q: 0.520569, mean_eps: 0.000000
 1386/5000: episode: 57, duration: 0.591s, episode steps:  24, steps per second:  41, episode reward: -35.630, mean reward: -1.485 [-32.258,  2.510], mean action: 6.417 [0.000, 19.000],  loss: 0.019939, mae: 0.339732, mean_q: 0.537473, mean_eps: 0.000000
 1429/5000: episode: 58, duration: 0.614s, episode steps:  43, steps per second:  70, episode reward: 32.188, mean reward:  0.749 [-2.731, 31.852], mean action: 8.116 [0.000, 19.000],  loss: 0.019855, mae: 0.341800, mean_q: 0.540939, mean_eps: 0.000000
 1450/5000: episode: 59, duration: 0.304s, episode steps:  21, steps per second:  69, episode reward: 35.317, mean reward:  1.682 [-2.321, 32.351], mean action: 2.952 [0.000, 15.000],  loss: 0.019881, mae: 0.338763, mean_q: 0.509030, mean_eps: 0.000000
 1465/5000: episode: 60, duration: 0.224s, episode steps:  15, steps per second:  67, episode reward: 35.922, mean reward:  2.395 [-3.000, 32.330], mean action: 5.733 [0.000, 19.000],  loss: 0.020666, mae: 0.344855, mean_q: 0.523508, mean_eps: 0.000000
 1482/5000: episode: 61, duration: 0.257s, episode steps:  17, steps per second:  66, episode reward: 43.852, mean reward:  2.580 [-2.207, 32.208], mean action: 2.471 [0.000, 19.000],  loss: 0.020305, mae: 0.347356, mean_q: 0.473597, mean_eps: 0.000000
 1502/5000: episode: 62, duration: 0.297s, episode steps:  20, steps per second:  67, episode reward: 44.262, mean reward:  2.213 [-2.069, 32.250], mean action: 1.400 [0.000, 3.000],  loss: 0.019887, mae: 0.340280, mean_q: 0.483057, mean_eps: 0.000000
 1528/5000: episode: 63, duration: 0.405s, episode steps:  26, steps per second:  64, episode reward: 38.364, mean reward:  1.476 [-3.000, 32.683], mean action: 3.538 [0.000, 16.000],  loss: 0.018325, mae: 0.330023, mean_q: 0.501704, mean_eps: 0.000000
 1550/5000: episode: 64, duration: 0.318s, episode steps:  22, steps per second:  69, episode reward: 32.494, mean reward:  1.477 [-3.000, 32.890], mean action: 5.682 [0.000, 16.000],  loss: 0.021574, mae: 0.342781, mean_q: 0.559106, mean_eps: 0.000000
 1572/5000: episode: 65, duration: 0.328s, episode steps:  22, steps per second:  67, episode reward: 38.202, mean reward:  1.736 [-2.311, 33.000], mean action: 3.545 [0.000, 15.000],  loss: 0.023530, mae: 0.354730, mean_q: 0.491907, mean_eps: 0.000000
 1595/5000: episode: 66, duration: 0.343s, episode steps:  23, steps per second:  67, episode reward: 35.903, mean reward:  1.561 [-2.464, 32.273], mean action: 3.739 [0.000, 15.000],  loss: 0.021454, mae: 0.349758, mean_q: 0.465541, mean_eps: 0.000000
 1626/5000: episode: 67, duration: 0.445s, episode steps:  31, steps per second:  70, episode reward: -32.860, mean reward: -1.060 [-32.294,  2.530], mean action: 6.065 [0.000, 16.000],  loss: 0.021676, mae: 0.352789, mean_q: 0.493806, mean_eps: 0.000000
 1645/5000: episode: 68, duration: 0.277s, episode steps:  19, steps per second:  69, episode reward: 42.000, mean reward:  2.211 [-2.260, 32.430], mean action: 2.842 [0.000, 16.000],  loss: 0.021713, mae: 0.340618, mean_q: 0.560739, mean_eps: 0.000000
 1657/5000: episode: 69, duration: 0.190s, episode steps:  12, steps per second:  63, episode reward: 41.930, mean reward:  3.494 [-2.312, 32.727], mean action: 5.250 [0.000, 16.000],  loss: 0.022361, mae: 0.338829, mean_q: 0.538859, mean_eps: 0.000000
 1677/5000: episode: 70, duration: 0.285s, episode steps:  20, steps per second:  70, episode reward: 36.000, mean reward:  1.800 [-3.000, 32.230], mean action: 3.650 [0.000, 16.000],  loss: 0.018945, mae: 0.324422, mean_q: 0.617126, mean_eps: 0.000000
 1695/5000: episode: 71, duration: 0.267s, episode steps:  18, steps per second:  67, episode reward: 32.789, mean reward:  1.822 [-3.000, 32.459], mean action: 6.333 [0.000, 19.000],  loss: 0.017328, mae: 0.326405, mean_q: 0.592726, mean_eps: 0.000000
 1721/5000: episode: 72, duration: 0.396s, episode steps:  26, steps per second:  66, episode reward: 33.000, mean reward:  1.269 [-2.753, 32.330], mean action: 5.615 [0.000, 16.000],  loss: 0.020000, mae: 0.342381, mean_q: 0.521154, mean_eps: 0.000000
 1745/5000: episode: 73, duration: 0.343s, episode steps:  24, steps per second:  70, episode reward: -33.000, mean reward: -1.375 [-32.071,  2.250], mean action: 4.542 [0.000, 18.000],  loss: 0.021028, mae: 0.343770, mean_q: 0.481938, mean_eps: 0.000000
 1764/5000: episode: 74, duration: 0.283s, episode steps:  19, steps per second:  67, episode reward: 38.195, mean reward:  2.010 [-2.701, 33.000], mean action: 3.158 [0.000, 16.000],  loss: 0.019522, mae: 0.333477, mean_q: 0.488208, mean_eps: 0.000000
 1781/5000: episode: 75, duration: 0.264s, episode steps:  17, steps per second:  64, episode reward: 44.512, mean reward:  2.618 [-2.169, 32.470], mean action: 1.529 [0.000, 3.000],  loss: 0.022202, mae: 0.346253, mean_q: 0.501408, mean_eps: 0.000000
 1803/5000: episode: 76, duration: 0.316s, episode steps:  22, steps per second:  70, episode reward: 38.337, mean reward:  1.743 [-3.000, 30.105], mean action: 4.045 [1.000, 15.000],  loss: 0.020968, mae: 0.342164, mean_q: 0.518436, mean_eps: 0.000000
 1823/5000: episode: 77, duration: 0.293s, episode steps:  20, steps per second:  68, episode reward: 35.472, mean reward:  1.774 [-2.447, 32.140], mean action: 3.700 [0.000, 15.000],  loss: 0.019241, mae: 0.338898, mean_q: 0.508742, mean_eps: 0.000000
 1854/5000: episode: 78, duration: 0.449s, episode steps:  31, steps per second:  69, episode reward: -35.600, mean reward: -1.148 [-32.464,  2.019], mean action: 5.097 [0.000, 15.000],  loss: 0.021777, mae: 0.350129, mean_q: 0.497639, mean_eps: 0.000000
 1873/5000: episode: 79, duration: 0.283s, episode steps:  19, steps per second:  67, episode reward: 41.141, mean reward:  2.165 [-2.340, 32.185], mean action: 2.474 [0.000, 9.000],  loss: 0.019456, mae: 0.337477, mean_q: 0.545769, mean_eps: 0.000000
 1904/5000: episode: 80, duration: 0.448s, episode steps:  31, steps per second:  69, episode reward: -35.230, mean reward: -1.136 [-32.103,  2.450], mean action: 5.774 [0.000, 18.000],  loss: 0.020890, mae: 0.339384, mean_q: 0.556648, mean_eps: 0.000000
 1921/5000: episode: 81, duration: 0.256s, episode steps:  17, steps per second:  66, episode reward: 38.070, mean reward:  2.239 [-2.652, 32.901], mean action: 7.529 [0.000, 16.000],  loss: 0.018571, mae: 0.334954, mean_q: 0.503197, mean_eps: 0.000000
 1954/5000: episode: 82, duration: 0.462s, episode steps:  33, steps per second:  71, episode reward: 35.319, mean reward:  1.070 [-3.000, 31.805], mean action: 1.606 [0.000, 16.000],  loss: 0.018141, mae: 0.328512, mean_q: 0.488346, mean_eps: 0.000000
 1967/5000: episode: 83, duration: 0.202s, episode steps:  13, steps per second:  64, episode reward: 44.246, mean reward:  3.404 [-2.007, 33.000], mean action: 1.615 [0.000, 11.000],  loss: 0.019471, mae: 0.345943, mean_q: 0.485512, mean_eps: 0.000000
 1991/5000: episode: 84, duration: 0.357s, episode steps:  24, steps per second:  67, episode reward: 32.911, mean reward:  1.371 [-2.901, 32.201], mean action: 3.042 [0.000, 11.000],  loss: 0.020979, mae: 0.340557, mean_q: 0.538729, mean_eps: 0.000000
 2010/5000: episode: 85, duration: 0.275s, episode steps:  19, steps per second:  69, episode reward: -33.000, mean reward: -1.737 [-32.239,  2.280], mean action: 3.737 [0.000, 12.000],  loss: 0.016181, mae: 0.321949, mean_q: 0.488574, mean_eps: 0.000000
 2030/5000: episode: 86, duration: 0.295s, episode steps:  20, steps per second:  68, episode reward: -35.660, mean reward: -1.783 [-32.730,  2.523], mean action: 6.300 [0.000, 18.000],  loss: 0.018610, mae: 0.340633, mean_q: 0.494047, mean_eps: 0.000000
 2055/5000: episode: 87, duration: 0.356s, episode steps:  25, steps per second:  70, episode reward: 35.163, mean reward:  1.407 [-3.000, 32.070], mean action: 4.720 [0.000, 19.000],  loss: 0.017291, mae: 0.330264, mean_q: 0.515491, mean_eps: 0.000000
 2096/5000: episode: 88, duration: 0.581s, episode steps:  41, steps per second:  71, episode reward: 39.333, mean reward:  0.959 [-2.358, 33.000], mean action: 6.220 [0.000, 16.000],  loss: 0.020485, mae: 0.352092, mean_q: 0.483997, mean_eps: 0.000000
 2121/5000: episode: 89, duration: 0.371s, episode steps:  25, steps per second:  67, episode reward: 32.835, mean reward:  1.313 [-2.337, 31.961], mean action: 5.280 [0.000, 16.000],  loss: 0.018296, mae: 0.345909, mean_q: 0.507924, mean_eps: 0.000000
 2162/5000: episode: 90, duration: 0.593s, episode steps:  41, steps per second:  69, episode reward: 35.623, mean reward:  0.869 [-2.369, 32.260], mean action: 4.195 [0.000, 16.000],  loss: 0.023468, mae: 0.355270, mean_q: 0.548345, mean_eps: 0.000000
 2180/5000: episode: 91, duration: 0.270s, episode steps:  18, steps per second:  67, episode reward: 36.000, mean reward:  2.000 [-2.457, 32.610], mean action: 5.111 [0.000, 16.000],  loss: 0.016813, mae: 0.332788, mean_q: 0.531685, mean_eps: 0.000000
 2190/5000: episode: 92, duration: 0.161s, episode steps:  10, steps per second:  62, episode reward: 41.697, mean reward:  4.170 [-2.904, 32.017], mean action: 2.400 [0.000, 11.000],  loss: 0.019232, mae: 0.346547, mean_q: 0.538881, mean_eps: 0.000000
 2210/5000: episode: 93, duration: 0.288s, episode steps:  20, steps per second:  69, episode reward: 40.556, mean reward:  2.028 [-2.551, 32.530], mean action: 6.250 [0.000, 19.000],  loss: 0.017611, mae: 0.334080, mean_q: 0.525657, mean_eps: 0.000000
 2239/5000: episode: 94, duration: 0.411s, episode steps:  29, steps per second:  71, episode reward: -32.020, mean reward: -1.104 [-31.897,  2.690], mean action: 4.690 [0.000, 19.000],  loss: 0.024750, mae: 0.367362, mean_q: 0.540718, mean_eps: 0.000000
 2259/5000: episode: 95, duration: 0.304s, episode steps:  20, steps per second:  66, episode reward: 38.215, mean reward:  1.911 [-2.350, 32.608], mean action: 7.350 [0.000, 21.000],  loss: 0.020083, mae: 0.339015, mean_q: 0.538080, mean_eps: 0.000000
 2284/5000: episode: 96, duration: 0.381s, episode steps:  25, steps per second:  66, episode reward: 41.411, mean reward:  1.656 [-2.237, 31.830], mean action: 2.000 [0.000, 3.000],  loss: 0.022178, mae: 0.353300, mean_q: 0.530643, mean_eps: 0.000000
 2297/5000: episode: 97, duration: 0.213s, episode steps:  13, steps per second:  61, episode reward: 44.711, mean reward:  3.439 [-2.436, 32.711], mean action: 1.923 [0.000, 3.000],  loss: 0.021146, mae: 0.346009, mean_q: 0.507056, mean_eps: 0.000000
 2326/5000: episode: 98, duration: 0.426s, episode steps:  29, steps per second:  68, episode reward: -32.180, mean reward: -1.110 [-32.206,  3.000], mean action: 9.172 [0.000, 18.000],  loss: 0.019942, mae: 0.344188, mean_q: 0.552173, mean_eps: 0.000000
 2359/5000: episode: 99, duration: 0.473s, episode steps:  33, steps per second:  70, episode reward: 32.016, mean reward:  0.970 [-2.472, 32.090], mean action: 7.788 [0.000, 20.000],  loss: 0.022829, mae: 0.350192, mean_q: 0.507424, mean_eps: 0.000000
 2380/5000: episode: 100, duration: 0.297s, episode steps:  21, steps per second:  71, episode reward: 35.521, mean reward:  1.691 [-3.000, 31.621], mean action: 3.714 [0.000, 13.000],  loss: 0.015247, mae: 0.311626, mean_q: 0.513648, mean_eps: 0.000000
 2404/5000: episode: 101, duration: 0.348s, episode steps:  24, steps per second:  69, episode reward: 36.000, mean reward:  1.500 [-2.517, 29.793], mean action: 2.458 [0.000, 11.000],  loss: 0.018358, mae: 0.329571, mean_q: 0.527256, mean_eps: 0.000000
 2425/5000: episode: 102, duration: 0.305s, episode steps:  21, steps per second:  69, episode reward: 35.850, mean reward:  1.707 [-2.575, 32.550], mean action: 5.095 [0.000, 19.000],  loss: 0.023786, mae: 0.365221, mean_q: 0.510431, mean_eps: 0.000000
 2454/5000: episode: 103, duration: 0.408s, episode steps:  29, steps per second:  71, episode reward: -32.150, mean reward: -1.109 [-32.158,  2.860], mean action: 6.517 [0.000, 19.000],  loss: 0.020672, mae: 0.348085, mean_q: 0.541610, mean_eps: 0.000000
 2480/5000: episode: 104, duration: 0.377s, episode steps:  26, steps per second:  69, episode reward: 32.105, mean reward:  1.235 [-2.612, 32.020], mean action: 7.731 [0.000, 19.000],  loss: 0.022805, mae: 0.357690, mean_q: 0.516627, mean_eps: 0.000000
 2496/5000: episode: 105, duration: 0.236s, episode steps:  16, steps per second:  68, episode reward: 42.000, mean reward:  2.625 [-3.000, 33.000], mean action: 4.312 [0.000, 16.000],  loss: 0.023418, mae: 0.365502, mean_q: 0.512032, mean_eps: 0.000000
 2526/5000: episode: 106, duration: 0.562s, episode steps:  30, steps per second:  53, episode reward: -32.390, mean reward: -1.080 [-31.775,  2.170], mean action: 5.400 [0.000, 19.000],  loss: 0.022323, mae: 0.350450, mean_q: 0.516827, mean_eps: 0.000000
 2557/5000: episode: 107, duration: 0.496s, episode steps:  31, steps per second:  62, episode reward: 32.291, mean reward:  1.042 [-3.000, 31.981], mean action: 4.355 [0.000, 19.000],  loss: 0.020223, mae: 0.340532, mean_q: 0.552027, mean_eps: 0.000000
 2586/5000: episode: 108, duration: 0.411s, episode steps:  29, steps per second:  71, episode reward: 32.444, mean reward:  1.119 [-2.315, 31.914], mean action: 5.138 [0.000, 19.000],  loss: 0.020867, mae: 0.346514, mean_q: 0.530437, mean_eps: 0.000000
 2607/5000: episode: 109, duration: 0.304s, episode steps:  21, steps per second:  69, episode reward: 35.856, mean reward:  1.707 [-2.740, 32.500], mean action: 3.952 [0.000, 19.000],  loss: 0.019302, mae: 0.352361, mean_q: 0.498329, mean_eps: 0.000000
 2619/5000: episode: 110, duration: 0.189s, episode steps:  12, steps per second:  63, episode reward: 44.083, mean reward:  3.674 [-2.165, 33.000], mean action: 3.917 [0.000, 12.000],  loss: 0.017311, mae: 0.330033, mean_q: 0.506466, mean_eps: 0.000000
 2635/5000: episode: 111, duration: 0.240s, episode steps:  16, steps per second:  67, episode reward: 43.702, mean reward:  2.731 [-2.296, 32.532], mean action: 3.688 [0.000, 15.000],  loss: 0.021229, mae: 0.345941, mean_q: 0.554472, mean_eps: 0.000000
 2654/5000: episode: 112, duration: 0.285s, episode steps:  19, steps per second:  67, episode reward: 41.387, mean reward:  2.178 [-2.479, 30.429], mean action: 4.316 [0.000, 20.000],  loss: 0.017736, mae: 0.328573, mean_q: 0.523904, mean_eps: 0.000000
 2671/5000: episode: 113, duration: 0.245s, episode steps:  17, steps per second:  69, episode reward: -42.000, mean reward: -2.471 [-32.172,  2.300], mean action: 3.941 [0.000, 16.000],  loss: 0.018016, mae: 0.334758, mean_q: 0.555060, mean_eps: 0.000000
 2699/5000: episode: 114, duration: 0.519s, episode steps:  28, steps per second:  54, episode reward: 32.398, mean reward:  1.157 [-2.477, 32.008], mean action: 2.750 [0.000, 12.000],  loss: 0.019889, mae: 0.343363, mean_q: 0.510162, mean_eps: 0.000000
 2721/5000: episode: 115, duration: 0.321s, episode steps:  22, steps per second:  68, episode reward: -32.330, mean reward: -1.470 [-32.004,  2.540], mean action: 3.818 [0.000, 19.000],  loss: 0.020920, mae: 0.359037, mean_q: 0.509098, mean_eps: 0.000000
 2750/5000: episode: 116, duration: 0.446s, episode steps:  29, steps per second:  65, episode reward: -38.600, mean reward: -1.331 [-32.046,  3.060], mean action: 6.552 [0.000, 19.000],  loss: 0.019202, mae: 0.347683, mean_q: 0.515301, mean_eps: 0.000000
 2778/5000: episode: 117, duration: 0.404s, episode steps:  28, steps per second:  69, episode reward: 32.282, mean reward:  1.153 [-3.000, 31.952], mean action: 3.357 [0.000, 12.000],  loss: 0.022095, mae: 0.355803, mean_q: 0.494922, mean_eps: 0.000000
 2793/5000: episode: 118, duration: 0.232s, episode steps:  15, steps per second:  65, episode reward: 38.576, mean reward:  2.572 [-3.000, 32.080], mean action: 3.200 [0.000, 12.000],  loss: 0.017518, mae: 0.328572, mean_q: 0.495881, mean_eps: 0.000000
 2810/5000: episode: 119, duration: 0.248s, episode steps:  17, steps per second:  69, episode reward: 41.229, mean reward:  2.425 [-2.236, 32.390], mean action: 3.882 [0.000, 19.000],  loss: 0.021979, mae: 0.353652, mean_q: 0.529691, mean_eps: 0.000000
 2834/5000: episode: 120, duration: 0.344s, episode steps:  24, steps per second:  70, episode reward: 35.706, mean reward:  1.488 [-2.836, 32.340], mean action: 3.625 [0.000, 19.000],  loss: 0.020803, mae: 0.353555, mean_q: 0.494247, mean_eps: 0.000000
 2854/5000: episode: 121, duration: 0.324s, episode steps:  20, steps per second:  62, episode reward: 38.289, mean reward:  1.914 [-3.000, 32.020], mean action: 4.050 [0.000, 19.000],  loss: 0.025162, mae: 0.382989, mean_q: 0.507326, mean_eps: 0.000000
 2873/5000: episode: 122, duration: 0.291s, episode steps:  19, steps per second:  65, episode reward: 41.709, mean reward:  2.195 [-2.239, 32.619], mean action: 4.947 [0.000, 16.000],  loss: 0.021643, mae: 0.358796, mean_q: 0.498292, mean_eps: 0.000000
 2892/5000: episode: 123, duration: 0.286s, episode steps:  19, steps per second:  66, episode reward: 41.326, mean reward:  2.175 [-2.796, 32.360], mean action: 3.105 [1.000, 16.000],  loss: 0.016896, mae: 0.329345, mean_q: 0.508671, mean_eps: 0.000000
 2914/5000: episode: 124, duration: 0.345s, episode steps:  22, steps per second:  64, episode reward: 35.433, mean reward:  1.611 [-2.504, 31.965], mean action: 4.500 [0.000, 16.000],  loss: 0.016448, mae: 0.318622, mean_q: 0.591828, mean_eps: 0.000000
 2937/5000: episode: 125, duration: 0.334s, episode steps:  23, steps per second:  69, episode reward: 32.751, mean reward:  1.424 [-2.944, 32.011], mean action: 3.174 [0.000, 16.000],  loss: 0.018715, mae: 0.334646, mean_q: 0.570497, mean_eps: 0.000000
 2959/5000: episode: 126, duration: 0.330s, episode steps:  22, steps per second:  67, episode reward: 38.427, mean reward:  1.747 [-2.140, 31.824], mean action: 3.818 [0.000, 13.000],  loss: 0.025176, mae: 0.366221, mean_q: 0.605992, mean_eps: 0.000000
 2974/5000: episode: 127, duration: 0.232s, episode steps:  15, steps per second:  65, episode reward: 40.404, mean reward:  2.694 [-2.156, 31.788], mean action: 6.000 [0.000, 19.000],  loss: 0.023526, mae: 0.376906, mean_q: 0.616242, mean_eps: 0.000000
 3001/5000: episode: 128, duration: 0.396s, episode steps:  27, steps per second:  68, episode reward: -32.550, mean reward: -1.206 [-32.428,  2.715], mean action: 7.037 [0.000, 18.000],  loss: 0.022400, mae: 0.370112, mean_q: 0.616015, mean_eps: 0.000000
 3025/5000: episode: 129, duration: 0.350s, episode steps:  24, steps per second:  68, episode reward: 38.593, mean reward:  1.608 [-2.360, 32.137], mean action: 2.958 [0.000, 15.000],  loss: 0.022986, mae: 0.374101, mean_q: 0.583408, mean_eps: 0.000000
 3049/5000: episode: 130, duration: 0.347s, episode steps:  24, steps per second:  69, episode reward: 32.132, mean reward:  1.339 [-2.607, 31.518], mean action: 3.792 [0.000, 15.000],  loss: 0.023960, mae: 0.377107, mean_q: 0.643714, mean_eps: 0.000000
 3080/5000: episode: 131, duration: 0.442s, episode steps:  31, steps per second:  70, episode reward: -32.810, mean reward: -1.058 [-33.000,  2.240], mean action: 3.742 [0.000, 18.000],  loss: 0.019500, mae: 0.353494, mean_q: 0.631789, mean_eps: 0.000000
 3098/5000: episode: 132, duration: 0.270s, episode steps:  18, steps per second:  67, episode reward: 35.756, mean reward:  1.986 [-3.000, 32.756], mean action: 6.556 [0.000, 14.000],  loss: 0.019067, mae: 0.346708, mean_q: 0.578127, mean_eps: 0.000000
 3128/5000: episode: 133, duration: 0.437s, episode steps:  30, steps per second:  69, episode reward: 41.271, mean reward:  1.376 [-2.131, 32.320], mean action: 3.200 [0.000, 14.000],  loss: 0.020220, mae: 0.346984, mean_q: 0.555653, mean_eps: 0.000000
 3144/5000: episode: 134, duration: 0.240s, episode steps:  16, steps per second:  67, episode reward: 41.018, mean reward:  2.564 [-2.307, 32.493], mean action: 3.938 [1.000, 19.000],  loss: 0.021788, mae: 0.354004, mean_q: 0.563561, mean_eps: 0.000000
 3166/5000: episode: 135, duration: 0.321s, episode steps:  22, steps per second:  68, episode reward: 32.694, mean reward:  1.486 [-3.000, 31.994], mean action: 7.409 [0.000, 19.000],  loss: 0.025974, mae: 0.364548, mean_q: 0.589550, mean_eps: 0.000000
 3201/5000: episode: 136, duration: 0.489s, episode steps:  35, steps per second:  72, episode reward: 32.759, mean reward:  0.936 [-3.000, 32.300], mean action: 4.771 [0.000, 15.000],  loss: 0.023137, mae: 0.360019, mean_q: 0.553641, mean_eps: 0.000000
 3223/5000: episode: 137, duration: 0.317s, episode steps:  22, steps per second:  69, episode reward: 38.170, mean reward:  1.735 [-3.000, 33.000], mean action: 5.273 [1.000, 16.000],  loss: 0.020548, mae: 0.341067, mean_q: 0.520985, mean_eps: 0.000000
 3250/5000: episode: 138, duration: 0.389s, episode steps:  27, steps per second:  69, episode reward: -32.130, mean reward: -1.190 [-31.369,  2.944], mean action: 6.222 [0.000, 16.000],  loss: 0.022533, mae: 0.362258, mean_q: 0.538600, mean_eps: 0.000000
 3271/5000: episode: 139, duration: 0.397s, episode steps:  21, steps per second:  53, episode reward: -35.160, mean reward: -1.674 [-32.460,  2.208], mean action: 5.667 [0.000, 16.000],  loss: 0.023781, mae: 0.365337, mean_q: 0.505077, mean_eps: 0.000000
 3294/5000: episode: 140, duration: 0.339s, episode steps:  23, steps per second:  68, episode reward: 42.000, mean reward:  1.826 [-2.280, 32.260], mean action: 2.652 [0.000, 16.000],  loss: 0.019747, mae: 0.348949, mean_q: 0.545548, mean_eps: 0.000000
 3319/5000: episode: 141, duration: 0.357s, episode steps:  25, steps per second:  70, episode reward: 35.549, mean reward:  1.422 [-2.870, 32.793], mean action: 7.400 [0.000, 18.000],  loss: 0.022755, mae: 0.364059, mean_q: 0.510151, mean_eps: 0.000000
 3335/5000: episode: 142, duration: 0.239s, episode steps:  16, steps per second:  67, episode reward: 35.694, mean reward:  2.231 [-2.420, 32.034], mean action: 4.500 [0.000, 15.000],  loss: 0.019799, mae: 0.358182, mean_q: 0.547922, mean_eps: 0.000000
 3355/5000: episode: 143, duration: 0.292s, episode steps:  20, steps per second:  69, episode reward: 37.973, mean reward:  1.899 [-2.363, 31.857], mean action: 5.900 [0.000, 15.000],  loss: 0.021822, mae: 0.355381, mean_q: 0.555971, mean_eps: 0.000000
 3373/5000: episode: 144, duration: 0.259s, episode steps:  18, steps per second:  70, episode reward: 38.901, mean reward:  2.161 [-2.809, 32.271], mean action: 4.389 [0.000, 19.000],  loss: 0.020035, mae: 0.344040, mean_q: 0.636853, mean_eps: 0.000000
 3394/5000: episode: 145, duration: 0.305s, episode steps:  21, steps per second:  69, episode reward: -35.260, mean reward: -1.679 [-32.070,  2.240], mean action: 5.333 [0.000, 19.000],  loss: 0.023981, mae: 0.361830, mean_q: 0.588895, mean_eps: 0.000000
 3412/5000: episode: 146, duration: 0.261s, episode steps:  18, steps per second:  69, episode reward: 38.574, mean reward:  2.143 [-3.000, 32.260], mean action: 5.278 [0.000, 16.000],  loss: 0.021908, mae: 0.359426, mean_q: 0.591898, mean_eps: 0.000000
 3435/5000: episode: 147, duration: 0.330s, episode steps:  23, steps per second:  70, episode reward: -39.000, mean reward: -1.696 [-32.328,  2.270], mean action: 5.870 [0.000, 16.000],  loss: 0.025390, mae: 0.371681, mean_q: 0.624912, mean_eps: 0.000000
 3471/5000: episode: 148, duration: 0.509s, episode steps:  36, steps per second:  71, episode reward: 32.714, mean reward:  0.909 [-2.704, 32.714], mean action: 3.250 [0.000, 16.000],  loss: 0.019868, mae: 0.342759, mean_q: 0.572256, mean_eps: 0.000000
 3493/5000: episode: 149, duration: 0.308s, episode steps:  22, steps per second:  71, episode reward: -32.350, mean reward: -1.470 [-31.915,  3.000], mean action: 3.591 [0.000, 15.000],  loss: 0.024180, mae: 0.362016, mean_q: 0.496901, mean_eps: 0.000000
 3514/5000: episode: 150, duration: 0.316s, episode steps:  21, steps per second:  67, episode reward: 38.940, mean reward:  1.854 [-2.217, 33.000], mean action: 2.095 [0.000, 9.000],  loss: 0.018662, mae: 0.328872, mean_q: 0.552799, mean_eps: 0.000000
 3542/5000: episode: 151, duration: 0.412s, episode steps:  28, steps per second:  68, episode reward: 35.914, mean reward:  1.283 [-2.496, 32.370], mean action: 3.536 [0.000, 12.000],  loss: 0.018793, mae: 0.329640, mean_q: 0.557095, mean_eps: 0.000000
 3569/5000: episode: 152, duration: 0.396s, episode steps:  27, steps per second:  68, episode reward: -30.000, mean reward: -1.111 [-30.014,  2.750], mean action: 4.741 [0.000, 16.000],  loss: 0.020625, mae: 0.343386, mean_q: 0.567021, mean_eps: 0.000000
 3589/5000: episode: 153, duration: 0.300s, episode steps:  20, steps per second:  67, episode reward: 35.097, mean reward:  1.755 [-2.737, 31.897], mean action: 5.050 [1.000, 16.000],  loss: 0.019770, mae: 0.337192, mean_q: 0.508295, mean_eps: 0.000000
 3620/5000: episode: 154, duration: 0.451s, episode steps:  31, steps per second:  69, episode reward: -39.000, mean reward: -1.258 [-32.112,  2.480], mean action: 11.774 [1.000, 20.000],  loss: 0.021742, mae: 0.348239, mean_q: 0.517994, mean_eps: 0.000000
 3643/5000: episode: 155, duration: 0.331s, episode steps:  23, steps per second:  70, episode reward: 36.000, mean reward:  1.565 [-2.480, 32.380], mean action: 7.261 [0.000, 19.000],  loss: 0.021984, mae: 0.355424, mean_q: 0.509862, mean_eps: 0.000000
 3670/5000: episode: 156, duration: 0.393s, episode steps:  27, steps per second:  69, episode reward: -39.000, mean reward: -1.444 [-32.095,  2.461], mean action: 8.000 [0.000, 20.000],  loss: 0.021656, mae: 0.352553, mean_q: 0.546624, mean_eps: 0.000000
 3678/5000: episode: 157, duration: 0.137s, episode steps:   8, steps per second:  58, episode reward: 47.399, mean reward:  5.925 [ 0.300, 33.000], mean action: 1.875 [0.000, 3.000],  loss: 0.027226, mae: 0.371180, mean_q: 0.594040, mean_eps: 0.000000
 3699/5000: episode: 158, duration: 0.305s, episode steps:  21, steps per second:  69, episode reward: 32.799, mean reward:  1.562 [-2.849, 32.348], mean action: 6.667 [0.000, 16.000],  loss: 0.021430, mae: 0.345931, mean_q: 0.549004, mean_eps: 0.000000
 3713/5000: episode: 159, duration: 0.293s, episode steps:  14, steps per second:  48, episode reward: 41.239, mean reward:  2.946 [-3.000, 31.805], mean action: 4.429 [0.000, 14.000],  loss: 0.022603, mae: 0.350739, mean_q: 0.563023, mean_eps: 0.000000
 3731/5000: episode: 160, duration: 0.308s, episode steps:  18, steps per second:  58, episode reward: 38.244, mean reward:  2.125 [-2.369, 31.891], mean action: 3.556 [0.000, 11.000],  loss: 0.018396, mae: 0.333526, mean_q: 0.540460, mean_eps: 0.000000
 3780/5000: episode: 161, duration: 0.694s, episode steps:  49, steps per second:  71, episode reward: 35.496, mean reward:  0.724 [-2.524, 32.314], mean action: 3.224 [0.000, 20.000],  loss: 0.019720, mae: 0.342411, mean_q: 0.568868, mean_eps: 0.000000
 3809/5000: episode: 162, duration: 0.436s, episode steps:  29, steps per second:  67, episode reward: -33.000, mean reward: -1.138 [-29.915,  2.243], mean action: 4.069 [0.000, 16.000],  loss: 0.025768, mae: 0.368848, mean_q: 0.558698, mean_eps: 0.000000
 3834/5000: episode: 163, duration: 0.362s, episode steps:  25, steps per second:  69, episode reward: 36.000, mean reward:  1.440 [-3.000, 32.030], mean action: 5.080 [0.000, 15.000],  loss: 0.020782, mae: 0.339888, mean_q: 0.572497, mean_eps: 0.000000
 3858/5000: episode: 164, duration: 0.343s, episode steps:  24, steps per second:  70, episode reward: 35.082, mean reward:  1.462 [-2.451, 32.820], mean action: 2.792 [0.000, 16.000],  loss: 0.021535, mae: 0.349958, mean_q: 0.553180, mean_eps: 0.000000
 3883/5000: episode: 165, duration: 0.370s, episode steps:  25, steps per second:  68, episode reward: 32.716, mean reward:  1.309 [-2.517, 32.060], mean action: 4.960 [0.000, 19.000],  loss: 0.022917, mae: 0.356024, mean_q: 0.562542, mean_eps: 0.000000
 3900/5000: episode: 166, duration: 0.249s, episode steps:  17, steps per second:  68, episode reward: 35.208, mean reward:  2.071 [-3.000, 32.840], mean action: 4.294 [0.000, 16.000],  loss: 0.015514, mae: 0.318437, mean_q: 0.566631, mean_eps: 0.000000
 3920/5000: episode: 167, duration: 0.295s, episode steps:  20, steps per second:  68, episode reward: 39.000, mean reward:  1.950 [-2.147, 32.440], mean action: 4.100 [1.000, 15.000],  loss: 0.024134, mae: 0.358640, mean_q: 0.571220, mean_eps: 0.000000
 3940/5000: episode: 168, duration: 0.295s, episode steps:  20, steps per second:  68, episode reward: 41.097, mean reward:  2.055 [-2.349, 32.016], mean action: 3.250 [0.000, 15.000],  loss: 0.018199, mae: 0.328670, mean_q: 0.568402, mean_eps: 0.000000
 3963/5000: episode: 169, duration: 0.366s, episode steps:  23, steps per second:  63, episode reward: 36.000, mean reward:  1.565 [-2.221, 32.420], mean action: 4.261 [0.000, 15.000],  loss: 0.024115, mae: 0.366910, mean_q: 0.511305, mean_eps: 0.000000
 3983/5000: episode: 170, duration: 0.298s, episode steps:  20, steps per second:  67, episode reward: 38.077, mean reward:  1.904 [-2.605, 33.000], mean action: 4.100 [0.000, 16.000],  loss: 0.020939, mae: 0.341860, mean_q: 0.538330, mean_eps: 0.000000
 4014/5000: episode: 171, duration: 0.444s, episode steps:  31, steps per second:  70, episode reward: -35.810, mean reward: -1.155 [-32.900,  2.140], mean action: 5.903 [0.000, 16.000],  loss: 0.020600, mae: 0.339169, mean_q: 0.535414, mean_eps: 0.000000
 4031/5000: episode: 172, duration: 0.248s, episode steps:  17, steps per second:  69, episode reward: 38.115, mean reward:  2.242 [-3.000, 31.738], mean action: 3.882 [0.000, 16.000],  loss: 0.019214, mae: 0.330944, mean_q: 0.488479, mean_eps: 0.000000
 4058/5000: episode: 173, duration: 0.421s, episode steps:  27, steps per second:  64, episode reward: 35.451, mean reward:  1.313 [-3.000, 31.934], mean action: 4.296 [1.000, 15.000],  loss: 0.020728, mae: 0.346605, mean_q: 0.524342, mean_eps: 0.000000
 4080/5000: episode: 174, duration: 0.322s, episode steps:  22, steps per second:  68, episode reward: 38.954, mean reward:  1.771 [-2.314, 32.344], mean action: 3.091 [0.000, 15.000],  loss: 0.020519, mae: 0.346763, mean_q: 0.563543, mean_eps: 0.000000
 4095/5000: episode: 175, duration: 0.227s, episode steps:  15, steps per second:  66, episode reward: 41.192, mean reward:  2.746 [-2.900, 31.910], mean action: 4.400 [0.000, 15.000],  loss: 0.021283, mae: 0.351268, mean_q: 0.531621, mean_eps: 0.000000
 4115/5000: episode: 176, duration: 0.492s, episode steps:  20, steps per second:  41, episode reward: 36.000, mean reward:  1.800 [-3.000, 32.300], mean action: 5.950 [0.000, 15.000],  loss: 0.026049, mae: 0.373421, mean_q: 0.558799, mean_eps: 0.000000
 4141/5000: episode: 177, duration: 0.376s, episode steps:  26, steps per second:  69, episode reward: -38.140, mean reward: -1.467 [-32.051,  2.900], mean action: 8.769 [0.000, 15.000],  loss: 0.022859, mae: 0.354471, mean_q: 0.593165, mean_eps: 0.000000
 4166/5000: episode: 178, duration: 0.351s, episode steps:  25, steps per second:  71, episode reward: 36.000, mean reward:  1.440 [-3.000, 33.000], mean action: 3.240 [0.000, 15.000],  loss: 0.016894, mae: 0.330868, mean_q: 0.613901, mean_eps: 0.000000
 4187/5000: episode: 179, duration: 0.299s, episode steps:  21, steps per second:  70, episode reward: -35.350, mean reward: -1.683 [-32.350,  3.000], mean action: 5.048 [0.000, 15.000],  loss: 0.018210, mae: 0.338902, mean_q: 0.574500, mean_eps: 0.000000
 4209/5000: episode: 180, duration: 0.328s, episode steps:  22, steps per second:  67, episode reward: -37.810, mean reward: -1.719 [-32.790,  2.150], mean action: 6.273 [0.000, 15.000],  loss: 0.022960, mae: 0.363242, mean_q: 0.589513, mean_eps: 0.000000
 4237/5000: episode: 181, duration: 0.401s, episode steps:  28, steps per second:  70, episode reward: -35.320, mean reward: -1.261 [-32.059,  2.640], mean action: 4.143 [0.000, 16.000],  loss: 0.021006, mae: 0.346545, mean_q: 0.499794, mean_eps: 0.000000
 4254/5000: episode: 182, duration: 0.250s, episode steps:  17, steps per second:  68, episode reward: 38.051, mean reward:  2.238 [-2.390, 32.519], mean action: 5.412 [2.000, 16.000],  loss: 0.021020, mae: 0.356582, mean_q: 0.500279, mean_eps: 0.000000
 4284/5000: episode: 183, duration: 0.431s, episode steps:  30, steps per second:  70, episode reward: 35.276, mean reward:  1.176 [-3.000, 32.090], mean action: 3.233 [0.000, 16.000],  loss: 0.020128, mae: 0.354384, mean_q: 0.497429, mean_eps: 0.000000
 4325/5000: episode: 184, duration: 0.564s, episode steps:  41, steps per second:  73, episode reward: 32.567, mean reward:  0.794 [-3.000, 31.914], mean action: 7.268 [0.000, 18.000],  loss: 0.021121, mae: 0.347374, mean_q: 0.528840, mean_eps: 0.000000
 4345/5000: episode: 185, duration: 0.290s, episode steps:  20, steps per second:  69, episode reward: 36.000, mean reward:  1.800 [-3.000, 32.280], mean action: 4.350 [0.000, 12.000],  loss: 0.019565, mae: 0.350424, mean_q: 0.472809, mean_eps: 0.000000
 4370/5000: episode: 186, duration: 0.371s, episode steps:  25, steps per second:  67, episode reward: 41.151, mean reward:  1.646 [-2.237, 33.000], mean action: 3.120 [0.000, 19.000],  loss: 0.018502, mae: 0.339951, mean_q: 0.519679, mean_eps: 0.000000
 4393/5000: episode: 187, duration: 0.341s, episode steps:  23, steps per second:  68, episode reward: 38.409, mean reward:  1.670 [-2.454, 32.200], mean action: 3.348 [0.000, 19.000],  loss: 0.019863, mae: 0.336378, mean_q: 0.559281, mean_eps: 0.000000
 4423/5000: episode: 188, duration: 0.426s, episode steps:  30, steps per second:  70, episode reward: 35.409, mean reward:  1.180 [-2.241, 32.166], mean action: 9.133 [0.000, 20.000],  loss: 0.021140, mae: 0.352422, mean_q: 0.597210, mean_eps: 0.000000
 4440/5000: episode: 189, duration: 0.249s, episode steps:  17, steps per second:  68, episode reward: 39.000, mean reward:  2.294 [-2.366, 32.140], mean action: 3.412 [0.000, 12.000],  loss: 0.022312, mae: 0.358271, mean_q: 0.558256, mean_eps: 0.000000
 4459/5000: episode: 190, duration: 0.288s, episode steps:  19, steps per second:  66, episode reward: 33.000, mean reward:  1.737 [-3.000, 32.700], mean action: 5.000 [0.000, 12.000],  loss: 0.019715, mae: 0.343794, mean_q: 0.512729, mean_eps: 0.000000
 4476/5000: episode: 191, duration: 0.258s, episode steps:  17, steps per second:  66, episode reward: 38.561, mean reward:  2.268 [-2.387, 31.920], mean action: 2.765 [0.000, 12.000],  loss: 0.025504, mae: 0.375606, mean_q: 0.561953, mean_eps: 0.000000
 4500/5000: episode: 192, duration: 0.344s, episode steps:  24, steps per second:  70, episode reward: 32.612, mean reward:  1.359 [-3.000, 31.892], mean action: 3.708 [0.000, 15.000],  loss: 0.018131, mae: 0.342621, mean_q: 0.611795, mean_eps: 0.000000
 4514/5000: episode: 193, duration: 0.215s, episode steps:  14, steps per second:  65, episode reward: 42.000, mean reward:  3.000 [-2.276, 30.485], mean action: 3.143 [0.000, 15.000],  loss: 0.024353, mae: 0.363260, mean_q: 0.563193, mean_eps: 0.000000
 4540/5000: episode: 194, duration: 0.373s, episode steps:  26, steps per second:  70, episode reward: 38.494, mean reward:  1.481 [-2.711, 32.160], mean action: 3.154 [0.000, 15.000],  loss: 0.026639, mae: 0.377800, mean_q: 0.588938, mean_eps: 0.000000
 4564/5000: episode: 195, duration: 0.341s, episode steps:  24, steps per second:  70, episode reward: 35.224, mean reward:  1.468 [-3.000, 32.620], mean action: 5.125 [0.000, 15.000],  loss: 0.021202, mae: 0.361123, mean_q: 0.537615, mean_eps: 0.000000
 4593/5000: episode: 196, duration: 0.404s, episode steps:  29, steps per second:  72, episode reward: 32.229, mean reward:  1.111 [-2.504, 32.210], mean action: 7.448 [0.000, 19.000],  loss: 0.018311, mae: 0.343043, mean_q: 0.528601, mean_eps: 0.000000
 4614/5000: episode: 197, duration: 0.304s, episode steps:  21, steps per second:  69, episode reward: 32.903, mean reward:  1.567 [-3.000, 32.163], mean action: 6.524 [0.000, 19.000],  loss: 0.022579, mae: 0.358368, mean_q: 0.532201, mean_eps: 0.000000
 4637/5000: episode: 198, duration: 0.335s, episode steps:  23, steps per second:  69, episode reward: 32.702, mean reward:  1.422 [-2.286, 31.868], mean action: 6.130 [0.000, 19.000],  loss: 0.024573, mae: 0.373278, mean_q: 0.545038, mean_eps: 0.000000
 4657/5000: episode: 199, duration: 0.292s, episode steps:  20, steps per second:  68, episode reward: 39.000, mean reward:  1.950 [-2.627, 32.130], mean action: 3.600 [0.000, 15.000],  loss: 0.023519, mae: 0.372845, mean_q: 0.544622, mean_eps: 0.000000
 4679/5000: episode: 200, duration: 0.323s, episode steps:  22, steps per second:  68, episode reward: -35.480, mean reward: -1.613 [-33.000,  2.808], mean action: 6.500 [0.000, 19.000],  loss: 0.018916, mae: 0.355796, mean_q: 0.553055, mean_eps: 0.000000
 4701/5000: episode: 201, duration: 0.324s, episode steps:  22, steps per second:  68, episode reward: 32.503, mean reward:  1.477 [-3.000, 32.190], mean action: 5.091 [0.000, 14.000],  loss: 0.019686, mae: 0.346274, mean_q: 0.567183, mean_eps: 0.000000
 4720/5000: episode: 202, duration: 0.276s, episode steps:  19, steps per second:  69, episode reward: 38.472, mean reward:  2.025 [-2.940, 32.350], mean action: 3.684 [0.000, 14.000],  loss: 0.018543, mae: 0.334529, mean_q: 0.581126, mean_eps: 0.000000
 4740/5000: episode: 203, duration: 0.298s, episode steps:  20, steps per second:  67, episode reward: 38.903, mean reward:  1.945 [-3.000, 32.053], mean action: 2.800 [0.000, 15.000],  loss: 0.021150, mae: 0.355875, mean_q: 0.476030, mean_eps: 0.000000
 4769/5000: episode: 204, duration: 0.452s, episode steps:  29, steps per second:  64, episode reward: 33.000, mean reward:  1.138 [-3.000, 32.270], mean action: 6.517 [0.000, 20.000],  loss: 0.020169, mae: 0.350567, mean_q: 0.477493, mean_eps: 0.000000
 4799/5000: episode: 205, duration: 0.425s, episode steps:  30, steps per second:  71, episode reward: -35.080, mean reward: -1.169 [-31.877,  2.711], mean action: 7.967 [0.000, 14.000],  loss: 0.019688, mae: 0.344591, mean_q: 0.502926, mean_eps: 0.000000
 4823/5000: episode: 206, duration: 0.348s, episode steps:  24, steps per second:  69, episode reward: 38.286, mean reward:  1.595 [-2.823, 32.110], mean action: 3.375 [0.000, 13.000],  loss: 0.017610, mae: 0.334792, mean_q: 0.531282, mean_eps: 0.000000
 4845/5000: episode: 207, duration: 0.318s, episode steps:  22, steps per second:  69, episode reward: -32.350, mean reward: -1.470 [-32.136,  2.800], mean action: 4.864 [0.000, 14.000],  loss: 0.016828, mae: 0.326894, mean_q: 0.505701, mean_eps: 0.000000
 4871/5000: episode: 208, duration: 0.373s, episode steps:  26, steps per second:  70, episode reward: 35.828, mean reward:  1.378 [-3.000, 32.503], mean action: 3.154 [0.000, 19.000],  loss: 0.018618, mae: 0.336472, mean_q: 0.511100, mean_eps: 0.000000
 4894/5000: episode: 209, duration: 0.333s, episode steps:  23, steps per second:  69, episode reward: -35.200, mean reward: -1.530 [-32.460,  2.370], mean action: 4.130 [0.000, 19.000],  loss: 0.023320, mae: 0.366573, mean_q: 0.483337, mean_eps: 0.000000
 4935/5000: episode: 210, duration: 0.569s, episode steps:  41, steps per second:  72, episode reward: 32.953, mean reward:  0.804 [-3.000, 32.242], mean action: 6.634 [0.000, 18.000],  loss: 0.019152, mae: 0.342753, mean_q: 0.500726, mean_eps: 0.000000
 4957/5000: episode: 211, duration: 0.327s, episode steps:  22, steps per second:  67, episode reward: -35.080, mean reward: -1.595 [-32.193,  2.416], mean action: 6.955 [0.000, 19.000],  loss: 0.018304, mae: 0.347245, mean_q: 0.474609, mean_eps: 0.000000
 4977/5000: episode: 212, duration: 0.297s, episode steps:  20, steps per second:  67, episode reward: 40.993, mean reward:  2.050 [-2.147, 32.030], mean action: 3.450 [0.000, 15.000],  loss: 0.020569, mae: 0.360085, mean_q: 0.484704, mean_eps: 0.000000
 4996/5000: episode: 213, duration: 0.281s, episode steps:  19, steps per second:  68, episode reward: 38.360, mean reward:  2.019 [-2.479, 32.038], mean action: 5.158 [0.000, 15.000],  loss: 0.025920, mae: 0.385355, mean_q: 0.472021, mean_eps: 0.000000
done, took 66.430 seconds
DQN Evaluation: 9208 victories out of 10778 episodes
Training for 5000 steps ...
   37/5000: episode: 1, duration: 0.287s, episode steps:  37, steps per second: 129, episode reward: 41.521, mean reward:  1.122 [-2.419, 31.928], mean action: 1.270 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   69/5000: episode: 2, duration: 0.215s, episode steps:  32, steps per second: 149, episode reward: 41.426, mean reward:  1.295 [-2.307, 32.200], mean action: 2.031 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   87/5000: episode: 3, duration: 0.132s, episode steps:  18, steps per second: 136, episode reward: 44.527, mean reward:  2.474 [-2.312, 32.300], mean action: 3.389 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  119/5000: episode: 4, duration: 0.201s, episode steps:  32, steps per second: 159, episode reward: 41.962, mean reward:  1.311 [-2.604, 32.330], mean action: 3.406 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  137/5000: episode: 5, duration: 0.126s, episode steps:  18, steps per second: 143, episode reward: 42.000, mean reward:  2.333 [-2.201, 32.940], mean action: 2.778 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  155/5000: episode: 6, duration: 0.133s, episode steps:  18, steps per second: 136, episode reward: 47.845, mean reward:  2.658 [-0.480, 32.422], mean action: 1.222 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  185/5000: episode: 7, duration: 0.200s, episode steps:  30, steps per second: 150, episode reward: 38.619, mean reward:  1.287 [-2.442, 32.412], mean action: 5.733 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  222/5000: episode: 8, duration: 0.242s, episode steps:  37, steps per second: 153, episode reward: 41.902, mean reward:  1.132 [-2.464, 32.079], mean action: 4.973 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  257/5000: episode: 9, duration: 0.219s, episode steps:  35, steps per second: 160, episode reward: 32.828, mean reward:  0.938 [-3.000, 32.112], mean action: 6.229 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  282/5000: episode: 10, duration: 0.170s, episode steps:  25, steps per second: 147, episode reward: 40.990, mean reward:  1.640 [-2.285, 32.230], mean action: 3.720 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  307/5000: episode: 11, duration: 0.183s, episode steps:  25, steps per second: 137, episode reward: 42.000, mean reward:  1.680 [-2.622, 32.170], mean action: 4.640 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  338/5000: episode: 12, duration: 0.206s, episode steps:  31, steps per second: 151, episode reward: 33.000, mean reward:  1.065 [-2.674, 32.430], mean action: 4.903 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  373/5000: episode: 13, duration: 0.236s, episode steps:  35, steps per second: 148, episode reward: 41.671, mean reward:  1.191 [-2.179, 32.107], mean action: 2.229 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  407/5000: episode: 14, duration: 0.214s, episode steps:  34, steps per second: 159, episode reward: 39.000, mean reward:  1.147 [-2.415, 32.690], mean action: 5.000 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  429/5000: episode: 15, duration: 0.154s, episode steps:  22, steps per second: 142, episode reward: 41.434, mean reward:  1.883 [-2.405, 33.000], mean action: 2.909 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  446/5000: episode: 16, duration: 0.126s, episode steps:  17, steps per second: 135, episode reward: 44.127, mean reward:  2.596 [-2.321, 32.190], mean action: 2.647 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  470/5000: episode: 17, duration: 0.157s, episode steps:  24, steps per second: 153, episode reward: 42.000, mean reward:  1.750 [-2.085, 32.110], mean action: 3.667 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  494/5000: episode: 18, duration: 0.162s, episode steps:  24, steps per second: 148, episode reward: 43.518, mean reward:  1.813 [-2.396, 32.175], mean action: 2.542 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  529/5000: episode: 19, duration: 0.224s, episode steps:  35, steps per second: 156, episode reward: 47.164, mean reward:  1.348 [-0.560, 32.080], mean action: 1.971 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  554/5000: episode: 20, duration: 0.178s, episode steps:  25, steps per second: 141, episode reward: 40.894, mean reward:  1.636 [-2.523, 32.030], mean action: 4.360 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  572/5000: episode: 21, duration: 0.149s, episode steps:  18, steps per second: 120, episode reward: 44.704, mean reward:  2.484 [-2.222, 32.400], mean action: 2.944 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  593/5000: episode: 22, duration: 0.157s, episode steps:  21, steps per second: 133, episode reward: 44.506, mean reward:  2.119 [-2.034, 32.536], mean action: 3.905 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  624/5000: episode: 23, duration: 0.207s, episode steps:  31, steps per second: 149, episode reward: 38.855, mean reward:  1.253 [-3.000, 32.582], mean action: 2.323 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  643/5000: episode: 24, duration: 0.136s, episode steps:  19, steps per second: 139, episode reward: 45.000, mean reward:  2.368 [-2.025, 32.550], mean action: 4.684 [2.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  678/5000: episode: 25, duration: 0.237s, episode steps:  35, steps per second: 148, episode reward: 43.093, mean reward:  1.231 [-2.215, 32.050], mean action: 2.457 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  704/5000: episode: 26, duration: 0.173s, episode steps:  26, steps per second: 150, episode reward: 35.601, mean reward:  1.369 [-2.663, 32.290], mean action: 4.038 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  739/5000: episode: 27, duration: 0.218s, episode steps:  35, steps per second: 161, episode reward: 41.615, mean reward:  1.189 [-2.302, 32.760], mean action: 2.943 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  779/5000: episode: 28, duration: 0.268s, episode steps:  40, steps per second: 149, episode reward: 43.809, mean reward:  1.095 [-2.339, 32.017], mean action: 2.500 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  799/5000: episode: 29, duration: 0.151s, episode steps:  20, steps per second: 132, episode reward: 47.208, mean reward:  2.360 [-0.285, 34.532], mean action: 1.250 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  837/5000: episode: 30, duration: 0.250s, episode steps:  38, steps per second: 152, episode reward: 35.131, mean reward:  0.925 [-3.000, 32.023], mean action: 4.658 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  867/5000: episode: 31, duration: 0.244s, episode steps:  30, steps per second: 123, episode reward: 39.000, mean reward:  1.300 [-3.000, 32.350], mean action: 2.633 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  883/5000: episode: 32, duration: 0.125s, episode steps:  16, steps per second: 128, episode reward: 41.564, mean reward:  2.598 [-2.663, 32.034], mean action: 2.688 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  910/5000: episode: 33, duration: 0.178s, episode steps:  27, steps per second: 151, episode reward: 40.408, mean reward:  1.497 [-2.800, 31.759], mean action: 3.889 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  936/5000: episode: 34, duration: 0.157s, episode steps:  26, steps per second: 165, episode reward: 41.681, mean reward:  1.603 [-2.418, 32.413], mean action: 4.615 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  959/5000: episode: 35, duration: 0.161s, episode steps:  23, steps per second: 143, episode reward: 41.160, mean reward:  1.790 [-2.348, 32.020], mean action: 2.652 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  987/5000: episode: 36, duration: 0.182s, episode steps:  28, steps per second: 154, episode reward: 38.419, mean reward:  1.372 [-2.314, 32.283], mean action: 2.286 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1025/5000: episode: 37, duration: 0.434s, episode steps:  38, steps per second:  88, episode reward: 44.365, mean reward:  1.168 [-2.288, 32.170], mean action: 4.263 [0.000, 15.000],  loss: 0.022395, mae: 0.358400, mean_q: 0.492217, mean_eps: 0.000000
 1053/5000: episode: 38, duration: 0.400s, episode steps:  28, steps per second:  70, episode reward: 41.044, mean reward:  1.466 [-3.000, 31.760], mean action: 4.286 [0.000, 20.000],  loss: 0.021160, mae: 0.346287, mean_q: 0.549644, mean_eps: 0.000000
 1070/5000: episode: 39, duration: 0.246s, episode steps:  17, steps per second:  69, episode reward: 41.695, mean reward:  2.453 [-2.413, 32.695], mean action: 4.176 [1.000, 16.000],  loss: 0.020574, mae: 0.340975, mean_q: 0.561878, mean_eps: 0.000000
 1098/5000: episode: 40, duration: 0.408s, episode steps:  28, steps per second:  69, episode reward: 33.000, mean reward:  1.179 [-3.000, 32.210], mean action: 5.464 [0.000, 16.000],  loss: 0.021382, mae: 0.353947, mean_q: 0.528965, mean_eps: 0.000000
 1115/5000: episode: 41, duration: 0.256s, episode steps:  17, steps per second:  66, episode reward: 43.997, mean reward:  2.588 [-2.398, 32.162], mean action: 4.588 [0.000, 16.000],  loss: 0.017167, mae: 0.346612, mean_q: 0.524953, mean_eps: 0.000000
 1160/5000: episode: 42, duration: 0.623s, episode steps:  45, steps per second:  72, episode reward: 38.802, mean reward:  0.862 [-3.000, 32.360], mean action: 3.156 [0.000, 20.000],  loss: 0.022381, mae: 0.357100, mean_q: 0.511580, mean_eps: 0.000000
 1185/5000: episode: 43, duration: 0.594s, episode steps:  25, steps per second:  42, episode reward: 38.498, mean reward:  1.540 [-3.000, 32.310], mean action: 3.920 [1.000, 19.000],  loss: 0.021377, mae: 0.344915, mean_q: 0.565182, mean_eps: 0.000000
 1206/5000: episode: 44, duration: 0.306s, episode steps:  21, steps per second:  69, episode reward: 41.259, mean reward:  1.965 [-3.000, 32.080], mean action: 3.238 [0.000, 11.000],  loss: 0.021832, mae: 0.351301, mean_q: 0.541304, mean_eps: 0.000000
 1242/5000: episode: 45, duration: 0.522s, episode steps:  36, steps per second:  69, episode reward: 32.573, mean reward:  0.905 [-3.000, 32.573], mean action: 3.972 [0.000, 19.000],  loss: 0.023797, mae: 0.357627, mean_q: 0.504092, mean_eps: 0.000000
 1261/5000: episode: 46, duration: 0.292s, episode steps:  19, steps per second:  65, episode reward: 44.817, mean reward:  2.359 [-2.169, 32.220], mean action: 3.158 [0.000, 12.000],  loss: 0.020816, mae: 0.338031, mean_q: 0.565070, mean_eps: 0.000000
 1291/5000: episode: 47, duration: 0.433s, episode steps:  30, steps per second:  69, episode reward: 38.561, mean reward:  1.285 [-2.469, 31.922], mean action: 2.633 [0.000, 16.000],  loss: 0.019990, mae: 0.342288, mean_q: 0.573734, mean_eps: 0.000000
 1316/5000: episode: 48, duration: 0.370s, episode steps:  25, steps per second:  68, episode reward: 38.234, mean reward:  1.529 [-2.642, 31.907], mean action: 4.320 [0.000, 20.000],  loss: 0.021942, mae: 0.353242, mean_q: 0.565542, mean_eps: 0.000000
 1349/5000: episode: 49, duration: 0.463s, episode steps:  33, steps per second:  71, episode reward: 41.461, mean reward:  1.256 [-2.069, 32.350], mean action: 2.939 [0.000, 14.000],  loss: 0.021722, mae: 0.351623, mean_q: 0.517271, mean_eps: 0.000000
 1384/5000: episode: 50, duration: 0.513s, episode steps:  35, steps per second:  68, episode reward: 37.619, mean reward:  1.075 [-2.077, 29.628], mean action: 5.086 [0.000, 20.000],  loss: 0.023581, mae: 0.364266, mean_q: 0.549001, mean_eps: 0.000000
 1413/5000: episode: 51, duration: 0.404s, episode steps:  29, steps per second:  72, episode reward: 39.000, mean reward:  1.345 [-2.309, 32.190], mean action: 3.207 [1.000, 11.000],  loss: 0.022023, mae: 0.343620, mean_q: 0.524571, mean_eps: 0.000000
 1436/5000: episode: 52, duration: 0.335s, episode steps:  23, steps per second:  69, episode reward: 44.478, mean reward:  1.934 [-2.210, 32.310], mean action: 1.261 [0.000, 11.000],  loss: 0.023137, mae: 0.345598, mean_q: 0.544275, mean_eps: 0.000000
 1455/5000: episode: 53, duration: 0.296s, episode steps:  19, steps per second:  64, episode reward: 47.423, mean reward:  2.496 [-0.010, 31.939], mean action: 4.053 [0.000, 12.000],  loss: 0.020897, mae: 0.338345, mean_q: 0.593850, mean_eps: 0.000000
 1473/5000: episode: 54, duration: 0.266s, episode steps:  18, steps per second:  68, episode reward: 42.000, mean reward:  2.333 [-2.569, 32.640], mean action: 1.833 [0.000, 11.000],  loss: 0.021504, mae: 0.335963, mean_q: 0.554025, mean_eps: 0.000000
 1492/5000: episode: 55, duration: 0.292s, episode steps:  19, steps per second:  65, episode reward: 44.107, mean reward:  2.321 [-2.165, 32.289], mean action: 3.474 [0.000, 13.000],  loss: 0.020950, mae: 0.340328, mean_q: 0.461382, mean_eps: 0.000000
 1523/5000: episode: 56, duration: 0.445s, episode steps:  31, steps per second:  70, episode reward: 35.323, mean reward:  1.139 [-3.000, 32.180], mean action: 4.452 [0.000, 15.000],  loss: 0.024572, mae: 0.358833, mean_q: 0.498870, mean_eps: 0.000000
 1551/5000: episode: 57, duration: 0.397s, episode steps:  28, steps per second:  71, episode reward: 39.000, mean reward:  1.393 [-2.773, 32.220], mean action: 3.393 [0.000, 15.000],  loss: 0.020226, mae: 0.332392, mean_q: 0.497061, mean_eps: 0.000000
 1577/5000: episode: 58, duration: 0.381s, episode steps:  26, steps per second:  68, episode reward: 37.268, mean reward:  1.433 [-2.070, 28.670], mean action: 2.154 [0.000, 15.000],  loss: 0.024308, mae: 0.347479, mean_q: 0.536545, mean_eps: 0.000000
 1599/5000: episode: 59, duration: 0.325s, episode steps:  22, steps per second:  68, episode reward: 43.990, mean reward:  2.000 [-3.000, 32.210], mean action: 4.455 [0.000, 16.000],  loss: 0.023565, mae: 0.349058, mean_q: 0.566812, mean_eps: 0.000000
 1624/5000: episode: 60, duration: 0.377s, episode steps:  25, steps per second:  66, episode reward: 44.292, mean reward:  1.772 [-2.147, 31.612], mean action: 2.360 [0.000, 16.000],  loss: 0.023913, mae: 0.359104, mean_q: 0.517408, mean_eps: 0.000000
 1667/5000: episode: 61, duration: 0.610s, episode steps:  43, steps per second:  71, episode reward: 35.035, mean reward:  0.815 [-3.000, 31.908], mean action: 4.628 [0.000, 20.000],  loss: 0.021366, mae: 0.339435, mean_q: 0.530401, mean_eps: 0.000000
 1690/5000: episode: 62, duration: 0.334s, episode steps:  23, steps per second:  69, episode reward: 41.389, mean reward:  1.800 [-2.260, 31.906], mean action: 3.522 [0.000, 12.000],  loss: 0.019632, mae: 0.326627, mean_q: 0.536995, mean_eps: 0.000000
 1722/5000: episode: 63, duration: 0.462s, episode steps:  32, steps per second:  69, episode reward: 32.903, mean reward:  1.028 [-2.514, 32.203], mean action: 5.906 [0.000, 21.000],  loss: 0.020340, mae: 0.336749, mean_q: 0.477355, mean_eps: 0.000000
 1763/5000: episode: 64, duration: 0.585s, episode steps:  41, steps per second:  70, episode reward: 38.988, mean reward:  0.951 [-2.876, 33.000], mean action: 2.585 [1.000, 12.000],  loss: 0.023996, mae: 0.359962, mean_q: 0.520950, mean_eps: 0.000000
 1809/5000: episode: 65, duration: 0.632s, episode steps:  46, steps per second:  73, episode reward: 38.891, mean reward:  0.845 [-2.762, 32.170], mean action: 2.587 [0.000, 19.000],  loss: 0.021468, mae: 0.334865, mean_q: 0.518461, mean_eps: 0.000000
 1826/5000: episode: 66, duration: 0.257s, episode steps:  17, steps per second:  66, episode reward: 42.000, mean reward:  2.471 [-2.081, 30.303], mean action: 0.588 [0.000, 2.000],  loss: 0.020990, mae: 0.337422, mean_q: 0.532579, mean_eps: 0.000000
 1849/5000: episode: 67, duration: 0.343s, episode steps:  23, steps per second:  67, episode reward: 38.015, mean reward:  1.653 [-2.392, 31.596], mean action: 3.043 [0.000, 12.000],  loss: 0.021205, mae: 0.343397, mean_q: 0.535868, mean_eps: 0.000000
 1881/5000: episode: 68, duration: 0.462s, episode steps:  32, steps per second:  69, episode reward: 41.169, mean reward:  1.287 [-2.196, 32.570], mean action: 3.156 [0.000, 16.000],  loss: 0.020844, mae: 0.341744, mean_q: 0.570555, mean_eps: 0.000000
 1912/5000: episode: 69, duration: 0.449s, episode steps:  31, steps per second:  69, episode reward: 39.000, mean reward:  1.258 [-2.309, 32.770], mean action: 3.194 [0.000, 12.000],  loss: 0.016739, mae: 0.322222, mean_q: 0.548582, mean_eps: 0.000000
 1943/5000: episode: 70, duration: 0.429s, episode steps:  31, steps per second:  72, episode reward: -37.590, mean reward: -1.213 [-33.000,  3.000], mean action: 4.419 [0.000, 12.000],  loss: 0.018522, mae: 0.329030, mean_q: 0.545911, mean_eps: 0.000000
 1962/5000: episode: 71, duration: 0.278s, episode steps:  19, steps per second:  68, episode reward: 47.281, mean reward:  2.488 [-0.060, 32.290], mean action: 2.053 [1.000, 3.000],  loss: 0.022780, mae: 0.352501, mean_q: 0.563764, mean_eps: 0.000000
 2003/5000: episode: 72, duration: 0.576s, episode steps:  41, steps per second:  71, episode reward: 35.212, mean reward:  0.859 [-3.000, 32.410], mean action: 4.780 [0.000, 18.000],  loss: 0.023066, mae: 0.363441, mean_q: 0.580091, mean_eps: 0.000000
 2028/5000: episode: 73, duration: 0.363s, episode steps:  25, steps per second:  69, episode reward: 40.885, mean reward:  1.635 [-3.000, 32.292], mean action: 2.640 [0.000, 19.000],  loss: 0.024279, mae: 0.365326, mean_q: 0.629982, mean_eps: 0.000000
 2061/5000: episode: 74, duration: 0.488s, episode steps:  33, steps per second:  68, episode reward: 44.690, mean reward:  1.354 [-2.053, 32.140], mean action: 2.576 [0.000, 8.000],  loss: 0.017671, mae: 0.330177, mean_q: 0.616036, mean_eps: 0.000000
 2119/5000: episode: 75, duration: 0.790s, episode steps:  58, steps per second:  73, episode reward: 32.814, mean reward:  0.566 [-2.910, 32.180], mean action: 4.983 [0.000, 19.000],  loss: 0.021661, mae: 0.343104, mean_q: 0.586151, mean_eps: 0.000000
 2142/5000: episode: 76, duration: 0.341s, episode steps:  23, steps per second:  67, episode reward: 38.110, mean reward:  1.657 [-2.407, 32.002], mean action: 4.435 [0.000, 16.000],  loss: 0.021950, mae: 0.350471, mean_q: 0.536996, mean_eps: 0.000000
 2173/5000: episode: 77, duration: 0.442s, episode steps:  31, steps per second:  70, episode reward: 36.000, mean reward:  1.161 [-2.498, 32.120], mean action: 4.194 [0.000, 19.000],  loss: 0.021984, mae: 0.348561, mean_q: 0.555880, mean_eps: 0.000000
 2204/5000: episode: 78, duration: 0.441s, episode steps:  31, steps per second:  70, episode reward: 47.775, mean reward:  1.541 [-0.433, 32.090], mean action: 2.355 [1.000, 19.000],  loss: 0.019906, mae: 0.335054, mean_q: 0.560318, mean_eps: 0.000000
 2221/5000: episode: 79, duration: 0.254s, episode steps:  17, steps per second:  67, episode reward: 43.344, mean reward:  2.550 [-3.000, 33.000], mean action: 3.412 [0.000, 12.000],  loss: 0.016598, mae: 0.325313, mean_q: 0.525706, mean_eps: 0.000000
 2242/5000: episode: 80, duration: 0.305s, episode steps:  21, steps per second:  69, episode reward: 44.594, mean reward:  2.124 [-2.560, 32.080], mean action: 2.762 [0.000, 13.000],  loss: 0.024916, mae: 0.370698, mean_q: 0.549582, mean_eps: 0.000000
 2274/5000: episode: 81, duration: 0.470s, episode steps:  32, steps per second:  68, episode reward: 35.599, mean reward:  1.112 [-2.617, 32.080], mean action: 3.344 [0.000, 19.000],  loss: 0.019495, mae: 0.340180, mean_q: 0.534707, mean_eps: 0.000000
 2299/5000: episode: 82, duration: 0.368s, episode steps:  25, steps per second:  68, episode reward: 35.905, mean reward:  1.436 [-2.690, 32.105], mean action: 4.520 [0.000, 19.000],  loss: 0.018619, mae: 0.324950, mean_q: 0.509772, mean_eps: 0.000000
 2334/5000: episode: 83, duration: 0.687s, episode steps:  35, steps per second:  51, episode reward: 41.444, mean reward:  1.184 [-2.178, 32.200], mean action: 2.743 [0.000, 19.000],  loss: 0.021984, mae: 0.333615, mean_q: 0.498297, mean_eps: 0.000000
 2356/5000: episode: 84, duration: 0.348s, episode steps:  22, steps per second:  63, episode reward: 41.840, mean reward:  1.902 [-2.882, 32.200], mean action: 3.409 [1.000, 19.000],  loss: 0.019178, mae: 0.325138, mean_q: 0.514495, mean_eps: 0.000000
 2403/5000: episode: 85, duration: 0.664s, episode steps:  47, steps per second:  71, episode reward: 38.579, mean reward:  0.821 [-2.215, 32.740], mean action: 3.830 [0.000, 19.000],  loss: 0.021397, mae: 0.341370, mean_q: 0.508053, mean_eps: 0.000000
 2421/5000: episode: 86, duration: 0.268s, episode steps:  18, steps per second:  67, episode reward: 46.492, mean reward:  2.583 [-0.313, 32.580], mean action: 6.278 [0.000, 14.000],  loss: 0.021022, mae: 0.336126, mean_q: 0.571472, mean_eps: 0.000000
 2438/5000: episode: 87, duration: 0.256s, episode steps:  17, steps per second:  66, episode reward: 44.472, mean reward:  2.616 [-2.333, 33.000], mean action: 4.059 [2.000, 14.000],  loss: 0.025395, mae: 0.353580, mean_q: 0.508406, mean_eps: 0.000000
 2473/5000: episode: 88, duration: 0.507s, episode steps:  35, steps per second:  69, episode reward: 38.203, mean reward:  1.092 [-2.576, 32.253], mean action: 4.343 [0.000, 19.000],  loss: 0.020555, mae: 0.335220, mean_q: 0.583935, mean_eps: 0.000000
 2505/5000: episode: 89, duration: 0.452s, episode steps:  32, steps per second:  71, episode reward: 41.121, mean reward:  1.285 [-3.000, 31.688], mean action: 1.969 [0.000, 11.000],  loss: 0.018611, mae: 0.324470, mean_q: 0.539498, mean_eps: 0.000000
 2532/5000: episode: 90, duration: 0.388s, episode steps:  27, steps per second:  70, episode reward: 44.509, mean reward:  1.648 [-2.106, 32.770], mean action: 2.148 [0.000, 12.000],  loss: 0.021242, mae: 0.331334, mean_q: 0.562685, mean_eps: 0.000000
 2546/5000: episode: 91, duration: 0.238s, episode steps:  14, steps per second:  59, episode reward: 42.000, mean reward:  3.000 [-2.650, 32.370], mean action: 4.143 [0.000, 16.000],  loss: 0.019713, mae: 0.331818, mean_q: 0.579610, mean_eps: 0.000000
 2577/5000: episode: 92, duration: 0.445s, episode steps:  31, steps per second:  70, episode reward: 41.569, mean reward:  1.341 [-2.614, 32.154], mean action: 5.484 [0.000, 16.000],  loss: 0.018289, mae: 0.320645, mean_q: 0.532696, mean_eps: 0.000000
 2615/5000: episode: 93, duration: 0.537s, episode steps:  38, steps per second:  71, episode reward: 35.996, mean reward:  0.947 [-2.938, 32.006], mean action: 4.421 [0.000, 16.000],  loss: 0.021638, mae: 0.341634, mean_q: 0.593939, mean_eps: 0.000000
 2638/5000: episode: 94, duration: 0.338s, episode steps:  23, steps per second:  68, episode reward: 46.687, mean reward:  2.030 [-0.136, 32.044], mean action: 1.957 [0.000, 12.000],  loss: 0.022625, mae: 0.355708, mean_q: 0.629811, mean_eps: 0.000000
 2661/5000: episode: 95, duration: 0.607s, episode steps:  23, steps per second:  38, episode reward: 46.858, mean reward:  2.037 [-0.220, 32.110], mean action: 3.696 [0.000, 12.000],  loss: 0.019338, mae: 0.348536, mean_q: 0.606811, mean_eps: 0.000000
 2696/5000: episode: 96, duration: 0.500s, episode steps:  35, steps per second:  70, episode reward: 35.026, mean reward:  1.001 [-3.000, 32.060], mean action: 3.457 [0.000, 19.000],  loss: 0.023877, mae: 0.357755, mean_q: 0.580593, mean_eps: 0.000000
 2724/5000: episode: 97, duration: 0.410s, episode steps:  28, steps per second:  68, episode reward: 43.304, mean reward:  1.547 [-2.803, 31.549], mean action: 5.643 [0.000, 19.000],  loss: 0.021867, mae: 0.349203, mean_q: 0.620851, mean_eps: 0.000000
 2749/5000: episode: 98, duration: 0.361s, episode steps:  25, steps per second:  69, episode reward: 41.779, mean reward:  1.671 [-2.807, 32.254], mean action: 2.640 [0.000, 12.000],  loss: 0.019617, mae: 0.342208, mean_q: 0.634446, mean_eps: 0.000000
 2772/5000: episode: 99, duration: 0.335s, episode steps:  23, steps per second:  69, episode reward: 35.364, mean reward:  1.538 [-3.000, 31.654], mean action: 2.913 [0.000, 12.000],  loss: 0.017242, mae: 0.327802, mean_q: 0.606181, mean_eps: 0.000000
 2792/5000: episode: 100, duration: 0.315s, episode steps:  20, steps per second:  63, episode reward: 42.000, mean reward:  2.100 [-2.203, 32.180], mean action: 3.650 [0.000, 19.000],  loss: 0.022307, mae: 0.352481, mean_q: 0.566064, mean_eps: 0.000000
 2816/5000: episode: 101, duration: 0.478s, episode steps:  24, steps per second:  50, episode reward: -35.630, mean reward: -1.485 [-32.319,  2.456], mean action: 7.208 [0.000, 19.000],  loss: 0.020704, mae: 0.344461, mean_q: 0.560691, mean_eps: 0.000000
 2835/5000: episode: 102, duration: 0.294s, episode steps:  19, steps per second:  65, episode reward: 41.712, mean reward:  2.195 [-2.958, 32.500], mean action: 2.842 [0.000, 19.000],  loss: 0.024059, mae: 0.362133, mean_q: 0.583107, mean_eps: 0.000000
 2861/5000: episode: 103, duration: 0.377s, episode steps:  26, steps per second:  69, episode reward: 38.160, mean reward:  1.468 [-3.000, 32.070], mean action: 5.423 [0.000, 19.000],  loss: 0.021425, mae: 0.353335, mean_q: 0.552578, mean_eps: 0.000000
 2879/5000: episode: 104, duration: 0.260s, episode steps:  18, steps per second:  69, episode reward: 41.568, mean reward:  2.309 [-2.779, 32.190], mean action: 4.222 [0.000, 16.000],  loss: 0.019852, mae: 0.343232, mean_q: 0.553000, mean_eps: 0.000000
 2947/5000: episode: 105, duration: 0.982s, episode steps:  68, steps per second:  69, episode reward: 33.000, mean reward:  0.485 [-2.379, 32.624], mean action: 4.559 [0.000, 16.000],  loss: 0.019611, mae: 0.340608, mean_q: 0.596110, mean_eps: 0.000000
 2985/5000: episode: 106, duration: 0.560s, episode steps:  38, steps per second:  68, episode reward: -34.570, mean reward: -0.910 [-32.470,  2.250], mean action: 5.211 [0.000, 20.000],  loss: 0.020405, mae: 0.345710, mean_q: 0.583697, mean_eps: 0.000000
 3009/5000: episode: 107, duration: 0.342s, episode steps:  24, steps per second:  70, episode reward: 39.000, mean reward:  1.625 [-2.466, 32.090], mean action: 2.125 [0.000, 9.000],  loss: 0.020349, mae: 0.346249, mean_q: 0.598533, mean_eps: 0.000000
 3037/5000: episode: 108, duration: 0.389s, episode steps:  28, steps per second:  72, episode reward: 40.629, mean reward:  1.451 [-3.000, 32.250], mean action: 4.607 [0.000, 14.000],  loss: 0.021039, mae: 0.347464, mean_q: 0.610525, mean_eps: 0.000000
 3073/5000: episode: 109, duration: 0.501s, episode steps:  36, steps per second:  72, episode reward: 38.224, mean reward:  1.062 [-2.552, 32.130], mean action: 3.444 [0.000, 16.000],  loss: 0.018803, mae: 0.326670, mean_q: 0.566074, mean_eps: 0.000000
 3091/5000: episode: 110, duration: 0.264s, episode steps:  18, steps per second:  68, episode reward: 47.046, mean reward:  2.614 [-0.389, 32.140], mean action: 2.889 [0.000, 14.000],  loss: 0.028497, mae: 0.383566, mean_q: 0.567721, mean_eps: 0.000000
 3109/5000: episode: 111, duration: 0.321s, episode steps:  18, steps per second:  56, episode reward: 44.840, mean reward:  2.491 [-2.004, 33.000], mean action: 3.444 [0.000, 19.000],  loss: 0.020164, mae: 0.350763, mean_q: 0.614413, mean_eps: 0.000000
 3148/5000: episode: 112, duration: 0.540s, episode steps:  39, steps per second:  72, episode reward: 38.805, mean reward:  0.995 [-2.725, 32.003], mean action: 4.179 [0.000, 19.000],  loss: 0.022182, mae: 0.352235, mean_q: 0.577809, mean_eps: 0.000000
 3171/5000: episode: 113, duration: 0.327s, episode steps:  23, steps per second:  70, episode reward: 48.000, mean reward:  2.087 [-0.403, 32.100], mean action: 1.435 [0.000, 14.000],  loss: 0.020842, mae: 0.347600, mean_q: 0.618731, mean_eps: 0.000000
 3191/5000: episode: 114, duration: 0.295s, episode steps:  20, steps per second:  68, episode reward: 40.903, mean reward:  2.045 [-2.361, 32.910], mean action: 5.600 [0.000, 19.000],  loss: 0.017367, mae: 0.334944, mean_q: 0.593280, mean_eps: 0.000000
 3265/5000: episode: 115, duration: 1.113s, episode steps:  74, steps per second:  67, episode reward: 43.626, mean reward:  0.590 [-2.018, 32.270], mean action: 6.135 [0.000, 19.000],  loss: 0.021589, mae: 0.351450, mean_q: 0.535925, mean_eps: 0.000000
 3294/5000: episode: 116, duration: 0.409s, episode steps:  29, steps per second:  71, episode reward: 41.327, mean reward:  1.425 [-2.296, 32.270], mean action: 4.793 [0.000, 20.000],  loss: 0.022514, mae: 0.349375, mean_q: 0.568279, mean_eps: 0.000000
 3317/5000: episode: 117, duration: 0.338s, episode steps:  23, steps per second:  68, episode reward: 41.725, mean reward:  1.814 [-2.485, 32.360], mean action: 2.000 [0.000, 12.000],  loss: 0.022705, mae: 0.346584, mean_q: 0.558981, mean_eps: 0.000000
 3360/5000: episode: 118, duration: 0.590s, episode steps:  43, steps per second:  73, episode reward: -32.590, mean reward: -0.758 [-32.055,  2.501], mean action: 4.721 [0.000, 19.000],  loss: 0.019029, mae: 0.337181, mean_q: 0.497088, mean_eps: 0.000000
 3389/5000: episode: 119, duration: 0.424s, episode steps:  29, steps per second:  68, episode reward: 43.265, mean reward:  1.492 [-2.565, 32.210], mean action: 2.931 [0.000, 12.000],  loss: 0.019688, mae: 0.337039, mean_q: 0.525672, mean_eps: 0.000000
 3415/5000: episode: 120, duration: 0.364s, episode steps:  26, steps per second:  71, episode reward: 35.922, mean reward:  1.382 [-3.000, 32.100], mean action: 3.962 [0.000, 20.000],  loss: 0.018999, mae: 0.344618, mean_q: 0.546718, mean_eps: 0.000000
 3436/5000: episode: 121, duration: 0.304s, episode steps:  21, steps per second:  69, episode reward: 38.803, mean reward:  1.848 [-2.387, 32.713], mean action: 2.952 [0.000, 16.000],  loss: 0.020243, mae: 0.336666, mean_q: 0.537921, mean_eps: 0.000000
 3452/5000: episode: 122, duration: 0.235s, episode steps:  16, steps per second:  68, episode reward: 39.000, mean reward:  2.438 [-2.485, 30.686], mean action: 3.125 [1.000, 11.000],  loss: 0.020791, mae: 0.341426, mean_q: 0.513666, mean_eps: 0.000000
 3479/5000: episode: 123, duration: 0.390s, episode steps:  27, steps per second:  69, episode reward: 44.253, mean reward:  1.639 [-2.409, 32.030], mean action: 1.185 [0.000, 11.000],  loss: 0.022131, mae: 0.343098, mean_q: 0.545419, mean_eps: 0.000000
 3509/5000: episode: 124, duration: 0.421s, episode steps:  30, steps per second:  71, episode reward: 38.464, mean reward:  1.282 [-2.268, 32.170], mean action: 6.167 [0.000, 21.000],  loss: 0.021548, mae: 0.337117, mean_q: 0.573079, mean_eps: 0.000000
 3537/5000: episode: 125, duration: 0.399s, episode steps:  28, steps per second:  70, episode reward: 44.129, mean reward:  1.576 [-2.094, 32.770], mean action: 3.286 [0.000, 19.000],  loss: 0.022629, mae: 0.337756, mean_q: 0.555365, mean_eps: 0.000000
 3565/5000: episode: 126, duration: 0.404s, episode steps:  28, steps per second:  69, episode reward: 47.288, mean reward:  1.689 [-0.500, 32.140], mean action: 2.714 [0.000, 14.000],  loss: 0.018930, mae: 0.327974, mean_q: 0.495019, mean_eps: 0.000000
 3589/5000: episode: 127, duration: 0.339s, episode steps:  24, steps per second:  71, episode reward: 38.903, mean reward:  1.621 [-2.483, 32.673], mean action: 5.125 [1.000, 15.000],  loss: 0.020618, mae: 0.332911, mean_q: 0.531216, mean_eps: 0.000000
 3623/5000: episode: 128, duration: 0.493s, episode steps:  34, steps per second:  69, episode reward: 43.411, mean reward:  1.277 [-2.401, 32.403], mean action: 1.559 [0.000, 13.000],  loss: 0.019751, mae: 0.324958, mean_q: 0.569075, mean_eps: 0.000000
 3644/5000: episode: 129, duration: 0.303s, episode steps:  21, steps per second:  69, episode reward: 39.000, mean reward:  1.857 [-2.963, 32.310], mean action: 3.524 [0.000, 15.000],  loss: 0.022542, mae: 0.344213, mean_q: 0.527476, mean_eps: 0.000000
 3669/5000: episode: 130, duration: 0.348s, episode steps:  25, steps per second:  72, episode reward: 38.648, mean reward:  1.546 [-2.704, 32.580], mean action: 3.600 [0.000, 15.000],  loss: 0.023248, mae: 0.346776, mean_q: 0.567526, mean_eps: 0.000000
 3715/5000: episode: 131, duration: 0.631s, episode steps:  46, steps per second:  73, episode reward: 41.288, mean reward:  0.898 [-2.226, 32.093], mean action: 2.087 [0.000, 15.000],  loss: 0.023089, mae: 0.355105, mean_q: 0.555157, mean_eps: 0.000000
 3741/5000: episode: 132, duration: 0.386s, episode steps:  26, steps per second:  67, episode reward: 38.234, mean reward:  1.471 [-2.819, 31.757], mean action: 2.808 [0.000, 15.000],  loss: 0.019900, mae: 0.350436, mean_q: 0.520654, mean_eps: 0.000000
 3767/5000: episode: 133, duration: 0.370s, episode steps:  26, steps per second:  70, episode reward: 36.000, mean reward:  1.385 [-2.821, 32.380], mean action: 5.769 [0.000, 21.000],  loss: 0.021060, mae: 0.352893, mean_q: 0.563607, mean_eps: 0.000000
 3794/5000: episode: 134, duration: 0.382s, episode steps:  27, steps per second:  71, episode reward: 41.836, mean reward:  1.549 [-2.721, 32.250], mean action: 3.630 [0.000, 15.000],  loss: 0.016696, mae: 0.331026, mean_q: 0.567341, mean_eps: 0.000000
 3838/5000: episode: 135, duration: 0.622s, episode steps:  44, steps per second:  71, episode reward: 35.224, mean reward:  0.801 [-2.329, 32.020], mean action: 5.136 [0.000, 15.000],  loss: 0.021053, mae: 0.345631, mean_q: 0.527670, mean_eps: 0.000000
 3867/5000: episode: 136, duration: 0.430s, episode steps:  29, steps per second:  67, episode reward: 37.798, mean reward:  1.303 [-3.000, 32.360], mean action: 4.621 [0.000, 15.000],  loss: 0.018804, mae: 0.338119, mean_q: 0.531203, mean_eps: 0.000000
 3900/5000: episode: 137, duration: 0.632s, episode steps:  33, steps per second:  52, episode reward: 41.255, mean reward:  1.250 [-2.381, 32.120], mean action: 4.606 [0.000, 14.000],  loss: 0.019266, mae: 0.338875, mean_q: 0.504865, mean_eps: 0.000000
 3925/5000: episode: 138, duration: 0.355s, episode steps:  25, steps per second:  70, episode reward: 41.519, mean reward:  1.661 [-2.208, 32.130], mean action: 0.800 [0.000, 6.000],  loss: 0.018757, mae: 0.326236, mean_q: 0.523090, mean_eps: 0.000000
 3946/5000: episode: 139, duration: 0.303s, episode steps:  21, steps per second:  69, episode reward: 41.159, mean reward:  1.960 [-2.396, 32.585], mean action: 1.667 [0.000, 6.000],  loss: 0.021074, mae: 0.337119, mean_q: 0.503564, mean_eps: 0.000000
 3970/5000: episode: 140, duration: 0.361s, episode steps:  24, steps per second:  66, episode reward: 41.652, mean reward:  1.735 [-2.264, 32.030], mean action: 2.667 [0.000, 16.000],  loss: 0.020386, mae: 0.334267, mean_q: 0.520355, mean_eps: 0.000000
 3991/5000: episode: 141, duration: 0.397s, episode steps:  21, steps per second:  53, episode reward: 38.807, mean reward:  1.848 [-2.832, 32.087], mean action: 3.571 [0.000, 19.000],  loss: 0.020817, mae: 0.344321, mean_q: 0.493202, mean_eps: 0.000000
 4020/5000: episode: 142, duration: 0.526s, episode steps:  29, steps per second:  55, episode reward: 35.593, mean reward:  1.227 [-3.000, 32.310], mean action: 5.000 [0.000, 16.000],  loss: 0.022378, mae: 0.350388, mean_q: 0.532107, mean_eps: 0.000000
 4041/5000: episode: 143, duration: 0.309s, episode steps:  21, steps per second:  68, episode reward: 41.091, mean reward:  1.957 [-2.801, 31.321], mean action: 2.524 [0.000, 6.000],  loss: 0.019382, mae: 0.336596, mean_q: 0.525544, mean_eps: 0.000000
 4073/5000: episode: 144, duration: 0.457s, episode steps:  32, steps per second:  70, episode reward: 39.000, mean reward:  1.219 [-3.000, 32.200], mean action: 4.469 [0.000, 16.000],  loss: 0.019654, mae: 0.330166, mean_q: 0.536806, mean_eps: 0.000000
 4119/5000: episode: 145, duration: 0.630s, episode steps:  46, steps per second:  73, episode reward: 35.903, mean reward:  0.780 [-2.488, 32.060], mean action: 4.978 [0.000, 16.000],  loss: 0.018867, mae: 0.334816, mean_q: 0.558009, mean_eps: 0.000000
 4149/5000: episode: 146, duration: 0.421s, episode steps:  30, steps per second:  71, episode reward: 41.197, mean reward:  1.373 [-2.071, 31.512], mean action: 2.033 [1.000, 16.000],  loss: 0.022558, mae: 0.347390, mean_q: 0.477789, mean_eps: 0.000000
 4166/5000: episode: 147, duration: 0.249s, episode steps:  17, steps per second:  68, episode reward: 41.581, mean reward:  2.446 [-2.759, 32.590], mean action: 2.706 [0.000, 16.000],  loss: 0.025018, mae: 0.350690, mean_q: 0.500992, mean_eps: 0.000000
 4188/5000: episode: 148, duration: 0.318s, episode steps:  22, steps per second:  69, episode reward: 42.000, mean reward:  1.909 [-2.649, 32.570], mean action: 2.727 [0.000, 16.000],  loss: 0.023707, mae: 0.343573, mean_q: 0.510386, mean_eps: 0.000000
 4231/5000: episode: 149, duration: 0.592s, episode steps:  43, steps per second:  73, episode reward: 41.477, mean reward:  0.965 [-2.360, 32.328], mean action: 3.070 [0.000, 20.000],  loss: 0.017953, mae: 0.326140, mean_q: 0.463157, mean_eps: 0.000000
 4274/5000: episode: 150, duration: 0.601s, episode steps:  43, steps per second:  71, episode reward: 41.070, mean reward:  0.955 [-2.418, 32.153], mean action: 2.512 [0.000, 19.000],  loss: 0.022362, mae: 0.347892, mean_q: 0.510074, mean_eps: 0.000000
 4301/5000: episode: 151, duration: 0.382s, episode steps:  27, steps per second:  71, episode reward: 36.000, mean reward:  1.333 [-2.600, 32.330], mean action: 3.889 [0.000, 16.000],  loss: 0.025817, mae: 0.358619, mean_q: 0.552822, mean_eps: 0.000000
 4320/5000: episode: 152, duration: 0.279s, episode steps:  19, steps per second:  68, episode reward: 46.132, mean reward:  2.428 [-0.322, 32.070], mean action: 3.579 [0.000, 15.000],  loss: 0.023208, mae: 0.347296, mean_q: 0.557151, mean_eps: 0.000000
 4342/5000: episode: 153, duration: 0.321s, episode steps:  22, steps per second:  69, episode reward: 44.659, mean reward:  2.030 [-2.321, 32.490], mean action: 1.636 [1.000, 3.000],  loss: 0.019790, mae: 0.333866, mean_q: 0.576457, mean_eps: 0.000000
 4372/5000: episode: 154, duration: 0.424s, episode steps:  30, steps per second:  71, episode reward: 37.573, mean reward:  1.252 [-2.578, 32.343], mean action: 2.933 [0.000, 16.000],  loss: 0.022685, mae: 0.352625, mean_q: 0.545453, mean_eps: 0.000000
 4409/5000: episode: 155, duration: 0.519s, episode steps:  37, steps per second:  71, episode reward: 38.558, mean reward:  1.042 [-2.307, 32.290], mean action: 2.216 [0.000, 14.000],  loss: 0.019697, mae: 0.333143, mean_q: 0.522055, mean_eps: 0.000000
 4456/5000: episode: 156, duration: 0.657s, episode steps:  47, steps per second:  72, episode reward: 41.019, mean reward:  0.873 [-2.007, 31.696], mean action: 3.702 [0.000, 13.000],  loss: 0.022413, mae: 0.345515, mean_q: 0.560253, mean_eps: 0.000000
 4473/5000: episode: 157, duration: 0.249s, episode steps:  17, steps per second:  68, episode reward: 40.890, mean reward:  2.405 [-2.569, 32.320], mean action: 2.765 [0.000, 12.000],  loss: 0.019506, mae: 0.323900, mean_q: 0.492619, mean_eps: 0.000000
 4499/5000: episode: 158, duration: 0.373s, episode steps:  26, steps per second:  70, episode reward: 46.835, mean reward:  1.801 [-0.180, 32.089], mean action: 3.423 [0.000, 14.000],  loss: 0.023085, mae: 0.357307, mean_q: 0.510012, mean_eps: 0.000000
 4532/5000: episode: 159, duration: 0.490s, episode steps:  33, steps per second:  67, episode reward: 33.000, mean reward:  1.000 [-3.000, 32.340], mean action: 3.879 [0.000, 15.000],  loss: 0.023084, mae: 0.354856, mean_q: 0.525765, mean_eps: 0.000000
 4549/5000: episode: 160, duration: 0.250s, episode steps:  17, steps per second:  68, episode reward: 41.317, mean reward:  2.430 [-2.998, 32.260], mean action: 3.588 [0.000, 19.000],  loss: 0.018033, mae: 0.334790, mean_q: 0.524213, mean_eps: 0.000000
 4576/5000: episode: 161, duration: 0.384s, episode steps:  27, steps per second:  70, episode reward: 41.077, mean reward:  1.521 [-2.249, 31.873], mean action: 2.074 [0.000, 3.000],  loss: 0.020342, mae: 0.341002, mean_q: 0.588512, mean_eps: 0.000000
 4598/5000: episode: 162, duration: 0.316s, episode steps:  22, steps per second:  70, episode reward: 38.713, mean reward:  1.760 [-3.000, 32.350], mean action: 3.455 [0.000, 15.000],  loss: 0.022206, mae: 0.345661, mean_q: 0.634164, mean_eps: 0.000000
 4642/5000: episode: 163, duration: 0.618s, episode steps:  44, steps per second:  71, episode reward: 32.779, mean reward:  0.745 [-2.388, 32.662], mean action: 3.545 [0.000, 15.000],  loss: 0.021649, mae: 0.347290, mean_q: 0.569044, mean_eps: 0.000000
 4657/5000: episode: 164, duration: 0.223s, episode steps:  15, steps per second:  67, episode reward: 47.736, mean reward:  3.182 [ 0.000, 32.056], mean action: 0.867 [0.000, 3.000],  loss: 0.018264, mae: 0.343504, mean_q: 0.435949, mean_eps: 0.000000
 4678/5000: episode: 165, duration: 0.304s, episode steps:  21, steps per second:  69, episode reward: 44.752, mean reward:  2.131 [-2.095, 32.200], mean action: 2.619 [0.000, 14.000],  loss: 0.020636, mae: 0.351805, mean_q: 0.499810, mean_eps: 0.000000
 4719/5000: episode: 166, duration: 0.574s, episode steps:  41, steps per second:  71, episode reward: -32.550, mean reward: -0.794 [-32.184,  3.000], mean action: 7.585 [0.000, 20.000],  loss: 0.018799, mae: 0.328852, mean_q: 0.497805, mean_eps: 0.000000
 4748/5000: episode: 167, duration: 0.400s, episode steps:  29, steps per second:  72, episode reward: 39.000, mean reward:  1.345 [-2.792, 32.020], mean action: 3.793 [0.000, 18.000],  loss: 0.018454, mae: 0.321467, mean_q: 0.521021, mean_eps: 0.000000
 4766/5000: episode: 168, duration: 0.263s, episode steps:  18, steps per second:  69, episode reward: 41.494, mean reward:  2.305 [-2.285, 32.350], mean action: 3.722 [0.000, 12.000],  loss: 0.019202, mae: 0.328566, mean_q: 0.584013, mean_eps: 0.000000
 4791/5000: episode: 169, duration: 0.357s, episode steps:  25, steps per second:  70, episode reward: 35.539, mean reward:  1.422 [-2.750, 32.013], mean action: 3.440 [0.000, 15.000],  loss: 0.020403, mae: 0.331723, mean_q: 0.540405, mean_eps: 0.000000
 4820/5000: episode: 170, duration: 0.405s, episode steps:  29, steps per second:  72, episode reward: 35.718, mean reward:  1.232 [-3.000, 32.710], mean action: 7.138 [1.000, 15.000],  loss: 0.019118, mae: 0.323638, mean_q: 0.503314, mean_eps: 0.000000
 4849/5000: episode: 171, duration: 0.411s, episode steps:  29, steps per second:  71, episode reward: 44.900, mean reward:  1.548 [-2.476, 32.590], mean action: 2.897 [0.000, 9.000],  loss: 0.020445, mae: 0.331680, mean_q: 0.517872, mean_eps: 0.000000
 4874/5000: episode: 172, duration: 0.357s, episode steps:  25, steps per second:  70, episode reward: 39.000, mean reward:  1.560 [-2.803, 32.693], mean action: 2.400 [0.000, 16.000],  loss: 0.021031, mae: 0.330967, mean_q: 0.550317, mean_eps: 0.000000
 4899/5000: episode: 173, duration: 0.355s, episode steps:  25, steps per second:  70, episode reward: 43.490, mean reward:  1.740 [-2.216, 32.360], mean action: 3.880 [0.000, 19.000],  loss: 0.021217, mae: 0.342615, mean_q: 0.562978, mean_eps: 0.000000
 4930/5000: episode: 174, duration: 0.432s, episode steps:  31, steps per second:  72, episode reward: 41.765, mean reward:  1.347 [-3.000, 32.030], mean action: 2.484 [0.000, 16.000],  loss: 0.023566, mae: 0.355240, mean_q: 0.551893, mean_eps: 0.000000
 4952/5000: episode: 175, duration: 0.328s, episode steps:  22, steps per second:  67, episode reward: 44.311, mean reward:  2.014 [-2.742, 32.025], mean action: 4.182 [0.000, 14.000],  loss: 0.020329, mae: 0.330908, mean_q: 0.596360, mean_eps: 0.000000
 4981/5000: episode: 176, duration: 0.413s, episode steps:  29, steps per second:  70, episode reward: 43.994, mean reward:  1.517 [-2.118, 32.410], mean action: 3.379 [1.000, 20.000],  loss: 0.020835, mae: 0.328448, mean_q: 0.562989, mean_eps: 0.000000
 4993/5000: episode: 177, duration: 0.189s, episode steps:  12, steps per second:  64, episode reward: 44.047, mean reward:  3.671 [-2.165, 32.250], mean action: 4.417 [0.000, 19.000],  loss: 0.021586, mae: 0.339044, mean_q: 0.517673, mean_eps: 0.000000
done, took 65.710 seconds
DQN Evaluation: 9381 victories out of 10956 episodes
Training for 5000 steps ...
   29/5000: episode: 1, duration: 0.240s, episode steps:  29, steps per second: 121, episode reward: 32.467, mean reward:  1.120 [-3.000, 32.437], mean action: 7.310 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   53/5000: episode: 2, duration: 0.182s, episode steps:  24, steps per second: 132, episode reward: -32.790, mean reward: -1.366 [-31.875,  2.310], mean action: 5.875 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   78/5000: episode: 3, duration: 0.168s, episode steps:  25, steps per second: 149, episode reward: 35.901, mean reward:  1.436 [-3.000, 32.291], mean action: 3.880 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  105/5000: episode: 4, duration: 0.178s, episode steps:  27, steps per second: 152, episode reward: -35.370, mean reward: -1.310 [-33.000,  2.570], mean action: 7.667 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  131/5000: episode: 5, duration: 0.173s, episode steps:  26, steps per second: 151, episode reward: 33.000, mean reward:  1.269 [-3.000, 32.050], mean action: 4.000 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  160/5000: episode: 6, duration: 0.194s, episode steps:  29, steps per second: 149, episode reward: 32.886, mean reward:  1.134 [-3.000, 31.916], mean action: 3.655 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  186/5000: episode: 7, duration: 0.170s, episode steps:  26, steps per second: 153, episode reward: 35.353, mean reward:  1.360 [-2.310, 32.410], mean action: 3.385 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  203/5000: episode: 8, duration: 0.128s, episode steps:  17, steps per second: 133, episode reward: 37.996, mean reward:  2.235 [-3.000, 32.738], mean action: 7.588 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  217/5000: episode: 9, duration: 0.107s, episode steps:  14, steps per second: 131, episode reward: 47.047, mean reward:  3.361 [ 0.149, 32.210], mean action: 1.571 [1.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  242/5000: episode: 10, duration: 0.165s, episode steps:  25, steps per second: 152, episode reward: -32.810, mean reward: -1.312 [-32.085,  2.910], mean action: 8.360 [1.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  265/5000: episode: 11, duration: 0.158s, episode steps:  23, steps per second: 146, episode reward: -32.230, mean reward: -1.401 [-31.966,  2.418], mean action: 4.652 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  279/5000: episode: 12, duration: 0.104s, episode steps:  14, steps per second: 135, episode reward: 41.583, mean reward:  2.970 [-2.234, 31.963], mean action: 3.857 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  299/5000: episode: 13, duration: 0.135s, episode steps:  20, steps per second: 149, episode reward: -41.040, mean reward: -2.052 [-32.698,  2.343], mean action: 7.000 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  324/5000: episode: 14, duration: 0.160s, episode steps:  25, steps per second: 156, episode reward: 32.553, mean reward:  1.302 [-3.000, 32.523], mean action: 4.920 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  353/5000: episode: 15, duration: 0.189s, episode steps:  29, steps per second: 153, episode reward: -32.260, mean reward: -1.112 [-32.395,  2.843], mean action: 4.690 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  401/5000: episode: 16, duration: 0.303s, episode steps:  48, steps per second: 159, episode reward: -32.910, mean reward: -0.686 [-32.308,  2.320], mean action: 10.333 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  424/5000: episode: 17, duration: 0.150s, episode steps:  23, steps per second: 154, episode reward: 35.330, mean reward:  1.536 [-2.337, 32.020], mean action: 8.435 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  435/5000: episode: 18, duration: 0.093s, episode steps:  11, steps per second: 119, episode reward: 44.790, mean reward:  4.072 [-2.128, 33.000], mean action: 4.091 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  459/5000: episode: 19, duration: 0.160s, episode steps:  24, steps per second: 150, episode reward: -35.570, mean reward: -1.482 [-32.294,  2.218], mean action: 8.125 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  482/5000: episode: 20, duration: 0.151s, episode steps:  23, steps per second: 152, episode reward: 36.000, mean reward:  1.565 [-2.562, 32.220], mean action: 5.435 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  503/5000: episode: 21, duration: 0.139s, episode steps:  21, steps per second: 151, episode reward: 32.806, mean reward:  1.562 [-3.000, 32.903], mean action: 5.571 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  531/5000: episode: 22, duration: 0.184s, episode steps:  28, steps per second: 152, episode reward: -37.950, mean reward: -1.355 [-32.203,  2.790], mean action: 10.071 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  547/5000: episode: 23, duration: 0.110s, episode steps:  16, steps per second: 145, episode reward: 41.476, mean reward:  2.592 [-2.732, 32.051], mean action: 3.562 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  566/5000: episode: 24, duration: 0.129s, episode steps:  19, steps per second: 148, episode reward: -35.420, mean reward: -1.864 [-32.900,  2.901], mean action: 6.737 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  592/5000: episode: 25, duration: 0.170s, episode steps:  26, steps per second: 153, episode reward: -32.770, mean reward: -1.260 [-31.837,  2.360], mean action: 4.000 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  615/5000: episode: 26, duration: 0.150s, episode steps:  23, steps per second: 153, episode reward: 35.580, mean reward:  1.547 [-2.189, 32.902], mean action: 4.000 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  626/5000: episode: 27, duration: 0.094s, episode steps:  11, steps per second: 117, episode reward: 42.000, mean reward:  3.818 [-2.403, 32.700], mean action: 4.818 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  652/5000: episode: 28, duration: 0.169s, episode steps:  26, steps per second: 153, episode reward: 32.179, mean reward:  1.238 [-3.000, 32.125], mean action: 9.577 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  672/5000: episode: 29, duration: 0.138s, episode steps:  20, steps per second: 145, episode reward: 32.733, mean reward:  1.637 [-2.679, 32.173], mean action: 5.100 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  691/5000: episode: 30, duration: 0.128s, episode steps:  19, steps per second: 148, episode reward: 30.000, mean reward:  1.579 [-3.000, 30.097], mean action: 6.684 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  723/5000: episode: 31, duration: 0.200s, episode steps:  32, steps per second: 160, episode reward: 32.823, mean reward:  1.026 [-2.900, 32.100], mean action: 6.281 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  736/5000: episode: 32, duration: 0.102s, episode steps:  13, steps per second: 127, episode reward: 44.382, mean reward:  3.414 [-2.368, 32.220], mean action: 4.538 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  757/5000: episode: 33, duration: 0.137s, episode steps:  21, steps per second: 153, episode reward: 34.650, mean reward:  1.650 [-3.000, 31.959], mean action: 8.667 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  777/5000: episode: 34, duration: 0.142s, episode steps:  20, steps per second: 141, episode reward: 38.740, mean reward:  1.937 [-2.820, 32.030], mean action: 3.950 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  798/5000: episode: 35, duration: 0.169s, episode steps:  21, steps per second: 124, episode reward: -30.000, mean reward: -1.429 [-30.000,  2.950], mean action: 8.286 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  822/5000: episode: 36, duration: 0.156s, episode steps:  24, steps per second: 154, episode reward: 34.834, mean reward:  1.451 [-2.503, 32.886], mean action: 6.333 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  838/5000: episode: 37, duration: 0.112s, episode steps:  16, steps per second: 142, episode reward: 38.132, mean reward:  2.383 [-2.687, 32.162], mean action: 8.062 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  868/5000: episode: 38, duration: 0.186s, episode steps:  30, steps per second: 162, episode reward: 35.059, mean reward:  1.169 [-2.220, 32.400], mean action: 7.933 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  894/5000: episode: 39, duration: 0.169s, episode steps:  26, steps per second: 153, episode reward: 35.087, mean reward:  1.349 [-2.667, 32.396], mean action: 6.846 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  916/5000: episode: 40, duration: 0.149s, episode steps:  22, steps per second: 148, episode reward: 38.132, mean reward:  1.733 [-2.426, 32.310], mean action: 4.818 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  949/5000: episode: 41, duration: 0.210s, episode steps:  33, steps per second: 157, episode reward: 38.769, mean reward:  1.175 [-2.339, 32.200], mean action: 6.030 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  960/5000: episode: 42, duration: 0.091s, episode steps:  11, steps per second: 121, episode reward: 41.847, mean reward:  3.804 [-3.000, 32.710], mean action: 4.091 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  989/5000: episode: 43, duration: 0.187s, episode steps:  29, steps per second: 155, episode reward: -36.000, mean reward: -1.241 [-32.319,  2.340], mean action: 7.310 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1004/5000: episode: 44, duration: 0.223s, episode steps:  15, steps per second:  67, episode reward: 41.288, mean reward:  2.753 [-2.236, 32.288], mean action: 3.667 [0.000, 19.000],  loss: 0.034491, mae: 0.388227, mean_q: 0.518894, mean_eps: 0.000000
 1041/5000: episode: 45, duration: 0.626s, episode steps:  37, steps per second:  59, episode reward: 35.212, mean reward:  0.952 [-3.000, 32.199], mean action: 8.459 [0.000, 21.000],  loss: 0.020610, mae: 0.333945, mean_q: 0.565728, mean_eps: 0.000000
 1056/5000: episode: 46, duration: 0.233s, episode steps:  15, steps per second:  64, episode reward: 41.065, mean reward:  2.738 [-2.253, 32.400], mean action: 4.933 [0.000, 19.000],  loss: 0.018061, mae: 0.321766, mean_q: 0.531849, mean_eps: 0.000000
 1091/5000: episode: 47, duration: 0.484s, episode steps:  35, steps per second:  72, episode reward: 32.631, mean reward:  0.932 [-2.774, 32.110], mean action: 8.029 [0.000, 19.000],  loss: 0.017540, mae: 0.327704, mean_q: 0.498933, mean_eps: 0.000000
 1112/5000: episode: 48, duration: 0.301s, episode steps:  21, steps per second:  70, episode reward: 38.589, mean reward:  1.838 [-2.332, 32.150], mean action: 2.429 [0.000, 11.000],  loss: 0.020966, mae: 0.333643, mean_q: 0.508343, mean_eps: 0.000000
 1136/5000: episode: 49, duration: 0.344s, episode steps:  24, steps per second:  70, episode reward: 36.000, mean reward:  1.500 [-2.900, 32.120], mean action: 3.417 [0.000, 11.000],  loss: 0.020354, mae: 0.332009, mean_q: 0.506802, mean_eps: 0.000000
 1156/5000: episode: 50, duration: 0.296s, episode steps:  20, steps per second:  68, episode reward: 32.156, mean reward:  1.608 [-3.000, 31.941], mean action: 4.850 [0.000, 12.000],  loss: 0.020317, mae: 0.336240, mean_q: 0.551514, mean_eps: 0.000000
 1174/5000: episode: 51, duration: 0.258s, episode steps:  18, steps per second:  70, episode reward: -41.090, mean reward: -2.283 [-31.991,  2.191], mean action: 5.611 [0.000, 16.000],  loss: 0.019063, mae: 0.331802, mean_q: 0.508948, mean_eps: 0.000000
 1205/5000: episode: 52, duration: 0.427s, episode steps:  31, steps per second:  73, episode reward: -35.050, mean reward: -1.131 [-32.184,  2.430], mean action: 7.935 [0.000, 16.000],  loss: 0.020749, mae: 0.344246, mean_q: 0.520997, mean_eps: 0.000000
 1228/5000: episode: 53, duration: 0.329s, episode steps:  23, steps per second:  70, episode reward: 41.325, mean reward:  1.797 [-2.251, 32.500], mean action: 3.826 [0.000, 14.000],  loss: 0.018170, mae: 0.329252, mean_q: 0.492332, mean_eps: 0.000000
 1247/5000: episode: 54, duration: 0.279s, episode steps:  19, steps per second:  68, episode reward: 38.894, mean reward:  2.047 [-2.374, 32.894], mean action: 3.895 [0.000, 11.000],  loss: 0.022030, mae: 0.337754, mean_q: 0.501848, mean_eps: 0.000000
 1266/5000: episode: 55, duration: 0.275s, episode steps:  19, steps per second:  69, episode reward: 38.019, mean reward:  2.001 [-2.188, 32.079], mean action: 4.211 [0.000, 15.000],  loss: 0.019554, mae: 0.325975, mean_q: 0.524823, mean_eps: 0.000000
 1294/5000: episode: 56, duration: 0.388s, episode steps:  28, steps per second:  72, episode reward: -32.170, mean reward: -1.149 [-31.925,  2.910], mean action: 6.250 [0.000, 20.000],  loss: 0.021188, mae: 0.329509, mean_q: 0.546242, mean_eps: 0.000000
 1313/5000: episode: 57, duration: 0.289s, episode steps:  19, steps per second:  66, episode reward: 37.786, mean reward:  1.989 [-3.000, 32.616], mean action: 4.947 [0.000, 16.000],  loss: 0.021721, mae: 0.335934, mean_q: 0.542747, mean_eps: 0.000000
 1332/5000: episode: 58, duration: 0.270s, episode steps:  19, steps per second:  70, episode reward: 35.752, mean reward:  1.882 [-2.579, 32.901], mean action: 6.789 [0.000, 16.000],  loss: 0.019050, mae: 0.322263, mean_q: 0.556578, mean_eps: 0.000000
 1358/5000: episode: 59, duration: 0.372s, episode steps:  26, steps per second:  70, episode reward: 32.683, mean reward:  1.257 [-2.415, 32.910], mean action: 5.577 [1.000, 19.000],  loss: 0.024231, mae: 0.352695, mean_q: 0.516564, mean_eps: 0.000000
 1376/5000: episode: 60, duration: 0.269s, episode steps:  18, steps per second:  67, episode reward: 37.859, mean reward:  2.103 [-2.560, 32.137], mean action: 3.778 [0.000, 12.000],  loss: 0.022823, mae: 0.344968, mean_q: 0.526121, mean_eps: 0.000000
 1390/5000: episode: 61, duration: 0.212s, episode steps:  14, steps per second:  66, episode reward: 44.423, mean reward:  3.173 [-2.572, 32.515], mean action: 2.714 [1.000, 14.000],  loss: 0.017280, mae: 0.314130, mean_q: 0.520497, mean_eps: 0.000000
 1413/5000: episode: 62, duration: 0.325s, episode steps:  23, steps per second:  71, episode reward: 35.453, mean reward:  1.541 [-2.466, 32.344], mean action: 4.957 [0.000, 14.000],  loss: 0.020426, mae: 0.333800, mean_q: 0.521111, mean_eps: 0.000000
 1435/5000: episode: 63, duration: 0.323s, episode steps:  22, steps per second:  68, episode reward: 38.291, mean reward:  1.740 [-2.140, 32.679], mean action: 2.727 [0.000, 9.000],  loss: 0.020134, mae: 0.325983, mean_q: 0.508149, mean_eps: 0.000000
 1455/5000: episode: 64, duration: 0.293s, episode steps:  20, steps per second:  68, episode reward: 38.092, mean reward:  1.905 [-2.609, 32.380], mean action: 2.950 [0.000, 12.000],  loss: 0.020262, mae: 0.328022, mean_q: 0.465427, mean_eps: 0.000000
 1472/5000: episode: 65, duration: 0.249s, episode steps:  17, steps per second:  68, episode reward: 38.119, mean reward:  2.242 [-2.458, 32.070], mean action: 2.588 [0.000, 14.000],  loss: 0.022558, mae: 0.340875, mean_q: 0.465647, mean_eps: 0.000000
 1497/5000: episode: 66, duration: 0.357s, episode steps:  25, steps per second:  70, episode reward: 35.892, mean reward:  1.436 [-2.483, 32.892], mean action: 2.960 [0.000, 16.000],  loss: 0.022241, mae: 0.339162, mean_q: 0.546057, mean_eps: 0.000000
 1522/5000: episode: 67, duration: 0.367s, episode steps:  25, steps per second:  68, episode reward: 35.429, mean reward:  1.417 [-2.768, 32.780], mean action: 3.320 [0.000, 16.000],  loss: 0.020155, mae: 0.329899, mean_q: 0.538264, mean_eps: 0.000000
 1554/5000: episode: 68, duration: 0.451s, episode steps:  32, steps per second:  71, episode reward: 32.102, mean reward:  1.003 [-3.000, 32.210], mean action: 5.562 [0.000, 14.000],  loss: 0.018866, mae: 0.327583, mean_q: 0.528497, mean_eps: 0.000000
 1583/5000: episode: 69, duration: 0.409s, episode steps:  29, steps per second:  71, episode reward: 35.343, mean reward:  1.219 [-2.497, 32.253], mean action: 5.448 [0.000, 16.000],  loss: 0.019988, mae: 0.335877, mean_q: 0.560850, mean_eps: 0.000000
 1600/5000: episode: 70, duration: 0.246s, episode steps:  17, steps per second:  69, episode reward: 39.000, mean reward:  2.294 [-2.277, 32.390], mean action: 4.059 [1.000, 16.000],  loss: 0.024198, mae: 0.357238, mean_q: 0.583978, mean_eps: 0.000000
 1620/5000: episode: 71, duration: 0.286s, episode steps:  20, steps per second:  70, episode reward: 38.320, mean reward:  1.916 [-2.248, 32.310], mean action: 3.100 [0.000, 12.000],  loss: 0.019195, mae: 0.320421, mean_q: 0.528815, mean_eps: 0.000000
 1642/5000: episode: 72, duration: 0.320s, episode steps:  22, steps per second:  69, episode reward: 37.850, mean reward:  1.720 [-2.667, 31.780], mean action: 4.727 [0.000, 20.000],  loss: 0.019998, mae: 0.326618, mean_q: 0.543686, mean_eps: 0.000000
 1665/5000: episode: 73, duration: 0.323s, episode steps:  23, steps per second:  71, episode reward: 32.719, mean reward:  1.423 [-2.843, 33.000], mean action: 7.435 [0.000, 19.000],  loss: 0.016626, mae: 0.316178, mean_q: 0.510059, mean_eps: 0.000000
 1690/5000: episode: 74, duration: 0.351s, episode steps:  25, steps per second:  71, episode reward: -32.790, mean reward: -1.312 [-32.698,  2.542], mean action: 5.360 [0.000, 20.000],  loss: 0.020914, mae: 0.331061, mean_q: 0.472861, mean_eps: 0.000000
 1705/5000: episode: 75, duration: 0.222s, episode steps:  15, steps per second:  68, episode reward: 41.646, mean reward:  2.776 [-2.438, 32.200], mean action: 1.267 [0.000, 6.000],  loss: 0.020215, mae: 0.329094, mean_q: 0.432080, mean_eps: 0.000000
 1730/5000: episode: 76, duration: 0.343s, episode steps:  25, steps per second:  73, episode reward: -42.000, mean reward: -1.680 [-32.181,  2.910], mean action: 11.040 [1.000, 20.000],  loss: 0.022505, mae: 0.335464, mean_q: 0.453051, mean_eps: 0.000000
 1752/5000: episode: 77, duration: 0.316s, episode steps:  22, steps per second:  70, episode reward: 38.308, mean reward:  1.741 [-2.476, 32.140], mean action: 3.318 [0.000, 14.000],  loss: 0.020976, mae: 0.323082, mean_q: 0.481777, mean_eps: 0.000000
 1776/5000: episode: 78, duration: 0.348s, episode steps:  24, steps per second:  69, episode reward: 35.243, mean reward:  1.468 [-2.414, 33.000], mean action: 3.750 [0.000, 16.000],  loss: 0.021100, mae: 0.326096, mean_q: 0.497470, mean_eps: 0.000000
 1798/5000: episode: 79, duration: 0.312s, episode steps:  22, steps per second:  71, episode reward: -33.000, mean reward: -1.500 [-33.000,  2.903], mean action: 6.364 [0.000, 19.000],  loss: 0.021629, mae: 0.330066, mean_q: 0.487735, mean_eps: 0.000000
 1819/5000: episode: 80, duration: 0.301s, episode steps:  21, steps per second:  70, episode reward: 34.906, mean reward:  1.662 [-2.393, 32.080], mean action: 6.619 [1.000, 15.000],  loss: 0.021563, mae: 0.325301, mean_q: 0.506213, mean_eps: 0.000000
 1835/5000: episode: 81, duration: 0.231s, episode steps:  16, steps per second:  69, episode reward: 35.580, mean reward:  2.224 [-3.000, 31.620], mean action: 4.438 [0.000, 15.000],  loss: 0.019293, mae: 0.314716, mean_q: 0.513127, mean_eps: 0.000000
 1855/5000: episode: 82, duration: 0.299s, episode steps:  20, steps per second:  67, episode reward: 35.718, mean reward:  1.786 [-2.277, 33.000], mean action: 5.800 [0.000, 15.000],  loss: 0.024157, mae: 0.341885, mean_q: 0.546857, mean_eps: 0.000000
 1869/5000: episode: 83, duration: 0.213s, episode steps:  14, steps per second:  66, episode reward: 41.043, mean reward:  2.932 [-2.469, 32.390], mean action: 3.143 [0.000, 12.000],  loss: 0.018271, mae: 0.315492, mean_q: 0.507030, mean_eps: 0.000000
 1890/5000: episode: 84, duration: 0.305s, episode steps:  21, steps per second:  69, episode reward: 36.000, mean reward:  1.714 [-2.628, 32.370], mean action: 3.143 [0.000, 12.000],  loss: 0.020484, mae: 0.319810, mean_q: 0.498477, mean_eps: 0.000000
 1912/5000: episode: 85, duration: 0.324s, episode steps:  22, steps per second:  68, episode reward: -35.730, mean reward: -1.624 [-32.702,  2.576], mean action: 8.091 [0.000, 20.000],  loss: 0.024578, mae: 0.344137, mean_q: 0.519497, mean_eps: 0.000000
 1928/5000: episode: 86, duration: 0.240s, episode steps:  16, steps per second:  67, episode reward: 41.694, mean reward:  2.606 [-2.123, 32.200], mean action: 3.188 [0.000, 12.000],  loss: 0.019440, mae: 0.325356, mean_q: 0.589943, mean_eps: 0.000000
 1946/5000: episode: 87, duration: 0.260s, episode steps:  18, steps per second:  69, episode reward: -32.760, mean reward: -1.820 [-33.000,  3.000], mean action: 8.056 [0.000, 18.000],  loss: 0.014851, mae: 0.302698, mean_q: 0.551896, mean_eps: 0.000000
 1974/5000: episode: 88, duration: 0.390s, episode steps:  28, steps per second:  72, episode reward: 35.536, mean reward:  1.269 [-3.000, 32.900], mean action: 4.964 [0.000, 15.000],  loss: 0.021972, mae: 0.334640, mean_q: 0.574463, mean_eps: 0.000000
 1993/5000: episode: 89, duration: 0.277s, episode steps:  19, steps per second:  69, episode reward: 42.000, mean reward:  2.211 [-2.625, 32.300], mean action: 2.368 [0.000, 12.000],  loss: 0.017771, mae: 0.323235, mean_q: 0.563769, mean_eps: 0.000000
 2021/5000: episode: 90, duration: 0.391s, episode steps:  28, steps per second:  72, episode reward: -33.000, mean reward: -1.179 [-32.014,  3.000], mean action: 2.393 [0.000, 9.000],  loss: 0.016722, mae: 0.314710, mean_q: 0.610899, mean_eps: 0.000000
 2050/5000: episode: 91, duration: 0.558s, episode steps:  29, steps per second:  52, episode reward: 36.000, mean reward:  1.241 [-2.442, 32.100], mean action: 2.069 [0.000, 16.000],  loss: 0.023371, mae: 0.338738, mean_q: 0.593356, mean_eps: 0.000000
 2074/5000: episode: 92, duration: 0.355s, episode steps:  24, steps per second:  68, episode reward: 38.597, mean reward:  1.608 [-3.000, 31.717], mean action: 7.000 [0.000, 16.000],  loss: 0.021339, mae: 0.336320, mean_q: 0.552929, mean_eps: 0.000000
 2091/5000: episode: 93, duration: 0.249s, episode steps:  17, steps per second:  68, episode reward: 39.000, mean reward:  2.294 [-2.964, 32.160], mean action: 5.176 [0.000, 17.000],  loss: 0.022941, mae: 0.348034, mean_q: 0.539569, mean_eps: 0.000000
 2108/5000: episode: 94, duration: 0.248s, episode steps:  17, steps per second:  68, episode reward: 39.000, mean reward:  2.294 [-2.701, 32.170], mean action: 4.765 [0.000, 16.000],  loss: 0.019721, mae: 0.338723, mean_q: 0.620656, mean_eps: 0.000000
 2146/5000: episode: 95, duration: 0.521s, episode steps:  38, steps per second:  73, episode reward: -32.680, mean reward: -0.860 [-32.261,  2.430], mean action: 5.316 [0.000, 16.000],  loss: 0.016429, mae: 0.314319, mean_q: 0.553754, mean_eps: 0.000000
 2162/5000: episode: 96, duration: 0.231s, episode steps:  16, steps per second:  69, episode reward: 38.903, mean reward:  2.431 [-2.536, 32.373], mean action: 3.312 [0.000, 16.000],  loss: 0.020490, mae: 0.330021, mean_q: 0.547832, mean_eps: 0.000000
 2181/5000: episode: 97, duration: 0.272s, episode steps:  19, steps per second:  70, episode reward: 41.252, mean reward:  2.171 [-2.215, 32.552], mean action: 2.737 [0.000, 16.000],  loss: 0.019718, mae: 0.334531, mean_q: 0.458948, mean_eps: 0.000000
 2199/5000: episode: 98, duration: 0.268s, episode steps:  18, steps per second:  67, episode reward: 35.904, mean reward:  1.995 [-3.000, 31.914], mean action: 3.778 [0.000, 15.000],  loss: 0.018816, mae: 0.326590, mean_q: 0.510780, mean_eps: 0.000000
 2219/5000: episode: 99, duration: 0.289s, episode steps:  20, steps per second:  69, episode reward: 35.192, mean reward:  1.760 [-3.000, 32.483], mean action: 6.600 [0.000, 16.000],  loss: 0.019095, mae: 0.336932, mean_q: 0.487686, mean_eps: 0.000000
 2237/5000: episode: 100, duration: 0.262s, episode steps:  18, steps per second:  69, episode reward: 43.688, mean reward:  2.427 [-2.358, 32.463], mean action: 1.611 [0.000, 8.000],  loss: 0.019674, mae: 0.330879, mean_q: 0.474381, mean_eps: 0.000000
 2264/5000: episode: 101, duration: 0.401s, episode steps:  27, steps per second:  67, episode reward: 32.389, mean reward:  1.200 [-3.000, 31.823], mean action: 7.815 [0.000, 18.000],  loss: 0.018898, mae: 0.329575, mean_q: 0.521374, mean_eps: 0.000000
 2283/5000: episode: 102, duration: 0.278s, episode steps:  19, steps per second:  68, episode reward: 40.460, mean reward:  2.129 [-2.420, 32.470], mean action: 4.263 [0.000, 19.000],  loss: 0.023314, mae: 0.340368, mean_q: 0.567382, mean_eps: 0.000000
 2304/5000: episode: 103, duration: 0.305s, episode steps:  21, steps per second:  69, episode reward: 39.000, mean reward:  1.857 [-2.352, 32.560], mean action: 2.762 [0.000, 12.000],  loss: 0.021167, mae: 0.338455, mean_q: 0.579833, mean_eps: 0.000000
 2331/5000: episode: 104, duration: 0.389s, episode steps:  27, steps per second:  69, episode reward: 38.324, mean reward:  1.419 [-3.000, 33.000], mean action: 3.778 [0.000, 12.000],  loss: 0.022483, mae: 0.347084, mean_q: 0.502818, mean_eps: 0.000000
 2357/5000: episode: 105, duration: 0.368s, episode steps:  26, steps per second:  71, episode reward: 35.457, mean reward:  1.364 [-2.239, 32.390], mean action: 6.231 [0.000, 18.000],  loss: 0.019229, mae: 0.336244, mean_q: 0.481876, mean_eps: 0.000000
 2379/5000: episode: 106, duration: 0.312s, episode steps:  22, steps per second:  71, episode reward: 33.000, mean reward:  1.500 [-2.244, 32.430], mean action: 4.500 [0.000, 14.000],  loss: 0.021160, mae: 0.338020, mean_q: 0.464917, mean_eps: 0.000000
 2401/5000: episode: 107, duration: 0.318s, episode steps:  22, steps per second:  69, episode reward: -36.000, mean reward: -1.636 [-32.311,  2.363], mean action: 6.682 [0.000, 16.000],  loss: 0.020671, mae: 0.335479, mean_q: 0.491226, mean_eps: 0.000000
 2422/5000: episode: 108, duration: 0.301s, episode steps:  21, steps per second:  70, episode reward: -38.340, mean reward: -1.826 [-32.177,  2.290], mean action: 6.905 [1.000, 16.000],  loss: 0.021221, mae: 0.339269, mean_q: 0.497353, mean_eps: 0.000000
 2442/5000: episode: 109, duration: 0.316s, episode steps:  20, steps per second:  63, episode reward: 35.112, mean reward:  1.756 [-3.000, 32.903], mean action: 7.250 [0.000, 16.000],  loss: 0.018686, mae: 0.318324, mean_q: 0.499044, mean_eps: 0.000000
 2457/5000: episode: 110, duration: 0.281s, episode steps:  15, steps per second:  53, episode reward: 41.003, mean reward:  2.734 [-3.000, 32.603], mean action: 2.800 [0.000, 12.000],  loss: 0.019053, mae: 0.324960, mean_q: 0.519834, mean_eps: 0.000000
 2473/5000: episode: 111, duration: 0.239s, episode steps:  16, steps per second:  67, episode reward: 38.399, mean reward:  2.400 [-2.576, 32.450], mean action: 3.812 [0.000, 12.000],  loss: 0.023033, mae: 0.356524, mean_q: 0.509861, mean_eps: 0.000000
 2488/5000: episode: 112, duration: 0.226s, episode steps:  15, steps per second:  67, episode reward: 46.508, mean reward:  3.101 [-0.259, 32.220], mean action: 3.933 [0.000, 15.000],  loss: 0.020125, mae: 0.334608, mean_q: 0.472710, mean_eps: 0.000000
 2504/5000: episode: 113, duration: 0.241s, episode steps:  16, steps per second:  67, episode reward: 38.396, mean reward:  2.400 [-2.489, 32.410], mean action: 4.000 [0.000, 19.000],  loss: 0.026639, mae: 0.361708, mean_q: 0.480135, mean_eps: 0.000000
 2530/5000: episode: 114, duration: 0.374s, episode steps:  26, steps per second:  70, episode reward: -32.110, mean reward: -1.235 [-31.964,  2.470], mean action: 4.923 [0.000, 16.000],  loss: 0.016361, mae: 0.316468, mean_q: 0.484582, mean_eps: 0.000000
 2544/5000: episode: 115, duration: 0.212s, episode steps:  14, steps per second:  66, episode reward: 41.319, mean reward:  2.951 [-2.188, 32.320], mean action: 2.357 [0.000, 9.000],  loss: 0.019613, mae: 0.339994, mean_q: 0.519230, mean_eps: 0.000000
 2568/5000: episode: 116, duration: 0.342s, episode steps:  24, steps per second:  70, episode reward: -32.410, mean reward: -1.350 [-32.198,  3.000], mean action: 6.333 [0.000, 15.000],  loss: 0.018295, mae: 0.324404, mean_q: 0.510256, mean_eps: 0.000000
 2589/5000: episode: 117, duration: 0.298s, episode steps:  21, steps per second:  70, episode reward: 35.897, mean reward:  1.709 [-3.000, 32.067], mean action: 3.381 [0.000, 14.000],  loss: 0.021193, mae: 0.338887, mean_q: 0.569040, mean_eps: 0.000000
 2615/5000: episode: 118, duration: 0.391s, episode steps:  26, steps per second:  66, episode reward: 41.759, mean reward:  1.606 [-2.427, 32.080], mean action: 1.808 [0.000, 9.000],  loss: 0.021107, mae: 0.341033, mean_q: 0.584546, mean_eps: 0.000000
 2641/5000: episode: 119, duration: 0.369s, episode steps:  26, steps per second:  71, episode reward: -32.460, mean reward: -1.248 [-31.974,  2.505], mean action: 4.308 [0.000, 19.000],  loss: 0.021297, mae: 0.341930, mean_q: 0.503891, mean_eps: 0.000000
 2664/5000: episode: 120, duration: 0.329s, episode steps:  23, steps per second:  70, episode reward: 33.000, mean reward:  1.435 [-2.901, 29.953], mean action: 3.783 [0.000, 12.000],  loss: 0.020746, mae: 0.329228, mean_q: 0.572920, mean_eps: 0.000000
 2687/5000: episode: 121, duration: 0.335s, episode steps:  23, steps per second:  69, episode reward: -35.170, mean reward: -1.529 [-32.496,  2.528], mean action: 5.652 [0.000, 16.000],  loss: 0.026088, mae: 0.358629, mean_q: 0.585968, mean_eps: 0.000000
 2706/5000: episode: 122, duration: 0.276s, episode steps:  19, steps per second:  69, episode reward: 38.699, mean reward:  2.037 [-2.498, 31.939], mean action: 4.211 [0.000, 16.000],  loss: 0.019643, mae: 0.326115, mean_q: 0.558127, mean_eps: 0.000000
 2726/5000: episode: 123, duration: 0.297s, episode steps:  20, steps per second:  67, episode reward: 38.621, mean reward:  1.931 [-2.039, 32.120], mean action: 4.450 [0.000, 16.000],  loss: 0.017625, mae: 0.318140, mean_q: 0.564734, mean_eps: 0.000000
 2749/5000: episode: 124, duration: 0.346s, episode steps:  23, steps per second:  66, episode reward: 40.432, mean reward:  1.758 [-2.567, 32.290], mean action: 5.696 [1.000, 19.000],  loss: 0.019824, mae: 0.329252, mean_q: 0.535237, mean_eps: 0.000000
 2772/5000: episode: 125, duration: 0.333s, episode steps:  23, steps per second:  69, episode reward: 35.042, mean reward:  1.524 [-3.000, 33.000], mean action: 4.348 [0.000, 19.000],  loss: 0.021263, mae: 0.336541, mean_q: 0.537628, mean_eps: 0.000000
 2797/5000: episode: 126, duration: 0.352s, episode steps:  25, steps per second:  71, episode reward: 35.197, mean reward:  1.408 [-2.843, 32.110], mean action: 8.000 [0.000, 16.000],  loss: 0.021118, mae: 0.340077, mean_q: 0.594004, mean_eps: 0.000000
 2830/5000: episode: 127, duration: 0.460s, episode steps:  33, steps per second:  72, episode reward: 32.830, mean reward:  0.995 [-2.507, 32.256], mean action: 4.758 [0.000, 16.000],  loss: 0.024592, mae: 0.357275, mean_q: 0.546331, mean_eps: 0.000000
 2852/5000: episode: 128, duration: 0.306s, episode steps:  22, steps per second:  72, episode reward: -32.620, mean reward: -1.483 [-33.000,  2.534], mean action: 4.364 [0.000, 16.000],  loss: 0.018393, mae: 0.327741, mean_q: 0.566323, mean_eps: 0.000000
 2867/5000: episode: 129, duration: 0.218s, episode steps:  15, steps per second:  69, episode reward: 39.000, mean reward:  2.600 [-2.351, 32.060], mean action: 5.333 [0.000, 16.000],  loss: 0.021519, mae: 0.336897, mean_q: 0.477915, mean_eps: 0.000000
 2891/5000: episode: 130, duration: 0.345s, episode steps:  24, steps per second:  69, episode reward: 38.360, mean reward:  1.598 [-2.351, 32.670], mean action: 5.042 [0.000, 16.000],  loss: 0.019137, mae: 0.326746, mean_q: 0.524267, mean_eps: 0.000000
 2936/5000: episode: 131, duration: 0.604s, episode steps:  45, steps per second:  74, episode reward: -38.580, mean reward: -0.857 [-32.166,  2.340], mean action: 3.533 [0.000, 16.000],  loss: 0.020464, mae: 0.338093, mean_q: 0.515171, mean_eps: 0.000000
 2950/5000: episode: 132, duration: 0.210s, episode steps:  14, steps per second:  67, episode reward: 41.789, mean reward:  2.985 [-2.386, 32.669], mean action: 1.429 [0.000, 3.000],  loss: 0.018429, mae: 0.325776, mean_q: 0.478522, mean_eps: 0.000000
 2974/5000: episode: 133, duration: 0.340s, episode steps:  24, steps per second:  71, episode reward: 35.210, mean reward:  1.467 [-2.901, 32.290], mean action: 2.625 [0.000, 12.000],  loss: 0.023518, mae: 0.347165, mean_q: 0.483928, mean_eps: 0.000000
 2992/5000: episode: 134, duration: 0.260s, episode steps:  18, steps per second:  69, episode reward: 41.723, mean reward:  2.318 [-2.377, 33.000], mean action: 1.444 [0.000, 12.000],  loss: 0.022938, mae: 0.344154, mean_q: 0.481509, mean_eps: 0.000000
 3015/5000: episode: 135, duration: 0.323s, episode steps:  23, steps per second:  71, episode reward: -38.280, mean reward: -1.664 [-31.984,  2.813], mean action: 9.043 [0.000, 16.000],  loss: 0.020191, mae: 0.327480, mean_q: 0.476625, mean_eps: 0.000000
 3039/5000: episode: 136, duration: 0.338s, episode steps:  24, steps per second:  71, episode reward: 35.766, mean reward:  1.490 [-3.000, 32.100], mean action: 2.500 [0.000, 16.000],  loss: 0.021614, mae: 0.334333, mean_q: 0.562930, mean_eps: 0.000000
 3071/5000: episode: 137, duration: 0.449s, episode steps:  32, steps per second:  71, episode reward: -32.820, mean reward: -1.026 [-31.863,  2.287], mean action: 4.281 [0.000, 18.000],  loss: 0.021760, mae: 0.339629, mean_q: 0.515666, mean_eps: 0.000000
 3092/5000: episode: 138, duration: 0.300s, episode steps:  21, steps per second:  70, episode reward: 35.726, mean reward:  1.701 [-2.497, 31.846], mean action: 3.048 [0.000, 9.000],  loss: 0.024395, mae: 0.353720, mean_q: 0.505178, mean_eps: 0.000000
 3119/5000: episode: 139, duration: 0.378s, episode steps:  27, steps per second:  71, episode reward: -32.930, mean reward: -1.220 [-32.093,  2.903], mean action: 3.778 [0.000, 9.000],  loss: 0.020263, mae: 0.331341, mean_q: 0.477440, mean_eps: 0.000000
 3139/5000: episode: 140, duration: 0.301s, episode steps:  20, steps per second:  66, episode reward: 41.220, mean reward:  2.061 [-2.760, 31.830], mean action: 2.200 [0.000, 9.000],  loss: 0.022371, mae: 0.336933, mean_q: 0.501084, mean_eps: 0.000000
 3168/5000: episode: 141, duration: 0.409s, episode steps:  29, steps per second:  71, episode reward: 32.812, mean reward:  1.131 [-2.475, 32.030], mean action: 4.690 [0.000, 14.000],  loss: 0.020164, mae: 0.320842, mean_q: 0.524196, mean_eps: 0.000000
 3190/5000: episode: 142, duration: 0.428s, episode steps:  22, steps per second:  51, episode reward: 35.482, mean reward:  1.613 [-2.806, 32.579], mean action: 5.500 [0.000, 15.000],  loss: 0.022787, mae: 0.332515, mean_q: 0.514045, mean_eps: 0.000000
 3202/5000: episode: 143, duration: 0.182s, episode steps:  12, steps per second:  66, episode reward: 42.000, mean reward:  3.500 [-2.417, 32.440], mean action: 3.083 [0.000, 11.000],  loss: 0.019859, mae: 0.316594, mean_q: 0.516514, mean_eps: 0.000000
 3216/5000: episode: 144, duration: 0.210s, episode steps:  14, steps per second:  67, episode reward: 42.000, mean reward:  3.000 [-2.490, 33.000], mean action: 3.786 [0.000, 14.000],  loss: 0.023635, mae: 0.346387, mean_q: 0.526798, mean_eps: 0.000000
 3237/5000: episode: 145, duration: 0.304s, episode steps:  21, steps per second:  69, episode reward: 35.902, mean reward:  1.710 [-3.000, 32.432], mean action: 3.429 [0.000, 12.000],  loss: 0.022342, mae: 0.339499, mean_q: 0.542311, mean_eps: 0.000000
 3260/5000: episode: 146, duration: 0.327s, episode steps:  23, steps per second:  70, episode reward: 35.557, mean reward:  1.546 [-3.000, 32.039], mean action: 7.348 [0.000, 18.000],  loss: 0.026128, mae: 0.354638, mean_q: 0.529736, mean_eps: 0.000000
 3295/5000: episode: 147, duration: 0.483s, episode steps:  35, steps per second:  72, episode reward: 32.984, mean reward:  0.942 [-2.432, 32.360], mean action: 9.286 [0.000, 19.000],  loss: 0.019880, mae: 0.325397, mean_q: 0.523739, mean_eps: 0.000000
 3312/5000: episode: 148, duration: 0.252s, episode steps:  17, steps per second:  68, episode reward: 41.104, mean reward:  2.418 [-2.020, 32.010], mean action: 4.118 [0.000, 19.000],  loss: 0.023133, mae: 0.332497, mean_q: 0.583894, mean_eps: 0.000000
 3331/5000: episode: 149, duration: 0.275s, episode steps:  19, steps per second:  69, episode reward: 44.191, mean reward:  2.326 [-2.259, 32.480], mean action: 2.526 [1.000, 19.000],  loss: 0.021109, mae: 0.329476, mean_q: 0.519160, mean_eps: 0.000000
 3358/5000: episode: 150, duration: 0.376s, episode steps:  27, steps per second:  72, episode reward: -32.620, mean reward: -1.208 [-32.493,  2.543], mean action: 5.037 [0.000, 19.000],  loss: 0.019826, mae: 0.325685, mean_q: 0.569328, mean_eps: 0.000000
 3401/5000: episode: 151, duration: 0.583s, episode steps:  43, steps per second:  74, episode reward: -35.700, mean reward: -0.830 [-32.232,  2.100], mean action: 3.209 [0.000, 14.000],  loss: 0.020259, mae: 0.327636, mean_q: 0.560418, mean_eps: 0.000000
 3431/5000: episode: 152, duration: 0.422s, episode steps:  30, steps per second:  71, episode reward: 38.879, mean reward:  1.296 [-2.247, 32.039], mean action: 4.400 [0.000, 10.000],  loss: 0.016887, mae: 0.312150, mean_q: 0.464914, mean_eps: 0.000000
 3445/5000: episode: 153, duration: 0.214s, episode steps:  14, steps per second:  65, episode reward: 39.000, mean reward:  2.786 [-2.728, 30.662], mean action: 3.429 [0.000, 15.000],  loss: 0.023556, mae: 0.336912, mean_q: 0.509008, mean_eps: 0.000000
 3465/5000: episode: 154, duration: 0.288s, episode steps:  20, steps per second:  69, episode reward: 41.543, mean reward:  2.077 [-2.603, 32.080], mean action: 4.550 [1.000, 15.000],  loss: 0.021158, mae: 0.334295, mean_q: 0.532546, mean_eps: 0.000000
 3488/5000: episode: 155, duration: 0.333s, episode steps:  23, steps per second:  69, episode reward: -35.070, mean reward: -1.525 [-32.070,  2.270], mean action: 6.348 [0.000, 16.000],  loss: 0.023296, mae: 0.352160, mean_q: 0.523270, mean_eps: 0.000000
 3523/5000: episode: 156, duration: 0.485s, episode steps:  35, steps per second:  72, episode reward: 34.987, mean reward:  1.000 [-2.691, 31.906], mean action: 7.000 [0.000, 19.000],  loss: 0.018148, mae: 0.326710, mean_q: 0.599463, mean_eps: 0.000000
 3544/5000: episode: 157, duration: 0.305s, episode steps:  21, steps per second:  69, episode reward: 32.780, mean reward:  1.561 [-2.651, 32.780], mean action: 5.048 [0.000, 16.000],  loss: 0.022496, mae: 0.341182, mean_q: 0.579721, mean_eps: 0.000000
 3568/5000: episode: 158, duration: 0.347s, episode steps:  24, steps per second:  69, episode reward: 36.000, mean reward:  1.500 [-2.345, 32.080], mean action: 4.250 [0.000, 18.000],  loss: 0.023955, mae: 0.350568, mean_q: 0.587551, mean_eps: 0.000000
 3592/5000: episode: 159, duration: 0.343s, episode steps:  24, steps per second:  70, episode reward: -32.270, mean reward: -1.345 [-32.150,  2.615], mean action: 2.875 [0.000, 11.000],  loss: 0.019636, mae: 0.330153, mean_q: 0.595881, mean_eps: 0.000000
 3611/5000: episode: 160, duration: 0.273s, episode steps:  19, steps per second:  69, episode reward: 35.146, mean reward:  1.850 [-3.000, 32.474], mean action: 4.526 [0.000, 18.000],  loss: 0.018757, mae: 0.323980, mean_q: 0.510063, mean_eps: 0.000000
 3623/5000: episode: 161, duration: 0.188s, episode steps:  12, steps per second:  64, episode reward: 44.249, mean reward:  3.687 [-2.019, 32.131], mean action: 3.500 [0.000, 14.000],  loss: 0.022880, mae: 0.337706, mean_q: 0.505855, mean_eps: 0.000000
 3667/5000: episode: 162, duration: 0.604s, episode steps:  44, steps per second:  73, episode reward: 32.523, mean reward:  0.739 [-2.237, 31.583], mean action: 3.023 [0.000, 12.000],  loss: 0.021018, mae: 0.332589, mean_q: 0.551438, mean_eps: 0.000000
 3692/5000: episode: 163, duration: 0.354s, episode steps:  25, steps per second:  71, episode reward: 37.977, mean reward:  1.519 [-2.459, 32.157], mean action: 2.720 [0.000, 12.000],  loss: 0.019702, mae: 0.329651, mean_q: 0.518750, mean_eps: 0.000000
 3712/5000: episode: 164, duration: 0.288s, episode steps:  20, steps per second:  69, episode reward: -35.160, mean reward: -1.758 [-32.204,  2.310], mean action: 4.200 [0.000, 12.000],  loss: 0.019739, mae: 0.327003, mean_q: 0.594700, mean_eps: 0.000000
 3730/5000: episode: 165, duration: 0.265s, episode steps:  18, steps per second:  68, episode reward: 41.546, mean reward:  2.308 [-2.502, 32.210], mean action: 3.889 [0.000, 15.000],  loss: 0.017331, mae: 0.318223, mean_q: 0.585416, mean_eps: 0.000000
 3744/5000: episode: 166, duration: 0.207s, episode steps:  14, steps per second:  68, episode reward: 41.721, mean reward:  2.980 [-2.138, 32.631], mean action: 3.071 [0.000, 15.000],  loss: 0.020131, mae: 0.329668, mean_q: 0.543681, mean_eps: 0.000000
 3777/5000: episode: 167, duration: 0.471s, episode steps:  33, steps per second:  70, episode reward: 36.000, mean reward:  1.091 [-3.000, 32.120], mean action: 3.970 [0.000, 15.000],  loss: 0.023684, mae: 0.350900, mean_q: 0.539050, mean_eps: 0.000000
 3803/5000: episode: 168, duration: 0.562s, episode steps:  26, steps per second:  46, episode reward: -33.000, mean reward: -1.269 [-32.506,  2.860], mean action: 6.769 [0.000, 15.000],  loss: 0.020520, mae: 0.334143, mean_q: 0.555023, mean_eps: 0.000000
 3822/5000: episode: 169, duration: 0.278s, episode steps:  19, steps per second:  68, episode reward: 35.320, mean reward:  1.859 [-3.000, 32.240], mean action: 3.947 [0.000, 16.000],  loss: 0.022319, mae: 0.340547, mean_q: 0.580628, mean_eps: 0.000000
 3842/5000: episode: 170, duration: 0.285s, episode steps:  20, steps per second:  70, episode reward: 33.000, mean reward:  1.650 [-2.901, 33.000], mean action: 5.650 [1.000, 19.000],  loss: 0.018348, mae: 0.329588, mean_q: 0.591908, mean_eps: 0.000000
 3867/5000: episode: 171, duration: 0.357s, episode steps:  25, steps per second:  70, episode reward: 38.134, mean reward:  1.525 [-2.188, 31.965], mean action: 3.200 [0.000, 19.000],  loss: 0.020424, mae: 0.344705, mean_q: 0.522673, mean_eps: 0.000000
 3883/5000: episode: 172, duration: 0.236s, episode steps:  16, steps per second:  68, episode reward: 44.600, mean reward:  2.787 [-2.077, 32.420], mean action: 2.688 [0.000, 14.000],  loss: 0.025082, mae: 0.361788, mean_q: 0.533168, mean_eps: 0.000000
 3904/5000: episode: 173, duration: 0.305s, episode steps:  21, steps per second:  69, episode reward: 36.000, mean reward:  1.714 [-2.349, 32.270], mean action: 3.762 [0.000, 19.000],  loss: 0.019836, mae: 0.333353, mean_q: 0.576613, mean_eps: 0.000000
 3917/5000: episode: 174, duration: 0.197s, episode steps:  13, steps per second:  66, episode reward: 44.315, mean reward:  3.409 [-2.086, 32.090], mean action: 2.692 [1.000, 11.000],  loss: 0.015911, mae: 0.312085, mean_q: 0.581652, mean_eps: 0.000000
 3945/5000: episode: 175, duration: 0.391s, episode steps:  28, steps per second:  72, episode reward: 35.482, mean reward:  1.267 [-2.498, 32.640], mean action: 5.607 [0.000, 18.000],  loss: 0.021658, mae: 0.334806, mean_q: 0.569932, mean_eps: 0.000000
 3975/5000: episode: 176, duration: 0.422s, episode steps:  30, steps per second:  71, episode reward: 33.000, mean reward:  1.100 [-3.000, 32.490], mean action: 5.133 [2.000, 18.000],  loss: 0.021491, mae: 0.332859, mean_q: 0.514505, mean_eps: 0.000000
 3995/5000: episode: 177, duration: 0.296s, episode steps:  20, steps per second:  67, episode reward: 38.917, mean reward:  1.946 [-2.232, 32.737], mean action: 4.100 [1.000, 11.000],  loss: 0.020959, mae: 0.336923, mean_q: 0.487687, mean_eps: 0.000000
 4021/5000: episode: 178, duration: 0.365s, episode steps:  26, steps per second:  71, episode reward: 35.850, mean reward:  1.379 [-2.252, 32.320], mean action: 4.346 [0.000, 20.000],  loss: 0.016036, mae: 0.317695, mean_q: 0.500923, mean_eps: 0.000000
 4045/5000: episode: 179, duration: 0.337s, episode steps:  24, steps per second:  71, episode reward: -32.470, mean reward: -1.353 [-32.197,  3.000], mean action: 4.417 [0.000, 16.000],  loss: 0.018580, mae: 0.331522, mean_q: 0.522225, mean_eps: 0.000000
 4066/5000: episode: 180, duration: 0.306s, episode steps:  21, steps per second:  69, episode reward: 39.931, mean reward:  1.901 [-2.427, 32.180], mean action: 4.619 [0.000, 16.000],  loss: 0.018430, mae: 0.337771, mean_q: 0.633685, mean_eps: 0.000000
 4090/5000: episode: 181, duration: 0.365s, episode steps:  24, steps per second:  66, episode reward: 38.473, mean reward:  1.603 [-2.343, 32.160], mean action: 6.375 [0.000, 18.000],  loss: 0.021199, mae: 0.344898, mean_q: 0.572230, mean_eps: 0.000000
 4114/5000: episode: 182, duration: 0.339s, episode steps:  24, steps per second:  71, episode reward: -35.230, mean reward: -1.468 [-32.373,  3.000], mean action: 4.625 [0.000, 16.000],  loss: 0.025477, mae: 0.360371, mean_q: 0.563803, mean_eps: 0.000000
 4131/5000: episode: 183, duration: 0.255s, episode steps:  17, steps per second:  67, episode reward: 39.000, mean reward:  2.294 [-2.188, 32.500], mean action: 2.647 [0.000, 16.000],  loss: 0.016674, mae: 0.325536, mean_q: 0.596947, mean_eps: 0.000000
 4158/5000: episode: 184, duration: 0.373s, episode steps:  27, steps per second:  72, episode reward: 32.509, mean reward:  1.204 [-3.000, 32.500], mean action: 4.185 [0.000, 14.000],  loss: 0.020289, mae: 0.350077, mean_q: 0.565501, mean_eps: 0.000000
 4169/5000: episode: 185, duration: 0.174s, episode steps:  11, steps per second:  63, episode reward: 44.521, mean reward:  4.047 [-2.552, 31.933], mean action: 5.182 [0.000, 14.000],  loss: 0.020141, mae: 0.354677, mean_q: 0.544290, mean_eps: 0.000000
 4189/5000: episode: 186, duration: 0.287s, episode steps:  20, steps per second:  70, episode reward: 36.000, mean reward:  1.800 [-2.522, 32.440], mean action: 6.950 [2.000, 15.000],  loss: 0.018876, mae: 0.345651, mean_q: 0.543209, mean_eps: 0.000000
 4217/5000: episode: 187, duration: 0.389s, episode steps:  28, steps per second:  72, episode reward: -32.530, mean reward: -1.162 [-32.210,  2.320], mean action: 7.143 [0.000, 19.000],  loss: 0.019530, mae: 0.342677, mean_q: 0.531725, mean_eps: 0.000000
 4237/5000: episode: 188, duration: 0.309s, episode steps:  20, steps per second:  65, episode reward: 35.069, mean reward:  1.753 [-3.000, 32.351], mean action: 4.050 [0.000, 11.000],  loss: 0.024982, mae: 0.359225, mean_q: 0.590003, mean_eps: 0.000000
 4268/5000: episode: 189, duration: 0.429s, episode steps:  31, steps per second:  72, episode reward: 33.000, mean reward:  1.065 [-2.543, 32.120], mean action: 4.226 [0.000, 19.000],  loss: 0.018439, mae: 0.334430, mean_q: 0.555068, mean_eps: 0.000000
 4289/5000: episode: 190, duration: 0.307s, episode steps:  21, steps per second:  68, episode reward: 38.827, mean reward:  1.849 [-2.878, 32.217], mean action: 4.048 [0.000, 15.000],  loss: 0.023463, mae: 0.365116, mean_q: 0.502133, mean_eps: 0.000000
 4311/5000: episode: 191, duration: 0.314s, episode steps:  22, steps per second:  70, episode reward: 41.187, mean reward:  1.872 [-2.453, 32.732], mean action: 4.091 [0.000, 15.000],  loss: 0.022004, mae: 0.351117, mean_q: 0.488667, mean_eps: 0.000000
 4350/5000: episode: 192, duration: 0.544s, episode steps:  39, steps per second:  72, episode reward: 36.790, mean reward:  0.943 [-2.801, 32.489], mean action: 2.385 [0.000, 15.000],  loss: 0.020901, mae: 0.344740, mean_q: 0.502613, mean_eps: 0.000000
 4379/5000: episode: 193, duration: 0.405s, episode steps:  29, steps per second:  72, episode reward: 38.976, mean reward:  1.344 [-2.940, 32.006], mean action: 2.586 [0.000, 15.000],  loss: 0.018818, mae: 0.328625, mean_q: 0.488670, mean_eps: 0.000000
 4397/5000: episode: 194, duration: 0.254s, episode steps:  18, steps per second:  71, episode reward: -38.130, mean reward: -2.118 [-32.081,  2.250], mean action: 7.111 [0.000, 19.000],  loss: 0.021560, mae: 0.342142, mean_q: 0.490215, mean_eps: 0.000000
 4417/5000: episode: 195, duration: 0.297s, episode steps:  20, steps per second:  67, episode reward: 39.000, mean reward:  1.950 [-2.822, 32.170], mean action: 4.100 [1.000, 19.000],  loss: 0.025209, mae: 0.357221, mean_q: 0.501578, mean_eps: 0.000000
 4440/5000: episode: 196, duration: 0.335s, episode steps:  23, steps per second:  69, episode reward: 42.988, mean reward:  1.869 [-2.169, 31.534], mean action: 2.043 [1.000, 5.000],  loss: 0.021660, mae: 0.338286, mean_q: 0.483438, mean_eps: 0.000000
 4466/5000: episode: 197, duration: 0.377s, episode steps:  26, steps per second:  69, episode reward: 35.130, mean reward:  1.351 [-2.901, 32.250], mean action: 2.346 [0.000, 15.000],  loss: 0.016036, mae: 0.314454, mean_q: 0.463610, mean_eps: 0.000000
 4491/5000: episode: 198, duration: 0.354s, episode steps:  25, steps per second:  71, episode reward: 35.683, mean reward:  1.427 [-2.482, 32.421], mean action: 3.480 [0.000, 18.000],  loss: 0.023045, mae: 0.344968, mean_q: 0.512583, mean_eps: 0.000000
 4515/5000: episode: 199, duration: 0.341s, episode steps:  24, steps per second:  70, episode reward: 35.021, mean reward:  1.459 [-2.280, 32.876], mean action: 4.042 [0.000, 19.000],  loss: 0.021951, mae: 0.336396, mean_q: 0.512980, mean_eps: 0.000000
 4542/5000: episode: 200, duration: 0.376s, episode steps:  27, steps per second:  72, episode reward: 32.667, mean reward:  1.210 [-2.749, 32.087], mean action: 5.148 [0.000, 14.000],  loss: 0.019008, mae: 0.333411, mean_q: 0.476758, mean_eps: 0.000000
 4555/5000: episode: 201, duration: 0.213s, episode steps:  13, steps per second:  61, episode reward: 41.485, mean reward:  3.191 [-2.310, 32.015], mean action: 3.231 [1.000, 15.000],  loss: 0.019219, mae: 0.331380, mean_q: 0.484937, mean_eps: 0.000000
 4585/5000: episode: 202, duration: 0.428s, episode steps:  30, steps per second:  70, episode reward: 38.816, mean reward:  1.294 [-2.348, 32.132], mean action: 3.000 [0.000, 11.000],  loss: 0.020010, mae: 0.325177, mean_q: 0.495816, mean_eps: 0.000000
 4604/5000: episode: 203, duration: 0.288s, episode steps:  19, steps per second:  66, episode reward: 41.170, mean reward:  2.167 [-2.313, 32.280], mean action: 5.895 [0.000, 20.000],  loss: 0.022506, mae: 0.344313, mean_q: 0.483171, mean_eps: 0.000000
 4633/5000: episode: 204, duration: 0.405s, episode steps:  29, steps per second:  72, episode reward: -36.000, mean reward: -1.241 [-32.181,  2.420], mean action: 9.483 [0.000, 19.000],  loss: 0.018744, mae: 0.322264, mean_q: 0.521417, mean_eps: 0.000000
 4655/5000: episode: 205, duration: 0.323s, episode steps:  22, steps per second:  68, episode reward: 34.625, mean reward:  1.574 [-2.678, 31.923], mean action: 3.591 [0.000, 19.000],  loss: 0.022476, mae: 0.343299, mean_q: 0.527966, mean_eps: 0.000000
 4678/5000: episode: 206, duration: 0.331s, episode steps:  23, steps per second:  69, episode reward: 35.599, mean reward:  1.548 [-3.000, 32.253], mean action: 4.826 [0.000, 19.000],  loss: 0.024138, mae: 0.350761, mean_q: 0.487869, mean_eps: 0.000000
 4694/5000: episode: 207, duration: 0.235s, episode steps:  16, steps per second:  68, episode reward: 39.000, mean reward:  2.438 [-2.449, 32.440], mean action: 5.312 [0.000, 15.000],  loss: 0.021943, mae: 0.342409, mean_q: 0.540853, mean_eps: 0.000000
 4735/5000: episode: 208, duration: 0.571s, episode steps:  41, steps per second:  72, episode reward: 37.726, mean reward:  0.920 [-2.163, 32.140], mean action: 4.024 [0.000, 19.000],  loss: 0.020546, mae: 0.336089, mean_q: 0.540352, mean_eps: 0.000000
 4755/5000: episode: 209, duration: 0.294s, episode steps:  20, steps per second:  68, episode reward: 39.000, mean reward:  1.950 [-2.310, 32.120], mean action: 3.150 [0.000, 12.000],  loss: 0.017749, mae: 0.322428, mean_q: 0.549396, mean_eps: 0.000000
 4770/5000: episode: 210, duration: 0.226s, episode steps:  15, steps per second:  66, episode reward: 41.381, mean reward:  2.759 [-2.304, 33.000], mean action: 3.133 [0.000, 16.000],  loss: 0.018279, mae: 0.319099, mean_q: 0.567334, mean_eps: 0.000000
 4796/5000: episode: 211, duration: 0.364s, episode steps:  26, steps per second:  72, episode reward: -35.260, mean reward: -1.356 [-32.238,  2.420], mean action: 8.885 [0.000, 20.000],  loss: 0.021063, mae: 0.339434, mean_q: 0.554536, mean_eps: 0.000000
 4816/5000: episode: 212, duration: 0.283s, episode steps:  20, steps per second:  71, episode reward: -38.130, mean reward: -1.906 [-32.317,  2.561], mean action: 6.900 [0.000, 16.000],  loss: 0.020555, mae: 0.332859, mean_q: 0.515996, mean_eps: 0.000000
 4833/5000: episode: 213, duration: 0.249s, episode steps:  17, steps per second:  68, episode reward: 41.717, mean reward:  2.454 [-2.113, 32.400], mean action: 3.824 [0.000, 15.000],  loss: 0.024646, mae: 0.357926, mean_q: 0.498490, mean_eps: 0.000000
 4852/5000: episode: 214, duration: 0.268s, episode steps:  19, steps per second:  71, episode reward: -39.000, mean reward: -2.053 [-30.286,  2.350], mean action: 7.263 [0.000, 16.000],  loss: 0.024833, mae: 0.362251, mean_q: 0.493089, mean_eps: 0.000000
 4868/5000: episode: 215, duration: 0.234s, episode steps:  16, steps per second:  68, episode reward: 37.936, mean reward:  2.371 [-3.000, 32.528], mean action: 3.375 [0.000, 15.000],  loss: 0.021055, mae: 0.344159, mean_q: 0.520619, mean_eps: 0.000000
 4889/5000: episode: 216, duration: 0.303s, episode steps:  21, steps per second:  69, episode reward: 35.234, mean reward:  1.678 [-2.903, 32.078], mean action: 4.524 [0.000, 15.000],  loss: 0.020932, mae: 0.335376, mean_q: 0.527917, mean_eps: 0.000000
 4906/5000: episode: 217, duration: 0.246s, episode steps:  17, steps per second:  69, episode reward: 35.900, mean reward:  2.112 [-3.000, 32.580], mean action: 5.765 [0.000, 15.000],  loss: 0.019578, mae: 0.328815, mean_q: 0.500029, mean_eps: 0.000000
 4923/5000: episode: 218, duration: 0.248s, episode steps:  17, steps per second:  69, episode reward: 38.842, mean reward:  2.285 [-2.515, 33.000], mean action: 4.824 [0.000, 15.000],  loss: 0.020123, mae: 0.325834, mean_q: 0.560609, mean_eps: 0.000000
 4946/5000: episode: 219, duration: 0.334s, episode steps:  23, steps per second:  69, episode reward: 35.338, mean reward:  1.536 [-2.504, 32.330], mean action: 4.609 [0.000, 15.000],  loss: 0.019669, mae: 0.331239, mean_q: 0.573753, mean_eps: 0.000000
 4980/5000: episode: 220, duration: 0.468s, episode steps:  34, steps per second:  73, episode reward: 35.913, mean reward:  1.056 [-2.469, 32.062], mean action: 3.735 [0.000, 21.000],  loss: 0.022178, mae: 0.341472, mean_q: 0.562206, mean_eps: 0.000000
 4999/5000: episode: 221, duration: 0.272s, episode steps:  19, steps per second:  70, episode reward: 35.015, mean reward:  1.843 [-2.900, 32.180], mean action: 4.632 [0.000, 19.000],  loss: 0.020601, mae: 0.339621, mean_q: 0.580364, mean_eps: 0.000000
done, took 65.075 seconds
DQN Evaluation: 9554 victories out of 11178 episodes
Training for 5000 steps ...
   22/5000: episode: 1, duration: 0.199s, episode steps:  22, steps per second: 110, episode reward: 44.235, mean reward:  2.011 [-2.128, 32.098], mean action: 3.864 [1.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   44/5000: episode: 2, duration: 0.161s, episode steps:  22, steps per second: 137, episode reward: 47.069, mean reward:  2.140 [ 0.000, 31.986], mean action: 4.045 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   76/5000: episode: 3, duration: 0.226s, episode steps:  32, steps per second: 141, episode reward: 41.677, mean reward:  1.302 [-2.153, 32.030], mean action: 3.250 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  116/5000: episode: 4, duration: 0.250s, episode steps:  40, steps per second: 160, episode reward: 35.369, mean reward:  0.884 [-2.879, 33.000], mean action: 5.200 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  150/5000: episode: 5, duration: 0.234s, episode steps:  34, steps per second: 145, episode reward: 41.642, mean reward:  1.225 [-2.243, 32.072], mean action: 2.324 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  167/5000: episode: 6, duration: 0.132s, episode steps:  17, steps per second: 129, episode reward: 44.100, mean reward:  2.594 [-2.675, 32.000], mean action: 2.000 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  195/5000: episode: 7, duration: 0.181s, episode steps:  28, steps per second: 155, episode reward: 41.654, mean reward:  1.488 [-2.198, 32.322], mean action: 3.821 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  233/5000: episode: 8, duration: 0.241s, episode steps:  38, steps per second: 158, episode reward: 43.184, mean reward:  1.136 [-2.389, 32.050], mean action: 2.395 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  253/5000: episode: 9, duration: 0.141s, episode steps:  20, steps per second: 142, episode reward: 44.993, mean reward:  2.250 [-2.121, 32.390], mean action: 2.450 [1.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  285/5000: episode: 10, duration: 0.204s, episode steps:  32, steps per second: 157, episode reward: 32.411, mean reward:  1.013 [-3.000, 31.961], mean action: 4.375 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  311/5000: episode: 11, duration: 0.174s, episode steps:  26, steps per second: 150, episode reward: 38.552, mean reward:  1.483 [-3.000, 29.700], mean action: 2.577 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  343/5000: episode: 12, duration: 0.203s, episode steps:  32, steps per second: 158, episode reward: 36.000, mean reward:  1.125 [-3.000, 32.150], mean action: 3.375 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  359/5000: episode: 13, duration: 0.119s, episode steps:  16, steps per second: 134, episode reward: 44.941, mean reward:  2.809 [-2.106, 32.310], mean action: 1.250 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  391/5000: episode: 14, duration: 0.208s, episode steps:  32, steps per second: 154, episode reward: 37.832, mean reward:  1.182 [-3.000, 32.610], mean action: 3.406 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  416/5000: episode: 15, duration: 0.164s, episode steps:  25, steps per second: 153, episode reward: 38.541, mean reward:  1.542 [-3.000, 32.090], mean action: 3.880 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  437/5000: episode: 16, duration: 0.153s, episode steps:  21, steps per second: 137, episode reward: 44.888, mean reward:  2.138 [-2.259, 32.660], mean action: 3.095 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  458/5000: episode: 17, duration: 0.144s, episode steps:  21, steps per second: 146, episode reward: 41.107, mean reward:  1.957 [-2.221, 32.013], mean action: 2.571 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  496/5000: episode: 18, duration: 0.236s, episode steps:  38, steps per second: 161, episode reward: 35.901, mean reward:  0.945 [-3.000, 32.061], mean action: 3.132 [1.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  536/5000: episode: 19, duration: 0.254s, episode steps:  40, steps per second: 158, episode reward: 41.558, mean reward:  1.039 [-2.046, 30.505], mean action: 3.525 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  558/5000: episode: 20, duration: 0.154s, episode steps:  22, steps per second: 143, episode reward: 42.000, mean reward:  1.909 [-2.343, 32.380], mean action: 4.591 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  588/5000: episode: 21, duration: 0.194s, episode steps:  30, steps per second: 155, episode reward: 44.402, mean reward:  1.480 [-2.938, 32.181], mean action: 3.400 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  605/5000: episode: 22, duration: 0.124s, episode steps:  17, steps per second: 137, episode reward: 47.217, mean reward:  2.777 [ 0.000, 32.346], mean action: 2.471 [1.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  644/5000: episode: 23, duration: 0.252s, episode steps:  39, steps per second: 155, episode reward: 43.370, mean reward:  1.112 [-3.000, 32.390], mean action: 3.538 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  665/5000: episode: 24, duration: 0.147s, episode steps:  21, steps per second: 143, episode reward: 44.283, mean reward:  2.109 [-2.232, 32.080], mean action: 3.095 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  692/5000: episode: 25, duration: 0.184s, episode steps:  27, steps per second: 147, episode reward: 43.210, mean reward:  1.600 [-1.866, 32.477], mean action: 2.815 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  732/5000: episode: 26, duration: 0.269s, episode steps:  40, steps per second: 149, episode reward: 44.607, mean reward:  1.115 [-1.900, 32.150], mean action: 2.500 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  754/5000: episode: 27, duration: 0.153s, episode steps:  22, steps per second: 144, episode reward: 42.000, mean reward:  1.909 [-2.230, 32.610], mean action: 2.500 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  780/5000: episode: 28, duration: 0.171s, episode steps:  26, steps per second: 152, episode reward: 35.769, mean reward:  1.376 [-2.683, 32.169], mean action: 2.846 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  810/5000: episode: 29, duration: 0.201s, episode steps:  30, steps per second: 150, episode reward: 44.067, mean reward:  1.469 [-2.863, 31.576], mean action: 4.567 [1.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  844/5000: episode: 30, duration: 0.297s, episode steps:  34, steps per second: 115, episode reward: 40.757, mean reward:  1.199 [-2.877, 31.981], mean action: 3.706 [1.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  866/5000: episode: 31, duration: 0.315s, episode steps:  22, steps per second:  70, episode reward: 41.072, mean reward:  1.867 [-2.147, 32.220], mean action: 2.091 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  890/5000: episode: 32, duration: 0.170s, episode steps:  24, steps per second: 141, episode reward: 41.228, mean reward:  1.718 [-3.000, 32.220], mean action: 2.583 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  915/5000: episode: 33, duration: 0.165s, episode steps:  25, steps per second: 152, episode reward: 35.560, mean reward:  1.422 [-2.822, 31.910], mean action: 6.880 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  947/5000: episode: 34, duration: 0.202s, episode steps:  32, steps per second: 158, episode reward: 41.019, mean reward:  1.282 [-3.000, 32.180], mean action: 6.500 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  979/5000: episode: 35, duration: 0.210s, episode steps:  32, steps per second: 153, episode reward: 43.935, mean reward:  1.373 [-2.250, 32.507], mean action: 4.688 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1017/5000: episode: 36, duration: 0.376s, episode steps:  38, steps per second: 101, episode reward: 38.750, mean reward:  1.020 [-3.000, 32.170], mean action: 4.605 [0.000, 20.000],  loss: 0.020914, mae: 0.329750, mean_q: 0.542267, mean_eps: 0.000000
 1053/5000: episode: 37, duration: 0.505s, episode steps:  36, steps per second:  71, episode reward: 32.018, mean reward:  0.889 [-3.000, 31.764], mean action: 5.500 [0.000, 21.000],  loss: 0.019518, mae: 0.326324, mean_q: 0.577378, mean_eps: 0.000000
 1076/5000: episode: 38, duration: 0.335s, episode steps:  23, steps per second:  69, episode reward: 44.326, mean reward:  1.927 [-2.104, 32.050], mean action: 3.261 [0.000, 16.000],  loss: 0.019651, mae: 0.327205, mean_q: 0.554930, mean_eps: 0.000000
 1091/5000: episode: 39, duration: 0.226s, episode steps:  15, steps per second:  66, episode reward: 42.000, mean reward:  2.800 [-2.199, 29.439], mean action: 4.600 [0.000, 15.000],  loss: 0.020538, mae: 0.334542, mean_q: 0.551512, mean_eps: 0.000000
 1127/5000: episode: 40, duration: 0.506s, episode steps:  36, steps per second:  71, episode reward: 38.416, mean reward:  1.067 [-2.268, 31.656], mean action: 4.722 [0.000, 18.000],  loss: 0.022030, mae: 0.335656, mean_q: 0.553544, mean_eps: 0.000000
 1148/5000: episode: 41, duration: 0.303s, episode steps:  21, steps per second:  69, episode reward: 44.601, mean reward:  2.124 [-2.449, 32.480], mean action: 3.857 [1.000, 19.000],  loss: 0.020071, mae: 0.324113, mean_q: 0.542412, mean_eps: 0.000000
 1176/5000: episode: 42, duration: 0.397s, episode steps:  28, steps per second:  71, episode reward: 40.696, mean reward:  1.453 [-2.889, 32.038], mean action: 3.036 [0.000, 15.000],  loss: 0.021132, mae: 0.323221, mean_q: 0.522931, mean_eps: 0.000000
 1227/5000: episode: 43, duration: 0.722s, episode steps:  51, steps per second:  71, episode reward: 39.000, mean reward:  0.765 [-2.419, 32.150], mean action: 1.804 [0.000, 15.000],  loss: 0.022838, mae: 0.333498, mean_q: 0.521238, mean_eps: 0.000000
 1252/5000: episode: 44, duration: 0.355s, episode steps:  25, steps per second:  70, episode reward: 39.000, mean reward:  1.560 [-2.419, 32.190], mean action: 2.120 [0.000, 11.000],  loss: 0.020611, mae: 0.329373, mean_q: 0.583515, mean_eps: 0.000000
 1286/5000: episode: 45, duration: 0.471s, episode steps:  34, steps per second:  72, episode reward: 33.000, mean reward:  0.971 [-3.000, 29.772], mean action: 2.500 [0.000, 11.000],  loss: 0.017509, mae: 0.321142, mean_q: 0.605842, mean_eps: 0.000000
 1318/5000: episode: 46, duration: 0.453s, episode steps:  32, steps per second:  71, episode reward: 44.376, mean reward:  1.387 [-2.015, 32.070], mean action: 1.656 [0.000, 12.000],  loss: 0.021152, mae: 0.339324, mean_q: 0.567821, mean_eps: 0.000000
 1341/5000: episode: 47, duration: 0.329s, episode steps:  23, steps per second:  70, episode reward: 41.030, mean reward:  1.784 [-2.517, 32.390], mean action: 2.435 [0.000, 15.000],  loss: 0.020655, mae: 0.331145, mean_q: 0.517682, mean_eps: 0.000000
 1362/5000: episode: 48, duration: 0.308s, episode steps:  21, steps per second:  68, episode reward: 43.712, mean reward:  2.082 [-2.665, 32.255], mean action: 4.667 [0.000, 19.000],  loss: 0.023567, mae: 0.350232, mean_q: 0.502100, mean_eps: 0.000000
 1378/5000: episode: 49, duration: 0.259s, episode steps:  16, steps per second:  62, episode reward: 46.523, mean reward:  2.908 [-0.111, 32.160], mean action: 2.188 [0.000, 8.000],  loss: 0.025038, mae: 0.348554, mean_q: 0.505153, mean_eps: 0.000000
 1404/5000: episode: 50, duration: 0.375s, episode steps:  26, steps per second:  69, episode reward: 41.977, mean reward:  1.615 [-2.136, 32.207], mean action: 3.962 [1.000, 19.000],  loss: 0.023342, mae: 0.342287, mean_q: 0.577439, mean_eps: 0.000000
 1429/5000: episode: 51, duration: 0.362s, episode steps:  25, steps per second:  69, episode reward: 41.569, mean reward:  1.663 [-2.463, 32.430], mean action: 2.520 [0.000, 19.000],  loss: 0.020007, mae: 0.334126, mean_q: 0.545467, mean_eps: 0.000000
 1461/5000: episode: 52, duration: 0.454s, episode steps:  32, steps per second:  70, episode reward: 41.873, mean reward:  1.309 [-2.266, 32.323], mean action: 3.906 [0.000, 19.000],  loss: 0.022109, mae: 0.344170, mean_q: 0.563917, mean_eps: 0.000000
 1479/5000: episode: 53, duration: 0.287s, episode steps:  18, steps per second:  63, episode reward: 41.047, mean reward:  2.280 [-2.506, 33.000], mean action: 3.000 [1.000, 19.000],  loss: 0.015765, mae: 0.307810, mean_q: 0.544796, mean_eps: 0.000000
 1506/5000: episode: 54, duration: 0.386s, episode steps:  27, steps per second:  70, episode reward: 41.283, mean reward:  1.529 [-3.000, 32.287], mean action: 2.037 [0.000, 3.000],  loss: 0.021714, mae: 0.336305, mean_q: 0.603971, mean_eps: 0.000000
 1527/5000: episode: 55, duration: 0.307s, episode steps:  21, steps per second:  68, episode reward: 44.414, mean reward:  2.115 [-2.076, 32.900], mean action: 0.952 [0.000, 3.000],  loss: 0.023188, mae: 0.338994, mean_q: 0.570451, mean_eps: 0.000000
 1557/5000: episode: 56, duration: 0.422s, episode steps:  30, steps per second:  71, episode reward: 40.403, mean reward:  1.347 [-2.462, 31.794], mean action: 4.800 [0.000, 19.000],  loss: 0.022688, mae: 0.341596, mean_q: 0.517674, mean_eps: 0.000000
 1601/5000: episode: 57, duration: 0.611s, episode steps:  44, steps per second:  72, episode reward: 40.912, mean reward:  0.930 [-3.000, 32.550], mean action: 4.273 [0.000, 18.000],  loss: 0.019585, mae: 0.324925, mean_q: 0.577815, mean_eps: 0.000000
 1625/5000: episode: 58, duration: 0.345s, episode steps:  24, steps per second:  70, episode reward: 41.861, mean reward:  1.744 [-2.245, 32.020], mean action: 3.125 [0.000, 16.000],  loss: 0.022055, mae: 0.337581, mean_q: 0.573890, mean_eps: 0.000000
 1648/5000: episode: 59, duration: 0.334s, episode steps:  23, steps per second:  69, episode reward: 43.791, mean reward:  1.904 [-2.028, 32.290], mean action: 2.348 [0.000, 19.000],  loss: 0.023875, mae: 0.350919, mean_q: 0.523253, mean_eps: 0.000000
 1666/5000: episode: 60, duration: 0.259s, episode steps:  18, steps per second:  69, episode reward: 38.121, mean reward:  2.118 [-3.000, 32.398], mean action: 4.278 [0.000, 19.000],  loss: 0.018371, mae: 0.317632, mean_q: 0.511800, mean_eps: 0.000000
 1686/5000: episode: 61, duration: 0.298s, episode steps:  20, steps per second:  67, episode reward: 45.000, mean reward:  2.250 [-2.055, 32.040], mean action: 0.650 [0.000, 2.000],  loss: 0.016457, mae: 0.315582, mean_q: 0.585694, mean_eps: 0.000000
 1712/5000: episode: 62, duration: 0.375s, episode steps:  26, steps per second:  69, episode reward: 41.732, mean reward:  1.605 [-2.659, 32.340], mean action: 3.000 [0.000, 16.000],  loss: 0.022091, mae: 0.344994, mean_q: 0.588399, mean_eps: 0.000000
 1748/5000: episode: 63, duration: 0.503s, episode steps:  36, steps per second:  72, episode reward: 38.276, mean reward:  1.063 [-2.806, 32.110], mean action: 3.694 [0.000, 15.000],  loss: 0.021618, mae: 0.340112, mean_q: 0.625706, mean_eps: 0.000000
 1776/5000: episode: 64, duration: 0.442s, episode steps:  28, steps per second:  63, episode reward: -32.100, mean reward: -1.146 [-32.282,  2.702], mean action: 6.679 [0.000, 15.000],  loss: 0.021147, mae: 0.340945, mean_q: 0.567741, mean_eps: 0.000000
 1796/5000: episode: 65, duration: 0.309s, episode steps:  20, steps per second:  65, episode reward: 44.339, mean reward:  2.217 [-2.105, 32.340], mean action: 4.300 [0.000, 13.000],  loss: 0.021577, mae: 0.351903, mean_q: 0.590640, mean_eps: 0.000000
 1804/5000: episode: 66, duration: 0.133s, episode steps:   8, steps per second:  60, episode reward: 47.172, mean reward:  5.896 [ 0.002, 32.320], mean action: 1.250 [0.000, 2.000],  loss: 0.023067, mae: 0.346007, mean_q: 0.588889, mean_eps: 0.000000
 1832/5000: episode: 67, duration: 0.406s, episode steps:  28, steps per second:  69, episode reward: 35.941, mean reward:  1.284 [-3.000, 32.070], mean action: 3.250 [0.000, 11.000],  loss: 0.021757, mae: 0.345467, mean_q: 0.582378, mean_eps: 0.000000
 1863/5000: episode: 68, duration: 0.444s, episode steps:  31, steps per second:  70, episode reward: 35.182, mean reward:  1.135 [-3.000, 32.250], mean action: 7.419 [1.000, 19.000],  loss: 0.016041, mae: 0.313807, mean_q: 0.532239, mean_eps: 0.000000
 1885/5000: episode: 69, duration: 0.322s, episode steps:  22, steps per second:  68, episode reward: 38.003, mean reward:  1.727 [-2.314, 31.703], mean action: 2.864 [0.000, 11.000],  loss: 0.021219, mae: 0.347737, mean_q: 0.547866, mean_eps: 0.000000
 1916/5000: episode: 70, duration: 0.441s, episode steps:  31, steps per second:  70, episode reward: 38.900, mean reward:  1.255 [-2.736, 32.460], mean action: 2.839 [0.000, 11.000],  loss: 0.020464, mae: 0.341572, mean_q: 0.551653, mean_eps: 0.000000
 1926/5000: episode: 71, duration: 0.163s, episode steps:  10, steps per second:  61, episode reward: 47.792, mean reward:  4.779 [ 0.086, 32.424], mean action: 2.300 [0.000, 14.000],  loss: 0.023206, mae: 0.348975, mean_q: 0.602471, mean_eps: 0.000000
 1961/5000: episode: 72, duration: 0.512s, episode steps:  35, steps per second:  68, episode reward: 36.000, mean reward:  1.029 [-2.886, 32.310], mean action: 5.371 [0.000, 19.000],  loss: 0.022521, mae: 0.351533, mean_q: 0.560109, mean_eps: 0.000000
 1983/5000: episode: 73, duration: 0.328s, episode steps:  22, steps per second:  67, episode reward: 41.254, mean reward:  1.875 [-2.153, 32.380], mean action: 3.818 [0.000, 20.000],  loss: 0.019977, mae: 0.345839, mean_q: 0.546131, mean_eps: 0.000000
 2010/5000: episode: 74, duration: 0.388s, episode steps:  27, steps per second:  70, episode reward: 35.656, mean reward:  1.321 [-3.000, 32.090], mean action: 5.185 [0.000, 19.000],  loss: 0.017384, mae: 0.329366, mean_q: 0.538012, mean_eps: 0.000000
 2032/5000: episode: 75, duration: 0.381s, episode steps:  22, steps per second:  58, episode reward: 44.904, mean reward:  2.041 [-2.130, 32.214], mean action: 1.909 [1.000, 9.000],  loss: 0.019274, mae: 0.338046, mean_q: 0.537130, mean_eps: 0.000000
 2082/5000: episode: 76, duration: 0.682s, episode steps:  50, steps per second:  73, episode reward: -32.300, mean reward: -0.646 [-32.642,  2.723], mean action: 3.820 [0.000, 19.000],  loss: 0.022207, mae: 0.359491, mean_q: 0.485669, mean_eps: 0.000000
 2113/5000: episode: 77, duration: 0.437s, episode steps:  31, steps per second:  71, episode reward: 43.693, mean reward:  1.409 [-3.000, 32.409], mean action: 3.194 [0.000, 13.000],  loss: 0.020195, mae: 0.341016, mean_q: 0.545262, mean_eps: 0.000000
 2148/5000: episode: 78, duration: 0.499s, episode steps:  35, steps per second:  70, episode reward: 44.688, mean reward:  1.277 [-2.383, 32.100], mean action: 2.686 [1.000, 3.000],  loss: 0.021830, mae: 0.353268, mean_q: 0.544565, mean_eps: 0.000000
 2174/5000: episode: 79, duration: 0.365s, episode steps:  26, steps per second:  71, episode reward: 41.220, mean reward:  1.585 [-2.149, 32.132], mean action: 3.808 [1.000, 12.000],  loss: 0.020153, mae: 0.348567, mean_q: 0.551419, mean_eps: 0.000000
 2201/5000: episode: 80, duration: 0.384s, episode steps:  27, steps per second:  70, episode reward: 37.898, mean reward:  1.404 [-3.000, 31.808], mean action: 5.000 [0.000, 20.000],  loss: 0.020017, mae: 0.346984, mean_q: 0.483092, mean_eps: 0.000000
 2222/5000: episode: 81, duration: 0.301s, episode steps:  21, steps per second:  70, episode reward: 44.630, mean reward:  2.125 [-2.260, 32.470], mean action: 1.048 [0.000, 3.000],  loss: 0.019790, mae: 0.335379, mean_q: 0.458701, mean_eps: 0.000000
 2247/5000: episode: 82, duration: 0.358s, episode steps:  25, steps per second:  70, episode reward: 43.494, mean reward:  1.740 [-2.045, 32.178], mean action: 0.880 [0.000, 11.000],  loss: 0.021713, mae: 0.341022, mean_q: 0.543706, mean_eps: 0.000000
 2279/5000: episode: 83, duration: 0.453s, episode steps:  32, steps per second:  71, episode reward: 38.295, mean reward:  1.197 [-3.000, 32.083], mean action: 4.062 [0.000, 20.000],  loss: 0.018972, mae: 0.327363, mean_q: 0.543616, mean_eps: 0.000000
 2301/5000: episode: 84, duration: 0.313s, episode steps:  22, steps per second:  70, episode reward: 41.940, mean reward:  1.906 [-2.523, 32.680], mean action: 1.591 [0.000, 3.000],  loss: 0.025488, mae: 0.352614, mean_q: 0.514409, mean_eps: 0.000000
 2334/5000: episode: 85, duration: 0.458s, episode steps:  33, steps per second:  72, episode reward: 38.777, mean reward:  1.175 [-3.000, 32.330], mean action: 4.515 [0.000, 19.000],  loss: 0.020165, mae: 0.332327, mean_q: 0.530508, mean_eps: 0.000000
 2358/5000: episode: 86, duration: 0.352s, episode steps:  24, steps per second:  68, episode reward: 44.083, mean reward:  1.837 [-2.011, 32.260], mean action: 3.708 [0.000, 14.000],  loss: 0.022168, mae: 0.342647, mean_q: 0.580783, mean_eps: 0.000000
 2389/5000: episode: 87, duration: 0.431s, episode steps:  31, steps per second:  72, episode reward: 44.005, mean reward:  1.420 [-2.426, 32.264], mean action: 1.710 [0.000, 3.000],  loss: 0.017784, mae: 0.325903, mean_q: 0.585971, mean_eps: 0.000000
 2410/5000: episode: 88, duration: 0.313s, episode steps:  21, steps per second:  67, episode reward: 41.186, mean reward:  1.961 [-3.000, 32.281], mean action: 4.333 [0.000, 18.000],  loss: 0.015503, mae: 0.315125, mean_q: 0.525762, mean_eps: 0.000000
 2439/5000: episode: 89, duration: 0.419s, episode steps:  29, steps per second:  69, episode reward: 47.301, mean reward:  1.631 [-0.150, 32.290], mean action: 1.586 [1.000, 3.000],  loss: 0.017192, mae: 0.327961, mean_q: 0.468928, mean_eps: 0.000000
 2463/5000: episode: 90, duration: 0.348s, episode steps:  24, steps per second:  69, episode reward: 44.168, mean reward:  1.840 [-2.098, 32.490], mean action: 3.542 [2.000, 16.000],  loss: 0.021030, mae: 0.340810, mean_q: 0.460735, mean_eps: 0.000000
 2490/5000: episode: 91, duration: 0.390s, episode steps:  27, steps per second:  69, episode reward: 38.930, mean reward:  1.442 [-2.444, 32.121], mean action: 3.852 [0.000, 16.000],  loss: 0.018947, mae: 0.333438, mean_q: 0.472698, mean_eps: 0.000000
 2512/5000: episode: 92, duration: 0.319s, episode steps:  22, steps per second:  69, episode reward: 41.207, mean reward:  1.873 [-3.000, 32.000], mean action: 1.727 [0.000, 16.000],  loss: 0.021661, mae: 0.354918, mean_q: 0.453653, mean_eps: 0.000000
 2542/5000: episode: 93, duration: 0.427s, episode steps:  30, steps per second:  70, episode reward: -34.520, mean reward: -1.151 [-32.651,  2.303], mean action: 8.633 [0.000, 21.000],  loss: 0.018768, mae: 0.334096, mean_q: 0.474266, mean_eps: 0.000000
 2567/5000: episode: 94, duration: 0.352s, episode steps:  25, steps per second:  71, episode reward: 35.136, mean reward:  1.405 [-3.000, 32.546], mean action: 6.480 [0.000, 19.000],  loss: 0.020893, mae: 0.335023, mean_q: 0.507884, mean_eps: 0.000000
 2612/5000: episode: 95, duration: 0.614s, episode steps:  45, steps per second:  73, episode reward: 40.661, mean reward:  0.904 [-2.534, 31.942], mean action: 5.667 [0.000, 20.000],  loss: 0.021687, mae: 0.341128, mean_q: 0.490552, mean_eps: 0.000000
 2648/5000: episode: 96, duration: 0.501s, episode steps:  36, steps per second:  72, episode reward: 35.661, mean reward:  0.991 [-3.000, 32.190], mean action: 3.417 [0.000, 16.000],  loss: 0.020834, mae: 0.339590, mean_q: 0.522919, mean_eps: 0.000000
 2681/5000: episode: 97, duration: 0.464s, episode steps:  33, steps per second:  71, episode reward: 44.291, mean reward:  1.342 [-2.030, 31.904], mean action: 1.818 [0.000, 11.000],  loss: 0.020495, mae: 0.341826, mean_q: 0.549875, mean_eps: 0.000000
 2706/5000: episode: 98, duration: 0.358s, episode steps:  25, steps per second:  70, episode reward: 41.451, mean reward:  1.658 [-2.377, 32.382], mean action: 1.480 [0.000, 11.000],  loss: 0.023437, mae: 0.354460, mean_q: 0.547750, mean_eps: 0.000000
 2727/5000: episode: 99, duration: 0.300s, episode steps:  21, steps per second:  70, episode reward: 35.492, mean reward:  1.690 [-2.882, 32.370], mean action: 3.048 [0.000, 12.000],  loss: 0.017628, mae: 0.335676, mean_q: 0.479365, mean_eps: 0.000000
 2756/5000: episode: 100, duration: 0.406s, episode steps:  29, steps per second:  71, episode reward: 44.495, mean reward:  1.534 [-2.160, 32.090], mean action: 1.207 [0.000, 3.000],  loss: 0.019780, mae: 0.339272, mean_q: 0.501958, mean_eps: 0.000000
 2793/5000: episode: 101, duration: 0.516s, episode steps:  37, steps per second:  72, episode reward: 41.266, mean reward:  1.115 [-2.506, 32.460], mean action: 2.162 [0.000, 12.000],  loss: 0.018375, mae: 0.333768, mean_q: 0.512135, mean_eps: 0.000000
 2823/5000: episode: 102, duration: 0.422s, episode steps:  30, steps per second:  71, episode reward: 33.000, mean reward:  1.100 [-3.000, 32.210], mean action: 4.467 [0.000, 19.000],  loss: 0.018258, mae: 0.333824, mean_q: 0.514272, mean_eps: 0.000000
 2854/5000: episode: 103, duration: 0.432s, episode steps:  31, steps per second:  72, episode reward: 38.412, mean reward:  1.239 [-2.805, 32.860], mean action: 5.581 [0.000, 19.000],  loss: 0.020127, mae: 0.332500, mean_q: 0.492205, mean_eps: 0.000000
 2874/5000: episode: 104, duration: 0.287s, episode steps:  20, steps per second:  70, episode reward: 41.168, mean reward:  2.058 [-3.000, 32.540], mean action: 2.900 [0.000, 19.000],  loss: 0.021295, mae: 0.337732, mean_q: 0.483675, mean_eps: 0.000000
 2894/5000: episode: 105, duration: 18.957s, episode steps:  20, steps per second:   1, episode reward: 39.000, mean reward:  1.950 [-3.000, 33.000], mean action: 3.850 [0.000, 19.000],  loss: 0.021587, mae: 0.339200, mean_q: 0.490995, mean_eps: 0.000000
 2915/5000: episode: 106, duration: 0.310s, episode steps:  21, steps per second:  68, episode reward: 44.203, mean reward:  2.105 [-2.129, 31.690], mean action: 2.667 [1.000, 16.000],  loss: 0.018594, mae: 0.330093, mean_q: 0.539830, mean_eps: 0.000000
 2939/5000: episode: 107, duration: 0.364s, episode steps:  24, steps per second:  66, episode reward: 44.046, mean reward:  1.835 [-2.179, 31.554], mean action: 3.750 [3.000, 16.000],  loss: 0.019181, mae: 0.328936, mean_q: 0.515522, mean_eps: 0.000000
 2979/5000: episode: 108, duration: 0.554s, episode steps:  40, steps per second:  72, episode reward: 33.000, mean reward:  0.825 [-2.730, 32.485], mean action: 6.925 [0.000, 19.000],  loss: 0.018323, mae: 0.324470, mean_q: 0.555168, mean_eps: 0.000000
 3001/5000: episode: 109, duration: 0.324s, episode steps:  22, steps per second:  68, episode reward: 39.000, mean reward:  1.773 [-2.162, 32.390], mean action: 6.000 [0.000, 19.000],  loss: 0.020265, mae: 0.335571, mean_q: 0.563136, mean_eps: 0.000000
 3021/5000: episode: 110, duration: 0.294s, episode steps:  20, steps per second:  68, episode reward: 41.254, mean reward:  2.063 [-2.807, 32.410], mean action: 3.150 [0.000, 15.000],  loss: 0.021352, mae: 0.355351, mean_q: 0.540264, mean_eps: 0.000000
 3045/5000: episode: 111, duration: 0.348s, episode steps:  24, steps per second:  69, episode reward: 44.054, mean reward:  1.836 [-2.707, 32.240], mean action: 2.792 [0.000, 12.000],  loss: 0.018467, mae: 0.333078, mean_q: 0.487286, mean_eps: 0.000000
 3093/5000: episode: 112, duration: 0.673s, episode steps:  48, steps per second:  71, episode reward: 38.289, mean reward:  0.798 [-2.283, 32.404], mean action: 4.083 [0.000, 15.000],  loss: 0.017523, mae: 0.319883, mean_q: 0.546001, mean_eps: 0.000000
 3132/5000: episode: 113, duration: 0.544s, episode steps:  39, steps per second:  72, episode reward: 38.911, mean reward:  0.998 [-2.562, 32.140], mean action: 3.821 [0.000, 15.000],  loss: 0.019160, mae: 0.327942, mean_q: 0.497205, mean_eps: 0.000000
 3148/5000: episode: 114, duration: 0.245s, episode steps:  16, steps per second:  65, episode reward: 47.320, mean reward:  2.958 [-0.013, 32.090], mean action: 0.125 [0.000, 2.000],  loss: 0.018289, mae: 0.325126, mean_q: 0.486959, mean_eps: 0.000000
 3174/5000: episode: 115, duration: 0.382s, episode steps:  26, steps per second:  68, episode reward: 41.736, mean reward:  1.605 [-2.218, 32.290], mean action: 3.808 [0.000, 14.000],  loss: 0.021509, mae: 0.334023, mean_q: 0.488000, mean_eps: 0.000000
 3188/5000: episode: 116, duration: 0.217s, episode steps:  14, steps per second:  65, episode reward: 39.000, mean reward:  2.786 [-2.817, 29.678], mean action: 1.857 [0.000, 9.000],  loss: 0.021031, mae: 0.336989, mean_q: 0.478446, mean_eps: 0.000000
 3231/5000: episode: 117, duration: 0.607s, episode steps:  43, steps per second:  71, episode reward: 37.661, mean reward:  0.876 [-3.000, 32.110], mean action: 6.140 [1.000, 16.000],  loss: 0.018895, mae: 0.324788, mean_q: 0.521695, mean_eps: 0.000000
 3261/5000: episode: 118, duration: 0.431s, episode steps:  30, steps per second:  70, episode reward: 42.000, mean reward:  1.400 [-2.760, 32.260], mean action: 3.367 [0.000, 13.000],  loss: 0.024719, mae: 0.351064, mean_q: 0.493199, mean_eps: 0.000000
 3288/5000: episode: 119, duration: 0.389s, episode steps:  27, steps per second:  69, episode reward: 44.033, mean reward:  1.631 [-2.141, 32.377], mean action: 2.889 [0.000, 14.000],  loss: 0.019177, mae: 0.329362, mean_q: 0.507000, mean_eps: 0.000000
 3321/5000: episode: 120, duration: 0.463s, episode steps:  33, steps per second:  71, episode reward: 41.936, mean reward:  1.271 [-2.602, 32.170], mean action: 1.667 [0.000, 13.000],  loss: 0.019372, mae: 0.322856, mean_q: 0.531711, mean_eps: 0.000000
 3351/5000: episode: 121, duration: 0.418s, episode steps:  30, steps per second:  72, episode reward: 38.315, mean reward:  1.277 [-2.215, 31.895], mean action: 2.000 [0.000, 12.000],  loss: 0.019153, mae: 0.337959, mean_q: 0.503897, mean_eps: 0.000000
 3372/5000: episode: 122, duration: 0.309s, episode steps:  21, steps per second:  68, episode reward: 41.097, mean reward:  1.957 [-3.000, 32.807], mean action: 2.190 [0.000, 12.000],  loss: 0.021776, mae: 0.341719, mean_q: 0.499077, mean_eps: 0.000000
 3396/5000: episode: 123, duration: 0.341s, episode steps:  24, steps per second:  70, episode reward: 38.179, mean reward:  1.591 [-2.841, 31.569], mean action: 3.833 [0.000, 15.000],  loss: 0.018647, mae: 0.326848, mean_q: 0.531497, mean_eps: 0.000000
 3428/5000: episode: 124, duration: 0.451s, episode steps:  32, steps per second:  71, episode reward: 32.175, mean reward:  1.005 [-2.644, 32.330], mean action: 4.188 [0.000, 19.000],  loss: 0.021973, mae: 0.344492, mean_q: 0.535422, mean_eps: 0.000000
 3453/5000: episode: 125, duration: 0.356s, episode steps:  25, steps per second:  70, episode reward: 38.437, mean reward:  1.537 [-3.000, 32.210], mean action: 3.680 [0.000, 19.000],  loss: 0.020944, mae: 0.343262, mean_q: 0.491975, mean_eps: 0.000000
 3486/5000: episode: 126, duration: 0.478s, episode steps:  33, steps per second:  69, episode reward: 42.985, mean reward:  1.303 [-2.456, 32.505], mean action: 2.606 [0.000, 15.000],  loss: 0.018262, mae: 0.335567, mean_q: 0.474876, mean_eps: 0.000000
 3507/5000: episode: 127, duration: 0.307s, episode steps:  21, steps per second:  68, episode reward: 41.236, mean reward:  1.964 [-3.000, 32.052], mean action: 3.571 [0.000, 15.000],  loss: 0.017296, mae: 0.333497, mean_q: 0.483447, mean_eps: 0.000000
 3533/5000: episode: 128, duration: 0.379s, episode steps:  26, steps per second:  69, episode reward: 40.517, mean reward:  1.558 [-2.574, 31.655], mean action: 4.192 [0.000, 13.000],  loss: 0.018206, mae: 0.332974, mean_q: 0.499484, mean_eps: 0.000000
 3552/5000: episode: 129, duration: 0.285s, episode steps:  19, steps per second:  67, episode reward: 47.277, mean reward:  2.488 [-0.097, 32.480], mean action: 4.211 [0.000, 15.000],  loss: 0.022348, mae: 0.344690, mean_q: 0.521053, mean_eps: 0.000000
 3580/5000: episode: 130, duration: 0.401s, episode steps:  28, steps per second:  70, episode reward: 38.531, mean reward:  1.376 [-2.290, 32.140], mean action: 6.464 [0.000, 19.000],  loss: 0.018728, mae: 0.336894, mean_q: 0.461767, mean_eps: 0.000000
 3613/5000: episode: 131, duration: 0.471s, episode steps:  33, steps per second:  70, episode reward: 40.302, mean reward:  1.221 [-3.000, 32.210], mean action: 5.121 [0.000, 19.000],  loss: 0.018658, mae: 0.328527, mean_q: 0.505805, mean_eps: 0.000000
 3649/5000: episode: 132, duration: 0.507s, episode steps:  36, steps per second:  71, episode reward: 35.032, mean reward:  0.973 [-3.000, 31.993], mean action: 4.778 [0.000, 19.000],  loss: 0.020820, mae: 0.334256, mean_q: 0.546283, mean_eps: 0.000000
 3676/5000: episode: 133, duration: 0.384s, episode steps:  27, steps per second:  70, episode reward: 36.000, mean reward:  1.333 [-2.463, 32.120], mean action: 2.963 [0.000, 16.000],  loss: 0.026079, mae: 0.351711, mean_q: 0.543504, mean_eps: 0.000000
 3729/5000: episode: 134, duration: 0.731s, episode steps:  53, steps per second:  72, episode reward: 39.000, mean reward:  0.736 [-2.039, 32.110], mean action: 2.585 [0.000, 16.000],  loss: 0.022485, mae: 0.340287, mean_q: 0.573418, mean_eps: 0.000000
 3755/5000: episode: 135, duration: 0.374s, episode steps:  26, steps per second:  70, episode reward: 41.672, mean reward:  1.603 [-2.151, 32.396], mean action: 2.038 [0.000, 12.000],  loss: 0.018960, mae: 0.317447, mean_q: 0.480875, mean_eps: 0.000000
 3773/5000: episode: 136, duration: 0.272s, episode steps:  18, steps per second:  66, episode reward: 44.305, mean reward:  2.461 [-2.744, 32.280], mean action: 2.111 [1.000, 11.000],  loss: 0.018725, mae: 0.318107, mean_q: 0.516481, mean_eps: 0.000000
 3792/5000: episode: 137, duration: 0.283s, episode steps:  19, steps per second:  67, episode reward: 44.801, mean reward:  2.358 [-2.085, 32.290], mean action: 1.842 [0.000, 13.000],  loss: 0.018785, mae: 0.322611, mean_q: 0.529499, mean_eps: 0.000000
 3820/5000: episode: 138, duration: 0.390s, episode steps:  28, steps per second:  72, episode reward: 41.614, mean reward:  1.486 [-2.724, 32.564], mean action: 2.964 [0.000, 11.000],  loss: 0.020280, mae: 0.325772, mean_q: 0.518848, mean_eps: 0.000000
 3844/5000: episode: 139, duration: 0.340s, episode steps:  24, steps per second:  70, episode reward: 36.000, mean reward:  1.500 [-2.901, 32.320], mean action: 3.875 [0.000, 19.000],  loss: 0.018395, mae: 0.328506, mean_q: 0.501126, mean_eps: 0.000000
 3863/5000: episode: 140, duration: 0.280s, episode steps:  19, steps per second:  68, episode reward: 41.552, mean reward:  2.187 [-2.702, 32.070], mean action: 6.263 [0.000, 20.000],  loss: 0.018362, mae: 0.325574, mean_q: 0.477078, mean_eps: 0.000000
 3894/5000: episode: 141, duration: 0.439s, episode steps:  31, steps per second:  71, episode reward: -33.000, mean reward: -1.065 [-32.639,  3.000], mean action: 5.613 [0.000, 15.000],  loss: 0.019928, mae: 0.328778, mean_q: 0.517846, mean_eps: 0.000000
 3926/5000: episode: 142, duration: 0.453s, episode steps:  32, steps per second:  71, episode reward: 38.735, mean reward:  1.210 [-2.329, 32.605], mean action: 3.719 [0.000, 15.000],  loss: 0.020938, mae: 0.327833, mean_q: 0.538879, mean_eps: 0.000000
 3942/5000: episode: 143, duration: 0.240s, episode steps:  16, steps per second:  67, episode reward: 47.016, mean reward:  2.939 [ 0.000, 32.480], mean action: 0.500 [0.000, 1.000],  loss: 0.020743, mae: 0.333240, mean_q: 0.497899, mean_eps: 0.000000
 3967/5000: episode: 144, duration: 0.359s, episode steps:  25, steps per second:  70, episode reward: 41.150, mean reward:  1.646 [-2.357, 32.330], mean action: 4.480 [0.000, 19.000],  loss: 0.019389, mae: 0.331474, mean_q: 0.536380, mean_eps: 0.000000
 4023/5000: episode: 145, duration: 0.763s, episode steps:  56, steps per second:  73, episode reward: 32.453, mean reward:  0.580 [-2.370, 31.713], mean action: 3.893 [0.000, 16.000],  loss: 0.022513, mae: 0.340661, mean_q: 0.563905, mean_eps: 0.000000
 4057/5000: episode: 146, duration: 0.475s, episode steps:  34, steps per second:  72, episode reward: 38.298, mean reward:  1.126 [-2.190, 32.280], mean action: 4.029 [0.000, 16.000],  loss: 0.022170, mae: 0.343195, mean_q: 0.533817, mean_eps: 0.000000
 4091/5000: episode: 147, duration: 0.483s, episode steps:  34, steps per second:  70, episode reward: 41.421, mean reward:  1.218 [-3.000, 32.410], mean action: 4.441 [0.000, 16.000],  loss: 0.021413, mae: 0.334795, mean_q: 0.559426, mean_eps: 0.000000
 4111/5000: episode: 148, duration: 0.295s, episode steps:  20, steps per second:  68, episode reward: 41.897, mean reward:  2.095 [-2.752, 32.330], mean action: 3.300 [0.000, 16.000],  loss: 0.022053, mae: 0.342570, mean_q: 0.436658, mean_eps: 0.000000
 4137/5000: episode: 149, duration: 0.367s, episode steps:  26, steps per second:  71, episode reward: 35.121, mean reward:  1.351 [-2.461, 31.601], mean action: 4.615 [0.000, 19.000],  loss: 0.019931, mae: 0.323116, mean_q: 0.524110, mean_eps: 0.000000
 4158/5000: episode: 150, duration: 0.298s, episode steps:  21, steps per second:  70, episode reward: 41.674, mean reward:  1.984 [-3.000, 32.180], mean action: 2.524 [0.000, 19.000],  loss: 0.022667, mae: 0.327164, mean_q: 0.525269, mean_eps: 0.000000
 4183/5000: episode: 151, duration: 0.375s, episode steps:  25, steps per second:  67, episode reward: 46.856, mean reward:  1.874 [-0.540, 32.261], mean action: 4.200 [2.000, 14.000],  loss: 0.021544, mae: 0.334030, mean_q: 0.488731, mean_eps: 0.000000
 4213/5000: episode: 152, duration: 0.434s, episode steps:  30, steps per second:  69, episode reward: 41.002, mean reward:  1.367 [-2.167, 32.183], mean action: 4.000 [0.000, 18.000],  loss: 0.020539, mae: 0.328218, mean_q: 0.518387, mean_eps: 0.000000
 4238/5000: episode: 153, duration: 0.352s, episode steps:  25, steps per second:  71, episode reward: 39.000, mean reward:  1.560 [-2.790, 32.010], mean action: 4.040 [1.000, 15.000],  loss: 0.016230, mae: 0.309439, mean_q: 0.528397, mean_eps: 0.000000
 4259/5000: episode: 154, duration: 0.304s, episode steps:  21, steps per second:  69, episode reward: 41.125, mean reward:  1.958 [-3.000, 31.880], mean action: 3.476 [0.000, 13.000],  loss: 0.021664, mae: 0.325767, mean_q: 0.545062, mean_eps: 0.000000
 4282/5000: episode: 155, duration: 0.517s, episode steps:  23, steps per second:  44, episode reward: 44.914, mean reward:  1.953 [-2.069, 32.580], mean action: 3.087 [0.000, 14.000],  loss: 0.018476, mae: 0.322351, mean_q: 0.601141, mean_eps: 0.000000
 4301/5000: episode: 156, duration: 0.305s, episode steps:  19, steps per second:  62, episode reward: 41.140, mean reward:  2.165 [-2.886, 32.020], mean action: 2.316 [0.000, 11.000],  loss: 0.013394, mae: 0.283881, mean_q: 0.570664, mean_eps: 0.000000
 4320/5000: episode: 157, duration: 0.279s, episode steps:  19, steps per second:  68, episode reward: 41.436, mean reward:  2.181 [-2.226, 32.317], mean action: 4.263 [0.000, 19.000],  loss: 0.022962, mae: 0.337118, mean_q: 0.497999, mean_eps: 0.000000
 4340/5000: episode: 158, duration: 0.293s, episode steps:  20, steps per second:  68, episode reward: 44.264, mean reward:  2.213 [-2.287, 32.140], mean action: 4.500 [1.000, 19.000],  loss: 0.020129, mae: 0.325571, mean_q: 0.461312, mean_eps: 0.000000
 4380/5000: episode: 159, duration: 0.555s, episode steps:  40, steps per second:  72, episode reward: 38.843, mean reward:  0.971 [-3.000, 32.600], mean action: 3.900 [0.000, 19.000],  loss: 0.022077, mae: 0.331652, mean_q: 0.499727, mean_eps: 0.000000
 4412/5000: episode: 160, duration: 0.448s, episode steps:  32, steps per second:  71, episode reward: 40.775, mean reward:  1.274 [-2.386, 32.054], mean action: 5.688 [0.000, 13.000],  loss: 0.020236, mae: 0.322972, mean_q: 0.546671, mean_eps: 0.000000
 4448/5000: episode: 161, duration: 0.507s, episode steps:  36, steps per second:  71, episode reward: 44.070, mean reward:  1.224 [-2.330, 31.913], mean action: 3.250 [0.000, 14.000],  loss: 0.020196, mae: 0.328881, mean_q: 0.554937, mean_eps: 0.000000
 4468/5000: episode: 162, duration: 0.298s, episode steps:  20, steps per second:  67, episode reward: 41.116, mean reward:  2.056 [-2.328, 32.180], mean action: 2.350 [0.000, 9.000],  loss: 0.024089, mae: 0.346628, mean_q: 0.560357, mean_eps: 0.000000
 4484/5000: episode: 163, duration: 0.239s, episode steps:  16, steps per second:  67, episode reward: 41.903, mean reward:  2.619 [-2.548, 32.393], mean action: 4.688 [1.000, 14.000],  loss: 0.019366, mae: 0.315751, mean_q: 0.588176, mean_eps: 0.000000
 4511/5000: episode: 164, duration: 0.381s, episode steps:  27, steps per second:  71, episode reward: 38.428, mean reward:  1.423 [-3.000, 31.810], mean action: 4.778 [0.000, 14.000],  loss: 0.022590, mae: 0.326150, mean_q: 0.566019, mean_eps: 0.000000
 4539/5000: episode: 165, duration: 0.398s, episode steps:  28, steps per second:  70, episode reward: 41.222, mean reward:  1.472 [-3.000, 32.590], mean action: 2.250 [0.000, 13.000],  loss: 0.020712, mae: 0.316919, mean_q: 0.518161, mean_eps: 0.000000
 4566/5000: episode: 166, duration: 0.381s, episode steps:  27, steps per second:  71, episode reward: 41.062, mean reward:  1.521 [-2.549, 32.270], mean action: 3.926 [0.000, 16.000],  loss: 0.019526, mae: 0.313731, mean_q: 0.524201, mean_eps: 0.000000
 4584/5000: episode: 167, duration: 0.263s, episode steps:  18, steps per second:  69, episode reward: 38.182, mean reward:  2.121 [-2.700, 31.402], mean action: 5.056 [1.000, 15.000],  loss: 0.021296, mae: 0.328708, mean_q: 0.509980, mean_eps: 0.000000
 4612/5000: episode: 168, duration: 0.399s, episode steps:  28, steps per second:  70, episode reward: 41.024, mean reward:  1.465 [-2.691, 31.912], mean action: 3.036 [1.000, 19.000],  loss: 0.025189, mae: 0.344591, mean_q: 0.516518, mean_eps: 0.000000
 4641/5000: episode: 169, duration: 0.443s, episode steps:  29, steps per second:  65, episode reward: 35.520, mean reward:  1.225 [-3.000, 31.963], mean action: 3.759 [0.000, 19.000],  loss: 0.017476, mae: 0.311740, mean_q: 0.578772, mean_eps: 0.000000
 4670/5000: episode: 170, duration: 0.433s, episode steps:  29, steps per second:  67, episode reward: 38.353, mean reward:  1.323 [-2.685, 31.981], mean action: 3.862 [0.000, 19.000],  loss: 0.021788, mae: 0.326144, mean_q: 0.539165, mean_eps: 0.000000
 4688/5000: episode: 171, duration: 0.276s, episode steps:  18, steps per second:  65, episode reward: 44.336, mean reward:  2.463 [-2.710, 32.270], mean action: 4.056 [0.000, 19.000],  loss: 0.021339, mae: 0.322652, mean_q: 0.499809, mean_eps: 0.000000
 4718/5000: episode: 172, duration: 0.432s, episode steps:  30, steps per second:  70, episode reward: 35.381, mean reward:  1.179 [-2.803, 32.280], mean action: 5.067 [0.000, 15.000],  loss: 0.021160, mae: 0.325508, mean_q: 0.520485, mean_eps: 0.000000
 4754/5000: episode: 173, duration: 0.521s, episode steps:  36, steps per second:  69, episode reward: 44.413, mean reward:  1.234 [-2.055, 32.470], mean action: 5.167 [0.000, 19.000],  loss: 0.019212, mae: 0.312389, mean_q: 0.566613, mean_eps: 0.000000
 4786/5000: episode: 174, duration: 0.453s, episode steps:  32, steps per second:  71, episode reward: 38.901, mean reward:  1.216 [-3.000, 32.321], mean action: 2.156 [0.000, 19.000],  loss: 0.025750, mae: 0.348245, mean_q: 0.545698, mean_eps: 0.000000
 4822/5000: episode: 175, duration: 0.508s, episode steps:  36, steps per second:  71, episode reward: 34.966, mean reward:  0.971 [-2.176, 32.570], mean action: 3.583 [0.000, 19.000],  loss: 0.021237, mae: 0.330256, mean_q: 0.570731, mean_eps: 0.000000
 4849/5000: episode: 176, duration: 0.391s, episode steps:  27, steps per second:  69, episode reward: 40.934, mean reward:  1.516 [-2.235, 32.250], mean action: 5.222 [0.000, 16.000],  loss: 0.021457, mae: 0.333719, mean_q: 0.529438, mean_eps: 0.000000
 4873/5000: episode: 177, duration: 0.348s, episode steps:  24, steps per second:  69, episode reward: 38.735, mean reward:  1.614 [-3.000, 31.917], mean action: 3.167 [0.000, 15.000],  loss: 0.022036, mae: 0.337209, mean_q: 0.592883, mean_eps: 0.000000
 4904/5000: episode: 178, duration: 0.436s, episode steps:  31, steps per second:  71, episode reward: 38.498, mean reward:  1.242 [-2.510, 32.010], mean action: 2.613 [1.000, 16.000],  loss: 0.021561, mae: 0.335501, mean_q: 0.586509, mean_eps: 0.000000
 4944/5000: episode: 179, duration: 0.569s, episode steps:  40, steps per second:  70, episode reward: 43.876, mean reward:  1.097 [-2.242, 32.280], mean action: 1.825 [0.000, 14.000],  loss: 0.022468, mae: 0.340928, mean_q: 0.584724, mean_eps: 0.000000
 4980/5000: episode: 180, duration: 0.512s, episode steps:  36, steps per second:  70, episode reward: 38.464, mean reward:  1.068 [-2.357, 32.260], mean action: 3.639 [0.000, 19.000],  loss: 0.023854, mae: 0.339328, mean_q: 0.540861, mean_eps: 0.000000
done, took 83.310 seconds
DQN Evaluation: 9730 victories out of 11359 episodes
Training for 5000 steps ...
   25/5000: episode: 1, duration: 0.194s, episode steps:  25, steps per second: 129, episode reward: -32.520, mean reward: -1.301 [-31.848,  2.560], mean action: 5.600 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   42/5000: episode: 2, duration: 0.127s, episode steps:  17, steps per second: 134, episode reward: -41.940, mean reward: -2.467 [-33.000,  2.222], mean action: 5.706 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   56/5000: episode: 3, duration: 0.117s, episode steps:  14, steps per second: 119, episode reward: 41.617, mean reward:  2.973 [-3.000, 32.904], mean action: 5.786 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   70/5000: episode: 4, duration: 0.112s, episode steps:  14, steps per second: 125, episode reward: 40.812, mean reward:  2.915 [-2.117, 31.757], mean action: 4.929 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   90/5000: episode: 5, duration: 0.147s, episode steps:  20, steps per second: 136, episode reward: 37.597, mean reward:  1.880 [-3.000, 32.390], mean action: 6.300 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  125/5000: episode: 6, duration: 0.220s, episode steps:  35, steps per second: 159, episode reward: 32.464, mean reward:  0.928 [-2.599, 32.655], mean action: 4.429 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  152/5000: episode: 7, duration: 0.173s, episode steps:  27, steps per second: 156, episode reward: 34.669, mean reward:  1.284 [-3.000, 32.527], mean action: 7.148 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  178/5000: episode: 8, duration: 0.172s, episode steps:  26, steps per second: 151, episode reward: 32.280, mean reward:  1.242 [-3.000, 31.940], mean action: 5.846 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  202/5000: episode: 9, duration: 0.170s, episode steps:  24, steps per second: 141, episode reward: 38.278, mean reward:  1.595 [-2.382, 31.693], mean action: 3.625 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  222/5000: episode: 10, duration: 0.136s, episode steps:  20, steps per second: 148, episode reward: -42.000, mean reward: -2.100 [-32.076,  2.181], mean action: 7.350 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  238/5000: episode: 11, duration: 0.118s, episode steps:  16, steps per second: 136, episode reward: 41.160, mean reward:  2.572 [-2.105, 32.903], mean action: 4.125 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  264/5000: episode: 12, duration: 0.180s, episode steps:  26, steps per second: 145, episode reward: -33.000, mean reward: -1.269 [-30.210,  2.187], mean action: 5.115 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  285/5000: episode: 13, duration: 0.149s, episode steps:  21, steps per second: 141, episode reward: 36.000, mean reward:  1.714 [-3.000, 32.330], mean action: 5.238 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  316/5000: episode: 14, duration: 0.198s, episode steps:  31, steps per second: 156, episode reward: 35.398, mean reward:  1.142 [-2.391, 32.503], mean action: 5.226 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  345/5000: episode: 15, duration: 0.199s, episode steps:  29, steps per second: 146, episode reward: 34.802, mean reward:  1.200 [-3.000, 32.343], mean action: 6.621 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  354/5000: episode: 16, duration: 0.077s, episode steps:   9, steps per second: 117, episode reward: 44.223, mean reward:  4.914 [-2.073, 32.787], mean action: 3.333 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  373/5000: episode: 17, duration: 0.135s, episode steps:  19, steps per second: 141, episode reward: 40.231, mean reward:  2.117 [-2.628, 31.382], mean action: 5.421 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  409/5000: episode: 18, duration: 0.216s, episode steps:  36, steps per second: 166, episode reward: -32.620, mean reward: -0.906 [-31.921,  3.000], mean action: 3.167 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  427/5000: episode: 19, duration: 0.128s, episode steps:  18, steps per second: 141, episode reward: 35.666, mean reward:  1.981 [-2.515, 32.206], mean action: 5.500 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  449/5000: episode: 20, duration: 0.156s, episode steps:  22, steps per second: 141, episode reward: 35.860, mean reward:  1.630 [-2.388, 32.310], mean action: 5.773 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  479/5000: episode: 21, duration: 0.187s, episode steps:  30, steps per second: 161, episode reward: 32.365, mean reward:  1.079 [-3.000, 32.230], mean action: 5.633 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  492/5000: episode: 22, duration: 0.099s, episode steps:  13, steps per second: 132, episode reward: 42.000, mean reward:  3.231 [-2.360, 32.630], mean action: 4.462 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  510/5000: episode: 23, duration: 0.110s, episode steps:  18, steps per second: 164, episode reward: -41.880, mean reward: -2.327 [-33.000,  2.230], mean action: 5.667 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  536/5000: episode: 24, duration: 0.176s, episode steps:  26, steps per second: 147, episode reward: 42.000, mean reward:  1.615 [-2.429, 32.050], mean action: 3.731 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  554/5000: episode: 25, duration: 0.123s, episode steps:  18, steps per second: 147, episode reward: 36.000, mean reward:  2.000 [-2.843, 33.000], mean action: 4.167 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  577/5000: episode: 26, duration: 0.152s, episode steps:  23, steps per second: 152, episode reward: 32.377, mean reward:  1.408 [-2.567, 32.127], mean action: 4.739 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  599/5000: episode: 27, duration: 0.150s, episode steps:  22, steps per second: 147, episode reward: 39.000, mean reward:  1.773 [-2.532, 32.220], mean action: 3.909 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  617/5000: episode: 28, duration: 0.130s, episode steps:  18, steps per second: 138, episode reward: 35.777, mean reward:  1.988 [-3.000, 32.507], mean action: 6.111 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  640/5000: episode: 29, duration: 0.238s, episode steps:  23, steps per second:  97, episode reward: 35.275, mean reward:  1.534 [-2.168, 29.746], mean action: 4.348 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  661/5000: episode: 30, duration: 0.149s, episode steps:  21, steps per second: 141, episode reward: 35.191, mean reward:  1.676 [-2.417, 31.871], mean action: 5.190 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  681/5000: episode: 31, duration: 0.144s, episode steps:  20, steps per second: 138, episode reward: 38.217, mean reward:  1.911 [-2.424, 32.238], mean action: 3.300 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  706/5000: episode: 32, duration: 0.163s, episode steps:  25, steps per second: 154, episode reward: 35.839, mean reward:  1.434 [-3.000, 32.119], mean action: 3.360 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  718/5000: episode: 33, duration: 0.093s, episode steps:  12, steps per second: 129, episode reward: 44.890, mean reward:  3.741 [-2.646, 32.349], mean action: 3.417 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  741/5000: episode: 34, duration: 0.157s, episode steps:  23, steps per second: 146, episode reward: 40.616, mean reward:  1.766 [-2.464, 32.210], mean action: 4.609 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  761/5000: episode: 35, duration: 0.137s, episode steps:  20, steps per second: 146, episode reward: -32.660, mean reward: -1.633 [-32.329,  2.383], mean action: 5.000 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  788/5000: episode: 36, duration: 0.185s, episode steps:  27, steps per second: 146, episode reward: 41.408, mean reward:  1.534 [-2.835, 32.160], mean action: 3.185 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  809/5000: episode: 37, duration: 0.137s, episode steps:  21, steps per second: 153, episode reward: -33.000, mean reward: -1.571 [-29.917,  3.061], mean action: 5.905 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  830/5000: episode: 38, duration: 0.152s, episode steps:  21, steps per second: 138, episode reward: 32.068, mean reward:  1.527 [-2.972, 33.000], mean action: 5.714 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  856/5000: episode: 39, duration: 0.175s, episode steps:  26, steps per second: 148, episode reward: -32.500, mean reward: -1.250 [-31.832,  2.650], mean action: 4.154 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  878/5000: episode: 40, duration: 0.153s, episode steps:  22, steps per second: 144, episode reward: 39.000, mean reward:  1.773 [-2.903, 32.210], mean action: 3.545 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  906/5000: episode: 41, duration: 0.187s, episode steps:  28, steps per second: 150, episode reward: -33.000, mean reward: -1.179 [-32.328,  2.502], mean action: 4.107 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  922/5000: episode: 42, duration: 0.119s, episode steps:  16, steps per second: 135, episode reward: 41.198, mean reward:  2.575 [-2.624, 32.087], mean action: 4.062 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  942/5000: episode: 43, duration: 0.137s, episode steps:  20, steps per second: 146, episode reward: 42.000, mean reward:  2.100 [-2.328, 32.300], mean action: 4.100 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  958/5000: episode: 44, duration: 0.121s, episode steps:  16, steps per second: 132, episode reward: 43.436, mean reward:  2.715 [-2.179, 32.290], mean action: 5.625 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  980/5000: episode: 45, duration: 0.150s, episode steps:  22, steps per second: 147, episode reward: 32.233, mean reward:  1.465 [-2.803, 31.673], mean action: 4.682 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1005/5000: episode: 46, duration: 0.204s, episode steps:  25, steps per second: 123, episode reward: 37.834, mean reward:  1.513 [-2.668, 32.394], mean action: 4.800 [0.000, 19.000],  loss: 0.013619, mae: 0.302672, mean_q: 0.566958, mean_eps: 0.000000
 1021/5000: episode: 47, duration: 0.234s, episode steps:  16, steps per second:  68, episode reward: 38.633, mean reward:  2.415 [-2.216, 32.742], mean action: 4.750 [1.000, 19.000],  loss: 0.021012, mae: 0.325730, mean_q: 0.580429, mean_eps: 0.000000
 1040/5000: episode: 48, duration: 0.281s, episode steps:  19, steps per second:  68, episode reward: 38.164, mean reward:  2.009 [-2.417, 32.420], mean action: 3.211 [0.000, 16.000],  loss: 0.020705, mae: 0.329902, mean_q: 0.579675, mean_eps: 0.000000
 1064/5000: episode: 49, duration: 0.338s, episode steps:  24, steps per second:  71, episode reward: 35.325, mean reward:  1.472 [-2.455, 32.020], mean action: 4.500 [1.000, 16.000],  loss: 0.022285, mae: 0.347227, mean_q: 0.663454, mean_eps: 0.000000
 1081/5000: episode: 50, duration: 0.279s, episode steps:  17, steps per second:  61, episode reward: 36.000, mean reward:  2.118 [-2.704, 32.750], mean action: 4.588 [1.000, 16.000],  loss: 0.022104, mae: 0.343476, mean_q: 0.667230, mean_eps: 0.000000
 1101/5000: episode: 51, duration: 0.455s, episode steps:  20, steps per second:  44, episode reward: 41.431, mean reward:  2.072 [-2.266, 32.330], mean action: 2.050 [0.000, 16.000],  loss: 0.019316, mae: 0.333805, mean_q: 0.658330, mean_eps: 0.000000
 1116/5000: episode: 52, duration: 0.250s, episode steps:  15, steps per second:  60, episode reward: 39.000, mean reward:  2.600 [-3.000, 32.120], mean action: 4.200 [0.000, 16.000],  loss: 0.019729, mae: 0.334278, mean_q: 0.619304, mean_eps: 0.000000
 1144/5000: episode: 53, duration: 0.399s, episode steps:  28, steps per second:  70, episode reward: 32.648, mean reward:  1.166 [-2.328, 32.270], mean action: 3.179 [0.000, 9.000],  loss: 0.022516, mae: 0.349016, mean_q: 0.608536, mean_eps: 0.000000
 1165/5000: episode: 54, duration: 0.296s, episode steps:  21, steps per second:  71, episode reward: 32.562, mean reward:  1.551 [-3.000, 32.440], mean action: 8.476 [0.000, 19.000],  loss: 0.020627, mae: 0.338521, mean_q: 0.566516, mean_eps: 0.000000
 1182/5000: episode: 55, duration: 0.252s, episode steps:  17, steps per second:  67, episode reward: 38.626, mean reward:  2.272 [-2.251, 33.000], mean action: 2.235 [0.000, 9.000],  loss: 0.017428, mae: 0.314462, mean_q: 0.599748, mean_eps: 0.000000
 1205/5000: episode: 56, duration: 0.333s, episode steps:  23, steps per second:  69, episode reward: 35.590, mean reward:  1.547 [-2.663, 32.221], mean action: 2.739 [0.000, 16.000],  loss: 0.020767, mae: 0.337559, mean_q: 0.590949, mean_eps: 0.000000
 1225/5000: episode: 57, duration: 0.294s, episode steps:  20, steps per second:  68, episode reward: 41.605, mean reward:  2.080 [-2.219, 31.952], mean action: 1.750 [0.000, 9.000],  loss: 0.015970, mae: 0.317952, mean_q: 0.570990, mean_eps: 0.000000
 1247/5000: episode: 58, duration: 0.316s, episode steps:  22, steps per second:  70, episode reward: 32.904, mean reward:  1.496 [-3.000, 32.324], mean action: 5.727 [0.000, 19.000],  loss: 0.018477, mae: 0.334455, mean_q: 0.541477, mean_eps: 0.000000
 1273/5000: episode: 59, duration: 0.368s, episode steps:  26, steps per second:  71, episode reward: -32.790, mean reward: -1.261 [-31.891,  2.904], mean action: 6.385 [0.000, 19.000],  loss: 0.019474, mae: 0.341061, mean_q: 0.500760, mean_eps: 0.000000
 1298/5000: episode: 60, duration: 0.352s, episode steps:  25, steps per second:  71, episode reward: -38.050, mean reward: -1.522 [-31.955,  2.210], mean action: 8.520 [0.000, 19.000],  loss: 0.023244, mae: 0.348157, mean_q: 0.497205, mean_eps: 0.000000
 1324/5000: episode: 61, duration: 0.379s, episode steps:  26, steps per second:  69, episode reward: 37.047, mean reward:  1.425 [-2.421, 31.992], mean action: 4.731 [1.000, 15.000],  loss: 0.019619, mae: 0.334341, mean_q: 0.549869, mean_eps: 0.000000
 1338/5000: episode: 62, duration: 0.208s, episode steps:  14, steps per second:  67, episode reward: 38.940, mean reward:  2.781 [-2.907, 33.000], mean action: 4.071 [0.000, 15.000],  loss: 0.023135, mae: 0.350113, mean_q: 0.571428, mean_eps: 0.000000
 1363/5000: episode: 63, duration: 0.356s, episode steps:  25, steps per second:  70, episode reward: 32.508, mean reward:  1.300 [-3.000, 32.090], mean action: 5.640 [0.000, 15.000],  loss: 0.023742, mae: 0.348450, mean_q: 0.551393, mean_eps: 0.000000
 1386/5000: episode: 64, duration: 0.332s, episode steps:  23, steps per second:  69, episode reward: 38.166, mean reward:  1.659 [-2.511, 32.080], mean action: 6.565 [0.000, 15.000],  loss: 0.020659, mae: 0.338807, mean_q: 0.635600, mean_eps: 0.000000
 1413/5000: episode: 65, duration: 0.391s, episode steps:  27, steps per second:  69, episode reward: -32.350, mean reward: -1.198 [-32.111,  2.320], mean action: 3.667 [0.000, 16.000],  loss: 0.017877, mae: 0.318193, mean_q: 0.524645, mean_eps: 0.000000
 1426/5000: episode: 66, duration: 0.195s, episode steps:  13, steps per second:  67, episode reward: 41.508, mean reward:  3.193 [-3.000, 33.000], mean action: 4.538 [0.000, 15.000],  loss: 0.027486, mae: 0.371476, mean_q: 0.506014, mean_eps: 0.000000
 1452/5000: episode: 67, duration: 0.366s, episode steps:  26, steps per second:  71, episode reward: -35.490, mean reward: -1.365 [-32.017,  2.219], mean action: 5.769 [0.000, 19.000],  loss: 0.020359, mae: 0.340219, mean_q: 0.489338, mean_eps: 0.000000
 1475/5000: episode: 68, duration: 0.330s, episode steps:  23, steps per second:  70, episode reward: -32.150, mean reward: -1.398 [-31.985,  2.520], mean action: 3.391 [0.000, 12.000],  loss: 0.022128, mae: 0.342386, mean_q: 0.493884, mean_eps: 0.000000
 1497/5000: episode: 69, duration: 0.317s, episode steps:  22, steps per second:  69, episode reward: 38.229, mean reward:  1.738 [-2.204, 32.190], mean action: 5.045 [0.000, 20.000],  loss: 0.022607, mae: 0.343011, mean_q: 0.529186, mean_eps: 0.000000
 1514/5000: episode: 70, duration: 0.249s, episode steps:  17, steps per second:  68, episode reward: 36.000, mean reward:  2.118 [-3.000, 33.000], mean action: 7.882 [0.000, 19.000],  loss: 0.017596, mae: 0.312369, mean_q: 0.542096, mean_eps: 0.000000
 1540/5000: episode: 71, duration: 0.377s, episode steps:  26, steps per second:  69, episode reward: 36.922, mean reward:  1.420 [-2.368, 32.470], mean action: 4.885 [0.000, 19.000],  loss: 0.019531, mae: 0.329640, mean_q: 0.507623, mean_eps: 0.000000
 1562/5000: episode: 72, duration: 0.316s, episode steps:  22, steps per second:  70, episode reward: 41.202, mean reward:  1.873 [-2.374, 32.010], mean action: 2.818 [0.000, 12.000],  loss: 0.020523, mae: 0.330540, mean_q: 0.517102, mean_eps: 0.000000
 1588/5000: episode: 73, duration: 0.380s, episode steps:  26, steps per second:  68, episode reward: 34.449, mean reward:  1.325 [-2.535, 32.476], mean action: 7.538 [0.000, 21.000],  loss: 0.024129, mae: 0.357121, mean_q: 0.542666, mean_eps: 0.000000
 1611/5000: episode: 74, duration: 0.331s, episode steps:  23, steps per second:  70, episode reward: 44.881, mean reward:  1.951 [-2.023, 32.250], mean action: 2.217 [0.000, 16.000],  loss: 0.017618, mae: 0.328696, mean_q: 0.497094, mean_eps: 0.000000
 1635/5000: episode: 75, duration: 0.340s, episode steps:  24, steps per second:  71, episode reward: 35.160, mean reward:  1.465 [-2.467, 33.000], mean action: 4.125 [2.000, 12.000],  loss: 0.021406, mae: 0.334675, mean_q: 0.478332, mean_eps: 0.000000
 1654/5000: episode: 76, duration: 0.273s, episode steps:  19, steps per second:  69, episode reward: -39.000, mean reward: -2.053 [-32.187,  2.220], mean action: 4.158 [0.000, 12.000],  loss: 0.022042, mae: 0.348335, mean_q: 0.500561, mean_eps: 0.000000
 1689/5000: episode: 77, duration: 0.486s, episode steps:  35, steps per second:  72, episode reward: 34.298, mean reward:  0.980 [-2.385, 32.225], mean action: 6.286 [0.000, 19.000],  loss: 0.020634, mae: 0.340416, mean_q: 0.546801, mean_eps: 0.000000
 1707/5000: episode: 78, duration: 0.264s, episode steps:  18, steps per second:  68, episode reward: 44.111, mean reward:  2.451 [-2.236, 32.470], mean action: 1.611 [0.000, 11.000],  loss: 0.018797, mae: 0.329305, mean_q: 0.534688, mean_eps: 0.000000
 1738/5000: episode: 79, duration: 0.454s, episode steps:  31, steps per second:  68, episode reward: 32.398, mean reward:  1.045 [-3.000, 32.550], mean action: 10.613 [0.000, 19.000],  loss: 0.020911, mae: 0.336719, mean_q: 0.523253, mean_eps: 0.000000
 1762/5000: episode: 80, duration: 0.345s, episode steps:  24, steps per second:  69, episode reward: -32.320, mean reward: -1.347 [-32.118,  2.845], mean action: 6.208 [0.000, 19.000],  loss: 0.021905, mae: 0.334288, mean_q: 0.558009, mean_eps: 0.000000
 1782/5000: episode: 81, duration: 0.287s, episode steps:  20, steps per second:  70, episode reward: 41.163, mean reward:  2.058 [-2.248, 33.000], mean action: 2.300 [0.000, 11.000],  loss: 0.020940, mae: 0.327295, mean_q: 0.567059, mean_eps: 0.000000
 1804/5000: episode: 82, duration: 0.328s, episode steps:  22, steps per second:  67, episode reward: 32.486, mean reward:  1.477 [-3.000, 32.045], mean action: 4.455 [0.000, 15.000],  loss: 0.020643, mae: 0.331395, mean_q: 0.556064, mean_eps: 0.000000
 1828/5000: episode: 83, duration: 0.348s, episode steps:  24, steps per second:  69, episode reward: 38.591, mean reward:  1.608 [-2.533, 32.738], mean action: 4.000 [0.000, 11.000],  loss: 0.027286, mae: 0.367652, mean_q: 0.544158, mean_eps: 0.000000
 1854/5000: episode: 84, duration: 0.378s, episode steps:  26, steps per second:  69, episode reward: 35.655, mean reward:  1.371 [-2.354, 31.866], mean action: 4.654 [0.000, 15.000],  loss: 0.024019, mae: 0.347725, mean_q: 0.536219, mean_eps: 0.000000
 1879/5000: episode: 85, duration: 0.370s, episode steps:  25, steps per second:  68, episode reward: -30.000, mean reward: -1.200 [-29.594,  2.570], mean action: 7.240 [0.000, 19.000],  loss: 0.020069, mae: 0.320951, mean_q: 0.521023, mean_eps: 0.000000
 1900/5000: episode: 86, duration: 0.309s, episode steps:  21, steps per second:  68, episode reward: 38.679, mean reward:  1.842 [-2.343, 32.101], mean action: 3.286 [0.000, 12.000],  loss: 0.023811, mae: 0.337623, mean_q: 0.521668, mean_eps: 0.000000
 1928/5000: episode: 87, duration: 0.410s, episode steps:  28, steps per second:  68, episode reward: 37.577, mean reward:  1.342 [-3.000, 32.134], mean action: 2.893 [0.000, 12.000],  loss: 0.020326, mae: 0.325438, mean_q: 0.515203, mean_eps: 0.000000
 1955/5000: episode: 88, duration: 0.492s, episode steps:  27, steps per second:  55, episode reward: 38.534, mean reward:  1.427 [-2.371, 33.000], mean action: 5.333 [3.000, 12.000],  loss: 0.021631, mae: 0.341328, mean_q: 0.483047, mean_eps: 0.000000
 1975/5000: episode: 89, duration: 0.291s, episode steps:  20, steps per second:  69, episode reward: 37.521, mean reward:  1.876 [-2.584, 32.148], mean action: 4.200 [0.000, 19.000],  loss: 0.017039, mae: 0.315841, mean_q: 0.512068, mean_eps: 0.000000
 1996/5000: episode: 90, duration: 0.303s, episode steps:  21, steps per second:  69, episode reward: 33.000, mean reward:  1.571 [-3.000, 32.210], mean action: 4.381 [0.000, 19.000],  loss: 0.019065, mae: 0.323385, mean_q: 0.543012, mean_eps: 0.000000
 2019/5000: episode: 91, duration: 0.332s, episode steps:  23, steps per second:  69, episode reward: 35.757, mean reward:  1.555 [-2.285, 31.807], mean action: 3.391 [0.000, 15.000],  loss: 0.018176, mae: 0.313749, mean_q: 0.522753, mean_eps: 0.000000
 2041/5000: episode: 92, duration: 0.317s, episode steps:  22, steps per second:  69, episode reward: 35.436, mean reward:  1.611 [-2.126, 32.387], mean action: 3.500 [0.000, 15.000],  loss: 0.019117, mae: 0.331665, mean_q: 0.509700, mean_eps: 0.000000
 2064/5000: episode: 93, duration: 0.324s, episode steps:  23, steps per second:  71, episode reward: -36.000, mean reward: -1.565 [-32.152,  2.690], mean action: 5.087 [0.000, 19.000],  loss: 0.022038, mae: 0.337504, mean_q: 0.538797, mean_eps: 0.000000
 2091/5000: episode: 94, duration: 0.382s, episode steps:  27, steps per second:  71, episode reward: 38.222, mean reward:  1.416 [-2.611, 31.988], mean action: 2.185 [0.000, 15.000],  loss: 0.021023, mae: 0.335006, mean_q: 0.545678, mean_eps: 0.000000
 2135/5000: episode: 95, duration: 0.615s, episode steps:  44, steps per second:  72, episode reward: 34.897, mean reward:  0.793 [-2.319, 32.300], mean action: 3.295 [0.000, 19.000],  loss: 0.020344, mae: 0.335484, mean_q: 0.522475, mean_eps: 0.000000
 2155/5000: episode: 96, duration: 0.290s, episode steps:  20, steps per second:  69, episode reward: 33.000, mean reward:  1.650 [-2.504, 30.559], mean action: 5.550 [0.000, 19.000],  loss: 0.020643, mae: 0.343644, mean_q: 0.545841, mean_eps: 0.000000
 2178/5000: episode: 97, duration: 0.329s, episode steps:  23, steps per second:  70, episode reward: 44.213, mean reward:  1.922 [-2.822, 32.160], mean action: 1.087 [0.000, 9.000],  loss: 0.026585, mae: 0.365360, mean_q: 0.580186, mean_eps: 0.000000
 2203/5000: episode: 98, duration: 0.353s, episode steps:  25, steps per second:  71, episode reward: -32.520, mean reward: -1.301 [-33.000,  2.904], mean action: 5.600 [0.000, 21.000],  loss: 0.020562, mae: 0.342310, mean_q: 0.572949, mean_eps: 0.000000
 2227/5000: episode: 99, duration: 0.340s, episode steps:  24, steps per second:  71, episode reward: -32.440, mean reward: -1.352 [-31.958,  2.490], mean action: 3.333 [0.000, 16.000],  loss: 0.019638, mae: 0.335464, mean_q: 0.588593, mean_eps: 0.000000
 2248/5000: episode: 100, duration: 0.298s, episode steps:  21, steps per second:  70, episode reward: 35.124, mean reward:  1.673 [-3.000, 33.000], mean action: 3.429 [0.000, 19.000],  loss: 0.021012, mae: 0.350079, mean_q: 0.583556, mean_eps: 0.000000
 2283/5000: episode: 101, duration: 0.493s, episode steps:  35, steps per second:  71, episode reward: 38.008, mean reward:  1.086 [-2.546, 33.000], mean action: 5.286 [0.000, 9.000],  loss: 0.020910, mae: 0.344959, mean_q: 0.567952, mean_eps: 0.000000
 2304/5000: episode: 102, duration: 0.305s, episode steps:  21, steps per second:  69, episode reward: 37.295, mean reward:  1.776 [-2.468, 32.195], mean action: 3.333 [0.000, 19.000],  loss: 0.018637, mae: 0.339577, mean_q: 0.533365, mean_eps: 0.000000
 2325/5000: episode: 103, duration: 0.298s, episode steps:  21, steps per second:  70, episode reward: 35.064, mean reward:  1.670 [-2.613, 31.950], mean action: 3.381 [0.000, 16.000],  loss: 0.023487, mae: 0.356134, mean_q: 0.513048, mean_eps: 0.000000
 2349/5000: episode: 104, duration: 0.354s, episode steps:  24, steps per second:  68, episode reward: 35.498, mean reward:  1.479 [-3.000, 32.318], mean action: 4.542 [0.000, 16.000],  loss: 0.019946, mae: 0.334102, mean_q: 0.476288, mean_eps: 0.000000
 2376/5000: episode: 105, duration: 0.381s, episode steps:  27, steps per second:  71, episode reward: 32.017, mean reward:  1.186 [-2.861, 31.556], mean action: 4.963 [0.000, 19.000],  loss: 0.020102, mae: 0.337256, mean_q: 0.563133, mean_eps: 0.000000
 2400/5000: episode: 106, duration: 0.348s, episode steps:  24, steps per second:  69, episode reward: 35.610, mean reward:  1.484 [-3.000, 32.105], mean action: 3.083 [0.000, 15.000],  loss: 0.020886, mae: 0.340694, mean_q: 0.507526, mean_eps: 0.000000
 2428/5000: episode: 107, duration: 0.392s, episode steps:  28, steps per second:  72, episode reward: -32.410, mean reward: -1.158 [-32.187,  2.971], mean action: 7.536 [0.000, 19.000],  loss: 0.024293, mae: 0.354109, mean_q: 0.481327, mean_eps: 0.000000
 2463/5000: episode: 108, duration: 0.491s, episode steps:  35, steps per second:  71, episode reward: 32.548, mean reward:  0.930 [-2.366, 32.209], mean action: 9.486 [1.000, 21.000],  loss: 0.021167, mae: 0.351278, mean_q: 0.478249, mean_eps: 0.000000
 2490/5000: episode: 109, duration: 0.385s, episode steps:  27, steps per second:  70, episode reward: -36.000, mean reward: -1.333 [-32.900,  2.610], mean action: 5.111 [0.000, 16.000],  loss: 0.021936, mae: 0.346414, mean_q: 0.509598, mean_eps: 0.000000
 2513/5000: episode: 110, duration: 0.321s, episode steps:  23, steps per second:  72, episode reward: -38.500, mean reward: -1.674 [-32.420,  2.272], mean action: 4.609 [0.000, 19.000],  loss: 0.017197, mae: 0.320620, mean_q: 0.530450, mean_eps: 0.000000
 2554/5000: episode: 111, duration: 0.564s, episode steps:  41, steps per second:  73, episode reward: 39.000, mean reward:  0.951 [-3.000, 32.910], mean action: 3.780 [1.000, 13.000],  loss: 0.022154, mae: 0.350619, mean_q: 0.493033, mean_eps: 0.000000
 2584/5000: episode: 112, duration: 0.416s, episode steps:  30, steps per second:  72, episode reward: -35.300, mean reward: -1.177 [-32.075,  2.901], mean action: 7.600 [0.000, 20.000],  loss: 0.019480, mae: 0.341900, mean_q: 0.477468, mean_eps: 0.000000
 2610/5000: episode: 113, duration: 0.374s, episode steps:  26, steps per second:  70, episode reward: 34.684, mean reward:  1.334 [-3.000, 32.270], mean action: 4.000 [0.000, 14.000],  loss: 0.024366, mae: 0.358498, mean_q: 0.502317, mean_eps: 0.000000
 2639/5000: episode: 114, duration: 0.402s, episode steps:  29, steps per second:  72, episode reward: 32.484, mean reward:  1.120 [-3.000, 33.000], mean action: 2.931 [0.000, 9.000],  loss: 0.023369, mae: 0.354058, mean_q: 0.531577, mean_eps: 0.000000
 2657/5000: episode: 115, duration: 0.266s, episode steps:  18, steps per second:  68, episode reward: 38.484, mean reward:  2.138 [-2.254, 32.394], mean action: 3.056 [0.000, 9.000],  loss: 0.019278, mae: 0.344478, mean_q: 0.496967, mean_eps: 0.000000
 2682/5000: episode: 116, duration: 0.354s, episode steps:  25, steps per second:  71, episode reward: 38.715, mean reward:  1.549 [-2.340, 32.440], mean action: 4.280 [0.000, 19.000],  loss: 0.021756, mae: 0.351142, mean_q: 0.475516, mean_eps: 0.000000
 2711/5000: episode: 117, duration: 0.403s, episode steps:  29, steps per second:  72, episode reward: -38.030, mean reward: -1.311 [-31.733,  2.280], mean action: 6.862 [0.000, 19.000],  loss: 0.019542, mae: 0.337373, mean_q: 0.538063, mean_eps: 0.000000
 2738/5000: episode: 118, duration: 0.379s, episode steps:  27, steps per second:  71, episode reward: 39.000, mean reward:  1.444 [-2.305, 32.480], mean action: 3.926 [0.000, 19.000],  loss: 0.019754, mae: 0.336879, mean_q: 0.580001, mean_eps: 0.000000
 2770/5000: episode: 119, duration: 0.448s, episode steps:  32, steps per second:  71, episode reward: -32.170, mean reward: -1.005 [-32.210,  2.654], mean action: 7.188 [0.000, 21.000],  loss: 0.021996, mae: 0.342648, mean_q: 0.524423, mean_eps: 0.000000
 2789/5000: episode: 120, duration: 0.267s, episode steps:  19, steps per second:  71, episode reward: -41.130, mean reward: -2.165 [-32.920,  2.240], mean action: 7.000 [0.000, 19.000],  loss: 0.024481, mae: 0.352474, mean_q: 0.518488, mean_eps: 0.000000
 2827/5000: episode: 121, duration: 0.520s, episode steps:  38, steps per second:  73, episode reward: 32.837, mean reward:  0.864 [-2.419, 32.240], mean action: 4.132 [0.000, 19.000],  loss: 0.021281, mae: 0.343436, mean_q: 0.567630, mean_eps: 0.000000
 2847/5000: episode: 122, duration: 0.289s, episode steps:  20, steps per second:  69, episode reward: 38.024, mean reward:  1.901 [-2.802, 32.520], mean action: 5.850 [1.000, 15.000],  loss: 0.020787, mae: 0.344249, mean_q: 0.535548, mean_eps: 0.000000
 2863/5000: episode: 123, duration: 0.236s, episode steps:  16, steps per second:  68, episode reward: 41.511, mean reward:  2.594 [-2.181, 32.220], mean action: 3.062 [0.000, 14.000],  loss: 0.022657, mae: 0.342327, mean_q: 0.476822, mean_eps: 0.000000
 2882/5000: episode: 124, duration: 0.265s, episode steps:  19, steps per second:  72, episode reward: 33.000, mean reward:  1.737 [-3.000, 32.340], mean action: 4.000 [0.000, 12.000],  loss: 0.023730, mae: 0.347620, mean_q: 0.532654, mean_eps: 0.000000
 2901/5000: episode: 125, duration: 0.273s, episode steps:  19, steps per second:  70, episode reward: 38.806, mean reward:  2.042 [-2.184, 32.330], mean action: 3.789 [1.000, 12.000],  loss: 0.019901, mae: 0.325896, mean_q: 0.532167, mean_eps: 0.000000
 2933/5000: episode: 126, duration: 0.449s, episode steps:  32, steps per second:  71, episode reward: -35.350, mean reward: -1.105 [-32.147,  2.330], mean action: 5.625 [0.000, 21.000],  loss: 0.025216, mae: 0.356482, mean_q: 0.548606, mean_eps: 0.000000
 2948/5000: episode: 127, duration: 0.226s, episode steps:  15, steps per second:  66, episode reward: 43.537, mean reward:  2.902 [-2.095, 32.904], mean action: 1.733 [0.000, 12.000],  loss: 0.022230, mae: 0.329630, mean_q: 0.526035, mean_eps: 0.000000
 2964/5000: episode: 128, duration: 0.234s, episode steps:  16, steps per second:  68, episode reward: 38.803, mean reward:  2.425 [-2.505, 32.901], mean action: 2.375 [0.000, 16.000],  loss: 0.022895, mae: 0.336534, mean_q: 0.555149, mean_eps: 0.000000
 2993/5000: episode: 129, duration: 0.613s, episode steps:  29, steps per second:  47, episode reward: -35.950, mean reward: -1.240 [-32.121,  2.530], mean action: 9.414 [0.000, 21.000],  loss: 0.023575, mae: 0.349516, mean_q: 0.504434, mean_eps: 0.000000
 3012/5000: episode: 130, duration: 0.276s, episode steps:  19, steps per second:  69, episode reward: 41.012, mean reward:  2.159 [-2.354, 32.100], mean action: 2.158 [0.000, 16.000],  loss: 0.020633, mae: 0.335992, mean_q: 0.496660, mean_eps: 0.000000
 3031/5000: episode: 131, duration: 0.277s, episode steps:  19, steps per second:  69, episode reward: 35.151, mean reward:  1.850 [-3.000, 31.743], mean action: 2.579 [0.000, 9.000],  loss: 0.023780, mae: 0.347421, mean_q: 0.509722, mean_eps: 0.000000
 3054/5000: episode: 132, duration: 0.339s, episode steps:  23, steps per second:  68, episode reward: 38.803, mean reward:  1.687 [-2.475, 32.901], mean action: 2.261 [0.000, 9.000],  loss: 0.018843, mae: 0.326589, mean_q: 0.527511, mean_eps: 0.000000
 3072/5000: episode: 133, duration: 0.265s, episode steps:  18, steps per second:  68, episode reward: 35.776, mean reward:  1.988 [-2.630, 32.320], mean action: 6.611 [0.000, 16.000],  loss: 0.023113, mae: 0.347594, mean_q: 0.524949, mean_eps: 0.000000
 3091/5000: episode: 134, duration: 0.269s, episode steps:  19, steps per second:  71, episode reward: -38.760, mean reward: -2.040 [-33.000,  3.000], mean action: 7.632 [0.000, 20.000],  loss: 0.023508, mae: 0.354788, mean_q: 0.497463, mean_eps: 0.000000
 3111/5000: episode: 135, duration: 0.298s, episode steps:  20, steps per second:  67, episode reward: 38.445, mean reward:  1.922 [-2.678, 32.355], mean action: 5.100 [0.000, 19.000],  loss: 0.019307, mae: 0.335818, mean_q: 0.557955, mean_eps: 0.000000
 3136/5000: episode: 136, duration: 0.360s, episode steps:  25, steps per second:  69, episode reward: 41.261, mean reward:  1.650 [-3.000, 31.853], mean action: 3.080 [0.000, 20.000],  loss: 0.021280, mae: 0.336374, mean_q: 0.552880, mean_eps: 0.000000
 3172/5000: episode: 137, duration: 0.492s, episode steps:  36, steps per second:  73, episode reward: 40.814, mean reward:  1.134 [-2.309, 32.579], mean action: 2.889 [0.000, 19.000],  loss: 0.023368, mae: 0.351832, mean_q: 0.607397, mean_eps: 0.000000
 3192/5000: episode: 138, duration: 0.292s, episode steps:  20, steps per second:  69, episode reward: 35.250, mean reward:  1.762 [-2.576, 32.657], mean action: 7.700 [0.000, 19.000],  loss: 0.023210, mae: 0.359644, mean_q: 0.610596, mean_eps: 0.000000
 3210/5000: episode: 139, duration: 0.266s, episode steps:  18, steps per second:  68, episode reward: 39.000, mean reward:  2.167 [-2.651, 32.100], mean action: 3.944 [0.000, 19.000],  loss: 0.023052, mae: 0.353207, mean_q: 0.575350, mean_eps: 0.000000
 3236/5000: episode: 140, duration: 0.372s, episode steps:  26, steps per second:  70, episode reward: 35.194, mean reward:  1.354 [-2.990, 32.194], mean action: 4.923 [0.000, 16.000],  loss: 0.020986, mae: 0.347086, mean_q: 0.563971, mean_eps: 0.000000
 3251/5000: episode: 141, duration: 0.225s, episode steps:  15, steps per second:  67, episode reward: 41.657, mean reward:  2.777 [-2.498, 32.190], mean action: 4.867 [1.000, 20.000],  loss: 0.025481, mae: 0.370328, mean_q: 0.588440, mean_eps: 0.000000
 3270/5000: episode: 142, duration: 0.285s, episode steps:  19, steps per second:  67, episode reward: 38.800, mean reward:  2.042 [-2.515, 33.000], mean action: 3.211 [0.000, 15.000],  loss: 0.020690, mae: 0.347752, mean_q: 0.537078, mean_eps: 0.000000
 3299/5000: episode: 143, duration: 0.410s, episode steps:  29, steps per second:  71, episode reward: 44.357, mean reward:  1.530 [-2.201, 31.692], mean action: 1.586 [0.000, 11.000],  loss: 0.022953, mae: 0.347825, mean_q: 0.547838, mean_eps: 0.000000
 3316/5000: episode: 144, duration: 0.248s, episode steps:  17, steps per second:  69, episode reward: 41.822, mean reward:  2.460 [-2.233, 33.000], mean action: 4.118 [0.000, 14.000],  loss: 0.021213, mae: 0.340513, mean_q: 0.520767, mean_eps: 0.000000
 3338/5000: episode: 145, duration: 0.324s, episode steps:  22, steps per second:  68, episode reward: 41.117, mean reward:  1.869 [-2.257, 32.866], mean action: 5.636 [0.000, 20.000],  loss: 0.023744, mae: 0.351371, mean_q: 0.521062, mean_eps: 0.000000
 3359/5000: episode: 146, duration: 0.300s, episode steps:  21, steps per second:  70, episode reward: -35.120, mean reward: -1.672 [-32.023,  2.610], mean action: 6.000 [0.000, 15.000],  loss: 0.020149, mae: 0.340509, mean_q: 0.487068, mean_eps: 0.000000
 3374/5000: episode: 147, duration: 0.225s, episode steps:  15, steps per second:  67, episode reward: 44.245, mean reward:  2.950 [-2.063, 32.460], mean action: 1.933 [0.000, 12.000],  loss: 0.023420, mae: 0.354171, mean_q: 0.467554, mean_eps: 0.000000
 3397/5000: episode: 148, duration: 0.330s, episode steps:  23, steps per second:  70, episode reward: 38.710, mean reward:  1.683 [-2.380, 32.020], mean action: 2.652 [0.000, 12.000],  loss: 0.019624, mae: 0.339686, mean_q: 0.484772, mean_eps: 0.000000
 3432/5000: episode: 149, duration: 0.476s, episode steps:  35, steps per second:  73, episode reward: -41.200, mean reward: -1.177 [-31.787,  2.430], mean action: 4.114 [0.000, 16.000],  loss: 0.017229, mae: 0.320143, mean_q: 0.443164, mean_eps: 0.000000
 3466/5000: episode: 150, duration: 0.471s, episode steps:  34, steps per second:  72, episode reward: 38.760, mean reward:  1.140 [-2.619, 32.952], mean action: 2.618 [0.000, 14.000],  loss: 0.019526, mae: 0.331958, mean_q: 0.477201, mean_eps: 0.000000
 3485/5000: episode: 151, duration: 0.279s, episode steps:  19, steps per second:  68, episode reward: 42.000, mean reward:  2.211 [-2.162, 32.910], mean action: 2.895 [0.000, 15.000],  loss: 0.018356, mae: 0.320227, mean_q: 0.509319, mean_eps: 0.000000
 3506/5000: episode: 152, duration: 0.305s, episode steps:  21, steps per second:  69, episode reward: 42.000, mean reward:  2.000 [-3.000, 32.210], mean action: 3.143 [0.000, 20.000],  loss: 0.017241, mae: 0.322324, mean_q: 0.543496, mean_eps: 0.000000
 3527/5000: episode: 153, duration: 0.304s, episode steps:  21, steps per second:  69, episode reward: 35.300, mean reward:  1.681 [-2.514, 32.050], mean action: 3.429 [0.000, 19.000],  loss: 0.020979, mae: 0.346694, mean_q: 0.539550, mean_eps: 0.000000
 3546/5000: episode: 154, duration: 0.306s, episode steps:  19, steps per second:  62, episode reward: 34.747, mean reward:  1.829 [-2.770, 32.638], mean action: 4.842 [0.000, 15.000],  loss: 0.022130, mae: 0.357775, mean_q: 0.503644, mean_eps: 0.000000
 3566/5000: episode: 155, duration: 0.291s, episode steps:  20, steps per second:  69, episode reward: -35.420, mean reward: -1.771 [-33.000,  2.773], mean action: 5.350 [0.000, 15.000],  loss: 0.018559, mae: 0.341104, mean_q: 0.488127, mean_eps: 0.000000
 3583/5000: episode: 156, duration: 0.246s, episode steps:  17, steps per second:  69, episode reward: 35.562, mean reward:  2.092 [-3.000, 32.130], mean action: 4.588 [0.000, 19.000],  loss: 0.019400, mae: 0.338014, mean_q: 0.559067, mean_eps: 0.000000
 3602/5000: episode: 157, duration: 0.277s, episode steps:  19, steps per second:  68, episode reward: 36.000, mean reward:  1.895 [-2.604, 32.320], mean action: 4.316 [0.000, 19.000],  loss: 0.020885, mae: 0.348720, mean_q: 0.516469, mean_eps: 0.000000
 3628/5000: episode: 158, duration: 0.373s, episode steps:  26, steps per second:  70, episode reward: 33.000, mean reward:  1.269 [-2.228, 30.483], mean action: 4.500 [0.000, 19.000],  loss: 0.024092, mae: 0.362097, mean_q: 0.490385, mean_eps: 0.000000
 3647/5000: episode: 159, duration: 0.271s, episode steps:  19, steps per second:  70, episode reward: 35.335, mean reward:  1.860 [-3.000, 33.000], mean action: 5.368 [0.000, 19.000],  loss: 0.019603, mae: 0.337586, mean_q: 0.544942, mean_eps: 0.000000
 3669/5000: episode: 160, duration: 0.317s, episode steps:  22, steps per second:  69, episode reward: -35.600, mean reward: -1.618 [-31.707,  3.000], mean action: 8.045 [0.000, 19.000],  loss: 0.021621, mae: 0.346810, mean_q: 0.550281, mean_eps: 0.000000
 3694/5000: episode: 161, duration: 0.371s, episode steps:  25, steps per second:  67, episode reward: 36.000, mean reward:  1.440 [-2.361, 32.300], mean action: 5.440 [0.000, 15.000],  loss: 0.024918, mae: 0.364532, mean_q: 0.555570, mean_eps: 0.000000
 3725/5000: episode: 162, duration: 0.432s, episode steps:  31, steps per second:  72, episode reward: 37.880, mean reward:  1.222 [-2.167, 32.163], mean action: 3.226 [0.000, 16.000],  loss: 0.017846, mae: 0.333311, mean_q: 0.578732, mean_eps: 0.000000
 3750/5000: episode: 163, duration: 0.359s, episode steps:  25, steps per second:  70, episode reward: -35.120, mean reward: -1.405 [-31.604,  2.720], mean action: 5.000 [0.000, 14.000],  loss: 0.021751, mae: 0.347569, mean_q: 0.588399, mean_eps: 0.000000
 3788/5000: episode: 164, duration: 0.516s, episode steps:  38, steps per second:  74, episode reward: 44.761, mean reward:  1.178 [-3.000, 32.500], mean action: 1.368 [1.000, 11.000],  loss: 0.018914, mae: 0.350565, mean_q: 0.506056, mean_eps: 0.000000
 3807/5000: episode: 165, duration: 0.273s, episode steps:  19, steps per second:  70, episode reward: 35.380, mean reward:  1.862 [-3.000, 32.400], mean action: 6.474 [0.000, 19.000],  loss: 0.019880, mae: 0.360448, mean_q: 0.445240, mean_eps: 0.000000
 3826/5000: episode: 166, duration: 0.278s, episode steps:  19, steps per second:  68, episode reward: 38.834, mean reward:  2.044 [-2.648, 32.834], mean action: 5.158 [0.000, 19.000],  loss: 0.024958, mae: 0.368863, mean_q: 0.495153, mean_eps: 0.000000
 3844/5000: episode: 167, duration: 0.272s, episode steps:  18, steps per second:  66, episode reward: 35.000, mean reward:  1.944 [-3.000, 32.415], mean action: 7.222 [0.000, 19.000],  loss: 0.016887, mae: 0.327067, mean_q: 0.513358, mean_eps: 0.000000
 3868/5000: episode: 168, duration: 0.342s, episode steps:  24, steps per second:  70, episode reward: 33.000, mean reward:  1.375 [-2.426, 33.000], mean action: 7.417 [0.000, 19.000],  loss: 0.018159, mae: 0.324606, mean_q: 0.495331, mean_eps: 0.000000
 3892/5000: episode: 169, duration: 0.342s, episode steps:  24, steps per second:  70, episode reward: -35.740, mean reward: -1.489 [-31.905,  2.220], mean action: 7.292 [0.000, 19.000],  loss: 0.020043, mae: 0.329596, mean_q: 0.493023, mean_eps: 0.000000
 3910/5000: episode: 170, duration: 0.264s, episode steps:  18, steps per second:  68, episode reward: 43.505, mean reward:  2.417 [-2.431, 32.012], mean action: 4.056 [0.000, 13.000],  loss: 0.021369, mae: 0.329969, mean_q: 0.511252, mean_eps: 0.000000
 3932/5000: episode: 171, duration: 0.331s, episode steps:  22, steps per second:  66, episode reward: 39.000, mean reward:  1.773 [-2.562, 32.070], mean action: 2.909 [0.000, 16.000],  loss: 0.019391, mae: 0.326425, mean_q: 0.545395, mean_eps: 0.000000
 3950/5000: episode: 172, duration: 0.257s, episode steps:  18, steps per second:  70, episode reward: -38.900, mean reward: -2.161 [-32.075,  2.175], mean action: 6.944 [0.000, 16.000],  loss: 0.021542, mae: 0.338343, mean_q: 0.496659, mean_eps: 0.000000
 3968/5000: episode: 173, duration: 0.258s, episode steps:  18, steps per second:  70, episode reward: 35.573, mean reward:  1.976 [-3.000, 32.081], mean action: 5.389 [0.000, 19.000],  loss: 0.016643, mae: 0.324657, mean_q: 0.514165, mean_eps: 0.000000
 3985/5000: episode: 174, duration: 0.248s, episode steps:  17, steps per second:  69, episode reward: 35.393, mean reward:  2.082 [-3.000, 32.309], mean action: 6.000 [0.000, 19.000],  loss: 0.022987, mae: 0.349771, mean_q: 0.524828, mean_eps: 0.000000
 4008/5000: episode: 175, duration: 0.333s, episode steps:  23, steps per second:  69, episode reward: -35.400, mean reward: -1.539 [-31.761,  2.390], mean action: 7.522 [0.000, 19.000],  loss: 0.020643, mae: 0.344100, mean_q: 0.562664, mean_eps: 0.000000
 4036/5000: episode: 176, duration: 0.395s, episode steps:  28, steps per second:  71, episode reward: -35.380, mean reward: -1.264 [-31.844,  3.000], mean action: 4.286 [0.000, 19.000],  loss: 0.021687, mae: 0.344418, mean_q: 0.612968, mean_eps: 0.000000
 4062/5000: episode: 177, duration: 0.377s, episode steps:  26, steps per second:  69, episode reward: 35.400, mean reward:  1.362 [-2.903, 32.080], mean action: 5.462 [0.000, 20.000],  loss: 0.019211, mae: 0.338387, mean_q: 0.551887, mean_eps: 0.000000
 4080/5000: episode: 178, duration: 0.272s, episode steps:  18, steps per second:  66, episode reward: 35.316, mean reward:  1.962 [-3.000, 32.178], mean action: 6.167 [0.000, 19.000],  loss: 0.019893, mae: 0.332392, mean_q: 0.529579, mean_eps: 0.000000
 4107/5000: episode: 179, duration: 0.391s, episode steps:  27, steps per second:  69, episode reward: 38.145, mean reward:  1.413 [-2.465, 31.959], mean action: 3.667 [0.000, 19.000],  loss: 0.022288, mae: 0.353673, mean_q: 0.581753, mean_eps: 0.000000
 4129/5000: episode: 180, duration: 0.407s, episode steps:  22, steps per second:  54, episode reward: -38.810, mean reward: -1.764 [-32.186,  2.220], mean action: 6.500 [0.000, 20.000],  loss: 0.017740, mae: 0.326314, mean_q: 0.567241, mean_eps: 0.000000
 4148/5000: episode: 181, duration: 0.286s, episode steps:  19, steps per second:  66, episode reward: 39.000, mean reward:  2.053 [-2.614, 32.430], mean action: 5.000 [0.000, 19.000],  loss: 0.020738, mae: 0.341760, mean_q: 0.553514, mean_eps: 0.000000
 4171/5000: episode: 182, duration: 0.325s, episode steps:  23, steps per second:  71, episode reward: -35.390, mean reward: -1.539 [-32.297,  3.492], mean action: 6.435 [0.000, 19.000],  loss: 0.023259, mae: 0.357832, mean_q: 0.577152, mean_eps: 0.000000
 4195/5000: episode: 183, duration: 0.392s, episode steps:  24, steps per second:  61, episode reward: -41.120, mean reward: -1.713 [-32.393,  3.000], mean action: 8.208 [0.000, 19.000],  loss: 0.017313, mae: 0.326124, mean_q: 0.530249, mean_eps: 0.000000
 4211/5000: episode: 184, duration: 0.270s, episode steps:  16, steps per second:  59, episode reward: 40.741, mean reward:  2.546 [-2.903, 31.327], mean action: 6.125 [0.000, 14.000],  loss: 0.024613, mae: 0.353552, mean_q: 0.604157, mean_eps: 0.000000
 4237/5000: episode: 185, duration: 0.373s, episode steps:  26, steps per second:  70, episode reward: -32.700, mean reward: -1.258 [-31.939,  3.000], mean action: 5.231 [0.000, 16.000],  loss: 0.019028, mae: 0.325527, mean_q: 0.536457, mean_eps: 0.000000
 4265/5000: episode: 186, duration: 0.401s, episode steps:  28, steps per second:  70, episode reward: 32.707, mean reward:  1.168 [-2.597, 31.891], mean action: 6.036 [0.000, 16.000],  loss: 0.019237, mae: 0.333103, mean_q: 0.548128, mean_eps: 0.000000
 4298/5000: episode: 187, duration: 0.616s, episode steps:  33, steps per second:  54, episode reward: 32.686, mean reward:  0.990 [-3.000, 31.976], mean action: 7.758 [0.000, 21.000],  loss: 0.016923, mae: 0.328865, mean_q: 0.487006, mean_eps: 0.000000
 4324/5000: episode: 188, duration: 0.379s, episode steps:  26, steps per second:  69, episode reward: 36.000, mean reward:  1.385 [-3.000, 32.120], mean action: 3.654 [0.000, 19.000],  loss: 0.021963, mae: 0.343514, mean_q: 0.512719, mean_eps: 0.000000
 4358/5000: episode: 189, duration: 0.540s, episode steps:  34, steps per second:  63, episode reward: 33.000, mean reward:  0.971 [-2.441, 33.000], mean action: 3.500 [0.000, 18.000],  loss: 0.018781, mae: 0.322768, mean_q: 0.476666, mean_eps: 0.000000
 4386/5000: episode: 190, duration: 0.417s, episode steps:  28, steps per second:  67, episode reward: -35.990, mean reward: -1.285 [-32.077,  2.220], mean action: 6.429 [0.000, 15.000],  loss: 0.020197, mae: 0.326204, mean_q: 0.563150, mean_eps: 0.000000
 4405/5000: episode: 191, duration: 0.278s, episode steps:  19, steps per second:  68, episode reward: 35.217, mean reward:  1.854 [-3.000, 32.604], mean action: 4.211 [0.000, 12.000],  loss: 0.022529, mae: 0.340189, mean_q: 0.488162, mean_eps: 0.000000
 4425/5000: episode: 192, duration: 0.291s, episode steps:  20, steps per second:  69, episode reward: 38.900, mean reward:  1.945 [-2.564, 32.900], mean action: 5.200 [0.000, 14.000],  loss: 0.021244, mae: 0.325198, mean_q: 0.502561, mean_eps: 0.000000
 4444/5000: episode: 193, duration: 0.279s, episode steps:  19, steps per second:  68, episode reward: 39.000, mean reward:  2.053 [-2.123, 33.000], mean action: 2.053 [0.000, 11.000],  loss: 0.020005, mae: 0.326230, mean_q: 0.523385, mean_eps: 0.000000
 4469/5000: episode: 194, duration: 0.375s, episode steps:  25, steps per second:  67, episode reward: 32.747, mean reward:  1.310 [-2.746, 32.060], mean action: 4.760 [0.000, 16.000],  loss: 0.020358, mae: 0.335969, mean_q: 0.549276, mean_eps: 0.000000
 4495/5000: episode: 195, duration: 0.378s, episode steps:  26, steps per second:  69, episode reward: 32.625, mean reward:  1.255 [-2.945, 31.955], mean action: 5.538 [0.000, 16.000],  loss: 0.020315, mae: 0.330980, mean_q: 0.532015, mean_eps: 0.000000
 4514/5000: episode: 196, duration: 0.275s, episode steps:  19, steps per second:  69, episode reward: 38.549, mean reward:  2.029 [-2.532, 32.619], mean action: 5.947 [1.000, 16.000],  loss: 0.016629, mae: 0.311630, mean_q: 0.577895, mean_eps: 0.000000
 4536/5000: episode: 197, duration: 0.322s, episode steps:  22, steps per second:  68, episode reward: 38.753, mean reward:  1.762 [-3.000, 32.500], mean action: 4.909 [0.000, 16.000],  loss: 0.021779, mae: 0.336262, mean_q: 0.501307, mean_eps: 0.000000
 4570/5000: episode: 198, duration: 0.484s, episode steps:  34, steps per second:  70, episode reward: -32.320, mean reward: -0.951 [-31.879,  2.262], mean action: 8.147 [0.000, 21.000],  loss: 0.027027, mae: 0.360606, mean_q: 0.602919, mean_eps: 0.000000
 4596/5000: episode: 199, duration: 0.386s, episode steps:  26, steps per second:  67, episode reward: 40.615, mean reward:  1.562 [-2.357, 32.184], mean action: 4.731 [0.000, 14.000],  loss: 0.020440, mae: 0.333857, mean_q: 0.568303, mean_eps: 0.000000
 4617/5000: episode: 200, duration: 0.305s, episode steps:  21, steps per second:  69, episode reward: 38.374, mean reward:  1.827 [-2.817, 32.592], mean action: 2.333 [0.000, 11.000],  loss: 0.021167, mae: 0.338278, mean_q: 0.597785, mean_eps: 0.000000
 4639/5000: episode: 201, duration: 0.330s, episode steps:  22, steps per second:  67, episode reward: -32.230, mean reward: -1.465 [-32.013,  2.920], mean action: 3.636 [0.000, 12.000],  loss: 0.017878, mae: 0.323643, mean_q: 0.587487, mean_eps: 0.000000
 4659/5000: episode: 202, duration: 0.285s, episode steps:  20, steps per second:  70, episode reward: 35.299, mean reward:  1.765 [-3.000, 32.140], mean action: 3.650 [0.000, 12.000],  loss: 0.021658, mae: 0.347372, mean_q: 0.571708, mean_eps: 0.000000
 4691/5000: episode: 203, duration: 0.449s, episode steps:  32, steps per second:  71, episode reward: 38.785, mean reward:  1.212 [-2.453, 32.015], mean action: 2.844 [0.000, 12.000],  loss: 0.021474, mae: 0.347109, mean_q: 0.551306, mean_eps: 0.000000
 4716/5000: episode: 204, duration: 0.361s, episode steps:  25, steps per second:  69, episode reward: 35.067, mean reward:  1.403 [-3.000, 31.681], mean action: 6.920 [1.000, 14.000],  loss: 0.024043, mae: 0.360736, mean_q: 0.543313, mean_eps: 0.000000
 4737/5000: episode: 205, duration: 0.357s, episode steps:  21, steps per second:  59, episode reward: 35.729, mean reward:  1.701 [-3.000, 32.449], mean action: 4.476 [0.000, 19.000],  loss: 0.019346, mae: 0.330121, mean_q: 0.552681, mean_eps: 0.000000
 4758/5000: episode: 206, duration: 0.475s, episode steps:  21, steps per second:  44, episode reward: 35.903, mean reward:  1.710 [-2.291, 32.203], mean action: 3.714 [0.000, 19.000],  loss: 0.022933, mae: 0.349638, mean_q: 0.534593, mean_eps: 0.000000
 4782/5000: episode: 207, duration: 0.355s, episode steps:  24, steps per second:  68, episode reward: 35.519, mean reward:  1.480 [-2.228, 32.040], mean action: 5.042 [0.000, 19.000],  loss: 0.019025, mae: 0.337758, mean_q: 0.544638, mean_eps: 0.000000
 4804/5000: episode: 208, duration: 0.318s, episode steps:  22, steps per second:  69, episode reward: 40.769, mean reward:  1.853 [-2.511, 32.450], mean action: 5.818 [0.000, 19.000],  loss: 0.022169, mae: 0.344941, mean_q: 0.590512, mean_eps: 0.000000
 4813/5000: episode: 209, duration: 0.143s, episode steps:   9, steps per second:  63, episode reward: 48.000, mean reward:  5.333 [ 0.580, 32.230], mean action: 1.333 [0.000, 3.000],  loss: 0.019452, mae: 0.348869, mean_q: 0.610112, mean_eps: 0.000000
 4829/5000: episode: 210, duration: 0.240s, episode steps:  16, steps per second:  67, episode reward: 41.629, mean reward:  2.602 [-2.570, 31.979], mean action: 2.438 [0.000, 19.000],  loss: 0.017743, mae: 0.334650, mean_q: 0.551991, mean_eps: 0.000000
 4853/5000: episode: 211, duration: 0.360s, episode steps:  24, steps per second:  67, episode reward: 39.000, mean reward:  1.625 [-2.593, 32.360], mean action: 4.333 [0.000, 16.000],  loss: 0.019760, mae: 0.346072, mean_q: 0.532710, mean_eps: 0.000000
 4873/5000: episode: 212, duration: 0.290s, episode steps:  20, steps per second:  69, episode reward: 35.135, mean reward:  1.757 [-2.722, 32.100], mean action: 5.300 [0.000, 16.000],  loss: 0.021842, mae: 0.347988, mean_q: 0.500712, mean_eps: 0.000000
 4902/5000: episode: 213, duration: 0.419s, episode steps:  29, steps per second:  69, episode reward: -32.480, mean reward: -1.120 [-31.851,  2.480], mean action: 7.172 [0.000, 16.000],  loss: 0.018435, mae: 0.336865, mean_q: 0.501222, mean_eps: 0.000000
 4928/5000: episode: 214, duration: 0.373s, episode steps:  26, steps per second:  70, episode reward: 38.770, mean reward:  1.491 [-2.286, 32.310], mean action: 2.962 [0.000, 15.000],  loss: 0.023271, mae: 0.363387, mean_q: 0.511516, mean_eps: 0.000000
 4963/5000: episode: 215, duration: 0.488s, episode steps:  35, steps per second:  72, episode reward: 41.544, mean reward:  1.187 [-2.258, 30.368], mean action: 1.829 [0.000, 8.000],  loss: 0.018782, mae: 0.338168, mean_q: 0.509549, mean_eps: 0.000000
 4984/5000: episode: 216, duration: 0.308s, episode steps:  21, steps per second:  68, episode reward: 35.084, mean reward:  1.671 [-3.000, 32.035], mean action: 4.190 [1.000, 12.000],  loss: 0.019953, mae: 0.334925, mean_q: 0.523900, mean_eps: 0.000000
done, took 65.782 seconds
DQN Evaluation: 9899 victories out of 11576 episodes
Training for 5000 steps ...
   27/5000: episode: 1, duration: 0.213s, episode steps:  27, steps per second: 127, episode reward: 41.074, mean reward:  1.521 [-2.080, 32.130], mean action: 3.111 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   54/5000: episode: 2, duration: 0.184s, episode steps:  27, steps per second: 147, episode reward: -35.560, mean reward: -1.317 [-32.234,  2.393], mean action: 8.481 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   85/5000: episode: 3, duration: 0.237s, episode steps:  31, steps per second: 131, episode reward: 38.022, mean reward:  1.227 [-3.000, 32.290], mean action: 5.968 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  110/5000: episode: 4, duration: 0.176s, episode steps:  25, steps per second: 142, episode reward: 39.000, mean reward:  1.560 [-2.227, 32.150], mean action: 3.200 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  144/5000: episode: 5, duration: 0.225s, episode steps:  34, steps per second: 151, episode reward: 44.004, mean reward:  1.294 [-2.218, 32.039], mean action: 1.118 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  166/5000: episode: 6, duration: 0.157s, episode steps:  22, steps per second: 140, episode reward: 41.364, mean reward:  1.880 [-2.541, 32.180], mean action: 2.500 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  190/5000: episode: 7, duration: 0.155s, episode steps:  24, steps per second: 154, episode reward: 44.002, mean reward:  1.833 [-2.120, 32.129], mean action: 2.833 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  209/5000: episode: 8, duration: 0.133s, episode steps:  19, steps per second: 143, episode reward: 41.407, mean reward:  2.179 [-2.718, 32.070], mean action: 1.263 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  224/5000: episode: 9, duration: 0.117s, episode steps:  15, steps per second: 128, episode reward: 45.000, mean reward:  3.000 [-2.185, 32.590], mean action: 1.467 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  243/5000: episode: 10, duration: 0.142s, episode steps:  19, steps per second: 134, episode reward: 38.900, mean reward:  2.047 [-3.000, 31.950], mean action: 3.263 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  274/5000: episode: 11, duration: 0.201s, episode steps:  31, steps per second: 155, episode reward: 43.990, mean reward:  1.419 [-2.100, 32.060], mean action: 4.935 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  293/5000: episode: 12, duration: 0.134s, episode steps:  19, steps per second: 142, episode reward: 38.681, mean reward:  2.036 [-3.000, 32.019], mean action: 3.263 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  326/5000: episode: 13, duration: 0.209s, episode steps:  33, steps per second: 158, episode reward: 35.108, mean reward:  1.064 [-2.938, 32.477], mean action: 4.818 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  348/5000: episode: 14, duration: 0.147s, episode steps:  22, steps per second: 150, episode reward: 42.000, mean reward:  1.909 [-2.482, 32.020], mean action: 2.500 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  378/5000: episode: 15, duration: 0.192s, episode steps:  30, steps per second: 156, episode reward: 36.000, mean reward:  1.200 [-2.588, 32.300], mean action: 4.367 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  447/5000: episode: 16, duration: 0.417s, episode steps:  69, steps per second: 165, episode reward: 38.715, mean reward:  0.561 [-2.537, 32.190], mean action: 1.797 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  460/5000: episode: 17, duration: 0.103s, episode steps:  13, steps per second: 126, episode reward: 45.000, mean reward:  3.462 [-2.048, 32.170], mean action: 1.462 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  502/5000: episode: 18, duration: 0.263s, episode steps:  42, steps per second: 160, episode reward: 41.480, mean reward:  0.988 [-3.000, 32.410], mean action: 3.381 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  538/5000: episode: 19, duration: 0.226s, episode steps:  36, steps per second: 159, episode reward: 35.746, mean reward:  0.993 [-2.998, 32.220], mean action: 3.583 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  569/5000: episode: 20, duration: 0.202s, episode steps:  31, steps per second: 153, episode reward: 41.070, mean reward:  1.325 [-2.318, 31.809], mean action: 1.355 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  591/5000: episode: 21, duration: 0.156s, episode steps:  22, steps per second: 141, episode reward: 41.368, mean reward:  1.880 [-2.431, 32.273], mean action: 1.591 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  626/5000: episode: 22, duration: 0.229s, episode steps:  35, steps per second: 153, episode reward: 37.107, mean reward:  1.060 [-3.000, 31.889], mean action: 5.571 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  653/5000: episode: 23, duration: 0.182s, episode steps:  27, steps per second: 148, episode reward: 44.044, mean reward:  1.631 [-2.125, 32.180], mean action: 3.111 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  668/5000: episode: 24, duration: 0.109s, episode steps:  15, steps per second: 137, episode reward: 44.450, mean reward:  2.963 [-2.369, 32.171], mean action: 2.133 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  700/5000: episode: 25, duration: 0.209s, episode steps:  32, steps per second: 153, episode reward: 40.779, mean reward:  1.274 [-3.000, 32.200], mean action: 2.875 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  734/5000: episode: 26, duration: 0.219s, episode steps:  34, steps per second: 155, episode reward: 38.399, mean reward:  1.129 [-2.800, 31.822], mean action: 5.471 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  761/5000: episode: 27, duration: 0.185s, episode steps:  27, steps per second: 146, episode reward: 38.880, mean reward:  1.440 [-2.210, 32.240], mean action: 2.815 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  791/5000: episode: 28, duration: 0.203s, episode steps:  30, steps per second: 147, episode reward: 38.358, mean reward:  1.279 [-2.702, 32.017], mean action: 3.267 [1.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  819/5000: episode: 29, duration: 0.188s, episode steps:  28, steps per second: 149, episode reward: 41.596, mean reward:  1.486 [-2.368, 32.070], mean action: 1.179 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  833/5000: episode: 30, duration: 0.113s, episode steps:  14, steps per second: 124, episode reward: 47.273, mean reward:  3.377 [-0.053, 31.847], mean action: 4.643 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  856/5000: episode: 31, duration: 0.165s, episode steps:  23, steps per second: 139, episode reward: 44.741, mean reward:  1.945 [-2.532, 32.210], mean action: 2.783 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  875/5000: episode: 32, duration: 0.135s, episode steps:  19, steps per second: 141, episode reward: 44.309, mean reward:  2.332 [-2.951, 32.150], mean action: 3.000 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  915/5000: episode: 33, duration: 0.247s, episode steps:  40, steps per second: 162, episode reward: 38.663, mean reward:  0.967 [-2.576, 32.110], mean action: 3.800 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  941/5000: episode: 34, duration: 0.168s, episode steps:  26, steps per second: 155, episode reward: 38.625, mean reward:  1.486 [-2.214, 31.795], mean action: 2.269 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  974/5000: episode: 35, duration: 0.213s, episode steps:  33, steps per second: 155, episode reward: 38.318, mean reward:  1.161 [-3.000, 32.370], mean action: 4.697 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1005/5000: episode: 36, duration: 0.272s, episode steps:  31, steps per second: 114, episode reward: 35.540, mean reward:  1.146 [-2.587, 32.031], mean action: 4.161 [0.000, 14.000],  loss: 0.018960, mae: 0.321564, mean_q: 0.496314, mean_eps: 0.000000
 1061/5000: episode: 37, duration: 0.813s, episode steps:  56, steps per second:  69, episode reward: 38.303, mean reward:  0.684 [-2.411, 32.320], mean action: 4.304 [0.000, 19.000],  loss: 0.018952, mae: 0.331100, mean_q: 0.563992, mean_eps: 0.000000
 1086/5000: episode: 38, duration: 0.379s, episode steps:  25, steps per second:  66, episode reward: 41.615, mean reward:  1.665 [-2.819, 32.340], mean action: 3.080 [0.000, 12.000],  loss: 0.021695, mae: 0.345264, mean_q: 0.563718, mean_eps: 0.000000
 1112/5000: episode: 39, duration: 0.381s, episode steps:  26, steps per second:  68, episode reward: 44.366, mean reward:  1.706 [-2.221, 32.160], mean action: 4.423 [0.000, 15.000],  loss: 0.023348, mae: 0.353448, mean_q: 0.573965, mean_eps: 0.000000
 1149/5000: episode: 40, duration: 0.532s, episode steps:  37, steps per second:  69, episode reward: 41.112, mean reward:  1.111 [-2.175, 31.852], mean action: 2.811 [0.000, 12.000],  loss: 0.020752, mae: 0.344320, mean_q: 0.566913, mean_eps: 0.000000
 1173/5000: episode: 41, duration: 0.384s, episode steps:  24, steps per second:  62, episode reward: 41.332, mean reward:  1.722 [-2.414, 32.360], mean action: 4.000 [0.000, 20.000],  loss: 0.024928, mae: 0.362413, mean_q: 0.541193, mean_eps: 0.000000
 1197/5000: episode: 42, duration: 0.345s, episode steps:  24, steps per second:  70, episode reward: 37.948, mean reward:  1.581 [-3.000, 32.683], mean action: 4.708 [0.000, 20.000],  loss: 0.022007, mae: 0.359201, mean_q: 0.614247, mean_eps: 0.000000
 1224/5000: episode: 43, duration: 0.383s, episode steps:  27, steps per second:  70, episode reward: 39.000, mean reward:  1.444 [-2.331, 32.440], mean action: 2.000 [0.000, 12.000],  loss: 0.020231, mae: 0.342935, mean_q: 0.656296, mean_eps: 0.000000
 1251/5000: episode: 44, duration: 0.588s, episode steps:  27, steps per second:  46, episode reward: 47.307, mean reward:  1.752 [-0.248, 32.270], mean action: 3.296 [1.000, 14.000],  loss: 0.019131, mae: 0.337787, mean_q: 0.590484, mean_eps: 0.000000
 1280/5000: episode: 45, duration: 0.415s, episode steps:  29, steps per second:  70, episode reward: 38.303, mean reward:  1.321 [-3.000, 31.727], mean action: 4.034 [0.000, 20.000],  loss: 0.018508, mae: 0.327862, mean_q: 0.595563, mean_eps: 0.000000
 1301/5000: episode: 46, duration: 0.303s, episode steps:  21, steps per second:  69, episode reward: 41.472, mean reward:  1.975 [-2.377, 32.070], mean action: 5.333 [0.000, 20.000],  loss: 0.017254, mae: 0.326042, mean_q: 0.526776, mean_eps: 0.000000
 1345/5000: episode: 47, duration: 0.604s, episode steps:  44, steps per second:  73, episode reward: 41.313, mean reward:  0.939 [-2.187, 32.140], mean action: 2.750 [0.000, 14.000],  loss: 0.022020, mae: 0.347062, mean_q: 0.523415, mean_eps: 0.000000
 1374/5000: episode: 48, duration: 0.417s, episode steps:  29, steps per second:  69, episode reward: 44.374, mean reward:  1.530 [-2.519, 32.080], mean action: 1.276 [0.000, 15.000],  loss: 0.021296, mae: 0.342000, mean_q: 0.536763, mean_eps: 0.000000
 1407/5000: episode: 49, duration: 0.458s, episode steps:  33, steps per second:  72, episode reward: 40.247, mean reward:  1.220 [-3.000, 32.383], mean action: 4.091 [0.000, 20.000],  loss: 0.016405, mae: 0.321931, mean_q: 0.470400, mean_eps: 0.000000
 1443/5000: episode: 50, duration: 0.509s, episode steps:  36, steps per second:  71, episode reward: 41.478, mean reward:  1.152 [-2.450, 32.045], mean action: 4.167 [0.000, 20.000],  loss: 0.021076, mae: 0.348177, mean_q: 0.500657, mean_eps: 0.000000
 1466/5000: episode: 51, duration: 0.332s, episode steps:  23, steps per second:  69, episode reward: 43.563, mean reward:  1.894 [-3.000, 32.150], mean action: 5.174 [0.000, 16.000],  loss: 0.020546, mae: 0.335533, mean_q: 0.509434, mean_eps: 0.000000
 1487/5000: episode: 52, duration: 0.314s, episode steps:  21, steps per second:  67, episode reward: 41.617, mean reward:  1.982 [-3.000, 32.250], mean action: 2.810 [0.000, 20.000],  loss: 0.017318, mae: 0.318287, mean_q: 0.571282, mean_eps: 0.000000
 1516/5000: episode: 53, duration: 0.406s, episode steps:  29, steps per second:  71, episode reward: 36.000, mean reward:  1.241 [-3.000, 32.250], mean action: 2.379 [0.000, 15.000],  loss: 0.020785, mae: 0.342145, mean_q: 0.529718, mean_eps: 0.000000
 1547/5000: episode: 54, duration: 0.452s, episode steps:  31, steps per second:  69, episode reward: 46.970, mean reward:  1.515 [-0.560, 32.310], mean action: 4.871 [0.000, 15.000],  loss: 0.021144, mae: 0.349622, mean_q: 0.505622, mean_eps: 0.000000
 1568/5000: episode: 55, duration: 0.312s, episode steps:  21, steps per second:  67, episode reward: 41.213, mean reward:  1.963 [-2.263, 32.286], mean action: 1.714 [0.000, 12.000],  loss: 0.021606, mae: 0.340702, mean_q: 0.560279, mean_eps: 0.000000
 1590/5000: episode: 56, duration: 0.319s, episode steps:  22, steps per second:  69, episode reward: 38.032, mean reward:  1.729 [-2.998, 32.143], mean action: 4.818 [0.000, 16.000],  loss: 0.020036, mae: 0.335385, mean_q: 0.573084, mean_eps: 0.000000
 1612/5000: episode: 57, duration: 0.344s, episode steps:  22, steps per second:  64, episode reward: 42.000, mean reward:  1.909 [-2.248, 29.466], mean action: 1.682 [0.000, 16.000],  loss: 0.022616, mae: 0.342491, mean_q: 0.612153, mean_eps: 0.000000
 1637/5000: episode: 58, duration: 0.365s, episode steps:  25, steps per second:  69, episode reward: 41.161, mean reward:  1.646 [-2.245, 31.928], mean action: 3.880 [0.000, 20.000],  loss: 0.022655, mae: 0.347026, mean_q: 0.575002, mean_eps: 0.000000
 1656/5000: episode: 59, duration: 0.283s, episode steps:  19, steps per second:  67, episode reward: 44.374, mean reward:  2.335 [-2.121, 32.170], mean action: 0.579 [0.000, 4.000],  loss: 0.022793, mae: 0.346540, mean_q: 0.610024, mean_eps: 0.000000
 1702/5000: episode: 60, duration: 0.637s, episode steps:  46, steps per second:  72, episode reward: -32.580, mean reward: -0.708 [-32.130,  2.320], mean action: 5.717 [1.000, 18.000],  loss: 0.020281, mae: 0.340186, mean_q: 0.557824, mean_eps: 0.000000
 1721/5000: episode: 61, duration: 0.276s, episode steps:  19, steps per second:  69, episode reward: 36.000, mean reward:  1.895 [-2.412, 32.080], mean action: 3.211 [0.000, 15.000],  loss: 0.019992, mae: 0.350280, mean_q: 0.457797, mean_eps: 0.000000
 1756/5000: episode: 62, duration: 0.486s, episode steps:  35, steps per second:  72, episode reward: 32.888, mean reward:  0.940 [-3.000, 32.240], mean action: 2.657 [0.000, 15.000],  loss: 0.018071, mae: 0.330163, mean_q: 0.515137, mean_eps: 0.000000
 1783/5000: episode: 63, duration: 0.386s, episode steps:  27, steps per second:  70, episode reward: 41.904, mean reward:  1.552 [-3.000, 32.284], mean action: 3.333 [1.000, 19.000],  loss: 0.022615, mae: 0.345950, mean_q: 0.532319, mean_eps: 0.000000
 1810/5000: episode: 64, duration: 0.392s, episode steps:  27, steps per second:  69, episode reward: 44.939, mean reward:  1.664 [-2.354, 33.000], mean action: 1.926 [0.000, 19.000],  loss: 0.016542, mae: 0.324618, mean_q: 0.519542, mean_eps: 0.000000
 1843/5000: episode: 65, duration: 0.470s, episode steps:  33, steps per second:  70, episode reward: 39.000, mean reward:  1.182 [-3.000, 32.170], mean action: 3.182 [0.000, 19.000],  loss: 0.020387, mae: 0.334960, mean_q: 0.563317, mean_eps: 0.000000
 1872/5000: episode: 66, duration: 0.408s, episode steps:  29, steps per second:  71, episode reward: 38.818, mean reward:  1.339 [-2.277, 32.370], mean action: 2.759 [0.000, 13.000],  loss: 0.021538, mae: 0.336476, mean_q: 0.530859, mean_eps: 0.000000
 1895/5000: episode: 67, duration: 0.340s, episode steps:  23, steps per second:  68, episode reward: 44.212, mean reward:  1.922 [-2.322, 31.916], mean action: 1.652 [0.000, 4.000],  loss: 0.020477, mae: 0.339591, mean_q: 0.450893, mean_eps: 0.000000
 1929/5000: episode: 68, duration: 0.472s, episode steps:  34, steps per second:  72, episode reward: 36.000, mean reward:  1.059 [-2.586, 30.002], mean action: 4.000 [0.000, 19.000],  loss: 0.019569, mae: 0.334566, mean_q: 0.543939, mean_eps: 0.000000
 1953/5000: episode: 69, duration: 0.346s, episode steps:  24, steps per second:  69, episode reward: 41.325, mean reward:  1.722 [-2.711, 32.904], mean action: 2.125 [0.000, 12.000],  loss: 0.020476, mae: 0.331456, mean_q: 0.537969, mean_eps: 0.000000
 1988/5000: episode: 70, duration: 0.482s, episode steps:  35, steps per second:  73, episode reward: 38.250, mean reward:  1.093 [-3.000, 32.470], mean action: 5.057 [0.000, 15.000],  loss: 0.020435, mae: 0.336613, mean_q: 0.527559, mean_eps: 0.000000
 2009/5000: episode: 71, duration: 0.298s, episode steps:  21, steps per second:  71, episode reward: 41.554, mean reward:  1.979 [-2.751, 32.104], mean action: 3.143 [0.000, 16.000],  loss: 0.018041, mae: 0.324937, mean_q: 0.540047, mean_eps: 0.000000
 2034/5000: episode: 72, duration: 0.361s, episode steps:  25, steps per second:  69, episode reward: 43.903, mean reward:  1.756 [-2.188, 32.237], mean action: 2.760 [0.000, 12.000],  loss: 0.019888, mae: 0.328360, mean_q: 0.565666, mean_eps: 0.000000
 2064/5000: episode: 73, duration: 0.458s, episode steps:  30, steps per second:  65, episode reward: 40.988, mean reward:  1.366 [-3.000, 32.523], mean action: 4.833 [0.000, 16.000],  loss: 0.022248, mae: 0.338473, mean_q: 0.611721, mean_eps: 0.000000
 2092/5000: episode: 74, duration: 0.399s, episode steps:  28, steps per second:  70, episode reward: 40.810, mean reward:  1.458 [-2.939, 31.744], mean action: 2.964 [0.000, 16.000],  loss: 0.021795, mae: 0.331239, mean_q: 0.525370, mean_eps: 0.000000
 2122/5000: episode: 75, duration: 0.433s, episode steps:  30, steps per second:  69, episode reward: 38.056, mean reward:  1.269 [-2.627, 32.140], mean action: 3.767 [2.000, 16.000],  loss: 0.024674, mae: 0.344741, mean_q: 0.541153, mean_eps: 0.000000
 2163/5000: episode: 76, duration: 0.568s, episode steps:  41, steps per second:  72, episode reward: 41.120, mean reward:  1.003 [-3.000, 32.450], mean action: 2.293 [0.000, 15.000],  loss: 0.017465, mae: 0.314754, mean_q: 0.569131, mean_eps: 0.000000
 2184/5000: episode: 77, duration: 0.309s, episode steps:  21, steps per second:  68, episode reward: 43.471, mean reward:  2.070 [-2.097, 32.079], mean action: 2.190 [0.000, 15.000],  loss: 0.024585, mae: 0.355826, mean_q: 0.598938, mean_eps: 0.000000
 2204/5000: episode: 78, duration: 0.296s, episode steps:  20, steps per second:  68, episode reward: 44.875, mean reward:  2.244 [-2.272, 32.020], mean action: 2.000 [0.000, 16.000],  loss: 0.018962, mae: 0.325489, mean_q: 0.584129, mean_eps: 0.000000
 2224/5000: episode: 79, duration: 0.287s, episode steps:  20, steps per second:  70, episode reward: 41.248, mean reward:  2.062 [-3.000, 32.200], mean action: 2.600 [0.000, 16.000],  loss: 0.020034, mae: 0.339741, mean_q: 0.556323, mean_eps: 0.000000
 2243/5000: episode: 80, duration: 0.281s, episode steps:  19, steps per second:  68, episode reward: 41.521, mean reward:  2.185 [-2.064, 29.140], mean action: 3.000 [0.000, 16.000],  loss: 0.017994, mae: 0.328789, mean_q: 0.552440, mean_eps: 0.000000
 2264/5000: episode: 81, duration: 0.302s, episode steps:  21, steps per second:  70, episode reward: 41.470, mean reward:  1.975 [-2.602, 32.433], mean action: 3.286 [0.000, 16.000],  loss: 0.018055, mae: 0.328047, mean_q: 0.518605, mean_eps: 0.000000
 2289/5000: episode: 82, duration: 0.354s, episode steps:  25, steps per second:  71, episode reward: 43.920, mean reward:  1.757 [-2.034, 33.000], mean action: 6.040 [0.000, 20.000],  loss: 0.020429, mae: 0.339285, mean_q: 0.510516, mean_eps: 0.000000
 2347/5000: episode: 83, duration: 0.788s, episode steps:  58, steps per second:  74, episode reward: -32.090, mean reward: -0.553 [-32.340,  2.300], mean action: 3.621 [0.000, 19.000],  loss: 0.019675, mae: 0.329889, mean_q: 0.586536, mean_eps: 0.000000
 2385/5000: episode: 84, duration: 0.525s, episode steps:  38, steps per second:  72, episode reward: 38.667, mean reward:  1.018 [-2.328, 32.350], mean action: 4.026 [0.000, 21.000],  loss: 0.018376, mae: 0.318049, mean_q: 0.550356, mean_eps: 0.000000
 2420/5000: episode: 85, duration: 0.498s, episode steps:  35, steps per second:  70, episode reward: 36.000, mean reward:  1.029 [-2.644, 32.220], mean action: 3.543 [0.000, 16.000],  loss: 0.021460, mae: 0.334661, mean_q: 0.523098, mean_eps: 0.000000
 2448/5000: episode: 86, duration: 0.398s, episode steps:  28, steps per second:  70, episode reward: 37.826, mean reward:  1.351 [-2.474, 32.333], mean action: 3.714 [0.000, 16.000],  loss: 0.023507, mae: 0.336850, mean_q: 0.468041, mean_eps: 0.000000
 2477/5000: episode: 87, duration: 0.419s, episode steps:  29, steps per second:  69, episode reward: 41.906, mean reward:  1.445 [-2.129, 32.120], mean action: 3.621 [0.000, 16.000],  loss: 0.020187, mae: 0.329523, mean_q: 0.499253, mean_eps: 0.000000
 2505/5000: episode: 88, duration: 0.394s, episode steps:  28, steps per second:  71, episode reward: 42.000, mean reward:  1.500 [-3.000, 32.310], mean action: 2.464 [0.000, 16.000],  loss: 0.018711, mae: 0.325739, mean_q: 0.536554, mean_eps: 0.000000
 2559/5000: episode: 89, duration: 0.729s, episode steps:  54, steps per second:  74, episode reward: 40.152, mean reward:  0.744 [-3.000, 32.130], mean action: 2.204 [0.000, 19.000],  loss: 0.020808, mae: 0.343426, mean_q: 0.513099, mean_eps: 0.000000
 2580/5000: episode: 90, duration: 0.300s, episode steps:  21, steps per second:  70, episode reward: 39.000, mean reward:  1.857 [-2.863, 32.120], mean action: 4.714 [0.000, 15.000],  loss: 0.018006, mae: 0.323754, mean_q: 0.525882, mean_eps: 0.000000
 2617/5000: episode: 91, duration: 0.512s, episode steps:  37, steps per second:  72, episode reward: 32.704, mean reward:  0.884 [-3.000, 32.524], mean action: 5.541 [0.000, 20.000],  loss: 0.020008, mae: 0.330666, mean_q: 0.533207, mean_eps: 0.000000
 2638/5000: episode: 92, duration: 0.310s, episode steps:  21, steps per second:  68, episode reward: 41.647, mean reward:  1.983 [-2.971, 31.937], mean action: 2.190 [0.000, 15.000],  loss: 0.020444, mae: 0.331139, mean_q: 0.533119, mean_eps: 0.000000
 2675/5000: episode: 93, duration: 0.511s, episode steps:  37, steps per second:  72, episode reward: 39.000, mean reward:  1.054 [-2.940, 32.110], mean action: 2.243 [0.000, 15.000],  loss: 0.024702, mae: 0.349134, mean_q: 0.578584, mean_eps: 0.000000
 2704/5000: episode: 94, duration: 0.665s, episode steps:  29, steps per second:  44, episode reward: 41.141, mean reward:  1.419 [-2.296, 32.030], mean action: 2.552 [0.000, 12.000],  loss: 0.018849, mae: 0.323161, mean_q: 0.541444, mean_eps: 0.000000
 2735/5000: episode: 95, duration: 0.438s, episode steps:  31, steps per second:  71, episode reward: 39.000, mean reward:  1.258 [-2.964, 32.300], mean action: 3.032 [0.000, 14.000],  loss: 0.019647, mae: 0.338742, mean_q: 0.561472, mean_eps: 0.000000
 2777/5000: episode: 96, duration: 0.587s, episode steps:  42, steps per second:  72, episode reward: 41.640, mean reward:  0.991 [-2.107, 32.350], mean action: 2.500 [0.000, 13.000],  loss: 0.020071, mae: 0.333841, mean_q: 0.541729, mean_eps: 0.000000
 2803/5000: episode: 97, duration: 0.369s, episode steps:  26, steps per second:  71, episode reward: 43.868, mean reward:  1.687 [-2.457, 32.140], mean action: 2.346 [0.000, 11.000],  loss: 0.024034, mae: 0.349793, mean_q: 0.504112, mean_eps: 0.000000
 2831/5000: episode: 98, duration: 0.393s, episode steps:  28, steps per second:  71, episode reward: 35.062, mean reward:  1.252 [-2.753, 31.876], mean action: 5.071 [0.000, 20.000],  loss: 0.020177, mae: 0.323293, mean_q: 0.535748, mean_eps: 0.000000
 2906/5000: episode: 99, duration: 1.067s, episode steps:  75, steps per second:  70, episode reward: 32.877, mean reward:  0.438 [-2.382, 32.102], mean action: 4.800 [0.000, 20.000],  loss: 0.022507, mae: 0.347024, mean_q: 0.494928, mean_eps: 0.000000
 2929/5000: episode: 100, duration: 0.328s, episode steps:  23, steps per second:  70, episode reward: 38.712, mean reward:  1.683 [-2.372, 32.290], mean action: 3.522 [1.000, 14.000],  loss: 0.021273, mae: 0.338272, mean_q: 0.510112, mean_eps: 0.000000
 2957/5000: episode: 101, duration: 0.407s, episode steps:  28, steps per second:  69, episode reward: 37.634, mean reward:  1.344 [-3.000, 32.375], mean action: 5.964 [0.000, 14.000],  loss: 0.019992, mae: 0.333334, mean_q: 0.521055, mean_eps: 0.000000
 2978/5000: episode: 102, duration: 0.322s, episode steps:  21, steps per second:  65, episode reward: 39.000, mean reward:  1.857 [-3.000, 32.390], mean action: 3.143 [0.000, 11.000],  loss: 0.021077, mae: 0.327988, mean_q: 0.585811, mean_eps: 0.000000
 2998/5000: episode: 103, duration: 0.296s, episode steps:  20, steps per second:  68, episode reward: 41.629, mean reward:  2.081 [-2.070, 31.759], mean action: 2.650 [0.000, 11.000],  loss: 0.021323, mae: 0.333517, mean_q: 0.518907, mean_eps: 0.000000
 3024/5000: episode: 104, duration: 0.403s, episode steps:  26, steps per second:  64, episode reward: 42.000, mean reward:  1.615 [-2.467, 32.140], mean action: 1.615 [0.000, 11.000],  loss: 0.019703, mae: 0.321000, mean_q: 0.467868, mean_eps: 0.000000
 3044/5000: episode: 105, duration: 0.322s, episode steps:  20, steps per second:  62, episode reward: 41.729, mean reward:  2.086 [-2.407, 32.729], mean action: 1.900 [0.000, 11.000],  loss: 0.026616, mae: 0.353155, mean_q: 0.557944, mean_eps: 0.000000
 3058/5000: episode: 106, duration: 0.211s, episode steps:  14, steps per second:  66, episode reward: 47.616, mean reward:  3.401 [-0.044, 32.090], mean action: 2.786 [2.000, 3.000],  loss: 0.020803, mae: 0.327648, mean_q: 0.522858, mean_eps: 0.000000
 3096/5000: episode: 107, duration: 0.538s, episode steps:  38, steps per second:  71, episode reward: 44.379, mean reward:  1.168 [-3.000, 32.180], mean action: 2.237 [0.000, 20.000],  loss: 0.020758, mae: 0.332329, mean_q: 0.518096, mean_eps: 0.000000
 3136/5000: episode: 108, duration: 0.552s, episode steps:  40, steps per second:  72, episode reward: -32.760, mean reward: -0.819 [-33.000,  3.000], mean action: 3.700 [0.000, 16.000],  loss: 0.020442, mae: 0.330183, mean_q: 0.595913, mean_eps: 0.000000
 3157/5000: episode: 109, duration: 0.322s, episode steps:  21, steps per second:  65, episode reward: 41.786, mean reward:  1.990 [-2.535, 32.096], mean action: 1.810 [0.000, 16.000],  loss: 0.018391, mae: 0.319626, mean_q: 0.605764, mean_eps: 0.000000
 3177/5000: episode: 110, duration: 0.296s, episode steps:  20, steps per second:  68, episode reward: 44.795, mean reward:  2.240 [-2.132, 32.250], mean action: 1.950 [0.000, 12.000],  loss: 0.019835, mae: 0.336585, mean_q: 0.581612, mean_eps: 0.000000
 3198/5000: episode: 111, duration: 0.307s, episode steps:  21, steps per second:  68, episode reward: 41.605, mean reward:  1.981 [-2.726, 32.800], mean action: 2.190 [0.000, 14.000],  loss: 0.017881, mae: 0.327426, mean_q: 0.524742, mean_eps: 0.000000
 3220/5000: episode: 112, duration: 0.345s, episode steps:  22, steps per second:  64, episode reward: 38.517, mean reward:  1.751 [-2.572, 31.637], mean action: 4.318 [0.000, 16.000],  loss: 0.020474, mae: 0.343434, mean_q: 0.526542, mean_eps: 0.000000
 3234/5000: episode: 113, duration: 0.213s, episode steps:  14, steps per second:  66, episode reward: 44.338, mean reward:  3.167 [-2.819, 32.110], mean action: 2.571 [0.000, 16.000],  loss: 0.023448, mae: 0.348993, mean_q: 0.567563, mean_eps: 0.000000
 3269/5000: episode: 114, duration: 0.484s, episode steps:  35, steps per second:  72, episode reward: 36.000, mean reward:  1.029 [-2.306, 32.410], mean action: 7.686 [0.000, 16.000],  loss: 0.020346, mae: 0.333835, mean_q: 0.564008, mean_eps: 0.000000
 3315/5000: episode: 115, duration: 0.636s, episode steps:  46, steps per second:  72, episode reward: 33.000, mean reward:  0.717 [-3.000, 32.500], mean action: 6.848 [0.000, 19.000],  loss: 0.022190, mae: 0.334945, mean_q: 0.516229, mean_eps: 0.000000
 3332/5000: episode: 116, duration: 0.259s, episode steps:  17, steps per second:  66, episode reward: 41.054, mean reward:  2.415 [-2.245, 32.950], mean action: 4.941 [0.000, 16.000],  loss: 0.016208, mae: 0.312699, mean_q: 0.530789, mean_eps: 0.000000
 3358/5000: episode: 117, duration: 0.464s, episode steps:  26, steps per second:  56, episode reward: 38.558, mean reward:  1.483 [-2.359, 32.470], mean action: 3.654 [0.000, 20.000],  loss: 0.014909, mae: 0.311299, mean_q: 0.509587, mean_eps: 0.000000
 3385/5000: episode: 118, duration: 0.382s, episode steps:  27, steps per second:  71, episode reward: 44.887, mean reward:  1.662 [-2.143, 32.297], mean action: 1.000 [0.000, 11.000],  loss: 0.020969, mae: 0.329937, mean_q: 0.541455, mean_eps: 0.000000
 3399/5000: episode: 119, duration: 0.209s, episode steps:  14, steps per second:  67, episode reward: 41.444, mean reward:  2.960 [-2.099, 32.444], mean action: 3.143 [0.000, 12.000],  loss: 0.020243, mae: 0.326008, mean_q: 0.583954, mean_eps: 0.000000
 3421/5000: episode: 120, duration: 0.328s, episode steps:  22, steps per second:  67, episode reward: 40.402, mean reward:  1.836 [-2.170, 32.190], mean action: 3.136 [0.000, 16.000],  loss: 0.019462, mae: 0.331734, mean_q: 0.481697, mean_eps: 0.000000
 3448/5000: episode: 121, duration: 0.386s, episode steps:  27, steps per second:  70, episode reward: 38.807, mean reward:  1.437 [-3.000, 32.263], mean action: 3.370 [0.000, 16.000],  loss: 0.020712, mae: 0.331532, mean_q: 0.442614, mean_eps: 0.000000
 3469/5000: episode: 122, duration: 0.310s, episode steps:  21, steps per second:  68, episode reward: 44.654, mean reward:  2.126 [-2.050, 32.280], mean action: 1.905 [0.000, 3.000],  loss: 0.019372, mae: 0.320085, mean_q: 0.469246, mean_eps: 0.000000
 3486/5000: episode: 123, duration: 0.253s, episode steps:  17, steps per second:  67, episode reward: 44.545, mean reward:  2.620 [-2.008, 33.000], mean action: 1.294 [0.000, 3.000],  loss: 0.014606, mae: 0.299882, mean_q: 0.469740, mean_eps: 0.000000
 3507/5000: episode: 124, duration: 0.306s, episode steps:  21, steps per second:  69, episode reward: 44.708, mean reward:  2.129 [-2.142, 32.130], mean action: 2.286 [0.000, 20.000],  loss: 0.024335, mae: 0.350003, mean_q: 0.520815, mean_eps: 0.000000
 3534/5000: episode: 125, duration: 0.390s, episode steps:  27, steps per second:  69, episode reward: 41.679, mean reward:  1.544 [-2.425, 32.120], mean action: 2.111 [1.000, 9.000],  loss: 0.018989, mae: 0.315372, mean_q: 0.567214, mean_eps: 0.000000
 3562/5000: episode: 126, duration: 0.401s, episode steps:  28, steps per second:  70, episode reward: 41.901, mean reward:  1.496 [-2.689, 32.161], mean action: 3.286 [0.000, 12.000],  loss: 0.021246, mae: 0.325319, mean_q: 0.509510, mean_eps: 0.000000
 3580/5000: episode: 127, duration: 0.263s, episode steps:  18, steps per second:  69, episode reward: 44.043, mean reward:  2.447 [-2.435, 32.371], mean action: 3.500 [0.000, 20.000],  loss: 0.021539, mae: 0.334716, mean_q: 0.539771, mean_eps: 0.000000
 3611/5000: episode: 128, duration: 0.449s, episode steps:  31, steps per second:  69, episode reward: 44.144, mean reward:  1.424 [-2.079, 32.700], mean action: 3.323 [0.000, 20.000],  loss: 0.020466, mae: 0.330727, mean_q: 0.544311, mean_eps: 0.000000
 3629/5000: episode: 129, duration: 0.267s, episode steps:  18, steps per second:  67, episode reward: 44.860, mean reward:  2.492 [-2.357, 32.270], mean action: 2.167 [0.000, 11.000],  loss: 0.016276, mae: 0.304316, mean_q: 0.526567, mean_eps: 0.000000
 3656/5000: episode: 130, duration: 0.385s, episode steps:  27, steps per second:  70, episode reward: 44.172, mean reward:  1.636 [-2.418, 32.170], mean action: 2.889 [0.000, 16.000],  loss: 0.022224, mae: 0.335386, mean_q: 0.498334, mean_eps: 0.000000
 3676/5000: episode: 131, duration: 0.288s, episode steps:  20, steps per second:  69, episode reward: 41.691, mean reward:  2.085 [-2.385, 31.951], mean action: 5.050 [0.000, 19.000],  loss: 0.018756, mae: 0.324041, mean_q: 0.508052, mean_eps: 0.000000
 3701/5000: episode: 132, duration: 0.360s, episode steps:  25, steps per second:  69, episode reward: 40.886, mean reward:  1.635 [-2.418, 32.020], mean action: 3.920 [0.000, 20.000],  loss: 0.019288, mae: 0.320424, mean_q: 0.512908, mean_eps: 0.000000
 3723/5000: episode: 133, duration: 0.316s, episode steps:  22, steps per second:  70, episode reward: 38.872, mean reward:  1.767 [-2.774, 33.000], mean action: 5.455 [0.000, 19.000],  loss: 0.022021, mae: 0.332903, mean_q: 0.524181, mean_eps: 0.000000
 3748/5000: episode: 134, duration: 0.358s, episode steps:  25, steps per second:  70, episode reward: 38.120, mean reward:  1.525 [-2.193, 32.281], mean action: 4.600 [0.000, 20.000],  loss: 0.021012, mae: 0.331174, mean_q: 0.495431, mean_eps: 0.000000
 3772/5000: episode: 135, duration: 0.346s, episode steps:  24, steps per second:  69, episode reward: 41.796, mean reward:  1.741 [-3.000, 32.030], mean action: 1.000 [0.000, 12.000],  loss: 0.020569, mae: 0.321883, mean_q: 0.476324, mean_eps: 0.000000
 3800/5000: episode: 136, duration: 0.396s, episode steps:  28, steps per second:  71, episode reward: 40.415, mean reward:  1.443 [-2.326, 31.310], mean action: 1.643 [0.000, 8.000],  loss: 0.021668, mae: 0.328453, mean_q: 0.570252, mean_eps: 0.000000
 3821/5000: episode: 137, duration: 0.309s, episode steps:  21, steps per second:  68, episode reward: 47.283, mean reward:  2.252 [-0.040, 32.140], mean action: 2.524 [0.000, 20.000],  loss: 0.020193, mae: 0.324304, mean_q: 0.577472, mean_eps: 0.000000
 3834/5000: episode: 138, duration: 0.209s, episode steps:  13, steps per second:  62, episode reward: 46.894, mean reward:  3.607 [-0.282, 33.000], mean action: 2.154 [0.000, 12.000],  loss: 0.017612, mae: 0.316572, mean_q: 0.538244, mean_eps: 0.000000
 3863/5000: episode: 139, duration: 0.406s, episode steps:  29, steps per second:  71, episode reward: 38.400, mean reward:  1.324 [-2.263, 32.049], mean action: 5.931 [0.000, 16.000],  loss: 0.023142, mae: 0.348283, mean_q: 0.506407, mean_eps: 0.000000
 3885/5000: episode: 140, duration: 0.316s, episode steps:  22, steps per second:  70, episode reward: 38.366, mean reward:  1.744 [-3.000, 32.510], mean action: 2.955 [0.000, 16.000],  loss: 0.025401, mae: 0.354013, mean_q: 0.536503, mean_eps: 0.000000
 3909/5000: episode: 141, duration: 0.338s, episode steps:  24, steps per second:  71, episode reward: -35.920, mean reward: -1.497 [-33.000,  3.000], mean action: 6.375 [0.000, 16.000],  loss: 0.020424, mae: 0.332374, mean_q: 0.544487, mean_eps: 0.000000
 3937/5000: episode: 142, duration: 0.395s, episode steps:  28, steps per second:  71, episode reward: 39.000, mean reward:  1.393 [-2.877, 32.040], mean action: 3.429 [0.000, 16.000],  loss: 0.022601, mae: 0.340351, mean_q: 0.590868, mean_eps: 0.000000
 3968/5000: episode: 143, duration: 0.431s, episode steps:  31, steps per second:  72, episode reward: 43.923, mean reward:  1.417 [-2.185, 32.470], mean action: 3.452 [0.000, 8.000],  loss: 0.024266, mae: 0.345048, mean_q: 0.581457, mean_eps: 0.000000
 3988/5000: episode: 144, duration: 0.296s, episode steps:  20, steps per second:  67, episode reward: 41.173, mean reward:  2.059 [-2.328, 32.740], mean action: 2.400 [0.000, 3.000],  loss: 0.024741, mae: 0.352782, mean_q: 0.586120, mean_eps: 0.000000
 4020/5000: episode: 145, duration: 0.448s, episode steps:  32, steps per second:  71, episode reward: 38.077, mean reward:  1.190 [-2.847, 32.030], mean action: 2.531 [0.000, 12.000],  loss: 0.019492, mae: 0.329227, mean_q: 0.594373, mean_eps: 0.000000
 4045/5000: episode: 146, duration: 0.355s, episode steps:  25, steps per second:  70, episode reward: 32.193, mean reward:  1.288 [-3.000, 31.948], mean action: 4.800 [0.000, 15.000],  loss: 0.020753, mae: 0.329072, mean_q: 0.571582, mean_eps: 0.000000
 4080/5000: episode: 147, duration: 0.488s, episode steps:  35, steps per second:  72, episode reward: 42.000, mean reward:  1.200 [-2.268, 32.240], mean action: 2.229 [0.000, 9.000],  loss: 0.018822, mae: 0.320430, mean_q: 0.596666, mean_eps: 0.000000
 4105/5000: episode: 148, duration: 0.357s, episode steps:  25, steps per second:  70, episode reward: 41.714, mean reward:  1.669 [-2.605, 32.294], mean action: 2.000 [1.000, 12.000],  loss: 0.022399, mae: 0.342576, mean_q: 0.570707, mean_eps: 0.000000
 4124/5000: episode: 149, duration: 0.290s, episode steps:  19, steps per second:  66, episode reward: 46.296, mean reward:  2.437 [-0.393, 32.740], mean action: 3.947 [0.000, 20.000],  loss: 0.023216, mae: 0.343465, mean_q: 0.534579, mean_eps: 0.000000
 4157/5000: episode: 150, duration: 0.454s, episode steps:  33, steps per second:  73, episode reward: 39.000, mean reward:  1.182 [-2.213, 32.170], mean action: 3.424 [0.000, 14.000],  loss: 0.019481, mae: 0.324740, mean_q: 0.571183, mean_eps: 0.000000
 4181/5000: episode: 151, duration: 0.345s, episode steps:  24, steps per second:  69, episode reward: 45.000, mean reward:  1.875 [-2.085, 32.170], mean action: 1.750 [0.000, 9.000],  loss: 0.019055, mae: 0.334629, mean_q: 0.596962, mean_eps: 0.000000
 4199/5000: episode: 152, duration: 0.468s, episode steps:  18, steps per second:  38, episode reward: 38.850, mean reward:  2.158 [-3.000, 32.760], mean action: 4.556 [0.000, 19.000],  loss: 0.023509, mae: 0.359076, mean_q: 0.551925, mean_eps: 0.000000
 4226/5000: episode: 153, duration: 0.389s, episode steps:  27, steps per second:  69, episode reward: 42.000, mean reward:  1.556 [-2.361, 32.330], mean action: 2.222 [0.000, 16.000],  loss: 0.017854, mae: 0.324857, mean_q: 0.578462, mean_eps: 0.000000
 4258/5000: episode: 154, duration: 0.449s, episode steps:  32, steps per second:  71, episode reward: 35.939, mean reward:  1.123 [-3.000, 32.030], mean action: 5.125 [0.000, 20.000],  loss: 0.018435, mae: 0.322769, mean_q: 0.561906, mean_eps: 0.000000
 4278/5000: episode: 155, duration: 0.293s, episode steps:  20, steps per second:  68, episode reward: 44.270, mean reward:  2.213 [-2.487, 31.450], mean action: 3.200 [0.000, 12.000],  loss: 0.022295, mae: 0.349413, mean_q: 0.557990, mean_eps: 0.000000
 4301/5000: episode: 156, duration: 0.327s, episode steps:  23, steps per second:  70, episode reward: 35.202, mean reward:  1.531 [-3.000, 32.014], mean action: 4.609 [0.000, 16.000],  loss: 0.021951, mae: 0.334025, mean_q: 0.566690, mean_eps: 0.000000
 4369/5000: episode: 157, duration: 0.915s, episode steps:  68, steps per second:  74, episode reward: 38.510, mean reward:  0.566 [-2.332, 32.280], mean action: 2.588 [0.000, 12.000],  loss: 0.018819, mae: 0.327875, mean_q: 0.553400, mean_eps: 0.000000
 4393/5000: episode: 158, duration: 0.346s, episode steps:  24, steps per second:  69, episode reward: 44.558, mean reward:  1.857 [-2.276, 32.400], mean action: 2.375 [0.000, 14.000],  loss: 0.022338, mae: 0.342025, mean_q: 0.532209, mean_eps: 0.000000
 4422/5000: episode: 159, duration: 0.411s, episode steps:  29, steps per second:  71, episode reward: 38.619, mean reward:  1.332 [-2.425, 32.050], mean action: 2.345 [0.000, 16.000],  loss: 0.021936, mae: 0.329504, mean_q: 0.540787, mean_eps: 0.000000
 4446/5000: episode: 160, duration: 0.346s, episode steps:  24, steps per second:  69, episode reward: 42.000, mean reward:  1.750 [-2.180, 32.190], mean action: 2.500 [0.000, 16.000],  loss: 0.021284, mae: 0.329627, mean_q: 0.559951, mean_eps: 0.000000
 4464/5000: episode: 161, duration: 0.263s, episode steps:  18, steps per second:  69, episode reward: 41.886, mean reward:  2.327 [-2.680, 32.240], mean action: 4.944 [0.000, 16.000],  loss: 0.021086, mae: 0.334910, mean_q: 0.543530, mean_eps: 0.000000
 4509/5000: episode: 162, duration: 0.614s, episode steps:  45, steps per second:  73, episode reward: 44.021, mean reward:  0.978 [-2.323, 32.127], mean action: 1.778 [0.000, 13.000],  loss: 0.020728, mae: 0.336677, mean_q: 0.538828, mean_eps: 0.000000
 4565/5000: episode: 163, duration: 0.770s, episode steps:  56, steps per second:  73, episode reward: 32.507, mean reward:  0.580 [-3.000, 32.050], mean action: 5.446 [0.000, 19.000],  loss: 0.022026, mae: 0.342202, mean_q: 0.474884, mean_eps: 0.000000
 4600/5000: episode: 164, duration: 0.488s, episode steps:  35, steps per second:  72, episode reward: 44.344, mean reward:  1.267 [-2.067, 32.090], mean action: 1.200 [0.000, 11.000],  loss: 0.022207, mae: 0.332976, mean_q: 0.513769, mean_eps: 0.000000
 4625/5000: episode: 165, duration: 0.360s, episode steps:  25, steps per second:  69, episode reward: 44.176, mean reward:  1.767 [-3.000, 32.710], mean action: 3.200 [0.000, 13.000],  loss: 0.022393, mae: 0.329013, mean_q: 0.489891, mean_eps: 0.000000
 4642/5000: episode: 166, duration: 0.253s, episode steps:  17, steps per second:  67, episode reward: 44.214, mean reward:  2.601 [-2.393, 31.838], mean action: 3.529 [0.000, 14.000],  loss: 0.023622, mae: 0.330874, mean_q: 0.539323, mean_eps: 0.000000
 4674/5000: episode: 167, duration: 0.457s, episode steps:  32, steps per second:  70, episode reward: 44.760, mean reward:  1.399 [-2.297, 32.080], mean action: 1.344 [0.000, 9.000],  loss: 0.019929, mae: 0.313532, mean_q: 0.545387, mean_eps: 0.000000
 4696/5000: episode: 168, duration: 0.317s, episode steps:  22, steps per second:  69, episode reward: 42.000, mean reward:  1.909 [-2.227, 32.240], mean action: 3.682 [0.000, 20.000],  loss: 0.022159, mae: 0.323773, mean_q: 0.554018, mean_eps: 0.000000
 4723/5000: episode: 169, duration: 0.385s, episode steps:  27, steps per second:  70, episode reward: 44.078, mean reward:  1.633 [-2.403, 32.015], mean action: 1.037 [0.000, 4.000],  loss: 0.020678, mae: 0.326220, mean_q: 0.519900, mean_eps: 0.000000
 4742/5000: episode: 170, duration: 0.326s, episode steps:  19, steps per second:  58, episode reward: 44.305, mean reward:  2.332 [-2.254, 32.104], mean action: 2.789 [0.000, 15.000],  loss: 0.020271, mae: 0.320908, mean_q: 0.557873, mean_eps: 0.000000
 4790/5000: episode: 171, duration: 0.703s, episode steps:  48, steps per second:  68, episode reward: 41.844, mean reward:  0.872 [-2.292, 32.090], mean action: 3.417 [0.000, 20.000],  loss: 0.022030, mae: 0.335475, mean_q: 0.547735, mean_eps: 0.000000
 4826/5000: episode: 172, duration: 0.505s, episode steps:  36, steps per second:  71, episode reward: 33.000, mean reward:  0.917 [-3.000, 32.330], mean action: 6.500 [0.000, 19.000],  loss: 0.020920, mae: 0.330133, mean_q: 0.551948, mean_eps: 0.000000
 4847/5000: episode: 173, duration: 0.315s, episode steps:  21, steps per second:  67, episode reward: 44.133, mean reward:  2.102 [-2.344, 32.180], mean action: 3.238 [0.000, 12.000],  loss: 0.019064, mae: 0.320820, mean_q: 0.514231, mean_eps: 0.000000
 4893/5000: episode: 174, duration: 0.665s, episode steps:  46, steps per second:  69, episode reward: 44.579, mean reward:  0.969 [-2.045, 32.219], mean action: 2.457 [0.000, 16.000],  loss: 0.023781, mae: 0.340300, mean_q: 0.522301, mean_eps: 0.000000
 4916/5000: episode: 175, duration: 0.367s, episode steps:  23, steps per second:  63, episode reward: 39.000, mean reward:  1.696 [-3.000, 32.160], mean action: 2.130 [0.000, 9.000],  loss: 0.026669, mae: 0.350343, mean_q: 0.562203, mean_eps: 0.000000
 4939/5000: episode: 176, duration: 0.330s, episode steps:  23, steps per second:  70, episode reward: 41.331, mean reward:  1.797 [-2.239, 32.120], mean action: 1.174 [0.000, 4.000],  loss: 0.018845, mae: 0.307796, mean_q: 0.535678, mean_eps: 0.000000
 4953/5000: episode: 177, duration: 0.214s, episode steps:  14, steps per second:  65, episode reward: 47.175, mean reward:  3.370 [-0.060, 32.200], mean action: 1.000 [0.000, 14.000],  loss: 0.019964, mae: 0.313655, mean_q: 0.533383, mean_eps: 0.000000
 4972/5000: episode: 178, duration: 0.275s, episode steps:  19, steps per second:  69, episode reward: 41.237, mean reward:  2.170 [-2.574, 32.440], mean action: 1.053 [0.000, 11.000],  loss: 0.020620, mae: 0.319649, mean_q: 0.523291, mean_eps: 0.000000
 4987/5000: episode: 179, duration: 0.221s, episode steps:  15, steps per second:  68, episode reward: 48.000, mean reward:  3.200 [-0.030, 32.280], mean action: 1.800 [0.000, 11.000],  loss: 0.021528, mae: 0.325902, mean_q: 0.541519, mean_eps: 0.000000
done, took 65.000 seconds
DQN Evaluation: 10073 victories out of 11756 episodes
Training for 5000 steps ...
   23/5000: episode: 1, duration: 0.184s, episode steps:  23, steps per second: 125, episode reward: 32.106, mean reward:  1.396 [-2.901, 32.150], mean action: 7.304 [0.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   47/5000: episode: 2, duration: 0.175s, episode steps:  24, steps per second: 137, episode reward: -32.080, mean reward: -1.337 [-31.894,  3.000], mean action: 3.375 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   77/5000: episode: 3, duration: 0.203s, episode steps:  30, steps per second: 148, episode reward: 32.232, mean reward:  1.074 [-2.180, 31.965], mean action: 4.267 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  102/5000: episode: 4, duration: 0.179s, episode steps:  25, steps per second: 140, episode reward: 35.104, mean reward:  1.404 [-2.813, 32.671], mean action: 5.360 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  139/5000: episode: 5, duration: 0.228s, episode steps:  37, steps per second: 162, episode reward: 33.000, mean reward:  0.892 [-3.000, 32.170], mean action: 5.135 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  161/5000: episode: 6, duration: 0.156s, episode steps:  22, steps per second: 141, episode reward: 35.931, mean reward:  1.633 [-2.322, 32.331], mean action: 3.227 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  189/5000: episode: 7, duration: 0.191s, episode steps:  28, steps per second: 147, episode reward: 40.194, mean reward:  1.436 [-2.414, 31.897], mean action: 2.429 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  212/5000: episode: 8, duration: 0.160s, episode steps:  23, steps per second: 143, episode reward: -32.130, mean reward: -1.397 [-33.000,  3.000], mean action: 4.783 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  228/5000: episode: 9, duration: 0.119s, episode steps:  16, steps per second: 135, episode reward: 38.332, mean reward:  2.396 [-2.373, 31.921], mean action: 3.500 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  243/5000: episode: 10, duration: 0.108s, episode steps:  15, steps per second: 139, episode reward: 41.115, mean reward:  2.741 [-3.000, 32.310], mean action: 2.333 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  263/5000: episode: 11, duration: 0.144s, episode steps:  20, steps per second: 139, episode reward: -35.810, mean reward: -1.791 [-32.394,  2.490], mean action: 4.050 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  279/5000: episode: 12, duration: 0.121s, episode steps:  16, steps per second: 132, episode reward: 41.161, mean reward:  2.573 [-2.776, 32.161], mean action: 2.062 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  308/5000: episode: 13, duration: 0.199s, episode steps:  29, steps per second: 146, episode reward: -38.050, mean reward: -1.312 [-32.222,  2.181], mean action: 7.931 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  332/5000: episode: 14, duration: 0.163s, episode steps:  24, steps per second: 147, episode reward: 32.300, mean reward:  1.346 [-2.450, 32.080], mean action: 4.875 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  357/5000: episode: 15, duration: 0.166s, episode steps:  25, steps per second: 151, episode reward: 35.610, mean reward:  1.424 [-3.000, 31.757], mean action: 4.080 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  374/5000: episode: 16, duration: 0.126s, episode steps:  17, steps per second: 135, episode reward: 39.000, mean reward:  2.294 [-3.000, 32.230], mean action: 4.529 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  387/5000: episode: 17, duration: 0.099s, episode steps:  13, steps per second: 132, episode reward: 44.287, mean reward:  3.407 [-3.000, 32.070], mean action: 2.769 [1.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  410/5000: episode: 18, duration: 0.159s, episode steps:  23, steps per second: 145, episode reward: 36.000, mean reward:  1.565 [-2.707, 32.630], mean action: 3.391 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  432/5000: episode: 19, duration: 0.150s, episode steps:  22, steps per second: 147, episode reward: 32.296, mean reward:  1.468 [-3.000, 32.296], mean action: 4.045 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  451/5000: episode: 20, duration: 0.139s, episode steps:  19, steps per second: 137, episode reward: 38.923, mean reward:  2.049 [-2.920, 32.360], mean action: 3.789 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  466/5000: episode: 21, duration: 0.119s, episode steps:  15, steps per second: 127, episode reward: 43.482, mean reward:  2.899 [-2.452, 32.482], mean action: 4.600 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  486/5000: episode: 22, duration: 0.143s, episode steps:  20, steps per second: 140, episode reward: 38.254, mean reward:  1.913 [-2.481, 32.150], mean action: 4.600 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  510/5000: episode: 23, duration: 0.160s, episode steps:  24, steps per second: 150, episode reward: -35.690, mean reward: -1.487 [-32.180,  2.488], mean action: 7.000 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  528/5000: episode: 24, duration: 0.130s, episode steps:  18, steps per second: 138, episode reward: 39.000, mean reward:  2.167 [-3.000, 32.360], mean action: 2.500 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  562/5000: episode: 25, duration: 0.222s, episode steps:  34, steps per second: 153, episode reward: -32.360, mean reward: -0.952 [-32.172,  2.410], mean action: 6.618 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  588/5000: episode: 26, duration: 0.168s, episode steps:  26, steps per second: 155, episode reward: -35.280, mean reward: -1.357 [-31.978,  2.380], mean action: 7.308 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  607/5000: episode: 27, duration: 0.141s, episode steps:  19, steps per second: 135, episode reward: 41.902, mean reward:  2.205 [-2.130, 30.003], mean action: 4.211 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  630/5000: episode: 28, duration: 0.154s, episode steps:  23, steps per second: 150, episode reward: 32.368, mean reward:  1.407 [-2.609, 33.000], mean action: 5.304 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  655/5000: episode: 29, duration: 0.168s, episode steps:  25, steps per second: 149, episode reward: -33.000, mean reward: -1.320 [-32.377,  2.730], mean action: 6.520 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  676/5000: episode: 30, duration: 0.142s, episode steps:  21, steps per second: 148, episode reward: 35.736, mean reward:  1.702 [-3.000, 31.826], mean action: 5.333 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  690/5000: episode: 31, duration: 0.103s, episode steps:  14, steps per second: 136, episode reward: 41.010, mean reward:  2.929 [-2.401, 32.329], mean action: 4.714 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  720/5000: episode: 32, duration: 0.194s, episode steps:  30, steps per second: 155, episode reward: -32.060, mean reward: -1.069 [-32.239,  2.611], mean action: 5.267 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  734/5000: episode: 33, duration: 0.109s, episode steps:  14, steps per second: 128, episode reward: 41.081, mean reward:  2.934 [-2.090, 30.246], mean action: 5.071 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  756/5000: episode: 34, duration: 0.150s, episode steps:  22, steps per second: 146, episode reward: 35.482, mean reward:  1.613 [-2.520, 30.623], mean action: 5.909 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  779/5000: episode: 35, duration: 0.153s, episode steps:  23, steps per second: 150, episode reward: 35.439, mean reward:  1.541 [-3.000, 32.580], mean action: 4.304 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  805/5000: episode: 36, duration: 0.176s, episode steps:  26, steps per second: 148, episode reward: -32.490, mean reward: -1.250 [-32.053,  2.631], mean action: 6.115 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  820/5000: episode: 37, duration: 0.113s, episode steps:  15, steps per second: 133, episode reward: 47.094, mean reward:  3.140 [ 0.084, 32.865], mean action: 1.133 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  846/5000: episode: 38, duration: 0.165s, episode steps:  26, steps per second: 158, episode reward: -35.540, mean reward: -1.367 [-32.072,  2.337], mean action: 4.846 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  865/5000: episode: 39, duration: 0.129s, episode steps:  19, steps per second: 147, episode reward: 35.910, mean reward:  1.890 [-3.000, 32.410], mean action: 3.368 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  887/5000: episode: 40, duration: 0.148s, episode steps:  22, steps per second: 148, episode reward: 35.826, mean reward:  1.628 [-2.553, 32.150], mean action: 4.136 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  907/5000: episode: 41, duration: 0.141s, episode steps:  20, steps per second: 142, episode reward: 32.401, mean reward:  1.620 [-2.577, 32.027], mean action: 5.050 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  929/5000: episode: 42, duration: 0.154s, episode steps:  22, steps per second: 143, episode reward: -32.390, mean reward: -1.472 [-31.720,  2.855], mean action: 3.818 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  948/5000: episode: 43, duration: 0.128s, episode steps:  19, steps per second: 149, episode reward: 35.221, mean reward:  1.854 [-3.000, 31.611], mean action: 5.316 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  964/5000: episode: 44, duration: 0.119s, episode steps:  16, steps per second: 134, episode reward: 33.000, mean reward:  2.062 [-3.000, 29.950], mean action: 5.062 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  982/5000: episode: 45, duration: 0.147s, episode steps:  18, steps per second: 122, episode reward: 38.105, mean reward:  2.117 [-2.610, 31.829], mean action: 3.500 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  995/5000: episode: 46, duration: 0.101s, episode steps:  13, steps per second: 129, episode reward: 44.953, mean reward:  3.458 [-2.367, 33.563], mean action: 3.154 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1030/5000: episode: 47, duration: 0.456s, episode steps:  35, steps per second:  77, episode reward: 32.475, mean reward:  0.928 [-2.289, 32.510], mean action: 5.457 [0.000, 15.000],  loss: 0.021506, mae: 0.330590, mean_q: 0.520390, mean_eps: 0.000000
 1046/5000: episode: 48, duration: 0.239s, episode steps:  16, steps per second:  67, episode reward: 41.316, mean reward:  2.582 [-2.437, 32.290], mean action: 1.688 [0.000, 16.000],  loss: 0.023181, mae: 0.332851, mean_q: 0.561090, mean_eps: 0.000000
 1061/5000: episode: 49, duration: 0.238s, episode steps:  15, steps per second:  63, episode reward: 41.165, mean reward:  2.744 [-2.096, 32.520], mean action: 2.867 [0.000, 16.000],  loss: 0.020279, mae: 0.329369, mean_q: 0.564147, mean_eps: 0.000000
 1085/5000: episode: 50, duration: 0.515s, episode steps:  24, steps per second:  47, episode reward: 35.900, mean reward:  1.496 [-2.801, 32.370], mean action: 2.500 [0.000, 15.000],  loss: 0.021214, mae: 0.324993, mean_q: 0.517765, mean_eps: 0.000000
 1107/5000: episode: 51, duration: 0.335s, episode steps:  22, steps per second:  66, episode reward: -35.050, mean reward: -1.593 [-31.868,  2.657], mean action: 4.318 [0.000, 15.000],  loss: 0.027461, mae: 0.362817, mean_q: 0.561925, mean_eps: 0.000000
 1122/5000: episode: 52, duration: 0.228s, episode steps:  15, steps per second:  66, episode reward: 40.484, mean reward:  2.699 [-3.000, 31.864], mean action: 7.133 [0.000, 15.000],  loss: 0.023128, mae: 0.345632, mean_q: 0.576372, mean_eps: 0.000000
 1141/5000: episode: 53, duration: 0.276s, episode steps:  19, steps per second:  69, episode reward: 32.850, mean reward:  1.729 [-3.000, 32.100], mean action: 5.947 [0.000, 15.000],  loss: 0.024340, mae: 0.354237, mean_q: 0.552388, mean_eps: 0.000000
 1161/5000: episode: 54, duration: 0.291s, episode steps:  20, steps per second:  69, episode reward: 32.160, mean reward:  1.608 [-2.903, 32.728], mean action: 7.450 [0.000, 19.000],  loss: 0.016274, mae: 0.320670, mean_q: 0.475199, mean_eps: 0.000000
 1193/5000: episode: 55, duration: 0.457s, episode steps:  32, steps per second:  70, episode reward: 32.902, mean reward:  1.028 [-2.433, 32.632], mean action: 5.094 [0.000, 21.000],  loss: 0.020655, mae: 0.327168, mean_q: 0.577124, mean_eps: 0.000000
 1206/5000: episode: 56, duration: 0.201s, episode steps:  13, steps per second:  65, episode reward: 43.585, mean reward:  3.353 [-2.226, 32.507], mean action: 3.462 [0.000, 15.000],  loss: 0.020518, mae: 0.328699, mean_q: 0.569389, mean_eps: 0.000000
 1243/5000: episode: 57, duration: 0.535s, episode steps:  37, steps per second:  69, episode reward: 34.930, mean reward:  0.944 [-3.000, 32.480], mean action: 3.649 [0.000, 15.000],  loss: 0.018103, mae: 0.314996, mean_q: 0.539200, mean_eps: 0.000000
 1274/5000: episode: 58, duration: 0.442s, episode steps:  31, steps per second:  70, episode reward: 32.886, mean reward:  1.061 [-2.341, 32.886], mean action: 6.355 [0.000, 18.000],  loss: 0.024408, mae: 0.344092, mean_q: 0.541445, mean_eps: 0.000000
 1312/5000: episode: 59, duration: 0.540s, episode steps:  38, steps per second:  70, episode reward: 33.000, mean reward:  0.868 [-2.122, 33.000], mean action: 3.368 [0.000, 16.000],  loss: 0.021845, mae: 0.332227, mean_q: 0.571373, mean_eps: 0.000000
 1328/5000: episode: 60, duration: 0.239s, episode steps:  16, steps per second:  67, episode reward: 41.023, mean reward:  2.564 [-2.122, 31.614], mean action: 2.438 [0.000, 11.000],  loss: 0.026434, mae: 0.350083, mean_q: 0.559599, mean_eps: 0.000000
 1377/5000: episode: 61, duration: 0.701s, episode steps:  49, steps per second:  70, episode reward: -35.780, mean reward: -0.730 [-32.172,  2.721], mean action: 4.571 [0.000, 16.000],  loss: 0.022144, mae: 0.325266, mean_q: 0.527900, mean_eps: 0.000000
 1396/5000: episode: 62, duration: 0.286s, episode steps:  19, steps per second:  67, episode reward: 39.000, mean reward:  2.053 [-2.544, 32.700], mean action: 2.579 [0.000, 15.000],  loss: 0.017892, mae: 0.310038, mean_q: 0.510934, mean_eps: 0.000000
 1419/5000: episode: 63, duration: 0.329s, episode steps:  23, steps per second:  70, episode reward: -35.620, mean reward: -1.549 [-31.839,  2.240], mean action: 6.391 [0.000, 19.000],  loss: 0.017932, mae: 0.306040, mean_q: 0.483873, mean_eps: 0.000000
 1440/5000: episode: 64, duration: 0.298s, episode steps:  21, steps per second:  70, episode reward: -36.000, mean reward: -1.714 [-29.169,  2.169], mean action: 2.762 [0.000, 9.000],  loss: 0.020354, mae: 0.311149, mean_q: 0.514532, mean_eps: 0.000000
 1463/5000: episode: 65, duration: 0.349s, episode steps:  23, steps per second:  66, episode reward: 41.901, mean reward:  1.822 [-2.455, 32.701], mean action: 3.000 [2.000, 12.000],  loss: 0.020965, mae: 0.317864, mean_q: 0.555550, mean_eps: 0.000000
 1484/5000: episode: 66, duration: 0.311s, episode steps:  21, steps per second:  68, episode reward: 38.259, mean reward:  1.822 [-2.884, 32.190], mean action: 3.048 [0.000, 9.000],  loss: 0.020497, mae: 0.321245, mean_q: 0.513474, mean_eps: 0.000000
 1511/5000: episode: 67, duration: 0.536s, episode steps:  27, steps per second:  50, episode reward: -32.520, mean reward: -1.204 [-31.880,  2.611], mean action: 4.778 [0.000, 18.000],  loss: 0.021228, mae: 0.332493, mean_q: 0.454359, mean_eps: 0.000000
 1531/5000: episode: 68, duration: 0.293s, episode steps:  20, steps per second:  68, episode reward: 36.000, mean reward:  1.800 [-3.000, 32.680], mean action: 3.700 [0.000, 12.000],  loss: 0.017961, mae: 0.308276, mean_q: 0.481362, mean_eps: 0.000000
 1545/5000: episode: 69, duration: 0.230s, episode steps:  14, steps per second:  61, episode reward: 44.450, mean reward:  3.175 [-2.052, 32.103], mean action: 1.714 [1.000, 3.000],  loss: 0.015291, mae: 0.289607, mean_q: 0.539103, mean_eps: 0.000000
 1559/5000: episode: 70, duration: 0.221s, episode steps:  14, steps per second:  63, episode reward: 41.028, mean reward:  2.931 [-2.465, 29.964], mean action: 3.214 [0.000, 12.000],  loss: 0.021903, mae: 0.318422, mean_q: 0.505025, mean_eps: 0.000000
 1579/5000: episode: 71, duration: 0.299s, episode steps:  20, steps per second:  67, episode reward: 38.845, mean reward:  1.942 [-2.345, 32.590], mean action: 3.300 [0.000, 15.000],  loss: 0.019244, mae: 0.312603, mean_q: 0.485785, mean_eps: 0.000000
 1595/5000: episode: 72, duration: 0.259s, episode steps:  16, steps per second:  62, episode reward: 41.806, mean reward:  2.613 [-2.857, 33.000], mean action: 6.625 [1.000, 15.000],  loss: 0.021709, mae: 0.318048, mean_q: 0.511363, mean_eps: 0.000000
 1614/5000: episode: 73, duration: 0.282s, episode steps:  19, steps per second:  67, episode reward: 38.435, mean reward:  2.023 [-2.747, 32.334], mean action: 5.105 [0.000, 16.000],  loss: 0.019725, mae: 0.308067, mean_q: 0.523441, mean_eps: 0.000000
 1635/5000: episode: 74, duration: 0.324s, episode steps:  21, steps per second:  65, episode reward: 33.000, mean reward:  1.571 [-2.593, 29.240], mean action: 4.238 [0.000, 16.000],  loss: 0.020037, mae: 0.312564, mean_q: 0.544851, mean_eps: 0.000000
 1658/5000: episode: 75, duration: 0.384s, episode steps:  23, steps per second:  60, episode reward: -35.810, mean reward: -1.557 [-32.186,  2.428], mean action: 6.391 [0.000, 16.000],  loss: 0.021601, mae: 0.329001, mean_q: 0.533365, mean_eps: 0.000000
 1676/5000: episode: 76, duration: 0.289s, episode steps:  18, steps per second:  62, episode reward: 41.176, mean reward:  2.288 [-2.175, 32.100], mean action: 2.444 [0.000, 16.000],  loss: 0.023871, mae: 0.331253, mean_q: 0.527544, mean_eps: 0.000000
 1702/5000: episode: 77, duration: 0.403s, episode steps:  26, steps per second:  65, episode reward: 33.000, mean reward:  1.269 [-3.000, 30.112], mean action: 3.654 [0.000, 16.000],  loss: 0.021600, mae: 0.324650, mean_q: 0.566760, mean_eps: 0.000000
 1728/5000: episode: 78, duration: 0.382s, episode steps:  26, steps per second:  68, episode reward: 32.354, mean reward:  1.244 [-3.000, 31.438], mean action: 4.462 [0.000, 16.000],  loss: 0.020301, mae: 0.325336, mean_q: 0.606147, mean_eps: 0.000000
 1754/5000: episode: 79, duration: 0.464s, episode steps:  26, steps per second:  56, episode reward: -33.000, mean reward: -1.269 [-32.021,  3.000], mean action: 6.692 [0.000, 20.000],  loss: 0.023394, mae: 0.340413, mean_q: 0.561534, mean_eps: 0.000000
 1776/5000: episode: 80, duration: 0.341s, episode steps:  22, steps per second:  64, episode reward: 38.696, mean reward:  1.759 [-2.113, 32.198], mean action: 2.955 [0.000, 14.000],  loss: 0.019506, mae: 0.323625, mean_q: 0.479561, mean_eps: 0.000000
 1791/5000: episode: 81, duration: 0.235s, episode steps:  15, steps per second:  64, episode reward: 41.183, mean reward:  2.746 [-2.148, 32.320], mean action: 1.867 [0.000, 3.000],  loss: 0.021703, mae: 0.329688, mean_q: 0.486954, mean_eps: 0.000000
 1816/5000: episode: 82, duration: 0.362s, episode steps:  25, steps per second:  69, episode reward: 32.663, mean reward:  1.307 [-3.000, 32.523], mean action: 4.800 [0.000, 19.000],  loss: 0.017863, mae: 0.310908, mean_q: 0.499394, mean_eps: 0.000000
 1840/5000: episode: 83, duration: 0.354s, episode steps:  24, steps per second:  68, episode reward: -32.590, mean reward: -1.358 [-32.071,  3.005], mean action: 4.625 [0.000, 19.000],  loss: 0.022477, mae: 0.331882, mean_q: 0.520439, mean_eps: 0.000000
 1860/5000: episode: 84, duration: 0.289s, episode steps:  20, steps per second:  69, episode reward: 35.904, mean reward:  1.795 [-2.465, 32.184], mean action: 2.750 [0.000, 9.000],  loss: 0.017783, mae: 0.306831, mean_q: 0.558261, mean_eps: 0.000000
 1884/5000: episode: 85, duration: 0.351s, episode steps:  24, steps per second:  68, episode reward: 32.065, mean reward:  1.336 [-3.000, 32.020], mean action: 3.458 [0.000, 9.000],  loss: 0.021015, mae: 0.318661, mean_q: 0.585348, mean_eps: 0.000000
 1903/5000: episode: 86, duration: 0.283s, episode steps:  19, steps per second:  67, episode reward: 35.216, mean reward:  1.853 [-3.000, 33.000], mean action: 2.947 [0.000, 12.000],  loss: 0.017066, mae: 0.301778, mean_q: 0.575276, mean_eps: 0.000000
 1932/5000: episode: 87, duration: 0.418s, episode steps:  29, steps per second:  69, episode reward: 38.568, mean reward:  1.330 [-2.530, 32.862], mean action: 3.586 [0.000, 20.000],  loss: 0.019440, mae: 0.314722, mean_q: 0.550807, mean_eps: 0.000000
 1952/5000: episode: 88, duration: 0.345s, episode steps:  20, steps per second:  58, episode reward: -35.850, mean reward: -1.792 [-32.008,  2.450], mean action: 4.650 [0.000, 14.000],  loss: 0.022401, mae: 0.339096, mean_q: 0.523611, mean_eps: 0.000000
 1973/5000: episode: 89, duration: 0.468s, episode steps:  21, steps per second:  45, episode reward: 38.581, mean reward:  1.837 [-2.354, 32.491], mean action: 4.048 [0.000, 13.000],  loss: 0.023110, mae: 0.338874, mean_q: 0.503137, mean_eps: 0.000000
 1992/5000: episode: 90, duration: 0.289s, episode steps:  19, steps per second:  66, episode reward: 38.337, mean reward:  2.018 [-2.841, 32.001], mean action: 4.684 [1.000, 14.000],  loss: 0.022100, mae: 0.325972, mean_q: 0.529124, mean_eps: 0.000000
 2009/5000: episode: 91, duration: 0.258s, episode steps:  17, steps per second:  66, episode reward: 41.669, mean reward:  2.451 [-2.189, 32.032], mean action: 4.588 [0.000, 20.000],  loss: 0.021922, mae: 0.323722, mean_q: 0.543334, mean_eps: 0.000000
 2025/5000: episode: 92, duration: 0.242s, episode steps:  16, steps per second:  66, episode reward: 44.487, mean reward:  2.780 [-2.388, 32.200], mean action: 1.875 [0.000, 9.000],  loss: 0.019878, mae: 0.316168, mean_q: 0.498662, mean_eps: 0.000000
 2069/5000: episode: 93, duration: 0.616s, episode steps:  44, steps per second:  71, episode reward: 32.534, mean reward:  0.739 [-2.404, 32.110], mean action: 3.455 [0.000, 15.000],  loss: 0.020608, mae: 0.317241, mean_q: 0.475697, mean_eps: 0.000000
 2090/5000: episode: 94, duration: 0.322s, episode steps:  21, steps per second:  65, episode reward: 37.527, mean reward:  1.787 [-2.441, 32.387], mean action: 6.905 [0.000, 20.000],  loss: 0.025092, mae: 0.339748, mean_q: 0.512706, mean_eps: 0.000000
 2116/5000: episode: 95, duration: 0.375s, episode steps:  26, steps per second:  69, episode reward: 35.325, mean reward:  1.359 [-2.557, 32.235], mean action: 4.231 [0.000, 14.000],  loss: 0.021526, mae: 0.324036, mean_q: 0.564805, mean_eps: 0.000000
 2137/5000: episode: 96, duration: 0.330s, episode steps:  21, steps per second:  64, episode reward: 38.355, mean reward:  1.826 [-2.379, 32.540], mean action: 4.429 [0.000, 20.000],  loss: 0.023833, mae: 0.332983, mean_q: 0.531698, mean_eps: 0.000000
 2158/5000: episode: 97, duration: 0.321s, episode steps:  21, steps per second:  65, episode reward: 33.000, mean reward:  1.571 [-2.330, 29.736], mean action: 4.333 [0.000, 14.000],  loss: 0.018010, mae: 0.305276, mean_q: 0.526242, mean_eps: 0.000000
 2185/5000: episode: 98, duration: 0.389s, episode steps:  27, steps per second:  69, episode reward: 33.000, mean reward:  1.222 [-2.390, 32.410], mean action: 5.815 [0.000, 20.000],  loss: 0.024449, mae: 0.335064, mean_q: 0.511510, mean_eps: 0.000000
 2202/5000: episode: 99, duration: 0.249s, episode steps:  17, steps per second:  68, episode reward: 41.062, mean reward:  2.415 [-2.381, 32.340], mean action: 1.059 [0.000, 3.000],  loss: 0.022419, mae: 0.323056, mean_q: 0.541089, mean_eps: 0.000000
 2224/5000: episode: 100, duration: 0.327s, episode steps:  22, steps per second:  67, episode reward: 42.000, mean reward:  1.909 [-2.434, 32.520], mean action: 2.227 [1.000, 3.000],  loss: 0.021992, mae: 0.320573, mean_q: 0.565268, mean_eps: 0.000000
 2244/5000: episode: 101, duration: 0.292s, episode steps:  20, steps per second:  69, episode reward: 38.132, mean reward:  1.907 [-3.000, 32.140], mean action: 3.500 [0.000, 16.000],  loss: 0.018893, mae: 0.307542, mean_q: 0.552705, mean_eps: 0.000000
 2261/5000: episode: 102, duration: 0.256s, episode steps:  17, steps per second:  66, episode reward: 38.345, mean reward:  2.256 [-3.000, 32.153], mean action: 6.647 [1.000, 20.000],  loss: 0.018104, mae: 0.299361, mean_q: 0.505494, mean_eps: 0.000000
 2278/5000: episode: 103, duration: 0.331s, episode steps:  17, steps per second:  51, episode reward: 38.980, mean reward:  2.293 [-2.409, 32.370], mean action: 3.529 [0.000, 20.000],  loss: 0.018997, mae: 0.309123, mean_q: 0.554624, mean_eps: 0.000000
 2295/5000: episode: 104, duration: 0.252s, episode steps:  17, steps per second:  67, episode reward: 39.000, mean reward:  2.294 [-2.437, 32.910], mean action: 3.588 [0.000, 11.000],  loss: 0.019599, mae: 0.310645, mean_q: 0.550237, mean_eps: 0.000000
 2351/5000: episode: 105, duration: 0.749s, episode steps:  56, steps per second:  75, episode reward: 33.000, mean reward:  0.589 [-3.000, 32.070], mean action: 8.321 [0.000, 15.000],  loss: 0.023043, mae: 0.327772, mean_q: 0.552767, mean_eps: 0.000000
 2372/5000: episode: 106, duration: 0.316s, episode steps:  21, steps per second:  67, episode reward: 38.477, mean reward:  1.832 [-2.439, 32.190], mean action: 4.905 [0.000, 11.000],  loss: 0.021295, mae: 0.320277, mean_q: 0.563785, mean_eps: 0.000000
 2386/5000: episode: 107, duration: 0.210s, episode steps:  14, steps per second:  67, episode reward: 44.522, mean reward:  3.180 [-2.656, 32.904], mean action: 5.000 [0.000, 16.000],  loss: 0.019774, mae: 0.313622, mean_q: 0.546412, mean_eps: 0.000000
 2413/5000: episode: 108, duration: 0.400s, episode steps:  27, steps per second:  67, episode reward: 33.000, mean reward:  1.222 [-3.000, 32.150], mean action: 5.704 [0.000, 19.000],  loss: 0.023419, mae: 0.332014, mean_q: 0.556554, mean_eps: 0.000000
 2437/5000: episode: 109, duration: 0.365s, episode steps:  24, steps per second:  66, episode reward: -35.440, mean reward: -1.477 [-32.219,  2.479], mean action: 5.875 [0.000, 19.000],  loss: 0.020165, mae: 0.323884, mean_q: 0.537764, mean_eps: 0.000000
 2457/5000: episode: 110, duration: 0.335s, episode steps:  20, steps per second:  60, episode reward: 35.279, mean reward:  1.764 [-2.498, 32.019], mean action: 5.250 [0.000, 19.000],  loss: 0.023958, mae: 0.334344, mean_q: 0.505479, mean_eps: 0.000000
 2474/5000: episode: 111, duration: 0.253s, episode steps:  17, steps per second:  67, episode reward: 44.020, mean reward:  2.589 [-2.067, 32.813], mean action: 4.059 [0.000, 19.000],  loss: 0.021892, mae: 0.327921, mean_q: 0.502245, mean_eps: 0.000000
 2509/5000: episode: 112, duration: 0.492s, episode steps:  35, steps per second:  71, episode reward: 42.000, mean reward:  1.200 [-3.000, 32.320], mean action: 3.286 [0.000, 19.000],  loss: 0.023164, mae: 0.331889, mean_q: 0.583370, mean_eps: 0.000000
 2538/5000: episode: 113, duration: 0.418s, episode steps:  29, steps per second:  69, episode reward: -32.280, mean reward: -1.113 [-31.824,  2.600], mean action: 4.345 [0.000, 19.000],  loss: 0.021996, mae: 0.329996, mean_q: 0.613074, mean_eps: 0.000000
 2561/5000: episode: 114, duration: 0.329s, episode steps:  23, steps per second:  70, episode reward: 32.477, mean reward:  1.412 [-2.595, 31.507], mean action: 5.000 [0.000, 19.000],  loss: 0.020812, mae: 0.324802, mean_q: 0.553526, mean_eps: 0.000000
 2579/5000: episode: 115, duration: 0.271s, episode steps:  18, steps per second:  67, episode reward: 38.073, mean reward:  2.115 [-2.905, 32.143], mean action: 4.167 [0.000, 19.000],  loss: 0.021402, mae: 0.321329, mean_q: 0.568567, mean_eps: 0.000000
 2594/5000: episode: 116, duration: 0.256s, episode steps:  15, steps per second:  59, episode reward: 40.257, mean reward:  2.684 [-2.073, 32.199], mean action: 6.733 [0.000, 19.000],  loss: 0.025438, mae: 0.337308, mean_q: 0.537709, mean_eps: 0.000000
 2619/5000: episode: 117, duration: 0.361s, episode steps:  25, steps per second:  69, episode reward: 33.000, mean reward:  1.320 [-2.689, 32.490], mean action: 2.440 [0.000, 9.000],  loss: 0.017437, mae: 0.308371, mean_q: 0.533280, mean_eps: 0.000000
 2641/5000: episode: 118, duration: 0.327s, episode steps:  22, steps per second:  67, episode reward: 38.132, mean reward:  1.733 [-2.267, 32.070], mean action: 5.364 [0.000, 19.000],  loss: 0.020844, mae: 0.323895, mean_q: 0.487426, mean_eps: 0.000000
 2668/5000: episode: 119, duration: 0.390s, episode steps:  27, steps per second:  69, episode reward: 35.621, mean reward:  1.319 [-2.367, 32.709], mean action: 4.852 [0.000, 15.000],  loss: 0.018549, mae: 0.310694, mean_q: 0.466960, mean_eps: 0.000000
 2705/5000: episode: 120, duration: 0.528s, episode steps:  37, steps per second:  70, episode reward: 41.425, mean reward:  1.120 [-2.439, 32.137], mean action: 3.541 [1.000, 15.000],  loss: 0.021253, mae: 0.319108, mean_q: 0.436955, mean_eps: 0.000000
 2731/5000: episode: 121, duration: 0.375s, episode steps:  26, steps per second:  69, episode reward: 33.000, mean reward:  1.269 [-2.539, 32.080], mean action: 4.115 [0.000, 16.000],  loss: 0.019287, mae: 0.312828, mean_q: 0.460628, mean_eps: 0.000000
 2758/5000: episode: 122, duration: 0.382s, episode steps:  27, steps per second:  71, episode reward: 34.805, mean reward:  1.289 [-3.000, 32.123], mean action: 3.852 [0.000, 16.000],  loss: 0.023164, mae: 0.334670, mean_q: 0.539442, mean_eps: 0.000000
 2774/5000: episode: 123, duration: 0.239s, episode steps:  16, steps per second:  67, episode reward: 40.833, mean reward:  2.552 [-3.000, 32.073], mean action: 3.562 [0.000, 19.000],  loss: 0.019556, mae: 0.321508, mean_q: 0.538399, mean_eps: 0.000000
 2796/5000: episode: 124, duration: 0.327s, episode steps:  22, steps per second:  67, episode reward: 35.695, mean reward:  1.623 [-2.341, 31.705], mean action: 4.273 [0.000, 19.000],  loss: 0.021072, mae: 0.324451, mean_q: 0.539718, mean_eps: 0.000000
 2817/5000: episode: 125, duration: 0.341s, episode steps:  21, steps per second:  62, episode reward: 37.648, mean reward:  1.793 [-3.000, 32.756], mean action: 4.286 [0.000, 19.000],  loss: 0.019214, mae: 0.315710, mean_q: 0.510940, mean_eps: 0.000000
 2852/5000: episode: 126, duration: 0.647s, episode steps:  35, steps per second:  54, episode reward: 33.000, mean reward:  0.943 [-2.488, 32.040], mean action: 4.286 [0.000, 15.000],  loss: 0.018012, mae: 0.309231, mean_q: 0.550860, mean_eps: 0.000000
 2876/5000: episode: 127, duration: 0.400s, episode steps:  24, steps per second:  60, episode reward: 32.862, mean reward:  1.369 [-2.290, 32.672], mean action: 3.833 [0.000, 15.000],  loss: 0.017151, mae: 0.310181, mean_q: 0.552601, mean_eps: 0.000000
 2894/5000: episode: 128, duration: 0.269s, episode steps:  18, steps per second:  67, episode reward: 41.001, mean reward:  2.278 [-2.440, 33.000], mean action: 5.500 [0.000, 19.000],  loss: 0.022575, mae: 0.337690, mean_q: 0.567599, mean_eps: 0.000000
 2927/5000: episode: 129, duration: 0.466s, episode steps:  33, steps per second:  71, episode reward: -32.070, mean reward: -0.972 [-32.792,  2.491], mean action: 6.848 [0.000, 19.000],  loss: 0.021075, mae: 0.332309, mean_q: 0.606524, mean_eps: 0.000000
 2943/5000: episode: 130, duration: 0.247s, episode steps:  16, steps per second:  65, episode reward: 44.022, mean reward:  2.751 [-0.733, 30.193], mean action: 3.312 [2.000, 5.000],  loss: 0.022114, mae: 0.339079, mean_q: 0.601873, mean_eps: 0.000000
 2969/5000: episode: 131, duration: 0.377s, episode steps:  26, steps per second:  69, episode reward: 35.004, mean reward:  1.346 [-3.000, 32.005], mean action: 3.731 [0.000, 19.000],  loss: 0.019116, mae: 0.328646, mean_q: 0.557998, mean_eps: 0.000000
 2997/5000: episode: 132, duration: 0.397s, episode steps:  28, steps per second:  70, episode reward: 32.169, mean reward:  1.149 [-3.000, 32.019], mean action: 5.786 [0.000, 19.000],  loss: 0.026527, mae: 0.355145, mean_q: 0.570631, mean_eps: 0.000000
 3011/5000: episode: 133, duration: 0.210s, episode steps:  14, steps per second:  67, episode reward: 41.118, mean reward:  2.937 [-2.266, 32.632], mean action: 5.500 [1.000, 19.000],  loss: 0.027715, mae: 0.367439, mean_q: 0.547391, mean_eps: 0.000000
 3030/5000: episode: 134, duration: 0.294s, episode steps:  19, steps per second:  65, episode reward: 35.389, mean reward:  1.863 [-3.000, 33.000], mean action: 4.421 [0.000, 19.000],  loss: 0.019631, mae: 0.319188, mean_q: 0.530711, mean_eps: 0.000000
 3050/5000: episode: 135, duration: 0.286s, episode steps:  20, steps per second:  70, episode reward: 38.328, mean reward:  1.916 [-2.454, 33.000], mean action: 2.950 [0.000, 12.000],  loss: 0.018616, mae: 0.316496, mean_q: 0.514511, mean_eps: 0.000000
 3074/5000: episode: 136, duration: 0.351s, episode steps:  24, steps per second:  68, episode reward: -35.170, mean reward: -1.465 [-32.132,  2.498], mean action: 5.083 [1.000, 16.000],  loss: 0.017809, mae: 0.309296, mean_q: 0.542095, mean_eps: 0.000000
 3095/5000: episode: 137, duration: 0.321s, episode steps:  21, steps per second:  65, episode reward: -35.300, mean reward: -1.681 [-32.447,  3.000], mean action: 3.381 [0.000, 12.000],  loss: 0.018008, mae: 0.311852, mean_q: 0.459089, mean_eps: 0.000000
 3119/5000: episode: 138, duration: 0.338s, episode steps:  24, steps per second:  71, episode reward: -35.250, mean reward: -1.469 [-31.366,  2.830], mean action: 7.875 [0.000, 14.000],  loss: 0.024861, mae: 0.339556, mean_q: 0.520040, mean_eps: 0.000000
 3138/5000: episode: 139, duration: 0.284s, episode steps:  19, steps per second:  67, episode reward: 35.229, mean reward:  1.854 [-2.371, 32.480], mean action: 4.684 [0.000, 19.000],  loss: 0.020367, mae: 0.320017, mean_q: 0.571869, mean_eps: 0.000000
 3155/5000: episode: 140, duration: 0.254s, episode steps:  17, steps per second:  67, episode reward: 35.516, mean reward:  2.089 [-3.000, 32.116], mean action: 4.765 [0.000, 19.000],  loss: 0.022526, mae: 0.333910, mean_q: 0.573896, mean_eps: 0.000000
 3170/5000: episode: 141, duration: 0.229s, episode steps:  15, steps per second:  65, episode reward: 38.900, mean reward:  2.593 [-2.485, 32.070], mean action: 4.667 [0.000, 19.000],  loss: 0.016227, mae: 0.300335, mean_q: 0.517340, mean_eps: 0.000000
 3192/5000: episode: 142, duration: 0.412s, episode steps:  22, steps per second:  53, episode reward: 32.008, mean reward:  1.455 [-3.000, 32.160], mean action: 4.636 [0.000, 12.000],  loss: 0.019353, mae: 0.317089, mean_q: 0.470667, mean_eps: 0.000000
 3211/5000: episode: 143, duration: 0.318s, episode steps:  19, steps per second:  60, episode reward: 41.739, mean reward:  2.197 [-2.266, 32.399], mean action: 2.158 [0.000, 11.000],  loss: 0.024887, mae: 0.351657, mean_q: 0.431971, mean_eps: 0.000000
 3234/5000: episode: 144, duration: 0.361s, episode steps:  23, steps per second:  64, episode reward: 36.000, mean reward:  1.565 [-3.000, 32.040], mean action: 4.565 [0.000, 13.000],  loss: 0.017874, mae: 0.308456, mean_q: 0.482802, mean_eps: 0.000000
 3260/5000: episode: 145, duration: 0.403s, episode steps:  26, steps per second:  64, episode reward: 41.568, mean reward:  1.599 [-2.128, 31.898], mean action: 2.808 [1.000, 9.000],  loss: 0.018038, mae: 0.307319, mean_q: 0.530948, mean_eps: 0.000000
 3287/5000: episode: 146, duration: 0.399s, episode steps:  27, steps per second:  68, episode reward: -39.000, mean reward: -1.444 [-32.065,  2.340], mean action: 5.815 [0.000, 12.000],  loss: 0.022673, mae: 0.329015, mean_q: 0.499089, mean_eps: 0.000000
 3316/5000: episode: 147, duration: 0.481s, episode steps:  29, steps per second:  60, episode reward: 32.407, mean reward:  1.117 [-3.000, 32.383], mean action: 3.069 [0.000, 12.000],  loss: 0.020806, mae: 0.317886, mean_q: 0.532317, mean_eps: 0.000000
 3331/5000: episode: 148, duration: 0.238s, episode steps:  15, steps per second:  63, episode reward: 38.282, mean reward:  2.552 [-2.901, 33.000], mean action: 4.000 [0.000, 19.000],  loss: 0.024231, mae: 0.328402, mean_q: 0.527963, mean_eps: 0.000000
 3389/5000: episode: 149, duration: 0.852s, episode steps:  58, steps per second:  68, episode reward: 32.737, mean reward:  0.564 [-2.368, 32.370], mean action: 8.431 [0.000, 20.000],  loss: 0.021537, mae: 0.320309, mean_q: 0.567510, mean_eps: 0.000000
 3409/5000: episode: 150, duration: 0.301s, episode steps:  20, steps per second:  66, episode reward: 35.619, mean reward:  1.781 [-3.000, 32.429], mean action: 5.300 [0.000, 19.000],  loss: 0.022767, mae: 0.323420, mean_q: 0.584057, mean_eps: 0.000000
 3434/5000: episode: 151, duration: 0.394s, episode steps:  25, steps per second:  63, episode reward: 38.348, mean reward:  1.534 [-3.000, 32.340], mean action: 4.040 [0.000, 15.000],  loss: 0.018471, mae: 0.309342, mean_q: 0.545826, mean_eps: 0.000000
 3455/5000: episode: 152, duration: 0.303s, episode steps:  21, steps per second:  69, episode reward: 44.713, mean reward:  2.129 [-2.311, 32.080], mean action: 2.524 [0.000, 16.000],  loss: 0.020299, mae: 0.326439, mean_q: 0.569714, mean_eps: 0.000000
 3473/5000: episode: 153, duration: 0.266s, episode steps:  18, steps per second:  68, episode reward: 41.782, mean reward:  2.321 [-2.285, 32.090], mean action: 5.278 [0.000, 16.000],  loss: 0.021248, mae: 0.330515, mean_q: 0.588049, mean_eps: 0.000000
 3495/5000: episode: 154, duration: 0.317s, episode steps:  22, steps per second:  69, episode reward: 32.802, mean reward:  1.491 [-3.000, 32.901], mean action: 4.773 [0.000, 16.000],  loss: 0.021949, mae: 0.328434, mean_q: 0.586316, mean_eps: 0.000000
 3517/5000: episode: 155, duration: 0.319s, episode steps:  22, steps per second:  69, episode reward: 33.000, mean reward:  1.500 [-3.000, 32.030], mean action: 7.955 [0.000, 17.000],  loss: 0.018140, mae: 0.314901, mean_q: 0.526698, mean_eps: 0.000000
 3546/5000: episode: 156, duration: 0.428s, episode steps:  29, steps per second:  68, episode reward: -35.490, mean reward: -1.224 [-32.209,  2.383], mean action: 5.414 [0.000, 16.000],  loss: 0.021182, mae: 0.326096, mean_q: 0.541207, mean_eps: 0.000000
 3581/5000: episode: 157, duration: 0.486s, episode steps:  35, steps per second:  72, episode reward: -38.270, mean reward: -1.093 [-31.932,  2.580], mean action: 3.457 [0.000, 16.000],  loss: 0.020055, mae: 0.322779, mean_q: 0.485405, mean_eps: 0.000000
 3608/5000: episode: 158, duration: 0.386s, episode steps:  27, steps per second:  70, episode reward: 41.668, mean reward:  1.543 [-3.000, 31.903], mean action: 1.444 [0.000, 14.000],  loss: 0.020776, mae: 0.320764, mean_q: 0.535440, mean_eps: 0.000000
 3629/5000: episode: 159, duration: 0.315s, episode steps:  21, steps per second:  67, episode reward: 35.653, mean reward:  1.698 [-2.901, 32.398], mean action: 3.762 [0.000, 16.000],  loss: 0.021857, mae: 0.319387, mean_q: 0.589147, mean_eps: 0.000000
 3667/5000: episode: 160, duration: 0.536s, episode steps:  38, steps per second:  71, episode reward: 35.559, mean reward:  0.936 [-2.903, 31.904], mean action: 3.553 [0.000, 16.000],  loss: 0.021341, mae: 0.320263, mean_q: 0.561094, mean_eps: 0.000000
 3692/5000: episode: 161, duration: 0.369s, episode steps:  25, steps per second:  68, episode reward: 35.850, mean reward:  1.434 [-2.276, 32.990], mean action: 4.360 [0.000, 13.000],  loss: 0.016096, mae: 0.302196, mean_q: 0.504411, mean_eps: 0.000000
 3706/5000: episode: 162, duration: 0.212s, episode steps:  14, steps per second:  66, episode reward: 41.663, mean reward:  2.976 [-2.347, 32.663], mean action: 3.143 [0.000, 12.000],  loss: 0.018755, mae: 0.316875, mean_q: 0.536653, mean_eps: 0.000000
 3718/5000: episode: 163, duration: 0.186s, episode steps:  12, steps per second:  64, episode reward: 45.000, mean reward:  3.750 [-2.060, 32.060], mean action: 5.333 [0.000, 15.000],  loss: 0.022259, mae: 0.321706, mean_q: 0.522994, mean_eps: 0.000000
 3738/5000: episode: 164, duration: 0.292s, episode steps:  20, steps per second:  69, episode reward: -36.000, mean reward: -1.800 [-32.067,  2.170], mean action: 4.000 [0.000, 12.000],  loss: 0.022792, mae: 0.329059, mean_q: 0.524404, mean_eps: 0.000000
 3763/5000: episode: 165, duration: 0.367s, episode steps:  25, steps per second:  68, episode reward: 38.025, mean reward:  1.521 [-2.595, 32.200], mean action: 4.800 [0.000, 14.000],  loss: 0.020011, mae: 0.315824, mean_q: 0.570666, mean_eps: 0.000000
 3796/5000: episode: 166, duration: 0.470s, episode steps:  33, steps per second:  70, episode reward: -32.540, mean reward: -0.986 [-33.000,  2.636], mean action: 6.879 [1.000, 20.000],  loss: 0.020812, mae: 0.330081, mean_q: 0.554642, mean_eps: 0.000000
 3811/5000: episode: 167, duration: 0.226s, episode steps:  15, steps per second:  66, episode reward: 38.752, mean reward:  2.583 [-3.000, 32.752], mean action: 6.267 [0.000, 16.000],  loss: 0.023617, mae: 0.352881, mean_q: 0.528179, mean_eps: 0.000000
 3828/5000: episode: 168, duration: 0.273s, episode steps:  17, steps per second:  62, episode reward: 41.267, mean reward:  2.427 [-2.259, 32.267], mean action: 2.059 [0.000, 9.000],  loss: 0.022026, mae: 0.333499, mean_q: 0.527584, mean_eps: 0.000000
 3849/5000: episode: 169, duration: 0.309s, episode steps:  21, steps per second:  68, episode reward: 37.771, mean reward:  1.799 [-2.508, 31.781], mean action: 4.667 [0.000, 19.000],  loss: 0.018292, mae: 0.315420, mean_q: 0.538530, mean_eps: 0.000000
 3871/5000: episode: 170, duration: 0.319s, episode steps:  22, steps per second:  69, episode reward: 30.000, mean reward:  1.364 [-3.000, 30.984], mean action: 6.636 [0.000, 16.000],  loss: 0.020043, mae: 0.327093, mean_q: 0.477643, mean_eps: 0.000000
 3893/5000: episode: 171, duration: 0.318s, episode steps:  22, steps per second:  69, episode reward: -36.000, mean reward: -1.636 [-30.115,  2.070], mean action: 6.682 [0.000, 16.000],  loss: 0.020444, mae: 0.326508, mean_q: 0.463718, mean_eps: 0.000000
 3921/5000: episode: 172, duration: 0.396s, episode steps:  28, steps per second:  71, episode reward: 38.706, mean reward:  1.382 [-2.207, 32.902], mean action: 3.464 [0.000, 15.000],  loss: 0.022314, mae: 0.335014, mean_q: 0.523480, mean_eps: 0.000000
 3947/5000: episode: 173, duration: 0.375s, episode steps:  26, steps per second:  69, episode reward: -35.360, mean reward: -1.360 [-32.017,  2.510], mean action: 5.000 [0.000, 19.000],  loss: 0.020359, mae: 0.328894, mean_q: 0.497625, mean_eps: 0.000000
 3963/5000: episode: 174, duration: 0.247s, episode steps:  16, steps per second:  65, episode reward: 42.000, mean reward:  2.625 [-2.356, 32.020], mean action: 4.438 [1.000, 19.000],  loss: 0.022246, mae: 0.331840, mean_q: 0.495796, mean_eps: 0.000000
 3991/5000: episode: 175, duration: 0.401s, episode steps:  28, steps per second:  70, episode reward: -38.130, mean reward: -1.362 [-32.157,  2.140], mean action: 6.786 [0.000, 19.000],  loss: 0.020483, mae: 0.319441, mean_q: 0.515170, mean_eps: 0.000000
 4012/5000: episode: 176, duration: 0.307s, episode steps:  21, steps per second:  68, episode reward: 42.000, mean reward:  2.000 [-2.320, 32.260], mean action: 5.000 [0.000, 19.000],  loss: 0.021012, mae: 0.322772, mean_q: 0.584728, mean_eps: 0.000000
 4030/5000: episode: 177, duration: 0.286s, episode steps:  18, steps per second:  63, episode reward: 43.667, mean reward:  2.426 [-2.458, 32.021], mean action: 2.333 [0.000, 19.000],  loss: 0.023544, mae: 0.336646, mean_q: 0.523885, mean_eps: 0.000000
 4051/5000: episode: 178, duration: 0.305s, episode steps:  21, steps per second:  69, episode reward: -36.000, mean reward: -1.714 [-33.000,  2.134], mean action: 4.714 [0.000, 19.000],  loss: 0.022856, mae: 0.339883, mean_q: 0.526676, mean_eps: 0.000000
 4078/5000: episode: 179, duration: 0.379s, episode steps:  27, steps per second:  71, episode reward: -41.170, mean reward: -1.525 [-32.162,  2.550], mean action: 11.074 [0.000, 20.000],  loss: 0.023166, mae: 0.336543, mean_q: 0.500058, mean_eps: 0.000000
 4100/5000: episode: 180, duration: 0.330s, episode steps:  22, steps per second:  67, episode reward: 34.619, mean reward:  1.574 [-3.000, 31.881], mean action: 6.182 [0.000, 19.000],  loss: 0.020511, mae: 0.322897, mean_q: 0.550765, mean_eps: 0.000000
 4125/5000: episode: 181, duration: 0.364s, episode steps:  25, steps per second:  69, episode reward: -32.920, mean reward: -1.317 [-32.167,  2.190], mean action: 4.560 [0.000, 16.000],  loss: 0.019548, mae: 0.307355, mean_q: 0.556247, mean_eps: 0.000000
 4145/5000: episode: 182, duration: 0.290s, episode steps:  20, steps per second:  69, episode reward: 38.037, mean reward:  1.902 [-3.000, 31.927], mean action: 2.400 [0.000, 16.000],  loss: 0.017646, mae: 0.294865, mean_q: 0.532296, mean_eps: 0.000000
 4162/5000: episode: 183, duration: 0.259s, episode steps:  17, steps per second:  66, episode reward: 43.787, mean reward:  2.576 [-3.000, 32.848], mean action: 2.588 [0.000, 12.000],  loss: 0.023268, mae: 0.322266, mean_q: 0.536559, mean_eps: 0.000000
 4195/5000: episode: 184, duration: 0.470s, episode steps:  33, steps per second:  70, episode reward: 32.338, mean reward:  0.980 [-2.909, 33.000], mean action: 4.727 [0.000, 16.000],  loss: 0.020876, mae: 0.311280, mean_q: 0.484880, mean_eps: 0.000000
 4217/5000: episode: 185, duration: 0.320s, episode steps:  22, steps per second:  69, episode reward: 32.698, mean reward:  1.486 [-3.000, 32.340], mean action: 6.273 [0.000, 16.000],  loss: 0.025857, mae: 0.333816, mean_q: 0.520116, mean_eps: 0.000000
 4237/5000: episode: 186, duration: 0.299s, episode steps:  20, steps per second:  67, episode reward: 41.902, mean reward:  2.095 [-2.360, 32.242], mean action: 1.700 [0.000, 16.000],  loss: 0.022894, mae: 0.323912, mean_q: 0.544356, mean_eps: 0.000000
 4263/5000: episode: 187, duration: 0.378s, episode steps:  26, steps per second:  69, episode reward: 33.000, mean reward:  1.269 [-2.426, 30.438], mean action: 5.731 [0.000, 20.000],  loss: 0.023366, mae: 0.323523, mean_q: 0.506562, mean_eps: 0.000000
 4294/5000: episode: 188, duration: 0.437s, episode steps:  31, steps per second:  71, episode reward: 41.683, mean reward:  1.345 [-2.462, 32.035], mean action: 3.161 [0.000, 19.000],  loss: 0.023384, mae: 0.320355, mean_q: 0.536314, mean_eps: 0.000000
 4316/5000: episode: 189, duration: 0.324s, episode steps:  22, steps per second:  68, episode reward: 35.369, mean reward:  1.608 [-2.912, 32.690], mean action: 6.000 [0.000, 15.000],  loss: 0.020747, mae: 0.310237, mean_q: 0.488202, mean_eps: 0.000000
 4334/5000: episode: 190, duration: 0.264s, episode steps:  18, steps per second:  68, episode reward: 38.803, mean reward:  2.156 [-3.000, 32.030], mean action: 4.222 [0.000, 19.000],  loss: 0.022364, mae: 0.316400, mean_q: 0.513192, mean_eps: 0.000000
 4350/5000: episode: 191, duration: 0.237s, episode steps:  16, steps per second:  67, episode reward: 38.288, mean reward:  2.393 [-2.727, 32.151], mean action: 5.250 [0.000, 19.000],  loss: 0.019284, mae: 0.306836, mean_q: 0.559534, mean_eps: 0.000000
 4370/5000: episode: 192, duration: 0.301s, episode steps:  20, steps per second:  67, episode reward: 44.350, mean reward:  2.217 [-2.319, 33.000], mean action: 5.150 [0.000, 19.000],  loss: 0.016406, mae: 0.291748, mean_q: 0.575256, mean_eps: 0.000000
 4425/5000: episode: 193, duration: 0.963s, episode steps:  55, steps per second:  57, episode reward: -32.640, mean reward: -0.593 [-32.113,  3.000], mean action: 6.036 [0.000, 20.000],  loss: 0.018606, mae: 0.311562, mean_q: 0.496143, mean_eps: 0.000000
 4453/5000: episode: 194, duration: 0.410s, episode steps:  28, steps per second:  68, episode reward: 38.081, mean reward:  1.360 [-3.000, 32.069], mean action: 2.643 [0.000, 15.000],  loss: 0.019649, mae: 0.313542, mean_q: 0.472754, mean_eps: 0.000000
 4487/5000: episode: 195, duration: 0.479s, episode steps:  34, steps per second:  71, episode reward: 32.635, mean reward:  0.960 [-2.550, 31.997], mean action: 3.765 [0.000, 11.000],  loss: 0.025080, mae: 0.335249, mean_q: 0.550550, mean_eps: 0.000000
 4515/5000: episode: 196, duration: 0.407s, episode steps:  28, steps per second:  69, episode reward: 32.917, mean reward:  1.176 [-2.412, 32.017], mean action: 6.107 [0.000, 19.000],  loss: 0.016667, mae: 0.301125, mean_q: 0.537689, mean_eps: 0.000000
 4545/5000: episode: 197, duration: 0.428s, episode steps:  30, steps per second:  70, episode reward: 32.473, mean reward:  1.082 [-2.603, 32.290], mean action: 4.600 [0.000, 19.000],  loss: 0.022580, mae: 0.326312, mean_q: 0.541561, mean_eps: 0.000000
 4581/5000: episode: 198, duration: 0.500s, episode steps:  36, steps per second:  72, episode reward: -32.970, mean reward: -0.916 [-32.052,  2.570], mean action: 7.944 [0.000, 18.000],  loss: 0.019278, mae: 0.306879, mean_q: 0.516938, mean_eps: 0.000000
 4592/5000: episode: 199, duration: 0.176s, episode steps:  11, steps per second:  62, episode reward: 44.116, mean reward:  4.011 [-2.026, 32.847], mean action: 2.091 [0.000, 12.000],  loss: 0.018513, mae: 0.302629, mean_q: 0.531300, mean_eps: 0.000000
 4620/5000: episode: 200, duration: 0.401s, episode steps:  28, steps per second:  70, episode reward: 35.307, mean reward:  1.261 [-3.000, 32.070], mean action: 4.607 [0.000, 18.000],  loss: 0.026378, mae: 0.341382, mean_q: 0.582710, mean_eps: 0.000000
 4639/5000: episode: 201, duration: 0.278s, episode steps:  19, steps per second:  68, episode reward: 31.000, mean reward:  1.632 [-3.000, 32.010], mean action: 4.211 [0.000, 16.000],  loss: 0.020089, mae: 0.324140, mean_q: 0.561862, mean_eps: 0.000000
 4662/5000: episode: 202, duration: 0.330s, episode steps:  23, steps per second:  70, episode reward: 32.480, mean reward:  1.412 [-3.000, 31.510], mean action: 2.957 [0.000, 12.000],  loss: 0.017447, mae: 0.305579, mean_q: 0.545776, mean_eps: 0.000000
 4690/5000: episode: 203, duration: 0.403s, episode steps:  28, steps per second:  69, episode reward: -32.590, mean reward: -1.164 [-33.000,  2.540], mean action: 5.179 [0.000, 16.000],  loss: 0.016350, mae: 0.294118, mean_q: 0.564761, mean_eps: 0.000000
 4709/5000: episode: 204, duration: 0.284s, episode steps:  19, steps per second:  67, episode reward: 38.702, mean reward:  2.037 [-2.498, 31.812], mean action: 3.789 [0.000, 16.000],  loss: 0.020496, mae: 0.321053, mean_q: 0.613837, mean_eps: 0.000000
 4739/5000: episode: 205, duration: 0.429s, episode steps:  30, steps per second:  70, episode reward: 35.202, mean reward:  1.173 [-2.511, 32.220], mean action: 4.367 [0.000, 20.000],  loss: 0.019603, mae: 0.316060, mean_q: 0.518853, mean_eps: 0.000000
 4765/5000: episode: 206, duration: 0.387s, episode steps:  26, steps per second:  67, episode reward: -32.320, mean reward: -1.243 [-31.865,  2.520], mean action: 4.923 [0.000, 16.000],  loss: 0.022807, mae: 0.339742, mean_q: 0.461087, mean_eps: 0.000000
 4793/5000: episode: 207, duration: 0.418s, episode steps:  28, steps per second:  67, episode reward: 32.385, mean reward:  1.157 [-2.426, 32.270], mean action: 6.786 [0.000, 19.000],  loss: 0.022952, mae: 0.328819, mean_q: 0.500332, mean_eps: 0.000000
 4811/5000: episode: 208, duration: 0.267s, episode steps:  18, steps per second:  67, episode reward: 35.390, mean reward:  1.966 [-2.901, 32.470], mean action: 3.722 [0.000, 9.000],  loss: 0.022568, mae: 0.329414, mean_q: 0.550733, mean_eps: 0.000000
 4834/5000: episode: 209, duration: 0.327s, episode steps:  23, steps per second:  70, episode reward: -35.730, mean reward: -1.553 [-32.047,  2.221], mean action: 3.783 [0.000, 9.000],  loss: 0.019038, mae: 0.311171, mean_q: 0.515004, mean_eps: 0.000000
 4861/5000: episode: 210, duration: 0.387s, episode steps:  27, steps per second:  70, episode reward: 35.744, mean reward:  1.324 [-2.454, 32.120], mean action: 4.259 [2.000, 11.000],  loss: 0.019489, mae: 0.316446, mean_q: 0.534125, mean_eps: 0.000000
 4885/5000: episode: 211, duration: 0.354s, episode steps:  24, steps per second:  68, episode reward: 38.540, mean reward:  1.606 [-2.225, 32.330], mean action: 3.625 [0.000, 15.000],  loss: 0.020949, mae: 0.319285, mean_q: 0.539500, mean_eps: 0.000000
 4913/5000: episode: 212, duration: 0.414s, episode steps:  28, steps per second:  68, episode reward: -38.320, mean reward: -1.369 [-31.939,  2.553], mean action: 8.821 [0.000, 16.000],  loss: 0.020854, mae: 0.311865, mean_q: 0.547984, mean_eps: 0.000000
 4935/5000: episode: 213, duration: 0.321s, episode steps:  22, steps per second:  69, episode reward: 35.727, mean reward:  1.624 [-3.000, 33.058], mean action: 5.364 [0.000, 14.000],  loss: 0.019984, mae: 0.307266, mean_q: 0.496790, mean_eps: 0.000000
 4969/5000: episode: 214, duration: 0.478s, episode steps:  34, steps per second:  71, episode reward: 38.781, mean reward:  1.141 [-3.000, 32.230], mean action: 7.235 [0.000, 19.000],  loss: 0.019537, mae: 0.311952, mean_q: 0.480628, mean_eps: 0.000000
 4993/5000: episode: 215, duration: 15.147s, episode steps:  24, steps per second:   2, episode reward: -32.770, mean reward: -1.365 [-31.804,  2.302], mean action: 7.583 [1.000, 17.000],  loss: 0.024741, mae: 0.329140, mean_q: 0.487518, mean_eps: 0.000000
done, took 81.946 seconds
DQN Evaluation: 10243 victories out of 11972 episodes
Training for 5000 steps ...
   21/5000: episode: 1, duration: 0.220s, episode steps:  21, steps per second:  96, episode reward: 41.540, mean reward:  1.978 [-2.154, 32.403], mean action: 3.571 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   41/5000: episode: 2, duration: 0.154s, episode steps:  20, steps per second: 130, episode reward: 47.610, mean reward:  2.380 [-0.080, 31.692], mean action: 1.850 [1.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   64/5000: episode: 3, duration: 0.169s, episode steps:  23, steps per second: 136, episode reward: 40.960, mean reward:  1.781 [-2.220, 31.831], mean action: 2.826 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  104/5000: episode: 4, duration: 0.257s, episode steps:  40, steps per second: 156, episode reward: 32.699, mean reward:  0.817 [-3.000, 31.929], mean action: 2.300 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  127/5000: episode: 5, duration: 0.158s, episode steps:  23, steps per second: 146, episode reward: 41.706, mean reward:  1.813 [-2.743, 32.027], mean action: 1.174 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  154/5000: episode: 6, duration: 0.189s, episode steps:  27, steps per second: 143, episode reward: 43.788, mean reward:  1.622 [-2.117, 32.070], mean action: 3.296 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  174/5000: episode: 7, duration: 0.146s, episode steps:  20, steps per second: 137, episode reward: 42.000, mean reward:  2.100 [-2.304, 32.480], mean action: 3.000 [1.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  196/5000: episode: 8, duration: 0.161s, episode steps:  22, steps per second: 136, episode reward: 35.569, mean reward:  1.617 [-3.000, 31.997], mean action: 4.000 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  221/5000: episode: 9, duration: 0.176s, episode steps:  25, steps per second: 142, episode reward: 38.157, mean reward:  1.526 [-2.625, 31.733], mean action: 4.280 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  259/5000: episode: 10, duration: 0.245s, episode steps:  38, steps per second: 155, episode reward: -37.870, mean reward: -0.997 [-32.131,  2.455], mean action: 5.263 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  292/5000: episode: 11, duration: 0.214s, episode steps:  33, steps per second: 154, episode reward: 35.259, mean reward:  1.068 [-3.000, 32.280], mean action: 5.061 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  310/5000: episode: 12, duration: 0.124s, episode steps:  18, steps per second: 145, episode reward: 43.786, mean reward:  2.433 [-3.000, 32.900], mean action: 4.167 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  341/5000: episode: 13, duration: 0.203s, episode steps:  31, steps per second: 153, episode reward: 38.914, mean reward:  1.255 [-2.655, 32.130], mean action: 1.903 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  369/5000: episode: 14, duration: 0.189s, episode steps:  28, steps per second: 148, episode reward: 41.255, mean reward:  1.473 [-2.333, 32.340], mean action: 3.571 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  391/5000: episode: 15, duration: 0.151s, episode steps:  22, steps per second: 146, episode reward: 38.876, mean reward:  1.767 [-2.900, 32.350], mean action: 5.045 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  417/5000: episode: 16, duration: 0.166s, episode steps:  26, steps per second: 157, episode reward: 46.337, mean reward:  1.782 [-0.395, 31.292], mean action: 4.269 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  429/5000: episode: 17, duration: 0.107s, episode steps:  12, steps per second: 113, episode reward: 42.000, mean reward:  3.500 [-2.074, 30.497], mean action: 2.500 [1.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  462/5000: episode: 18, duration: 0.222s, episode steps:  33, steps per second: 148, episode reward: 32.549, mean reward:  0.986 [-2.894, 32.270], mean action: 2.939 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  488/5000: episode: 19, duration: 0.185s, episode steps:  26, steps per second: 140, episode reward: 41.330, mean reward:  1.590 [-2.306, 31.918], mean action: 3.192 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  511/5000: episode: 20, duration: 0.167s, episode steps:  23, steps per second: 137, episode reward: 38.701, mean reward:  1.683 [-2.220, 31.931], mean action: 1.696 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  537/5000: episode: 21, duration: 0.169s, episode steps:  26, steps per second: 154, episode reward: 42.000, mean reward:  1.615 [-2.622, 32.290], mean action: 3.269 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  562/5000: episode: 22, duration: 0.177s, episode steps:  25, steps per second: 141, episode reward: 38.405, mean reward:  1.536 [-3.000, 32.720], mean action: 4.400 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  583/5000: episode: 23, duration: 0.154s, episode steps:  21, steps per second: 137, episode reward: 38.240, mean reward:  1.821 [-3.000, 33.000], mean action: 3.905 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  609/5000: episode: 24, duration: 0.179s, episode steps:  26, steps per second: 146, episode reward: 41.523, mean reward:  1.597 [-2.263, 31.875], mean action: 1.269 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  621/5000: episode: 25, duration: 0.101s, episode steps:  12, steps per second: 119, episode reward: 44.767, mean reward:  3.731 [-2.468, 33.000], mean action: 2.417 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  648/5000: episode: 26, duration: 0.196s, episode steps:  27, steps per second: 137, episode reward: 42.829, mean reward:  1.586 [-3.000, 32.124], mean action: 3.111 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  684/5000: episode: 27, duration: 0.230s, episode steps:  36, steps per second: 157, episode reward: 41.149, mean reward:  1.143 [-3.000, 32.130], mean action: 3.278 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  734/5000: episode: 28, duration: 0.328s, episode steps:  50, steps per second: 153, episode reward: 35.787, mean reward:  0.716 [-3.000, 31.909], mean action: 4.180 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  761/5000: episode: 29, duration: 0.182s, episode steps:  27, steps per second: 149, episode reward: 41.442, mean reward:  1.535 [-2.037, 31.892], mean action: 2.519 [0.000, 8.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  791/5000: episode: 30, duration: 0.224s, episode steps:  30, steps per second: 134, episode reward: 39.000, mean reward:  1.300 [-2.494, 32.520], mean action: 2.433 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  832/5000: episode: 31, duration: 0.276s, episode steps:  41, steps per second: 149, episode reward: 35.565, mean reward:  0.867 [-2.893, 32.658], mean action: 3.098 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  856/5000: episode: 32, duration: 0.171s, episode steps:  24, steps per second: 140, episode reward: 43.937, mean reward:  1.831 [-2.057, 32.036], mean action: 4.250 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  882/5000: episode: 33, duration: 0.183s, episode steps:  26, steps per second: 142, episode reward: 38.902, mean reward:  1.496 [-2.514, 32.602], mean action: 1.846 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  904/5000: episode: 34, duration: 0.153s, episode steps:  22, steps per second: 144, episode reward: 37.770, mean reward:  1.717 [-3.000, 32.110], mean action: 3.091 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  931/5000: episode: 35, duration: 0.173s, episode steps:  27, steps per second: 156, episode reward: 42.000, mean reward:  1.556 [-2.276, 32.450], mean action: 1.667 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  953/5000: episode: 36, duration: 0.155s, episode steps:  22, steps per second: 142, episode reward: 41.019, mean reward:  1.865 [-2.260, 32.903], mean action: 3.045 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  994/5000: episode: 37, duration: 0.252s, episode steps:  41, steps per second: 162, episode reward: 41.985, mean reward:  1.024 [-2.458, 32.200], mean action: 2.634 [1.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1023/5000: episode: 38, duration: 0.368s, episode steps:  29, steps per second:  79, episode reward: 41.825, mean reward:  1.442 [-2.524, 32.211], mean action: 1.448 [0.000, 12.000],  loss: 0.022808, mae: 0.324237, mean_q: 0.500839, mean_eps: 0.000000
 1049/5000: episode: 39, duration: 0.392s, episode steps:  26, steps per second:  66, episode reward: 46.179, mean reward:  1.776 [-0.269, 36.260], mean action: 5.846 [0.000, 15.000],  loss: 0.019958, mae: 0.318521, mean_q: 0.467436, mean_eps: 0.000000
 1072/5000: episode: 40, duration: 0.336s, episode steps:  23, steps per second:  68, episode reward: 45.000, mean reward:  1.957 [-2.241, 32.340], mean action: 2.957 [0.000, 20.000],  loss: 0.018759, mae: 0.306577, mean_q: 0.520183, mean_eps: 0.000000
 1099/5000: episode: 41, duration: 0.387s, episode steps:  27, steps per second:  70, episode reward: 35.900, mean reward:  1.330 [-2.770, 32.860], mean action: 6.519 [0.000, 21.000],  loss: 0.019307, mae: 0.308586, mean_q: 0.545748, mean_eps: 0.000000
 1130/5000: episode: 42, duration: 0.431s, episode steps:  31, steps per second:  72, episode reward: 37.815, mean reward:  1.220 [-3.000, 31.474], mean action: 3.806 [0.000, 16.000],  loss: 0.021438, mae: 0.315182, mean_q: 0.505936, mean_eps: 0.000000
 1162/5000: episode: 43, duration: 0.451s, episode steps:  32, steps per second:  71, episode reward: 47.222, mean reward:  1.476 [-0.204, 32.040], mean action: 4.688 [0.000, 15.000],  loss: 0.027620, mae: 0.349384, mean_q: 0.535595, mean_eps: 0.000000
 1193/5000: episode: 44, duration: 0.603s, episode steps:  31, steps per second:  51, episode reward: 41.868, mean reward:  1.351 [-3.000, 32.177], mean action: 3.161 [0.000, 20.000],  loss: 0.019922, mae: 0.311996, mean_q: 0.535155, mean_eps: 0.000000
 1212/5000: episode: 45, duration: 0.344s, episode steps:  19, steps per second:  55, episode reward: 44.434, mean reward:  2.339 [-2.291, 32.043], mean action: 3.737 [0.000, 15.000],  loss: 0.019533, mae: 0.310472, mean_q: 0.558846, mean_eps: 0.000000
 1231/5000: episode: 46, duration: 0.286s, episode steps:  19, steps per second:  66, episode reward: 44.560, mean reward:  2.345 [-2.212, 32.087], mean action: 4.000 [0.000, 15.000],  loss: 0.020135, mae: 0.314083, mean_q: 0.568889, mean_eps: 0.000000
 1248/5000: episode: 47, duration: 0.254s, episode steps:  17, steps per second:  67, episode reward: 44.675, mean reward:  2.628 [-2.045, 31.755], mean action: 1.412 [0.000, 3.000],  loss: 0.020696, mae: 0.318906, mean_q: 0.551899, mean_eps: 0.000000
 1268/5000: episode: 48, duration: 0.290s, episode steps:  20, steps per second:  69, episode reward: 39.000, mean reward:  1.950 [-3.000, 32.700], mean action: 3.050 [0.000, 15.000],  loss: 0.018009, mae: 0.305322, mean_q: 0.479736, mean_eps: 0.000000
 1280/5000: episode: 49, duration: 0.187s, episode steps:  12, steps per second:  64, episode reward: 44.592, mean reward:  3.716 [-2.212, 32.340], mean action: 2.500 [0.000, 15.000],  loss: 0.024323, mae: 0.337041, mean_q: 0.456522, mean_eps: 0.000000
 1301/5000: episode: 50, duration: 0.316s, episode steps:  21, steps per second:  66, episode reward: 42.736, mean reward:  2.035 [-2.302, 32.420], mean action: 4.571 [0.000, 12.000],  loss: 0.021460, mae: 0.321965, mean_q: 0.493149, mean_eps: 0.000000
 1328/5000: episode: 51, duration: 0.387s, episode steps:  27, steps per second:  70, episode reward: 41.881, mean reward:  1.551 [-3.000, 32.780], mean action: 2.667 [0.000, 16.000],  loss: 0.022260, mae: 0.327867, mean_q: 0.551620, mean_eps: 0.000000
 1353/5000: episode: 52, duration: 0.369s, episode steps:  25, steps per second:  68, episode reward: 43.518, mean reward:  1.741 [-2.149, 32.211], mean action: 2.680 [0.000, 12.000],  loss: 0.018776, mae: 0.307274, mean_q: 0.571675, mean_eps: 0.000000
 1376/5000: episode: 53, duration: 0.331s, episode steps:  23, steps per second:  69, episode reward: 38.504, mean reward:  1.674 [-2.447, 32.410], mean action: 3.783 [0.000, 19.000],  loss: 0.019261, mae: 0.314264, mean_q: 0.532468, mean_eps: 0.000000
 1401/5000: episode: 54, duration: 0.359s, episode steps:  25, steps per second:  70, episode reward: 44.083, mean reward:  1.763 [-2.129, 31.882], mean action: 3.000 [0.000, 20.000],  loss: 0.020102, mae: 0.313733, mean_q: 0.505048, mean_eps: 0.000000
 1426/5000: episode: 55, duration: 0.358s, episode steps:  25, steps per second:  70, episode reward: 40.492, mean reward:  1.620 [-2.209, 32.170], mean action: 4.240 [0.000, 20.000],  loss: 0.022596, mae: 0.325481, mean_q: 0.543632, mean_eps: 0.000000
 1452/5000: episode: 56, duration: 0.381s, episode steps:  26, steps per second:  68, episode reward: 41.223, mean reward:  1.586 [-2.751, 31.688], mean action: 2.654 [0.000, 20.000],  loss: 0.020187, mae: 0.312089, mean_q: 0.503980, mean_eps: 0.000000
 1485/5000: episode: 57, duration: 0.473s, episode steps:  33, steps per second:  70, episode reward: 40.785, mean reward:  1.236 [-3.000, 32.234], mean action: 5.121 [0.000, 20.000],  loss: 0.017997, mae: 0.297648, mean_q: 0.479461, mean_eps: 0.000000
 1503/5000: episode: 58, duration: 0.270s, episode steps:  18, steps per second:  67, episode reward: 44.442, mean reward:  2.469 [-2.532, 32.080], mean action: 3.889 [0.000, 14.000],  loss: 0.021251, mae: 0.311261, mean_q: 0.547487, mean_eps: 0.000000
 1522/5000: episode: 59, duration: 0.277s, episode steps:  19, steps per second:  69, episode reward: 41.486, mean reward:  2.183 [-2.516, 32.070], mean action: 3.842 [0.000, 19.000],  loss: 0.022043, mae: 0.314299, mean_q: 0.536164, mean_eps: 0.000000
 1548/5000: episode: 60, duration: 0.374s, episode steps:  26, steps per second:  70, episode reward: 44.980, mean reward:  1.730 [-2.111, 32.250], mean action: 2.615 [1.000, 19.000],  loss: 0.020933, mae: 0.311086, mean_q: 0.498823, mean_eps: 0.000000
 1613/5000: episode: 61, duration: 0.895s, episode steps:  65, steps per second:  73, episode reward: -32.820, mean reward: -0.505 [-32.063,  2.620], mean action: 6.031 [0.000, 19.000],  loss: 0.021166, mae: 0.322219, mean_q: 0.575895, mean_eps: 0.000000
 1642/5000: episode: 62, duration: 0.432s, episode steps:  29, steps per second:  67, episode reward: 41.041, mean reward:  1.415 [-2.279, 32.171], mean action: 1.517 [0.000, 16.000],  loss: 0.018070, mae: 0.314587, mean_q: 0.598112, mean_eps: 0.000000
 1661/5000: episode: 63, duration: 0.281s, episode steps:  19, steps per second:  68, episode reward: 44.901, mean reward:  2.363 [-2.172, 32.391], mean action: 0.263 [0.000, 2.000],  loss: 0.021273, mae: 0.333389, mean_q: 0.619484, mean_eps: 0.000000
 1698/5000: episode: 64, duration: 0.508s, episode steps:  37, steps per second:  73, episode reward: 34.808, mean reward:  0.941 [-3.000, 32.440], mean action: 6.270 [0.000, 20.000],  loss: 0.021958, mae: 0.329728, mean_q: 0.585155, mean_eps: 0.000000
 1724/5000: episode: 65, duration: 0.381s, episode steps:  26, steps per second:  68, episode reward: 38.208, mean reward:  1.470 [-3.000, 32.040], mean action: 6.231 [0.000, 19.000],  loss: 0.023670, mae: 0.337080, mean_q: 0.597668, mean_eps: 0.000000
 1755/5000: episode: 66, duration: 0.443s, episode steps:  31, steps per second:  70, episode reward: 33.000, mean reward:  1.065 [-3.000, 32.690], mean action: 6.355 [0.000, 19.000],  loss: 0.019121, mae: 0.309385, mean_q: 0.597264, mean_eps: 0.000000
 1778/5000: episode: 67, duration: 0.330s, episode steps:  23, steps per second:  70, episode reward: 41.142, mean reward:  1.789 [-2.425, 31.659], mean action: 2.696 [1.000, 16.000],  loss: 0.023561, mae: 0.345546, mean_q: 0.538295, mean_eps: 0.000000
 1798/5000: episode: 68, duration: 0.312s, episode steps:  20, steps per second:  64, episode reward: 44.116, mean reward:  2.206 [-2.020, 32.120], mean action: 3.450 [0.000, 12.000],  loss: 0.023946, mae: 0.341506, mean_q: 0.525344, mean_eps: 0.000000
 1817/5000: episode: 69, duration: 0.277s, episode steps:  19, steps per second:  69, episode reward: 43.867, mean reward:  2.309 [-2.004, 32.814], mean action: 4.053 [0.000, 12.000],  loss: 0.023958, mae: 0.341428, mean_q: 0.537953, mean_eps: 0.000000
 1843/5000: episode: 70, duration: 0.377s, episode steps:  26, steps per second:  69, episode reward: 44.275, mean reward:  1.703 [-3.000, 32.290], mean action: 2.962 [0.000, 15.000],  loss: 0.022368, mae: 0.328853, mean_q: 0.558242, mean_eps: 0.000000
 1878/5000: episode: 71, duration: 0.496s, episode steps:  35, steps per second:  71, episode reward: 35.897, mean reward:  1.026 [-2.939, 32.156], mean action: 3.914 [0.000, 19.000],  loss: 0.019925, mae: 0.316253, mean_q: 0.569411, mean_eps: 0.000000
 1896/5000: episode: 72, duration: 0.269s, episode steps:  18, steps per second:  67, episode reward: 44.535, mean reward:  2.474 [-2.612, 33.000], mean action: 2.333 [0.000, 20.000],  loss: 0.020223, mae: 0.309642, mean_q: 0.523713, mean_eps: 0.000000
 1956/5000: episode: 73, duration: 0.818s, episode steps:  60, steps per second:  73, episode reward: -32.280, mean reward: -0.538 [-32.442,  3.061], mean action: 4.000 [0.000, 14.000],  loss: 0.020314, mae: 0.308365, mean_q: 0.492106, mean_eps: 0.000000
 1981/5000: episode: 74, duration: 0.364s, episode steps:  25, steps per second:  69, episode reward: 38.588, mean reward:  1.544 [-2.758, 32.070], mean action: 3.400 [0.000, 12.000],  loss: 0.020201, mae: 0.308936, mean_q: 0.507986, mean_eps: 0.000000
 2023/5000: episode: 75, duration: 0.585s, episode steps:  42, steps per second:  72, episode reward: 38.375, mean reward:  0.914 [-2.361, 31.715], mean action: 3.500 [0.000, 14.000],  loss: 0.019230, mae: 0.312489, mean_q: 0.521469, mean_eps: 0.000000
 2053/5000: episode: 76, duration: 0.436s, episode steps:  30, steps per second:  69, episode reward: 41.382, mean reward:  1.379 [-2.403, 32.240], mean action: 1.867 [0.000, 20.000],  loss: 0.020318, mae: 0.317258, mean_q: 0.527011, mean_eps: 0.000000
 2077/5000: episode: 77, duration: 0.361s, episode steps:  24, steps per second:  67, episode reward: 41.729, mean reward:  1.739 [-2.077, 32.710], mean action: 2.833 [0.000, 14.000],  loss: 0.020250, mae: 0.322943, mean_q: 0.509770, mean_eps: 0.000000
 2097/5000: episode: 78, duration: 6.283s, episode steps:  20, steps per second:   3, episode reward: 41.709, mean reward:  2.085 [-3.000, 32.153], mean action: 3.300 [2.000, 12.000],  loss: 0.017355, mae: 0.306380, mean_q: 0.511708, mean_eps: 0.000000
 2107/5000: episode: 79, duration: 0.171s, episode steps:  10, steps per second:  59, episode reward: 47.898, mean reward:  4.790 [ 0.080, 32.920], mean action: 3.700 [2.000, 14.000],  loss: 0.025736, mae: 0.336274, mean_q: 0.545850, mean_eps: 0.000000
 2136/5000: episode: 80, duration: 0.420s, episode steps:  29, steps per second:  69, episode reward: 43.558, mean reward:  1.502 [-2.232, 32.188], mean action: 4.379 [0.000, 20.000],  loss: 0.022810, mae: 0.327360, mean_q: 0.591023, mean_eps: 0.000000
 2152/5000: episode: 81, duration: 0.245s, episode steps:  16, steps per second:  65, episode reward: 44.391, mean reward:  2.774 [-2.677, 32.889], mean action: 2.688 [0.000, 13.000],  loss: 0.020714, mae: 0.307799, mean_q: 0.508319, mean_eps: 0.000000
 2171/5000: episode: 82, duration: 0.279s, episode steps:  19, steps per second:  68, episode reward: 44.309, mean reward:  2.332 [-2.561, 32.400], mean action: 1.526 [0.000, 3.000],  loss: 0.020293, mae: 0.308300, mean_q: 0.517432, mean_eps: 0.000000
 2198/5000: episode: 83, duration: 0.389s, episode steps:  27, steps per second:  69, episode reward: 41.709, mean reward:  1.545 [-2.227, 32.023], mean action: 2.333 [1.000, 12.000],  loss: 0.019168, mae: 0.298798, mean_q: 0.513046, mean_eps: 0.000000
 2216/5000: episode: 84, duration: 0.273s, episode steps:  18, steps per second:  66, episode reward: 46.298, mean reward:  2.572 [-0.550, 32.070], mean action: 2.556 [0.000, 10.000],  loss: 0.017795, mae: 0.288542, mean_q: 0.464010, mean_eps: 0.000000
 2240/5000: episode: 85, duration: 0.344s, episode steps:  24, steps per second:  70, episode reward: 35.553, mean reward:  1.481 [-3.000, 32.055], mean action: 4.667 [0.000, 14.000],  loss: 0.021526, mae: 0.316966, mean_q: 0.568616, mean_eps: 0.000000
 2264/5000: episode: 86, duration: 0.363s, episode steps:  24, steps per second:  66, episode reward: 38.865, mean reward:  1.619 [-3.000, 32.030], mean action: 3.458 [0.000, 15.000],  loss: 0.021477, mae: 0.312349, mean_q: 0.528425, mean_eps: 0.000000
 2291/5000: episode: 87, duration: 0.405s, episode steps:  27, steps per second:  67, episode reward: 43.117, mean reward:  1.597 [-2.212, 32.100], mean action: 4.815 [0.000, 15.000],  loss: 0.022268, mae: 0.316536, mean_q: 0.506387, mean_eps: 0.000000
 2322/5000: episode: 88, duration: 0.442s, episode steps:  31, steps per second:  70, episode reward: 41.878, mean reward:  1.351 [-3.000, 32.318], mean action: 4.871 [0.000, 15.000],  loss: 0.020858, mae: 0.314368, mean_q: 0.533034, mean_eps: 0.000000
 2373/5000: episode: 89, duration: 0.710s, episode steps:  51, steps per second:  72, episode reward: 38.886, mean reward:  0.762 [-3.000, 32.110], mean action: 3.275 [0.000, 15.000],  loss: 0.024575, mae: 0.335042, mean_q: 0.606654, mean_eps: 0.000000
 2410/5000: episode: 90, duration: 0.522s, episode steps:  37, steps per second:  71, episode reward: 38.167, mean reward:  1.032 [-3.000, 32.240], mean action: 4.973 [0.000, 15.000],  loss: 0.018707, mae: 0.307253, mean_q: 0.603760, mean_eps: 0.000000
 2445/5000: episode: 91, duration: 0.519s, episode steps:  35, steps per second:  67, episode reward: 38.880, mean reward:  1.111 [-3.000, 32.220], mean action: 3.657 [0.000, 15.000],  loss: 0.019415, mae: 0.322445, mean_q: 0.572070, mean_eps: 0.000000
 2466/5000: episode: 92, duration: 0.526s, episode steps:  21, steps per second:  40, episode reward: 44.257, mean reward:  2.107 [-2.079, 32.160], mean action: 2.190 [0.000, 11.000],  loss: 0.018689, mae: 0.313115, mean_q: 0.488675, mean_eps: 0.000000
 2496/5000: episode: 93, duration: 0.427s, episode steps:  30, steps per second:  70, episode reward: -32.600, mean reward: -1.087 [-33.000,  2.450], mean action: 5.633 [0.000, 18.000],  loss: 0.023155, mae: 0.332940, mean_q: 0.552860, mean_eps: 0.000000
 2524/5000: episode: 94, duration: 0.398s, episode steps:  28, steps per second:  70, episode reward: 41.918, mean reward:  1.497 [-2.280, 32.910], mean action: 3.214 [0.000, 19.000],  loss: 0.021380, mae: 0.325153, mean_q: 0.533773, mean_eps: 0.000000
 2554/5000: episode: 95, duration: 0.429s, episode steps:  30, steps per second:  70, episode reward: 41.538, mean reward:  1.385 [-2.301, 32.210], mean action: 5.200 [0.000, 16.000],  loss: 0.021373, mae: 0.329384, mean_q: 0.524054, mean_eps: 0.000000
 2575/5000: episode: 96, duration: 0.314s, episode steps:  21, steps per second:  67, episode reward: 41.803, mean reward:  1.991 [-2.092, 31.991], mean action: 3.143 [0.000, 16.000],  loss: 0.022563, mae: 0.330729, mean_q: 0.499622, mean_eps: 0.000000
 2620/5000: episode: 97, duration: 0.612s, episode steps:  45, steps per second:  74, episode reward: 37.741, mean reward:  0.839 [-3.000, 32.440], mean action: 3.578 [0.000, 18.000],  loss: 0.020261, mae: 0.311002, mean_q: 0.516691, mean_eps: 0.000000
 2641/5000: episode: 98, duration: 0.301s, episode steps:  21, steps per second:  70, episode reward: 41.333, mean reward:  1.968 [-3.000, 32.370], mean action: 4.762 [1.000, 16.000],  loss: 0.021876, mae: 0.322607, mean_q: 0.504121, mean_eps: 0.000000
 2663/5000: episode: 99, duration: 0.410s, episode steps:  22, steps per second:  54, episode reward: 44.097, mean reward:  2.004 [-2.131, 32.790], mean action: 1.182 [0.000, 6.000],  loss: 0.020137, mae: 0.311315, mean_q: 0.499460, mean_eps: 0.000000
 2680/5000: episode: 100, duration: 0.256s, episode steps:  17, steps per second:  67, episode reward: 43.778, mean reward:  2.575 [-2.855, 32.325], mean action: 3.412 [0.000, 20.000],  loss: 0.022077, mae: 0.319857, mean_q: 0.526109, mean_eps: 0.000000
 2699/5000: episode: 101, duration: 0.289s, episode steps:  19, steps per second:  66, episode reward: 45.000, mean reward:  2.368 [-2.628, 32.220], mean action: 1.421 [0.000, 11.000],  loss: 0.019256, mae: 0.312659, mean_q: 0.460944, mean_eps: 0.000000
 2723/5000: episode: 102, duration: 0.346s, episode steps:  24, steps per second:  69, episode reward: 41.411, mean reward:  1.725 [-2.610, 31.998], mean action: 3.958 [0.000, 19.000],  loss: 0.016311, mae: 0.297800, mean_q: 0.474014, mean_eps: 0.000000
 2750/5000: episode: 103, duration: 0.385s, episode steps:  27, steps per second:  70, episode reward: 41.691, mean reward:  1.544 [-2.283, 32.290], mean action: 2.444 [0.000, 12.000],  loss: 0.022923, mae: 0.327742, mean_q: 0.531282, mean_eps: 0.000000
 2777/5000: episode: 104, duration: 0.376s, episode steps:  27, steps per second:  72, episode reward: 38.791, mean reward:  1.437 [-2.333, 32.760], mean action: 3.444 [0.000, 19.000],  loss: 0.021547, mae: 0.319281, mean_q: 0.516778, mean_eps: 0.000000
 2799/5000: episode: 105, duration: 0.328s, episode steps:  22, steps per second:  67, episode reward: 41.793, mean reward:  1.900 [-3.000, 32.020], mean action: 4.818 [0.000, 19.000],  loss: 0.019431, mae: 0.315788, mean_q: 0.479009, mean_eps: 0.000000
 2824/5000: episode: 106, duration: 0.374s, episode steps:  25, steps per second:  67, episode reward: 44.655, mean reward:  1.786 [-2.070, 33.000], mean action: 2.960 [1.000, 19.000],  loss: 0.015967, mae: 0.298994, mean_q: 0.483921, mean_eps: 0.000000
 2865/5000: episode: 107, duration: 0.588s, episode steps:  41, steps per second:  70, episode reward: 44.521, mean reward:  1.086 [-2.218, 32.230], mean action: 2.512 [0.000, 6.000],  loss: 0.021404, mae: 0.312222, mean_q: 0.488174, mean_eps: 0.000000
 2888/5000: episode: 108, duration: 0.330s, episode steps:  23, steps per second:  70, episode reward: 41.618, mean reward:  1.809 [-2.234, 32.320], mean action: 3.304 [0.000, 19.000],  loss: 0.016328, mae: 0.293444, mean_q: 0.528362, mean_eps: 0.000000
 2918/5000: episode: 109, duration: 0.436s, episode steps:  30, steps per second:  69, episode reward: 36.000, mean reward:  1.200 [-2.443, 32.340], mean action: 3.000 [0.000, 19.000],  loss: 0.022533, mae: 0.312926, mean_q: 0.482801, mean_eps: 0.000000
 2963/5000: episode: 110, duration: 0.640s, episode steps:  45, steps per second:  70, episode reward: 35.554, mean reward:  0.790 [-3.000, 32.150], mean action: 3.778 [0.000, 19.000],  loss: 0.021716, mae: 0.316149, mean_q: 0.539440, mean_eps: 0.000000
 2993/5000: episode: 111, duration: 0.444s, episode steps:  30, steps per second:  68, episode reward: 41.178, mean reward:  1.373 [-2.128, 32.040], mean action: 2.300 [0.000, 19.000],  loss: 0.019876, mae: 0.303435, mean_q: 0.499051, mean_eps: 0.000000
 3017/5000: episode: 112, duration: 0.347s, episode steps:  24, steps per second:  69, episode reward: 38.055, mean reward:  1.586 [-2.807, 32.150], mean action: 5.125 [0.000, 18.000],  loss: 0.020857, mae: 0.310858, mean_q: 0.477311, mean_eps: 0.000000
 3034/5000: episode: 113, duration: 0.249s, episode steps:  17, steps per second:  68, episode reward: 41.635, mean reward:  2.449 [-2.157, 32.227], mean action: 2.471 [0.000, 12.000],  loss: 0.025953, mae: 0.336680, mean_q: 0.538610, mean_eps: 0.000000
 3056/5000: episode: 114, duration: 0.316s, episode steps:  22, steps per second:  70, episode reward: 41.627, mean reward:  1.892 [-2.296, 31.869], mean action: 3.000 [1.000, 12.000],  loss: 0.022780, mae: 0.325457, mean_q: 0.636496, mean_eps: 0.000000
 3064/5000: episode: 115, duration: 0.136s, episode steps:   8, steps per second:  59, episode reward: 44.074, mean reward:  5.509 [-3.000, 32.796], mean action: 3.375 [1.000, 12.000],  loss: 0.019532, mae: 0.327558, mean_q: 0.554946, mean_eps: 0.000000
 3092/5000: episode: 116, duration: 0.408s, episode steps:  28, steps per second:  69, episode reward: 46.853, mean reward:  1.673 [-0.100, 32.380], mean action: 3.357 [0.000, 12.000],  loss: 0.022744, mae: 0.336988, mean_q: 0.545313, mean_eps: 0.000000
 3126/5000: episode: 117, duration: 0.483s, episode steps:  34, steps per second:  70, episode reward: 41.464, mean reward:  1.220 [-2.124, 32.120], mean action: 3.676 [1.000, 12.000],  loss: 0.023162, mae: 0.329586, mean_q: 0.545002, mean_eps: 0.000000
 3151/5000: episode: 118, duration: 0.362s, episode steps:  25, steps per second:  69, episode reward: 38.244, mean reward:  1.530 [-3.000, 32.019], mean action: 2.880 [0.000, 12.000],  loss: 0.019985, mae: 0.316492, mean_q: 0.561794, mean_eps: 0.000000
 3175/5000: episode: 119, duration: 0.348s, episode steps:  24, steps per second:  69, episode reward: 43.640, mean reward:  1.818 [-2.540, 32.480], mean action: 3.042 [0.000, 20.000],  loss: 0.023377, mae: 0.335789, mean_q: 0.609175, mean_eps: 0.000000
 3229/5000: episode: 120, duration: 0.819s, episode steps:  54, steps per second:  66, episode reward: -32.270, mean reward: -0.598 [-32.193,  2.802], mean action: 5.759 [0.000, 20.000],  loss: 0.020937, mae: 0.325872, mean_q: 0.540522, mean_eps: 0.000000
 3262/5000: episode: 121, duration: 0.515s, episode steps:  33, steps per second:  64, episode reward: 44.136, mean reward:  1.337 [-2.688, 32.190], mean action: 2.333 [0.000, 14.000],  loss: 0.021407, mae: 0.320124, mean_q: 0.535093, mean_eps: 0.000000
 3281/5000: episode: 122, duration: 0.312s, episode steps:  19, steps per second:  61, episode reward: 44.272, mean reward:  2.330 [-2.054, 32.180], mean action: 2.579 [1.000, 12.000],  loss: 0.019802, mae: 0.316059, mean_q: 0.533548, mean_eps: 0.000000
 3315/5000: episode: 123, duration: 0.487s, episode steps:  34, steps per second:  70, episode reward: 38.575, mean reward:  1.135 [-2.360, 32.265], mean action: 3.088 [0.000, 12.000],  loss: 0.022287, mae: 0.332930, mean_q: 0.590711, mean_eps: 0.000000
 3342/5000: episode: 124, duration: 0.404s, episode steps:  27, steps per second:  67, episode reward: 47.261, mean reward:  1.750 [-0.250, 32.200], mean action: 3.926 [0.000, 13.000],  loss: 0.022311, mae: 0.327952, mean_q: 0.549938, mean_eps: 0.000000
 3367/5000: episode: 125, duration: 0.354s, episode steps:  25, steps per second:  71, episode reward: 39.000, mean reward:  1.560 [-2.483, 32.100], mean action: 3.040 [0.000, 15.000],  loss: 0.022951, mae: 0.327683, mean_q: 0.521349, mean_eps: 0.000000
 3392/5000: episode: 126, duration: 0.367s, episode steps:  25, steps per second:  68, episode reward: 39.000, mean reward:  1.560 [-2.301, 32.220], mean action: 3.200 [0.000, 15.000],  loss: 0.021685, mae: 0.321056, mean_q: 0.545698, mean_eps: 0.000000
 3408/5000: episode: 127, duration: 0.240s, episode steps:  16, steps per second:  67, episode reward: 45.000, mean reward:  2.812 [-2.093, 32.080], mean action: 1.438 [0.000, 3.000],  loss: 0.021827, mae: 0.326948, mean_q: 0.552332, mean_eps: 0.000000
 3431/5000: episode: 128, duration: 0.336s, episode steps:  23, steps per second:  69, episode reward: 39.000, mean reward:  1.696 [-2.417, 29.913], mean action: 2.913 [0.000, 19.000],  loss: 0.018207, mae: 0.299827, mean_q: 0.562618, mean_eps: 0.000000
 3452/5000: episode: 129, duration: 0.305s, episode steps:  21, steps per second:  69, episode reward: 44.318, mean reward:  2.110 [-3.000, 32.356], mean action: 3.381 [0.000, 19.000],  loss: 0.021777, mae: 0.320140, mean_q: 0.527977, mean_eps: 0.000000
 3476/5000: episode: 130, duration: 0.363s, episode steps:  24, steps per second:  66, episode reward: 44.290, mean reward:  1.845 [-2.177, 31.847], mean action: 3.333 [0.000, 15.000],  loss: 0.020811, mae: 0.318780, mean_q: 0.506563, mean_eps: 0.000000
 3502/5000: episode: 131, duration: 0.376s, episode steps:  26, steps per second:  69, episode reward: 41.875, mean reward:  1.611 [-2.069, 32.020], mean action: 2.038 [0.000, 15.000],  loss: 0.020215, mae: 0.312483, mean_q: 0.508083, mean_eps: 0.000000
 3533/5000: episode: 132, duration: 0.450s, episode steps:  31, steps per second:  69, episode reward: 41.198, mean reward:  1.329 [-2.440, 32.160], mean action: 5.032 [0.000, 15.000],  loss: 0.019052, mae: 0.314638, mean_q: 0.497225, mean_eps: 0.000000
 3556/5000: episode: 133, duration: 0.339s, episode steps:  23, steps per second:  68, episode reward: 41.560, mean reward:  1.807 [-3.000, 32.903], mean action: 4.609 [0.000, 15.000],  loss: 0.022941, mae: 0.344213, mean_q: 0.471739, mean_eps: 0.000000
 3572/5000: episode: 134, duration: 0.235s, episode steps:  16, steps per second:  68, episode reward: 44.128, mean reward:  2.758 [-2.337, 33.000], mean action: 4.188 [0.000, 14.000],  loss: 0.019674, mae: 0.318910, mean_q: 0.473369, mean_eps: 0.000000
 3602/5000: episode: 135, duration: 0.422s, episode steps:  30, steps per second:  71, episode reward: 35.835, mean reward:  1.195 [-3.000, 32.070], mean action: 6.267 [0.000, 15.000],  loss: 0.020808, mae: 0.326393, mean_q: 0.543124, mean_eps: 0.000000
 3621/5000: episode: 136, duration: 0.295s, episode steps:  19, steps per second:  64, episode reward: 44.161, mean reward:  2.324 [-2.165, 32.340], mean action: 1.316 [0.000, 11.000],  loss: 0.021592, mae: 0.335089, mean_q: 0.577257, mean_eps: 0.000000
 3648/5000: episode: 137, duration: 0.385s, episode steps:  27, steps per second:  70, episode reward: 44.816, mean reward:  1.660 [-2.312, 32.050], mean action: 4.778 [0.000, 14.000],  loss: 0.023067, mae: 0.342828, mean_q: 0.562780, mean_eps: 0.000000
 3671/5000: episode: 138, duration: 0.339s, episode steps:  23, steps per second:  68, episode reward: 38.875, mean reward:  1.690 [-2.238, 32.530], mean action: 3.826 [0.000, 13.000],  loss: 0.017810, mae: 0.326291, mean_q: 0.546382, mean_eps: 0.000000
 3699/5000: episode: 139, duration: 0.409s, episode steps:  28, steps per second:  68, episode reward: 35.837, mean reward:  1.280 [-3.000, 32.103], mean action: 3.464 [0.000, 12.000],  loss: 0.018407, mae: 0.320034, mean_q: 0.509904, mean_eps: 0.000000
 3720/5000: episode: 140, duration: 0.304s, episode steps:  21, steps per second:  69, episode reward: 38.227, mean reward:  1.820 [-2.879, 31.837], mean action: 4.381 [0.000, 15.000],  loss: 0.019113, mae: 0.323821, mean_q: 0.583961, mean_eps: 0.000000
 3752/5000: episode: 141, duration: 0.456s, episode steps:  32, steps per second:  70, episode reward: 38.104, mean reward:  1.191 [-3.000, 31.858], mean action: 4.875 [0.000, 15.000],  loss: 0.018606, mae: 0.316976, mean_q: 0.552929, mean_eps: 0.000000
 3773/5000: episode: 142, duration: 0.315s, episode steps:  21, steps per second:  67, episode reward: 42.000, mean reward:  2.000 [-2.162, 33.000], mean action: 2.048 [0.000, 12.000],  loss: 0.022201, mae: 0.341783, mean_q: 0.569174, mean_eps: 0.000000
 3800/5000: episode: 143, duration: 0.396s, episode steps:  27, steps per second:  68, episode reward: 41.457, mean reward:  1.535 [-2.391, 32.230], mean action: 4.741 [0.000, 15.000],  loss: 0.019282, mae: 0.322506, mean_q: 0.585591, mean_eps: 0.000000
 3815/5000: episode: 144, duration: 0.232s, episode steps:  15, steps per second:  65, episode reward: 44.476, mean reward:  2.965 [-2.508, 32.246], mean action: 1.733 [0.000, 3.000],  loss: 0.023782, mae: 0.332280, mean_q: 0.601512, mean_eps: 0.000000
 3835/5000: episode: 145, duration: 0.292s, episode steps:  20, steps per second:  68, episode reward: 36.000, mean reward:  1.800 [-3.000, 29.537], mean action: 4.350 [0.000, 19.000],  loss: 0.019120, mae: 0.315505, mean_q: 0.585022, mean_eps: 0.000000
 3855/5000: episode: 146, duration: 0.300s, episode steps:  20, steps per second:  67, episode reward: 47.562, mean reward:  2.378 [-0.025, 32.170], mean action: 3.000 [3.000, 3.000],  loss: 0.020107, mae: 0.323342, mean_q: 0.493557, mean_eps: 0.000000
 3876/5000: episode: 147, duration: 0.317s, episode steps:  21, steps per second:  66, episode reward: 41.453, mean reward:  1.974 [-2.339, 32.180], mean action: 3.762 [0.000, 15.000],  loss: 0.018155, mae: 0.308466, mean_q: 0.474586, mean_eps: 0.000000
 3896/5000: episode: 148, duration: 0.302s, episode steps:  20, steps per second:  66, episode reward: 41.019, mean reward:  2.051 [-2.119, 32.901], mean action: 3.650 [0.000, 13.000],  loss: 0.029593, mae: 0.364051, mean_q: 0.587941, mean_eps: 0.000000
 3929/5000: episode: 149, duration: 0.476s, episode steps:  33, steps per second:  69, episode reward: -32.330, mean reward: -0.980 [-32.420,  2.370], mean action: 4.939 [0.000, 20.000],  loss: 0.021112, mae: 0.324722, mean_q: 0.584203, mean_eps: 0.000000
 3951/5000: episode: 150, duration: 0.334s, episode steps:  22, steps per second:  66, episode reward: 46.862, mean reward:  2.130 [-0.586, 32.280], mean action: 2.091 [0.000, 13.000],  loss: 0.019317, mae: 0.315098, mean_q: 0.550740, mean_eps: 0.000000
 3996/5000: episode: 151, duration: 0.637s, episode steps:  45, steps per second:  71, episode reward: 34.677, mean reward:  0.771 [-3.000, 32.240], mean action: 7.178 [0.000, 20.000],  loss: 0.021574, mae: 0.324857, mean_q: 0.557531, mean_eps: 0.000000
 4022/5000: episode: 152, duration: 0.532s, episode steps:  26, steps per second:  49, episode reward: 41.403, mean reward:  1.592 [-2.711, 31.731], mean action: 1.423 [0.000, 2.000],  loss: 0.023220, mae: 0.334443, mean_q: 0.475860, mean_eps: 0.000000
 4042/5000: episode: 153, duration: 0.364s, episode steps:  20, steps per second:  55, episode reward: 44.777, mean reward:  2.239 [-2.118, 32.420], mean action: 1.850 [1.000, 6.000],  loss: 0.022171, mae: 0.318614, mean_q: 0.501813, mean_eps: 0.000000
 4053/5000: episode: 154, duration: 0.188s, episode steps:  11, steps per second:  59, episode reward: 47.380, mean reward:  4.307 [-0.275, 32.110], mean action: 3.909 [2.000, 15.000],  loss: 0.022381, mae: 0.314998, mean_q: 0.492397, mean_eps: 0.000000
 4081/5000: episode: 155, duration: 0.416s, episode steps:  28, steps per second:  67, episode reward: 38.905, mean reward:  1.389 [-2.389, 32.125], mean action: 2.821 [0.000, 15.000],  loss: 0.019507, mae: 0.312694, mean_q: 0.508641, mean_eps: 0.000000
 4095/5000: episode: 156, duration: 0.210s, episode steps:  14, steps per second:  67, episode reward: 46.601, mean reward:  3.329 [-0.627, 32.160], mean action: 2.571 [1.000, 6.000],  loss: 0.025265, mae: 0.344726, mean_q: 0.545772, mean_eps: 0.000000
 4123/5000: episode: 157, duration: 0.456s, episode steps:  28, steps per second:  61, episode reward: 44.509, mean reward:  1.590 [-2.608, 32.050], mean action: 2.321 [0.000, 6.000],  loss: 0.021664, mae: 0.319691, mean_q: 0.522344, mean_eps: 0.000000
 4156/5000: episode: 158, duration: 0.496s, episode steps:  33, steps per second:  67, episode reward: 41.088, mean reward:  1.245 [-2.345, 32.050], mean action: 3.303 [0.000, 19.000],  loss: 0.022379, mae: 0.322301, mean_q: 0.561906, mean_eps: 0.000000
 4193/5000: episode: 159, duration: 0.539s, episode steps:  37, steps per second:  69, episode reward: 37.401, mean reward:  1.011 [-2.651, 32.220], mean action: 5.973 [0.000, 18.000],  loss: 0.022872, mae: 0.328726, mean_q: 0.563735, mean_eps: 0.000000
 4219/5000: episode: 160, duration: 0.375s, episode steps:  26, steps per second:  69, episode reward: 38.363, mean reward:  1.475 [-3.000, 32.570], mean action: 3.731 [0.000, 14.000],  loss: 0.024227, mae: 0.334681, mean_q: 0.569977, mean_eps: 0.000000
 4237/5000: episode: 161, duration: 0.269s, episode steps:  18, steps per second:  67, episode reward: 44.586, mean reward:  2.477 [-2.165, 32.670], mean action: 2.056 [0.000, 12.000],  loss: 0.016387, mae: 0.303033, mean_q: 0.572917, mean_eps: 0.000000
 4262/5000: episode: 162, duration: 0.369s, episode steps:  25, steps per second:  68, episode reward: 44.820, mean reward:  1.793 [-3.000, 32.340], mean action: 0.320 [0.000, 6.000],  loss: 0.020879, mae: 0.320604, mean_q: 0.528495, mean_eps: 0.000000
 4280/5000: episode: 163, duration: 0.325s, episode steps:  18, steps per second:  55, episode reward: 41.611, mean reward:  2.312 [-2.801, 32.470], mean action: 2.278 [0.000, 12.000],  loss: 0.018123, mae: 0.311694, mean_q: 0.538276, mean_eps: 0.000000
 4303/5000: episode: 164, duration: 0.371s, episode steps:  23, steps per second:  62, episode reward: 42.000, mean reward:  1.826 [-2.253, 33.578], mean action: 2.174 [1.000, 8.000],  loss: 0.020262, mae: 0.321395, mean_q: 0.514666, mean_eps: 0.000000
 4336/5000: episode: 165, duration: 0.468s, episode steps:  33, steps per second:  71, episode reward: 44.309, mean reward:  1.343 [-2.067, 32.201], mean action: 1.909 [0.000, 8.000],  loss: 0.020132, mae: 0.324062, mean_q: 0.585621, mean_eps: 0.000000
 4398/5000: episode: 166, duration: 0.849s, episode steps:  62, steps per second:  73, episode reward: -32.050, mean reward: -0.517 [-32.316,  2.901], mean action: 2.855 [0.000, 15.000],  loss: 0.022351, mae: 0.332917, mean_q: 0.532775, mean_eps: 0.000000
 4417/5000: episode: 167, duration: 0.295s, episode steps:  19, steps per second:  64, episode reward: 44.363, mean reward:  2.335 [-2.110, 32.280], mean action: 3.105 [0.000, 15.000],  loss: 0.018503, mae: 0.332815, mean_q: 0.589486, mean_eps: 0.000000
 4437/5000: episode: 168, duration: 0.298s, episode steps:  20, steps per second:  67, episode reward: 41.300, mean reward:  2.065 [-2.189, 32.053], mean action: 4.950 [0.000, 15.000],  loss: 0.020598, mae: 0.327621, mean_q: 0.548600, mean_eps: 0.000000
 4453/5000: episode: 169, duration: 0.244s, episode steps:  16, steps per second:  65, episode reward: 43.509, mean reward:  2.719 [-2.131, 32.263], mean action: 3.312 [0.000, 16.000],  loss: 0.022326, mae: 0.337895, mean_q: 0.522272, mean_eps: 0.000000
 4477/5000: episode: 170, duration: 0.359s, episode steps:  24, steps per second:  67, episode reward: 43.836, mean reward:  1.827 [-2.325, 32.903], mean action: 4.292 [0.000, 15.000],  loss: 0.023265, mae: 0.339797, mean_q: 0.490002, mean_eps: 0.000000
 4512/5000: episode: 171, duration: 0.489s, episode steps:  35, steps per second:  72, episode reward: 35.401, mean reward:  1.011 [-2.968, 32.270], mean action: 5.343 [0.000, 15.000],  loss: 0.020779, mae: 0.324359, mean_q: 0.545663, mean_eps: 0.000000
 4543/5000: episode: 172, duration: 0.439s, episode steps:  31, steps per second:  71, episode reward: 32.875, mean reward:  1.060 [-2.500, 32.210], mean action: 6.226 [0.000, 15.000],  loss: 0.019827, mae: 0.319717, mean_q: 0.564557, mean_eps: 0.000000
 4576/5000: episode: 173, duration: 0.476s, episode steps:  33, steps per second:  69, episode reward: 33.000, mean reward:  1.000 [-3.000, 32.130], mean action: 4.333 [0.000, 16.000],  loss: 0.023001, mae: 0.336284, mean_q: 0.576879, mean_eps: 0.000000
 4601/5000: episode: 174, duration: 0.370s, episode steps:  25, steps per second:  68, episode reward: 41.135, mean reward:  1.645 [-3.000, 32.370], mean action: 3.040 [1.000, 16.000],  loss: 0.023615, mae: 0.338620, mean_q: 0.555289, mean_eps: 0.000000
 4624/5000: episode: 175, duration: 0.333s, episode steps:  23, steps per second:  69, episode reward: -32.040, mean reward: -1.393 [-32.271,  3.000], mean action: 3.435 [0.000, 19.000],  loss: 0.025580, mae: 0.341443, mean_q: 0.544338, mean_eps: 0.000000
 4648/5000: episode: 176, duration: 0.343s, episode steps:  24, steps per second:  70, episode reward: 43.879, mean reward:  1.828 [-2.176, 32.220], mean action: 2.500 [0.000, 12.000],  loss: 0.024385, mae: 0.336756, mean_q: 0.537137, mean_eps: 0.000000
 4670/5000: episode: 177, duration: 0.317s, episode steps:  22, steps per second:  69, episode reward: 38.450, mean reward:  1.748 [-3.000, 32.290], mean action: 3.909 [0.000, 19.000],  loss: 0.023893, mae: 0.337445, mean_q: 0.581598, mean_eps: 0.000000
 4711/5000: episode: 178, duration: 0.580s, episode steps:  41, steps per second:  71, episode reward: 35.632, mean reward:  0.869 [-3.000, 32.220], mean action: 3.805 [0.000, 19.000],  loss: 0.022672, mae: 0.333511, mean_q: 0.534464, mean_eps: 0.000000
 4731/5000: episode: 179, duration: 0.302s, episode steps:  20, steps per second:  66, episode reward: 44.562, mean reward:  2.228 [-2.901, 31.863], mean action: 3.300 [0.000, 19.000],  loss: 0.021892, mae: 0.331134, mean_q: 0.564486, mean_eps: 0.000000
 4758/5000: episode: 180, duration: 0.390s, episode steps:  27, steps per second:  69, episode reward: 41.808, mean reward:  1.548 [-2.751, 32.044], mean action: 2.741 [0.000, 19.000],  loss: 0.018663, mae: 0.312332, mean_q: 0.575071, mean_eps: 0.000000
 4786/5000: episode: 181, duration: 0.397s, episode steps:  28, steps per second:  71, episode reward: 38.600, mean reward:  1.379 [-2.330, 31.930], mean action: 3.393 [0.000, 19.000],  loss: 0.019968, mae: 0.314358, mean_q: 0.534315, mean_eps: 0.000000
 4807/5000: episode: 182, duration: 0.311s, episode steps:  21, steps per second:  68, episode reward: 43.786, mean reward:  2.085 [-2.233, 31.883], mean action: 3.952 [1.000, 12.000],  loss: 0.020132, mae: 0.313097, mean_q: 0.545365, mean_eps: 0.000000
 4825/5000: episode: 183, duration: 0.273s, episode steps:  18, steps per second:  66, episode reward: 41.630, mean reward:  2.313 [-2.610, 32.587], mean action: 2.889 [0.000, 12.000],  loss: 0.022010, mae: 0.324506, mean_q: 0.539296, mean_eps: 0.000000
 4859/5000: episode: 184, duration: 0.491s, episode steps:  34, steps per second:  69, episode reward: 35.790, mean reward:  1.053 [-3.000, 31.971], mean action: 4.088 [0.000, 16.000],  loss: 0.017128, mae: 0.306013, mean_q: 0.577137, mean_eps: 0.000000
 4878/5000: episode: 185, duration: 0.285s, episode steps:  19, steps per second:  67, episode reward: 43.774, mean reward:  2.304 [-2.992, 32.300], mean action: 3.053 [0.000, 12.000],  loss: 0.026668, mae: 0.344691, mean_q: 0.563971, mean_eps: 0.000000
 4900/5000: episode: 186, duration: 0.330s, episode steps:  22, steps per second:  67, episode reward: 46.815, mean reward:  2.128 [-0.484, 32.160], mean action: 2.364 [0.000, 12.000],  loss: 0.020274, mae: 0.314371, mean_q: 0.572891, mean_eps: 0.000000
 4917/5000: episode: 187, duration: 0.255s, episode steps:  17, steps per second:  67, episode reward: 44.294, mean reward:  2.606 [-2.181, 31.768], mean action: 1.059 [0.000, 11.000],  loss: 0.022784, mae: 0.324510, mean_q: 0.636743, mean_eps: 0.000000
 4942/5000: episode: 188, duration: 0.364s, episode steps:  25, steps per second:  69, episode reward: 41.144, mean reward:  1.646 [-2.160, 32.370], mean action: 2.720 [0.000, 16.000],  loss: 0.024195, mae: 0.336996, mean_q: 0.647342, mean_eps: 0.000000
 4973/5000: episode: 189, duration: 0.460s, episode steps:  31, steps per second:  67, episode reward: 44.753, mean reward:  1.444 [-2.554, 32.410], mean action: 2.452 [0.000, 3.000],  loss: 0.020632, mae: 0.320072, mean_q: 0.583727, mean_eps: 0.000000
 4994/5000: episode: 190, duration: 0.312s, episode steps:  21, steps per second:  67, episode reward: 41.623, mean reward:  1.982 [-2.506, 31.885], mean action: 2.286 [0.000, 20.000],  loss: 0.021655, mae: 0.327948, mean_q: 0.557630, mean_eps: 0.000000
done, took 72.149 seconds
DQN Evaluation: 10426 victories out of 12163 episodes
Training for 5000 steps ...
   20/5000: episode: 1, duration: 0.156s, episode steps:  20, steps per second: 128, episode reward: -32.480, mean reward: -1.624 [-32.192,  2.556], mean action: 4.350 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   42/5000: episode: 2, duration: 0.163s, episode steps:  22, steps per second: 135, episode reward: 35.132, mean reward:  1.597 [-2.154, 32.060], mean action: 2.818 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   57/5000: episode: 3, duration: 0.115s, episode steps:  15, steps per second: 130, episode reward: 35.903, mean reward:  2.394 [-3.000, 32.163], mean action: 4.933 [1.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   92/5000: episode: 4, duration: 0.239s, episode steps:  35, steps per second: 147, episode reward: -41.380, mean reward: -1.182 [-32.634,  2.070], mean action: 6.371 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  111/5000: episode: 5, duration: 0.135s, episode steps:  19, steps per second: 140, episode reward: 37.934, mean reward:  1.997 [-2.325, 32.902], mean action: 1.789 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  138/5000: episode: 6, duration: 0.183s, episode steps:  27, steps per second: 147, episode reward: 35.612, mean reward:  1.319 [-2.436, 32.396], mean action: 2.778 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  166/5000: episode: 7, duration: 0.190s, episode steps:  28, steps per second: 147, episode reward: -32.610, mean reward: -1.165 [-32.635,  2.853], mean action: 4.607 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  186/5000: episode: 8, duration: 0.145s, episode steps:  20, steps per second: 138, episode reward: 38.302, mean reward:  1.915 [-2.398, 32.320], mean action: 3.300 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  202/5000: episode: 9, duration: 0.119s, episode steps:  16, steps per second: 134, episode reward: 41.203, mean reward:  2.575 [-2.157, 32.403], mean action: 0.812 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  215/5000: episode: 10, duration: 0.108s, episode steps:  13, steps per second: 121, episode reward: 41.262, mean reward:  3.174 [-2.550, 32.835], mean action: 3.462 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  239/5000: episode: 11, duration: 0.163s, episode steps:  24, steps per second: 148, episode reward: 35.708, mean reward:  1.488 [-3.000, 31.788], mean action: 4.083 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  257/5000: episode: 12, duration: 0.133s, episode steps:  18, steps per second: 136, episode reward: 39.000, mean reward:  2.167 [-2.558, 33.000], mean action: 3.444 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  276/5000: episode: 13, duration: 0.139s, episode steps:  19, steps per second: 137, episode reward: 38.840, mean reward:  2.044 [-3.000, 32.006], mean action: 2.474 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  299/5000: episode: 14, duration: 0.167s, episode steps:  23, steps per second: 138, episode reward: 38.052, mean reward:  1.654 [-2.255, 32.290], mean action: 4.609 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  326/5000: episode: 15, duration: 0.166s, episode steps:  27, steps per second: 162, episode reward: 32.458, mean reward:  1.202 [-3.000, 32.185], mean action: 4.704 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  347/5000: episode: 16, duration: 0.149s, episode steps:  21, steps per second: 141, episode reward: 37.844, mean reward:  1.802 [-2.366, 32.030], mean action: 2.714 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  370/5000: episode: 17, duration: 0.166s, episode steps:  23, steps per second: 139, episode reward: 42.000, mean reward:  1.826 [-2.902, 32.190], mean action: 1.217 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  397/5000: episode: 18, duration: 0.178s, episode steps:  27, steps per second: 152, episode reward: 32.642, mean reward:  1.209 [-2.855, 32.190], mean action: 11.333 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  425/5000: episode: 19, duration: 0.206s, episode steps:  28, steps per second: 136, episode reward: -32.410, mean reward: -1.157 [-32.189,  2.440], mean action: 4.786 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  468/5000: episode: 20, duration: 0.255s, episode steps:  43, steps per second: 169, episode reward: 33.000, mean reward:  0.767 [-2.626, 33.000], mean action: 5.372 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  480/5000: episode: 21, duration: 0.100s, episode steps:  12, steps per second: 120, episode reward: 41.103, mean reward:  3.425 [-2.323, 31.621], mean action: 1.250 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  501/5000: episode: 22, duration: 0.147s, episode steps:  21, steps per second: 143, episode reward: 38.337, mean reward:  1.826 [-2.330, 32.210], mean action: 2.952 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  517/5000: episode: 23, duration: 0.119s, episode steps:  16, steps per second: 135, episode reward: 45.000, mean reward:  2.812 [-2.424, 32.260], mean action: 1.875 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  538/5000: episode: 24, duration: 0.139s, episode steps:  21, steps per second: 151, episode reward: -35.860, mean reward: -1.708 [-31.976,  2.770], mean action: 5.476 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  560/5000: episode: 25, duration: 0.149s, episode steps:  22, steps per second: 148, episode reward: 35.160, mean reward:  1.598 [-2.649, 32.350], mean action: 3.091 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  579/5000: episode: 26, duration: 0.137s, episode steps:  19, steps per second: 138, episode reward: 36.000, mean reward:  1.895 [-3.000, 32.100], mean action: 4.211 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  602/5000: episode: 27, duration: 0.143s, episode steps:  23, steps per second: 161, episode reward: 32.903, mean reward:  1.431 [-3.000, 32.193], mean action: 4.261 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  613/5000: episode: 28, duration: 0.080s, episode steps:  11, steps per second: 137, episode reward: 41.053, mean reward:  3.732 [-2.903, 31.535], mean action: 3.545 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  628/5000: episode: 29, duration: 0.113s, episode steps:  15, steps per second: 133, episode reward: 41.067, mean reward:  2.738 [-2.522, 33.000], mean action: 2.867 [1.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  650/5000: episode: 30, duration: 0.146s, episode steps:  22, steps per second: 151, episode reward: 38.138, mean reward:  1.734 [-2.391, 32.068], mean action: 1.955 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  671/5000: episode: 31, duration: 0.158s, episode steps:  21, steps per second: 133, episode reward: 35.247, mean reward:  1.678 [-3.000, 32.247], mean action: 3.143 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  694/5000: episode: 32, duration: 0.157s, episode steps:  23, steps per second: 147, episode reward: -35.220, mean reward: -1.531 [-32.900,  2.550], mean action: 4.913 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  719/5000: episode: 33, duration: 0.164s, episode steps:  25, steps per second: 153, episode reward: -32.340, mean reward: -1.294 [-32.148,  3.016], mean action: 3.760 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  738/5000: episode: 34, duration: 0.127s, episode steps:  19, steps per second: 150, episode reward: -38.170, mean reward: -2.009 [-31.699,  2.220], mean action: 6.842 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  770/5000: episode: 35, duration: 0.210s, episode steps:  32, steps per second: 152, episode reward: 35.463, mean reward:  1.108 [-3.000, 32.215], mean action: 5.938 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  796/5000: episode: 36, duration: 0.173s, episode steps:  26, steps per second: 150, episode reward: 38.808, mean reward:  1.493 [-2.335, 32.100], mean action: 3.654 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  812/5000: episode: 37, duration: 0.116s, episode steps:  16, steps per second: 138, episode reward: 41.653, mean reward:  2.603 [-2.473, 32.168], mean action: 1.375 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  838/5000: episode: 38, duration: 0.169s, episode steps:  26, steps per second: 154, episode reward: 33.000, mean reward:  1.269 [-3.000, 32.450], mean action: 4.462 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  858/5000: episode: 39, duration: 0.145s, episode steps:  20, steps per second: 138, episode reward: 38.187, mean reward:  1.909 [-2.903, 32.697], mean action: 5.150 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  876/5000: episode: 40, duration: 0.135s, episode steps:  18, steps per second: 134, episode reward: 38.319, mean reward:  2.129 [-2.453, 31.979], mean action: 1.278 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  896/5000: episode: 41, duration: 0.146s, episode steps:  20, steps per second: 137, episode reward: 36.000, mean reward:  1.800 [-2.482, 32.280], mean action: 2.100 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  916/5000: episode: 42, duration: 0.292s, episode steps:  20, steps per second:  69, episode reward: 32.325, mean reward:  1.616 [-3.000, 32.410], mean action: 3.800 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  933/5000: episode: 43, duration: 0.221s, episode steps:  17, steps per second:  77, episode reward: 43.293, mean reward:  2.547 [-2.362, 32.086], mean action: 3.765 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  953/5000: episode: 44, duration: 0.146s, episode steps:  20, steps per second: 137, episode reward: 36.000, mean reward:  1.800 [-2.796, 30.473], mean action: 3.800 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  970/5000: episode: 45, duration: 0.132s, episode steps:  17, steps per second: 129, episode reward: 36.000, mean reward:  2.118 [-2.589, 30.583], mean action: 3.294 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  986/5000: episode: 46, duration: 0.120s, episode steps:  16, steps per second: 134, episode reward: 38.860, mean reward:  2.429 [-2.191, 32.200], mean action: 2.375 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1008/5000: episode: 47, duration: 0.212s, episode steps:  22, steps per second: 104, episode reward: -33.000, mean reward: -1.500 [-32.019,  2.140], mean action: 5.091 [0.000, 14.000],  loss: 0.017858, mae: 0.321647, mean_q: 0.592815, mean_eps: 0.000000
 1030/5000: episode: 48, duration: 0.320s, episode steps:  22, steps per second:  69, episode reward: 39.000, mean reward:  1.773 [-2.458, 32.940], mean action: 5.455 [0.000, 20.000],  loss: 0.023084, mae: 0.331602, mean_q: 0.529220, mean_eps: 0.000000
 1044/5000: episode: 49, duration: 0.212s, episode steps:  14, steps per second:  66, episode reward: 41.903, mean reward:  2.993 [-2.615, 32.053], mean action: 3.857 [0.000, 11.000],  loss: 0.027040, mae: 0.352874, mean_q: 0.494268, mean_eps: 0.000000
 1076/5000: episode: 50, duration: 0.442s, episode steps:  32, steps per second:  72, episode reward: -41.580, mean reward: -1.299 [-32.291,  2.900], mean action: 3.125 [0.000, 12.000],  loss: 0.023015, mae: 0.322830, mean_q: 0.581685, mean_eps: 0.000000
 1110/5000: episode: 51, duration: 0.477s, episode steps:  34, steps per second:  71, episode reward: 33.000, mean reward:  0.971 [-3.000, 32.320], mean action: 3.912 [0.000, 14.000],  loss: 0.021703, mae: 0.322812, mean_q: 0.610236, mean_eps: 0.000000
 1130/5000: episode: 52, duration: 0.294s, episode steps:  20, steps per second:  68, episode reward: 32.479, mean reward:  1.624 [-2.887, 30.951], mean action: 5.150 [0.000, 12.000],  loss: 0.020688, mae: 0.315147, mean_q: 0.566107, mean_eps: 0.000000
 1160/5000: episode: 53, duration: 0.425s, episode steps:  30, steps per second:  71, episode reward: 33.000, mean reward:  1.100 [-2.463, 32.420], mean action: 5.500 [0.000, 20.000],  loss: 0.021723, mae: 0.325658, mean_q: 0.615830, mean_eps: 0.000000
 1177/5000: episode: 54, duration: 0.260s, episode steps:  17, steps per second:  65, episode reward: 36.000, mean reward:  2.118 [-2.711, 29.833], mean action: 4.353 [0.000, 15.000],  loss: 0.023027, mae: 0.329919, mean_q: 0.583844, mean_eps: 0.000000
 1195/5000: episode: 55, duration: 0.280s, episode steps:  18, steps per second:  64, episode reward: 44.689, mean reward:  2.483 [-2.004, 32.320], mean action: 1.722 [0.000, 15.000],  loss: 0.020361, mae: 0.315771, mean_q: 0.607231, mean_eps: 0.000000
 1213/5000: episode: 56, duration: 0.274s, episode steps:  18, steps per second:  66, episode reward: 47.212, mean reward:  2.623 [ 0.079, 32.049], mean action: 0.278 [0.000, 1.000],  loss: 0.020418, mae: 0.322888, mean_q: 0.615457, mean_eps: 0.000000
 1229/5000: episode: 57, duration: 0.238s, episode steps:  16, steps per second:  67, episode reward: 38.404, mean reward:  2.400 [-2.799, 32.240], mean action: 4.125 [0.000, 15.000],  loss: 0.014780, mae: 0.293697, mean_q: 0.637953, mean_eps: 0.000000
 1251/5000: episode: 58, duration: 0.323s, episode steps:  22, steps per second:  68, episode reward: 39.000, mean reward:  1.773 [-2.170, 30.883], mean action: 3.273 [0.000, 19.000],  loss: 0.022404, mae: 0.329854, mean_q: 0.576597, mean_eps: 0.000000
 1275/5000: episode: 59, duration: 0.344s, episode steps:  24, steps per second:  70, episode reward: 35.740, mean reward:  1.489 [-3.000, 32.140], mean action: 4.250 [0.000, 15.000],  loss: 0.017771, mae: 0.303048, mean_q: 0.607495, mean_eps: 0.000000
 1300/5000: episode: 60, duration: 0.370s, episode steps:  25, steps per second:  67, episode reward: -32.420, mean reward: -1.297 [-32.631,  2.455], mean action: 7.880 [0.000, 19.000],  loss: 0.017940, mae: 0.308617, mean_q: 0.621351, mean_eps: 0.000000
 1322/5000: episode: 61, duration: 0.326s, episode steps:  22, steps per second:  68, episode reward: 35.348, mean reward:  1.607 [-3.000, 32.300], mean action: 4.182 [0.000, 15.000],  loss: 0.017173, mae: 0.290541, mean_q: 0.527324, mean_eps: 0.000000
 1353/5000: episode: 62, duration: 0.442s, episode steps:  31, steps per second:  70, episode reward: 35.624, mean reward:  1.149 [-2.309, 32.880], mean action: 3.935 [0.000, 19.000],  loss: 0.020385, mae: 0.317056, mean_q: 0.521693, mean_eps: 0.000000
 1381/5000: episode: 63, duration: 0.397s, episode steps:  28, steps per second:  70, episode reward: 38.857, mean reward:  1.388 [-2.253, 32.549], mean action: 3.607 [0.000, 16.000],  loss: 0.019822, mae: 0.313571, mean_q: 0.532227, mean_eps: 0.000000
 1397/5000: episode: 64, duration: 0.247s, episode steps:  16, steps per second:  65, episode reward: 47.960, mean reward:  2.998 [ 0.304, 32.295], mean action: 1.250 [1.000, 2.000],  loss: 0.021822, mae: 0.324171, mean_q: 0.538757, mean_eps: 0.000000
 1414/5000: episode: 65, duration: 0.254s, episode steps:  17, steps per second:  67, episode reward: 41.550, mean reward:  2.444 [-3.000, 32.220], mean action: 2.647 [1.000, 16.000],  loss: 0.020809, mae: 0.316206, mean_q: 0.551025, mean_eps: 0.000000
 1430/5000: episode: 66, duration: 0.236s, episode steps:  16, steps per second:  68, episode reward: -41.780, mean reward: -2.611 [-32.900,  2.018], mean action: 5.062 [0.000, 16.000],  loss: 0.019013, mae: 0.300570, mean_q: 0.533880, mean_eps: 0.000000
 1451/5000: episode: 67, duration: 0.307s, episode steps:  21, steps per second:  68, episode reward: 43.991, mean reward:  2.095 [-2.110, 32.230], mean action: 3.190 [1.000, 14.000],  loss: 0.021224, mae: 0.315535, mean_q: 0.516058, mean_eps: 0.000000
 1468/5000: episode: 68, duration: 0.257s, episode steps:  17, steps per second:  66, episode reward: 44.229, mean reward:  2.602 [-2.565, 32.345], mean action: 0.941 [0.000, 6.000],  loss: 0.021270, mae: 0.320435, mean_q: 0.539262, mean_eps: 0.000000
 1488/5000: episode: 69, duration: 0.293s, episode steps:  20, steps per second:  68, episode reward: 38.207, mean reward:  1.910 [-2.815, 32.420], mean action: 3.850 [0.000, 14.000],  loss: 0.024972, mae: 0.337180, mean_q: 0.516420, mean_eps: 0.000000
 1511/5000: episode: 70, duration: 0.335s, episode steps:  23, steps per second:  69, episode reward: 38.890, mean reward:  1.691 [-2.242, 32.240], mean action: 4.870 [0.000, 14.000],  loss: 0.021372, mae: 0.314549, mean_q: 0.471325, mean_eps: 0.000000
 1543/5000: episode: 71, duration: 0.455s, episode steps:  32, steps per second:  70, episode reward: -32.600, mean reward: -1.019 [-31.842,  2.940], mean action: 6.562 [0.000, 16.000],  loss: 0.023396, mae: 0.324255, mean_q: 0.523728, mean_eps: 0.000000
 1566/5000: episode: 72, duration: 0.338s, episode steps:  23, steps per second:  68, episode reward: 33.000, mean reward:  1.435 [-2.901, 30.067], mean action: 3.478 [0.000, 15.000],  loss: 0.022244, mae: 0.315549, mean_q: 0.589753, mean_eps: 0.000000
 1590/5000: episode: 73, duration: 0.347s, episode steps:  24, steps per second:  69, episode reward: 41.215, mean reward:  1.717 [-2.332, 32.520], mean action: 1.125 [0.000, 6.000],  loss: 0.020065, mae: 0.316168, mean_q: 0.611987, mean_eps: 0.000000
 1619/5000: episode: 74, duration: 0.422s, episode steps:  29, steps per second:  69, episode reward: 32.559, mean reward:  1.123 [-3.000, 32.006], mean action: 6.138 [0.000, 19.000],  loss: 0.019440, mae: 0.308215, mean_q: 0.533384, mean_eps: 0.000000
 1642/5000: episode: 75, duration: 0.336s, episode steps:  23, steps per second:  68, episode reward: -36.000, mean reward: -1.565 [-32.232,  2.420], mean action: 3.130 [0.000, 15.000],  loss: 0.022959, mae: 0.329691, mean_q: 0.519395, mean_eps: 0.000000
 1669/5000: episode: 76, duration: 0.379s, episode steps:  27, steps per second:  71, episode reward: -39.000, mean reward: -1.444 [-30.800,  1.937], mean action: 7.148 [0.000, 15.000],  loss: 0.022570, mae: 0.322592, mean_q: 0.535780, mean_eps: 0.000000
 1686/5000: episode: 77, duration: 0.258s, episode steps:  17, steps per second:  66, episode reward: 36.000, mean reward:  2.118 [-3.000, 32.210], mean action: 6.294 [0.000, 16.000],  loss: 0.019240, mae: 0.313118, mean_q: 0.567837, mean_eps: 0.000000
 1709/5000: episode: 78, duration: 0.336s, episode steps:  23, steps per second:  68, episode reward: -35.370, mean reward: -1.538 [-32.041,  2.283], mean action: 5.783 [0.000, 16.000],  loss: 0.020121, mae: 0.315895, mean_q: 0.522162, mean_eps: 0.000000
 1729/5000: episode: 79, duration: 0.290s, episode steps:  20, steps per second:  69, episode reward: 35.548, mean reward:  1.777 [-2.834, 32.203], mean action: 4.600 [0.000, 16.000],  loss: 0.021301, mae: 0.322412, mean_q: 0.541085, mean_eps: 0.000000
 1750/5000: episode: 80, duration: 0.319s, episode steps:  21, steps per second:  66, episode reward: -38.040, mean reward: -1.811 [-32.116,  2.940], mean action: 7.381 [0.000, 19.000],  loss: 0.025130, mae: 0.339900, mean_q: 0.565690, mean_eps: 0.000000
 1782/5000: episode: 81, duration: 0.451s, episode steps:  32, steps per second:  71, episode reward: -32.080, mean reward: -1.002 [-31.841,  2.300], mean action: 6.312 [0.000, 16.000],  loss: 0.020480, mae: 0.312675, mean_q: 0.557045, mean_eps: 0.000000
 1803/5000: episode: 82, duration: 0.306s, episode steps:  21, steps per second:  69, episode reward: -35.300, mean reward: -1.681 [-32.102,  2.810], mean action: 6.048 [0.000, 19.000],  loss: 0.019559, mae: 0.313965, mean_q: 0.575429, mean_eps: 0.000000
 1828/5000: episode: 83, duration: 0.360s, episode steps:  25, steps per second:  69, episode reward: 38.593, mean reward:  1.544 [-2.672, 31.957], mean action: 6.200 [0.000, 19.000],  loss: 0.019292, mae: 0.313216, mean_q: 0.553764, mean_eps: 0.000000
 1856/5000: episode: 84, duration: 0.400s, episode steps:  28, steps per second:  70, episode reward: 32.902, mean reward:  1.175 [-2.368, 32.242], mean action: 7.643 [0.000, 19.000],  loss: 0.020460, mae: 0.315488, mean_q: 0.594537, mean_eps: 0.000000
 1891/5000: episode: 85, duration: 0.500s, episode steps:  35, steps per second:  70, episode reward: 32.284, mean reward:  0.922 [-2.298, 31.843], mean action: 8.029 [0.000, 20.000],  loss: 0.020081, mae: 0.316002, mean_q: 0.497298, mean_eps: 0.000000
 1924/5000: episode: 86, duration: 0.464s, episode steps:  33, steps per second:  71, episode reward: -32.580, mean reward: -0.987 [-32.127,  3.062], mean action: 8.333 [0.000, 19.000],  loss: 0.019707, mae: 0.315659, mean_q: 0.509437, mean_eps: 0.000000
 1947/5000: episode: 87, duration: 0.330s, episode steps:  23, steps per second:  70, episode reward: 32.710, mean reward:  1.422 [-3.000, 33.000], mean action: 10.565 [2.000, 19.000],  loss: 0.023748, mae: 0.331994, mean_q: 0.519684, mean_eps: 0.000000
 1969/5000: episode: 88, duration: 0.313s, episode steps:  22, steps per second:  70, episode reward: 32.603, mean reward:  1.482 [-2.742, 32.340], mean action: 4.591 [0.000, 19.000],  loss: 0.019001, mae: 0.305125, mean_q: 0.566442, mean_eps: 0.000000
 1980/5000: episode: 89, duration: 0.178s, episode steps:  11, steps per second:  62, episode reward: 41.580, mean reward:  3.780 [-2.452, 32.680], mean action: 5.818 [1.000, 19.000],  loss: 0.020954, mae: 0.312699, mean_q: 0.543645, mean_eps: 0.000000
 2001/5000: episode: 90, duration: 0.301s, episode steps:  21, steps per second:  70, episode reward: 33.000, mean reward:  1.571 [-3.000, 32.730], mean action: 6.762 [0.000, 19.000],  loss: 0.017675, mae: 0.299941, mean_q: 0.514715, mean_eps: 0.000000
 2029/5000: episode: 91, duration: 0.400s, episode steps:  28, steps per second:  70, episode reward: -35.670, mean reward: -1.274 [-31.695,  2.960], mean action: 8.143 [0.000, 19.000],  loss: 0.022176, mae: 0.334248, mean_q: 0.554521, mean_eps: 0.000000
 2040/5000: episode: 92, duration: 0.191s, episode steps:  11, steps per second:  58, episode reward: 47.450, mean reward:  4.314 [-0.186, 32.323], mean action: 3.727 [2.000, 14.000],  loss: 0.017615, mae: 0.304463, mean_q: 0.520347, mean_eps: 0.000000
 2061/5000: episode: 93, duration: 0.304s, episode steps:  21, steps per second:  69, episode reward: 34.995, mean reward:  1.666 [-2.806, 31.913], mean action: 5.571 [0.000, 19.000],  loss: 0.021664, mae: 0.319674, mean_q: 0.561341, mean_eps: 0.000000
 2081/5000: episode: 94, duration: 0.301s, episode steps:  20, steps per second:  66, episode reward: 38.030, mean reward:  1.902 [-2.639, 32.166], mean action: 4.050 [0.000, 14.000],  loss: 0.021199, mae: 0.321506, mean_q: 0.565119, mean_eps: 0.000000
 2095/5000: episode: 95, duration: 0.210s, episode steps:  14, steps per second:  67, episode reward: 45.000, mean reward:  3.214 [-2.280, 33.000], mean action: 2.214 [0.000, 12.000],  loss: 0.019198, mae: 0.307362, mean_q: 0.543679, mean_eps: 0.000000
 2127/5000: episode: 96, duration: 0.449s, episode steps:  32, steps per second:  71, episode reward: -32.420, mean reward: -1.013 [-32.109,  2.806], mean action: 6.719 [0.000, 17.000],  loss: 0.021112, mae: 0.322128, mean_q: 0.572745, mean_eps: 0.000000
 2158/5000: episode: 97, duration: 0.447s, episode steps:  31, steps per second:  69, episode reward: -32.910, mean reward: -1.062 [-31.990,  2.390], mean action: 6.097 [0.000, 19.000],  loss: 0.022222, mae: 0.319669, mean_q: 0.541345, mean_eps: 0.000000
 2177/5000: episode: 98, duration: 0.284s, episode steps:  19, steps per second:  67, episode reward: 38.260, mean reward:  2.014 [-2.274, 31.936], mean action: 3.474 [0.000, 15.000],  loss: 0.018860, mae: 0.301080, mean_q: 0.570606, mean_eps: 0.000000
 2200/5000: episode: 99, duration: 0.340s, episode steps:  23, steps per second:  68, episode reward: 35.605, mean reward:  1.548 [-2.674, 31.998], mean action: 5.174 [0.000, 15.000],  loss: 0.022570, mae: 0.321207, mean_q: 0.559609, mean_eps: 0.000000
 2226/5000: episode: 100, duration: 0.371s, episode steps:  26, steps per second:  70, episode reward: 32.136, mean reward:  1.236 [-3.000, 32.853], mean action: 6.038 [0.000, 15.000],  loss: 0.024518, mae: 0.323931, mean_q: 0.541431, mean_eps: 0.000000
 2251/5000: episode: 101, duration: 0.358s, episode steps:  25, steps per second:  70, episode reward: 42.000, mean reward:  1.680 [-2.238, 29.952], mean action: 2.720 [0.000, 6.000],  loss: 0.019937, mae: 0.308893, mean_q: 0.561297, mean_eps: 0.000000
 2273/5000: episode: 102, duration: 0.315s, episode steps:  22, steps per second:  70, episode reward: 38.924, mean reward:  1.769 [-2.643, 32.963], mean action: 5.636 [2.000, 19.000],  loss: 0.019817, mae: 0.307979, mean_q: 0.512414, mean_eps: 0.000000
 2293/5000: episode: 103, duration: 0.298s, episode steps:  20, steps per second:  67, episode reward: 38.903, mean reward:  1.945 [-2.436, 32.903], mean action: 3.950 [0.000, 12.000],  loss: 0.020105, mae: 0.305560, mean_q: 0.565462, mean_eps: 0.000000
 2316/5000: episode: 104, duration: 0.331s, episode steps:  23, steps per second:  70, episode reward: 32.837, mean reward:  1.428 [-3.000, 32.167], mean action: 4.696 [0.000, 18.000],  loss: 0.022394, mae: 0.317949, mean_q: 0.586845, mean_eps: 0.000000
 2347/5000: episode: 105, duration: 0.438s, episode steps:  31, steps per second:  71, episode reward: -32.910, mean reward: -1.062 [-32.323,  2.295], mean action: 4.839 [0.000, 14.000],  loss: 0.021068, mae: 0.312030, mean_q: 0.532434, mean_eps: 0.000000
 2362/5000: episode: 106, duration: 0.237s, episode steps:  15, steps per second:  63, episode reward: 41.109, mean reward:  2.741 [-2.332, 32.829], mean action: 2.800 [0.000, 11.000],  loss: 0.023425, mae: 0.322265, mean_q: 0.537758, mean_eps: 0.000000
 2386/5000: episode: 107, duration: 0.366s, episode steps:  24, steps per second:  66, episode reward: 38.210, mean reward:  1.592 [-2.757, 31.872], mean action: 2.917 [0.000, 12.000],  loss: 0.018063, mae: 0.306707, mean_q: 0.572175, mean_eps: 0.000000
 2413/5000: episode: 108, duration: 0.411s, episode steps:  27, steps per second:  66, episode reward: 35.513, mean reward:  1.315 [-2.565, 32.031], mean action: 4.037 [0.000, 18.000],  loss: 0.020996, mae: 0.318876, mean_q: 0.556786, mean_eps: 0.000000
 2453/5000: episode: 109, duration: 0.563s, episode steps:  40, steps per second:  71, episode reward: -32.640, mean reward: -0.816 [-31.974,  2.270], mean action: 7.200 [0.000, 18.000],  loss: 0.020504, mae: 0.317538, mean_q: 0.545412, mean_eps: 0.000000
 2467/5000: episode: 110, duration: 0.212s, episode steps:  14, steps per second:  66, episode reward: 38.206, mean reward:  2.729 [-2.099, 32.903], mean action: 3.643 [0.000, 11.000],  loss: 0.019950, mae: 0.318721, mean_q: 0.542278, mean_eps: 0.000000
 2493/5000: episode: 111, duration: 0.379s, episode steps:  26, steps per second:  69, episode reward: 32.530, mean reward:  1.251 [-2.389, 31.950], mean action: 6.538 [0.000, 20.000],  loss: 0.020460, mae: 0.322508, mean_q: 0.576627, mean_eps: 0.000000
 2543/5000: episode: 112, duration: 0.699s, episode steps:  50, steps per second:  72, episode reward: -36.000, mean reward: -0.720 [-32.198,  2.601], mean action: 2.180 [0.000, 15.000],  loss: 0.019876, mae: 0.320749, mean_q: 0.569610, mean_eps: 0.000000
 2567/5000: episode: 113, duration: 0.352s, episode steps:  24, steps per second:  68, episode reward: 38.234, mean reward:  1.593 [-3.000, 31.849], mean action: 3.375 [0.000, 16.000],  loss: 0.026592, mae: 0.351381, mean_q: 0.565311, mean_eps: 0.000000
 2595/5000: episode: 114, duration: 0.401s, episode steps:  28, steps per second:  70, episode reward: 42.000, mean reward:  1.500 [-2.559, 32.250], mean action: 3.893 [0.000, 16.000],  loss: 0.024530, mae: 0.342503, mean_q: 0.614405, mean_eps: 0.000000
 2609/5000: episode: 115, duration: 0.213s, episode steps:  14, steps per second:  66, episode reward: 43.887, mean reward:  3.135 [-2.061, 32.804], mean action: 3.429 [0.000, 13.000],  loss: 0.018240, mae: 0.305537, mean_q: 0.590229, mean_eps: 0.000000
 2631/5000: episode: 116, duration: 0.332s, episode steps:  22, steps per second:  66, episode reward: -32.430, mean reward: -1.474 [-32.189,  2.450], mean action: 5.000 [0.000, 16.000],  loss: 0.015502, mae: 0.301148, mean_q: 0.528569, mean_eps: 0.000000
 2654/5000: episode: 117, duration: 0.334s, episode steps:  23, steps per second:  69, episode reward: 35.211, mean reward:  1.531 [-2.789, 33.000], mean action: 3.435 [0.000, 16.000],  loss: 0.016232, mae: 0.304859, mean_q: 0.557503, mean_eps: 0.000000
 2683/5000: episode: 118, duration: 0.447s, episode steps:  29, steps per second:  65, episode reward: 32.074, mean reward:  1.106 [-3.000, 32.520], mean action: 8.103 [0.000, 14.000],  loss: 0.017889, mae: 0.315109, mean_q: 0.506979, mean_eps: 0.000000
 2712/5000: episode: 119, duration: 0.428s, episode steps:  29, steps per second:  68, episode reward: 34.621, mean reward:  1.194 [-2.526, 31.736], mean action: 7.241 [0.000, 19.000],  loss: 0.020141, mae: 0.319007, mean_q: 0.560568, mean_eps: 0.000000
 2738/5000: episode: 120, duration: 0.382s, episode steps:  26, steps per second:  68, episode reward: 35.807, mean reward:  1.377 [-2.616, 29.951], mean action: 8.038 [0.000, 19.000],  loss: 0.019070, mae: 0.314025, mean_q: 0.508206, mean_eps: 0.000000
 2766/5000: episode: 121, duration: 0.428s, episode steps:  28, steps per second:  65, episode reward: -32.100, mean reward: -1.146 [-31.673,  2.920], mean action: 5.607 [0.000, 19.000],  loss: 0.020533, mae: 0.321310, mean_q: 0.544623, mean_eps: 0.000000
 2811/5000: episode: 122, duration: 0.719s, episode steps:  45, steps per second:  63, episode reward: 38.485, mean reward:  0.855 [-3.000, 32.012], mean action: 8.956 [2.000, 14.000],  loss: 0.021589, mae: 0.327219, mean_q: 0.525289, mean_eps: 0.000000
 2831/5000: episode: 123, duration: 0.309s, episode steps:  20, steps per second:  65, episode reward: 41.356, mean reward:  2.068 [-2.375, 32.288], mean action: 3.200 [0.000, 12.000],  loss: 0.027624, mae: 0.357388, mean_q: 0.464604, mean_eps: 0.000000
 2865/5000: episode: 124, duration: 0.476s, episode steps:  34, steps per second:  71, episode reward: 32.624, mean reward:  0.960 [-3.000, 32.006], mean action: 10.176 [0.000, 16.000],  loss: 0.019206, mae: 0.316780, mean_q: 0.554232, mean_eps: 0.000000
 2890/5000: episode: 125, duration: 0.353s, episode steps:  25, steps per second:  71, episode reward: 37.582, mean reward:  1.503 [-3.000, 32.101], mean action: 3.280 [0.000, 15.000],  loss: 0.023135, mae: 0.329157, mean_q: 0.519368, mean_eps: 0.000000
 2924/5000: episode: 126, duration: 0.498s, episode steps:  34, steps per second:  68, episode reward: 34.961, mean reward:  1.028 [-2.320, 32.270], mean action: 9.206 [0.000, 15.000],  loss: 0.019788, mae: 0.312840, mean_q: 0.504580, mean_eps: 0.000000
 2959/5000: episode: 127, duration: 0.492s, episode steps:  35, steps per second:  71, episode reward: 32.590, mean reward:  0.931 [-2.226, 32.131], mean action: 6.286 [0.000, 19.000],  loss: 0.020211, mae: 0.320804, mean_q: 0.540177, mean_eps: 0.000000
 2996/5000: episode: 128, duration: 0.519s, episode steps:  37, steps per second:  71, episode reward: 40.557, mean reward:  1.096 [-2.370, 32.090], mean action: 5.000 [0.000, 20.000],  loss: 0.023713, mae: 0.330151, mean_q: 0.548884, mean_eps: 0.000000
 3020/5000: episode: 129, duration: 0.340s, episode steps:  24, steps per second:  71, episode reward: 32.129, mean reward:  1.339 [-3.000, 32.020], mean action: 4.667 [0.000, 15.000],  loss: 0.024363, mae: 0.344705, mean_q: 0.543518, mean_eps: 0.000000
 3030/5000: episode: 130, duration: 0.161s, episode steps:  10, steps per second:  62, episode reward: 45.000, mean reward:  4.500 [-2.005, 32.120], mean action: 2.600 [0.000, 9.000],  loss: 0.022022, mae: 0.336560, mean_q: 0.528180, mean_eps: 0.000000
 3056/5000: episode: 131, duration: 0.374s, episode steps:  26, steps per second:  69, episode reward: 32.266, mean reward:  1.241 [-3.000, 32.460], mean action: 3.385 [0.000, 12.000],  loss: 0.021841, mae: 0.338918, mean_q: 0.614176, mean_eps: 0.000000
 3115/5000: episode: 132, duration: 0.859s, episode steps:  59, steps per second:  69, episode reward: 35.386, mean reward:  0.600 [-2.230, 31.993], mean action: 6.356 [0.000, 15.000],  loss: 0.022492, mae: 0.336780, mean_q: 0.599197, mean_eps: 0.000000
 3149/5000: episode: 133, duration: 0.486s, episode steps:  34, steps per second:  70, episode reward: -32.520, mean reward: -0.956 [-32.104,  2.527], mean action: 5.794 [0.000, 19.000],  loss: 0.020517, mae: 0.331835, mean_q: 0.585380, mean_eps: 0.000000
 3181/5000: episode: 134, duration: 0.450s, episode steps:  32, steps per second:  71, episode reward: 34.810, mean reward:  1.088 [-2.242, 31.690], mean action: 7.594 [0.000, 15.000],  loss: 0.021659, mae: 0.329531, mean_q: 0.522331, mean_eps: 0.000000
 3202/5000: episode: 135, duration: 0.306s, episode steps:  21, steps per second:  69, episode reward: 36.000, mean reward:  1.714 [-2.619, 32.490], mean action: 4.476 [0.000, 15.000],  loss: 0.018721, mae: 0.315366, mean_q: 0.493595, mean_eps: 0.000000
 3234/5000: episode: 136, duration: 0.532s, episode steps:  32, steps per second:  60, episode reward: 33.000, mean reward:  1.031 [-2.285, 32.287], mean action: 9.812 [0.000, 17.000],  loss: 0.021126, mae: 0.332937, mean_q: 0.480631, mean_eps: 0.000000
 3256/5000: episode: 137, duration: 0.320s, episode steps:  22, steps per second:  69, episode reward: 34.737, mean reward:  1.579 [-2.901, 31.781], mean action: 4.909 [0.000, 14.000],  loss: 0.021170, mae: 0.323557, mean_q: 0.556656, mean_eps: 0.000000
 3276/5000: episode: 138, duration: 0.305s, episode steps:  20, steps per second:  66, episode reward: 38.445, mean reward:  1.922 [-3.000, 31.825], mean action: 2.450 [0.000, 15.000],  loss: 0.021074, mae: 0.320635, mean_q: 0.553344, mean_eps: 0.000000
 3307/5000: episode: 139, duration: 0.452s, episode steps:  31, steps per second:  69, episode reward: 32.175, mean reward:  1.038 [-2.496, 32.460], mean action: 6.806 [0.000, 15.000],  loss: 0.018695, mae: 0.311968, mean_q: 0.572457, mean_eps: 0.000000
 3325/5000: episode: 140, duration: 0.266s, episode steps:  18, steps per second:  68, episode reward: 47.938, mean reward:  2.663 [ 0.065, 32.050], mean action: 2.111 [1.000, 3.000],  loss: 0.022328, mae: 0.330447, mean_q: 0.621724, mean_eps: 0.000000
 3337/5000: episode: 141, duration: 0.188s, episode steps:  12, steps per second:  64, episode reward: 44.579, mean reward:  3.715 [-2.043, 33.000], mean action: 2.500 [0.000, 12.000],  loss: 0.023634, mae: 0.337740, mean_q: 0.532580, mean_eps: 0.000000
 3357/5000: episode: 142, duration: 0.290s, episode steps:  20, steps per second:  69, episode reward: 35.211, mean reward:  1.761 [-2.708, 31.948], mean action: 7.450 [0.000, 15.000],  loss: 0.017516, mae: 0.310981, mean_q: 0.567358, mean_eps: 0.000000
 3386/5000: episode: 143, duration: 0.427s, episode steps:  29, steps per second:  68, episode reward: -32.220, mean reward: -1.111 [-32.027,  2.708], mean action: 7.724 [0.000, 16.000],  loss: 0.019558, mae: 0.320939, mean_q: 0.533796, mean_eps: 0.000000
 3430/5000: episode: 144, duration: 0.618s, episode steps:  44, steps per second:  71, episode reward: 32.166, mean reward:  0.731 [-3.000, 32.460], mean action: 5.045 [0.000, 15.000],  loss: 0.016766, mae: 0.309282, mean_q: 0.495673, mean_eps: 0.000000
 3453/5000: episode: 145, duration: 0.331s, episode steps:  23, steps per second:  69, episode reward: 39.000, mean reward:  1.696 [-2.517, 32.390], mean action: 7.130 [0.000, 15.000],  loss: 0.020427, mae: 0.324368, mean_q: 0.458023, mean_eps: 0.000000
 3478/5000: episode: 146, duration: 0.376s, episode steps:  25, steps per second:  66, episode reward: 35.202, mean reward:  1.408 [-2.593, 31.937], mean action: 4.800 [0.000, 15.000],  loss: 0.016969, mae: 0.301944, mean_q: 0.517319, mean_eps: 0.000000
 3494/5000: episode: 147, duration: 0.235s, episode steps:  16, steps per second:  68, episode reward: 38.930, mean reward:  2.433 [-2.345, 33.000], mean action: 3.000 [0.000, 16.000],  loss: 0.025940, mae: 0.342925, mean_q: 0.610576, mean_eps: 0.000000
 3517/5000: episode: 148, duration: 0.340s, episode steps:  23, steps per second:  68, episode reward: 35.969, mean reward:  1.564 [-2.354, 32.580], mean action: 5.957 [0.000, 19.000],  loss: 0.019618, mae: 0.313544, mean_q: 0.568329, mean_eps: 0.000000
 3542/5000: episode: 149, duration: 0.361s, episode steps:  25, steps per second:  69, episode reward: 38.101, mean reward:  1.524 [-2.448, 31.321], mean action: 2.800 [0.000, 9.000],  loss: 0.018019, mae: 0.307059, mean_q: 0.533788, mean_eps: 0.000000
 3560/5000: episode: 150, duration: 0.269s, episode steps:  18, steps per second:  67, episode reward: 38.287, mean reward:  2.127 [-2.559, 31.903], mean action: 4.167 [0.000, 15.000],  loss: 0.026227, mae: 0.337149, mean_q: 0.535010, mean_eps: 0.000000
 3587/5000: episode: 151, duration: 0.392s, episode steps:  27, steps per second:  69, episode reward: -32.130, mean reward: -1.190 [-32.521,  2.442], mean action: 3.926 [0.000, 19.000],  loss: 0.019509, mae: 0.316913, mean_q: 0.560001, mean_eps: 0.000000
 3607/5000: episode: 152, duration: 0.289s, episode steps:  20, steps per second:  69, episode reward: 35.369, mean reward:  1.768 [-3.000, 33.000], mean action: 4.650 [0.000, 15.000],  loss: 0.020778, mae: 0.323253, mean_q: 0.529232, mean_eps: 0.000000
 3633/5000: episode: 153, duration: 0.366s, episode steps:  26, steps per second:  71, episode reward: 32.075, mean reward:  1.234 [-2.900, 31.866], mean action: 5.462 [0.000, 16.000],  loss: 0.019625, mae: 0.318096, mean_q: 0.574096, mean_eps: 0.000000
 3648/5000: episode: 154, duration: 0.231s, episode steps:  15, steps per second:  65, episode reward: 41.651, mean reward:  2.777 [-2.790, 32.360], mean action: 3.067 [0.000, 16.000],  loss: 0.021833, mae: 0.323711, mean_q: 0.550469, mean_eps: 0.000000
 3673/5000: episode: 155, duration: 0.398s, episode steps:  25, steps per second:  63, episode reward: -32.340, mean reward: -1.294 [-32.338,  2.550], mean action: 5.560 [0.000, 19.000],  loss: 0.022780, mae: 0.330667, mean_q: 0.555389, mean_eps: 0.000000
 3694/5000: episode: 156, duration: 0.318s, episode steps:  21, steps per second:  66, episode reward: 40.981, mean reward:  1.951 [-2.086, 31.920], mean action: 3.143 [0.000, 13.000],  loss: 0.019031, mae: 0.314628, mean_q: 0.506492, mean_eps: 0.000000
 3722/5000: episode: 157, duration: 0.397s, episode steps:  28, steps per second:  70, episode reward: 32.022, mean reward:  1.144 [-3.000, 32.274], mean action: 3.929 [0.000, 16.000],  loss: 0.021122, mae: 0.315132, mean_q: 0.486561, mean_eps: 0.000000
 3746/5000: episode: 158, duration: 0.336s, episode steps:  24, steps per second:  71, episode reward: -34.370, mean reward: -1.432 [-31.748,  2.903], mean action: 3.917 [0.000, 16.000],  loss: 0.022612, mae: 0.329504, mean_q: 0.582827, mean_eps: 0.000000
 3759/5000: episode: 159, duration: 0.207s, episode steps:  13, steps per second:  63, episode reward: 44.259, mean reward:  3.405 [-2.381, 33.000], mean action: 1.615 [0.000, 11.000],  loss: 0.018101, mae: 0.299906, mean_q: 0.630015, mean_eps: 0.000000
 3789/5000: episode: 160, duration: 0.422s, episode steps:  30, steps per second:  71, episode reward: -32.320, mean reward: -1.077 [-32.451,  2.490], mean action: 2.367 [0.000, 12.000],  loss: 0.020088, mae: 0.316628, mean_q: 0.575936, mean_eps: 0.000000
 3814/5000: episode: 161, duration: 0.368s, episode steps:  25, steps per second:  68, episode reward: 35.306, mean reward:  1.412 [-2.498, 32.380], mean action: 5.840 [0.000, 16.000],  loss: 0.020592, mae: 0.325226, mean_q: 0.502886, mean_eps: 0.000000
 3840/5000: episode: 162, duration: 0.369s, episode steps:  26, steps per second:  70, episode reward: 35.356, mean reward:  1.360 [-2.371, 32.120], mean action: 5.731 [0.000, 18.000],  loss: 0.018400, mae: 0.308822, mean_q: 0.509034, mean_eps: 0.000000
 3862/5000: episode: 163, duration: 0.331s, episode steps:  22, steps per second:  66, episode reward: 35.779, mean reward:  1.626 [-2.394, 32.779], mean action: 4.636 [0.000, 16.000],  loss: 0.017107, mae: 0.303626, mean_q: 0.504699, mean_eps: 0.000000
 3882/5000: episode: 164, duration: 0.291s, episode steps:  20, steps per second:  69, episode reward: -35.300, mean reward: -1.765 [-31.914,  2.340], mean action: 4.450 [0.000, 16.000],  loss: 0.021481, mae: 0.326958, mean_q: 0.494379, mean_eps: 0.000000
 3903/5000: episode: 165, duration: 0.311s, episode steps:  21, steps per second:  68, episode reward: 35.610, mean reward:  1.696 [-3.000, 32.200], mean action: 7.429 [1.000, 15.000],  loss: 0.022693, mae: 0.328790, mean_q: 0.483461, mean_eps: 0.000000
 3926/5000: episode: 166, duration: 0.339s, episode steps:  23, steps per second:  68, episode reward: 40.787, mean reward:  1.773 [-2.660, 31.984], mean action: 4.087 [0.000, 13.000],  loss: 0.021417, mae: 0.330081, mean_q: 0.491944, mean_eps: 0.000000
 3940/5000: episode: 167, duration: 0.211s, episode steps:  14, steps per second:  66, episode reward: 36.000, mean reward:  2.571 [-2.456, 30.000], mean action: 4.429 [0.000, 16.000],  loss: 0.016641, mae: 0.308548, mean_q: 0.443951, mean_eps: 0.000000
 3963/5000: episode: 168, duration: 0.351s, episode steps:  23, steps per second:  65, episode reward: 38.041, mean reward:  1.654 [-2.226, 31.849], mean action: 4.000 [0.000, 15.000],  loss: 0.021173, mae: 0.323048, mean_q: 0.454802, mean_eps: 0.000000
 3986/5000: episode: 169, duration: 0.333s, episode steps:  23, steps per second:  69, episode reward: 35.406, mean reward:  1.539 [-3.000, 32.170], mean action: 4.478 [0.000, 20.000],  loss: 0.018892, mae: 0.302385, mean_q: 0.565718, mean_eps: 0.000000
 4006/5000: episode: 170, duration: 0.315s, episode steps:  20, steps per second:  63, episode reward: 39.000, mean reward:  1.950 [-2.349, 32.360], mean action: 3.850 [0.000, 15.000],  loss: 0.020393, mae: 0.323577, mean_q: 0.649272, mean_eps: 0.000000
 4022/5000: episode: 171, duration: 0.236s, episode steps:  16, steps per second:  68, episode reward: 41.112, mean reward:  2.569 [-2.505, 33.000], mean action: 3.938 [0.000, 15.000],  loss: 0.021199, mae: 0.325519, mean_q: 0.599010, mean_eps: 0.000000
 4040/5000: episode: 172, duration: 0.269s, episode steps:  18, steps per second:  67, episode reward: 38.764, mean reward:  2.154 [-2.332, 32.224], mean action: 3.778 [0.000, 15.000],  loss: 0.020931, mae: 0.323521, mean_q: 0.579027, mean_eps: 0.000000
 4073/5000: episode: 173, duration: 0.477s, episode steps:  33, steps per second:  69, episode reward: 39.000, mean reward:  1.182 [-2.156, 33.000], mean action: 2.879 [0.000, 16.000],  loss: 0.019145, mae: 0.315480, mean_q: 0.521623, mean_eps: 0.000000
 4092/5000: episode: 174, duration: 0.282s, episode steps:  19, steps per second:  67, episode reward: 33.000, mean reward:  1.737 [-3.000, 29.719], mean action: 4.526 [0.000, 16.000],  loss: 0.019660, mae: 0.323615, mean_q: 0.463000, mean_eps: 0.000000
 4118/5000: episode: 175, duration: 0.376s, episode steps:  26, steps per second:  69, episode reward: 33.000, mean reward:  1.269 [-3.000, 33.000], mean action: 5.038 [0.000, 15.000],  loss: 0.020194, mae: 0.324075, mean_q: 0.491910, mean_eps: 0.000000
 4134/5000: episode: 176, duration: 0.246s, episode steps:  16, steps per second:  65, episode reward: 38.189, mean reward:  2.387 [-2.610, 31.945], mean action: 4.750 [1.000, 15.000],  loss: 0.019512, mae: 0.313937, mean_q: 0.497593, mean_eps: 0.000000
 4165/5000: episode: 177, duration: 0.558s, episode steps:  31, steps per second:  56, episode reward: -32.910, mean reward: -1.062 [-32.280,  2.374], mean action: 5.355 [0.000, 15.000],  loss: 0.025702, mae: 0.348589, mean_q: 0.529233, mean_eps: 0.000000
 4180/5000: episode: 178, duration: 0.342s, episode steps:  15, steps per second:  44, episode reward: 38.440, mean reward:  2.563 [-2.472, 32.350], mean action: 3.800 [0.000, 11.000],  loss: 0.021055, mae: 0.330503, mean_q: 0.571839, mean_eps: 0.000000
 4207/5000: episode: 179, duration: 0.389s, episode steps:  27, steps per second:  69, episode reward: 32.124, mean reward:  1.190 [-3.000, 32.134], mean action: 3.481 [0.000, 19.000],  loss: 0.018712, mae: 0.318294, mean_q: 0.519127, mean_eps: 0.000000
 4238/5000: episode: 180, duration: 0.430s, episode steps:  31, steps per second:  72, episode reward: -32.910, mean reward: -1.062 [-31.980,  2.270], mean action: 4.839 [0.000, 13.000],  loss: 0.022383, mae: 0.337081, mean_q: 0.478871, mean_eps: 0.000000
 4270/5000: episode: 181, duration: 0.444s, episode steps:  32, steps per second:  72, episode reward: -35.070, mean reward: -1.096 [-31.922,  2.281], mean action: 3.500 [0.000, 15.000],  loss: 0.022359, mae: 0.327566, mean_q: 0.592636, mean_eps: 0.000000
 4292/5000: episode: 182, duration: 0.323s, episode steps:  22, steps per second:  68, episode reward: 38.278, mean reward:  1.740 [-2.076, 31.848], mean action: 3.636 [0.000, 20.000],  loss: 0.017815, mae: 0.306325, mean_q: 0.547314, mean_eps: 0.000000
 4321/5000: episode: 183, duration: 0.411s, episode steps:  29, steps per second:  71, episode reward: 32.206, mean reward:  1.111 [-2.285, 33.000], mean action: 4.897 [0.000, 15.000],  loss: 0.018490, mae: 0.316254, mean_q: 0.498217, mean_eps: 0.000000
 4341/5000: episode: 184, duration: 0.297s, episode steps:  20, steps per second:  67, episode reward: -35.760, mean reward: -1.788 [-32.234,  2.420], mean action: 6.550 [0.000, 19.000],  loss: 0.022196, mae: 0.331739, mean_q: 0.510170, mean_eps: 0.000000
 4366/5000: episode: 185, duration: 0.361s, episode steps:  25, steps per second:  69, episode reward: 33.000, mean reward:  1.320 [-2.717, 32.320], mean action: 3.480 [0.000, 19.000],  loss: 0.017152, mae: 0.304025, mean_q: 0.500041, mean_eps: 0.000000
 4387/5000: episode: 186, duration: 0.308s, episode steps:  21, steps per second:  68, episode reward: 38.732, mean reward:  1.844 [-3.000, 32.450], mean action: 6.333 [0.000, 15.000],  loss: 0.021680, mae: 0.322146, mean_q: 0.487954, mean_eps: 0.000000
 4415/5000: episode: 187, duration: 0.390s, episode steps:  28, steps per second:  72, episode reward: -36.000, mean reward: -1.286 [-32.184,  2.160], mean action: 4.571 [0.000, 19.000],  loss: 0.021199, mae: 0.311515, mean_q: 0.530465, mean_eps: 0.000000
 4452/5000: episode: 188, duration: 0.586s, episode steps:  37, steps per second:  63, episode reward: 32.160, mean reward:  0.869 [-2.678, 32.460], mean action: 3.216 [0.000, 19.000],  loss: 0.019282, mae: 0.304237, mean_q: 0.493915, mean_eps: 0.000000
 4469/5000: episode: 189, duration: 0.264s, episode steps:  17, steps per second:  65, episode reward: 41.504, mean reward:  2.441 [-2.175, 32.736], mean action: 3.059 [0.000, 16.000],  loss: 0.017278, mae: 0.294230, mean_q: 0.513949, mean_eps: 0.000000
 4487/5000: episode: 190, duration: 0.278s, episode steps:  18, steps per second:  65, episode reward: 41.681, mean reward:  2.316 [-2.319, 32.121], mean action: 3.167 [0.000, 14.000],  loss: 0.021523, mae: 0.312534, mean_q: 0.471532, mean_eps: 0.000000
 4518/5000: episode: 191, duration: 0.439s, episode steps:  31, steps per second:  71, episode reward: 35.846, mean reward:  1.156 [-3.000, 32.270], mean action: 3.323 [0.000, 16.000],  loss: 0.022523, mae: 0.319143, mean_q: 0.453113, mean_eps: 0.000000
 4539/5000: episode: 192, duration: 0.325s, episode steps:  21, steps per second:  65, episode reward: 35.594, mean reward:  1.695 [-2.556, 32.404], mean action: 4.286 [0.000, 16.000],  loss: 0.021666, mae: 0.306865, mean_q: 0.459896, mean_eps: 0.000000
 4566/5000: episode: 193, duration: 0.401s, episode steps:  27, steps per second:  67, episode reward: 37.380, mean reward:  1.384 [-2.266, 32.593], mean action: 5.370 [0.000, 20.000],  loss: 0.022179, mae: 0.311925, mean_q: 0.481951, mean_eps: 0.000000
 4586/5000: episode: 194, duration: 0.313s, episode steps:  20, steps per second:  64, episode reward: 38.104, mean reward:  1.905 [-2.288, 31.945], mean action: 3.500 [0.000, 11.000],  loss: 0.021217, mae: 0.310932, mean_q: 0.459773, mean_eps: 0.000000
 4619/5000: episode: 195, duration: 0.512s, episode steps:  33, steps per second:  65, episode reward: -35.830, mean reward: -1.086 [-32.031,  2.200], mean action: 6.030 [0.000, 15.000],  loss: 0.022563, mae: 0.317971, mean_q: 0.508060, mean_eps: 0.000000
 4654/5000: episode: 196, duration: 0.498s, episode steps:  35, steps per second:  70, episode reward: 35.020, mean reward:  1.001 [-2.584, 32.314], mean action: 6.714 [0.000, 20.000],  loss: 0.019183, mae: 0.310053, mean_q: 0.561265, mean_eps: 0.000000
 4692/5000: episode: 197, duration: 0.529s, episode steps:  38, steps per second:  72, episode reward: 33.000, mean reward:  0.868 [-3.000, 32.030], mean action: 3.263 [0.000, 15.000],  loss: 0.019713, mae: 0.317786, mean_q: 0.565260, mean_eps: 0.000000
 4718/5000: episode: 198, duration: 0.369s, episode steps:  26, steps per second:  70, episode reward: -32.320, mean reward: -1.243 [-32.101,  2.290], mean action: 4.154 [0.000, 15.000],  loss: 0.020275, mae: 0.324708, mean_q: 0.537378, mean_eps: 0.000000
 4747/5000: episode: 199, duration: 0.417s, episode steps:  29, steps per second:  70, episode reward: 32.254, mean reward:  1.112 [-2.663, 32.130], mean action: 3.414 [0.000, 15.000],  loss: 0.022376, mae: 0.330629, mean_q: 0.566388, mean_eps: 0.000000
 4771/5000: episode: 200, duration: 0.350s, episode steps:  24, steps per second:  69, episode reward: -33.000, mean reward: -1.375 [-32.218,  2.519], mean action: 3.667 [0.000, 15.000],  loss: 0.022156, mae: 0.328823, mean_q: 0.584582, mean_eps: 0.000000
 4789/5000: episode: 201, duration: 0.279s, episode steps:  18, steps per second:  64, episode reward: 41.410, mean reward:  2.301 [-2.144, 32.200], mean action: 2.111 [0.000, 11.000],  loss: 0.020834, mae: 0.326305, mean_q: 0.534503, mean_eps: 0.000000
 4817/5000: episode: 202, duration: 0.392s, episode steps:  28, steps per second:  72, episode reward: -38.160, mean reward: -1.363 [-32.880,  2.813], mean action: 8.893 [0.000, 18.000],  loss: 0.018810, mae: 0.308821, mean_q: 0.522089, mean_eps: 0.000000
 4851/5000: episode: 203, duration: 0.534s, episode steps:  34, steps per second:  64, episode reward: 38.701, mean reward:  1.138 [-2.205, 32.900], mean action: 4.853 [1.000, 18.000],  loss: 0.019411, mae: 0.316724, mean_q: 0.553711, mean_eps: 0.000000
 4899/5000: episode: 204, duration: 0.669s, episode steps:  48, steps per second:  72, episode reward: 32.664, mean reward:  0.680 [-3.000, 31.951], mean action: 7.917 [0.000, 20.000],  loss: 0.021182, mae: 0.327564, mean_q: 0.462107, mean_eps: 0.000000
 4922/5000: episode: 205, duration: 0.329s, episode steps:  23, steps per second:  70, episode reward: 35.144, mean reward:  1.528 [-3.000, 31.996], mean action: 1.870 [0.000, 11.000],  loss: 0.022369, mae: 0.333726, mean_q: 0.504125, mean_eps: 0.000000
 4945/5000: episode: 206, duration: 0.322s, episode steps:  23, steps per second:  71, episode reward: 32.781, mean reward:  1.425 [-3.000, 32.161], mean action: 4.217 [0.000, 19.000],  loss: 0.022274, mae: 0.333803, mean_q: 0.521306, mean_eps: 0.000000
 4972/5000: episode: 207, duration: 0.380s, episode steps:  27, steps per second:  71, episode reward: -32.530, mean reward: -1.205 [-31.930,  2.451], mean action: 3.296 [0.000, 11.000],  loss: 0.018397, mae: 0.314013, mean_q: 0.524327, mean_eps: 0.000000
 4993/5000: episode: 208, duration: 0.313s, episode steps:  21, steps per second:  67, episode reward: 38.836, mean reward:  1.849 [-2.166, 32.260], mean action: 2.905 [0.000, 12.000],  loss: 0.022545, mae: 0.336123, mean_q: 0.600558, mean_eps: 0.000000
done, took 66.099 seconds
DQN Evaluation: 10589 victories out of 12372 episodes
Training for 5000 steps ...
   20/5000: episode: 1, duration: 0.165s, episode steps:  20, steps per second: 121, episode reward: 44.288, mean reward:  2.214 [-2.720, 31.866], mean action: 5.100 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   49/5000: episode: 2, duration: 0.200s, episode steps:  29, steps per second: 145, episode reward: 38.503, mean reward:  1.328 [-2.582, 32.220], mean action: 4.103 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   67/5000: episode: 3, duration: 0.134s, episode steps:  18, steps per second: 134, episode reward: 41.472, mean reward:  2.304 [-2.240, 32.390], mean action: 3.667 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   81/5000: episode: 4, duration: 0.113s, episode steps:  14, steps per second: 124, episode reward: 47.035, mean reward:  3.360 [ 0.000, 31.951], mean action: 1.857 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  100/5000: episode: 5, duration: 0.142s, episode steps:  19, steps per second: 134, episode reward: 44.279, mean reward:  2.330 [-2.620, 32.142], mean action: 3.421 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  125/5000: episode: 6, duration: 0.185s, episode steps:  25, steps per second: 135, episode reward: 44.248, mean reward:  1.770 [-2.626, 32.340], mean action: 4.320 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  152/5000: episode: 7, duration: 0.183s, episode steps:  27, steps per second: 148, episode reward: 38.038, mean reward:  1.409 [-3.000, 32.465], mean action: 5.000 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  180/5000: episode: 8, duration: 0.189s, episode steps:  28, steps per second: 148, episode reward: 38.638, mean reward:  1.380 [-3.000, 32.100], mean action: 3.286 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  199/5000: episode: 9, duration: 0.134s, episode steps:  19, steps per second: 141, episode reward: 38.374, mean reward:  2.020 [-2.611, 33.000], mean action: 4.105 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  227/5000: episode: 10, duration: 0.184s, episode steps:  28, steps per second: 152, episode reward: 35.223, mean reward:  1.258 [-2.439, 32.272], mean action: 4.357 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  253/5000: episode: 11, duration: 0.187s, episode steps:  26, steps per second: 139, episode reward: 44.495, mean reward:  1.711 [-2.386, 32.080], mean action: 2.231 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  283/5000: episode: 12, duration: 0.188s, episode steps:  30, steps per second: 160, episode reward: 35.412, mean reward:  1.180 [-2.195, 32.412], mean action: 5.633 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  304/5000: episode: 13, duration: 0.153s, episode steps:  21, steps per second: 137, episode reward: 44.556, mean reward:  2.122 [-2.401, 33.000], mean action: 3.714 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  322/5000: episode: 14, duration: 0.137s, episode steps:  18, steps per second: 132, episode reward: 44.271, mean reward:  2.459 [-2.004, 32.111], mean action: 2.333 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  345/5000: episode: 15, duration: 0.151s, episode steps:  23, steps per second: 152, episode reward: 38.380, mean reward:  1.669 [-3.000, 32.150], mean action: 5.348 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  389/5000: episode: 16, duration: 0.273s, episode steps:  44, steps per second: 161, episode reward: 35.868, mean reward:  0.815 [-2.900, 31.888], mean action: 7.318 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  425/5000: episode: 17, duration: 0.237s, episode steps:  36, steps per second: 152, episode reward: 36.000, mean reward:  1.000 [-2.683, 30.296], mean action: 4.333 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  461/5000: episode: 18, duration: 0.231s, episode steps:  36, steps per second: 156, episode reward: 41.529, mean reward:  1.154 [-2.233, 32.200], mean action: 5.000 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  482/5000: episode: 19, duration: 0.144s, episode steps:  21, steps per second: 146, episode reward: 41.463, mean reward:  1.974 [-2.473, 32.145], mean action: 2.238 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  499/5000: episode: 20, duration: 0.130s, episode steps:  17, steps per second: 131, episode reward: 47.814, mean reward:  2.813 [ 0.100, 32.180], mean action: 2.059 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  538/5000: episode: 21, duration: 0.259s, episode steps:  39, steps per second: 151, episode reward: 34.444, mean reward:  0.883 [-3.000, 32.150], mean action: 9.513 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  561/5000: episode: 22, duration: 0.167s, episode steps:  23, steps per second: 138, episode reward: 41.543, mean reward:  1.806 [-3.000, 32.300], mean action: 8.783 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  597/5000: episode: 23, duration: 0.222s, episode steps:  36, steps per second: 162, episode reward: -32.100, mean reward: -0.892 [-32.471,  2.630], mean action: 7.361 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  628/5000: episode: 24, duration: 0.202s, episode steps:  31, steps per second: 154, episode reward: 41.929, mean reward:  1.353 [-2.475, 32.230], mean action: 6.323 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  656/5000: episode: 25, duration: 0.181s, episode steps:  28, steps per second: 155, episode reward: 44.258, mean reward:  1.581 [-2.142, 32.110], mean action: 3.143 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  694/5000: episode: 26, duration: 0.239s, episode steps:  38, steps per second: 159, episode reward: 41.348, mean reward:  1.088 [-2.489, 32.080], mean action: 6.263 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  727/5000: episode: 27, duration: 0.233s, episode steps:  33, steps per second: 142, episode reward: 38.038, mean reward:  1.153 [-2.173, 32.170], mean action: 5.455 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  749/5000: episode: 28, duration: 0.146s, episode steps:  22, steps per second: 150, episode reward: 34.730, mean reward:  1.579 [-3.000, 32.068], mean action: 4.455 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  774/5000: episode: 29, duration: 0.182s, episode steps:  25, steps per second: 137, episode reward: 39.999, mean reward:  1.600 [-2.145, 31.846], mean action: 4.520 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  809/5000: episode: 30, duration: 0.227s, episode steps:  35, steps per second: 154, episode reward: 37.788, mean reward:  1.080 [-3.000, 31.559], mean action: 6.657 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  833/5000: episode: 31, duration: 0.160s, episode steps:  24, steps per second: 150, episode reward: 41.406, mean reward:  1.725 [-2.698, 32.102], mean action: 4.042 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  868/5000: episode: 32, duration: 0.227s, episode steps:  35, steps per second: 154, episode reward: 44.197, mean reward:  1.263 [-2.265, 32.150], mean action: 5.943 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  883/5000: episode: 33, duration: 0.113s, episode steps:  15, steps per second: 133, episode reward: 44.735, mean reward:  2.982 [-2.226, 32.070], mean action: 2.800 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  905/5000: episode: 34, duration: 0.154s, episode steps:  22, steps per second: 143, episode reward: 44.311, mean reward:  2.014 [-2.535, 32.540], mean action: 2.818 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  934/5000: episode: 35, duration: 0.196s, episode steps:  29, steps per second: 148, episode reward: 38.416, mean reward:  1.325 [-2.250, 31.857], mean action: 3.345 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  955/5000: episode: 36, duration: 0.149s, episode steps:  21, steps per second: 141, episode reward: 44.649, mean reward:  2.126 [-2.276, 32.390], mean action: 3.714 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1036/5000: episode: 37, duration: 0.776s, episode steps:  81, steps per second: 104, episode reward: 42.762, mean reward:  0.528 [-2.468, 32.071], mean action: 2.383 [0.000, 14.000],  loss: 0.019663, mae: 0.326658, mean_q: 0.578211, mean_eps: 0.000000
 1060/5000: episode: 38, duration: 0.354s, episode steps:  24, steps per second:  68, episode reward: 40.760, mean reward:  1.698 [-2.538, 32.420], mean action: 6.833 [0.000, 20.000],  loss: 0.022078, mae: 0.341580, mean_q: 0.532331, mean_eps: 0.000000
 1081/5000: episode: 39, duration: 0.308s, episode steps:  21, steps per second:  68, episode reward: 44.367, mean reward:  2.113 [-2.336, 32.107], mean action: 4.143 [2.000, 14.000],  loss: 0.027414, mae: 0.372296, mean_q: 0.541360, mean_eps: 0.000000
 1120/5000: episode: 40, duration: 0.542s, episode steps:  39, steps per second:  72, episode reward: 37.915, mean reward:  0.972 [-2.399, 31.902], mean action: 4.026 [0.000, 21.000],  loss: 0.020172, mae: 0.331007, mean_q: 0.537155, mean_eps: 0.000000
 1155/5000: episode: 41, duration: 0.490s, episode steps:  35, steps per second:  71, episode reward: 41.545, mean reward:  1.187 [-3.000, 32.100], mean action: 5.400 [0.000, 19.000],  loss: 0.020257, mae: 0.325764, mean_q: 0.623775, mean_eps: 0.000000
 1185/5000: episode: 42, duration: 0.442s, episode steps:  30, steps per second:  68, episode reward: 35.638, mean reward:  1.188 [-2.704, 32.460], mean action: 5.233 [0.000, 19.000],  loss: 0.022993, mae: 0.339835, mean_q: 0.592966, mean_eps: 0.000000
 1237/5000: episode: 43, duration: 0.741s, episode steps:  52, steps per second:  70, episode reward: 36.000, mean reward:  0.692 [-3.000, 32.160], mean action: 2.692 [0.000, 15.000],  loss: 0.020192, mae: 0.329281, mean_q: 0.633402, mean_eps: 0.000000
 1355/5000: episode: 44, duration: 1.767s, episode steps: 118, steps per second:  67, episode reward: 33.000, mean reward:  0.280 [-3.000, 32.370], mean action: 9.297 [0.000, 21.000],  loss: 0.019409, mae: 0.324977, mean_q: 0.568159, mean_eps: 0.000000
 1376/5000: episode: 45, duration: 0.317s, episode steps:  21, steps per second:  66, episode reward: 41.903, mean reward:  1.995 [-2.060, 32.003], mean action: 3.952 [0.000, 19.000],  loss: 0.024290, mae: 0.352686, mean_q: 0.570949, mean_eps: 0.000000
 1398/5000: episode: 46, duration: 0.324s, episode steps:  22, steps per second:  68, episode reward: 41.108, mean reward:  1.869 [-2.556, 32.490], mean action: 5.636 [0.000, 19.000],  loss: 0.018899, mae: 0.327130, mean_q: 0.557339, mean_eps: 0.000000
 1425/5000: episode: 47, duration: 0.409s, episode steps:  27, steps per second:  66, episode reward: 41.266, mean reward:  1.528 [-2.459, 32.380], mean action: 5.037 [0.000, 19.000],  loss: 0.018546, mae: 0.320211, mean_q: 0.589483, mean_eps: 0.000000
 1448/5000: episode: 48, duration: 0.341s, episode steps:  23, steps per second:  67, episode reward: 38.778, mean reward:  1.686 [-2.806, 32.020], mean action: 4.783 [0.000, 19.000],  loss: 0.024999, mae: 0.353651, mean_q: 0.547505, mean_eps: 0.000000
 1484/5000: episode: 49, duration: 0.508s, episode steps:  36, steps per second:  71, episode reward: 43.321, mean reward:  1.203 [-2.308, 31.284], mean action: 1.500 [0.000, 13.000],  loss: 0.022943, mae: 0.344225, mean_q: 0.591231, mean_eps: 0.000000
 1504/5000: episode: 50, duration: 0.291s, episode steps:  20, steps per second:  69, episode reward: 41.778, mean reward:  2.089 [-2.729, 32.168], mean action: 1.800 [0.000, 12.000],  loss: 0.022885, mae: 0.349959, mean_q: 0.566366, mean_eps: 0.000000
 1523/5000: episode: 51, duration: 0.292s, episode steps:  19, steps per second:  65, episode reward: 41.493, mean reward:  2.184 [-2.101, 31.543], mean action: 3.158 [0.000, 12.000],  loss: 0.026964, mae: 0.389402, mean_q: 0.581946, mean_eps: 0.000000
 1555/5000: episode: 52, duration: 0.460s, episode steps:  32, steps per second:  70, episode reward: 35.317, mean reward:  1.104 [-3.000, 33.853], mean action: 5.219 [0.000, 15.000],  loss: 0.020421, mae: 0.344898, mean_q: 0.534914, mean_eps: 0.000000
 1577/5000: episode: 53, duration: 0.333s, episode steps:  22, steps per second:  66, episode reward: 39.000, mean reward:  1.773 [-3.000, 32.110], mean action: 3.818 [0.000, 12.000],  loss: 0.021357, mae: 0.347508, mean_q: 0.589197, mean_eps: 0.000000
 1613/5000: episode: 54, duration: 0.515s, episode steps:  36, steps per second:  70, episode reward: 36.000, mean reward:  1.000 [-2.229, 32.160], mean action: 5.472 [0.000, 20.000],  loss: 0.022000, mae: 0.348841, mean_q: 0.587560, mean_eps: 0.000000
 1638/5000: episode: 55, duration: 0.365s, episode steps:  25, steps per second:  68, episode reward: 44.021, mean reward:  1.761 [-2.741, 32.160], mean action: 4.040 [0.000, 13.000],  loss: 0.018160, mae: 0.319008, mean_q: 0.552659, mean_eps: 0.000000
 1679/5000: episode: 56, duration: 0.571s, episode steps:  41, steps per second:  72, episode reward: 40.681, mean reward:  0.992 [-2.471, 31.897], mean action: 2.341 [0.000, 18.000],  loss: 0.022750, mae: 0.334764, mean_q: 0.549548, mean_eps: 0.000000
 1708/5000: episode: 57, duration: 0.438s, episode steps:  29, steps per second:  66, episode reward: 41.873, mean reward:  1.444 [-2.163, 32.490], mean action: 3.414 [0.000, 14.000],  loss: 0.021757, mae: 0.337741, mean_q: 0.576840, mean_eps: 0.000000
 1746/5000: episode: 58, duration: 0.546s, episode steps:  38, steps per second:  70, episode reward: 43.295, mean reward:  1.139 [-2.488, 32.020], mean action: 2.789 [0.000, 20.000],  loss: 0.021206, mae: 0.333592, mean_q: 0.526509, mean_eps: 0.000000
 1766/5000: episode: 59, duration: 0.289s, episode steps:  20, steps per second:  69, episode reward: 41.709, mean reward:  2.085 [-2.182, 32.453], mean action: 2.100 [0.000, 9.000],  loss: 0.024973, mae: 0.352153, mean_q: 0.539844, mean_eps: 0.000000
 1803/5000: episode: 60, duration: 0.521s, episode steps:  37, steps per second:  71, episode reward: 33.000, mean reward:  0.892 [-3.000, 32.320], mean action: 5.054 [0.000, 19.000],  loss: 0.020944, mae: 0.344664, mean_q: 0.549204, mean_eps: 0.000000
 1829/5000: episode: 61, duration: 0.375s, episode steps:  26, steps per second:  69, episode reward: 42.000, mean reward:  1.615 [-3.000, 32.500], mean action: 1.731 [0.000, 12.000],  loss: 0.019544, mae: 0.323229, mean_q: 0.587976, mean_eps: 0.000000
 1849/5000: episode: 62, duration: 0.306s, episode steps:  20, steps per second:  65, episode reward: 38.539, mean reward:  1.927 [-2.753, 31.829], mean action: 2.900 [0.000, 9.000],  loss: 0.026769, mae: 0.358963, mean_q: 0.608442, mean_eps: 0.000000
 1868/5000: episode: 63, duration: 0.284s, episode steps:  19, steps per second:  67, episode reward: 44.275, mean reward:  2.330 [-2.185, 31.797], mean action: 1.579 [0.000, 9.000],  loss: 0.019974, mae: 0.313813, mean_q: 0.541317, mean_eps: 0.000000
 1886/5000: episode: 64, duration: 0.270s, episode steps:  18, steps per second:  67, episode reward: 47.206, mean reward:  2.623 [ 0.000, 32.170], mean action: 5.611 [1.000, 20.000],  loss: 0.016454, mae: 0.297792, mean_q: 0.523211, mean_eps: 0.000000
 1908/5000: episode: 65, duration: 0.332s, episode steps:  22, steps per second:  66, episode reward: 41.338, mean reward:  1.879 [-2.711, 32.420], mean action: 2.364 [0.000, 11.000],  loss: 0.024811, mae: 0.337432, mean_q: 0.539913, mean_eps: 0.000000
 1938/5000: episode: 66, duration: 0.430s, episode steps:  30, steps per second:  70, episode reward: 41.903, mean reward:  1.397 [-2.799, 32.483], mean action: 1.400 [0.000, 11.000],  loss: 0.021685, mae: 0.322218, mean_q: 0.651231, mean_eps: 0.000000
 1964/5000: episode: 67, duration: 0.383s, episode steps:  26, steps per second:  68, episode reward: 39.000, mean reward:  1.500 [-2.684, 32.560], mean action: 5.115 [1.000, 19.000],  loss: 0.022421, mae: 0.325003, mean_q: 0.682000, mean_eps: 0.000000
 1995/5000: episode: 68, duration: 0.464s, episode steps:  31, steps per second:  67, episode reward: 41.599, mean reward:  1.342 [-2.407, 32.064], mean action: 1.387 [0.000, 9.000],  loss: 0.022418, mae: 0.325990, mean_q: 0.606341, mean_eps: 0.000000
 2019/5000: episode: 69, duration: 0.358s, episode steps:  24, steps per second:  67, episode reward: 40.570, mean reward:  1.690 [-2.861, 32.001], mean action: 3.000 [0.000, 12.000],  loss: 0.023002, mae: 0.334262, mean_q: 0.489082, mean_eps: 0.000000
 2039/5000: episode: 70, duration: 0.303s, episode steps:  20, steps per second:  66, episode reward: 44.071, mean reward:  2.204 [-3.000, 32.800], mean action: 1.250 [0.000, 4.000],  loss: 0.020699, mae: 0.321884, mean_q: 0.496080, mean_eps: 0.000000
 2060/5000: episode: 71, duration: 0.311s, episode steps:  21, steps per second:  67, episode reward: 44.109, mean reward:  2.100 [-2.058, 32.550], mean action: 4.286 [0.000, 15.000],  loss: 0.020053, mae: 0.314402, mean_q: 0.525014, mean_eps: 0.000000
 2079/5000: episode: 72, duration: 0.280s, episode steps:  19, steps per second:  68, episode reward: 44.026, mean reward:  2.317 [-2.358, 31.889], mean action: 3.474 [0.000, 14.000],  loss: 0.019490, mae: 0.313807, mean_q: 0.563521, mean_eps: 0.000000
 2106/5000: episode: 73, duration: 0.392s, episode steps:  27, steps per second:  69, episode reward: 33.000, mean reward:  1.222 [-3.000, 32.020], mean action: 3.444 [0.000, 16.000],  loss: 0.021134, mae: 0.326504, mean_q: 0.514896, mean_eps: 0.000000
 2136/5000: episode: 74, duration: 0.425s, episode steps:  30, steps per second:  71, episode reward: 32.810, mean reward:  1.094 [-3.000, 32.410], mean action: 7.967 [0.000, 16.000],  loss: 0.020342, mae: 0.322977, mean_q: 0.519786, mean_eps: 0.000000
 2154/5000: episode: 75, duration: 0.278s, episode steps:  18, steps per second:  65, episode reward: 45.000, mean reward:  2.500 [-2.064, 32.630], mean action: 1.500 [0.000, 11.000],  loss: 0.019221, mae: 0.310583, mean_q: 0.449385, mean_eps: 0.000000
 2183/5000: episode: 76, duration: 0.475s, episode steps:  29, steps per second:  61, episode reward: 43.468, mean reward:  1.499 [-2.211, 31.563], mean action: 2.207 [0.000, 16.000],  loss: 0.019098, mae: 0.305948, mean_q: 0.480707, mean_eps: 0.000000
 2213/5000: episode: 77, duration: 0.428s, episode steps:  30, steps per second:  70, episode reward: 41.226, mean reward:  1.374 [-2.532, 31.867], mean action: 3.067 [0.000, 16.000],  loss: 0.018384, mae: 0.303203, mean_q: 0.503310, mean_eps: 0.000000
 2237/5000: episode: 78, duration: 0.347s, episode steps:  24, steps per second:  69, episode reward: 41.678, mean reward:  1.737 [-2.172, 32.130], mean action: 3.875 [0.000, 15.000],  loss: 0.023349, mae: 0.324202, mean_q: 0.569563, mean_eps: 0.000000
 2272/5000: episode: 79, duration: 0.499s, episode steps:  35, steps per second:  70, episode reward: 34.680, mean reward:  0.991 [-3.000, 32.190], mean action: 6.114 [0.000, 21.000],  loss: 0.019918, mae: 0.314832, mean_q: 0.544893, mean_eps: 0.000000
 2306/5000: episode: 80, duration: 0.483s, episode steps:  34, steps per second:  70, episode reward: 44.738, mean reward:  1.316 [-2.416, 32.371], mean action: 1.559 [0.000, 13.000],  loss: 0.022053, mae: 0.327782, mean_q: 0.501266, mean_eps: 0.000000
 2338/5000: episode: 81, duration: 0.442s, episode steps:  32, steps per second:  72, episode reward: 35.807, mean reward:  1.119 [-3.000, 32.774], mean action: 3.188 [0.000, 15.000],  loss: 0.023206, mae: 0.337575, mean_q: 0.510149, mean_eps: 0.000000
 2373/5000: episode: 82, duration: 0.500s, episode steps:  35, steps per second:  70, episode reward: 41.704, mean reward:  1.192 [-2.478, 32.380], mean action: 3.114 [0.000, 18.000],  loss: 0.019596, mae: 0.319956, mean_q: 0.479131, mean_eps: 0.000000
 2389/5000: episode: 83, duration: 0.254s, episode steps:  16, steps per second:  63, episode reward: 44.219, mean reward:  2.764 [-2.391, 32.955], mean action: 2.062 [0.000, 13.000],  loss: 0.020495, mae: 0.321847, mean_q: 0.498793, mean_eps: 0.000000
 2412/5000: episode: 84, duration: 0.334s, episode steps:  23, steps per second:  69, episode reward: 47.008, mean reward:  2.044 [-0.250, 31.933], mean action: 0.609 [0.000, 1.000],  loss: 0.020975, mae: 0.320166, mean_q: 0.504168, mean_eps: 0.000000
 2434/5000: episode: 85, duration: 0.330s, episode steps:  22, steps per second:  67, episode reward: 41.106, mean reward:  1.868 [-2.110, 31.775], mean action: 3.091 [0.000, 13.000],  loss: 0.022097, mae: 0.314624, mean_q: 0.523822, mean_eps: 0.000000
 2466/5000: episode: 86, duration: 0.690s, episode steps:  32, steps per second:  46, episode reward: 38.804, mean reward:  1.213 [-2.433, 32.112], mean action: 3.844 [1.000, 16.000],  loss: 0.022019, mae: 0.329930, mean_q: 0.535621, mean_eps: 0.000000
 2494/5000: episode: 87, duration: 0.396s, episode steps:  28, steps per second:  71, episode reward: 32.415, mean reward:  1.158 [-2.708, 32.415], mean action: 3.536 [0.000, 16.000],  loss: 0.023059, mae: 0.330801, mean_q: 0.527873, mean_eps: 0.000000
 2517/5000: episode: 88, duration: 0.345s, episode steps:  23, steps per second:  67, episode reward: 43.709, mean reward:  1.900 [-2.463, 32.130], mean action: 2.739 [0.000, 18.000],  loss: 0.021796, mae: 0.341958, mean_q: 0.593978, mean_eps: 0.000000
 2539/5000: episode: 89, duration: 0.326s, episode steps:  22, steps per second:  68, episode reward: 44.057, mean reward:  2.003 [-3.000, 32.290], mean action: 3.273 [0.000, 13.000],  loss: 0.018570, mae: 0.329282, mean_q: 0.519410, mean_eps: 0.000000
 2573/5000: episode: 90, duration: 0.472s, episode steps:  34, steps per second:  72, episode reward: 32.791, mean reward:  0.964 [-3.000, 31.914], mean action: 7.324 [0.000, 15.000],  loss: 0.021641, mae: 0.336791, mean_q: 0.516916, mean_eps: 0.000000
 2599/5000: episode: 91, duration: 0.373s, episode steps:  26, steps per second:  70, episode reward: 38.606, mean reward:  1.485 [-3.000, 32.071], mean action: 4.000 [1.000, 15.000],  loss: 0.023021, mae: 0.326503, mean_q: 0.548641, mean_eps: 0.000000
 2617/5000: episode: 92, duration: 0.269s, episode steps:  18, steps per second:  67, episode reward: 44.245, mean reward:  2.458 [-2.429, 31.881], mean action: 2.111 [0.000, 12.000],  loss: 0.021742, mae: 0.330742, mean_q: 0.521877, mean_eps: 0.000000
 2638/5000: episode: 93, duration: 0.310s, episode steps:  21, steps per second:  68, episode reward: 41.484, mean reward:  1.975 [-3.000, 32.660], mean action: 3.238 [0.000, 16.000],  loss: 0.020704, mae: 0.325075, mean_q: 0.516458, mean_eps: 0.000000
 2676/5000: episode: 94, duration: 0.538s, episode steps:  38, steps per second:  71, episode reward: 35.922, mean reward:  0.945 [-3.000, 32.160], mean action: 4.263 [0.000, 19.000],  loss: 0.019840, mae: 0.324616, mean_q: 0.503785, mean_eps: 0.000000
 2703/5000: episode: 95, duration: 0.394s, episode steps:  27, steps per second:  69, episode reward: 41.249, mean reward:  1.528 [-2.766, 32.120], mean action: 5.000 [0.000, 16.000],  loss: 0.023645, mae: 0.329337, mean_q: 0.507222, mean_eps: 0.000000
 2734/5000: episode: 96, duration: 0.448s, episode steps:  31, steps per second:  69, episode reward: 41.667, mean reward:  1.344 [-2.597, 32.130], mean action: 5.774 [0.000, 21.000],  loss: 0.020500, mae: 0.311651, mean_q: 0.498985, mean_eps: 0.000000
 2762/5000: episode: 97, duration: 0.405s, episode steps:  28, steps per second:  69, episode reward: 38.670, mean reward:  1.381 [-2.435, 32.480], mean action: 4.857 [0.000, 20.000],  loss: 0.021029, mae: 0.319228, mean_q: 0.541507, mean_eps: 0.000000
 2780/5000: episode: 98, duration: 0.273s, episode steps:  18, steps per second:  66, episode reward: 41.174, mean reward:  2.287 [-2.137, 32.100], mean action: 2.333 [0.000, 15.000],  loss: 0.018889, mae: 0.300344, mean_q: 0.531296, mean_eps: 0.000000
 2800/5000: episode: 99, duration: 0.320s, episode steps:  20, steps per second:  62, episode reward: 41.629, mean reward:  2.081 [-2.105, 32.156], mean action: 1.500 [0.000, 11.000],  loss: 0.020145, mae: 0.313839, mean_q: 0.502619, mean_eps: 0.000000
 2830/5000: episode: 100, duration: 0.417s, episode steps:  30, steps per second:  72, episode reward: 38.653, mean reward:  1.288 [-2.435, 31.887], mean action: 3.667 [0.000, 20.000],  loss: 0.017488, mae: 0.298033, mean_q: 0.528647, mean_eps: 0.000000
 2870/5000: episode: 101, duration: 0.550s, episode steps:  40, steps per second:  73, episode reward: 38.553, mean reward:  0.964 [-2.231, 32.310], mean action: 1.700 [0.000, 15.000],  loss: 0.019696, mae: 0.315914, mean_q: 0.510810, mean_eps: 0.000000
 2904/5000: episode: 102, duration: 0.489s, episode steps:  34, steps per second:  69, episode reward: 40.480, mean reward:  1.191 [-2.549, 33.000], mean action: 5.529 [0.000, 21.000],  loss: 0.019994, mae: 0.320442, mean_q: 0.460226, mean_eps: 0.000000
 2943/5000: episode: 103, duration: 0.571s, episode steps:  39, steps per second:  68, episode reward: 41.489, mean reward:  1.064 [-2.342, 32.380], mean action: 3.000 [0.000, 15.000],  loss: 0.021924, mae: 0.328908, mean_q: 0.457012, mean_eps: 0.000000
 2971/5000: episode: 104, duration: 0.410s, episode steps:  28, steps per second:  68, episode reward: 41.651, mean reward:  1.488 [-2.321, 32.560], mean action: 2.643 [0.000, 11.000],  loss: 0.023351, mae: 0.336341, mean_q: 0.497803, mean_eps: 0.000000
 3012/5000: episode: 105, duration: 0.563s, episode steps:  41, steps per second:  73, episode reward: -33.000, mean reward: -0.805 [-32.033,  2.370], mean action: 5.488 [0.000, 15.000],  loss: 0.019592, mae: 0.311029, mean_q: 0.541910, mean_eps: 0.000000
 3034/5000: episode: 106, duration: 0.315s, episode steps:  22, steps per second:  70, episode reward: 41.186, mean reward:  1.872 [-2.421, 31.663], mean action: 4.500 [0.000, 19.000],  loss: 0.016108, mae: 0.297280, mean_q: 0.507794, mean_eps: 0.000000
 3050/5000: episode: 107, duration: 0.243s, episode steps:  16, steps per second:  66, episode reward: 44.806, mean reward:  2.800 [-2.025, 32.230], mean action: 3.250 [0.000, 19.000],  loss: 0.019944, mae: 0.311268, mean_q: 0.517180, mean_eps: 0.000000
 3076/5000: episode: 108, duration: 0.406s, episode steps:  26, steps per second:  64, episode reward: 35.329, mean reward:  1.359 [-2.505, 32.820], mean action: 4.654 [0.000, 19.000],  loss: 0.019128, mae: 0.309195, mean_q: 0.518011, mean_eps: 0.000000
 3103/5000: episode: 109, duration: 0.389s, episode steps:  27, steps per second:  69, episode reward: 36.000, mean reward:  1.333 [-2.668, 32.740], mean action: 3.926 [0.000, 16.000],  loss: 0.019493, mae: 0.310760, mean_q: 0.536924, mean_eps: 0.000000
 3143/5000: episode: 110, duration: 0.569s, episode steps:  40, steps per second:  70, episode reward: 38.858, mean reward:  0.971 [-2.412, 30.814], mean action: 4.100 [0.000, 20.000],  loss: 0.020685, mae: 0.310750, mean_q: 0.535775, mean_eps: 0.000000
 3161/5000: episode: 111, duration: 0.267s, episode steps:  18, steps per second:  67, episode reward: 41.214, mean reward:  2.290 [-2.369, 31.740], mean action: 2.833 [0.000, 16.000],  loss: 0.021189, mae: 0.309043, mean_q: 0.520670, mean_eps: 0.000000
 3180/5000: episode: 112, duration: 0.290s, episode steps:  19, steps per second:  65, episode reward: 43.750, mean reward:  2.303 [-3.000, 31.734], mean action: 6.526 [1.000, 16.000],  loss: 0.022082, mae: 0.317923, mean_q: 0.497301, mean_eps: 0.000000
 3207/5000: episode: 113, duration: 0.436s, episode steps:  27, steps per second:  62, episode reward: 38.111, mean reward:  1.412 [-3.000, 32.506], mean action: 10.000 [1.000, 20.000],  loss: 0.022128, mae: 0.321521, mean_q: 0.523834, mean_eps: 0.000000
 3230/5000: episode: 114, duration: 0.341s, episode steps:  23, steps per second:  67, episode reward: 41.232, mean reward:  1.793 [-2.461, 32.390], mean action: 2.348 [0.000, 9.000],  loss: 0.020357, mae: 0.311724, mean_q: 0.559675, mean_eps: 0.000000
 3248/5000: episode: 115, duration: 0.269s, episode steps:  18, steps per second:  67, episode reward: 41.413, mean reward:  2.301 [-2.174, 32.134], mean action: 1.889 [0.000, 9.000],  loss: 0.020501, mae: 0.319043, mean_q: 0.552534, mean_eps: 0.000000
 3276/5000: episode: 116, duration: 0.407s, episode steps:  28, steps per second:  69, episode reward: 41.088, mean reward:  1.467 [-2.507, 31.874], mean action: 3.036 [0.000, 19.000],  loss: 0.023372, mae: 0.330984, mean_q: 0.583104, mean_eps: 0.000000
 3323/5000: episode: 117, duration: 0.721s, episode steps:  47, steps per second:  65, episode reward: 36.000, mean reward:  0.766 [-2.655, 32.310], mean action: 3.894 [0.000, 20.000],  loss: 0.019784, mae: 0.314163, mean_q: 0.512341, mean_eps: 0.000000
 3353/5000: episode: 118, duration: 0.438s, episode steps:  30, steps per second:  69, episode reward: 35.498, mean reward:  1.183 [-3.000, 32.190], mean action: 3.767 [1.000, 16.000],  loss: 0.020438, mae: 0.315595, mean_q: 0.535731, mean_eps: 0.000000
 3370/5000: episode: 119, duration: 0.356s, episode steps:  17, steps per second:  48, episode reward: 44.157, mean reward:  2.597 [-2.459, 32.180], mean action: 1.529 [0.000, 6.000],  loss: 0.017009, mae: 0.297119, mean_q: 0.490560, mean_eps: 0.000000
 3414/5000: episode: 120, duration: 0.639s, episode steps:  44, steps per second:  69, episode reward: 38.550, mean reward:  0.876 [-2.228, 32.050], mean action: 4.523 [0.000, 19.000],  loss: 0.017087, mae: 0.301059, mean_q: 0.532447, mean_eps: 0.000000
 3450/5000: episode: 121, duration: 0.510s, episode steps:  36, steps per second:  71, episode reward: 41.431, mean reward:  1.151 [-2.357, 32.171], mean action: 3.611 [0.000, 8.000],  loss: 0.020249, mae: 0.319425, mean_q: 0.512976, mean_eps: 0.000000
 3481/5000: episode: 122, duration: 0.458s, episode steps:  31, steps per second:  68, episode reward: 42.000, mean reward:  1.355 [-2.174, 32.870], mean action: 2.129 [0.000, 12.000],  loss: 0.021550, mae: 0.320983, mean_q: 0.527845, mean_eps: 0.000000
 3500/5000: episode: 123, duration: 0.292s, episode steps:  19, steps per second:  65, episode reward: 38.421, mean reward:  2.022 [-2.494, 32.015], mean action: 3.000 [0.000, 16.000],  loss: 0.016269, mae: 0.293530, mean_q: 0.533931, mean_eps: 0.000000
 3517/5000: episode: 124, duration: 0.259s, episode steps:  17, steps per second:  66, episode reward: 44.009, mean reward:  2.589 [-2.161, 32.566], mean action: 2.353 [0.000, 14.000],  loss: 0.023889, mae: 0.331359, mean_q: 0.543518, mean_eps: 0.000000
 3542/5000: episode: 125, duration: 0.381s, episode steps:  25, steps per second:  66, episode reward: 40.645, mean reward:  1.626 [-2.502, 32.260], mean action: 2.440 [0.000, 12.000],  loss: 0.017745, mae: 0.302575, mean_q: 0.476877, mean_eps: 0.000000
 3566/5000: episode: 126, duration: 0.342s, episode steps:  24, steps per second:  70, episode reward: 42.000, mean reward:  1.750 [-2.474, 32.210], mean action: 1.833 [1.000, 12.000],  loss: 0.025151, mae: 0.341031, mean_q: 0.494904, mean_eps: 0.000000
 3596/5000: episode: 127, duration: 0.425s, episode steps:  30, steps per second:  71, episode reward: 36.000, mean reward:  1.200 [-2.923, 30.950], mean action: 2.567 [0.000, 18.000],  loss: 0.020219, mae: 0.314319, mean_q: 0.581013, mean_eps: 0.000000
 3611/5000: episode: 128, duration: 0.235s, episode steps:  15, steps per second:  64, episode reward: 47.408, mean reward:  3.161 [-0.325, 32.550], mean action: 2.600 [1.000, 13.000],  loss: 0.021970, mae: 0.330999, mean_q: 0.574757, mean_eps: 0.000000
 3632/5000: episode: 129, duration: 0.310s, episode steps:  21, steps per second:  68, episode reward: 43.805, mean reward:  2.086 [-2.470, 32.070], mean action: 4.429 [0.000, 15.000],  loss: 0.020029, mae: 0.323195, mean_q: 0.516785, mean_eps: 0.000000
 3649/5000: episode: 130, duration: 0.264s, episode steps:  17, steps per second:  64, episode reward: 44.338, mean reward:  2.608 [-2.329, 32.280], mean action: 3.647 [0.000, 15.000],  loss: 0.021914, mae: 0.337831, mean_q: 0.484261, mean_eps: 0.000000
 3669/5000: episode: 131, duration: 0.297s, episode steps:  20, steps per second:  67, episode reward: 43.859, mean reward:  2.193 [-2.309, 32.114], mean action: 2.750 [0.000, 15.000],  loss: 0.022032, mae: 0.327495, mean_q: 0.483057, mean_eps: 0.000000
 3696/5000: episode: 132, duration: 0.382s, episode steps:  27, steps per second:  71, episode reward: 38.612, mean reward:  1.430 [-2.320, 32.073], mean action: 2.889 [0.000, 15.000],  loss: 0.019329, mae: 0.321754, mean_q: 0.539345, mean_eps: 0.000000
 3718/5000: episode: 133, duration: 0.329s, episode steps:  22, steps per second:  67, episode reward: 44.336, mean reward:  2.015 [-2.556, 32.280], mean action: 4.091 [0.000, 19.000],  loss: 0.023586, mae: 0.347623, mean_q: 0.559283, mean_eps: 0.000000
 3777/5000: episode: 134, duration: 0.841s, episode steps:  59, steps per second:  70, episode reward: 40.991, mean reward:  0.695 [-2.159, 32.520], mean action: 3.864 [0.000, 20.000],  loss: 0.021246, mae: 0.328148, mean_q: 0.521700, mean_eps: 0.000000
 3794/5000: episode: 135, duration: 0.251s, episode steps:  17, steps per second:  68, episode reward: 41.345, mean reward:  2.432 [-3.000, 33.000], mean action: 2.765 [0.000, 9.000],  loss: 0.024568, mae: 0.342292, mean_q: 0.511003, mean_eps: 0.000000
 3826/5000: episode: 136, duration: 0.451s, episode steps:  32, steps per second:  71, episode reward: 35.262, mean reward:  1.102 [-3.000, 32.140], mean action: 3.812 [0.000, 16.000],  loss: 0.022260, mae: 0.325931, mean_q: 0.510244, mean_eps: 0.000000
 3851/5000: episode: 137, duration: 0.363s, episode steps:  25, steps per second:  69, episode reward: 42.000, mean reward:  1.680 [-2.683, 32.480], mean action: 3.520 [0.000, 19.000],  loss: 0.020116, mae: 0.316586, mean_q: 0.516411, mean_eps: 0.000000
 3869/5000: episode: 138, duration: 0.271s, episode steps:  18, steps per second:  66, episode reward: 41.441, mean reward:  2.302 [-2.074, 32.290], mean action: 4.500 [2.000, 19.000],  loss: 0.020803, mae: 0.313455, mean_q: 0.526259, mean_eps: 0.000000
 3889/5000: episode: 139, duration: 0.302s, episode steps:  20, steps per second:  66, episode reward: 44.343, mean reward:  2.217 [-2.418, 32.500], mean action: 1.700 [0.000, 19.000],  loss: 0.018257, mae: 0.305827, mean_q: 0.549958, mean_eps: 0.000000
 3921/5000: episode: 140, duration: 0.447s, episode steps:  32, steps per second:  72, episode reward: 41.455, mean reward:  1.295 [-2.744, 32.021], mean action: 1.219 [0.000, 11.000],  loss: 0.020844, mae: 0.318272, mean_q: 0.533217, mean_eps: 0.000000
 3951/5000: episode: 141, duration: 0.431s, episode steps:  30, steps per second:  70, episode reward: 45.000, mean reward:  1.500 [-2.199, 32.280], mean action: 0.767 [0.000, 6.000],  loss: 0.021815, mae: 0.322060, mean_q: 0.565806, mean_eps: 0.000000
 3968/5000: episode: 142, duration: 0.252s, episode steps:  17, steps per second:  68, episode reward: 41.297, mean reward:  2.429 [-3.000, 32.160], mean action: 2.647 [0.000, 18.000],  loss: 0.020364, mae: 0.319019, mean_q: 0.559959, mean_eps: 0.000000
 3996/5000: episode: 143, duration: 0.406s, episode steps:  28, steps per second:  69, episode reward: 38.939, mean reward:  1.391 [-2.140, 32.170], mean action: 3.250 [0.000, 15.000],  loss: 0.019447, mae: 0.315990, mean_q: 0.539050, mean_eps: 0.000000
 4022/5000: episode: 144, duration: 0.379s, episode steps:  26, steps per second:  69, episode reward: 43.950, mean reward:  1.690 [-2.096, 32.143], mean action: 2.577 [0.000, 13.000],  loss: 0.023694, mae: 0.344276, mean_q: 0.583336, mean_eps: 0.000000
 4049/5000: episode: 145, duration: 0.395s, episode steps:  27, steps per second:  68, episode reward: 38.487, mean reward:  1.425 [-3.000, 31.817], mean action: 2.741 [0.000, 15.000],  loss: 0.020692, mae: 0.323759, mean_q: 0.589605, mean_eps: 0.000000
 4066/5000: episode: 146, duration: 0.259s, episode steps:  17, steps per second:  66, episode reward: 44.444, mean reward:  2.614 [-2.232, 32.370], mean action: 1.706 [0.000, 19.000],  loss: 0.023483, mae: 0.336439, mean_q: 0.529435, mean_eps: 0.000000
 4090/5000: episode: 147, duration: 0.349s, episode steps:  24, steps per second:  69, episode reward: 39.000, mean reward:  1.625 [-2.688, 32.330], mean action: 3.042 [0.000, 16.000],  loss: 0.019808, mae: 0.315292, mean_q: 0.553009, mean_eps: 0.000000
 4103/5000: episode: 148, duration: 0.203s, episode steps:  13, steps per second:  64, episode reward: 41.939, mean reward:  3.226 [-3.000, 30.992], mean action: 2.846 [0.000, 18.000],  loss: 0.021918, mae: 0.332729, mean_q: 0.555282, mean_eps: 0.000000
 4128/5000: episode: 149, duration: 0.359s, episode steps:  25, steps per second:  70, episode reward: 37.677, mean reward:  1.507 [-2.578, 32.270], mean action: 4.560 [0.000, 19.000],  loss: 0.015906, mae: 0.301056, mean_q: 0.571065, mean_eps: 0.000000
 4153/5000: episode: 150, duration: 0.578s, episode steps:  25, steps per second:  43, episode reward: 41.143, mean reward:  1.646 [-2.360, 32.210], mean action: 5.080 [0.000, 19.000],  loss: 0.018227, mae: 0.311389, mean_q: 0.527951, mean_eps: 0.000000
 4177/5000: episode: 151, duration: 0.366s, episode steps:  24, steps per second:  66, episode reward: 44.341, mean reward:  1.848 [-2.238, 32.169], mean action: 7.750 [0.000, 19.000],  loss: 0.021968, mae: 0.326665, mean_q: 0.522735, mean_eps: 0.000000
 4193/5000: episode: 152, duration: 0.237s, episode steps:  16, steps per second:  68, episode reward: 41.705, mean reward:  2.607 [-2.443, 32.260], mean action: 3.812 [0.000, 19.000],  loss: 0.018809, mae: 0.313014, mean_q: 0.577260, mean_eps: 0.000000
 4225/5000: episode: 153, duration: 0.450s, episode steps:  32, steps per second:  71, episode reward: 44.804, mean reward:  1.400 [-2.574, 32.272], mean action: 2.906 [0.000, 14.000],  loss: 0.022239, mae: 0.328492, mean_q: 0.539595, mean_eps: 0.000000
 4250/5000: episode: 154, duration: 0.438s, episode steps:  25, steps per second:  57, episode reward: 44.267, mean reward:  1.771 [-2.171, 32.093], mean action: 1.960 [1.000, 9.000],  loss: 0.019031, mae: 0.313831, mean_q: 0.533628, mean_eps: 0.000000
 4279/5000: episode: 155, duration: 0.422s, episode steps:  29, steps per second:  69, episode reward: 35.939, mean reward:  1.239 [-2.553, 32.475], mean action: 2.724 [0.000, 16.000],  loss: 0.020568, mae: 0.324153, mean_q: 0.503459, mean_eps: 0.000000
 4305/5000: episode: 156, duration: 0.381s, episode steps:  26, steps per second:  68, episode reward: 41.460, mean reward:  1.595 [-3.000, 31.973], mean action: 2.846 [0.000, 9.000],  loss: 0.021779, mae: 0.327274, mean_q: 0.521245, mean_eps: 0.000000
 4323/5000: episode: 157, duration: 0.272s, episode steps:  18, steps per second:  66, episode reward: 43.511, mean reward:  2.417 [-3.000, 32.188], mean action: 3.167 [0.000, 13.000],  loss: 0.015985, mae: 0.299096, mean_q: 0.536902, mean_eps: 0.000000
 4357/5000: episode: 158, duration: 0.505s, episode steps:  34, steps per second:  67, episode reward: 34.450, mean reward:  1.013 [-2.752, 31.620], mean action: 3.735 [0.000, 14.000],  loss: 0.018948, mae: 0.310947, mean_q: 0.570352, mean_eps: 0.000000
 4376/5000: episode: 159, duration: 0.285s, episode steps:  19, steps per second:  67, episode reward: 42.000, mean reward:  2.211 [-2.450, 32.140], mean action: 3.000 [0.000, 16.000],  loss: 0.022486, mae: 0.330050, mean_q: 0.579970, mean_eps: 0.000000
 4393/5000: episode: 160, duration: 0.387s, episode steps:  17, steps per second:  44, episode reward: 41.893, mean reward:  2.464 [-2.041, 32.180], mean action: 4.353 [0.000, 16.000],  loss: 0.022644, mae: 0.335099, mean_q: 0.569247, mean_eps: 0.000000
 4423/5000: episode: 161, duration: 0.447s, episode steps:  30, steps per second:  67, episode reward: 35.450, mean reward:  1.182 [-2.903, 32.800], mean action: 4.633 [0.000, 18.000],  loss: 0.021110, mae: 0.325701, mean_q: 0.591571, mean_eps: 0.000000
 4445/5000: episode: 162, duration: 0.330s, episode steps:  22, steps per second:  67, episode reward: 43.756, mean reward:  1.989 [-2.464, 32.513], mean action: 4.136 [0.000, 18.000],  loss: 0.023421, mae: 0.341777, mean_q: 0.535369, mean_eps: 0.000000
 4474/5000: episode: 163, duration: 0.415s, episode steps:  29, steps per second:  70, episode reward: 34.313, mean reward:  1.183 [-3.000, 31.975], mean action: 6.448 [0.000, 16.000],  loss: 0.021158, mae: 0.326922, mean_q: 0.544946, mean_eps: 0.000000
 4509/5000: episode: 164, duration: 0.499s, episode steps:  35, steps per second:  70, episode reward: 41.950, mean reward:  1.199 [-2.171, 32.110], mean action: 3.771 [0.000, 16.000],  loss: 0.020837, mae: 0.326322, mean_q: 0.519365, mean_eps: 0.000000
 4531/5000: episode: 165, duration: 0.330s, episode steps:  22, steps per second:  67, episode reward: 44.376, mean reward:  2.017 [-2.353, 32.080], mean action: 1.409 [0.000, 16.000],  loss: 0.019540, mae: 0.314076, mean_q: 0.478612, mean_eps: 0.000000
 4565/5000: episode: 166, duration: 0.487s, episode steps:  34, steps per second:  70, episode reward: 38.632, mean reward:  1.136 [-2.162, 31.762], mean action: 3.118 [0.000, 15.000],  loss: 0.023426, mae: 0.336187, mean_q: 0.529280, mean_eps: 0.000000
 4590/5000: episode: 167, duration: 0.358s, episode steps:  25, steps per second:  70, episode reward: 41.703, mean reward:  1.668 [-2.311, 32.660], mean action: 3.720 [0.000, 15.000],  loss: 0.018865, mae: 0.317258, mean_q: 0.496100, mean_eps: 0.000000
 4615/5000: episode: 168, duration: 0.359s, episode steps:  25, steps per second:  70, episode reward: 32.384, mean reward:  1.295 [-2.869, 32.360], mean action: 6.400 [0.000, 19.000],  loss: 0.017517, mae: 0.309950, mean_q: 0.572799, mean_eps: 0.000000
 4633/5000: episode: 169, duration: 0.279s, episode steps:  18, steps per second:  65, episode reward: 46.662, mean reward:  2.592 [-0.243, 32.430], mean action: 3.444 [0.000, 12.000],  loss: 0.023929, mae: 0.335282, mean_q: 0.564558, mean_eps: 0.000000
 4668/5000: episode: 170, duration: 0.493s, episode steps:  35, steps per second:  71, episode reward: 38.898, mean reward:  1.111 [-2.521, 32.268], mean action: 3.857 [0.000, 20.000],  loss: 0.022177, mae: 0.332872, mean_q: 0.549668, mean_eps: 0.000000
 4693/5000: episode: 171, duration: 0.366s, episode steps:  25, steps per second:  68, episode reward: 38.617, mean reward:  1.545 [-2.615, 31.667], mean action: 2.840 [0.000, 16.000],  loss: 0.020489, mae: 0.330854, mean_q: 0.533022, mean_eps: 0.000000
 4733/5000: episode: 172, duration: 0.616s, episode steps:  40, steps per second:  65, episode reward: 38.137, mean reward:  0.953 [-3.000, 32.090], mean action: 2.425 [0.000, 15.000],  loss: 0.021613, mae: 0.331447, mean_q: 0.570956, mean_eps: 0.000000
 4760/5000: episode: 173, duration: 0.420s, episode steps:  27, steps per second:  64, episode reward: 41.318, mean reward:  1.530 [-2.419, 32.240], mean action: 2.778 [0.000, 16.000],  loss: 0.019383, mae: 0.320157, mean_q: 0.534131, mean_eps: 0.000000
 4816/5000: episode: 174, duration: 0.792s, episode steps:  56, steps per second:  71, episode reward: 35.127, mean reward:  0.627 [-2.471, 32.140], mean action: 3.946 [0.000, 19.000],  loss: 0.020201, mae: 0.321522, mean_q: 0.503461, mean_eps: 0.000000
 4889/5000: episode: 175, duration: 1.013s, episode steps:  73, steps per second:  72, episode reward: -32.650, mean reward: -0.447 [-31.746,  2.410], mean action: 3.329 [0.000, 15.000],  loss: 0.021218, mae: 0.317864, mean_q: 0.542591, mean_eps: 0.000000
 4909/5000: episode: 176, duration: 0.312s, episode steps:  20, steps per second:  64, episode reward: 44.760, mean reward:  2.238 [-2.750, 32.140], mean action: 3.500 [1.000, 16.000],  loss: 0.023913, mae: 0.338859, mean_q: 0.480885, mean_eps: 0.000000
 4934/5000: episode: 177, duration: 0.365s, episode steps:  25, steps per second:  69, episode reward: 38.797, mean reward:  1.552 [-3.000, 32.037], mean action: 4.960 [0.000, 19.000],  loss: 0.018922, mae: 0.313335, mean_q: 0.513494, mean_eps: 0.000000
 4956/5000: episode: 178, duration: 0.330s, episode steps:  22, steps per second:  67, episode reward: 39.000, mean reward:  1.773 [-3.000, 32.460], mean action: 3.545 [0.000, 19.000],  loss: 0.016477, mae: 0.302996, mean_q: 0.482452, mean_eps: 0.000000
 4992/5000: episode: 179, duration: 0.514s, episode steps:  36, steps per second:  70, episode reward: 32.878, mean reward:  0.913 [-2.872, 32.080], mean action: 5.778 [0.000, 21.000],  loss: 0.018492, mae: 0.305498, mean_q: 0.501829, mean_eps: 0.000000
done, took 66.076 seconds
DQN Evaluation: 10765 victories out of 12552 episodes
Training for 5000 steps ...
   22/5000: episode: 1, duration: 0.170s, episode steps:  22, steps per second: 129, episode reward: -39.000, mean reward: -1.773 [-32.118,  2.506], mean action: 7.864 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   45/5000: episode: 2, duration: 0.195s, episode steps:  23, steps per second: 118, episode reward: 38.467, mean reward:  1.672 [-2.161, 32.060], mean action: 5.826 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   73/5000: episode: 3, duration: 0.188s, episode steps:  28, steps per second: 149, episode reward: 35.413, mean reward:  1.265 [-2.561, 31.933], mean action: 4.071 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   97/5000: episode: 4, duration: 0.175s, episode steps:  24, steps per second: 137, episode reward: 35.873, mean reward:  1.495 [-3.000, 32.273], mean action: 5.167 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  123/5000: episode: 5, duration: 0.174s, episode steps:  26, steps per second: 149, episode reward: 37.655, mean reward:  1.448 [-2.389, 32.760], mean action: 4.538 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  141/5000: episode: 6, duration: 0.126s, episode steps:  18, steps per second: 143, episode reward: 38.818, mean reward:  2.157 [-3.000, 32.385], mean action: 4.222 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  158/5000: episode: 7, duration: 0.123s, episode steps:  17, steps per second: 138, episode reward: 47.090, mean reward:  2.770 [-0.383, 32.287], mean action: 3.235 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  180/5000: episode: 8, duration: 0.153s, episode steps:  22, steps per second: 144, episode reward: 35.658, mean reward:  1.621 [-2.186, 32.008], mean action: 4.136 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  206/5000: episode: 9, duration: 0.180s, episode steps:  26, steps per second: 145, episode reward: 33.000, mean reward:  1.269 [-2.378, 32.630], mean action: 4.115 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  222/5000: episode: 10, duration: 0.115s, episode steps:  16, steps per second: 139, episode reward: 38.097, mean reward:  2.381 [-3.000, 32.054], mean action: 3.875 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  245/5000: episode: 11, duration: 0.158s, episode steps:  23, steps per second: 145, episode reward: 36.000, mean reward:  1.565 [-2.267, 32.150], mean action: 3.870 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  264/5000: episode: 12, duration: 0.138s, episode steps:  19, steps per second: 137, episode reward: 38.677, mean reward:  2.036 [-3.000, 32.560], mean action: 2.526 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  290/5000: episode: 13, duration: 0.172s, episode steps:  26, steps per second: 151, episode reward: -33.000, mean reward: -1.269 [-32.154,  2.400], mean action: 6.115 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  308/5000: episode: 14, duration: 0.140s, episode steps:  18, steps per second: 128, episode reward: 36.000, mean reward:  2.000 [-3.000, 32.500], mean action: 3.278 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  326/5000: episode: 15, duration: 0.127s, episode steps:  18, steps per second: 141, episode reward: 38.483, mean reward:  2.138 [-2.604, 33.000], mean action: 2.500 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  345/5000: episode: 16, duration: 0.133s, episode steps:  19, steps per second: 143, episode reward: 41.171, mean reward:  2.167 [-2.459, 32.030], mean action: 2.895 [2.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  390/5000: episode: 17, duration: 0.280s, episode steps:  45, steps per second: 161, episode reward: 32.147, mean reward:  0.714 [-3.000, 31.787], mean action: 5.133 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  410/5000: episode: 18, duration: 0.140s, episode steps:  20, steps per second: 142, episode reward: 41.462, mean reward:  2.073 [-3.000, 32.290], mean action: 3.950 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  432/5000: episode: 19, duration: 0.149s, episode steps:  22, steps per second: 147, episode reward: 38.640, mean reward:  1.756 [-2.657, 32.685], mean action: 8.545 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  474/5000: episode: 20, duration: 0.274s, episode steps:  42, steps per second: 153, episode reward: 37.752, mean reward:  0.899 [-2.309, 31.983], mean action: 3.857 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  497/5000: episode: 21, duration: 0.156s, episode steps:  23, steps per second: 148, episode reward: 32.619, mean reward:  1.418 [-3.000, 33.000], mean action: 4.565 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  525/5000: episode: 22, duration: 0.178s, episode steps:  28, steps per second: 157, episode reward: 32.563, mean reward:  1.163 [-3.000, 32.563], mean action: 7.679 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  555/5000: episode: 23, duration: 0.193s, episode steps:  30, steps per second: 155, episode reward: -32.910, mean reward: -1.097 [-32.050,  2.626], mean action: 6.800 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  575/5000: episode: 24, duration: 0.148s, episode steps:  20, steps per second: 135, episode reward: 41.688, mean reward:  2.084 [-2.401, 32.598], mean action: 1.800 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  599/5000: episode: 25, duration: 0.167s, episode steps:  24, steps per second: 144, episode reward: 35.901, mean reward:  1.496 [-2.565, 32.431], mean action: 3.375 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  616/5000: episode: 26, duration: 0.129s, episode steps:  17, steps per second: 132, episode reward: 38.885, mean reward:  2.287 [-2.643, 32.390], mean action: 3.824 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  643/5000: episode: 27, duration: 0.174s, episode steps:  27, steps per second: 155, episode reward: 35.551, mean reward:  1.317 [-2.301, 32.420], mean action: 4.000 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  671/5000: episode: 28, duration: 0.193s, episode steps:  28, steps per second: 145, episode reward: -32.810, mean reward: -1.172 [-32.213,  2.304], mean action: 6.607 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  694/5000: episode: 29, duration: 0.223s, episode steps:  23, steps per second: 103, episode reward: 38.392, mean reward:  1.669 [-2.722, 32.040], mean action: 4.217 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  718/5000: episode: 30, duration: 0.160s, episode steps:  24, steps per second: 150, episode reward: 35.954, mean reward:  1.498 [-2.458, 32.274], mean action: 4.542 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  754/5000: episode: 31, duration: 0.224s, episode steps:  36, steps per second: 161, episode reward: -33.000, mean reward: -0.917 [-32.423,  2.470], mean action: 5.639 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  774/5000: episode: 32, duration: 0.137s, episode steps:  20, steps per second: 146, episode reward: -39.000, mean reward: -1.950 [-30.610,  2.610], mean action: 5.050 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  794/5000: episode: 33, duration: 0.129s, episode steps:  20, steps per second: 154, episode reward: 38.434, mean reward:  1.922 [-2.530, 31.984], mean action: 5.200 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  819/5000: episode: 34, duration: 0.271s, episode steps:  25, steps per second:  92, episode reward: -32.960, mean reward: -1.318 [-32.095,  3.000], mean action: 6.520 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  852/5000: episode: 35, duration: 0.255s, episode steps:  33, steps per second: 129, episode reward: 38.800, mean reward:  1.176 [-3.000, 32.180], mean action: 4.273 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  872/5000: episode: 36, duration: 0.166s, episode steps:  20, steps per second: 120, episode reward: 38.085, mean reward:  1.904 [-2.547, 32.350], mean action: 6.300 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  901/5000: episode: 37, duration: 0.201s, episode steps:  29, steps per second: 144, episode reward: -32.100, mean reward: -1.107 [-32.396,  2.302], mean action: 7.655 [1.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  921/5000: episode: 38, duration: 0.138s, episode steps:  20, steps per second: 145, episode reward: 35.518, mean reward:  1.776 [-2.560, 32.250], mean action: 4.100 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  945/5000: episode: 39, duration: 0.161s, episode steps:  24, steps per second: 149, episode reward: 35.051, mean reward:  1.460 [-3.000, 32.040], mean action: 5.417 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  968/5000: episode: 40, duration: 0.153s, episode steps:  23, steps per second: 150, episode reward: 32.835, mean reward:  1.428 [-2.423, 32.375], mean action: 6.304 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  988/5000: episode: 41, duration: 0.137s, episode steps:  20, steps per second: 146, episode reward: 37.949, mean reward:  1.897 [-2.268, 32.047], mean action: 2.950 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1016/5000: episode: 42, duration: 0.320s, episode steps:  28, steps per second:  88, episode reward: 33.000, mean reward:  1.179 [-2.763, 32.570], mean action: 4.857 [0.000, 19.000],  loss: 0.020442, mae: 0.321800, mean_q: 0.522473, mean_eps: 0.000000
 1039/5000: episode: 43, duration: 0.338s, episode steps:  23, steps per second:  68, episode reward: 32.270, mean reward:  1.403 [-2.407, 31.530], mean action: 6.826 [0.000, 19.000],  loss: 0.022444, mae: 0.319413, mean_q: 0.535157, mean_eps: 0.000000
 1065/5000: episode: 44, duration: 0.383s, episode steps:  26, steps per second:  68, episode reward: 37.215, mean reward:  1.431 [-3.000, 32.676], mean action: 5.154 [0.000, 19.000],  loss: 0.018836, mae: 0.309389, mean_q: 0.562442, mean_eps: 0.000000
 1090/5000: episode: 45, duration: 0.367s, episode steps:  25, steps per second:  68, episode reward: 37.831, mean reward:  1.513 [-2.456, 32.003], mean action: 4.920 [0.000, 19.000],  loss: 0.021804, mae: 0.324826, mean_q: 0.560271, mean_eps: 0.000000
 1109/5000: episode: 46, duration: 0.277s, episode steps:  19, steps per second:  69, episode reward: 38.515, mean reward:  2.027 [-2.248, 31.973], mean action: 4.526 [0.000, 19.000],  loss: 0.020660, mae: 0.311846, mean_q: 0.561032, mean_eps: 0.000000
 1129/5000: episode: 47, duration: 0.298s, episode steps:  20, steps per second:  67, episode reward: 38.471, mean reward:  1.924 [-2.345, 32.350], mean action: 4.150 [1.000, 19.000],  loss: 0.028149, mae: 0.342704, mean_q: 0.611796, mean_eps: 0.000000
 1148/5000: episode: 48, duration: 0.280s, episode steps:  19, steps per second:  68, episode reward: 37.900, mean reward:  1.995 [-2.494, 31.839], mean action: 2.895 [0.000, 19.000],  loss: 0.019677, mae: 0.309183, mean_q: 0.631445, mean_eps: 0.000000
 1170/5000: episode: 49, duration: 0.331s, episode steps:  22, steps per second:  67, episode reward: -33.000, mean reward: -1.500 [-30.510,  2.231], mean action: 5.455 [0.000, 19.000],  loss: 0.023705, mae: 0.326951, mean_q: 0.664161, mean_eps: 0.000000
 1197/5000: episode: 50, duration: 0.398s, episode steps:  27, steps per second:  68, episode reward: 35.976, mean reward:  1.332 [-2.455, 32.120], mean action: 4.593 [0.000, 19.000],  loss: 0.020619, mae: 0.317248, mean_q: 0.607032, mean_eps: 0.000000
 1218/5000: episode: 51, duration: 0.338s, episode steps:  21, steps per second:  62, episode reward: 41.214, mean reward:  1.963 [-2.277, 31.941], mean action: 5.619 [2.000, 19.000],  loss: 0.016899, mae: 0.293473, mean_q: 0.564391, mean_eps: 0.000000
 1234/5000: episode: 52, duration: 0.358s, episode steps:  16, steps per second:  45, episode reward: 41.478, mean reward:  2.592 [-3.000, 32.704], mean action: 3.562 [0.000, 15.000],  loss: 0.019717, mae: 0.315365, mean_q: 0.505817, mean_eps: 0.000000
 1255/5000: episode: 53, duration: 0.465s, episode steps:  21, steps per second:  45, episode reward: 35.193, mean reward:  1.676 [-2.417, 31.928], mean action: 4.476 [0.000, 19.000],  loss: 0.017703, mae: 0.309683, mean_q: 0.453489, mean_eps: 0.000000
 1277/5000: episode: 54, duration: 0.336s, episode steps:  22, steps per second:  66, episode reward: -32.280, mean reward: -1.467 [-32.069,  2.902], mean action: 5.455 [0.000, 16.000],  loss: 0.019528, mae: 0.310352, mean_q: 0.525169, mean_eps: 0.000000
 1304/5000: episode: 55, duration: 0.400s, episode steps:  27, steps per second:  67, episode reward: -33.000, mean reward: -1.222 [-32.693,  2.800], mean action: 3.778 [0.000, 12.000],  loss: 0.023644, mae: 0.318753, mean_q: 0.516180, mean_eps: 0.000000
 1327/5000: episode: 56, duration: 0.343s, episode steps:  23, steps per second:  67, episode reward: 32.464, mean reward:  1.411 [-2.931, 32.102], mean action: 4.696 [1.000, 12.000],  loss: 0.018673, mae: 0.297801, mean_q: 0.547938, mean_eps: 0.000000
 1358/5000: episode: 57, duration: 0.446s, episode steps:  31, steps per second:  69, episode reward: -32.580, mean reward: -1.051 [-32.329,  2.450], mean action: 3.742 [1.000, 16.000],  loss: 0.019873, mae: 0.310990, mean_q: 0.508405, mean_eps: 0.000000
 1381/5000: episode: 58, duration: 0.359s, episode steps:  23, steps per second:  64, episode reward: -32.980, mean reward: -1.434 [-32.018,  2.840], mean action: 4.739 [0.000, 19.000],  loss: 0.017467, mae: 0.296493, mean_q: 0.548158, mean_eps: 0.000000
 1410/5000: episode: 59, duration: 0.439s, episode steps:  29, steps per second:  66, episode reward: 35.198, mean reward:  1.214 [-3.000, 32.856], mean action: 6.207 [0.000, 20.000],  loss: 0.019706, mae: 0.315884, mean_q: 0.587647, mean_eps: 0.000000
 1438/5000: episode: 60, duration: 0.411s, episode steps:  28, steps per second:  68, episode reward: 37.733, mean reward:  1.348 [-2.378, 32.110], mean action: 5.786 [0.000, 19.000],  loss: 0.018839, mae: 0.317434, mean_q: 0.585345, mean_eps: 0.000000
 1456/5000: episode: 61, duration: 0.275s, episode steps:  18, steps per second:  65, episode reward: 41.394, mean reward:  2.300 [-3.000, 32.260], mean action: 3.333 [1.000, 19.000],  loss: 0.021643, mae: 0.326918, mean_q: 0.507222, mean_eps: 0.000000
 1482/5000: episode: 62, duration: 0.374s, episode steps:  26, steps per second:  70, episode reward: -32.520, mean reward: -1.251 [-31.960,  3.000], mean action: 3.885 [0.000, 19.000],  loss: 0.022113, mae: 0.333219, mean_q: 0.576817, mean_eps: 0.000000
 1505/5000: episode: 63, duration: 0.337s, episode steps:  23, steps per second:  68, episode reward: -32.790, mean reward: -1.426 [-32.145,  3.470], mean action: 3.261 [0.000, 9.000],  loss: 0.019091, mae: 0.315189, mean_q: 0.602789, mean_eps: 0.000000
 1529/5000: episode: 64, duration: 0.345s, episode steps:  24, steps per second:  69, episode reward: 35.245, mean reward:  1.469 [-2.402, 32.130], mean action: 4.750 [0.000, 16.000],  loss: 0.020252, mae: 0.324855, mean_q: 0.591484, mean_eps: 0.000000
 1551/5000: episode: 65, duration: 0.313s, episode steps:  22, steps per second:  70, episode reward: -32.910, mean reward: -1.496 [-32.850,  2.940], mean action: 4.818 [0.000, 16.000],  loss: 0.021145, mae: 0.333933, mean_q: 0.564580, mean_eps: 0.000000
 1569/5000: episode: 66, duration: 0.273s, episode steps:  18, steps per second:  66, episode reward: 41.319, mean reward:  2.295 [-2.701, 32.460], mean action: 2.333 [0.000, 16.000],  loss: 0.019078, mae: 0.324571, mean_q: 0.584139, mean_eps: 0.000000
 1595/5000: episode: 67, duration: 0.375s, episode steps:  26, steps per second:  69, episode reward: -32.330, mean reward: -1.243 [-32.236,  2.170], mean action: 5.385 [0.000, 19.000],  loss: 0.019860, mae: 0.324358, mean_q: 0.507535, mean_eps: 0.000000
 1610/5000: episode: 68, duration: 0.231s, episode steps:  15, steps per second:  65, episode reward: 41.521, mean reward:  2.768 [-2.537, 32.300], mean action: 2.400 [0.000, 16.000],  loss: 0.022226, mae: 0.331755, mean_q: 0.561182, mean_eps: 0.000000
 1629/5000: episode: 69, duration: 0.282s, episode steps:  19, steps per second:  67, episode reward: 39.000, mean reward:  2.053 [-2.326, 32.250], mean action: 4.474 [0.000, 16.000],  loss: 0.021465, mae: 0.322332, mean_q: 0.521486, mean_eps: 0.000000
 1666/5000: episode: 70, duration: 0.514s, episode steps:  37, steps per second:  72, episode reward: 41.350, mean reward:  1.118 [-2.188, 32.340], mean action: 3.162 [0.000, 16.000],  loss: 0.020080, mae: 0.322417, mean_q: 0.568404, mean_eps: 0.000000
 1691/5000: episode: 71, duration: 0.358s, episode steps:  25, steps per second:  70, episode reward: 32.900, mean reward:  1.316 [-2.463, 32.710], mean action: 4.320 [0.000, 18.000],  loss: 0.021616, mae: 0.325206, mean_q: 0.524231, mean_eps: 0.000000
 1702/5000: episode: 72, duration: 0.176s, episode steps:  11, steps per second:  63, episode reward: 44.343, mean reward:  4.031 [-2.088, 33.000], mean action: 1.909 [0.000, 3.000],  loss: 0.019012, mae: 0.307516, mean_q: 0.495719, mean_eps: 0.000000
 1721/5000: episode: 73, duration: 0.287s, episode steps:  19, steps per second:  66, episode reward: 38.469, mean reward:  2.025 [-2.213, 32.901], mean action: 3.263 [0.000, 12.000],  loss: 0.020690, mae: 0.314725, mean_q: 0.548823, mean_eps: 0.000000
 1745/5000: episode: 74, duration: 0.353s, episode steps:  24, steps per second:  68, episode reward: 34.233, mean reward:  1.426 [-2.262, 32.047], mean action: 5.250 [0.000, 18.000],  loss: 0.019914, mae: 0.309288, mean_q: 0.488305, mean_eps: 0.000000
 1786/5000: episode: 75, duration: 0.568s, episode steps:  41, steps per second:  72, episode reward: 38.493, mean reward:  0.939 [-3.000, 32.180], mean action: 6.195 [1.000, 14.000],  loss: 0.023420, mae: 0.333717, mean_q: 0.529470, mean_eps: 0.000000
 1808/5000: episode: 76, duration: 0.333s, episode steps:  22, steps per second:  66, episode reward: 40.746, mean reward:  1.852 [-2.023, 31.924], mean action: 3.318 [0.000, 19.000],  loss: 0.019055, mae: 0.317736, mean_q: 0.530722, mean_eps: 0.000000
 1834/5000: episode: 77, duration: 0.386s, episode steps:  26, steps per second:  67, episode reward: 38.237, mean reward:  1.471 [-2.349, 31.669], mean action: 1.769 [0.000, 11.000],  loss: 0.018246, mae: 0.312653, mean_q: 0.564430, mean_eps: 0.000000
 1860/5000: episode: 78, duration: 0.373s, episode steps:  26, steps per second:  70, episode reward: 32.673, mean reward:  1.257 [-2.183, 32.673], mean action: 4.192 [0.000, 11.000],  loss: 0.017979, mae: 0.312128, mean_q: 0.544169, mean_eps: 0.000000
 1882/5000: episode: 79, duration: 0.324s, episode steps:  22, steps per second:  68, episode reward: 38.195, mean reward:  1.736 [-2.498, 32.450], mean action: 1.364 [0.000, 11.000],  loss: 0.019179, mae: 0.320235, mean_q: 0.541275, mean_eps: 0.000000
 1902/5000: episode: 80, duration: 0.294s, episode steps:  20, steps per second:  68, episode reward: 35.082, mean reward:  1.754 [-2.397, 32.398], mean action: 3.250 [0.000, 11.000],  loss: 0.018176, mae: 0.312689, mean_q: 0.545003, mean_eps: 0.000000
 1923/5000: episode: 81, duration: 0.315s, episode steps:  21, steps per second:  67, episode reward: 35.080, mean reward:  1.670 [-3.000, 32.150], mean action: 3.238 [0.000, 11.000],  loss: 0.018367, mae: 0.314994, mean_q: 0.528142, mean_eps: 0.000000
 1948/5000: episode: 82, duration: 0.351s, episode steps:  25, steps per second:  71, episode reward: 35.283, mean reward:  1.411 [-2.508, 32.110], mean action: 8.800 [0.000, 19.000],  loss: 0.017554, mae: 0.314195, mean_q: 0.503581, mean_eps: 0.000000
 1973/5000: episode: 83, duration: 0.367s, episode steps:  25, steps per second:  68, episode reward: 35.052, mean reward:  1.402 [-2.806, 32.054], mean action: 3.560 [0.000, 14.000],  loss: 0.023842, mae: 0.340784, mean_q: 0.536464, mean_eps: 0.000000
 1996/5000: episode: 84, duration: 0.333s, episode steps:  23, steps per second:  69, episode reward: -32.120, mean reward: -1.397 [-32.420,  2.412], mean action: 3.435 [0.000, 11.000],  loss: 0.018608, mae: 0.308963, mean_q: 0.532193, mean_eps: 0.000000
 2014/5000: episode: 85, duration: 0.265s, episode steps:  18, steps per second:  68, episode reward: 38.507, mean reward:  2.139 [-2.458, 32.507], mean action: 2.667 [0.000, 11.000],  loss: 0.026154, mae: 0.344809, mean_q: 0.537309, mean_eps: 0.000000
 2040/5000: episode: 86, duration: 0.372s, episode steps:  26, steps per second:  70, episode reward: -32.300, mean reward: -1.242 [-29.654,  2.624], mean action: 7.000 [0.000, 19.000],  loss: 0.019802, mae: 0.320502, mean_q: 0.581671, mean_eps: 0.000000
 2070/5000: episode: 87, duration: 0.431s, episode steps:  30, steps per second:  70, episode reward: 38.506, mean reward:  1.284 [-3.000, 31.586], mean action: 1.967 [0.000, 12.000],  loss: 0.022836, mae: 0.331143, mean_q: 0.567726, mean_eps: 0.000000
 2088/5000: episode: 88, duration: 0.264s, episode steps:  18, steps per second:  68, episode reward: 39.000, mean reward:  2.167 [-2.090, 30.473], mean action: 3.167 [1.000, 15.000],  loss: 0.019113, mae: 0.312486, mean_q: 0.552335, mean_eps: 0.000000
 2111/5000: episode: 89, duration: 0.342s, episode steps:  23, steps per second:  67, episode reward: 38.374, mean reward:  1.668 [-2.357, 32.012], mean action: 7.435 [0.000, 20.000],  loss: 0.020717, mae: 0.322433, mean_q: 0.501603, mean_eps: 0.000000
 2142/5000: episode: 90, duration: 0.442s, episode steps:  31, steps per second:  70, episode reward: 41.520, mean reward:  1.339 [-2.478, 32.200], mean action: 2.161 [0.000, 9.000],  loss: 0.016869, mae: 0.301333, mean_q: 0.482350, mean_eps: 0.000000
 2168/5000: episode: 91, duration: 0.376s, episode steps:  26, steps per second:  69, episode reward: 32.257, mean reward:  1.241 [-2.767, 32.677], mean action: 6.308 [0.000, 19.000],  loss: 0.023442, mae: 0.321963, mean_q: 0.499134, mean_eps: 0.000000
 2210/5000: episode: 92, duration: 0.584s, episode steps:  42, steps per second:  72, episode reward: 35.900, mean reward:  0.855 [-2.283, 32.210], mean action: 2.905 [0.000, 16.000],  loss: 0.018509, mae: 0.306063, mean_q: 0.510538, mean_eps: 0.000000
 2226/5000: episode: 93, duration: 0.232s, episode steps:  16, steps per second:  69, episode reward: 42.000, mean reward:  2.625 [-2.084, 32.180], mean action: 2.500 [1.000, 5.000],  loss: 0.019766, mae: 0.307135, mean_q: 0.505450, mean_eps: 0.000000
 2247/5000: episode: 94, duration: 0.324s, episode steps:  21, steps per second:  65, episode reward: 35.902, mean reward:  1.710 [-3.000, 32.182], mean action: 4.810 [0.000, 19.000],  loss: 0.022667, mae: 0.330658, mean_q: 0.534863, mean_eps: 0.000000
 2271/5000: episode: 95, duration: 0.348s, episode steps:  24, steps per second:  69, episode reward: -35.250, mean reward: -1.469 [-32.070,  2.830], mean action: 8.792 [0.000, 19.000],  loss: 0.019604, mae: 0.313913, mean_q: 0.545653, mean_eps: 0.000000
 2291/5000: episode: 96, duration: 0.293s, episode steps:  20, steps per second:  68, episode reward: -35.430, mean reward: -1.772 [-31.713,  2.400], mean action: 7.000 [0.000, 19.000],  loss: 0.016407, mae: 0.305874, mean_q: 0.547279, mean_eps: 0.000000
 2318/5000: episode: 97, duration: 0.387s, episode steps:  27, steps per second:  70, episode reward: 33.000, mean reward:  1.222 [-2.753, 30.265], mean action: 5.296 [0.000, 19.000],  loss: 0.021259, mae: 0.327318, mean_q: 0.535961, mean_eps: 0.000000
 2353/5000: episode: 98, duration: 0.486s, episode steps:  35, steps per second:  72, episode reward: 33.000, mean reward:  0.943 [-2.489, 32.150], mean action: 5.143 [0.000, 19.000],  loss: 0.021526, mae: 0.325109, mean_q: 0.551622, mean_eps: 0.000000
 2383/5000: episode: 99, duration: 0.436s, episode steps:  30, steps per second:  69, episode reward: 35.079, mean reward:  1.169 [-2.610, 32.210], mean action: 3.367 [0.000, 19.000],  loss: 0.022371, mae: 0.326813, mean_q: 0.540119, mean_eps: 0.000000
 2414/5000: episode: 100, duration: 0.448s, episode steps:  31, steps per second:  69, episode reward: 38.271, mean reward:  1.235 [-2.269, 32.119], mean action: 2.581 [0.000, 19.000],  loss: 0.022847, mae: 0.326239, mean_q: 0.484406, mean_eps: 0.000000
 2438/5000: episode: 101, duration: 0.351s, episode steps:  24, steps per second:  68, episode reward: 35.445, mean reward:  1.477 [-3.000, 32.900], mean action: 5.542 [0.000, 19.000],  loss: 0.018771, mae: 0.316542, mean_q: 0.413266, mean_eps: 0.000000
 2457/5000: episode: 102, duration: 0.280s, episode steps:  19, steps per second:  68, episode reward: 41.780, mean reward:  2.199 [-2.320, 33.000], mean action: 1.895 [0.000, 19.000],  loss: 0.021885, mae: 0.322747, mean_q: 0.483619, mean_eps: 0.000000
 2484/5000: episode: 103, duration: 0.391s, episode steps:  27, steps per second:  69, episode reward: 35.938, mean reward:  1.331 [-2.502, 32.360], mean action: 3.111 [0.000, 20.000],  loss: 0.023250, mae: 0.337861, mean_q: 0.539460, mean_eps: 0.000000
 2502/5000: episode: 104, duration: 0.268s, episode steps:  18, steps per second:  67, episode reward: 38.540, mean reward:  2.141 [-2.431, 32.886], mean action: 2.000 [0.000, 12.000],  loss: 0.015885, mae: 0.303560, mean_q: 0.529096, mean_eps: 0.000000
 2522/5000: episode: 105, duration: 0.304s, episode steps:  20, steps per second:  66, episode reward: 39.000, mean reward:  1.950 [-2.278, 32.910], mean action: 2.050 [0.000, 12.000],  loss: 0.015747, mae: 0.304160, mean_q: 0.487651, mean_eps: 0.000000
 2547/5000: episode: 106, duration: 0.358s, episode steps:  25, steps per second:  70, episode reward: 32.102, mean reward:  1.284 [-2.340, 32.120], mean action: 4.640 [0.000, 14.000],  loss: 0.020439, mae: 0.313880, mean_q: 0.539155, mean_eps: 0.000000
 2564/5000: episode: 107, duration: 0.256s, episode steps:  17, steps per second:  66, episode reward: 41.639, mean reward:  2.449 [-2.301, 32.549], mean action: 2.000 [0.000, 14.000],  loss: 0.019809, mae: 0.315702, mean_q: 0.547717, mean_eps: 0.000000
 2600/5000: episode: 108, duration: 0.526s, episode steps:  36, steps per second:  68, episode reward: 38.159, mean reward:  1.060 [-2.999, 32.160], mean action: 4.083 [1.000, 14.000],  loss: 0.020968, mae: 0.320349, mean_q: 0.517661, mean_eps: 0.000000
 2628/5000: episode: 109, duration: 0.398s, episode steps:  28, steps per second:  70, episode reward: -38.080, mean reward: -1.360 [-33.000,  2.300], mean action: 2.750 [0.000, 12.000],  loss: 0.020394, mae: 0.319855, mean_q: 0.542448, mean_eps: 0.000000
 2655/5000: episode: 110, duration: 0.384s, episode steps:  27, steps per second:  70, episode reward: 32.250, mean reward:  1.194 [-2.691, 31.870], mean action: 4.593 [0.000, 14.000],  loss: 0.021679, mae: 0.328742, mean_q: 0.579313, mean_eps: 0.000000
 2671/5000: episode: 111, duration: 0.239s, episode steps:  16, steps per second:  67, episode reward: -38.350, mean reward: -2.397 [-32.350,  2.800], mean action: 7.375 [1.000, 20.000],  loss: 0.020252, mae: 0.316212, mean_q: 0.483641, mean_eps: 0.000000
 2691/5000: episode: 112, duration: 0.504s, episode steps:  20, steps per second:  40, episode reward: 35.581, mean reward:  1.779 [-2.701, 32.581], mean action: 3.900 [0.000, 15.000],  loss: 0.020030, mae: 0.323045, mean_q: 0.488972, mean_eps: 0.000000
 2713/5000: episode: 113, duration: 0.321s, episode steps:  22, steps per second:  69, episode reward: 42.000, mean reward:  1.909 [-2.528, 32.130], mean action: 1.636 [0.000, 12.000],  loss: 0.023931, mae: 0.336908, mean_q: 0.533089, mean_eps: 0.000000
 2740/5000: episode: 114, duration: 0.389s, episode steps:  27, steps per second:  69, episode reward: 35.221, mean reward:  1.304 [-2.434, 32.824], mean action: 3.852 [0.000, 14.000],  loss: 0.020769, mae: 0.321448, mean_q: 0.538090, mean_eps: 0.000000
 2754/5000: episode: 115, duration: 0.215s, episode steps:  14, steps per second:  65, episode reward: 38.540, mean reward:  2.753 [-2.398, 32.360], mean action: 3.286 [0.000, 12.000],  loss: 0.022796, mae: 0.326799, mean_q: 0.564905, mean_eps: 0.000000
 2781/5000: episode: 116, duration: 0.386s, episode steps:  27, steps per second:  70, episode reward: 35.681, mean reward:  1.322 [-2.230, 32.530], mean action: 3.963 [0.000, 12.000],  loss: 0.019137, mae: 0.318701, mean_q: 0.633002, mean_eps: 0.000000
 2802/5000: episode: 117, duration: 0.303s, episode steps:  21, steps per second:  69, episode reward: 38.233, mean reward:  1.821 [-2.271, 32.320], mean action: 4.762 [1.000, 13.000],  loss: 0.019973, mae: 0.323392, mean_q: 0.568819, mean_eps: 0.000000
 2827/5000: episode: 118, duration: 0.357s, episode steps:  25, steps per second:  70, episode reward: -33.000, mean reward: -1.320 [-32.133,  2.497], mean action: 6.440 [0.000, 20.000],  loss: 0.019161, mae: 0.323293, mean_q: 0.481510, mean_eps: 0.000000
 2855/5000: episode: 119, duration: 0.406s, episode steps:  28, steps per second:  69, episode reward: -36.000, mean reward: -1.286 [-29.257,  2.102], mean action: 9.143 [0.000, 19.000],  loss: 0.021075, mae: 0.328857, mean_q: 0.502634, mean_eps: 0.000000
 2877/5000: episode: 120, duration: 0.321s, episode steps:  22, steps per second:  69, episode reward: 35.728, mean reward:  1.624 [-2.533, 32.320], mean action: 6.591 [0.000, 19.000],  loss: 0.023069, mae: 0.328868, mean_q: 0.487662, mean_eps: 0.000000
 2901/5000: episode: 121, duration: 0.353s, episode steps:  24, steps per second:  68, episode reward: 35.514, mean reward:  1.480 [-3.000, 33.000], mean action: 7.917 [0.000, 19.000],  loss: 0.019004, mae: 0.309860, mean_q: 0.507833, mean_eps: 0.000000
 2952/5000: episode: 122, duration: 0.715s, episode steps:  51, steps per second:  71, episode reward: 35.589, mean reward:  0.698 [-2.455, 32.440], mean action: 9.412 [0.000, 19.000],  loss: 0.020813, mae: 0.317763, mean_q: 0.491657, mean_eps: 0.000000
 2972/5000: episode: 123, duration: 0.295s, episode steps:  20, steps per second:  68, episode reward: 32.345, mean reward:  1.617 [-3.000, 32.160], mean action: 5.000 [0.000, 12.000],  loss: 0.022036, mae: 0.326900, mean_q: 0.457237, mean_eps: 0.000000
 2992/5000: episode: 124, duration: 0.295s, episode steps:  20, steps per second:  68, episode reward: 38.355, mean reward:  1.918 [-2.437, 31.874], mean action: 3.650 [1.000, 11.000],  loss: 0.021539, mae: 0.318289, mean_q: 0.539281, mean_eps: 0.000000
 3016/5000: episode: 125, duration: 0.347s, episode steps:  24, steps per second:  69, episode reward: 35.145, mean reward:  1.464 [-2.413, 31.975], mean action: 4.375 [0.000, 19.000],  loss: 0.021427, mae: 0.313788, mean_q: 0.518226, mean_eps: 0.000000
 3037/5000: episode: 126, duration: 0.295s, episode steps:  21, steps per second:  71, episode reward: -32.450, mean reward: -1.545 [-32.186,  2.490], mean action: 4.333 [0.000, 14.000],  loss: 0.021652, mae: 0.316679, mean_q: 0.574290, mean_eps: 0.000000
 3061/5000: episode: 127, duration: 0.353s, episode steps:  24, steps per second:  68, episode reward: 35.183, mean reward:  1.466 [-2.387, 31.943], mean action: 3.667 [0.000, 12.000],  loss: 0.020521, mae: 0.309833, mean_q: 0.587228, mean_eps: 0.000000
 3085/5000: episode: 128, duration: 0.349s, episode steps:  24, steps per second:  69, episode reward: 32.900, mean reward:  1.371 [-3.000, 32.320], mean action: 4.750 [0.000, 12.000],  loss: 0.020649, mae: 0.314053, mean_q: 0.561318, mean_eps: 0.000000
 3113/5000: episode: 129, duration: 0.394s, episode steps:  28, steps per second:  71, episode reward: 35.928, mean reward:  1.283 [-2.891, 32.240], mean action: 5.036 [0.000, 19.000],  loss: 0.021832, mae: 0.318678, mean_q: 0.483148, mean_eps: 0.000000
 3124/5000: episode: 130, duration: 0.185s, episode steps:  11, steps per second:  59, episode reward: 44.515, mean reward:  4.047 [-2.530, 33.000], mean action: 2.182 [0.000, 12.000],  loss: 0.021796, mae: 0.319101, mean_q: 0.475285, mean_eps: 0.000000
 3144/5000: episode: 131, duration: 0.298s, episode steps:  20, steps per second:  67, episode reward: 36.000, mean reward:  1.800 [-2.523, 32.540], mean action: 4.350 [0.000, 15.000],  loss: 0.022681, mae: 0.319799, mean_q: 0.469230, mean_eps: 0.000000
 3178/5000: episode: 132, duration: 0.502s, episode steps:  34, steps per second:  68, episode reward: -33.000, mean reward: -0.971 [-33.000,  2.376], mean action: 4.000 [0.000, 15.000],  loss: 0.021963, mae: 0.320737, mean_q: 0.457255, mean_eps: 0.000000
 3207/5000: episode: 133, duration: 0.417s, episode steps:  29, steps per second:  70, episode reward: 35.051, mean reward:  1.209 [-2.524, 32.080], mean action: 7.690 [0.000, 19.000],  loss: 0.023587, mae: 0.324881, mean_q: 0.535199, mean_eps: 0.000000
 3232/5000: episode: 134, duration: 0.357s, episode steps:  25, steps per second:  70, episode reward: 33.000, mean reward:  1.320 [-2.704, 32.080], mean action: 4.240 [0.000, 15.000],  loss: 0.020849, mae: 0.310349, mean_q: 0.553929, mean_eps: 0.000000
 3276/5000: episode: 135, duration: 0.717s, episode steps:  44, steps per second:  61, episode reward: 40.800, mean reward:  0.927 [-2.273, 32.050], mean action: 2.841 [0.000, 19.000],  loss: 0.021730, mae: 0.314876, mean_q: 0.555141, mean_eps: 0.000000
 3291/5000: episode: 136, duration: 0.229s, episode steps:  15, steps per second:  65, episode reward: 39.000, mean reward:  2.600 [-2.383, 33.000], mean action: 3.400 [0.000, 19.000],  loss: 0.018858, mae: 0.305478, mean_q: 0.620542, mean_eps: 0.000000
 3309/5000: episode: 137, duration: 0.266s, episode steps:  18, steps per second:  68, episode reward: -41.550, mean reward: -2.308 [-32.780,  2.260], mean action: 3.222 [0.000, 11.000],  loss: 0.023749, mae: 0.325061, mean_q: 0.553150, mean_eps: 0.000000
 3331/5000: episode: 138, duration: 0.315s, episode steps:  22, steps per second:  70, episode reward: -38.370, mean reward: -1.744 [-32.450,  2.470], mean action: 5.000 [0.000, 19.000],  loss: 0.016973, mae: 0.301593, mean_q: 0.544584, mean_eps: 0.000000
 3351/5000: episode: 139, duration: 0.291s, episode steps:  20, steps per second:  69, episode reward: -33.000, mean reward: -1.650 [-32.055,  2.322], mean action: 5.200 [0.000, 19.000],  loss: 0.023930, mae: 0.336638, mean_q: 0.584442, mean_eps: 0.000000
 3365/5000: episode: 140, duration: 0.215s, episode steps:  14, steps per second:  65, episode reward: 41.412, mean reward:  2.958 [-3.000, 32.763], mean action: 3.000 [0.000, 11.000],  loss: 0.021131, mae: 0.326794, mean_q: 0.604629, mean_eps: 0.000000
 3383/5000: episode: 141, duration: 0.270s, episode steps:  18, steps per second:  67, episode reward: 38.003, mean reward:  2.111 [-2.350, 31.932], mean action: 3.389 [0.000, 19.000],  loss: 0.017973, mae: 0.318832, mean_q: 0.614076, mean_eps: 0.000000
 3412/5000: episode: 142, duration: 0.410s, episode steps:  29, steps per second:  71, episode reward: 35.097, mean reward:  1.210 [-2.141, 33.000], mean action: 6.241 [0.000, 19.000],  loss: 0.018664, mae: 0.323618, mean_q: 0.596277, mean_eps: 0.000000
 3430/5000: episode: 143, duration: 0.271s, episode steps:  18, steps per second:  66, episode reward: 38.143, mean reward:  2.119 [-2.405, 31.975], mean action: 3.556 [1.000, 16.000],  loss: 0.022071, mae: 0.326197, mean_q: 0.581579, mean_eps: 0.000000
 3458/5000: episode: 144, duration: 0.401s, episode steps:  28, steps per second:  70, episode reward: 32.766, mean reward:  1.170 [-2.781, 32.066], mean action: 4.143 [0.000, 15.000],  loss: 0.020430, mae: 0.320223, mean_q: 0.568153, mean_eps: 0.000000
 3483/5000: episode: 145, duration: 0.361s, episode steps:  25, steps per second:  69, episode reward: -33.000, mean reward: -1.320 [-29.255,  2.250], mean action: 3.280 [0.000, 14.000],  loss: 0.021978, mae: 0.339596, mean_q: 0.557262, mean_eps: 0.000000
 3504/5000: episode: 146, duration: 0.308s, episode steps:  21, steps per second:  68, episode reward: 35.338, mean reward:  1.683 [-2.354, 31.468], mean action: 3.952 [0.000, 16.000],  loss: 0.022425, mae: 0.332165, mean_q: 0.550071, mean_eps: 0.000000
 3524/5000: episode: 147, duration: 0.284s, episode steps:  20, steps per second:  70, episode reward: -36.000, mean reward: -1.800 [-32.083,  2.380], mean action: 5.500 [0.000, 14.000],  loss: 0.018769, mae: 0.316793, mean_q: 0.557018, mean_eps: 0.000000
 3544/5000: episode: 148, duration: 0.300s, episode steps:  20, steps per second:  67, episode reward: 35.799, mean reward:  1.790 [-3.000, 32.020], mean action: 4.700 [0.000, 19.000],  loss: 0.020546, mae: 0.316081, mean_q: 0.548127, mean_eps: 0.000000
 3567/5000: episode: 149, duration: 0.334s, episode steps:  23, steps per second:  69, episode reward: 38.711, mean reward:  1.683 [-2.699, 32.904], mean action: 3.000 [0.000, 12.000],  loss: 0.024277, mae: 0.335855, mean_q: 0.527314, mean_eps: 0.000000
 3584/5000: episode: 150, duration: 0.250s, episode steps:  17, steps per second:  68, episode reward: 38.596, mean reward:  2.270 [-2.275, 32.636], mean action: 3.118 [0.000, 12.000],  loss: 0.020317, mae: 0.321981, mean_q: 0.519208, mean_eps: 0.000000
 3615/5000: episode: 151, duration: 0.436s, episode steps:  31, steps per second:  71, episode reward: 35.466, mean reward:  1.144 [-2.589, 32.273], mean action: 4.613 [0.000, 16.000],  loss: 0.019889, mae: 0.319121, mean_q: 0.566249, mean_eps: 0.000000
 3651/5000: episode: 152, duration: 0.501s, episode steps:  36, steps per second:  72, episode reward: 32.943, mean reward:  0.915 [-2.415, 32.043], mean action: 7.806 [0.000, 21.000],  loss: 0.020401, mae: 0.320363, mean_q: 0.504092, mean_eps: 0.000000
 3681/5000: episode: 153, duration: 0.427s, episode steps:  30, steps per second:  70, episode reward: 32.759, mean reward:  1.092 [-2.527, 33.000], mean action: 4.800 [0.000, 16.000],  loss: 0.018822, mae: 0.311883, mean_q: 0.519089, mean_eps: 0.000000
 3699/5000: episode: 154, duration: 0.260s, episode steps:  18, steps per second:  69, episode reward: 35.515, mean reward:  1.973 [-3.000, 32.903], mean action: 4.333 [0.000, 16.000],  loss: 0.021038, mae: 0.320508, mean_q: 0.515041, mean_eps: 0.000000
 3717/5000: episode: 155, duration: 0.275s, episode steps:  18, steps per second:  65, episode reward: 40.829, mean reward:  2.268 [-2.394, 31.924], mean action: 4.889 [1.000, 16.000],  loss: 0.020237, mae: 0.316726, mean_q: 0.539474, mean_eps: 0.000000
 3738/5000: episode: 156, duration: 0.305s, episode steps:  21, steps per second:  69, episode reward: 35.094, mean reward:  1.671 [-3.000, 32.263], mean action: 4.571 [0.000, 16.000],  loss: 0.022700, mae: 0.325507, mean_q: 0.544680, mean_eps: 0.000000
 3756/5000: episode: 157, duration: 0.312s, episode steps:  18, steps per second:  58, episode reward: 38.437, mean reward:  2.135 [-2.792, 31.447], mean action: 4.222 [0.000, 12.000],  loss: 0.019980, mae: 0.317173, mean_q: 0.551481, mean_eps: 0.000000
 3782/5000: episode: 158, duration: 0.373s, episode steps:  26, steps per second:  70, episode reward: 32.078, mean reward:  1.234 [-3.000, 31.799], mean action: 3.115 [0.000, 12.000],  loss: 0.020828, mae: 0.324908, mean_q: 0.535050, mean_eps: 0.000000
 3807/5000: episode: 159, duration: 0.368s, episode steps:  25, steps per second:  68, episode reward: 35.617, mean reward:  1.425 [-2.392, 32.530], mean action: 4.800 [0.000, 19.000],  loss: 0.021938, mae: 0.328433, mean_q: 0.565470, mean_eps: 0.000000
 3831/5000: episode: 160, duration: 0.338s, episode steps:  24, steps per second:  71, episode reward: -36.000, mean reward: -1.500 [-33.000,  2.910], mean action: 5.208 [0.000, 19.000],  loss: 0.022473, mae: 0.333577, mean_q: 0.575262, mean_eps: 0.000000
 3863/5000: episode: 161, duration: 0.448s, episode steps:  32, steps per second:  71, episode reward: -41.230, mean reward: -1.288 [-32.185,  3.000], mean action: 6.469 [1.000, 12.000],  loss: 0.022904, mae: 0.335053, mean_q: 0.529767, mean_eps: 0.000000
 3883/5000: episode: 162, duration: 0.295s, episode steps:  20, steps per second:  68, episode reward: 35.377, mean reward:  1.769 [-3.000, 32.013], mean action: 3.000 [0.000, 9.000],  loss: 0.022035, mae: 0.339918, mean_q: 0.565286, mean_eps: 0.000000
 3905/5000: episode: 163, duration: 0.314s, episode steps:  22, steps per second:  70, episode reward: 38.451, mean reward:  1.748 [-2.491, 31.601], mean action: 3.818 [1.000, 11.000],  loss: 0.022807, mae: 0.335304, mean_q: 0.569883, mean_eps: 0.000000
 3923/5000: episode: 164, duration: 0.263s, episode steps:  18, steps per second:  69, episode reward: -38.330, mean reward: -2.129 [-31.716,  2.239], mean action: 5.944 [1.000, 19.000],  loss: 0.019912, mae: 0.313431, mean_q: 0.533855, mean_eps: 0.000000
 3950/5000: episode: 165, duration: 0.389s, episode steps:  27, steps per second:  69, episode reward: -33.000, mean reward: -1.222 [-32.135,  2.360], mean action: 8.926 [0.000, 20.000],  loss: 0.019827, mae: 0.309434, mean_q: 0.514460, mean_eps: 0.000000
 3969/5000: episode: 166, duration: 0.273s, episode steps:  19, steps per second:  69, episode reward: 32.829, mean reward:  1.728 [-3.000, 32.829], mean action: 6.053 [1.000, 19.000],  loss: 0.025068, mae: 0.336617, mean_q: 0.514191, mean_eps: 0.000000
 3997/5000: episode: 167, duration: 0.415s, episode steps:  28, steps per second:  68, episode reward: 34.908, mean reward:  1.247 [-2.227, 31.938], mean action: 4.643 [0.000, 19.000],  loss: 0.022097, mae: 0.324673, mean_q: 0.578076, mean_eps: 0.000000
 4016/5000: episode: 168, duration: 0.286s, episode steps:  19, steps per second:  66, episode reward: 41.481, mean reward:  2.183 [-2.361, 32.552], mean action: 2.579 [0.000, 16.000],  loss: 0.018742, mae: 0.311341, mean_q: 0.573562, mean_eps: 0.000000
 4039/5000: episode: 169, duration: 0.335s, episode steps:  23, steps per second:  69, episode reward: 38.489, mean reward:  1.673 [-2.949, 32.714], mean action: 8.304 [0.000, 20.000],  loss: 0.023429, mae: 0.335370, mean_q: 0.518579, mean_eps: 0.000000
 4071/5000: episode: 170, duration: 0.447s, episode steps:  32, steps per second:  72, episode reward: 32.134, mean reward:  1.004 [-2.558, 31.884], mean action: 7.281 [0.000, 20.000],  loss: 0.019006, mae: 0.314303, mean_q: 0.498190, mean_eps: 0.000000
 4096/5000: episode: 171, duration: 0.362s, episode steps:  25, steps per second:  69, episode reward: 32.223, mean reward:  1.289 [-2.174, 32.123], mean action: 5.560 [0.000, 16.000],  loss: 0.019135, mae: 0.309609, mean_q: 0.554975, mean_eps: 0.000000
 4119/5000: episode: 172, duration: 0.377s, episode steps:  23, steps per second:  61, episode reward: 35.234, mean reward:  1.532 [-2.336, 31.704], mean action: 3.739 [0.000, 11.000],  loss: 0.022414, mae: 0.331718, mean_q: 0.601805, mean_eps: 0.000000
 4144/5000: episode: 173, duration: 0.363s, episode steps:  25, steps per second:  69, episode reward: 32.194, mean reward:  1.288 [-3.000, 32.194], mean action: 4.240 [1.000, 16.000],  loss: 0.020577, mae: 0.327164, mean_q: 0.651161, mean_eps: 0.000000
 4172/5000: episode: 174, duration: 0.398s, episode steps:  28, steps per second:  70, episode reward: -36.000, mean reward: -1.286 [-30.480,  2.765], mean action: 2.821 [0.000, 11.000],  loss: 0.020738, mae: 0.329813, mean_q: 0.598233, mean_eps: 0.000000
 4189/5000: episode: 175, duration: 0.257s, episode steps:  17, steps per second:  66, episode reward: 42.000, mean reward:  2.471 [-2.556, 33.000], mean action: 1.941 [0.000, 16.000],  loss: 0.025094, mae: 0.348753, mean_q: 0.600708, mean_eps: 0.000000
 4209/5000: episode: 176, duration: 0.300s, episode steps:  20, steps per second:  67, episode reward: 32.643, mean reward:  1.632 [-3.000, 31.723], mean action: 4.700 [0.000, 15.000],  loss: 0.022698, mae: 0.337232, mean_q: 0.568045, mean_eps: 0.000000
 4222/5000: episode: 177, duration: 0.246s, episode steps:  13, steps per second:  53, episode reward: 43.643, mean reward:  3.357 [-2.256, 33.000], mean action: 3.231 [0.000, 13.000],  loss: 0.020451, mae: 0.327762, mean_q: 0.583352, mean_eps: 0.000000
 4239/5000: episode: 178, duration: 0.250s, episode steps:  17, steps per second:  68, episode reward: 42.000, mean reward:  2.471 [-2.450, 32.030], mean action: 1.588 [0.000, 11.000],  loss: 0.021449, mae: 0.323583, mean_q: 0.566486, mean_eps: 0.000000
 4264/5000: episode: 179, duration: 0.372s, episode steps:  25, steps per second:  67, episode reward: 38.891, mean reward:  1.556 [-2.570, 32.121], mean action: 3.360 [0.000, 11.000],  loss: 0.022770, mae: 0.337754, mean_q: 0.618660, mean_eps: 0.000000
 4290/5000: episode: 180, duration: 0.368s, episode steps:  26, steps per second:  71, episode reward: -35.190, mean reward: -1.353 [-32.032,  2.535], mean action: 5.423 [0.000, 16.000],  loss: 0.020842, mae: 0.332260, mean_q: 0.603172, mean_eps: 0.000000
 4316/5000: episode: 181, duration: 0.385s, episode steps:  26, steps per second:  67, episode reward: 38.609, mean reward:  1.485 [-2.297, 32.250], mean action: 6.000 [0.000, 18.000],  loss: 0.024407, mae: 0.348546, mean_q: 0.551676, mean_eps: 0.000000
 4342/5000: episode: 182, duration: 0.383s, episode steps:  26, steps per second:  68, episode reward: 35.140, mean reward:  1.352 [-3.000, 31.512], mean action: 6.115 [1.000, 19.000],  loss: 0.019534, mae: 0.321206, mean_q: 0.527931, mean_eps: 0.000000
 4369/5000: episode: 183, duration: 0.390s, episode steps:  27, steps per second:  69, episode reward: -38.380, mean reward: -1.421 [-32.317,  3.037], mean action: 8.963 [0.000, 20.000],  loss: 0.022283, mae: 0.334667, mean_q: 0.498687, mean_eps: 0.000000
 4388/5000: episode: 184, duration: 0.356s, episode steps:  19, steps per second:  53, episode reward: 32.751, mean reward:  1.724 [-2.877, 32.190], mean action: 3.158 [0.000, 11.000],  loss: 0.021422, mae: 0.328430, mean_q: 0.573031, mean_eps: 0.000000
 4399/5000: episode: 185, duration: 0.346s, episode steps:  11, steps per second:  32, episode reward: 44.900, mean reward:  4.082 [-2.115, 32.130], mean action: 1.818 [0.000, 12.000],  loss: 0.022809, mae: 0.342586, mean_q: 0.545210, mean_eps: 0.000000
 4418/5000: episode: 186, duration: 0.282s, episode steps:  19, steps per second:  67, episode reward: 39.000, mean reward:  2.053 [-2.630, 32.440], mean action: 2.947 [1.000, 11.000],  loss: 0.018633, mae: 0.318955, mean_q: 0.505590, mean_eps: 0.000000
 4435/5000: episode: 187, duration: 0.262s, episode steps:  17, steps per second:  65, episode reward: 44.056, mean reward:  2.592 [-2.161, 32.407], mean action: 3.882 [0.000, 13.000],  loss: 0.019116, mae: 0.321434, mean_q: 0.512693, mean_eps: 0.000000
 4453/5000: episode: 188, duration: 0.321s, episode steps:  18, steps per second:  56, episode reward: 41.265, mean reward:  2.292 [-2.370, 32.133], mean action: 2.556 [0.000, 14.000],  loss: 0.019815, mae: 0.318717, mean_q: 0.511932, mean_eps: 0.000000
 4469/5000: episode: 189, duration: 0.244s, episode steps:  16, steps per second:  66, episode reward: 39.000, mean reward:  2.438 [-2.566, 33.000], mean action: 4.188 [0.000, 19.000],  loss: 0.029596, mae: 0.372570, mean_q: 0.510873, mean_eps: 0.000000
 4492/5000: episode: 190, duration: 0.342s, episode steps:  23, steps per second:  67, episode reward: 41.274, mean reward:  1.795 [-2.408, 32.190], mean action: 3.391 [0.000, 13.000],  loss: 0.022730, mae: 0.337935, mean_q: 0.563948, mean_eps: 0.000000
 4514/5000: episode: 191, duration: 0.327s, episode steps:  22, steps per second:  67, episode reward: 35.618, mean reward:  1.619 [-2.332, 32.618], mean action: 4.409 [0.000, 16.000],  loss: 0.018610, mae: 0.317455, mean_q: 0.493676, mean_eps: 0.000000
 4536/5000: episode: 192, duration: 0.331s, episode steps:  22, steps per second:  66, episode reward: 37.822, mean reward:  1.719 [-3.000, 32.220], mean action: 6.773 [0.000, 20.000],  loss: 0.023446, mae: 0.345444, mean_q: 0.530013, mean_eps: 0.000000
 4555/5000: episode: 193, duration: 0.286s, episode steps:  19, steps per second:  66, episode reward: 41.259, mean reward:  2.172 [-2.332, 32.169], mean action: 5.842 [0.000, 16.000],  loss: 0.021877, mae: 0.329081, mean_q: 0.562033, mean_eps: 0.000000
 4580/5000: episode: 194, duration: 0.363s, episode steps:  25, steps per second:  69, episode reward: -32.170, mean reward: -1.287 [-33.000,  2.724], mean action: 4.360 [0.000, 16.000],  loss: 0.019195, mae: 0.321662, mean_q: 0.511370, mean_eps: 0.000000
 4609/5000: episode: 195, duration: 0.422s, episode steps:  29, steps per second:  69, episode reward: 36.000, mean reward:  1.241 [-2.558, 30.221], mean action: 4.517 [0.000, 16.000],  loss: 0.019078, mae: 0.314338, mean_q: 0.553076, mean_eps: 0.000000
 4632/5000: episode: 196, duration: 0.332s, episode steps:  23, steps per second:  69, episode reward: 39.000, mean reward:  1.696 [-2.709, 32.230], mean action: 4.696 [0.000, 16.000],  loss: 0.021385, mae: 0.324216, mean_q: 0.526416, mean_eps: 0.000000
 4667/5000: episode: 197, duration: 0.497s, episode steps:  35, steps per second:  70, episode reward: 34.941, mean reward:  0.998 [-2.330, 32.260], mean action: 5.400 [0.000, 19.000],  loss: 0.026756, mae: 0.346502, mean_q: 0.605938, mean_eps: 0.000000
 4690/5000: episode: 198, duration: 0.329s, episode steps:  23, steps per second:  70, episode reward: 34.657, mean reward:  1.507 [-2.463, 32.208], mean action: 7.043 [0.000, 19.000],  loss: 0.019726, mae: 0.323234, mean_q: 0.594773, mean_eps: 0.000000
 4710/5000: episode: 199, duration: 0.310s, episode steps:  20, steps per second:  65, episode reward: 32.825, mean reward:  1.641 [-3.000, 32.390], mean action: 3.850 [0.000, 15.000],  loss: 0.021893, mae: 0.342772, mean_q: 0.554563, mean_eps: 0.000000
 4735/5000: episode: 200, duration: 0.364s, episode steps:  25, steps per second:  69, episode reward: 38.016, mean reward:  1.521 [-2.053, 32.102], mean action: 3.800 [1.000, 15.000],  loss: 0.024784, mae: 0.351834, mean_q: 0.526650, mean_eps: 0.000000
 4775/5000: episode: 201, duration: 0.592s, episode steps:  40, steps per second:  68, episode reward: -35.070, mean reward: -0.877 [-31.919,  2.854], mean action: 5.850 [0.000, 21.000],  loss: 0.022556, mae: 0.343444, mean_q: 0.524120, mean_eps: 0.000000
 4796/5000: episode: 202, duration: 0.292s, episode steps:  21, steps per second:  72, episode reward: -35.090, mean reward: -1.671 [-32.510,  3.000], mean action: 6.286 [0.000, 21.000],  loss: 0.022200, mae: 0.337003, mean_q: 0.566578, mean_eps: 0.000000
 4833/5000: episode: 203, duration: 0.525s, episode steps:  37, steps per second:  70, episode reward: -32.520, mean reward: -0.879 [-32.073,  2.519], mean action: 8.108 [0.000, 20.000],  loss: 0.017679, mae: 0.307919, mean_q: 0.514505, mean_eps: 0.000000
 4860/5000: episode: 204, duration: 0.401s, episode steps:  27, steps per second:  67, episode reward: -33.000, mean reward: -1.222 [-33.000,  3.000], mean action: 7.593 [0.000, 21.000],  loss: 0.020011, mae: 0.313596, mean_q: 0.504898, mean_eps: 0.000000
 4889/5000: episode: 205, duration: 0.477s, episode steps:  29, steps per second:  61, episode reward: 32.819, mean reward:  1.132 [-2.406, 32.100], mean action: 2.897 [0.000, 15.000],  loss: 0.020931, mae: 0.318616, mean_q: 0.518186, mean_eps: 0.000000
 4912/5000: episode: 206, duration: 0.392s, episode steps:  23, steps per second:  59, episode reward: 35.646, mean reward:  1.550 [-3.000, 31.904], mean action: 4.391 [0.000, 15.000],  loss: 0.021809, mae: 0.331035, mean_q: 0.541464, mean_eps: 0.000000
 4935/5000: episode: 207, duration: 0.353s, episode steps:  23, steps per second:  65, episode reward: 32.473, mean reward:  1.412 [-2.925, 32.510], mean action: 4.174 [0.000, 14.000],  loss: 0.021659, mae: 0.329109, mean_q: 0.541018, mean_eps: 0.000000
 4948/5000: episode: 208, duration: 0.201s, episode steps:  13, steps per second:  65, episode reward: 42.000, mean reward:  3.231 [-2.434, 33.000], mean action: 3.000 [0.000, 16.000],  loss: 0.013660, mae: 0.294380, mean_q: 0.541877, mean_eps: 0.000000
 4970/5000: episode: 209, duration: 0.336s, episode steps:  22, steps per second:  66, episode reward: 41.620, mean reward:  1.892 [-2.320, 32.720], mean action: 4.500 [1.000, 16.000],  loss: 0.018438, mae: 0.325772, mean_q: 0.515847, mean_eps: 0.000000
 4997/5000: episode: 210, duration: 0.424s, episode steps:  27, steps per second:  64, episode reward: 32.329, mean reward:  1.197 [-2.729, 31.747], mean action: 5.333 [0.000, 16.000],  loss: 0.022185, mae: 0.340904, mean_q: 0.472233, mean_eps: 0.000000
done, took 66.774 seconds
DQN Evaluation: 10932 victories out of 12763 episodes
Training for 5000 steps ...
   38/5000: episode: 1, duration: 0.286s, episode steps:  38, steps per second: 133, episode reward: 37.918, mean reward:  0.998 [-3.000, 32.480], mean action: 3.237 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   77/5000: episode: 2, duration: 0.263s, episode steps:  39, steps per second: 148, episode reward: 41.237, mean reward:  1.057 [-3.000, 32.160], mean action: 3.026 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  149/5000: episode: 3, duration: 0.437s, episode steps:  72, steps per second: 165, episode reward: -35.090, mean reward: -0.487 [-32.129,  2.330], mean action: 4.792 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  170/5000: episode: 4, duration: 0.143s, episode steps:  21, steps per second: 147, episode reward: 44.010, mean reward:  2.096 [-2.029, 32.050], mean action: 3.429 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  201/5000: episode: 5, duration: 0.211s, episode steps:  31, steps per second: 147, episode reward: 41.517, mean reward:  1.339 [-2.714, 32.343], mean action: 3.032 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  218/5000: episode: 6, duration: 0.143s, episode steps:  17, steps per second: 119, episode reward: 47.647, mean reward:  2.803 [-0.243, 32.630], mean action: 4.000 [1.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  246/5000: episode: 7, duration: 0.189s, episode steps:  28, steps per second: 148, episode reward: 44.338, mean reward:  1.584 [-2.301, 32.230], mean action: 3.286 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  260/5000: episode: 8, duration: 0.108s, episode steps:  14, steps per second: 130, episode reward: 47.028, mean reward:  3.359 [-0.381, 32.224], mean action: 1.429 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  282/5000: episode: 9, duration: 0.158s, episode steps:  22, steps per second: 140, episode reward: 41.913, mean reward:  1.905 [-2.403, 32.093], mean action: 2.591 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  316/5000: episode: 10, duration: 0.217s, episode steps:  34, steps per second: 156, episode reward: 37.869, mean reward:  1.114 [-2.412, 32.040], mean action: 4.588 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  337/5000: episode: 11, duration: 0.147s, episode steps:  21, steps per second: 143, episode reward: 43.692, mean reward:  2.081 [-2.766, 32.090], mean action: 2.048 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  366/5000: episode: 12, duration: 0.195s, episode steps:  29, steps per second: 149, episode reward: 45.000, mean reward:  1.552 [-2.134, 32.110], mean action: 2.310 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  393/5000: episode: 13, duration: 0.183s, episode steps:  27, steps per second: 147, episode reward: 38.697, mean reward:  1.433 [-3.000, 32.610], mean action: 3.741 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  448/5000: episode: 14, duration: 0.346s, episode steps:  55, steps per second: 159, episode reward: 37.985, mean reward:  0.691 [-2.343, 32.290], mean action: 2.655 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  472/5000: episode: 15, duration: 0.167s, episode steps:  24, steps per second: 144, episode reward: 42.000, mean reward:  1.750 [-2.863, 32.190], mean action: 2.833 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  498/5000: episode: 16, duration: 0.175s, episode steps:  26, steps per second: 149, episode reward: 38.020, mean reward:  1.462 [-2.903, 31.754], mean action: 3.269 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  529/5000: episode: 17, duration: 0.201s, episode steps:  31, steps per second: 155, episode reward: 39.000, mean reward:  1.258 [-2.805, 32.210], mean action: 5.871 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  559/5000: episode: 18, duration: 0.191s, episode steps:  30, steps per second: 157, episode reward: 41.130, mean reward:  1.371 [-2.519, 31.981], mean action: 3.333 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  584/5000: episode: 19, duration: 0.169s, episode steps:  25, steps per second: 148, episode reward: 41.365, mean reward:  1.655 [-2.487, 32.222], mean action: 2.680 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  615/5000: episode: 20, duration: 0.205s, episode steps:  31, steps per second: 151, episode reward: 39.000, mean reward:  1.258 [-2.483, 32.310], mean action: 3.806 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  647/5000: episode: 21, duration: 0.213s, episode steps:  32, steps per second: 151, episode reward: 44.872, mean reward:  1.402 [-2.116, 32.070], mean action: 0.688 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  665/5000: episode: 22, duration: 0.133s, episode steps:  18, steps per second: 136, episode reward: 43.932, mean reward:  2.441 [-2.393, 32.390], mean action: 3.833 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  699/5000: episode: 23, duration: 0.232s, episode steps:  34, steps per second: 146, episode reward: 37.305, mean reward:  1.097 [-2.853, 32.018], mean action: 3.824 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  735/5000: episode: 24, duration: 0.235s, episode steps:  36, steps per second: 153, episode reward: 38.653, mean reward:  1.074 [-2.438, 32.434], mean action: 3.583 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  759/5000: episode: 25, duration: 0.164s, episode steps:  24, steps per second: 147, episode reward: 38.834, mean reward:  1.618 [-2.282, 32.520], mean action: 3.042 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  794/5000: episode: 26, duration: 0.226s, episode steps:  35, steps per second: 155, episode reward: 41.904, mean reward:  1.197 [-2.903, 32.024], mean action: 2.257 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  824/5000: episode: 27, duration: 0.198s, episode steps:  30, steps per second: 151, episode reward: 41.769, mean reward:  1.392 [-2.261, 31.961], mean action: 3.233 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  863/5000: episode: 28, duration: 0.246s, episode steps:  39, steps per second: 159, episode reward: 44.278, mean reward:  1.135 [-2.195, 32.264], mean action: 2.282 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  910/5000: episode: 29, duration: 0.295s, episode steps:  47, steps per second: 159, episode reward: 44.320, mean reward:  0.943 [-2.004, 32.230], mean action: 4.447 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  944/5000: episode: 30, duration: 0.247s, episode steps:  34, steps per second: 138, episode reward: 32.053, mean reward:  0.943 [-2.671, 31.420], mean action: 4.647 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  965/5000: episode: 31, duration: 0.143s, episode steps:  21, steps per second: 147, episode reward: 41.662, mean reward:  1.984 [-2.108, 32.120], mean action: 3.476 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  985/5000: episode: 32, duration: 0.150s, episode steps:  20, steps per second: 134, episode reward: 42.000, mean reward:  2.100 [-2.583, 32.460], mean action: 1.900 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1016/5000: episode: 33, duration: 0.327s, episode steps:  31, steps per second:  95, episode reward: 41.782, mean reward:  1.348 [-2.456, 32.082], mean action: 1.774 [0.000, 15.000],  loss: 0.032345, mae: 0.378605, mean_q: 0.514598, mean_eps: 0.000000
 1046/5000: episode: 34, duration: 0.429s, episode steps:  30, steps per second:  70, episode reward: 35.063, mean reward:  1.169 [-3.000, 31.710], mean action: 3.133 [0.000, 15.000],  loss: 0.021992, mae: 0.327800, mean_q: 0.584339, mean_eps: 0.000000
 1075/5000: episode: 35, duration: 0.420s, episode steps:  29, steps per second:  69, episode reward: 30.875, mean reward:  1.065 [-2.782, 32.400], mean action: 3.345 [0.000, 11.000],  loss: 0.019438, mae: 0.317457, mean_q: 0.536766, mean_eps: 0.000000
 1099/5000: episode: 36, duration: 0.355s, episode steps:  24, steps per second:  68, episode reward: 44.309, mean reward:  1.846 [-3.000, 31.961], mean action: 3.417 [0.000, 14.000],  loss: 0.023418, mae: 0.330654, mean_q: 0.551831, mean_eps: 0.000000
 1125/5000: episode: 37, duration: 0.388s, episode steps:  26, steps per second:  67, episode reward: 41.333, mean reward:  1.590 [-2.889, 32.400], mean action: 3.615 [0.000, 14.000],  loss: 0.020720, mae: 0.323234, mean_q: 0.508305, mean_eps: 0.000000
 1154/5000: episode: 38, duration: 0.414s, episode steps:  29, steps per second:  70, episode reward: 41.235, mean reward:  1.422 [-2.481, 32.040], mean action: 4.241 [0.000, 14.000],  loss: 0.019600, mae: 0.323792, mean_q: 0.528855, mean_eps: 0.000000
 1181/5000: episode: 39, duration: 0.384s, episode steps:  27, steps per second:  70, episode reward: 43.398, mean reward:  1.607 [-2.343, 32.220], mean action: 4.519 [1.000, 14.000],  loss: 0.022088, mae: 0.337221, mean_q: 0.522151, mean_eps: 0.000000
 1198/5000: episode: 40, duration: 0.254s, episode steps:  17, steps per second:  67, episode reward: 44.945, mean reward:  2.644 [-2.544, 32.550], mean action: 3.941 [0.000, 14.000],  loss: 0.016849, mae: 0.313621, mean_q: 0.518785, mean_eps: 0.000000
 1218/5000: episode: 41, duration: 0.292s, episode steps:  20, steps per second:  69, episode reward: 41.062, mean reward:  2.053 [-2.609, 32.567], mean action: 4.150 [0.000, 14.000],  loss: 0.018465, mae: 0.320727, mean_q: 0.563851, mean_eps: 0.000000
 1242/5000: episode: 42, duration: 0.346s, episode steps:  24, steps per second:  69, episode reward: 45.842, mean reward:  1.910 [-0.320, 32.150], mean action: 4.417 [1.000, 13.000],  loss: 0.019794, mae: 0.322219, mean_q: 0.552788, mean_eps: 0.000000
 1262/5000: episode: 43, duration: 0.302s, episode steps:  20, steps per second:  66, episode reward: 41.181, mean reward:  2.059 [-3.000, 31.978], mean action: 3.400 [0.000, 12.000],  loss: 0.020022, mae: 0.328071, mean_q: 0.564000, mean_eps: 0.000000
 1281/5000: episode: 44, duration: 0.495s, episode steps:  19, steps per second:  38, episode reward: 40.476, mean reward:  2.130 [-3.000, 32.850], mean action: 4.579 [0.000, 13.000],  loss: 0.024761, mae: 0.348828, mean_q: 0.535849, mean_eps: 0.000000
 1307/5000: episode: 45, duration: 0.415s, episode steps:  26, steps per second:  63, episode reward: 43.187, mean reward:  1.661 [-3.000, 33.000], mean action: 4.846 [0.000, 14.000],  loss: 0.021406, mae: 0.335312, mean_q: 0.529671, mean_eps: 0.000000
 1338/5000: episode: 46, duration: 0.455s, episode steps:  31, steps per second:  68, episode reward: 41.388, mean reward:  1.335 [-2.715, 32.130], mean action: 4.258 [0.000, 16.000],  loss: 0.024218, mae: 0.352512, mean_q: 0.530656, mean_eps: 0.000000
 1375/5000: episode: 47, duration: 0.573s, episode steps:  37, steps per second:  65, episode reward: -32.100, mean reward: -0.868 [-32.675,  2.380], mean action: 4.757 [0.000, 16.000],  loss: 0.022421, mae: 0.342830, mean_q: 0.606583, mean_eps: 0.000000
 1425/5000: episode: 48, duration: 5.564s, episode steps:  50, steps per second:   9, episode reward: 43.973, mean reward:  0.879 [-2.291, 31.549], mean action: 2.520 [0.000, 19.000],  loss: 0.020823, mae: 0.335086, mean_q: 0.552600, mean_eps: 0.000000
 1448/5000: episode: 49, duration: 0.391s, episode steps:  23, steps per second:  59, episode reward: 41.390, mean reward:  1.800 [-2.334, 31.680], mean action: 1.304 [0.000, 3.000],  loss: 0.018915, mae: 0.322685, mean_q: 0.542703, mean_eps: 0.000000
 1472/5000: episode: 50, duration: 0.411s, episode steps:  24, steps per second:  58, episode reward: 35.354, mean reward:  1.473 [-3.000, 29.723], mean action: 4.250 [0.000, 16.000],  loss: 0.020198, mae: 0.332009, mean_q: 0.587205, mean_eps: 0.000000
 1491/5000: episode: 51, duration: 0.292s, episode steps:  19, steps per second:  65, episode reward: 41.496, mean reward:  2.184 [-3.000, 32.080], mean action: 4.000 [1.000, 12.000],  loss: 0.020724, mae: 0.335706, mean_q: 0.556953, mean_eps: 0.000000
 1501/5000: episode: 52, duration: 0.174s, episode steps:  10, steps per second:  57, episode reward: 47.634, mean reward:  4.763 [ 0.271, 32.270], mean action: 2.400 [0.000, 14.000],  loss: 0.024876, mae: 0.348698, mean_q: 0.500262, mean_eps: 0.000000
 1526/5000: episode: 53, duration: 0.361s, episode steps:  25, steps per second:  69, episode reward: 38.216, mean reward:  1.529 [-2.632, 32.113], mean action: 5.280 [0.000, 14.000],  loss: 0.020359, mae: 0.325018, mean_q: 0.531874, mean_eps: 0.000000
 1545/5000: episode: 54, duration: 0.293s, episode steps:  19, steps per second:  65, episode reward: 38.616, mean reward:  2.032 [-2.428, 31.916], mean action: 3.053 [0.000, 12.000],  loss: 0.021647, mae: 0.330906, mean_q: 0.522058, mean_eps: 0.000000
 1574/5000: episode: 55, duration: 0.442s, episode steps:  29, steps per second:  66, episode reward: 44.976, mean reward:  1.551 [-2.136, 32.190], mean action: 1.517 [0.000, 11.000],  loss: 0.019111, mae: 0.321289, mean_q: 0.548703, mean_eps: 0.000000
 1599/5000: episode: 56, duration: 0.381s, episode steps:  25, steps per second:  66, episode reward: 45.000, mean reward:  1.800 [-2.300, 32.300], mean action: 4.400 [0.000, 12.000],  loss: 0.023337, mae: 0.341989, mean_q: 0.561058, mean_eps: 0.000000
 1624/5000: episode: 57, duration: 0.370s, episode steps:  25, steps per second:  68, episode reward: 41.875, mean reward:  1.675 [-2.302, 32.250], mean action: 3.680 [0.000, 16.000],  loss: 0.016525, mae: 0.317801, mean_q: 0.546639, mean_eps: 0.000000
 1640/5000: episode: 58, duration: 0.257s, episode steps:  16, steps per second:  62, episode reward: 44.331, mean reward:  2.771 [-2.021, 31.840], mean action: 3.312 [0.000, 16.000],  loss: 0.023141, mae: 0.342938, mean_q: 0.499090, mean_eps: 0.000000
 1662/5000: episode: 59, duration: 0.325s, episode steps:  22, steps per second:  68, episode reward: 41.327, mean reward:  1.878 [-2.408, 32.140], mean action: 3.773 [0.000, 16.000],  loss: 0.021370, mae: 0.337289, mean_q: 0.513873, mean_eps: 0.000000
 1695/5000: episode: 60, duration: 0.489s, episode steps:  33, steps per second:  67, episode reward: 44.400, mean reward:  1.345 [-2.538, 33.000], mean action: 5.121 [0.000, 20.000],  loss: 0.016526, mae: 0.306614, mean_q: 0.514046, mean_eps: 0.000000
 1718/5000: episode: 61, duration: 0.361s, episode steps:  23, steps per second:  64, episode reward: 41.490, mean reward:  1.804 [-2.061, 32.090], mean action: 2.522 [0.000, 12.000],  loss: 0.021906, mae: 0.330067, mean_q: 0.490255, mean_eps: 0.000000
 1746/5000: episode: 62, duration: 0.405s, episode steps:  28, steps per second:  69, episode reward: 38.427, mean reward:  1.372 [-2.339, 32.040], mean action: 3.143 [1.000, 16.000],  loss: 0.018734, mae: 0.312833, mean_q: 0.519507, mean_eps: 0.000000
 1770/5000: episode: 63, duration: 0.362s, episode steps:  24, steps per second:  66, episode reward: 44.638, mean reward:  1.860 [-2.427, 32.270], mean action: 2.542 [0.000, 14.000],  loss: 0.021021, mae: 0.324423, mean_q: 0.540622, mean_eps: 0.000000
 1791/5000: episode: 64, duration: 0.324s, episode steps:  21, steps per second:  65, episode reward: 38.429, mean reward:  1.830 [-2.609, 32.218], mean action: 4.571 [0.000, 19.000],  loss: 0.021158, mae: 0.323269, mean_q: 0.528163, mean_eps: 0.000000
 1820/5000: episode: 65, duration: 0.413s, episode steps:  29, steps per second:  70, episode reward: 41.172, mean reward:  1.420 [-3.000, 32.200], mean action: 3.483 [0.000, 19.000],  loss: 0.020086, mae: 0.322808, mean_q: 0.546590, mean_eps: 0.000000
 1842/5000: episode: 66, duration: 0.324s, episode steps:  22, steps per second:  68, episode reward: 43.833, mean reward:  1.992 [-2.596, 32.523], mean action: 4.455 [0.000, 19.000],  loss: 0.020101, mae: 0.314807, mean_q: 0.472167, mean_eps: 0.000000
 1870/5000: episode: 67, duration: 0.412s, episode steps:  28, steps per second:  68, episode reward: 38.534, mean reward:  1.376 [-3.000, 32.244], mean action: 4.429 [0.000, 20.000],  loss: 0.022859, mae: 0.332552, mean_q: 0.470315, mean_eps: 0.000000
 1916/5000: episode: 68, duration: 0.658s, episode steps:  46, steps per second:  70, episode reward: 40.650, mean reward:  0.884 [-2.097, 32.091], mean action: 3.174 [0.000, 19.000],  loss: 0.020745, mae: 0.317287, mean_q: 0.494477, mean_eps: 0.000000
 1952/5000: episode: 69, duration: 0.514s, episode steps:  36, steps per second:  70, episode reward: 46.823, mean reward:  1.301 [-0.753, 33.000], mean action: 2.806 [0.000, 19.000],  loss: 0.021857, mae: 0.320841, mean_q: 0.531391, mean_eps: 0.000000
 1978/5000: episode: 70, duration: 0.374s, episode steps:  26, steps per second:  70, episode reward: 32.489, mean reward:  1.250 [-2.570, 32.170], mean action: 4.385 [0.000, 19.000],  loss: 0.019777, mae: 0.306769, mean_q: 0.543025, mean_eps: 0.000000
 2002/5000: episode: 71, duration: 0.352s, episode steps:  24, steps per second:  68, episode reward: 39.000, mean reward:  1.625 [-2.805, 32.120], mean action: 3.500 [0.000, 19.000],  loss: 0.018543, mae: 0.305832, mean_q: 0.562516, mean_eps: 0.000000
 2034/5000: episode: 72, duration: 0.489s, episode steps:  32, steps per second:  65, episode reward: -32.070, mean reward: -1.002 [-32.410,  2.745], mean action: 3.719 [1.000, 19.000],  loss: 0.019696, mae: 0.306573, mean_q: 0.516819, mean_eps: 0.000000
 2058/5000: episode: 73, duration: 0.343s, episode steps:  24, steps per second:  70, episode reward: 38.377, mean reward:  1.599 [-3.000, 32.000], mean action: 6.083 [0.000, 20.000],  loss: 0.019921, mae: 0.311422, mean_q: 0.563638, mean_eps: 0.000000
 2080/5000: episode: 74, duration: 0.322s, episode steps:  22, steps per second:  68, episode reward: 44.492, mean reward:  2.022 [-2.201, 32.070], mean action: 1.045 [0.000, 12.000],  loss: 0.020829, mae: 0.316187, mean_q: 0.537379, mean_eps: 0.000000
 2107/5000: episode: 75, duration: 0.396s, episode steps:  27, steps per second:  68, episode reward: 40.885, mean reward:  1.514 [-2.465, 32.091], mean action: 4.630 [1.000, 19.000],  loss: 0.021474, mae: 0.308985, mean_q: 0.495203, mean_eps: 0.000000
 2128/5000: episode: 76, duration: 0.314s, episode steps:  21, steps per second:  67, episode reward: 44.250, mean reward:  2.107 [-2.806, 32.060], mean action: 4.286 [0.000, 13.000],  loss: 0.018700, mae: 0.297749, mean_q: 0.435890, mean_eps: 0.000000
 2146/5000: episode: 77, duration: 0.268s, episode steps:  18, steps per second:  67, episode reward: 41.941, mean reward:  2.330 [-2.644, 32.350], mean action: 4.333 [0.000, 14.000],  loss: 0.019657, mae: 0.306361, mean_q: 0.429249, mean_eps: 0.000000
 2168/5000: episode: 78, duration: 0.324s, episode steps:  22, steps per second:  68, episode reward: 38.501, mean reward:  1.750 [-2.998, 32.904], mean action: 4.318 [0.000, 20.000],  loss: 0.020851, mae: 0.308545, mean_q: 0.517372, mean_eps: 0.000000
 2194/5000: episode: 79, duration: 0.375s, episode steps:  26, steps per second:  69, episode reward: 41.903, mean reward:  1.612 [-3.000, 32.153], mean action: 3.692 [2.000, 16.000],  loss: 0.019976, mae: 0.303490, mean_q: 0.497652, mean_eps: 0.000000
 2214/5000: episode: 80, duration: 0.291s, episode steps:  20, steps per second:  69, episode reward: 38.940, mean reward:  1.947 [-2.777, 32.430], mean action: 3.050 [0.000, 16.000],  loss: 0.022637, mae: 0.321361, mean_q: 0.509682, mean_eps: 0.000000
 2235/5000: episode: 81, duration: 0.314s, episode steps:  21, steps per second:  67, episode reward: 44.709, mean reward:  2.129 [-2.293, 32.160], mean action: 2.000 [1.000, 11.000],  loss: 0.024263, mae: 0.327006, mean_q: 0.515185, mean_eps: 0.000000
 2257/5000: episode: 82, duration: 0.318s, episode steps:  22, steps per second:  69, episode reward: 38.328, mean reward:  1.742 [-2.588, 32.580], mean action: 2.636 [0.000, 15.000],  loss: 0.023983, mae: 0.326384, mean_q: 0.542462, mean_eps: 0.000000
 2299/5000: episode: 83, duration: 0.602s, episode steps:  42, steps per second:  70, episode reward: 37.951, mean reward:  0.904 [-3.000, 32.210], mean action: 4.048 [0.000, 19.000],  loss: 0.021334, mae: 0.315837, mean_q: 0.526536, mean_eps: 0.000000
 2339/5000: episode: 84, duration: 0.586s, episode steps:  40, steps per second:  68, episode reward: 43.515, mean reward:  1.088 [-2.617, 32.230], mean action: 5.900 [0.000, 20.000],  loss: 0.021568, mae: 0.325551, mean_q: 0.481143, mean_eps: 0.000000
 2388/5000: episode: 85, duration: 0.669s, episode steps:  49, steps per second:  73, episode reward: 32.934, mean reward:  0.672 [-2.704, 32.030], mean action: 7.020 [1.000, 19.000],  loss: 0.020092, mae: 0.315662, mean_q: 0.531695, mean_eps: 0.000000
 2426/5000: episode: 86, duration: 0.529s, episode steps:  38, steps per second:  72, episode reward: 38.453, mean reward:  1.012 [-3.000, 32.140], mean action: 6.211 [0.000, 15.000],  loss: 0.022251, mae: 0.331227, mean_q: 0.491322, mean_eps: 0.000000
 2474/5000: episode: 87, duration: 0.685s, episode steps:  48, steps per second:  70, episode reward: -32.580, mean reward: -0.679 [-31.951,  2.690], mean action: 8.021 [0.000, 21.000],  loss: 0.020338, mae: 0.315834, mean_q: 0.533170, mean_eps: 0.000000
 2491/5000: episode: 88, duration: 0.255s, episode steps:  17, steps per second:  67, episode reward: 43.730, mean reward:  2.572 [-3.000, 32.312], mean action: 3.706 [0.000, 12.000],  loss: 0.024695, mae: 0.332093, mean_q: 0.537269, mean_eps: 0.000000
 2517/5000: episode: 89, duration: 0.381s, episode steps:  26, steps per second:  68, episode reward: 42.000, mean reward:  1.615 [-2.520, 32.230], mean action: 3.654 [1.000, 15.000],  loss: 0.019212, mae: 0.310207, mean_q: 0.523289, mean_eps: 0.000000
 2534/5000: episode: 90, duration: 0.260s, episode steps:  17, steps per second:  65, episode reward: 41.821, mean reward:  2.460 [-3.000, 32.320], mean action: 3.529 [0.000, 15.000],  loss: 0.022962, mae: 0.318941, mean_q: 0.526487, mean_eps: 0.000000
 2587/5000: episode: 91, duration: 0.736s, episode steps:  53, steps per second:  72, episode reward: 37.345, mean reward:  0.705 [-2.353, 32.088], mean action: 3.962 [0.000, 19.000],  loss: 0.022702, mae: 0.326273, mean_q: 0.567597, mean_eps: 0.000000
 2613/5000: episode: 92, duration: 0.402s, episode steps:  26, steps per second:  65, episode reward: 41.279, mean reward:  1.588 [-3.000, 32.314], mean action: 2.346 [0.000, 13.000],  loss: 0.021128, mae: 0.318113, mean_q: 0.581469, mean_eps: 0.000000
 2639/5000: episode: 93, duration: 0.368s, episode steps:  26, steps per second:  71, episode reward: 39.000, mean reward:  1.500 [-2.216, 30.000], mean action: 2.077 [0.000, 15.000],  loss: 0.020759, mae: 0.315729, mean_q: 0.597836, mean_eps: 0.000000
 2658/5000: episode: 94, duration: 0.284s, episode steps:  19, steps per second:  67, episode reward: 44.618, mean reward:  2.348 [-2.900, 32.060], mean action: 3.474 [0.000, 19.000],  loss: 0.019336, mae: 0.303180, mean_q: 0.559404, mean_eps: 0.000000
 2696/5000: episode: 95, duration: 0.826s, episode steps:  38, steps per second:  46, episode reward: 38.493, mean reward:  1.013 [-2.066, 32.060], mean action: 2.789 [0.000, 15.000],  loss: 0.021329, mae: 0.316812, mean_q: 0.529463, mean_eps: 0.000000
 2726/5000: episode: 96, duration: 0.444s, episode steps:  30, steps per second:  68, episode reward: 41.102, mean reward:  1.370 [-2.134, 32.230], mean action: 4.033 [0.000, 15.000],  loss: 0.019709, mae: 0.305765, mean_q: 0.526403, mean_eps: 0.000000
 2755/5000: episode: 97, duration: 0.427s, episode steps:  29, steps per second:  68, episode reward: 45.000, mean reward:  1.552 [-2.192, 32.290], mean action: 3.172 [0.000, 15.000],  loss: 0.023577, mae: 0.321586, mean_q: 0.538415, mean_eps: 0.000000
 2774/5000: episode: 98, duration: 0.272s, episode steps:  19, steps per second:  70, episode reward: 38.562, mean reward:  2.030 [-2.363, 32.661], mean action: 3.526 [0.000, 16.000],  loss: 0.020671, mae: 0.304861, mean_q: 0.546251, mean_eps: 0.000000
 2792/5000: episode: 99, duration: 0.270s, episode steps:  18, steps per second:  67, episode reward: 44.564, mean reward:  2.476 [-2.024, 32.220], mean action: 4.611 [0.000, 13.000],  loss: 0.017910, mae: 0.303919, mean_q: 0.532456, mean_eps: 0.000000
 2815/5000: episode: 100, duration: 0.387s, episode steps:  23, steps per second:  59, episode reward: 41.114, mean reward:  1.788 [-2.286, 31.956], mean action: 4.217 [0.000, 14.000],  loss: 0.021691, mae: 0.315608, mean_q: 0.533513, mean_eps: 0.000000
 2840/5000: episode: 101, duration: 0.363s, episode steps:  25, steps per second:  69, episode reward: 39.000, mean reward:  1.560 [-2.798, 32.190], mean action: 4.840 [0.000, 15.000],  loss: 0.021206, mae: 0.312611, mean_q: 0.472504, mean_eps: 0.000000
 2859/5000: episode: 102, duration: 0.294s, episode steps:  19, steps per second:  65, episode reward: 44.160, mean reward:  2.324 [-2.521, 32.002], mean action: 1.789 [0.000, 3.000],  loss: 0.018683, mae: 0.306389, mean_q: 0.440256, mean_eps: 0.000000
 2886/5000: episode: 103, duration: 0.385s, episode steps:  27, steps per second:  70, episode reward: 40.882, mean reward:  1.514 [-2.366, 32.360], mean action: 3.037 [0.000, 14.000],  loss: 0.020975, mae: 0.309206, mean_q: 0.523196, mean_eps: 0.000000
 2907/5000: episode: 104, duration: 0.487s, episode steps:  21, steps per second:  43, episode reward: 40.858, mean reward:  1.946 [-2.459, 30.204], mean action: 2.857 [0.000, 12.000],  loss: 0.021160, mae: 0.308224, mean_q: 0.550563, mean_eps: 0.000000
 2922/5000: episode: 105, duration: 0.253s, episode steps:  15, steps per second:  59, episode reward: 42.000, mean reward:  2.800 [-2.325, 29.229], mean action: 2.267 [0.000, 3.000],  loss: 0.019937, mae: 0.312497, mean_q: 0.550842, mean_eps: 0.000000
 2945/5000: episode: 106, duration: 0.346s, episode steps:  23, steps per second:  66, episode reward: 44.074, mean reward:  1.916 [-2.141, 32.148], mean action: 2.217 [0.000, 3.000],  loss: 0.024957, mae: 0.334475, mean_q: 0.512223, mean_eps: 0.000000
 2968/5000: episode: 107, duration: 0.389s, episode steps:  23, steps per second:  59, episode reward: 46.420, mean reward:  2.018 [-0.330, 31.871], mean action: 3.261 [0.000, 13.000],  loss: 0.023261, mae: 0.317623, mean_q: 0.535356, mean_eps: 0.000000
 3004/5000: episode: 108, duration: 0.642s, episode steps:  36, steps per second:  56, episode reward: 38.638, mean reward:  1.073 [-2.938, 32.250], mean action: 1.806 [0.000, 14.000],  loss: 0.022120, mae: 0.323608, mean_q: 0.613442, mean_eps: 0.000000
 3024/5000: episode: 109, duration: 0.302s, episode steps:  20, steps per second:  66, episode reward: 41.613, mean reward:  2.081 [-2.627, 32.490], mean action: 1.900 [0.000, 4.000],  loss: 0.017313, mae: 0.291374, mean_q: 0.512220, mean_eps: 0.000000
 3056/5000: episode: 110, duration: 0.508s, episode steps:  32, steps per second:  63, episode reward: 47.282, mean reward:  1.478 [-0.365, 32.230], mean action: 3.281 [0.000, 12.000],  loss: 0.022330, mae: 0.321048, mean_q: 0.520174, mean_eps: 0.000000
 3096/5000: episode: 111, duration: 0.570s, episode steps:  40, steps per second:  70, episode reward: 42.000, mean reward:  1.050 [-2.230, 32.260], mean action: 3.500 [0.000, 15.000],  loss: 0.018617, mae: 0.305512, mean_q: 0.505022, mean_eps: 0.000000
 3143/5000: episode: 112, duration: 0.720s, episode steps:  47, steps per second:  65, episode reward: 36.000, mean reward:  0.766 [-2.482, 32.580], mean action: 2.404 [0.000, 15.000],  loss: 0.020599, mae: 0.315046, mean_q: 0.534042, mean_eps: 0.000000
 3173/5000: episode: 113, duration: 0.424s, episode steps:  30, steps per second:  71, episode reward: 44.352, mean reward:  1.478 [-2.283, 32.807], mean action: 1.067 [0.000, 3.000],  loss: 0.019000, mae: 0.306337, mean_q: 0.567970, mean_eps: 0.000000
 3218/5000: episode: 114, duration: 0.926s, episode steps:  45, steps per second:  49, episode reward: 40.815, mean reward:  0.907 [-2.566, 32.140], mean action: 4.622 [0.000, 11.000],  loss: 0.021549, mae: 0.322435, mean_q: 0.517285, mean_eps: 0.000000
 3226/5000: episode: 115, duration: 0.139s, episode steps:   8, steps per second:  57, episode reward: 47.211, mean reward:  5.901 [ 0.740, 33.000], mean action: 2.875 [0.000, 14.000],  loss: 0.020182, mae: 0.320756, mean_q: 0.507742, mean_eps: 0.000000
 3253/5000: episode: 116, duration: 0.387s, episode steps:  27, steps per second:  70, episode reward: 41.316, mean reward:  1.530 [-2.417, 32.590], mean action: 3.259 [0.000, 14.000],  loss: 0.022780, mae: 0.330557, mean_q: 0.479284, mean_eps: 0.000000
 3275/5000: episode: 117, duration: 0.365s, episode steps:  22, steps per second:  60, episode reward: 41.118, mean reward:  1.869 [-3.000, 32.677], mean action: 6.091 [0.000, 16.000],  loss: 0.020836, mae: 0.325399, mean_q: 0.524787, mean_eps: 0.000000
 3300/5000: episode: 118, duration: 0.411s, episode steps:  25, steps per second:  61, episode reward: 41.267, mean reward:  1.651 [-2.162, 31.527], mean action: 1.320 [0.000, 11.000],  loss: 0.021078, mae: 0.323318, mean_q: 0.528000, mean_eps: 0.000000
 3356/5000: episode: 119, duration: 0.787s, episode steps:  56, steps per second:  71, episode reward: 40.872, mean reward:  0.730 [-3.000, 32.070], mean action: 2.911 [1.000, 14.000],  loss: 0.018739, mae: 0.315670, mean_q: 0.486511, mean_eps: 0.000000
 3388/5000: episode: 120, duration: 0.480s, episode steps:  32, steps per second:  67, episode reward: 43.064, mean reward:  1.346 [-2.408, 32.620], mean action: 1.406 [0.000, 5.000],  loss: 0.019876, mae: 0.308399, mean_q: 0.588524, mean_eps: 0.000000
 3410/5000: episode: 121, duration: 0.390s, episode steps:  22, steps per second:  56, episode reward: 44.833, mean reward:  2.038 [-2.836, 32.313], mean action: 1.136 [1.000, 3.000],  loss: 0.014572, mae: 0.283074, mean_q: 0.580455, mean_eps: 0.000000
 3438/5000: episode: 122, duration: 0.413s, episode steps:  28, steps per second:  68, episode reward: 38.802, mean reward:  1.386 [-2.537, 32.101], mean action: 3.214 [0.000, 16.000],  loss: 0.023045, mae: 0.327693, mean_q: 0.655626, mean_eps: 0.000000
 3466/5000: episode: 123, duration: 0.404s, episode steps:  28, steps per second:  69, episode reward: 36.000, mean reward:  1.286 [-2.573, 32.500], mean action: 3.286 [0.000, 15.000],  loss: 0.021845, mae: 0.318277, mean_q: 0.608119, mean_eps: 0.000000
 3496/5000: episode: 124, duration: 0.450s, episode steps:  30, steps per second:  67, episode reward: 41.820, mean reward:  1.394 [-2.313, 32.110], mean action: 3.967 [0.000, 14.000],  loss: 0.019957, mae: 0.307596, mean_q: 0.538489, mean_eps: 0.000000
 3515/5000: episode: 125, duration: 0.289s, episode steps:  19, steps per second:  66, episode reward: 42.000, mean reward:  2.211 [-3.000, 32.070], mean action: 2.579 [0.000, 12.000],  loss: 0.021353, mae: 0.314307, mean_q: 0.523069, mean_eps: 0.000000
 3531/5000: episode: 126, duration: 0.243s, episode steps:  16, steps per second:  66, episode reward: 44.777, mean reward:  2.799 [-2.395, 32.549], mean action: 1.250 [0.000, 12.000],  loss: 0.021965, mae: 0.319313, mean_q: 0.481424, mean_eps: 0.000000
 3564/5000: episode: 127, duration: 0.474s, episode steps:  33, steps per second:  70, episode reward: 33.000, mean reward:  1.000 [-3.000, 32.310], mean action: 2.939 [0.000, 12.000],  loss: 0.021076, mae: 0.314179, mean_q: 0.570080, mean_eps: 0.000000
 3593/5000: episode: 128, duration: 0.408s, episode steps:  29, steps per second:  71, episode reward: 38.127, mean reward:  1.315 [-2.729, 32.901], mean action: 3.724 [0.000, 14.000],  loss: 0.020648, mae: 0.314722, mean_q: 0.550472, mean_eps: 0.000000
 3648/5000: episode: 129, duration: 0.760s, episode steps:  55, steps per second:  72, episode reward: 37.991, mean reward:  0.691 [-2.263, 32.020], mean action: 4.218 [0.000, 15.000],  loss: 0.020300, mae: 0.313707, mean_q: 0.563481, mean_eps: 0.000000
 3664/5000: episode: 130, duration: 0.242s, episode steps:  16, steps per second:  66, episode reward: 44.684, mean reward:  2.793 [-2.768, 32.470], mean action: 1.875 [0.000, 15.000],  loss: 0.024244, mae: 0.337470, mean_q: 0.531060, mean_eps: 0.000000
 3700/5000: episode: 131, duration: 0.517s, episode steps:  36, steps per second:  70, episode reward: 47.489, mean reward:  1.319 [-0.318, 32.020], mean action: 2.389 [0.000, 15.000],  loss: 0.025628, mae: 0.337500, mean_q: 0.519998, mean_eps: 0.000000
 3719/5000: episode: 132, duration: 0.279s, episode steps:  19, steps per second:  68, episode reward: 41.656, mean reward:  2.192 [-2.812, 31.916], mean action: 3.158 [0.000, 9.000],  loss: 0.020915, mae: 0.314082, mean_q: 0.522555, mean_eps: 0.000000
 3766/5000: episode: 133, duration: 0.743s, episode steps:  47, steps per second:  63, episode reward: 35.250, mean reward:  0.750 [-3.000, 32.054], mean action: 2.957 [0.000, 15.000],  loss: 0.019832, mae: 0.308552, mean_q: 0.543518, mean_eps: 0.000000
 3793/5000: episode: 134, duration: 0.392s, episode steps:  27, steps per second:  69, episode reward: 41.843, mean reward:  1.550 [-2.376, 32.583], mean action: 2.259 [0.000, 9.000],  loss: 0.021311, mae: 0.318634, mean_q: 0.514688, mean_eps: 0.000000
 3819/5000: episode: 135, duration: 0.370s, episode steps:  26, steps per second:  70, episode reward: 41.114, mean reward:  1.581 [-2.289, 32.223], mean action: 2.269 [0.000, 19.000],  loss: 0.021108, mae: 0.313562, mean_q: 0.510802, mean_eps: 0.000000
 3852/5000: episode: 136, duration: 0.517s, episode steps:  33, steps per second:  64, episode reward: -32.830, mean reward: -0.995 [-32.876,  3.000], mean action: 2.818 [0.000, 15.000],  loss: 0.022690, mae: 0.320618, mean_q: 0.546373, mean_eps: 0.000000
 3876/5000: episode: 137, duration: 0.363s, episode steps:  24, steps per second:  66, episode reward: 45.000, mean reward:  1.875 [-2.076, 32.480], mean action: 2.917 [0.000, 11.000],  loss: 0.016492, mae: 0.296418, mean_q: 0.451925, mean_eps: 0.000000
 3903/5000: episode: 138, duration: 0.400s, episode steps:  27, steps per second:  68, episode reward: 39.000, mean reward:  1.444 [-2.675, 32.720], mean action: 4.037 [0.000, 13.000],  loss: 0.017998, mae: 0.306310, mean_q: 0.475429, mean_eps: 0.000000
 3933/5000: episode: 139, duration: 0.431s, episode steps:  30, steps per second:  70, episode reward: 35.154, mean reward:  1.172 [-3.000, 31.594], mean action: 5.567 [0.000, 19.000],  loss: 0.020679, mae: 0.317739, mean_q: 0.591423, mean_eps: 0.000000
 3955/5000: episode: 140, duration: 0.321s, episode steps:  22, steps per second:  69, episode reward: 46.349, mean reward:  2.107 [-0.414, 32.140], mean action: 3.091 [0.000, 13.000],  loss: 0.018402, mae: 0.308785, mean_q: 0.524245, mean_eps: 0.000000
 3978/5000: episode: 141, duration: 0.335s, episode steps:  23, steps per second:  69, episode reward: 41.834, mean reward:  1.819 [-2.394, 32.080], mean action: 5.739 [1.000, 19.000],  loss: 0.019998, mae: 0.317251, mean_q: 0.477879, mean_eps: 0.000000
 4015/5000: episode: 142, duration: 0.530s, episode steps:  37, steps per second:  70, episode reward: -34.620, mean reward: -0.936 [-32.882,  2.509], mean action: 5.054 [0.000, 16.000],  loss: 0.019266, mae: 0.308373, mean_q: 0.504387, mean_eps: 0.000000
 4039/5000: episode: 143, duration: 0.353s, episode steps:  24, steps per second:  68, episode reward: 41.426, mean reward:  1.726 [-2.240, 32.151], mean action: 4.542 [0.000, 13.000],  loss: 0.019880, mae: 0.310601, mean_q: 0.494002, mean_eps: 0.000000
 4059/5000: episode: 144, duration: 0.291s, episode steps:  20, steps per second:  69, episode reward: 38.687, mean reward:  1.934 [-2.900, 32.687], mean action: 3.100 [0.000, 13.000],  loss: 0.018893, mae: 0.308481, mean_q: 0.491623, mean_eps: 0.000000
 4084/5000: episode: 145, duration: 0.362s, episode steps:  25, steps per second:  69, episode reward: 43.622, mean reward:  1.745 [-2.240, 32.340], mean action: 2.200 [0.000, 11.000],  loss: 0.021729, mae: 0.320893, mean_q: 0.505396, mean_eps: 0.000000
 4149/5000: episode: 146, duration: 0.893s, episode steps:  65, steps per second:  73, episode reward: -33.000, mean reward: -0.508 [-32.291,  3.000], mean action: 11.415 [0.000, 20.000],  loss: 0.022928, mae: 0.322544, mean_q: 0.511273, mean_eps: 0.000000
 4191/5000: episode: 147, duration: 0.589s, episode steps:  42, steps per second:  71, episode reward: 41.671, mean reward:  0.992 [-2.127, 29.772], mean action: 1.452 [0.000, 14.000],  loss: 0.023485, mae: 0.326214, mean_q: 0.526520, mean_eps: 0.000000
 4214/5000: episode: 148, duration: 0.335s, episode steps:  23, steps per second:  69, episode reward: 44.510, mean reward:  1.935 [-2.060, 32.140], mean action: 0.478 [0.000, 2.000],  loss: 0.019045, mae: 0.305699, mean_q: 0.567081, mean_eps: 0.000000
 4244/5000: episode: 149, duration: 0.422s, episode steps:  30, steps per second:  71, episode reward: 35.060, mean reward:  1.169 [-3.000, 31.969], mean action: 3.733 [1.000, 14.000],  loss: 0.020463, mae: 0.317767, mean_q: 0.568945, mean_eps: 0.000000
 4263/5000: episode: 150, duration: 0.280s, episode steps:  19, steps per second:  68, episode reward: 44.186, mean reward:  2.326 [-2.540, 32.090], mean action: 2.737 [1.000, 20.000],  loss: 0.017690, mae: 0.309579, mean_q: 0.536673, mean_eps: 0.000000
 4289/5000: episode: 151, duration: 0.377s, episode steps:  26, steps per second:  69, episode reward: 43.809, mean reward:  1.685 [-2.085, 31.843], mean action: 4.154 [1.000, 14.000],  loss: 0.023775, mae: 0.336436, mean_q: 0.547772, mean_eps: 0.000000
 4326/5000: episode: 152, duration: 0.522s, episode steps:  37, steps per second:  71, episode reward: 38.775, mean reward:  1.048 [-2.437, 32.120], mean action: 4.081 [0.000, 19.000],  loss: 0.019737, mae: 0.313798, mean_q: 0.571430, mean_eps: 0.000000
 4355/5000: episode: 153, duration: 0.424s, episode steps:  29, steps per second:  68, episode reward: 38.795, mean reward:  1.338 [-2.683, 32.215], mean action: 3.379 [0.000, 19.000],  loss: 0.023070, mae: 0.330070, mean_q: 0.555802, mean_eps: 0.000000
 4391/5000: episode: 154, duration: 0.517s, episode steps:  36, steps per second:  70, episode reward: 42.000, mean reward:  1.167 [-2.288, 33.000], mean action: 4.917 [0.000, 19.000],  loss: 0.021723, mae: 0.320207, mean_q: 0.524350, mean_eps: 0.000000
 4412/5000: episode: 155, duration: 0.303s, episode steps:  21, steps per second:  69, episode reward: 36.000, mean reward:  1.714 [-3.000, 32.090], mean action: 5.095 [0.000, 19.000],  loss: 0.022574, mae: 0.327173, mean_q: 0.503756, mean_eps: 0.000000
 4449/5000: episode: 156, duration: 0.738s, episode steps:  37, steps per second:  50, episode reward: 35.299, mean reward:  0.954 [-2.500, 32.180], mean action: 6.027 [0.000, 20.000],  loss: 0.021749, mae: 0.318376, mean_q: 0.543956, mean_eps: 0.000000
 4493/5000: episode: 157, duration: 0.630s, episode steps:  44, steps per second:  70, episode reward: 38.016, mean reward:  0.864 [-2.626, 32.240], mean action: 2.886 [0.000, 12.000],  loss: 0.022717, mae: 0.331593, mean_q: 0.570569, mean_eps: 0.000000
 4581/5000: episode: 158, duration: 1.237s, episode steps:  88, steps per second:  71, episode reward: -32.890, mean reward: -0.374 [-32.027,  2.655], mean action: 8.227 [0.000, 18.000],  loss: 0.022419, mae: 0.330911, mean_q: 0.583200, mean_eps: 0.000000
 4613/5000: episode: 159, duration: 0.459s, episode steps:  32, steps per second:  70, episode reward: 44.358, mean reward:  1.386 [-2.535, 32.090], mean action: 2.625 [0.000, 14.000],  loss: 0.019800, mae: 0.322403, mean_q: 0.554927, mean_eps: 0.000000
 4630/5000: episode: 160, duration: 0.255s, episode steps:  17, steps per second:  67, episode reward: 41.952, mean reward:  2.468 [-2.122, 32.140], mean action: 4.118 [0.000, 15.000],  loss: 0.019283, mae: 0.317641, mean_q: 0.521769, mean_eps: 0.000000
 4662/5000: episode: 161, duration: 0.450s, episode steps:  32, steps per second:  71, episode reward: 37.917, mean reward:  1.185 [-3.000, 31.827], mean action: 2.500 [0.000, 19.000],  loss: 0.021973, mae: 0.326463, mean_q: 0.537138, mean_eps: 0.000000
 4689/5000: episode: 162, duration: 0.466s, episode steps:  27, steps per second:  58, episode reward: 41.747, mean reward:  1.546 [-2.056, 32.741], mean action: 2.778 [0.000, 15.000],  loss: 0.020943, mae: 0.330557, mean_q: 0.537612, mean_eps: 0.000000
 4704/5000: episode: 163, duration: 0.276s, episode steps:  15, steps per second:  54, episode reward: 44.162, mean reward:  2.944 [-2.192, 31.710], mean action: 4.267 [0.000, 14.000],  loss: 0.020129, mae: 0.321362, mean_q: 0.517809, mean_eps: 0.000000
 4740/5000: episode: 164, duration: 0.517s, episode steps:  36, steps per second:  70, episode reward: 39.000, mean reward:  1.083 [-2.260, 33.000], mean action: 5.833 [0.000, 21.000],  loss: 0.021668, mae: 0.330809, mean_q: 0.484243, mean_eps: 0.000000
 4760/5000: episode: 165, duration: 0.299s, episode steps:  20, steps per second:  67, episode reward: 38.701, mean reward:  1.935 [-2.836, 32.240], mean action: 4.550 [1.000, 19.000],  loss: 0.018617, mae: 0.309481, mean_q: 0.544384, mean_eps: 0.000000
 4784/5000: episode: 166, duration: 0.360s, episode steps:  24, steps per second:  67, episode reward: 41.629, mean reward:  1.735 [-2.354, 31.799], mean action: 4.250 [1.000, 15.000],  loss: 0.020505, mae: 0.320192, mean_q: 0.476286, mean_eps: 0.000000
 4824/5000: episode: 167, duration: 0.588s, episode steps:  40, steps per second:  68, episode reward: 38.599, mean reward:  0.965 [-2.690, 30.490], mean action: 6.050 [0.000, 20.000],  loss: 0.018259, mae: 0.312986, mean_q: 0.484771, mean_eps: 0.000000
 4854/5000: episode: 168, duration: 0.502s, episode steps:  30, steps per second:  60, episode reward: 35.614, mean reward:  1.187 [-3.000, 32.242], mean action: 3.933 [0.000, 19.000],  loss: 0.019201, mae: 0.315025, mean_q: 0.506447, mean_eps: 0.000000
 4881/5000: episode: 169, duration: 0.413s, episode steps:  27, steps per second:  65, episode reward: 35.116, mean reward:  1.301 [-3.000, 32.070], mean action: 4.889 [0.000, 19.000],  loss: 0.019415, mae: 0.317366, mean_q: 0.516800, mean_eps: 0.000000
 4928/5000: episode: 170, duration: 0.658s, episode steps:  47, steps per second:  71, episode reward: 37.856, mean reward:  0.805 [-2.554, 32.180], mean action: 2.660 [0.000, 14.000],  loss: 0.022710, mae: 0.331645, mean_q: 0.554586, mean_eps: 0.000000
 4944/5000: episode: 171, duration: 0.257s, episode steps:  16, steps per second:  62, episode reward: 41.575, mean reward:  2.598 [-2.226, 32.196], mean action: 1.562 [0.000, 9.000],  loss: 0.020184, mae: 0.320908, mean_q: 0.586117, mean_eps: 0.000000
 4964/5000: episode: 172, duration: 0.296s, episode steps:  20, steps per second:  68, episode reward: 41.316, mean reward:  2.066 [-2.849, 32.310], mean action: 4.250 [0.000, 14.000],  loss: 0.020110, mae: 0.325526, mean_q: 0.554085, mean_eps: 0.000000
 4990/5000: episode: 173, duration: 0.406s, episode steps:  26, steps per second:  64, episode reward: 46.918, mean reward:  1.805 [-0.280, 32.230], mean action: 1.885 [0.000, 14.000],  loss: 0.021366, mae: 0.329476, mean_q: 0.561008, mean_eps: 0.000000
done, took 72.010 seconds
DQN Evaluation: 11098 victories out of 12937 episodes
Training for 5000 steps ...
   24/5000: episode: 1, duration: 0.193s, episode steps:  24, steps per second: 125, episode reward: -35.660, mean reward: -1.486 [-32.185,  2.570], mean action: 6.833 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   45/5000: episode: 2, duration: 0.159s, episode steps:  21, steps per second: 132, episode reward: 39.000, mean reward:  1.857 [-2.302, 32.110], mean action: 5.048 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   63/5000: episode: 3, duration: 0.141s, episode steps:  18, steps per second: 128, episode reward: 38.301, mean reward:  2.128 [-2.301, 32.200], mean action: 3.556 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   97/5000: episode: 4, duration: 0.231s, episode steps:  34, steps per second: 147, episode reward: 35.336, mean reward:  1.039 [-2.089, 31.932], mean action: 6.382 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  109/5000: episode: 5, duration: 0.097s, episode steps:  12, steps per second: 124, episode reward: 42.000, mean reward:  3.500 [-3.000, 32.480], mean action: 1.917 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  129/5000: episode: 6, duration: 0.153s, episode steps:  20, steps per second: 131, episode reward: 38.150, mean reward:  1.907 [-3.000, 32.522], mean action: 6.150 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  156/5000: episode: 7, duration: 0.185s, episode steps:  27, steps per second: 146, episode reward: -33.000, mean reward: -1.222 [-32.674,  2.310], mean action: 4.704 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  176/5000: episode: 8, duration: 0.147s, episode steps:  20, steps per second: 136, episode reward: 41.374, mean reward:  2.069 [-2.237, 32.290], mean action: 4.150 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  202/5000: episode: 9, duration: 0.189s, episode steps:  26, steps per second: 138, episode reward: 41.713, mean reward:  1.604 [-2.191, 32.384], mean action: 3.269 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  226/5000: episode: 10, duration: 0.168s, episode steps:  24, steps per second: 143, episode reward: 32.319, mean reward:  1.347 [-2.998, 32.702], mean action: 6.958 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  245/5000: episode: 11, duration: 0.143s, episode steps:  19, steps per second: 133, episode reward: 38.231, mean reward:  2.012 [-2.229, 32.689], mean action: 5.842 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  265/5000: episode: 12, duration: 0.143s, episode steps:  20, steps per second: 140, episode reward: 37.437, mean reward:  1.872 [-2.562, 32.659], mean action: 6.400 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  291/5000: episode: 13, duration: 0.180s, episode steps:  26, steps per second: 144, episode reward: 37.610, mean reward:  1.447 [-2.340, 32.050], mean action: 4.038 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  312/5000: episode: 14, duration: 0.146s, episode steps:  21, steps per second: 144, episode reward: 35.310, mean reward:  1.681 [-2.708, 32.766], mean action: 8.286 [1.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  333/5000: episode: 15, duration: 0.154s, episode steps:  21, steps per second: 137, episode reward: 36.000, mean reward:  1.714 [-2.433, 32.250], mean action: 3.714 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  359/5000: episode: 16, duration: 0.174s, episode steps:  26, steps per second: 149, episode reward: 35.412, mean reward:  1.362 [-2.710, 32.300], mean action: 6.885 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  380/5000: episode: 17, duration: 0.148s, episode steps:  21, steps per second: 142, episode reward: 41.817, mean reward:  1.991 [-2.433, 32.330], mean action: 4.667 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  398/5000: episode: 18, duration: 0.134s, episode steps:  18, steps per second: 134, episode reward: 44.182, mean reward:  2.455 [-2.128, 32.314], mean action: 4.833 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  422/5000: episode: 19, duration: 0.159s, episode steps:  24, steps per second: 151, episode reward: 38.849, mean reward:  1.619 [-2.234, 32.399], mean action: 4.875 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  455/5000: episode: 20, duration: 0.213s, episode steps:  33, steps per second: 155, episode reward: -33.000, mean reward: -1.000 [-32.194,  2.421], mean action: 5.152 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  473/5000: episode: 21, duration: 0.140s, episode steps:  18, steps per second: 129, episode reward: 41.442, mean reward:  2.302 [-2.849, 31.965], mean action: 3.111 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  487/5000: episode: 22, duration: 0.112s, episode steps:  14, steps per second: 125, episode reward: 38.494, mean reward:  2.750 [-2.625, 32.790], mean action: 6.143 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  505/5000: episode: 23, duration: 0.128s, episode steps:  18, steps per second: 141, episode reward: 38.117, mean reward:  2.118 [-2.834, 32.170], mean action: 6.556 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  521/5000: episode: 24, duration: 0.114s, episode steps:  16, steps per second: 141, episode reward: 38.688, mean reward:  2.418 [-3.000, 32.688], mean action: 4.562 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  552/5000: episode: 25, duration: 0.209s, episode steps:  31, steps per second: 148, episode reward: 32.420, mean reward:  1.046 [-2.833, 32.460], mean action: 7.065 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  580/5000: episode: 26, duration: 0.181s, episode steps:  28, steps per second: 155, episode reward: 32.902, mean reward:  1.175 [-2.490, 32.282], mean action: 7.250 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  598/5000: episode: 27, duration: 0.134s, episode steps:  18, steps per second: 134, episode reward: 38.567, mean reward:  2.143 [-2.363, 31.807], mean action: 4.111 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  621/5000: episode: 28, duration: 0.160s, episode steps:  23, steps per second: 144, episode reward: 38.154, mean reward:  1.659 [-2.193, 31.923], mean action: 5.696 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  639/5000: episode: 29, duration: 0.115s, episode steps:  18, steps per second: 156, episode reward: 35.743, mean reward:  1.986 [-2.904, 32.743], mean action: 4.333 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  655/5000: episode: 30, duration: 0.119s, episode steps:  16, steps per second: 134, episode reward: 45.934, mean reward:  2.871 [-0.155, 32.380], mean action: 4.688 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  673/5000: episode: 31, duration: 0.133s, episode steps:  18, steps per second: 136, episode reward: 38.136, mean reward:  2.119 [-3.000, 31.639], mean action: 3.000 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  693/5000: episode: 32, duration: 0.143s, episode steps:  20, steps per second: 140, episode reward: -32.740, mean reward: -1.637 [-32.532,  3.000], mean action: 5.800 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  723/5000: episode: 33, duration: 0.193s, episode steps:  30, steps per second: 156, episode reward: 32.902, mean reward:  1.097 [-2.317, 32.152], mean action: 6.533 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  745/5000: episode: 34, duration: 0.151s, episode steps:  22, steps per second: 146, episode reward: 33.000, mean reward:  1.500 [-3.000, 32.450], mean action: 7.136 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  766/5000: episode: 35, duration: 0.147s, episode steps:  21, steps per second: 143, episode reward: 33.000, mean reward:  1.571 [-3.000, 32.010], mean action: 6.238 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  789/5000: episode: 36, duration: 0.152s, episode steps:  23, steps per second: 151, episode reward: 36.000, mean reward:  1.565 [-2.533, 32.110], mean action: 3.217 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  813/5000: episode: 37, duration: 0.159s, episode steps:  24, steps per second: 151, episode reward: 36.000, mean reward:  1.500 [-2.778, 29.894], mean action: 4.167 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  832/5000: episode: 38, duration: 0.134s, episode steps:  19, steps per second: 142, episode reward: 41.381, mean reward:  2.178 [-2.249, 32.170], mean action: 2.158 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  868/5000: episode: 39, duration: 0.228s, episode steps:  36, steps per second: 158, episode reward: -35.480, mean reward: -0.986 [-32.171,  2.360], mean action: 5.250 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  899/5000: episode: 40, duration: 0.203s, episode steps:  31, steps per second: 153, episode reward: 32.264, mean reward:  1.041 [-2.493, 32.380], mean action: 5.710 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  926/5000: episode: 41, duration: 0.173s, episode steps:  27, steps per second: 156, episode reward: 35.860, mean reward:  1.328 [-2.140, 32.550], mean action: 6.148 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  953/5000: episode: 42, duration: 0.179s, episode steps:  27, steps per second: 151, episode reward: -32.140, mean reward: -1.190 [-32.065,  2.225], mean action: 4.704 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  977/5000: episode: 43, duration: 0.163s, episode steps:  24, steps per second: 148, episode reward: 35.911, mean reward:  1.496 [-2.775, 32.081], mean action: 5.792 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  998/5000: episode: 44, duration: 0.159s, episode steps:  21, steps per second: 132, episode reward: -32.860, mean reward: -1.565 [-32.339,  3.000], mean action: 7.048 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1030/5000: episode: 45, duration: 0.447s, episode steps:  32, steps per second:  72, episode reward: 32.465, mean reward:  1.015 [-3.000, 32.780], mean action: 3.031 [0.000, 15.000],  loss: 0.023702, mae: 0.342427, mean_q: 0.567612, mean_eps: 0.000000
 1064/5000: episode: 46, duration: 0.478s, episode steps:  34, steps per second:  71, episode reward: -35.460, mean reward: -1.043 [-32.113,  2.360], mean action: 4.324 [0.000, 15.000],  loss: 0.020413, mae: 0.334235, mean_q: 0.608283, mean_eps: 0.000000
 1089/5000: episode: 47, duration: 0.358s, episode steps:  25, steps per second:  70, episode reward: 32.008, mean reward:  1.280 [-2.188, 31.726], mean action: 4.160 [0.000, 15.000],  loss: 0.024522, mae: 0.351104, mean_q: 0.549066, mean_eps: 0.000000
 1112/5000: episode: 48, duration: 0.335s, episode steps:  23, steps per second:  69, episode reward: -32.310, mean reward: -1.405 [-32.149,  3.000], mean action: 7.130 [2.000, 15.000],  loss: 0.025180, mae: 0.353830, mean_q: 0.549309, mean_eps: 0.000000
 1132/5000: episode: 49, duration: 0.334s, episode steps:  20, steps per second:  60, episode reward: -38.510, mean reward: -1.925 [-32.591,  2.370], mean action: 4.800 [0.000, 15.000],  loss: 0.022199, mae: 0.340462, mean_q: 0.505329, mean_eps: 0.000000
 1160/5000: episode: 50, duration: 0.466s, episode steps:  28, steps per second:  60, episode reward: -32.800, mean reward: -1.171 [-32.564,  2.555], mean action: 5.929 [0.000, 17.000],  loss: 0.021965, mae: 0.344282, mean_q: 0.518986, mean_eps: 0.000000
 1173/5000: episode: 51, duration: 0.288s, episode steps:  13, steps per second:  45, episode reward: 44.903, mean reward:  3.454 [-2.248, 32.273], mean action: 1.846 [1.000, 3.000],  loss: 0.023212, mae: 0.344074, mean_q: 0.593345, mean_eps: 0.000000
 1205/5000: episode: 52, duration: 0.635s, episode steps:  32, steps per second:  50, episode reward: 32.660, mean reward:  1.021 [-3.000, 32.120], mean action: 7.094 [0.000, 20.000],  loss: 0.019709, mae: 0.330782, mean_q: 0.595835, mean_eps: 0.000000
 1218/5000: episode: 53, duration: 0.207s, episode steps:  13, steps per second:  63, episode reward: 42.000, mean reward:  3.231 [-2.687, 33.000], mean action: 4.000 [1.000, 19.000],  loss: 0.018418, mae: 0.319380, mean_q: 0.589062, mean_eps: 0.000000
 1248/5000: episode: 54, duration: 0.438s, episode steps:  30, steps per second:  68, episode reward: -35.570, mean reward: -1.186 [-32.127,  2.786], mean action: 4.867 [0.000, 19.000],  loss: 0.020700, mae: 0.331418, mean_q: 0.547741, mean_eps: 0.000000
 1281/5000: episode: 55, duration: 0.538s, episode steps:  33, steps per second:  61, episode reward: 32.614, mean reward:  0.988 [-3.000, 32.614], mean action: 5.576 [0.000, 18.000],  loss: 0.021617, mae: 0.337681, mean_q: 0.573414, mean_eps: 0.000000
 1300/5000: episode: 56, duration: 0.307s, episode steps:  19, steps per second:  62, episode reward: 38.819, mean reward:  2.043 [-2.619, 32.260], mean action: 3.053 [0.000, 12.000],  loss: 0.018528, mae: 0.317378, mean_q: 0.568257, mean_eps: 0.000000
 1324/5000: episode: 57, duration: 0.342s, episode steps:  24, steps per second:  70, episode reward: 33.000, mean reward:  1.375 [-2.806, 32.130], mean action: 4.208 [0.000, 16.000],  loss: 0.022213, mae: 0.330071, mean_q: 0.573053, mean_eps: 0.000000
 1353/5000: episode: 58, duration: 0.417s, episode steps:  29, steps per second:  70, episode reward: -33.000, mean reward: -1.138 [-32.271,  2.360], mean action: 6.828 [0.000, 16.000],  loss: 0.019784, mae: 0.318291, mean_q: 0.556608, mean_eps: 0.000000
 1377/5000: episode: 59, duration: 0.357s, episode steps:  24, steps per second:  67, episode reward: 38.868, mean reward:  1.620 [-2.368, 32.035], mean action: 2.833 [0.000, 15.000],  loss: 0.016981, mae: 0.316780, mean_q: 0.601529, mean_eps: 0.000000
 1397/5000: episode: 60, duration: 0.301s, episode steps:  20, steps per second:  67, episode reward: 32.951, mean reward:  1.648 [-3.000, 32.851], mean action: 4.800 [0.000, 15.000],  loss: 0.022140, mae: 0.336795, mean_q: 0.583998, mean_eps: 0.000000
 1419/5000: episode: 61, duration: 0.327s, episode steps:  22, steps per second:  67, episode reward: 32.247, mean reward:  1.466 [-3.000, 32.585], mean action: 7.773 [0.000, 19.000],  loss: 0.019781, mae: 0.325962, mean_q: 0.529023, mean_eps: 0.000000
 1449/5000: episode: 62, duration: 0.430s, episode steps:  30, steps per second:  70, episode reward: -39.000, mean reward: -1.300 [-32.255,  2.210], mean action: 4.467 [0.000, 15.000],  loss: 0.018387, mae: 0.319338, mean_q: 0.500672, mean_eps: 0.000000
 1471/5000: episode: 63, duration: 0.335s, episode steps:  22, steps per second:  66, episode reward: 38.248, mean reward:  1.739 [-2.188, 32.281], mean action: 7.000 [1.000, 15.000],  loss: 0.023583, mae: 0.343008, mean_q: 0.555550, mean_eps: 0.000000
 1502/5000: episode: 64, duration: 0.434s, episode steps:  31, steps per second:  72, episode reward: 32.538, mean reward:  1.050 [-2.465, 31.874], mean action: 5.226 [0.000, 15.000],  loss: 0.019820, mae: 0.325622, mean_q: 0.543616, mean_eps: 0.000000
 1544/5000: episode: 65, duration: 0.611s, episode steps:  42, steps per second:  69, episode reward: 32.593, mean reward:  0.776 [-2.242, 32.053], mean action: 6.000 [0.000, 14.000],  loss: 0.021791, mae: 0.331845, mean_q: 0.528932, mean_eps: 0.000000
 1566/5000: episode: 66, duration: 0.350s, episode steps:  22, steps per second:  63, episode reward: 38.559, mean reward:  1.753 [-3.000, 33.000], mean action: 3.000 [0.000, 12.000],  loss: 0.022676, mae: 0.331348, mean_q: 0.511817, mean_eps: 0.000000
 1593/5000: episode: 67, duration: 0.516s, episode steps:  27, steps per second:  52, episode reward: -33.000, mean reward: -1.222 [-32.394,  2.256], mean action: 4.074 [0.000, 19.000],  loss: 0.021273, mae: 0.324379, mean_q: 0.493002, mean_eps: 0.000000
 1614/5000: episode: 68, duration: 0.318s, episode steps:  21, steps per second:  66, episode reward: 35.942, mean reward:  1.712 [-2.846, 33.152], mean action: 3.952 [0.000, 19.000],  loss: 0.024055, mae: 0.343707, mean_q: 0.484007, mean_eps: 0.000000
 1632/5000: episode: 69, duration: 0.276s, episode steps:  18, steps per second:  65, episode reward: 33.000, mean reward:  1.833 [-3.000, 32.180], mean action: 5.778 [0.000, 19.000],  loss: 0.017698, mae: 0.313768, mean_q: 0.513792, mean_eps: 0.000000
 1647/5000: episode: 70, duration: 0.228s, episode steps:  15, steps per second:  66, episode reward: 38.793, mean reward:  2.586 [-2.502, 32.875], mean action: 5.533 [1.000, 19.000],  loss: 0.020019, mae: 0.321529, mean_q: 0.513493, mean_eps: 0.000000
 1673/5000: episode: 71, duration: 0.387s, episode steps:  26, steps per second:  67, episode reward: -32.320, mean reward: -1.243 [-32.287,  2.861], mean action: 7.269 [0.000, 20.000],  loss: 0.020007, mae: 0.324867, mean_q: 0.482975, mean_eps: 0.000000
 1694/5000: episode: 72, duration: 0.311s, episode steps:  21, steps per second:  68, episode reward: 35.766, mean reward:  1.703 [-2.441, 32.770], mean action: 5.476 [0.000, 19.000],  loss: 0.019815, mae: 0.329538, mean_q: 0.443449, mean_eps: 0.000000
 1740/5000: episode: 73, duration: 0.660s, episode steps:  46, steps per second:  70, episode reward: -38.730, mean reward: -0.842 [-32.079,  2.693], mean action: 5.391 [0.000, 19.000],  loss: 0.020020, mae: 0.312666, mean_q: 0.484749, mean_eps: 0.000000
 1756/5000: episode: 74, duration: 0.247s, episode steps:  16, steps per second:  65, episode reward: 41.205, mean reward:  2.575 [-2.643, 32.789], mean action: 4.500 [1.000, 15.000],  loss: 0.021256, mae: 0.321539, mean_q: 0.457963, mean_eps: 0.000000
 1806/5000: episode: 75, duration: 0.690s, episode steps:  50, steps per second:  72, episode reward: -32.890, mean reward: -0.658 [-32.123,  3.000], mean action: 12.280 [0.000, 20.000],  loss: 0.017546, mae: 0.304990, mean_q: 0.428843, mean_eps: 0.000000
 1832/5000: episode: 76, duration: 0.372s, episode steps:  26, steps per second:  70, episode reward: 38.531, mean reward:  1.482 [-2.591, 32.510], mean action: 4.423 [0.000, 12.000],  loss: 0.020661, mae: 0.315110, mean_q: 0.480065, mean_eps: 0.000000
 1852/5000: episode: 77, duration: 0.294s, episode steps:  20, steps per second:  68, episode reward: 35.125, mean reward:  1.756 [-2.408, 32.430], mean action: 4.000 [0.000, 15.000],  loss: 0.021442, mae: 0.322948, mean_q: 0.492394, mean_eps: 0.000000
 1870/5000: episode: 78, duration: 0.269s, episode steps:  18, steps per second:  67, episode reward: 39.000, mean reward:  2.167 [-2.233, 32.360], mean action: 3.556 [0.000, 11.000],  loss: 0.019679, mae: 0.316403, mean_q: 0.474042, mean_eps: 0.000000
 1893/5000: episode: 79, duration: 0.331s, episode steps:  23, steps per second:  69, episode reward: -32.390, mean reward: -1.408 [-32.266,  2.964], mean action: 6.391 [0.000, 20.000],  loss: 0.019184, mae: 0.316969, mean_q: 0.432609, mean_eps: 0.000000
 1919/5000: episode: 80, duration: 0.380s, episode steps:  26, steps per second:  69, episode reward: 36.000, mean reward:  1.385 [-2.504, 32.370], mean action: 4.500 [0.000, 15.000],  loss: 0.020198, mae: 0.318486, mean_q: 0.470886, mean_eps: 0.000000
 1934/5000: episode: 81, duration: 0.224s, episode steps:  15, steps per second:  67, episode reward: 36.000, mean reward:  2.400 [-3.000, 33.000], mean action: 6.200 [0.000, 16.000],  loss: 0.023461, mae: 0.325449, mean_q: 0.468395, mean_eps: 0.000000
 1951/5000: episode: 82, duration: 0.259s, episode steps:  17, steps per second:  66, episode reward: 38.094, mean reward:  2.241 [-3.000, 32.210], mean action: 3.000 [0.000, 16.000],  loss: 0.022657, mae: 0.323066, mean_q: 0.537817, mean_eps: 0.000000
 1976/5000: episode: 83, duration: 0.371s, episode steps:  25, steps per second:  67, episode reward: 38.414, mean reward:  1.537 [-2.217, 32.160], mean action: 2.840 [0.000, 15.000],  loss: 0.020973, mae: 0.322051, mean_q: 0.499664, mean_eps: 0.000000
 2003/5000: episode: 84, duration: 0.380s, episode steps:  27, steps per second:  71, episode reward: 35.224, mean reward:  1.305 [-3.000, 32.904], mean action: 6.037 [1.000, 16.000],  loss: 0.016679, mae: 0.304842, mean_q: 0.471044, mean_eps: 0.000000
 2026/5000: episode: 85, duration: 0.333s, episode steps:  23, steps per second:  69, episode reward: -36.000, mean reward: -1.565 [-33.000,  2.781], mean action: 6.870 [0.000, 20.000],  loss: 0.021142, mae: 0.325153, mean_q: 0.500636, mean_eps: 0.000000
 2049/5000: episode: 86, duration: 0.337s, episode steps:  23, steps per second:  68, episode reward: 35.737, mean reward:  1.554 [-2.756, 32.230], mean action: 3.826 [0.000, 15.000],  loss: 0.019319, mae: 0.313724, mean_q: 0.541947, mean_eps: 0.000000
 2079/5000: episode: 87, duration: 0.436s, episode steps:  30, steps per second:  69, episode reward: -32.160, mean reward: -1.072 [-32.098,  2.541], mean action: 7.333 [0.000, 19.000],  loss: 0.024813, mae: 0.338151, mean_q: 0.531914, mean_eps: 0.000000
 2114/5000: episode: 88, duration: 0.496s, episode steps:  35, steps per second:  71, episode reward: 34.693, mean reward:  0.991 [-3.000, 33.000], mean action: 6.200 [0.000, 19.000],  loss: 0.021514, mae: 0.320047, mean_q: 0.540119, mean_eps: 0.000000
 2145/5000: episode: 89, duration: 0.431s, episode steps:  31, steps per second:  72, episode reward: -32.180, mean reward: -1.038 [-32.213,  3.127], mean action: 7.161 [0.000, 19.000],  loss: 0.022682, mae: 0.323128, mean_q: 0.517267, mean_eps: 0.000000
 2163/5000: episode: 90, duration: 0.293s, episode steps:  18, steps per second:  61, episode reward: 38.322, mean reward:  2.129 [-2.223, 32.470], mean action: 6.278 [0.000, 19.000],  loss: 0.021798, mae: 0.322896, mean_q: 0.460215, mean_eps: 0.000000
 2185/5000: episode: 91, duration: 0.318s, episode steps:  22, steps per second:  69, episode reward: 35.967, mean reward:  1.635 [-2.353, 32.477], mean action: 6.045 [0.000, 19.000],  loss: 0.021233, mae: 0.317308, mean_q: 0.473217, mean_eps: 0.000000
 2211/5000: episode: 92, duration: 0.364s, episode steps:  26, steps per second:  71, episode reward: -44.550, mean reward: -1.713 [-32.198,  2.100], mean action: 4.962 [0.000, 20.000],  loss: 0.020637, mae: 0.312534, mean_q: 0.518370, mean_eps: 0.000000
 2232/5000: episode: 93, duration: 0.307s, episode steps:  21, steps per second:  68, episode reward: -36.000, mean reward: -1.714 [-32.394,  2.210], mean action: 7.286 [0.000, 18.000],  loss: 0.025076, mae: 0.329919, mean_q: 0.552999, mean_eps: 0.000000
 2246/5000: episode: 94, duration: 0.214s, episode steps:  14, steps per second:  65, episode reward: 43.894, mean reward:  3.135 [-2.169, 32.270], mean action: 4.643 [0.000, 20.000],  loss: 0.020801, mae: 0.315347, mean_q: 0.559809, mean_eps: 0.000000
 2275/5000: episode: 95, duration: 0.415s, episode steps:  29, steps per second:  70, episode reward: 35.526, mean reward:  1.225 [-3.000, 32.040], mean action: 3.862 [0.000, 16.000],  loss: 0.021320, mae: 0.319705, mean_q: 0.519831, mean_eps: 0.000000
 2304/5000: episode: 96, duration: 0.415s, episode steps:  29, steps per second:  70, episode reward: -32.380, mean reward: -1.117 [-32.201,  2.172], mean action: 5.345 [0.000, 16.000],  loss: 0.026430, mae: 0.348949, mean_q: 0.539564, mean_eps: 0.000000
 2340/5000: episode: 97, duration: 0.505s, episode steps:  36, steps per second:  71, episode reward: -41.360, mean reward: -1.149 [-32.319,  2.270], mean action: 4.083 [0.000, 16.000],  loss: 0.019266, mae: 0.314364, mean_q: 0.485017, mean_eps: 0.000000
 2373/5000: episode: 98, duration: 0.474s, episode steps:  33, steps per second:  70, episode reward: 33.000, mean reward:  1.000 [-3.000, 32.340], mean action: 7.758 [0.000, 19.000],  loss: 0.018893, mae: 0.298671, mean_q: 0.480860, mean_eps: 0.000000
 2395/5000: episode: 99, duration: 0.329s, episode steps:  22, steps per second:  67, episode reward: 32.455, mean reward:  1.475 [-3.000, 31.775], mean action: 5.955 [0.000, 19.000],  loss: 0.021999, mae: 0.313780, mean_q: 0.447373, mean_eps: 0.000000
 2429/5000: episode: 100, duration: 0.502s, episode steps:  34, steps per second:  68, episode reward: 32.869, mean reward:  0.967 [-2.492, 32.062], mean action: 7.382 [0.000, 18.000],  loss: 0.022181, mae: 0.316933, mean_q: 0.485419, mean_eps: 0.000000
 2472/5000: episode: 101, duration: 0.608s, episode steps:  43, steps per second:  71, episode reward: 32.626, mean reward:  0.759 [-2.366, 32.260], mean action: 6.651 [0.000, 17.000],  loss: 0.021335, mae: 0.317617, mean_q: 0.536262, mean_eps: 0.000000
 2493/5000: episode: 102, duration: 0.308s, episode steps:  21, steps per second:  68, episode reward: 35.079, mean reward:  1.670 [-2.518, 32.940], mean action: 5.333 [0.000, 15.000],  loss: 0.021702, mae: 0.318820, mean_q: 0.507595, mean_eps: 0.000000
 2510/5000: episode: 103, duration: 0.273s, episode steps:  17, steps per second:  62, episode reward: 38.700, mean reward:  2.276 [-2.336, 32.900], mean action: 3.059 [1.000, 11.000],  loss: 0.021401, mae: 0.323193, mean_q: 0.556204, mean_eps: 0.000000
 2534/5000: episode: 104, duration: 0.549s, episode steps:  24, steps per second:  44, episode reward: -32.770, mean reward: -1.365 [-32.279,  2.379], mean action: 6.292 [0.000, 16.000],  loss: 0.019285, mae: 0.316195, mean_q: 0.588358, mean_eps: 0.000000
 2553/5000: episode: 105, duration: 0.293s, episode steps:  19, steps per second:  65, episode reward: 38.245, mean reward:  2.013 [-2.558, 31.325], mean action: 4.105 [0.000, 16.000],  loss: 0.029388, mae: 0.362718, mean_q: 0.570876, mean_eps: 0.000000
 2583/5000: episode: 106, duration: 0.429s, episode steps:  30, steps per second:  70, episode reward: -36.000, mean reward: -1.200 [-32.496,  2.520], mean action: 6.033 [0.000, 17.000],  loss: 0.026802, mae: 0.353098, mean_q: 0.586806, mean_eps: 0.000000
 2603/5000: episode: 107, duration: 0.296s, episode steps:  20, steps per second:  68, episode reward: 35.592, mean reward:  1.780 [-2.464, 31.912], mean action: 5.500 [0.000, 16.000],  loss: 0.019116, mae: 0.312040, mean_q: 0.558367, mean_eps: 0.000000
 2623/5000: episode: 108, duration: 0.290s, episode steps:  20, steps per second:  69, episode reward: 33.000, mean reward:  1.650 [-3.000, 33.000], mean action: 5.150 [0.000, 15.000],  loss: 0.023370, mae: 0.337847, mean_q: 0.558438, mean_eps: 0.000000
 2645/5000: episode: 109, duration: 0.319s, episode steps:  22, steps per second:  69, episode reward: 33.000, mean reward:  1.500 [-2.903, 32.380], mean action: 6.227 [0.000, 16.000],  loss: 0.019228, mae: 0.322491, mean_q: 0.499275, mean_eps: 0.000000
 2675/5000: episode: 110, duration: 0.428s, episode steps:  30, steps per second:  70, episode reward: -32.490, mean reward: -1.083 [-31.798,  2.805], mean action: 6.433 [0.000, 16.000],  loss: 0.026355, mae: 0.350853, mean_q: 0.538914, mean_eps: 0.000000
 2710/5000: episode: 111, duration: 0.489s, episode steps:  35, steps per second:  72, episode reward: -36.000, mean reward: -1.029 [-32.407,  2.290], mean action: 7.286 [0.000, 18.000],  loss: 0.020708, mae: 0.321392, mean_q: 0.483233, mean_eps: 0.000000
 2734/5000: episode: 112, duration: 0.366s, episode steps:  24, steps per second:  66, episode reward: 35.620, mean reward:  1.484 [-2.420, 32.430], mean action: 3.667 [0.000, 19.000],  loss: 0.020692, mae: 0.317603, mean_q: 0.544509, mean_eps: 0.000000
 2747/5000: episode: 113, duration: 0.203s, episode steps:  13, steps per second:  64, episode reward: 41.875, mean reward:  3.221 [-2.612, 32.520], mean action: 3.308 [0.000, 12.000],  loss: 0.020731, mae: 0.322257, mean_q: 0.561674, mean_eps: 0.000000
 2779/5000: episode: 114, duration: 0.452s, episode steps:  32, steps per second:  71, episode reward: -32.810, mean reward: -1.025 [-32.373,  2.402], mean action: 6.219 [0.000, 15.000],  loss: 0.018862, mae: 0.313875, mean_q: 0.535810, mean_eps: 0.000000
 2805/5000: episode: 115, duration: 0.376s, episode steps:  26, steps per second:  69, episode reward: 37.230, mean reward:  1.432 [-2.471, 31.876], mean action: 3.000 [0.000, 15.000],  loss: 0.022004, mae: 0.334484, mean_q: 0.573812, mean_eps: 0.000000
 2823/5000: episode: 116, duration: 0.260s, episode steps:  18, steps per second:  69, episode reward: -35.910, mean reward: -1.995 [-32.811,  2.890], mean action: 4.889 [0.000, 17.000],  loss: 0.018503, mae: 0.318945, mean_q: 0.600216, mean_eps: 0.000000
 2840/5000: episode: 117, duration: 0.262s, episode steps:  17, steps per second:  65, episode reward: 41.214, mean reward:  2.424 [-2.367, 31.920], mean action: 1.235 [0.000, 14.000],  loss: 0.019846, mae: 0.327827, mean_q: 0.611599, mean_eps: 0.000000
 2864/5000: episode: 118, duration: 0.349s, episode steps:  24, steps per second:  69, episode reward: 32.161, mean reward:  1.340 [-2.900, 32.176], mean action: 3.417 [0.000, 15.000],  loss: 0.023604, mae: 0.337357, mean_q: 0.541361, mean_eps: 0.000000
 2894/5000: episode: 119, duration: 0.440s, episode steps:  30, steps per second:  68, episode reward: -35.580, mean reward: -1.186 [-32.107,  2.631], mean action: 5.500 [0.000, 18.000],  loss: 0.021779, mae: 0.322656, mean_q: 0.507430, mean_eps: 0.000000
 2920/5000: episode: 120, duration: 0.376s, episode steps:  26, steps per second:  69, episode reward: 32.627, mean reward:  1.255 [-3.000, 32.627], mean action: 5.808 [0.000, 20.000],  loss: 0.018363, mae: 0.312002, mean_q: 0.554400, mean_eps: 0.000000
 2943/5000: episode: 121, duration: 0.343s, episode steps:  23, steps per second:  67, episode reward: 32.277, mean reward:  1.403 [-2.947, 32.291], mean action: 5.130 [0.000, 15.000],  loss: 0.018918, mae: 0.316170, mean_q: 0.553661, mean_eps: 0.000000
 2977/5000: episode: 122, duration: 0.481s, episode steps:  34, steps per second:  71, episode reward: 37.935, mean reward:  1.116 [-2.532, 32.094], mean action: 3.118 [1.000, 16.000],  loss: 0.021025, mae: 0.322587, mean_q: 0.512361, mean_eps: 0.000000
 2995/5000: episode: 123, duration: 0.269s, episode steps:  18, steps per second:  67, episode reward: 38.715, mean reward:  2.151 [-2.274, 32.350], mean action: 5.056 [1.000, 16.000],  loss: 0.018478, mae: 0.319475, mean_q: 0.511943, mean_eps: 0.000000
 3016/5000: episode: 124, duration: 0.300s, episode steps:  21, steps per second:  70, episode reward: 35.079, mean reward:  1.670 [-2.459, 33.000], mean action: 4.238 [0.000, 12.000],  loss: 0.020816, mae: 0.323514, mean_q: 0.495428, mean_eps: 0.000000
 3044/5000: episode: 125, duration: 0.404s, episode steps:  28, steps per second:  69, episode reward: 35.312, mean reward:  1.261 [-2.387, 32.160], mean action: 5.321 [0.000, 16.000],  loss: 0.019551, mae: 0.312734, mean_q: 0.570132, mean_eps: 0.000000
 3059/5000: episode: 126, duration: 0.223s, episode steps:  15, steps per second:  67, episode reward: 41.183, mean reward:  2.746 [-2.389, 32.023], mean action: 4.067 [0.000, 16.000],  loss: 0.015509, mae: 0.298295, mean_q: 0.564394, mean_eps: 0.000000
 3093/5000: episode: 127, duration: 0.493s, episode steps:  34, steps per second:  69, episode reward: -32.120, mean reward: -0.945 [-32.053,  2.903], mean action: 2.294 [0.000, 15.000],  loss: 0.020795, mae: 0.325205, mean_q: 0.590585, mean_eps: 0.000000
 3116/5000: episode: 128, duration: 0.333s, episode steps:  23, steps per second:  69, episode reward: 38.996, mean reward:  1.695 [-2.373, 33.498], mean action: 2.913 [0.000, 11.000],  loss: 0.022210, mae: 0.338084, mean_q: 0.556386, mean_eps: 0.000000
 3140/5000: episode: 129, duration: 0.356s, episode steps:  24, steps per second:  67, episode reward: 36.000, mean reward:  1.500 [-2.544, 32.500], mean action: 5.208 [0.000, 15.000],  loss: 0.020438, mae: 0.331345, mean_q: 0.540918, mean_eps: 0.000000
 3165/5000: episode: 130, duration: 0.361s, episode steps:  25, steps per second:  69, episode reward: -35.100, mean reward: -1.404 [-32.034,  2.550], mean action: 5.120 [0.000, 15.000],  loss: 0.020991, mae: 0.335352, mean_q: 0.561200, mean_eps: 0.000000
 3197/5000: episode: 131, duration: 0.448s, episode steps:  32, steps per second:  71, episode reward: 38.249, mean reward:  1.195 [-3.000, 32.568], mean action: 10.250 [0.000, 20.000],  loss: 0.020336, mae: 0.323817, mean_q: 0.530066, mean_eps: 0.000000
 3221/5000: episode: 132, duration: 0.356s, episode steps:  24, steps per second:  67, episode reward: 35.615, mean reward:  1.484 [-2.416, 33.094], mean action: 5.167 [0.000, 14.000],  loss: 0.021799, mae: 0.329504, mean_q: 0.547183, mean_eps: 0.000000
 3243/5000: episode: 133, duration: 0.322s, episode steps:  22, steps per second:  68, episode reward: 36.000, mean reward:  1.636 [-2.126, 30.126], mean action: 3.773 [1.000, 16.000],  loss: 0.018419, mae: 0.329705, mean_q: 0.557362, mean_eps: 0.000000
 3261/5000: episode: 134, duration: 0.386s, episode steps:  18, steps per second:  47, episode reward: 38.259, mean reward:  2.126 [-2.214, 32.159], mean action: 4.056 [0.000, 16.000],  loss: 0.025372, mae: 0.347645, mean_q: 0.579197, mean_eps: 0.000000
 3283/5000: episode: 135, duration: 0.321s, episode steps:  22, steps per second:  69, episode reward: 32.903, mean reward:  1.496 [-3.000, 32.903], mean action: 4.409 [0.000, 14.000],  loss: 0.023162, mae: 0.342351, mean_q: 0.547838, mean_eps: 0.000000
 3300/5000: episode: 136, duration: 0.259s, episode steps:  17, steps per second:  66, episode reward: 41.709, mean reward:  2.453 [-2.506, 32.791], mean action: 4.353 [0.000, 20.000],  loss: 0.018473, mae: 0.314806, mean_q: 0.540087, mean_eps: 0.000000
 3317/5000: episode: 137, duration: 0.256s, episode steps:  17, steps per second:  66, episode reward: 40.701, mean reward:  2.394 [-2.398, 32.221], mean action: 4.235 [0.000, 16.000],  loss: 0.022418, mae: 0.334796, mean_q: 0.547278, mean_eps: 0.000000
 3345/5000: episode: 138, duration: 0.403s, episode steps:  28, steps per second:  70, episode reward: -33.000, mean reward: -1.179 [-32.204,  2.535], mean action: 7.750 [0.000, 16.000],  loss: 0.022845, mae: 0.345851, mean_q: 0.506102, mean_eps: 0.000000
 3364/5000: episode: 139, duration: 0.284s, episode steps:  19, steps per second:  67, episode reward: 35.456, mean reward:  1.866 [-2.332, 32.109], mean action: 5.526 [0.000, 16.000],  loss: 0.021110, mae: 0.337029, mean_q: 0.541264, mean_eps: 0.000000
 3394/5000: episode: 140, duration: 0.434s, episode steps:  30, steps per second:  69, episode reward: 36.628, mean reward:  1.221 [-3.000, 33.000], mean action: 7.867 [1.000, 19.000],  loss: 0.025601, mae: 0.352463, mean_q: 0.585962, mean_eps: 0.000000
 3416/5000: episode: 141, duration: 0.317s, episode steps:  22, steps per second:  69, episode reward: -32.520, mean reward: -1.478 [-31.982,  3.000], mean action: 5.864 [0.000, 16.000],  loss: 0.021524, mae: 0.343670, mean_q: 0.598931, mean_eps: 0.000000
 3443/5000: episode: 142, duration: 0.387s, episode steps:  27, steps per second:  70, episode reward: -35.230, mean reward: -1.305 [-32.224,  3.000], mean action: 4.481 [0.000, 16.000],  loss: 0.023441, mae: 0.343975, mean_q: 0.524228, mean_eps: 0.000000
 3467/5000: episode: 143, duration: 0.352s, episode steps:  24, steps per second:  68, episode reward: 35.320, mean reward:  1.472 [-3.000, 32.367], mean action: 5.750 [0.000, 18.000],  loss: 0.016841, mae: 0.314308, mean_q: 0.483300, mean_eps: 0.000000
 3482/5000: episode: 144, duration: 0.231s, episode steps:  15, steps per second:  65, episode reward: 38.704, mean reward:  2.580 [-2.268, 32.901], mean action: 4.333 [0.000, 15.000],  loss: 0.021139, mae: 0.334694, mean_q: 0.495012, mean_eps: 0.000000
 3499/5000: episode: 145, duration: 0.259s, episode steps:  17, steps per second:  66, episode reward: 41.803, mean reward:  2.459 [-2.609, 31.886], mean action: 5.000 [0.000, 15.000],  loss: 0.021718, mae: 0.335125, mean_q: 0.507666, mean_eps: 0.000000
 3517/5000: episode: 146, duration: 0.267s, episode steps:  18, steps per second:  67, episode reward: 38.088, mean reward:  2.116 [-2.904, 32.230], mean action: 3.889 [1.000, 12.000],  loss: 0.022747, mae: 0.340377, mean_q: 0.519206, mean_eps: 0.000000
 3536/5000: episode: 147, duration: 0.291s, episode steps:  19, steps per second:  65, episode reward: 34.854, mean reward:  1.834 [-3.000, 31.888], mean action: 5.737 [0.000, 15.000],  loss: 0.025480, mae: 0.350960, mean_q: 0.512365, mean_eps: 0.000000
 3556/5000: episode: 148, duration: 0.314s, episode steps:  20, steps per second:  64, episode reward: 43.720, mean reward:  2.186 [-2.067, 32.175], mean action: 2.400 [1.000, 12.000],  loss: 0.018403, mae: 0.312065, mean_q: 0.506432, mean_eps: 0.000000
 3586/5000: episode: 149, duration: 0.428s, episode steps:  30, steps per second:  70, episode reward: 35.690, mean reward:  1.190 [-2.421, 33.000], mean action: 6.167 [0.000, 21.000],  loss: 0.023890, mae: 0.338086, mean_q: 0.504890, mean_eps: 0.000000
 3606/5000: episode: 150, duration: 0.298s, episode steps:  20, steps per second:  67, episode reward: 36.000, mean reward:  1.800 [-2.706, 32.420], mean action: 4.850 [0.000, 16.000],  loss: 0.017062, mae: 0.300984, mean_q: 0.510483, mean_eps: 0.000000
 3621/5000: episode: 151, duration: 0.227s, episode steps:  15, steps per second:  66, episode reward: 40.601, mean reward:  2.707 [-2.927, 31.271], mean action: 5.000 [1.000, 19.000],  loss: 0.024232, mae: 0.352742, mean_q: 0.568265, mean_eps: 0.000000
 3671/5000: episode: 152, duration: 0.714s, episode steps:  50, steps per second:  70, episode reward: 32.372, mean reward:  0.647 [-2.941, 31.805], mean action: 4.000 [0.000, 14.000],  loss: 0.022674, mae: 0.340957, mean_q: 0.571608, mean_eps: 0.000000
 3693/5000: episode: 153, duration: 0.344s, episode steps:  22, steps per second:  64, episode reward: 38.224, mean reward:  1.737 [-2.479, 33.000], mean action: 6.091 [0.000, 15.000],  loss: 0.019529, mae: 0.320637, mean_q: 0.535730, mean_eps: 0.000000
 3731/5000: episode: 154, duration: 0.539s, episode steps:  38, steps per second:  70, episode reward: 32.650, mean reward:  0.859 [-2.436, 32.712], mean action: 9.000 [0.000, 15.000],  loss: 0.021455, mae: 0.332964, mean_q: 0.573773, mean_eps: 0.000000
 3758/5000: episode: 155, duration: 0.389s, episode steps:  27, steps per second:  69, episode reward: -36.000, mean reward: -1.333 [-32.357,  2.531], mean action: 8.222 [0.000, 20.000],  loss: 0.019676, mae: 0.326971, mean_q: 0.593600, mean_eps: 0.000000
 3780/5000: episode: 156, duration: 0.333s, episode steps:  22, steps per second:  66, episode reward: -32.820, mean reward: -1.492 [-32.042,  2.660], mean action: 4.591 [0.000, 16.000],  loss: 0.021315, mae: 0.323687, mean_q: 0.574399, mean_eps: 0.000000
 3808/5000: episode: 157, duration: 0.406s, episode steps:  28, steps per second:  69, episode reward: 35.668, mean reward:  1.274 [-2.476, 32.270], mean action: 4.964 [1.000, 20.000],  loss: 0.022474, mae: 0.334842, mean_q: 0.510123, mean_eps: 0.000000
 3832/5000: episode: 158, duration: 0.355s, episode steps:  24, steps per second:  68, episode reward: 32.167, mean reward:  1.340 [-2.532, 32.273], mean action: 6.083 [0.000, 15.000],  loss: 0.020329, mae: 0.327486, mean_q: 0.584819, mean_eps: 0.000000
 3854/5000: episode: 159, duration: 0.317s, episode steps:  22, steps per second:  69, episode reward: 38.755, mean reward:  1.762 [-2.261, 32.215], mean action: 2.500 [0.000, 16.000],  loss: 0.020331, mae: 0.321719, mean_q: 0.592719, mean_eps: 0.000000
 3869/5000: episode: 160, duration: 0.239s, episode steps:  15, steps per second:  63, episode reward: 41.221, mean reward:  2.748 [-2.770, 33.000], mean action: 3.933 [0.000, 20.000],  loss: 0.022448, mae: 0.333570, mean_q: 0.552283, mean_eps: 0.000000
 3898/5000: episode: 161, duration: 0.419s, episode steps:  29, steps per second:  69, episode reward: -32.120, mean reward: -1.108 [-32.612,  2.262], mean action: 5.138 [0.000, 20.000],  loss: 0.021190, mae: 0.332558, mean_q: 0.503162, mean_eps: 0.000000
 3915/5000: episode: 162, duration: 0.254s, episode steps:  17, steps per second:  67, episode reward: 38.776, mean reward:  2.281 [-2.901, 32.776], mean action: 3.706 [0.000, 11.000],  loss: 0.020273, mae: 0.320677, mean_q: 0.545674, mean_eps: 0.000000
 3945/5000: episode: 163, duration: 0.417s, episode steps:  30, steps per second:  72, episode reward: -41.810, mean reward: -1.394 [-32.602,  2.903], mean action: 7.267 [1.000, 15.000],  loss: 0.019521, mae: 0.316139, mean_q: 0.540500, mean_eps: 0.000000
 3977/5000: episode: 164, duration: 0.464s, episode steps:  32, steps per second:  69, episode reward: 32.261, mean reward:  1.008 [-2.612, 32.320], mean action: 4.562 [0.000, 14.000],  loss: 0.025905, mae: 0.342724, mean_q: 0.559817, mean_eps: 0.000000
 4001/5000: episode: 165, duration: 0.340s, episode steps:  24, steps per second:  71, episode reward: -38.230, mean reward: -1.593 [-32.368,  2.240], mean action: 7.792 [0.000, 19.000],  loss: 0.019863, mae: 0.319130, mean_q: 0.512610, mean_eps: 0.000000
 4027/5000: episode: 166, duration: 0.368s, episode steps:  26, steps per second:  71, episode reward: 35.319, mean reward:  1.358 [-2.460, 32.390], mean action: 6.154 [1.000, 19.000],  loss: 0.018106, mae: 0.316313, mean_q: 0.568001, mean_eps: 0.000000
 4060/5000: episode: 167, duration: 0.481s, episode steps:  33, steps per second:  69, episode reward: -38.680, mean reward: -1.172 [-32.168,  2.174], mean action: 9.455 [0.000, 20.000],  loss: 0.017427, mae: 0.311437, mean_q: 0.561623, mean_eps: 0.000000
 4083/5000: episode: 168, duration: 0.335s, episode steps:  23, steps per second:  69, episode reward: 37.161, mean reward:  1.616 [-2.693, 30.811], mean action: 1.870 [0.000, 11.000],  loss: 0.020985, mae: 0.330780, mean_q: 0.554854, mean_eps: 0.000000
 4100/5000: episode: 169, duration: 0.258s, episode steps:  17, steps per second:  66, episode reward: 44.911, mean reward:  2.642 [-2.213, 33.000], mean action: 1.118 [0.000, 11.000],  loss: 0.016355, mae: 0.309790, mean_q: 0.596492, mean_eps: 0.000000
 4114/5000: episode: 170, duration: 0.425s, episode steps:  14, steps per second:  33, episode reward: 46.972, mean reward:  3.355 [-0.287, 32.536], mean action: 5.071 [0.000, 20.000],  loss: 0.019375, mae: 0.319505, mean_q: 0.603302, mean_eps: 0.000000
 4137/5000: episode: 171, duration: 0.360s, episode steps:  23, steps per second:  64, episode reward: 35.887, mean reward:  1.560 [-2.804, 32.071], mean action: 2.957 [0.000, 11.000],  loss: 0.021106, mae: 0.327038, mean_q: 0.541011, mean_eps: 0.000000
 4166/5000: episode: 172, duration: 0.416s, episode steps:  29, steps per second:  70, episode reward: 32.303, mean reward:  1.114 [-2.480, 31.433], mean action: 3.793 [1.000, 11.000],  loss: 0.021442, mae: 0.324650, mean_q: 0.552355, mean_eps: 0.000000
 4187/5000: episode: 173, duration: 0.300s, episode steps:  21, steps per second:  70, episode reward: 32.901, mean reward:  1.567 [-2.389, 32.901], mean action: 3.095 [0.000, 11.000],  loss: 0.019879, mae: 0.319163, mean_q: 0.518443, mean_eps: 0.000000
 4218/5000: episode: 174, duration: 0.436s, episode steps:  31, steps per second:  71, episode reward: -36.000, mean reward: -1.161 [-32.148,  2.502], mean action: 3.161 [0.000, 11.000],  loss: 0.021648, mae: 0.335512, mean_q: 0.515536, mean_eps: 0.000000
 4235/5000: episode: 175, duration: 0.257s, episode steps:  17, steps per second:  66, episode reward: 42.000, mean reward:  2.471 [-2.901, 32.200], mean action: 2.353 [0.000, 11.000],  loss: 0.019873, mae: 0.331709, mean_q: 0.519903, mean_eps: 0.000000
 4257/5000: episode: 176, duration: 0.313s, episode steps:  22, steps per second:  70, episode reward: 36.000, mean reward:  1.636 [-3.000, 32.140], mean action: 3.409 [0.000, 15.000],  loss: 0.018125, mae: 0.318404, mean_q: 0.492852, mean_eps: 0.000000
 4276/5000: episode: 177, duration: 0.286s, episode steps:  19, steps per second:  66, episode reward: 35.410, mean reward:  1.864 [-2.458, 32.174], mean action: 3.895 [0.000, 15.000],  loss: 0.015809, mae: 0.310488, mean_q: 0.506338, mean_eps: 0.000000
 4303/5000: episode: 178, duration: 0.386s, episode steps:  27, steps per second:  70, episode reward: 35.165, mean reward:  1.302 [-2.388, 32.700], mean action: 7.889 [0.000, 19.000],  loss: 0.020660, mae: 0.331254, mean_q: 0.483332, mean_eps: 0.000000
 4319/5000: episode: 179, duration: 0.244s, episode steps:  16, steps per second:  66, episode reward: 36.000, mean reward:  2.250 [-3.000, 29.881], mean action: 4.812 [0.000, 19.000],  loss: 0.025263, mae: 0.356065, mean_q: 0.551045, mean_eps: 0.000000
 4336/5000: episode: 180, duration: 0.252s, episode steps:  17, steps per second:  67, episode reward: 38.141, mean reward:  2.244 [-2.969, 31.926], mean action: 4.353 [0.000, 19.000],  loss: 0.023795, mae: 0.352310, mean_q: 0.553025, mean_eps: 0.000000
 4359/5000: episode: 181, duration: 0.341s, episode steps:  23, steps per second:  67, episode reward: 35.660, mean reward:  1.550 [-2.335, 31.790], mean action: 4.826 [0.000, 19.000],  loss: 0.020344, mae: 0.331358, mean_q: 0.632425, mean_eps: 0.000000
 4381/5000: episode: 182, duration: 0.311s, episode steps:  22, steps per second:  71, episode reward: -36.000, mean reward: -1.636 [-33.000,  2.460], mean action: 5.909 [0.000, 19.000],  loss: 0.021656, mae: 0.334797, mean_q: 0.637021, mean_eps: 0.000000
 4398/5000: episode: 183, duration: 0.264s, episode steps:  17, steps per second:  64, episode reward: 41.465, mean reward:  2.439 [-2.522, 32.110], mean action: 4.118 [0.000, 19.000],  loss: 0.017770, mae: 0.316664, mean_q: 0.621640, mean_eps: 0.000000
 4446/5000: episode: 184, duration: 0.691s, episode steps:  48, steps per second:  69, episode reward: -33.000, mean reward: -0.688 [-32.389,  2.463], mean action: 7.000 [0.000, 21.000],  loss: 0.022508, mae: 0.341553, mean_q: 0.606239, mean_eps: 0.000000
 4481/5000: episode: 185, duration: 0.504s, episode steps:  35, steps per second:  69, episode reward: -33.000, mean reward: -0.943 [-32.023,  2.838], mean action: 6.971 [2.000, 19.000],  loss: 0.021974, mae: 0.339045, mean_q: 0.569208, mean_eps: 0.000000
 4496/5000: episode: 186, duration: 0.323s, episode steps:  15, steps per second:  46, episode reward: 41.335, mean reward:  2.756 [-2.470, 32.211], mean action: 4.333 [1.000, 14.000],  loss: 0.022051, mae: 0.340556, mean_q: 0.567628, mean_eps: 0.000000
 4536/5000: episode: 187, duration: 0.785s, episode steps:  40, steps per second:  51, episode reward: 33.000, mean reward:  0.825 [-2.372, 32.590], mean action: 5.450 [0.000, 20.000],  loss: 0.019901, mae: 0.329008, mean_q: 0.512818, mean_eps: 0.000000
 4556/5000: episode: 188, duration: 0.297s, episode steps:  20, steps per second:  67, episode reward: 32.647, mean reward:  1.632 [-3.000, 32.101], mean action: 4.800 [0.000, 16.000],  loss: 0.023686, mae: 0.341812, mean_q: 0.541124, mean_eps: 0.000000
 4578/5000: episode: 189, duration: 0.318s, episode steps:  22, steps per second:  69, episode reward: 35.327, mean reward:  1.606 [-2.378, 32.158], mean action: 3.591 [0.000, 16.000],  loss: 0.016939, mae: 0.310043, mean_q: 0.539944, mean_eps: 0.000000
 4592/5000: episode: 190, duration: 0.210s, episode steps:  14, steps per second:  67, episode reward: 47.214, mean reward:  3.372 [-0.329, 33.000], mean action: 5.357 [1.000, 14.000],  loss: 0.017178, mae: 0.311157, mean_q: 0.476923, mean_eps: 0.000000
 4615/5000: episode: 191, duration: 0.339s, episode steps:  23, steps per second:  68, episode reward: 35.719, mean reward:  1.553 [-2.360, 32.860], mean action: 5.696 [0.000, 15.000],  loss: 0.018264, mae: 0.327984, mean_q: 0.541879, mean_eps: 0.000000
 4644/5000: episode: 192, duration: 0.417s, episode steps:  29, steps per second:  69, episode reward: 32.498, mean reward:  1.121 [-2.348, 32.200], mean action: 4.448 [0.000, 19.000],  loss: 0.017182, mae: 0.324814, mean_q: 0.542991, mean_eps: 0.000000
 4664/5000: episode: 193, duration: 0.296s, episode steps:  20, steps per second:  68, episode reward: 37.423, mean reward:  1.871 [-2.939, 31.462], mean action: 5.800 [0.000, 15.000],  loss: 0.014065, mae: 0.306727, mean_q: 0.566881, mean_eps: 0.000000
 4688/5000: episode: 194, duration: 0.350s, episode steps:  24, steps per second:  69, episode reward: 35.607, mean reward:  1.484 [-3.000, 32.610], mean action: 7.583 [1.000, 15.000],  loss: 0.021907, mae: 0.341118, mean_q: 0.620876, mean_eps: 0.000000
 4723/5000: episode: 195, duration: 0.487s, episode steps:  35, steps per second:  72, episode reward: -35.020, mean reward: -1.001 [-32.122,  2.322], mean action: 4.800 [0.000, 15.000],  loss: 0.021606, mae: 0.329657, mean_q: 0.584245, mean_eps: 0.000000
 4744/5000: episode: 196, duration: 0.308s, episode steps:  21, steps per second:  68, episode reward: 36.000, mean reward:  1.714 [-2.320, 32.070], mean action: 3.667 [0.000, 19.000],  loss: 0.024349, mae: 0.347608, mean_q: 0.569173, mean_eps: 0.000000
 4766/5000: episode: 197, duration: 0.331s, episode steps:  22, steps per second:  67, episode reward: 35.871, mean reward:  1.630 [-2.713, 32.660], mean action: 5.545 [0.000, 15.000],  loss: 0.021579, mae: 0.329080, mean_q: 0.573489, mean_eps: 0.000000
 4794/5000: episode: 198, duration: 0.409s, episode steps:  28, steps per second:  68, episode reward: 35.343, mean reward:  1.262 [-2.164, 31.553], mean action: 6.143 [0.000, 15.000],  loss: 0.020862, mae: 0.330494, mean_q: 0.580952, mean_eps: 0.000000
 4820/5000: episode: 199, duration: 0.381s, episode steps:  26, steps per second:  68, episode reward: 32.815, mean reward:  1.262 [-2.537, 31.925], mean action: 5.000 [0.000, 15.000],  loss: 0.022053, mae: 0.334978, mean_q: 0.616503, mean_eps: 0.000000
 4833/5000: episode: 200, duration: 0.210s, episode steps:  13, steps per second:  62, episode reward: 44.127, mean reward:  3.394 [-2.293, 33.000], mean action: 2.000 [0.000, 11.000],  loss: 0.018270, mae: 0.326536, mean_q: 0.614136, mean_eps: 0.000000
 4856/5000: episode: 201, duration: 0.327s, episode steps:  23, steps per second:  70, episode reward: -35.180, mean reward: -1.530 [-32.540,  2.130], mean action: 5.217 [0.000, 15.000],  loss: 0.021151, mae: 0.331814, mean_q: 0.546640, mean_eps: 0.000000
 4876/5000: episode: 202, duration: 0.302s, episode steps:  20, steps per second:  66, episode reward: 36.000, mean reward:  1.800 [-2.842, 32.320], mean action: 5.350 [0.000, 15.000],  loss: 0.021531, mae: 0.325279, mean_q: 0.499381, mean_eps: 0.000000
 4893/5000: episode: 203, duration: 0.258s, episode steps:  17, steps per second:  66, episode reward: 39.000, mean reward:  2.294 [-3.000, 32.230], mean action: 3.353 [1.000, 15.000],  loss: 0.019847, mae: 0.321677, mean_q: 0.485542, mean_eps: 0.000000
 4913/5000: episode: 204, duration: 0.299s, episode steps:  20, steps per second:  67, episode reward: 36.000, mean reward:  1.800 [-2.716, 30.199], mean action: 5.300 [0.000, 15.000],  loss: 0.015809, mae: 0.297826, mean_q: 0.525666, mean_eps: 0.000000
 4935/5000: episode: 205, duration: 0.322s, episode steps:  22, steps per second:  68, episode reward: 39.000, mean reward:  1.773 [-2.142, 32.730], mean action: 4.545 [0.000, 19.000],  loss: 0.018215, mae: 0.303050, mean_q: 0.502670, mean_eps: 0.000000
 4953/5000: episode: 206, duration: 0.267s, episode steps:  18, steps per second:  67, episode reward: 38.206, mean reward:  2.123 [-2.639, 32.230], mean action: 4.333 [0.000, 16.000],  loss: 0.020891, mae: 0.324746, mean_q: 0.488888, mean_eps: 0.000000
 4981/5000: episode: 207, duration: 0.405s, episode steps:  28, steps per second:  69, episode reward: -33.000, mean reward: -1.179 [-32.178,  2.800], mean action: 5.607 [0.000, 16.000],  loss: 0.017915, mae: 0.307525, mean_q: 0.523873, mean_eps: 0.000000
 4999/5000: episode: 208, duration: 0.265s, episode steps:  18, steps per second:  68, episode reward: 35.397, mean reward:  1.966 [-3.000, 33.000], mean action: 7.500 [0.000, 16.000],  loss: 0.017866, mae: 0.307949, mean_q: 0.557498, mean_eps: 0.000000
done, took 66.964 seconds
DQN Evaluation: 11256 victories out of 13146 episodes
Training for 5000 steps ...
   20/5000: episode: 1, duration: 0.178s, episode steps:  20, steps per second: 113, episode reward: 42.000, mean reward:  2.100 [-2.120, 32.560], mean action: 3.000 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   50/5000: episode: 2, duration: 0.198s, episode steps:  30, steps per second: 152, episode reward: 38.185, mean reward:  1.273 [-3.000, 32.070], mean action: 3.433 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   78/5000: episode: 3, duration: 0.203s, episode steps:  28, steps per second: 138, episode reward: 42.000, mean reward:  1.500 [-2.237, 32.020], mean action: 3.321 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   95/5000: episode: 4, duration: 0.139s, episode steps:  17, steps per second: 123, episode reward: 44.181, mean reward:  2.599 [-2.107, 32.110], mean action: 4.647 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  134/5000: episode: 5, duration: 0.251s, episode steps:  39, steps per second: 155, episode reward: -32.620, mean reward: -0.836 [-32.222,  2.770], mean action: 8.641 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  157/5000: episode: 6, duration: 0.157s, episode steps:  23, steps per second: 147, episode reward: 41.692, mean reward:  1.813 [-2.233, 31.702], mean action: 3.783 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  175/5000: episode: 7, duration: 0.141s, episode steps:  18, steps per second: 127, episode reward: 41.640, mean reward:  2.313 [-2.532, 31.749], mean action: 3.944 [1.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  202/5000: episode: 8, duration: 0.177s, episode steps:  27, steps per second: 152, episode reward: -35.050, mean reward: -1.298 [-32.229,  2.211], mean action: 9.926 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  246/5000: episode: 9, duration: 0.293s, episode steps:  44, steps per second: 150, episode reward: 36.780, mean reward:  0.836 [-2.781, 32.162], mean action: 7.932 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  260/5000: episode: 10, duration: 0.113s, episode steps:  14, steps per second: 124, episode reward: 41.475, mean reward:  2.963 [-2.745, 32.086], mean action: 4.000 [1.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  298/5000: episode: 11, duration: 0.259s, episode steps:  38, steps per second: 147, episode reward: 41.877, mean reward:  1.102 [-2.231, 32.180], mean action: 0.921 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  329/5000: episode: 12, duration: 0.215s, episode steps:  31, steps per second: 144, episode reward: 43.827, mean reward:  1.414 [-2.565, 32.740], mean action: 3.258 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  360/5000: episode: 13, duration: 0.205s, episode steps:  31, steps per second: 151, episode reward: 40.863, mean reward:  1.318 [-2.998, 32.010], mean action: 3.613 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  375/5000: episode: 14, duration: 0.119s, episode steps:  15, steps per second: 126, episode reward: 44.394, mean reward:  2.960 [-2.685, 32.160], mean action: 3.800 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  399/5000: episode: 15, duration: 0.163s, episode steps:  24, steps per second: 147, episode reward: 41.062, mean reward:  1.711 [-2.388, 32.500], mean action: 5.500 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  423/5000: episode: 16, duration: 0.167s, episode steps:  24, steps per second: 143, episode reward: 40.257, mean reward:  1.677 [-3.000, 32.682], mean action: 6.625 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  457/5000: episode: 17, duration: 0.227s, episode steps:  34, steps per second: 150, episode reward: 41.690, mean reward:  1.226 [-2.207, 32.060], mean action: 3.559 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  477/5000: episode: 18, duration: 0.145s, episode steps:  20, steps per second: 138, episode reward: 44.462, mean reward:  2.223 [-2.345, 31.856], mean action: 5.200 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  510/5000: episode: 19, duration: 0.212s, episode steps:  33, steps per second: 156, episode reward: 38.757, mean reward:  1.174 [-2.890, 32.200], mean action: 4.818 [1.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  543/5000: episode: 20, duration: 0.215s, episode steps:  33, steps per second: 154, episode reward: 41.492, mean reward:  1.257 [-2.472, 32.030], mean action: 3.970 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  570/5000: episode: 21, duration: 0.175s, episode steps:  27, steps per second: 154, episode reward: 38.498, mean reward:  1.426 [-3.000, 32.019], mean action: 4.667 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  584/5000: episode: 22, duration: 0.104s, episode steps:  14, steps per second: 135, episode reward: 41.580, mean reward:  2.970 [-2.463, 32.350], mean action: 3.357 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  603/5000: episode: 23, duration: 0.135s, episode steps:  19, steps per second: 140, episode reward: 42.000, mean reward:  2.211 [-2.751, 32.390], mean action: 2.737 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  636/5000: episode: 24, duration: 0.210s, episode steps:  33, steps per second: 158, episode reward: 35.502, mean reward:  1.076 [-2.309, 32.039], mean action: 6.273 [1.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  657/5000: episode: 25, duration: 0.144s, episode steps:  21, steps per second: 146, episode reward: 38.288, mean reward:  1.823 [-3.000, 32.007], mean action: 4.810 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  706/5000: episode: 26, duration: 0.314s, episode steps:  49, steps per second: 156, episode reward: 35.053, mean reward:  0.715 [-2.118, 32.130], mean action: 4.571 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  741/5000: episode: 27, duration: 0.240s, episode steps:  35, steps per second: 146, episode reward: 35.189, mean reward:  1.005 [-3.000, 32.600], mean action: 4.800 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  778/5000: episode: 28, duration: 0.238s, episode steps:  37, steps per second: 156, episode reward: 41.904, mean reward:  1.133 [-2.646, 32.110], mean action: 2.216 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  804/5000: episode: 29, duration: 0.188s, episode steps:  26, steps per second: 139, episode reward: 38.474, mean reward:  1.480 [-3.000, 32.070], mean action: 7.500 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  826/5000: episode: 30, duration: 0.158s, episode steps:  22, steps per second: 139, episode reward: 41.879, mean reward:  1.904 [-2.348, 32.170], mean action: 3.682 [1.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  845/5000: episode: 31, duration: 0.139s, episode steps:  19, steps per second: 137, episode reward: 43.743, mean reward:  2.302 [-2.568, 32.200], mean action: 3.632 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  880/5000: episode: 32, duration: 0.222s, episode steps:  35, steps per second: 158, episode reward: 41.438, mean reward:  1.184 [-2.131, 31.932], mean action: 5.371 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  899/5000: episode: 33, duration: 0.144s, episode steps:  19, steps per second: 132, episode reward: 41.774, mean reward:  2.199 [-2.579, 31.973], mean action: 4.211 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  936/5000: episode: 34, duration: 0.241s, episode steps:  37, steps per second: 154, episode reward: 41.904, mean reward:  1.133 [-2.349, 32.134], mean action: 2.892 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  963/5000: episode: 35, duration: 0.185s, episode steps:  27, steps per second: 146, episode reward: 43.521, mean reward:  1.612 [-2.577, 32.200], mean action: 2.926 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  989/5000: episode: 36, duration: 0.171s, episode steps:  26, steps per second: 152, episode reward: 44.243, mean reward:  1.702 [-2.165, 32.160], mean action: 3.808 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1042/5000: episode: 37, duration: 0.668s, episode steps:  53, steps per second:  79, episode reward: 42.000, mean reward:  0.792 [-2.220, 32.470], mean action: 3.453 [0.000, 16.000],  loss: 0.021487, mae: 0.329972, mean_q: 0.580796, mean_eps: 0.000000
 1056/5000: episode: 38, duration: 0.223s, episode steps:  14, steps per second:  63, episode reward: 47.275, mean reward:  3.377 [-0.134, 32.290], mean action: 3.000 [3.000, 3.000],  loss: 0.016414, mae: 0.309432, mean_q: 0.499211, mean_eps: 0.000000
 1075/5000: episode: 39, duration: 0.289s, episode steps:  19, steps per second:  66, episode reward: 44.351, mean reward:  2.334 [-2.082, 32.390], mean action: 2.105 [0.000, 16.000],  loss: 0.019274, mae: 0.314555, mean_q: 0.504113, mean_eps: 0.000000
 1113/5000: episode: 40, duration: 0.554s, episode steps:  38, steps per second:  69, episode reward: -32.450, mean reward: -0.854 [-32.031,  2.620], mean action: 6.395 [0.000, 20.000],  loss: 0.019317, mae: 0.312684, mean_q: 0.533029, mean_eps: 0.000000
 1150/5000: episode: 41, duration: 0.721s, episode steps:  37, steps per second:  51, episode reward: 36.000, mean reward:  0.973 [-3.000, 32.050], mean action: 3.243 [0.000, 15.000],  loss: 0.020732, mae: 0.316224, mean_q: 0.544239, mean_eps: 0.000000
 1176/5000: episode: 42, duration: 0.395s, episode steps:  26, steps per second:  66, episode reward: 35.675, mean reward:  1.372 [-3.000, 32.610], mean action: 4.923 [0.000, 19.000],  loss: 0.020888, mae: 0.317651, mean_q: 0.513043, mean_eps: 0.000000
 1200/5000: episode: 43, duration: 0.343s, episode steps:  24, steps per second:  70, episode reward: 42.000, mean reward:  1.750 [-3.000, 32.100], mean action: 3.917 [1.000, 19.000],  loss: 0.018389, mae: 0.307900, mean_q: 0.517173, mean_eps: 0.000000
 1217/5000: episode: 44, duration: 0.260s, episode steps:  17, steps per second:  65, episode reward: 44.578, mean reward:  2.622 [-2.004, 32.480], mean action: 0.765 [0.000, 5.000],  loss: 0.019563, mae: 0.309169, mean_q: 0.499295, mean_eps: 0.000000
 1241/5000: episode: 45, duration: 0.345s, episode steps:  24, steps per second:  69, episode reward: 42.000, mean reward:  1.750 [-2.487, 32.150], mean action: 3.542 [0.000, 19.000],  loss: 0.024917, mae: 0.333774, mean_q: 0.538165, mean_eps: 0.000000
 1260/5000: episode: 46, duration: 0.289s, episode steps:  19, steps per second:  66, episode reward: 45.000, mean reward:  2.368 [-2.445, 32.610], mean action: 1.105 [0.000, 19.000],  loss: 0.020520, mae: 0.313865, mean_q: 0.553733, mean_eps: 0.000000
 1281/5000: episode: 47, duration: 0.309s, episode steps:  21, steps per second:  68, episode reward: 41.739, mean reward:  1.988 [-2.475, 32.302], mean action: 2.810 [0.000, 19.000],  loss: 0.016493, mae: 0.297621, mean_q: 0.562936, mean_eps: 0.000000
 1309/5000: episode: 48, duration: 7.487s, episode steps:  28, steps per second:   4, episode reward: 42.000, mean reward:  1.500 [-2.253, 32.080], mean action: 3.286 [0.000, 19.000],  loss: 0.018315, mae: 0.312708, mean_q: 0.569575, mean_eps: 0.000000
 1331/5000: episode: 49, duration: 0.334s, episode steps:  22, steps per second:  66, episode reward: 41.702, mean reward:  1.896 [-2.337, 31.872], mean action: 3.818 [1.000, 11.000],  loss: 0.021274, mae: 0.325687, mean_q: 0.529298, mean_eps: 0.000000
 1369/5000: episode: 50, duration: 0.548s, episode steps:  38, steps per second:  69, episode reward: 38.390, mean reward:  1.010 [-2.481, 32.040], mean action: 3.921 [0.000, 19.000],  loss: 0.021608, mae: 0.337398, mean_q: 0.552224, mean_eps: 0.000000
 1420/5000: episode: 51, duration: 0.716s, episode steps:  51, steps per second:  71, episode reward: 32.828, mean reward:  0.644 [-2.352, 32.160], mean action: 3.078 [0.000, 15.000],  loss: 0.021163, mae: 0.333279, mean_q: 0.619346, mean_eps: 0.000000
 1443/5000: episode: 52, duration: 0.365s, episode steps:  23, steps per second:  63, episode reward: 44.603, mean reward:  1.939 [-3.000, 31.792], mean action: 2.043 [0.000, 9.000],  loss: 0.018358, mae: 0.322036, mean_q: 0.532689, mean_eps: 0.000000
 1468/5000: episode: 53, duration: 0.389s, episode steps:  25, steps per second:  64, episode reward: 41.855, mean reward:  1.674 [-2.142, 32.233], mean action: 3.440 [0.000, 14.000],  loss: 0.021007, mae: 0.329130, mean_q: 0.482217, mean_eps: 0.000000
 1485/5000: episode: 54, duration: 0.266s, episode steps:  17, steps per second:  64, episode reward: 44.199, mean reward:  2.600 [-2.632, 31.887], mean action: 1.471 [0.000, 6.000],  loss: 0.019691, mae: 0.320030, mean_q: 0.530710, mean_eps: 0.000000
 1499/5000: episode: 55, duration: 0.219s, episode steps:  14, steps per second:  64, episode reward: 41.901, mean reward:  2.993 [-3.000, 32.781], mean action: 3.643 [0.000, 19.000],  loss: 0.019802, mae: 0.322296, mean_q: 0.549282, mean_eps: 0.000000
 1536/5000: episode: 56, duration: 0.523s, episode steps:  37, steps per second:  71, episode reward: 34.992, mean reward:  0.946 [-3.000, 32.140], mean action: 4.649 [0.000, 19.000],  loss: 0.017792, mae: 0.314768, mean_q: 0.578999, mean_eps: 0.000000
 1572/5000: episode: 57, duration: 0.514s, episode steps:  36, steps per second:  70, episode reward: 36.000, mean reward:  1.000 [-2.919, 29.964], mean action: 3.222 [0.000, 12.000],  loss: 0.019320, mae: 0.316555, mean_q: 0.572493, mean_eps: 0.000000
 1586/5000: episode: 58, duration: 0.224s, episode steps:  14, steps per second:  63, episode reward: 43.246, mean reward:  3.089 [-3.000, 32.574], mean action: 4.929 [0.000, 13.000],  loss: 0.023856, mae: 0.346045, mean_q: 0.588477, mean_eps: 0.000000
 1607/5000: episode: 59, duration: 0.335s, episode steps:  21, steps per second:  63, episode reward: 41.316, mean reward:  1.967 [-2.903, 32.080], mean action: 4.714 [0.000, 16.000],  loss: 0.021794, mae: 0.331267, mean_q: 0.570317, mean_eps: 0.000000
 1634/5000: episode: 60, duration: 0.397s, episode steps:  27, steps per second:  68, episode reward: 38.680, mean reward:  1.433 [-3.000, 31.941], mean action: 3.444 [0.000, 16.000],  loss: 0.024523, mae: 0.345048, mean_q: 0.631556, mean_eps: 0.000000
 1655/5000: episode: 61, duration: 0.320s, episode steps:  21, steps per second:  66, episode reward: 44.700, mean reward:  2.129 [-2.457, 32.180], mean action: 6.238 [0.000, 14.000],  loss: 0.023525, mae: 0.346884, mean_q: 0.632439, mean_eps: 0.000000
 1673/5000: episode: 62, duration: 0.268s, episode steps:  18, steps per second:  67, episode reward: 39.000, mean reward:  2.167 [-2.903, 32.060], mean action: 3.833 [0.000, 16.000],  loss: 0.019918, mae: 0.322901, mean_q: 0.597348, mean_eps: 0.000000
 1699/5000: episode: 63, duration: 0.384s, episode steps:  26, steps per second:  68, episode reward: 46.754, mean reward:  1.798 [-0.625, 32.190], mean action: 5.500 [1.000, 20.000],  loss: 0.017890, mae: 0.309719, mean_q: 0.543151, mean_eps: 0.000000
 1723/5000: episode: 64, duration: 0.367s, episode steps:  24, steps per second:  65, episode reward: 41.065, mean reward:  1.711 [-2.274, 31.988], mean action: 3.500 [0.000, 20.000],  loss: 0.023571, mae: 0.337279, mean_q: 0.562710, mean_eps: 0.000000
 1751/5000: episode: 65, duration: 0.423s, episode steps:  28, steps per second:  66, episode reward: 37.822, mean reward:  1.351 [-2.448, 32.110], mean action: 6.607 [0.000, 20.000],  loss: 0.019354, mae: 0.314299, mean_q: 0.578270, mean_eps: 0.000000
 1776/5000: episode: 66, duration: 0.366s, episode steps:  25, steps per second:  68, episode reward: 36.000, mean reward:  1.440 [-3.000, 32.110], mean action: 4.760 [0.000, 19.000],  loss: 0.021662, mae: 0.322900, mean_q: 0.552031, mean_eps: 0.000000
 1801/5000: episode: 67, duration: 0.381s, episode steps:  25, steps per second:  66, episode reward: 40.694, mean reward:  1.628 [-3.000, 31.775], mean action: 3.960 [0.000, 20.000],  loss: 0.017800, mae: 0.309771, mean_q: 0.555426, mean_eps: 0.000000
 1818/5000: episode: 68, duration: 0.256s, episode steps:  17, steps per second:  66, episode reward: 47.642, mean reward:  2.802 [-0.086, 32.133], mean action: 1.235 [0.000, 5.000],  loss: 0.020953, mae: 0.323027, mean_q: 0.521865, mean_eps: 0.000000
 1840/5000: episode: 69, duration: 0.319s, episode steps:  22, steps per second:  69, episode reward: 39.000, mean reward:  1.773 [-2.067, 29.263], mean action: 3.409 [0.000, 20.000],  loss: 0.021213, mae: 0.323028, mean_q: 0.507767, mean_eps: 0.000000
 1863/5000: episode: 70, duration: 0.359s, episode steps:  23, steps per second:  64, episode reward: 38.803, mean reward:  1.687 [-3.000, 32.160], mean action: 4.304 [0.000, 19.000],  loss: 0.024383, mae: 0.334554, mean_q: 0.548875, mean_eps: 0.000000
 1897/5000: episode: 71, duration: 0.494s, episode steps:  34, steps per second:  69, episode reward: 41.341, mean reward:  1.216 [-2.523, 32.320], mean action: 1.971 [0.000, 19.000],  loss: 0.020431, mae: 0.318635, mean_q: 0.560295, mean_eps: 0.000000
 1922/5000: episode: 72, duration: 0.360s, episode steps:  25, steps per second:  69, episode reward: 39.000, mean reward:  1.560 [-3.000, 32.070], mean action: 3.440 [0.000, 11.000],  loss: 0.017796, mae: 0.299141, mean_q: 0.553846, mean_eps: 0.000000
 1946/5000: episode: 73, duration: 0.349s, episode steps:  24, steps per second:  69, episode reward: 41.559, mean reward:  1.732 [-2.373, 32.130], mean action: 3.458 [1.000, 14.000],  loss: 0.020307, mae: 0.310349, mean_q: 0.514866, mean_eps: 0.000000
 1972/5000: episode: 74, duration: 0.376s, episode steps:  26, steps per second:  69, episode reward: 39.000, mean reward:  1.500 [-2.267, 32.360], mean action: 4.192 [2.000, 15.000],  loss: 0.020127, mae: 0.312565, mean_q: 0.512647, mean_eps: 0.000000
 2001/5000: episode: 75, duration: 0.437s, episode steps:  29, steps per second:  66, episode reward: 35.174, mean reward:  1.213 [-2.488, 30.245], mean action: 4.103 [0.000, 21.000],  loss: 0.020173, mae: 0.317389, mean_q: 0.545268, mean_eps: 0.000000
 2023/5000: episode: 76, duration: 0.334s, episode steps:  22, steps per second:  66, episode reward: 41.059, mean reward:  1.866 [-2.584, 31.981], mean action: 2.136 [0.000, 11.000],  loss: 0.020697, mae: 0.321825, mean_q: 0.578479, mean_eps: 0.000000
 2046/5000: episode: 77, duration: 0.342s, episode steps:  23, steps per second:  67, episode reward: 38.779, mean reward:  1.686 [-2.167, 32.571], mean action: 3.391 [0.000, 15.000],  loss: 0.019942, mae: 0.313053, mean_q: 0.547126, mean_eps: 0.000000
 2080/5000: episode: 78, duration: 0.493s, episode steps:  34, steps per second:  69, episode reward: 41.468, mean reward:  1.220 [-2.767, 31.588], mean action: 3.000 [0.000, 15.000],  loss: 0.021428, mae: 0.326170, mean_q: 0.526165, mean_eps: 0.000000
 2102/5000: episode: 79, duration: 0.327s, episode steps:  22, steps per second:  67, episode reward: 44.517, mean reward:  2.024 [-2.024, 31.963], mean action: 1.773 [0.000, 3.000],  loss: 0.020134, mae: 0.314699, mean_q: 0.555183, mean_eps: 0.000000
 2132/5000: episode: 80, duration: 0.443s, episode steps:  30, steps per second:  68, episode reward: 38.385, mean reward:  1.280 [-2.363, 31.695], mean action: 4.100 [1.000, 19.000],  loss: 0.022641, mae: 0.325296, mean_q: 0.547081, mean_eps: 0.000000
 2157/5000: episode: 81, duration: 0.376s, episode steps:  25, steps per second:  66, episode reward: 45.799, mean reward:  1.832 [-1.507, 32.170], mean action: 3.280 [0.000, 14.000],  loss: 0.021430, mae: 0.310410, mean_q: 0.490210, mean_eps: 0.000000
 2206/5000: episode: 82, duration: 0.699s, episode steps:  49, steps per second:  70, episode reward: 32.772, mean reward:  0.669 [-3.000, 32.310], mean action: 7.816 [0.000, 19.000],  loss: 0.021101, mae: 0.310770, mean_q: 0.499068, mean_eps: 0.000000
 2234/5000: episode: 83, duration: 0.409s, episode steps:  28, steps per second:  68, episode reward: 43.525, mean reward:  1.554 [-2.571, 32.180], mean action: 4.250 [0.000, 14.000],  loss: 0.022659, mae: 0.317248, mean_q: 0.481394, mean_eps: 0.000000
 2256/5000: episode: 84, duration: 0.335s, episode steps:  22, steps per second:  66, episode reward: 40.927, mean reward:  1.860 [-2.448, 31.790], mean action: 3.455 [0.000, 12.000],  loss: 0.018639, mae: 0.299843, mean_q: 0.522110, mean_eps: 0.000000
 2281/5000: episode: 85, duration: 0.377s, episode steps:  25, steps per second:  66, episode reward: 35.467, mean reward:  1.419 [-3.000, 32.340], mean action: 4.160 [0.000, 14.000],  loss: 0.021684, mae: 0.305228, mean_q: 0.524074, mean_eps: 0.000000
 2330/5000: episode: 86, duration: 0.677s, episode steps:  49, steps per second:  72, episode reward: 32.576, mean reward:  0.665 [-3.000, 31.985], mean action: 2.898 [0.000, 15.000],  loss: 0.018352, mae: 0.295516, mean_q: 0.504285, mean_eps: 0.000000
 2349/5000: episode: 87, duration: 0.377s, episode steps:  19, steps per second:  50, episode reward: 43.911, mean reward:  2.311 [-2.108, 31.691], mean action: 5.526 [1.000, 16.000],  loss: 0.020544, mae: 0.301859, mean_q: 0.473914, mean_eps: 0.000000
 2381/5000: episode: 88, duration: 0.559s, episode steps:  32, steps per second:  57, episode reward: 35.923, mean reward:  1.123 [-2.421, 32.260], mean action: 5.312 [0.000, 19.000],  loss: 0.023332, mae: 0.317234, mean_q: 0.548708, mean_eps: 0.000000
 2413/5000: episode: 89, duration: 0.459s, episode steps:  32, steps per second:  70, episode reward: 41.875, mean reward:  1.309 [-2.171, 32.520], mean action: 2.375 [0.000, 12.000],  loss: 0.025805, mae: 0.326962, mean_q: 0.530728, mean_eps: 0.000000
 2441/5000: episode: 90, duration: 0.411s, episode steps:  28, steps per second:  68, episode reward: 39.000, mean reward:  1.393 [-2.346, 32.010], mean action: 2.929 [0.000, 15.000],  loss: 0.021644, mae: 0.313183, mean_q: 0.563861, mean_eps: 0.000000
 2471/5000: episode: 91, duration: 0.434s, episode steps:  30, steps per second:  69, episode reward: 39.000, mean reward:  1.300 [-2.034, 32.030], mean action: 3.333 [0.000, 16.000],  loss: 0.021399, mae: 0.310702, mean_q: 0.519622, mean_eps: 0.000000
 2483/5000: episode: 92, duration: 0.198s, episode steps:  12, steps per second:  60, episode reward: 47.378, mean reward:  3.948 [-0.030, 32.072], mean action: 2.333 [0.000, 14.000],  loss: 0.017774, mae: 0.299301, mean_q: 0.559636, mean_eps: 0.000000
 2512/5000: episode: 93, duration: 0.431s, episode steps:  29, steps per second:  67, episode reward: 44.496, mean reward:  1.534 [-2.195, 32.404], mean action: 1.069 [0.000, 11.000],  loss: 0.017865, mae: 0.295738, mean_q: 0.550313, mean_eps: 0.000000
 2545/5000: episode: 94, duration: 0.476s, episode steps:  33, steps per second:  69, episode reward: 41.393, mean reward:  1.254 [-2.337, 32.290], mean action: 3.909 [1.000, 15.000],  loss: 0.021528, mae: 0.316701, mean_q: 0.527547, mean_eps: 0.000000
 2573/5000: episode: 95, duration: 0.406s, episode steps:  28, steps per second:  69, episode reward: 35.848, mean reward:  1.280 [-2.381, 32.028], mean action: 4.464 [0.000, 21.000],  loss: 0.021028, mae: 0.317504, mean_q: 0.543079, mean_eps: 0.000000
 2608/5000: episode: 96, duration: 0.488s, episode steps:  35, steps per second:  72, episode reward: 35.502, mean reward:  1.014 [-3.000, 31.819], mean action: 7.229 [0.000, 15.000],  loss: 0.023814, mae: 0.331079, mean_q: 0.506489, mean_eps: 0.000000
 2641/5000: episode: 97, duration: 0.474s, episode steps:  33, steps per second:  70, episode reward: 38.192, mean reward:  1.157 [-3.000, 32.340], mean action: 5.182 [0.000, 15.000],  loss: 0.018445, mae: 0.306400, mean_q: 0.505091, mean_eps: 0.000000
 2668/5000: episode: 98, duration: 0.385s, episode steps:  27, steps per second:  70, episode reward: 44.158, mean reward:  1.635 [-2.938, 32.080], mean action: 4.370 [0.000, 15.000],  loss: 0.021275, mae: 0.312893, mean_q: 0.512167, mean_eps: 0.000000
 2686/5000: episode: 99, duration: 0.276s, episode steps:  18, steps per second:  65, episode reward: 42.000, mean reward:  2.333 [-2.180, 32.170], mean action: 2.056 [0.000, 9.000],  loss: 0.022588, mae: 0.321874, mean_q: 0.524849, mean_eps: 0.000000
 2716/5000: episode: 100, duration: 0.438s, episode steps:  30, steps per second:  69, episode reward: 41.051, mean reward:  1.368 [-2.456, 31.991], mean action: 3.333 [0.000, 15.000],  loss: 0.022622, mae: 0.325660, mean_q: 0.552350, mean_eps: 0.000000
 2734/5000: episode: 101, duration: 0.268s, episode steps:  18, steps per second:  67, episode reward: 47.218, mean reward:  2.623 [ 0.000, 32.050], mean action: 4.000 [0.000, 15.000],  loss: 0.022146, mae: 0.320539, mean_q: 0.561458, mean_eps: 0.000000
 2778/5000: episode: 102, duration: 0.686s, episode steps:  44, steps per second:  64, episode reward: -33.000, mean reward: -0.750 [-32.053,  2.300], mean action: 6.273 [0.000, 20.000],  loss: 0.022088, mae: 0.328403, mean_q: 0.535431, mean_eps: 0.000000
 2813/5000: episode: 103, duration: 0.496s, episode steps:  35, steps per second:  71, episode reward: 32.434, mean reward:  0.927 [-2.910, 32.211], mean action: 5.114 [0.000, 19.000],  loss: 0.021387, mae: 0.315494, mean_q: 0.513632, mean_eps: 0.000000
 2844/5000: episode: 104, duration: 0.744s, episode steps:  31, steps per second:  42, episode reward: 32.508, mean reward:  1.049 [-3.000, 32.277], mean action: 5.774 [0.000, 19.000],  loss: 0.021226, mae: 0.314760, mean_q: 0.502010, mean_eps: 0.000000
 2873/5000: episode: 105, duration: 0.430s, episode steps:  29, steps per second:  67, episode reward: 41.399, mean reward:  1.428 [-2.575, 32.160], mean action: 3.241 [0.000, 14.000],  loss: 0.022115, mae: 0.321598, mean_q: 0.537782, mean_eps: 0.000000
 2912/5000: episode: 106, duration: 0.579s, episode steps:  39, steps per second:  67, episode reward: 42.560, mean reward:  1.091 [-2.370, 32.620], mean action: 4.538 [0.000, 14.000],  loss: 0.020554, mae: 0.314919, mean_q: 0.516435, mean_eps: 0.000000
 2940/5000: episode: 107, duration: 0.416s, episode steps:  28, steps per second:  67, episode reward: 37.789, mean reward:  1.350 [-2.621, 32.540], mean action: 3.786 [0.000, 14.000],  loss: 0.018800, mae: 0.302405, mean_q: 0.489617, mean_eps: 0.000000
 2971/5000: episode: 108, duration: 0.443s, episode steps:  31, steps per second:  70, episode reward: 44.583, mean reward:  1.438 [-2.359, 32.880], mean action: 4.097 [0.000, 14.000],  loss: 0.021592, mae: 0.323507, mean_q: 0.497713, mean_eps: 0.000000
 3004/5000: episode: 109, duration: 0.464s, episode steps:  33, steps per second:  71, episode reward: 42.000, mean reward:  1.273 [-2.354, 32.420], mean action: 5.152 [0.000, 20.000],  loss: 0.023889, mae: 0.336810, mean_q: 0.528185, mean_eps: 0.000000
 3033/5000: episode: 110, duration: 0.414s, episode steps:  29, steps per second:  70, episode reward: 41.053, mean reward:  1.416 [-2.210, 32.420], mean action: 2.897 [0.000, 20.000],  loss: 0.018062, mae: 0.306551, mean_q: 0.528740, mean_eps: 0.000000
 3056/5000: episode: 111, duration: 0.351s, episode steps:  23, steps per second:  65, episode reward: 41.904, mean reward:  1.822 [-2.505, 32.244], mean action: 1.957 [0.000, 11.000],  loss: 0.016046, mae: 0.294156, mean_q: 0.560191, mean_eps: 0.000000
 3079/5000: episode: 112, duration: 0.342s, episode steps:  23, steps per second:  67, episode reward: 43.947, mean reward:  1.911 [-2.550, 32.500], mean action: 2.870 [0.000, 15.000],  loss: 0.021845, mae: 0.320988, mean_q: 0.528955, mean_eps: 0.000000
 3096/5000: episode: 113, duration: 0.262s, episode steps:  17, steps per second:  65, episode reward: 44.232, mean reward:  2.602 [-2.136, 32.071], mean action: 5.471 [0.000, 15.000],  loss: 0.017986, mae: 0.309473, mean_q: 0.475296, mean_eps: 0.000000
 3123/5000: episode: 114, duration: 0.390s, episode steps:  27, steps per second:  69, episode reward: 39.000, mean reward:  1.444 [-3.000, 32.390], mean action: 3.815 [0.000, 16.000],  loss: 0.023614, mae: 0.330052, mean_q: 0.499435, mean_eps: 0.000000
 3146/5000: episode: 115, duration: 0.345s, episode steps:  23, steps per second:  67, episode reward: 41.251, mean reward:  1.794 [-2.638, 32.020], mean action: 3.217 [1.000, 15.000],  loss: 0.019930, mae: 0.311044, mean_q: 0.572327, mean_eps: 0.000000
 3174/5000: episode: 116, duration: 0.403s, episode steps:  28, steps per second:  69, episode reward: 44.770, mean reward:  1.599 [-2.518, 32.218], mean action: 1.286 [0.000, 11.000],  loss: 0.023611, mae: 0.323172, mean_q: 0.502049, mean_eps: 0.000000
 3207/5000: episode: 117, duration: 0.475s, episode steps:  33, steps per second:  69, episode reward: 32.876, mean reward:  0.996 [-2.629, 32.430], mean action: 4.576 [0.000, 16.000],  loss: 0.023724, mae: 0.325996, mean_q: 0.543714, mean_eps: 0.000000
 3232/5000: episode: 118, duration: 0.366s, episode steps:  25, steps per second:  68, episode reward: 38.517, mean reward:  1.541 [-2.403, 32.122], mean action: 4.160 [0.000, 16.000],  loss: 0.019978, mae: 0.306646, mean_q: 0.460025, mean_eps: 0.000000
 3249/5000: episode: 119, duration: 0.261s, episode steps:  17, steps per second:  65, episode reward: 44.460, mean reward:  2.615 [-2.242, 31.817], mean action: 2.824 [0.000, 16.000],  loss: 0.017230, mae: 0.297859, mean_q: 0.514408, mean_eps: 0.000000
 3276/5000: episode: 120, duration: 0.390s, episode steps:  27, steps per second:  69, episode reward: 41.177, mean reward:  1.525 [-2.464, 32.120], mean action: 4.074 [0.000, 16.000],  loss: 0.020481, mae: 0.304824, mean_q: 0.521612, mean_eps: 0.000000
 3297/5000: episode: 121, duration: 0.319s, episode steps:  21, steps per second:  66, episode reward: 44.313, mean reward:  2.110 [-2.038, 31.911], mean action: 2.238 [0.000, 15.000],  loss: 0.018719, mae: 0.296924, mean_q: 0.500936, mean_eps: 0.000000
 3325/5000: episode: 122, duration: 0.403s, episode steps:  28, steps per second:  69, episode reward: 38.645, mean reward:  1.380 [-3.000, 31.946], mean action: 6.464 [0.000, 20.000],  loss: 0.019106, mae: 0.300990, mean_q: 0.443082, mean_eps: 0.000000
 3350/5000: episode: 123, duration: 0.368s, episode steps:  25, steps per second:  68, episode reward: 39.000, mean reward:  1.560 [-2.880, 32.780], mean action: 4.000 [0.000, 19.000],  loss: 0.020019, mae: 0.299577, mean_q: 0.516054, mean_eps: 0.000000
 3376/5000: episode: 124, duration: 0.372s, episode steps:  26, steps per second:  70, episode reward: 41.015, mean reward:  1.577 [-2.517, 31.056], mean action: 3.077 [0.000, 16.000],  loss: 0.021506, mae: 0.311843, mean_q: 0.534961, mean_eps: 0.000000
 3412/5000: episode: 125, duration: 0.505s, episode steps:  36, steps per second:  71, episode reward: 40.215, mean reward:  1.117 [-2.176, 32.133], mean action: 2.194 [0.000, 16.000],  loss: 0.020184, mae: 0.308234, mean_q: 0.453533, mean_eps: 0.000000
 3439/5000: episode: 126, duration: 0.399s, episode steps:  27, steps per second:  68, episode reward: 44.462, mean reward:  1.647 [-2.109, 32.110], mean action: 2.333 [0.000, 9.000],  loss: 0.021429, mae: 0.315520, mean_q: 0.481639, mean_eps: 0.000000
 3460/5000: episode: 127, duration: 0.307s, episode steps:  21, steps per second:  68, episode reward: 41.764, mean reward:  1.989 [-2.692, 32.040], mean action: 3.381 [0.000, 12.000],  loss: 0.019157, mae: 0.306141, mean_q: 0.504403, mean_eps: 0.000000
 3484/5000: episode: 128, duration: 0.361s, episode steps:  24, steps per second:  67, episode reward: 39.000, mean reward:  1.625 [-3.000, 32.110], mean action: 2.333 [0.000, 9.000],  loss: 0.019601, mae: 0.304815, mean_q: 0.465273, mean_eps: 0.000000
 3514/5000: episode: 129, duration: 0.438s, episode steps:  30, steps per second:  69, episode reward: 41.938, mean reward:  1.398 [-2.152, 32.140], mean action: 2.600 [0.000, 16.000],  loss: 0.021714, mae: 0.321085, mean_q: 0.491223, mean_eps: 0.000000
 3543/5000: episode: 130, duration: 0.410s, episode steps:  29, steps per second:  71, episode reward: 40.478, mean reward:  1.396 [-2.358, 31.991], mean action: 2.138 [0.000, 16.000],  loss: 0.021334, mae: 0.314501, mean_q: 0.536066, mean_eps: 0.000000
 3570/5000: episode: 131, duration: 0.394s, episode steps:  27, steps per second:  69, episode reward: 41.824, mean reward:  1.549 [-2.371, 31.934], mean action: 2.370 [0.000, 16.000],  loss: 0.026021, mae: 0.337758, mean_q: 0.486344, mean_eps: 0.000000
 3604/5000: episode: 132, duration: 0.491s, episode steps:  34, steps per second:  69, episode reward: 33.000, mean reward:  0.971 [-3.000, 32.260], mean action: 4.294 [0.000, 19.000],  loss: 0.021646, mae: 0.320278, mean_q: 0.551954, mean_eps: 0.000000
 3624/5000: episode: 133, duration: 0.302s, episode steps:  20, steps per second:  66, episode reward: 39.000, mean reward:  1.950 [-2.578, 32.140], mean action: 3.750 [0.000, 19.000],  loss: 0.023581, mae: 0.323667, mean_q: 0.541040, mean_eps: 0.000000
 3654/5000: episode: 134, duration: 0.424s, episode steps:  30, steps per second:  71, episode reward: 35.722, mean reward:  1.191 [-3.000, 32.602], mean action: 2.933 [0.000, 19.000],  loss: 0.018690, mae: 0.303833, mean_q: 0.568856, mean_eps: 0.000000
 3669/5000: episode: 135, duration: 0.229s, episode steps:  15, steps per second:  66, episode reward: 44.047, mean reward:  2.936 [-2.226, 32.901], mean action: 2.667 [0.000, 19.000],  loss: 0.019989, mae: 0.309132, mean_q: 0.526041, mean_eps: 0.000000
 3689/5000: episode: 136, duration: 0.302s, episode steps:  20, steps per second:  66, episode reward: 44.316, mean reward:  2.216 [-2.047, 32.031], mean action: 3.600 [0.000, 19.000],  loss: 0.019606, mae: 0.315762, mean_q: 0.525225, mean_eps: 0.000000
 3719/5000: episode: 137, duration: 0.445s, episode steps:  30, steps per second:  67, episode reward: 45.000, mean reward:  1.500 [-2.586, 32.280], mean action: 2.500 [0.000, 19.000],  loss: 0.021462, mae: 0.319463, mean_q: 0.506402, mean_eps: 0.000000
 3746/5000: episode: 138, duration: 0.398s, episode steps:  27, steps per second:  68, episode reward: 41.900, mean reward:  1.552 [-2.419, 32.200], mean action: 2.185 [0.000, 19.000],  loss: 0.017559, mae: 0.305251, mean_q: 0.567935, mean_eps: 0.000000
 3768/5000: episode: 139, duration: 0.343s, episode steps:  22, steps per second:  64, episode reward: 41.544, mean reward:  1.888 [-2.668, 32.286], mean action: 1.455 [0.000, 12.000],  loss: 0.019558, mae: 0.318832, mean_q: 0.598788, mean_eps: 0.000000
 3789/5000: episode: 140, duration: 0.311s, episode steps:  21, steps per second:  68, episode reward: 41.844, mean reward:  1.993 [-2.318, 32.080], mean action: 2.190 [0.000, 11.000],  loss: 0.023215, mae: 0.331524, mean_q: 0.577769, mean_eps: 0.000000
 3820/5000: episode: 141, duration: 0.440s, episode steps:  31, steps per second:  70, episode reward: 39.000, mean reward:  1.258 [-2.671, 29.295], mean action: 4.290 [0.000, 19.000],  loss: 0.020453, mae: 0.320992, mean_q: 0.620139, mean_eps: 0.000000
 3855/5000: episode: 142, duration: 0.516s, episode steps:  35, steps per second:  68, episode reward: 41.006, mean reward:  1.172 [-2.279, 32.080], mean action: 4.286 [0.000, 19.000],  loss: 0.024814, mae: 0.338832, mean_q: 0.598323, mean_eps: 0.000000
 3877/5000: episode: 143, duration: 0.328s, episode steps:  22, steps per second:  67, episode reward: 38.835, mean reward:  1.765 [-3.000, 32.070], mean action: 6.318 [0.000, 19.000],  loss: 0.017374, mae: 0.304970, mean_q: 0.535758, mean_eps: 0.000000
 3899/5000: episode: 144, duration: 0.336s, episode steps:  22, steps per second:  65, episode reward: 44.726, mean reward:  2.033 [-2.153, 32.270], mean action: 3.227 [0.000, 19.000],  loss: 0.016795, mae: 0.304872, mean_q: 0.537566, mean_eps: 0.000000
 3926/5000: episode: 145, duration: 0.596s, episode steps:  27, steps per second:  45, episode reward: 38.036, mean reward:  1.409 [-3.000, 32.130], mean action: 5.148 [0.000, 19.000],  loss: 0.018032, mae: 0.307323, mean_q: 0.554233, mean_eps: 0.000000
 3948/5000: episode: 146, duration: 0.335s, episode steps:  22, steps per second:  66, episode reward: 41.487, mean reward:  1.886 [-2.584, 31.823], mean action: 1.727 [0.000, 16.000],  loss: 0.020810, mae: 0.320420, mean_q: 0.511144, mean_eps: 0.000000
 3981/5000: episode: 147, duration: 0.478s, episode steps:  33, steps per second:  69, episode reward: 40.349, mean reward:  1.223 [-3.000, 32.310], mean action: 3.303 [0.000, 19.000],  loss: 0.021076, mae: 0.322551, mean_q: 0.474281, mean_eps: 0.000000
 4005/5000: episode: 148, duration: 0.357s, episode steps:  24, steps per second:  67, episode reward: 45.969, mean reward:  1.915 [-0.700, 32.130], mean action: 3.417 [0.000, 16.000],  loss: 0.020502, mae: 0.311206, mean_q: 0.522631, mean_eps: 0.000000
 4035/5000: episode: 149, duration: 0.429s, episode steps:  30, steps per second:  70, episode reward: 35.560, mean reward:  1.185 [-3.000, 31.930], mean action: 2.633 [0.000, 19.000],  loss: 0.020638, mae: 0.312330, mean_q: 0.540381, mean_eps: 0.000000
 4054/5000: episode: 150, duration: 0.282s, episode steps:  19, steps per second:  67, episode reward: 41.680, mean reward:  2.194 [-2.720, 31.760], mean action: 2.842 [1.000, 15.000],  loss: 0.019493, mae: 0.310017, mean_q: 0.470434, mean_eps: 0.000000
 4078/5000: episode: 151, duration: 0.353s, episode steps:  24, steps per second:  68, episode reward: 38.473, mean reward:  1.603 [-3.000, 31.878], mean action: 3.292 [0.000, 11.000],  loss: 0.020755, mae: 0.322091, mean_q: 0.473219, mean_eps: 0.000000
 4108/5000: episode: 152, duration: 0.427s, episode steps:  30, steps per second:  70, episode reward: 40.797, mean reward:  1.360 [-2.510, 32.299], mean action: 1.167 [0.000, 3.000],  loss: 0.020994, mae: 0.317437, mean_q: 0.491554, mean_eps: 0.000000
 4125/5000: episode: 153, duration: 0.255s, episode steps:  17, steps per second:  67, episode reward: 44.190, mean reward:  2.599 [-2.124, 31.370], mean action: 2.000 [0.000, 11.000],  loss: 0.019163, mae: 0.315417, mean_q: 0.475148, mean_eps: 0.000000
 4160/5000: episode: 154, duration: 0.510s, episode steps:  35, steps per second:  69, episode reward: 38.579, mean reward:  1.102 [-2.327, 32.030], mean action: 2.943 [0.000, 15.000],  loss: 0.022571, mae: 0.327750, mean_q: 0.472856, mean_eps: 0.000000
 4198/5000: episode: 155, duration: 0.543s, episode steps:  38, steps per second:  70, episode reward: 41.241, mean reward:  1.085 [-3.000, 32.310], mean action: 3.263 [0.000, 16.000],  loss: 0.021857, mae: 0.323301, mean_q: 0.524320, mean_eps: 0.000000
 4241/5000: episode: 156, duration: 0.602s, episode steps:  43, steps per second:  71, episode reward: -34.840, mean reward: -0.810 [-32.508,  2.438], mean action: 5.256 [0.000, 20.000],  loss: 0.019522, mae: 0.315494, mean_q: 0.559066, mean_eps: 0.000000
 4278/5000: episode: 157, duration: 0.522s, episode steps:  37, steps per second:  71, episode reward: 36.000, mean reward:  0.973 [-3.000, 32.210], mean action: 4.514 [0.000, 21.000],  loss: 0.022066, mae: 0.325107, mean_q: 0.569261, mean_eps: 0.000000
 4307/5000: episode: 158, duration: 0.432s, episode steps:  29, steps per second:  67, episode reward: 46.359, mean reward:  1.599 [-0.309, 32.090], mean action: 3.276 [0.000, 14.000],  loss: 0.020806, mae: 0.315520, mean_q: 0.506680, mean_eps: 0.000000
 4327/5000: episode: 159, duration: 0.302s, episode steps:  20, steps per second:  66, episode reward: 44.504, mean reward:  2.225 [-2.055, 32.050], mean action: 4.000 [0.000, 16.000],  loss: 0.020517, mae: 0.309830, mean_q: 0.532809, mean_eps: 0.000000
 4353/5000: episode: 160, duration: 0.371s, episode steps:  26, steps per second:  70, episode reward: 42.000, mean reward:  1.615 [-2.222, 32.170], mean action: 3.192 [0.000, 16.000],  loss: 0.023831, mae: 0.325858, mean_q: 0.607572, mean_eps: 0.000000
 4368/5000: episode: 161, duration: 0.228s, episode steps:  15, steps per second:  66, episode reward: 47.704, mean reward:  3.180 [-0.369, 32.020], mean action: 2.267 [1.000, 3.000],  loss: 0.016000, mae: 0.295057, mean_q: 0.648168, mean_eps: 0.000000
 4412/5000: episode: 162, duration: 0.624s, episode steps:  44, steps per second:  71, episode reward: 38.831, mean reward:  0.883 [-2.701, 32.081], mean action: 3.205 [0.000, 16.000],  loss: 0.021062, mae: 0.316316, mean_q: 0.575602, mean_eps: 0.000000
 4439/5000: episode: 163, duration: 0.402s, episode steps:  27, steps per second:  67, episode reward: 41.577, mean reward:  1.540 [-2.272, 32.260], mean action: 3.852 [0.000, 20.000],  loss: 0.021314, mae: 0.309857, mean_q: 0.534951, mean_eps: 0.000000
 4466/5000: episode: 164, duration: 0.402s, episode steps:  27, steps per second:  67, episode reward: 40.336, mean reward:  1.494 [-2.659, 32.232], mean action: 4.000 [0.000, 20.000],  loss: 0.021659, mae: 0.311587, mean_q: 0.550236, mean_eps: 0.000000
 4480/5000: episode: 165, duration: 0.219s, episode steps:  14, steps per second:  64, episode reward: 47.450, mean reward:  3.389 [ 0.000, 33.000], mean action: 2.571 [1.000, 3.000],  loss: 0.021397, mae: 0.315247, mean_q: 0.554151, mean_eps: 0.000000
 4503/5000: episode: 166, duration: 0.355s, episode steps:  23, steps per second:  65, episode reward: 38.176, mean reward:  1.660 [-2.725, 31.286], mean action: 1.478 [0.000, 19.000],  loss: 0.023323, mae: 0.323548, mean_q: 0.553967, mean_eps: 0.000000
 4528/5000: episode: 167, duration: 0.438s, episode steps:  25, steps per second:  57, episode reward: 44.282, mean reward:  1.771 [-2.197, 32.430], mean action: 3.200 [0.000, 14.000],  loss: 0.021200, mae: 0.320793, mean_q: 0.583559, mean_eps: 0.000000
 4556/5000: episode: 168, duration: 0.409s, episode steps:  28, steps per second:  68, episode reward: 41.802, mean reward:  1.493 [-2.136, 32.121], mean action: 3.536 [1.000, 18.000],  loss: 0.020120, mae: 0.312482, mean_q: 0.544821, mean_eps: 0.000000
 4574/5000: episode: 169, duration: 0.271s, episode steps:  18, steps per second:  66, episode reward: 47.109, mean reward:  2.617 [ 0.000, 32.679], mean action: 1.278 [0.000, 7.000],  loss: 0.023010, mae: 0.322098, mean_q: 0.531936, mean_eps: 0.000000
 4610/5000: episode: 170, duration: 0.510s, episode steps:  36, steps per second:  71, episode reward: 41.569, mean reward:  1.155 [-2.252, 32.230], mean action: 2.861 [0.000, 16.000],  loss: 0.019480, mae: 0.314426, mean_q: 0.532342, mean_eps: 0.000000
 4639/5000: episode: 171, duration: 0.416s, episode steps:  29, steps per second:  70, episode reward: 37.165, mean reward:  1.282 [-2.397, 32.140], mean action: 5.207 [0.000, 20.000],  loss: 0.023014, mae: 0.331349, mean_q: 0.551021, mean_eps: 0.000000
 4675/5000: episode: 172, duration: 0.524s, episode steps:  36, steps per second:  69, episode reward: 41.148, mean reward:  1.143 [-2.447, 32.210], mean action: 4.472 [0.000, 16.000],  loss: 0.021743, mae: 0.325082, mean_q: 0.575211, mean_eps: 0.000000
 4705/5000: episode: 173, duration: 0.429s, episode steps:  30, steps per second:  70, episode reward: 41.804, mean reward:  1.393 [-2.242, 32.300], mean action: 4.533 [0.000, 16.000],  loss: 0.019535, mae: 0.312966, mean_q: 0.520827, mean_eps: 0.000000
 4731/5000: episode: 174, duration: 0.387s, episode steps:  26, steps per second:  67, episode reward: 40.742, mean reward:  1.567 [-3.000, 32.300], mean action: 2.962 [0.000, 20.000],  loss: 0.020878, mae: 0.316391, mean_q: 0.603610, mean_eps: 0.000000
 4764/5000: episode: 175, duration: 0.465s, episode steps:  33, steps per second:  71, episode reward: 38.027, mean reward:  1.152 [-2.654, 32.160], mean action: 4.485 [0.000, 16.000],  loss: 0.021470, mae: 0.323728, mean_q: 0.661222, mean_eps: 0.000000
 4786/5000: episode: 176, duration: 0.328s, episode steps:  22, steps per second:  67, episode reward: 44.718, mean reward:  2.033 [-2.535, 31.838], mean action: 1.000 [0.000, 16.000],  loss: 0.025580, mae: 0.338688, mean_q: 0.639265, mean_eps: 0.000000
 4814/5000: episode: 177, duration: 0.407s, episode steps:  28, steps per second:  69, episode reward: 38.657, mean reward:  1.381 [-2.297, 31.707], mean action: 3.607 [0.000, 16.000],  loss: 0.021550, mae: 0.324862, mean_q: 0.633210, mean_eps: 0.000000
 4825/5000: episode: 178, duration: 0.184s, episode steps:  11, steps per second:  60, episode reward: 48.000, mean reward:  4.364 [ 0.000, 33.000], mean action: 1.364 [0.000, 3.000],  loss: 0.018726, mae: 0.299366, mean_q: 0.550331, mean_eps: 0.000000
 4837/5000: episode: 179, duration: 0.196s, episode steps:  12, steps per second:  61, episode reward: 46.546, mean reward:  3.879 [-0.295, 32.842], mean action: 3.167 [2.000, 5.000],  loss: 0.026965, mae: 0.341531, mean_q: 0.604276, mean_eps: 0.000000
 4866/5000: episode: 180, duration: 0.420s, episode steps:  29, steps per second:  69, episode reward: 32.341, mean reward:  1.115 [-3.000, 31.811], mean action: 4.724 [0.000, 19.000],  loss: 0.017942, mae: 0.303476, mean_q: 0.577911, mean_eps: 0.000000
 4879/5000: episode: 181, duration: 0.203s, episode steps:  13, steps per second:  64, episode reward: 44.730, mean reward:  3.441 [-2.284, 32.463], mean action: 1.462 [0.000, 16.000],  loss: 0.018365, mae: 0.294563, mean_q: 0.500718, mean_eps: 0.000000
 4913/5000: episode: 182, duration: 0.482s, episode steps:  34, steps per second:  70, episode reward: 38.801, mean reward:  1.141 [-3.000, 32.210], mean action: 3.853 [0.000, 19.000],  loss: 0.021176, mae: 0.312479, mean_q: 0.516765, mean_eps: 0.000000
 4933/5000: episode: 183, duration: 0.308s, episode steps:  20, steps per second:  65, episode reward: 42.000, mean reward:  2.100 [-2.240, 32.290], mean action: 3.450 [0.000, 16.000],  loss: 0.022199, mae: 0.321287, mean_q: 0.522616, mean_eps: 0.000000
 4962/5000: episode: 184, duration: 0.417s, episode steps:  29, steps per second:  69, episode reward: 42.000, mean reward:  1.448 [-2.256, 32.530], mean action: 3.724 [2.000, 14.000],  loss: 0.022417, mae: 0.326416, mean_q: 0.571979, mean_eps: 0.000000
 4984/5000: episode: 185, duration: 0.323s, episode steps:  22, steps per second:  68, episode reward: 38.910, mean reward:  1.769 [-2.900, 32.117], mean action: 4.000 [0.000, 19.000],  loss: 0.023321, mae: 0.332248, mean_q: 0.515414, mean_eps: 0.000000
 5000/5000: episode: 186, duration: 0.244s, episode steps:  16, steps per second:  66, episode reward: 46.377, mean reward:  2.899 [-0.703, 32.070], mean action: 2.812 [0.000, 14.000],  loss: 0.022549, mae: 0.321288, mean_q: 0.581684, mean_eps: 0.000000
done, took 73.733 seconds
DQN Evaluation: 11437 victories out of 13332 episodes
Training for 5000 steps ...
   30/5000: episode: 1, duration: 0.281s, episode steps:  30, steps per second: 107, episode reward: 32.577, mean reward:  1.086 [-2.560, 32.390], mean action: 4.700 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   50/5000: episode: 2, duration: 0.164s, episode steps:  20, steps per second: 122, episode reward: 41.167, mean reward:  2.058 [-3.000, 32.047], mean action: 5.000 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   68/5000: episode: 3, duration: 0.138s, episode steps:  18, steps per second: 130, episode reward: 38.125, mean reward:  2.118 [-2.712, 32.330], mean action: 3.833 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   81/5000: episode: 4, duration: 0.112s, episode steps:  13, steps per second: 116, episode reward: 41.721, mean reward:  3.209 [-2.483, 32.721], mean action: 3.846 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   98/5000: episode: 5, duration: 0.152s, episode steps:  17, steps per second: 112, episode reward: 38.164, mean reward:  2.245 [-2.532, 32.263], mean action: 3.647 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  116/5000: episode: 6, duration: 0.130s, episode steps:  18, steps per second: 138, episode reward: 38.370, mean reward:  2.132 [-3.000, 32.620], mean action: 5.944 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  137/5000: episode: 7, duration: 0.152s, episode steps:  21, steps per second: 138, episode reward: 35.472, mean reward:  1.689 [-3.000, 32.382], mean action: 3.619 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  156/5000: episode: 8, duration: 0.141s, episode steps:  19, steps per second: 135, episode reward: 44.599, mean reward:  2.347 [-2.742, 32.660], mean action: 4.474 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  175/5000: episode: 9, duration: 0.142s, episode steps:  19, steps per second: 134, episode reward: 37.962, mean reward:  1.998 [-2.402, 32.023], mean action: 5.895 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  191/5000: episode: 10, duration: 0.120s, episode steps:  16, steps per second: 133, episode reward: 38.804, mean reward:  2.425 [-2.916, 32.310], mean action: 3.562 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  219/5000: episode: 11, duration: 0.189s, episode steps:  28, steps per second: 148, episode reward: -32.940, mean reward: -1.176 [-32.373,  2.833], mean action: 7.071 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  236/5000: episode: 12, duration: 0.131s, episode steps:  17, steps per second: 130, episode reward: 44.508, mean reward:  2.618 [-2.516, 32.182], mean action: 2.471 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  269/5000: episode: 13, duration: 0.207s, episode steps:  33, steps per second: 160, episode reward: -35.230, mean reward: -1.068 [-31.659,  2.203], mean action: 5.848 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  288/5000: episode: 14, duration: 0.153s, episode steps:  19, steps per second: 124, episode reward: 44.278, mean reward:  2.330 [-2.273, 32.163], mean action: 2.526 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  309/5000: episode: 15, duration: 0.143s, episode steps:  21, steps per second: 147, episode reward: -36.000, mean reward: -1.714 [-32.295,  2.260], mean action: 5.381 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  327/5000: episode: 16, duration: 0.129s, episode steps:  18, steps per second: 139, episode reward: 39.000, mean reward:  2.167 [-2.311, 32.320], mean action: 6.222 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  357/5000: episode: 17, duration: 0.203s, episode steps:  30, steps per second: 148, episode reward: 41.127, mean reward:  1.371 [-2.296, 31.939], mean action: 3.600 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  375/5000: episode: 18, duration: 0.123s, episode steps:  18, steps per second: 146, episode reward: -35.310, mean reward: -1.962 [-32.089,  2.813], mean action: 7.278 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  389/5000: episode: 19, duration: 0.100s, episode steps:  14, steps per second: 140, episode reward: -47.280, mean reward: -3.377 [-33.000,  0.540], mean action: 8.571 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  409/5000: episode: 20, duration: 0.143s, episode steps:  20, steps per second: 140, episode reward: 38.178, mean reward:  1.909 [-3.000, 31.997], mean action: 3.500 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  437/5000: episode: 21, duration: 0.182s, episode steps:  28, steps per second: 154, episode reward: 32.379, mean reward:  1.156 [-2.442, 32.350], mean action: 10.000 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  455/5000: episode: 22, duration: 0.149s, episode steps:  18, steps per second: 121, episode reward: 35.220, mean reward:  1.957 [-3.000, 31.830], mean action: 6.833 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  481/5000: episode: 23, duration: 0.182s, episode steps:  26, steps per second: 143, episode reward: 32.624, mean reward:  1.255 [-3.000, 31.986], mean action: 5.500 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  500/5000: episode: 24, duration: 0.139s, episode steps:  19, steps per second: 136, episode reward: 41.649, mean reward:  2.192 [-2.583, 32.317], mean action: 3.158 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  524/5000: episode: 25, duration: 0.162s, episode steps:  24, steps per second: 148, episode reward: 32.918, mean reward:  1.372 [-3.000, 32.270], mean action: 4.583 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  543/5000: episode: 26, duration: 0.133s, episode steps:  19, steps per second: 143, episode reward: -35.630, mean reward: -1.875 [-32.214,  2.852], mean action: 7.684 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  560/5000: episode: 27, duration: 0.125s, episode steps:  17, steps per second: 136, episode reward: 38.084, mean reward:  2.240 [-2.530, 32.858], mean action: 3.765 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  586/5000: episode: 28, duration: 0.172s, episode steps:  26, steps per second: 151, episode reward: -32.100, mean reward: -1.235 [-32.290,  2.950], mean action: 7.654 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  619/5000: episode: 29, duration: 0.211s, episode steps:  33, steps per second: 157, episode reward: 36.000, mean reward:  1.091 [-3.000, 32.190], mean action: 8.364 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  638/5000: episode: 30, duration: 0.136s, episode steps:  19, steps per second: 139, episode reward: 38.575, mean reward:  2.030 [-2.603, 32.200], mean action: 5.368 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  664/5000: episode: 31, duration: 0.185s, episode steps:  26, steps per second: 140, episode reward: 38.153, mean reward:  1.467 [-2.454, 31.945], mean action: 3.500 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  689/5000: episode: 32, duration: 0.173s, episode steps:  25, steps per second: 145, episode reward: 35.223, mean reward:  1.409 [-2.595, 32.420], mean action: 7.360 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  707/5000: episode: 33, duration: 0.129s, episode steps:  18, steps per second: 139, episode reward: 41.124, mean reward:  2.285 [-2.670, 32.170], mean action: 4.056 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  741/5000: episode: 34, duration: 0.221s, episode steps:  34, steps per second: 154, episode reward: -32.710, mean reward: -0.962 [-32.900,  2.155], mean action: 5.912 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  769/5000: episode: 35, duration: 0.179s, episode steps:  28, steps per second: 157, episode reward: -35.190, mean reward: -1.257 [-33.000,  2.380], mean action: 4.000 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  805/5000: episode: 36, duration: 0.232s, episode steps:  36, steps per second: 155, episode reward: 32.400, mean reward:  0.900 [-2.323, 32.570], mean action: 7.056 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  820/5000: episode: 37, duration: 0.231s, episode steps:  15, steps per second:  65, episode reward: 41.688, mean reward:  2.779 [-2.309, 32.294], mean action: 4.000 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  846/5000: episode: 38, duration: 0.388s, episode steps:  26, steps per second:  67, episode reward: -35.250, mean reward: -1.356 [-32.389,  2.284], mean action: 5.115 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  863/5000: episode: 39, duration: 0.136s, episode steps:  17, steps per second: 125, episode reward: 38.343, mean reward:  2.255 [-3.000, 32.030], mean action: 3.353 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  884/5000: episode: 40, duration: 0.149s, episode steps:  21, steps per second: 141, episode reward: 38.280, mean reward:  1.823 [-2.207, 32.245], mean action: 3.333 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  908/5000: episode: 41, duration: 0.170s, episode steps:  24, steps per second: 142, episode reward: 35.220, mean reward:  1.467 [-2.306, 32.760], mean action: 5.125 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  932/5000: episode: 42, duration: 0.162s, episode steps:  24, steps per second: 148, episode reward: 38.903, mean reward:  1.621 [-3.000, 32.903], mean action: 3.542 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  944/5000: episode: 43, duration: 0.094s, episode steps:  12, steps per second: 128, episode reward: 41.636, mean reward:  3.470 [-2.461, 33.000], mean action: 4.000 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  974/5000: episode: 44, duration: 0.192s, episode steps:  30, steps per second: 156, episode reward: 32.558, mean reward:  1.085 [-2.570, 31.718], mean action: 3.833 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  997/5000: episode: 45, duration: 0.166s, episode steps:  23, steps per second: 138, episode reward: -30.000, mean reward: -1.304 [-30.111,  2.433], mean action: 6.739 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1027/5000: episode: 46, duration: 0.441s, episode steps:  30, steps per second:  68, episode reward: 32.237, mean reward:  1.075 [-2.507, 32.180], mean action: 5.600 [0.000, 19.000],  loss: 0.021994, mae: 0.317912, mean_q: 0.616379, mean_eps: 0.000000
 1057/5000: episode: 47, duration: 0.435s, episode steps:  30, steps per second:  69, episode reward: 35.812, mean reward:  1.194 [-2.351, 32.130], mean action: 4.733 [0.000, 19.000],  loss: 0.020165, mae: 0.309981, mean_q: 0.552894, mean_eps: 0.000000
 1068/5000: episode: 48, duration: 0.188s, episode steps:  11, steps per second:  58, episode reward: 44.440, mean reward:  4.040 [-2.642, 32.620], mean action: 4.455 [2.000, 14.000],  loss: 0.020858, mae: 0.313334, mean_q: 0.455699, mean_eps: 0.000000
 1094/5000: episode: 49, duration: 0.403s, episode steps:  26, steps per second:  65, episode reward: 32.049, mean reward:  1.233 [-2.535, 31.159], mean action: 4.115 [1.000, 14.000],  loss: 0.019233, mae: 0.303956, mean_q: 0.474431, mean_eps: 0.000000
 1113/5000: episode: 50, duration: 0.286s, episode steps:  19, steps per second:  67, episode reward: -35.520, mean reward: -1.869 [-32.349,  2.353], mean action: 5.211 [0.000, 9.000],  loss: 0.016979, mae: 0.297756, mean_q: 0.562790, mean_eps: 0.000000
 1136/5000: episode: 51, duration: 0.341s, episode steps:  23, steps per second:  67, episode reward: 35.315, mean reward:  1.535 [-3.000, 31.853], mean action: 4.652 [0.000, 19.000],  loss: 0.021640, mae: 0.319154, mean_q: 0.516362, mean_eps: 0.000000
 1159/5000: episode: 52, duration: 0.337s, episode steps:  23, steps per second:  68, episode reward: -32.560, mean reward: -1.416 [-31.867,  3.000], mean action: 6.870 [0.000, 19.000],  loss: 0.018960, mae: 0.308691, mean_q: 0.493099, mean_eps: 0.000000
 1174/5000: episode: 53, duration: 0.242s, episode steps:  15, steps per second:  62, episode reward: 41.036, mean reward:  2.736 [-2.230, 32.350], mean action: 2.600 [0.000, 11.000],  loss: 0.026269, mae: 0.361324, mean_q: 0.513134, mean_eps: 0.000000
 1224/5000: episode: 54, duration: 0.703s, episode steps:  50, steps per second:  71, episode reward: 38.550, mean reward:  0.771 [-2.996, 32.560], mean action: 2.920 [0.000, 19.000],  loss: 0.020735, mae: 0.326515, mean_q: 0.537515, mean_eps: 0.000000
 1243/5000: episode: 55, duration: 0.302s, episode steps:  19, steps per second:  63, episode reward: 38.027, mean reward:  2.001 [-2.433, 31.600], mean action: 4.474 [0.000, 19.000],  loss: 0.024028, mae: 0.345144, mean_q: 0.548890, mean_eps: 0.000000
 1263/5000: episode: 56, duration: 0.295s, episode steps:  20, steps per second:  68, episode reward: -32.300, mean reward: -1.615 [-32.775,  3.000], mean action: 7.650 [0.000, 19.000],  loss: 0.020555, mae: 0.334398, mean_q: 0.575054, mean_eps: 0.000000
 1285/5000: episode: 57, duration: 0.319s, episode steps:  22, steps per second:  69, episode reward: 35.628, mean reward:  1.619 [-3.000, 32.350], mean action: 4.000 [0.000, 19.000],  loss: 0.020564, mae: 0.320559, mean_q: 0.606946, mean_eps: 0.000000
 1310/5000: episode: 58, duration: 0.365s, episode steps:  25, steps per second:  68, episode reward: 36.000, mean reward:  1.440 [-3.000, 32.340], mean action: 4.560 [0.000, 19.000],  loss: 0.023117, mae: 0.333205, mean_q: 0.549557, mean_eps: 0.000000
 1330/5000: episode: 59, duration: 0.298s, episode steps:  20, steps per second:  67, episode reward: -32.310, mean reward: -1.616 [-32.079,  3.031], mean action: 8.650 [0.000, 19.000],  loss: 0.024738, mae: 0.345291, mean_q: 0.558289, mean_eps: 0.000000
 1351/5000: episode: 60, duration: 0.312s, episode steps:  21, steps per second:  67, episode reward: 40.883, mean reward:  1.947 [-2.284, 32.810], mean action: 5.190 [0.000, 20.000],  loss: 0.021889, mae: 0.336843, mean_q: 0.543961, mean_eps: 0.000000
 1371/5000: episode: 61, duration: 0.301s, episode steps:  20, steps per second:  66, episode reward: 37.129, mean reward:  1.856 [-2.244, 32.190], mean action: 6.700 [0.000, 19.000],  loss: 0.023491, mae: 0.335849, mean_q: 0.493045, mean_eps: 0.000000
 1398/5000: episode: 62, duration: 0.395s, episode steps:  27, steps per second:  68, episode reward: 35.457, mean reward:  1.313 [-2.322, 32.310], mean action: 6.926 [0.000, 20.000],  loss: 0.018359, mae: 0.304699, mean_q: 0.480299, mean_eps: 0.000000
 1419/5000: episode: 63, duration: 0.311s, episode steps:  21, steps per second:  68, episode reward: -35.900, mean reward: -1.710 [-32.392,  2.220], mean action: 7.667 [0.000, 19.000],  loss: 0.019635, mae: 0.312966, mean_q: 0.515566, mean_eps: 0.000000
 1449/5000: episode: 64, duration: 0.429s, episode steps:  30, steps per second:  70, episode reward: 35.137, mean reward:  1.171 [-2.903, 32.230], mean action: 6.000 [0.000, 20.000],  loss: 0.017555, mae: 0.298764, mean_q: 0.479728, mean_eps: 0.000000
 1474/5000: episode: 65, duration: 0.363s, episode steps:  25, steps per second:  69, episode reward: 33.000, mean reward:  1.320 [-3.000, 32.070], mean action: 8.440 [1.000, 20.000],  loss: 0.022194, mae: 0.321023, mean_q: 0.511196, mean_eps: 0.000000
 1491/5000: episode: 66, duration: 0.283s, episode steps:  17, steps per second:  60, episode reward: 41.071, mean reward:  2.416 [-3.000, 32.086], mean action: 4.647 [0.000, 20.000],  loss: 0.017761, mae: 0.302294, mean_q: 0.525934, mean_eps: 0.000000
 1511/5000: episode: 67, duration: 0.308s, episode steps:  20, steps per second:  65, episode reward: 45.000, mean reward:  2.250 [-2.226, 33.000], mean action: 2.100 [0.000, 3.000],  loss: 0.019686, mae: 0.317503, mean_q: 0.521260, mean_eps: 0.000000
 1541/5000: episode: 68, duration: 0.429s, episode steps:  30, steps per second:  70, episode reward: 32.494, mean reward:  1.083 [-3.000, 33.000], mean action: 9.567 [0.000, 20.000],  loss: 0.022755, mae: 0.331599, mean_q: 0.528128, mean_eps: 0.000000
 1556/5000: episode: 69, duration: 0.242s, episode steps:  15, steps per second:  62, episode reward: 38.267, mean reward:  2.551 [-2.533, 32.645], mean action: 7.333 [0.000, 20.000],  loss: 0.015158, mae: 0.297178, mean_q: 0.545862, mean_eps: 0.000000
 1576/5000: episode: 70, duration: 0.303s, episode steps:  20, steps per second:  66, episode reward: 35.114, mean reward:  1.756 [-3.000, 32.200], mean action: 5.400 [0.000, 19.000],  loss: 0.022823, mae: 0.330776, mean_q: 0.570031, mean_eps: 0.000000
 1592/5000: episode: 71, duration: 0.246s, episode steps:  16, steps per second:  65, episode reward: 38.255, mean reward:  2.391 [-2.452, 31.715], mean action: 8.000 [0.000, 20.000],  loss: 0.024472, mae: 0.340000, mean_q: 0.548021, mean_eps: 0.000000
 1610/5000: episode: 72, duration: 0.264s, episode steps:  18, steps per second:  68, episode reward: 35.630, mean reward:  1.979 [-3.000, 32.320], mean action: 3.944 [0.000, 19.000],  loss: 0.018721, mae: 0.316535, mean_q: 0.574459, mean_eps: 0.000000
 1633/5000: episode: 73, duration: 0.333s, episode steps:  23, steps per second:  69, episode reward: 38.491, mean reward:  1.674 [-2.400, 32.470], mean action: 3.217 [0.000, 12.000],  loss: 0.017085, mae: 0.308420, mean_q: 0.589280, mean_eps: 0.000000
 1654/5000: episode: 74, duration: 0.316s, episode steps:  21, steps per second:  67, episode reward: 38.425, mean reward:  1.830 [-2.069, 32.346], mean action: 3.429 [0.000, 12.000],  loss: 0.020765, mae: 0.327790, mean_q: 0.568292, mean_eps: 0.000000
 1685/5000: episode: 75, duration: 0.442s, episode steps:  31, steps per second:  70, episode reward: 32.983, mean reward:  1.064 [-2.493, 32.160], mean action: 6.548 [1.000, 12.000],  loss: 0.021999, mae: 0.332198, mean_q: 0.561924, mean_eps: 0.000000
 1704/5000: episode: 76, duration: 0.331s, episode steps:  19, steps per second:  57, episode reward: 39.000, mean reward:  2.053 [-2.477, 32.250], mean action: 2.421 [0.000, 12.000],  loss: 0.021826, mae: 0.327576, mean_q: 0.597763, mean_eps: 0.000000
 1724/5000: episode: 77, duration: 0.295s, episode steps:  20, steps per second:  68, episode reward: 32.752, mean reward:  1.638 [-2.497, 32.262], mean action: 3.700 [0.000, 12.000],  loss: 0.022503, mae: 0.324305, mean_q: 0.559583, mean_eps: 0.000000
 1745/5000: episode: 78, duration: 0.337s, episode steps:  21, steps per second:  62, episode reward: 36.000, mean reward:  1.714 [-3.000, 33.000], mean action: 3.952 [0.000, 16.000],  loss: 0.022182, mae: 0.325514, mean_q: 0.561788, mean_eps: 0.000000
 1773/5000: episode: 79, duration: 0.403s, episode steps:  28, steps per second:  69, episode reward: 42.000, mean reward:  1.500 [-2.384, 32.120], mean action: 3.786 [1.000, 16.000],  loss: 0.024459, mae: 0.337138, mean_q: 0.543999, mean_eps: 0.000000
 1790/5000: episode: 80, duration: 0.256s, episode steps:  17, steps per second:  66, episode reward: 36.000, mean reward:  2.118 [-2.718, 32.200], mean action: 4.765 [0.000, 16.000],  loss: 0.024548, mae: 0.328442, mean_q: 0.523009, mean_eps: 0.000000
 1811/5000: episode: 81, duration: 0.308s, episode steps:  21, steps per second:  68, episode reward: 38.221, mean reward:  1.820 [-2.714, 31.716], mean action: 3.714 [0.000, 16.000],  loss: 0.021394, mae: 0.316370, mean_q: 0.481109, mean_eps: 0.000000
 1843/5000: episode: 82, duration: 0.462s, episode steps:  32, steps per second:  69, episode reward: 40.876, mean reward:  1.277 [-2.290, 31.881], mean action: 4.531 [0.000, 13.000],  loss: 0.019513, mae: 0.306010, mean_q: 0.506072, mean_eps: 0.000000
 1864/5000: episode: 83, duration: 0.302s, episode steps:  21, steps per second:  70, episode reward: 35.126, mean reward:  1.673 [-2.327, 32.903], mean action: 4.238 [1.000, 12.000],  loss: 0.018372, mae: 0.299655, mean_q: 0.506441, mean_eps: 0.000000
 1887/5000: episode: 84, duration: 0.346s, episode steps:  23, steps per second:  67, episode reward: 41.985, mean reward:  1.825 [-3.000, 32.220], mean action: 4.522 [0.000, 13.000],  loss: 0.023083, mae: 0.327040, mean_q: 0.504554, mean_eps: 0.000000
 1913/5000: episode: 85, duration: 0.382s, episode steps:  26, steps per second:  68, episode reward: 40.210, mean reward:  1.547 [-2.557, 32.130], mean action: 4.885 [0.000, 13.000],  loss: 0.022979, mae: 0.320533, mean_q: 0.524005, mean_eps: 0.000000
 1935/5000: episode: 86, duration: 0.321s, episode steps:  22, steps per second:  69, episode reward: 35.428, mean reward:  1.610 [-2.900, 33.000], mean action: 5.091 [0.000, 12.000],  loss: 0.021486, mae: 0.323683, mean_q: 0.464653, mean_eps: 0.000000
 1957/5000: episode: 87, duration: 0.337s, episode steps:  22, steps per second:  65, episode reward: 39.817, mean reward:  1.810 [-2.397, 31.937], mean action: 4.455 [0.000, 12.000],  loss: 0.023621, mae: 0.336612, mean_q: 0.539341, mean_eps: 0.000000
 1971/5000: episode: 88, duration: 0.225s, episode steps:  14, steps per second:  62, episode reward: 42.000, mean reward:  3.000 [-2.303, 33.000], mean action: 2.929 [0.000, 11.000],  loss: 0.018732, mae: 0.307432, mean_q: 0.556807, mean_eps: 0.000000
 1993/5000: episode: 89, duration: 0.331s, episode steps:  22, steps per second:  66, episode reward: 41.386, mean reward:  1.881 [-2.188, 32.050], mean action: 2.045 [0.000, 11.000],  loss: 0.020886, mae: 0.315062, mean_q: 0.530642, mean_eps: 0.000000
 2009/5000: episode: 90, duration: 0.237s, episode steps:  16, steps per second:  67, episode reward: 38.168, mean reward:  2.385 [-2.414, 31.771], mean action: 4.562 [0.000, 18.000],  loss: 0.020380, mae: 0.311981, mean_q: 0.530036, mean_eps: 0.000000
 2032/5000: episode: 91, duration: 0.329s, episode steps:  23, steps per second:  70, episode reward: 35.760, mean reward:  1.555 [-3.000, 32.310], mean action: 3.435 [0.000, 15.000],  loss: 0.020166, mae: 0.313180, mean_q: 0.533181, mean_eps: 0.000000
 2048/5000: episode: 92, duration: 0.240s, episode steps:  16, steps per second:  67, episode reward: 38.833, mean reward:  2.427 [-2.529, 32.233], mean action: 3.875 [0.000, 15.000],  loss: 0.020910, mae: 0.320952, mean_q: 0.564448, mean_eps: 0.000000
 2063/5000: episode: 93, duration: 0.251s, episode steps:  15, steps per second:  60, episode reward: 43.663, mean reward:  2.911 [-2.423, 32.891], mean action: 5.400 [0.000, 18.000],  loss: 0.024346, mae: 0.332291, mean_q: 0.551215, mean_eps: 0.000000
 2101/5000: episode: 94, duration: 0.545s, episode steps:  38, steps per second:  70, episode reward: 34.756, mean reward:  0.915 [-2.395, 31.342], mean action: 7.395 [1.000, 15.000],  loss: 0.020134, mae: 0.310015, mean_q: 0.557005, mean_eps: 0.000000
 2120/5000: episode: 95, duration: 0.275s, episode steps:  19, steps per second:  69, episode reward: 35.007, mean reward:  1.842 [-2.625, 32.900], mean action: 5.895 [0.000, 20.000],  loss: 0.024384, mae: 0.328491, mean_q: 0.589729, mean_eps: 0.000000
 2137/5000: episode: 96, duration: 0.251s, episode steps:  17, steps per second:  68, episode reward: 35.830, mean reward:  2.108 [-2.704, 32.100], mean action: 4.941 [0.000, 19.000],  loss: 0.017778, mae: 0.303339, mean_q: 0.618383, mean_eps: 0.000000
 2162/5000: episode: 97, duration: 0.361s, episode steps:  25, steps per second:  69, episode reward: 38.700, mean reward:  1.548 [-2.155, 32.710], mean action: 4.520 [1.000, 16.000],  loss: 0.021399, mae: 0.319158, mean_q: 0.579163, mean_eps: 0.000000
 2189/5000: episode: 98, duration: 0.399s, episode steps:  27, steps per second:  68, episode reward: 32.434, mean reward:  1.201 [-2.860, 32.150], mean action: 7.000 [0.000, 15.000],  loss: 0.021201, mae: 0.320836, mean_q: 0.575943, mean_eps: 0.000000
 2223/5000: episode: 99, duration: 0.514s, episode steps:  34, steps per second:  66, episode reward: 32.445, mean reward:  0.954 [-2.904, 32.463], mean action: 4.382 [0.000, 15.000],  loss: 0.021686, mae: 0.320346, mean_q: 0.595851, mean_eps: 0.000000
 2241/5000: episode: 100, duration: 0.271s, episode steps:  18, steps per second:  66, episode reward: 38.404, mean reward:  2.134 [-2.497, 31.920], mean action: 3.556 [0.000, 15.000],  loss: 0.024078, mae: 0.322708, mean_q: 0.547138, mean_eps: 0.000000
 2268/5000: episode: 101, duration: 0.390s, episode steps:  27, steps per second:  69, episode reward: -39.000, mean reward: -1.444 [-32.472,  2.360], mean action: 5.815 [0.000, 15.000],  loss: 0.021901, mae: 0.323313, mean_q: 0.546898, mean_eps: 0.000000
 2285/5000: episode: 102, duration: 0.257s, episode steps:  17, steps per second:  66, episode reward: 47.255, mean reward:  2.780 [-0.123, 32.237], mean action: 3.118 [1.000, 14.000],  loss: 0.018208, mae: 0.303961, mean_q: 0.492337, mean_eps: 0.000000
 2302/5000: episode: 103, duration: 0.262s, episode steps:  17, steps per second:  65, episode reward: 41.625, mean reward:  2.449 [-2.203, 31.655], mean action: 2.706 [0.000, 15.000],  loss: 0.023120, mae: 0.323465, mean_q: 0.482763, mean_eps: 0.000000
 2323/5000: episode: 104, duration: 0.314s, episode steps:  21, steps per second:  67, episode reward: 38.410, mean reward:  1.829 [-2.389, 32.333], mean action: 3.714 [0.000, 15.000],  loss: 0.022472, mae: 0.323374, mean_q: 0.520875, mean_eps: 0.000000
 2343/5000: episode: 105, duration: 0.419s, episode steps:  20, steps per second:  48, episode reward: 38.601, mean reward:  1.930 [-2.303, 32.310], mean action: 2.650 [0.000, 15.000],  loss: 0.021799, mae: 0.315599, mean_q: 0.502625, mean_eps: 0.000000
 2377/5000: episode: 106, duration: 0.490s, episode steps:  34, steps per second:  69, episode reward: 32.175, mean reward:  0.946 [-3.000, 32.012], mean action: 8.794 [0.000, 17.000],  loss: 0.020366, mae: 0.313877, mean_q: 0.553687, mean_eps: 0.000000
 2407/5000: episode: 107, duration: 0.466s, episode steps:  30, steps per second:  64, episode reward: 32.299, mean reward:  1.077 [-3.000, 32.080], mean action: 4.000 [0.000, 12.000],  loss: 0.020192, mae: 0.317794, mean_q: 0.582922, mean_eps: 0.000000
 2435/5000: episode: 108, duration: 0.412s, episode steps:  28, steps per second:  68, episode reward: -35.210, mean reward: -1.258 [-32.053,  2.460], mean action: 4.929 [0.000, 13.000],  loss: 0.021335, mae: 0.320944, mean_q: 0.567182, mean_eps: 0.000000
 2454/5000: episode: 109, duration: 0.284s, episode steps:  19, steps per second:  67, episode reward: 36.000, mean reward:  1.895 [-2.804, 32.470], mean action: 3.211 [0.000, 15.000],  loss: 0.021733, mae: 0.323246, mean_q: 0.580431, mean_eps: 0.000000
 2481/5000: episode: 110, duration: 0.402s, episode steps:  27, steps per second:  67, episode reward: 37.929, mean reward:  1.405 [-3.000, 32.376], mean action: 4.963 [1.000, 13.000],  loss: 0.020335, mae: 0.318146, mean_q: 0.593092, mean_eps: 0.000000
 2500/5000: episode: 111, duration: 0.286s, episode steps:  19, steps per second:  66, episode reward: 38.498, mean reward:  2.026 [-2.766, 32.498], mean action: 3.421 [0.000, 12.000],  loss: 0.023521, mae: 0.333613, mean_q: 0.556517, mean_eps: 0.000000
 2523/5000: episode: 112, duration: 0.333s, episode steps:  23, steps per second:  69, episode reward: 38.630, mean reward:  1.680 [-2.398, 32.020], mean action: 3.739 [0.000, 12.000],  loss: 0.020507, mae: 0.323594, mean_q: 0.569031, mean_eps: 0.000000
 2546/5000: episode: 113, duration: 0.338s, episode steps:  23, steps per second:  68, episode reward: 32.707, mean reward:  1.422 [-3.000, 32.202], mean action: 4.348 [0.000, 14.000],  loss: 0.018096, mae: 0.304891, mean_q: 0.512094, mean_eps: 0.000000
 2571/5000: episode: 114, duration: 0.373s, episode steps:  25, steps per second:  67, episode reward: 32.205, mean reward:  1.288 [-2.472, 32.033], mean action: 4.920 [1.000, 13.000],  loss: 0.023086, mae: 0.327463, mean_q: 0.581341, mean_eps: 0.000000
 2597/5000: episode: 115, duration: 0.377s, episode steps:  26, steps per second:  69, episode reward: 41.871, mean reward:  1.610 [-2.106, 32.120], mean action: 3.923 [2.000, 14.000],  loss: 0.026176, mae: 0.346141, mean_q: 0.567108, mean_eps: 0.000000
 2619/5000: episode: 116, duration: 0.327s, episode steps:  22, steps per second:  67, episode reward: -32.800, mean reward: -1.491 [-30.000,  2.590], mean action: 5.136 [1.000, 14.000],  loss: 0.020997, mae: 0.316947, mean_q: 0.608102, mean_eps: 0.000000
 2638/5000: episode: 117, duration: 0.288s, episode steps:  19, steps per second:  66, episode reward: 39.000, mean reward:  2.053 [-2.366, 33.000], mean action: 2.368 [0.000, 11.000],  loss: 0.019258, mae: 0.312527, mean_q: 0.583782, mean_eps: 0.000000
 2656/5000: episode: 118, duration: 0.264s, episode steps:  18, steps per second:  68, episode reward: 38.800, mean reward:  2.156 [-2.243, 32.900], mean action: 3.944 [0.000, 11.000],  loss: 0.021253, mae: 0.327504, mean_q: 0.539182, mean_eps: 0.000000
 2676/5000: episode: 119, duration: 0.311s, episode steps:  20, steps per second:  64, episode reward: 38.847, mean reward:  1.942 [-2.496, 32.480], mean action: 2.900 [0.000, 11.000],  loss: 0.023639, mae: 0.328877, mean_q: 0.563609, mean_eps: 0.000000
 2695/5000: episode: 120, duration: 0.296s, episode steps:  19, steps per second:  64, episode reward: 44.041, mean reward:  2.318 [-2.343, 32.377], mean action: 3.368 [1.000, 11.000],  loss: 0.024966, mae: 0.328468, mean_q: 0.534523, mean_eps: 0.000000
 2713/5000: episode: 121, duration: 0.264s, episode steps:  18, steps per second:  68, episode reward: 35.345, mean reward:  1.964 [-2.591, 32.052], mean action: 3.500 [0.000, 11.000],  loss: 0.023421, mae: 0.328214, mean_q: 0.511753, mean_eps: 0.000000
 2738/5000: episode: 122, duration: 0.364s, episode steps:  25, steps per second:  69, episode reward: 32.731, mean reward:  1.309 [-2.683, 32.351], mean action: 6.120 [0.000, 16.000],  loss: 0.021059, mae: 0.312373, mean_q: 0.520057, mean_eps: 0.000000
 2764/5000: episode: 123, duration: 0.403s, episode steps:  26, steps per second:  65, episode reward: 32.441, mean reward:  1.248 [-3.000, 31.701], mean action: 5.192 [0.000, 16.000],  loss: 0.022562, mae: 0.316328, mean_q: 0.546709, mean_eps: 0.000000
 2785/5000: episode: 124, duration: 0.299s, episode steps:  21, steps per second:  70, episode reward: 35.851, mean reward:  1.707 [-2.491, 32.500], mean action: 3.048 [0.000, 15.000],  loss: 0.021867, mae: 0.312418, mean_q: 0.575816, mean_eps: 0.000000
 2804/5000: episode: 125, duration: 0.282s, episode steps:  19, steps per second:  67, episode reward: 38.933, mean reward:  2.049 [-2.358, 32.173], mean action: 3.421 [0.000, 12.000],  loss: 0.018898, mae: 0.296546, mean_q: 0.561035, mean_eps: 0.000000
 2823/5000: episode: 126, duration: 0.271s, episode steps:  19, steps per second:  70, episode reward: 32.831, mean reward:  1.728 [-3.000, 32.254], mean action: 4.526 [0.000, 19.000],  loss: 0.020250, mae: 0.302675, mean_q: 0.523008, mean_eps: 0.000000
 2851/5000: episode: 127, duration: 0.396s, episode steps:  28, steps per second:  71, episode reward: 32.111, mean reward:  1.147 [-2.209, 32.146], mean action: 4.357 [0.000, 19.000],  loss: 0.017990, mae: 0.286709, mean_q: 0.536614, mean_eps: 0.000000
 2864/5000: episode: 128, duration: 0.209s, episode steps:  13, steps per second:  62, episode reward: 42.000, mean reward:  3.231 [-2.004, 30.974], mean action: 3.462 [0.000, 19.000],  loss: 0.025824, mae: 0.323414, mean_q: 0.533183, mean_eps: 0.000000
 2884/5000: episode: 129, duration: 0.301s, episode steps:  20, steps per second:  66, episode reward: 39.000, mean reward:  1.950 [-2.286, 32.410], mean action: 5.300 [1.000, 19.000],  loss: 0.023926, mae: 0.321836, mean_q: 0.568604, mean_eps: 0.000000
 2905/5000: episode: 130, duration: 0.309s, episode steps:  21, steps per second:  68, episode reward: 37.855, mean reward:  1.803 [-2.361, 32.927], mean action: 4.000 [0.000, 19.000],  loss: 0.019607, mae: 0.298340, mean_q: 0.500882, mean_eps: 0.000000
 2931/5000: episode: 131, duration: 0.405s, episode steps:  26, steps per second:  64, episode reward: 32.822, mean reward:  1.262 [-2.386, 32.300], mean action: 8.615 [0.000, 19.000],  loss: 0.018989, mae: 0.292067, mean_q: 0.516354, mean_eps: 0.000000
 2950/5000: episode: 132, duration: 0.278s, episode steps:  19, steps per second:  68, episode reward: 38.406, mean reward:  2.021 [-2.462, 31.952], mean action: 3.789 [0.000, 19.000],  loss: 0.024382, mae: 0.321938, mean_q: 0.520212, mean_eps: 0.000000
 2980/5000: episode: 133, duration: 0.430s, episode steps:  30, steps per second:  70, episode reward: 32.706, mean reward:  1.090 [-2.886, 32.363], mean action: 6.667 [0.000, 19.000],  loss: 0.021960, mae: 0.309998, mean_q: 0.545471, mean_eps: 0.000000
 3004/5000: episode: 134, duration: 0.340s, episode steps:  24, steps per second:  71, episode reward: -35.330, mean reward: -1.472 [-32.366,  2.713], mean action: 6.542 [0.000, 19.000],  loss: 0.022554, mae: 0.315203, mean_q: 0.502470, mean_eps: 0.000000
 3024/5000: episode: 135, duration: 0.299s, episode steps:  20, steps per second:  67, episode reward: 35.251, mean reward:  1.763 [-2.554, 32.080], mean action: 6.450 [0.000, 19.000],  loss: 0.019558, mae: 0.305869, mean_q: 0.464858, mean_eps: 0.000000
 3048/5000: episode: 136, duration: 0.346s, episode steps:  24, steps per second:  69, episode reward: -32.050, mean reward: -1.335 [-32.178,  2.731], mean action: 4.708 [0.000, 19.000],  loss: 0.019555, mae: 0.308223, mean_q: 0.516331, mean_eps: 0.000000
 3076/5000: episode: 137, duration: 0.406s, episode steps:  28, steps per second:  69, episode reward: 41.330, mean reward:  1.476 [-2.183, 32.370], mean action: 3.250 [0.000, 19.000],  loss: 0.020768, mae: 0.308233, mean_q: 0.557246, mean_eps: 0.000000
 3100/5000: episode: 138, duration: 0.359s, episode steps:  24, steps per second:  67, episode reward: -32.180, mean reward: -1.341 [-32.500,  2.952], mean action: 8.708 [0.000, 19.000],  loss: 0.020036, mae: 0.306181, mean_q: 0.484536, mean_eps: 0.000000
 3110/5000: episode: 139, duration: 0.167s, episode steps:  10, steps per second:  60, episode reward: 44.401, mean reward:  4.440 [-2.203, 33.000], mean action: 2.400 [0.000, 9.000],  loss: 0.026983, mae: 0.342061, mean_q: 0.509963, mean_eps: 0.000000
 3139/5000: episode: 140, duration: 0.425s, episode steps:  29, steps per second:  68, episode reward: 32.078, mean reward:  1.106 [-2.354, 32.230], mean action: 6.172 [0.000, 18.000],  loss: 0.014365, mae: 0.273953, mean_q: 0.505585, mean_eps: 0.000000
 3154/5000: episode: 141, duration: 0.236s, episode steps:  15, steps per second:  64, episode reward: 39.000, mean reward:  2.600 [-2.405, 30.194], mean action: 3.067 [0.000, 15.000],  loss: 0.025486, mae: 0.331687, mean_q: 0.484602, mean_eps: 0.000000
 3172/5000: episode: 142, duration: 0.267s, episode steps:  18, steps per second:  67, episode reward: 38.820, mean reward:  2.157 [-2.366, 32.320], mean action: 4.333 [0.000, 15.000],  loss: 0.021540, mae: 0.314386, mean_q: 0.457965, mean_eps: 0.000000
 3191/5000: episode: 143, duration: 0.365s, episode steps:  19, steps per second:  52, episode reward: 35.702, mean reward:  1.879 [-3.000, 32.901], mean action: 5.368 [0.000, 19.000],  loss: 0.021472, mae: 0.310340, mean_q: 0.513169, mean_eps: 0.000000
 3212/5000: episode: 144, duration: 0.366s, episode steps:  21, steps per second:  57, episode reward: -35.720, mean reward: -1.701 [-32.502,  2.420], mean action: 6.333 [0.000, 19.000],  loss: 0.016078, mae: 0.284598, mean_q: 0.546648, mean_eps: 0.000000
 3231/5000: episode: 145, duration: 0.291s, episode steps:  19, steps per second:  65, episode reward: 41.829, mean reward:  2.202 [-2.190, 32.829], mean action: 2.737 [0.000, 15.000],  loss: 0.024615, mae: 0.329659, mean_q: 0.537009, mean_eps: 0.000000
 3260/5000: episode: 146, duration: 0.419s, episode steps:  29, steps per second:  69, episode reward: 32.407, mean reward:  1.117 [-3.000, 31.936], mean action: 6.448 [0.000, 19.000],  loss: 0.021245, mae: 0.316691, mean_q: 0.539003, mean_eps: 0.000000
 3282/5000: episode: 147, duration: 0.340s, episode steps:  22, steps per second:  65, episode reward: -35.040, mean reward: -1.593 [-32.640,  2.570], mean action: 7.273 [0.000, 19.000],  loss: 0.022320, mae: 0.322299, mean_q: 0.530400, mean_eps: 0.000000
 3308/5000: episode: 148, duration: 0.388s, episode steps:  26, steps per second:  67, episode reward: 41.110, mean reward:  1.581 [-2.279, 32.160], mean action: 2.731 [1.000, 19.000],  loss: 0.023238, mae: 0.320673, mean_q: 0.555767, mean_eps: 0.000000
 3337/5000: episode: 149, duration: 0.425s, episode steps:  29, steps per second:  68, episode reward: -32.140, mean reward: -1.108 [-32.333,  2.242], mean action: 6.862 [0.000, 19.000],  loss: 0.019437, mae: 0.305455, mean_q: 0.535412, mean_eps: 0.000000
 3363/5000: episode: 150, duration: 0.379s, episode steps:  26, steps per second:  69, episode reward: 35.659, mean reward:  1.371 [-2.522, 32.128], mean action: 3.346 [0.000, 19.000],  loss: 0.021706, mae: 0.314265, mean_q: 0.492225, mean_eps: 0.000000
 3378/5000: episode: 151, duration: 0.227s, episode steps:  15, steps per second:  66, episode reward: 41.796, mean reward:  2.786 [-2.329, 32.390], mean action: 3.267 [0.000, 12.000],  loss: 0.017689, mae: 0.292243, mean_q: 0.507947, mean_eps: 0.000000
 3412/5000: episode: 152, duration: 0.482s, episode steps:  34, steps per second:  71, episode reward: -41.110, mean reward: -1.209 [-32.057,  2.572], mean action: 6.059 [0.000, 19.000],  loss: 0.018971, mae: 0.298096, mean_q: 0.513541, mean_eps: 0.000000
 3444/5000: episode: 153, duration: 0.466s, episode steps:  32, steps per second:  69, episode reward: -32.680, mean reward: -1.021 [-32.312,  2.510], mean action: 10.625 [0.000, 19.000],  loss: 0.021959, mae: 0.314142, mean_q: 0.531344, mean_eps: 0.000000
 3475/5000: episode: 154, duration: 0.433s, episode steps:  31, steps per second:  72, episode reward: 35.836, mean reward:  1.156 [-2.548, 32.716], mean action: 4.323 [0.000, 19.000],  loss: 0.021464, mae: 0.316255, mean_q: 0.526080, mean_eps: 0.000000
 3512/5000: episode: 155, duration: 0.594s, episode steps:  37, steps per second:  62, episode reward: -32.740, mean reward: -0.885 [-31.748,  2.690], mean action: 6.541 [0.000, 19.000],  loss: 0.022915, mae: 0.319068, mean_q: 0.528684, mean_eps: 0.000000
 3540/5000: episode: 156, duration: 0.552s, episode steps:  28, steps per second:  51, episode reward: 35.127, mean reward:  1.255 [-2.322, 31.835], mean action: 6.250 [0.000, 20.000],  loss: 0.020363, mae: 0.312172, mean_q: 0.583496, mean_eps: 0.000000
 3566/5000: episode: 157, duration: 0.409s, episode steps:  26, steps per second:  64, episode reward: -32.370, mean reward: -1.245 [-32.144,  2.530], mean action: 7.731 [0.000, 19.000],  loss: 0.020979, mae: 0.312700, mean_q: 0.618165, mean_eps: 0.000000
 3592/5000: episode: 158, duration: 0.379s, episode steps:  26, steps per second:  69, episode reward: 38.495, mean reward:  1.481 [-2.211, 31.959], mean action: 5.192 [1.000, 19.000],  loss: 0.022877, mae: 0.332082, mean_q: 0.565232, mean_eps: 0.000000
 3610/5000: episode: 159, duration: 0.268s, episode steps:  18, steps per second:  67, episode reward: 39.000, mean reward:  2.167 [-2.232, 32.250], mean action: 5.444 [0.000, 19.000],  loss: 0.022930, mae: 0.326500, mean_q: 0.506965, mean_eps: 0.000000
 3634/5000: episode: 160, duration: 0.347s, episode steps:  24, steps per second:  69, episode reward: -32.620, mean reward: -1.359 [-32.370,  3.000], mean action: 5.750 [0.000, 18.000],  loss: 0.020482, mae: 0.312496, mean_q: 0.494899, mean_eps: 0.000000
 3658/5000: episode: 161, duration: 0.378s, episode steps:  24, steps per second:  64, episode reward: 32.116, mean reward:  1.338 [-2.610, 31.647], mean action: 4.333 [0.000, 16.000],  loss: 0.020117, mae: 0.307666, mean_q: 0.522029, mean_eps: 0.000000
 3683/5000: episode: 162, duration: 0.358s, episode steps:  25, steps per second:  70, episode reward: -32.620, mean reward: -1.305 [-32.143,  3.000], mean action: 7.360 [0.000, 18.000],  loss: 0.016391, mae: 0.284045, mean_q: 0.501706, mean_eps: 0.000000
 3714/5000: episode: 163, duration: 0.440s, episode steps:  31, steps per second:  70, episode reward: -32.070, mean reward: -1.035 [-32.475,  2.510], mean action: 7.806 [0.000, 20.000],  loss: 0.024648, mae: 0.323397, mean_q: 0.494190, mean_eps: 0.000000
 3752/5000: episode: 164, duration: 0.553s, episode steps:  38, steps per second:  69, episode reward: 32.500, mean reward:  0.855 [-2.757, 31.972], mean action: 5.500 [0.000, 19.000],  loss: 0.019955, mae: 0.311055, mean_q: 0.502664, mean_eps: 0.000000
 3769/5000: episode: 165, duration: 0.260s, episode steps:  17, steps per second:  65, episode reward: 41.516, mean reward:  2.442 [-3.000, 29.797], mean action: 4.647 [0.000, 16.000],  loss: 0.018325, mae: 0.302644, mean_q: 0.529097, mean_eps: 0.000000
 3786/5000: episode: 166, duration: 0.256s, episode steps:  17, steps per second:  66, episode reward: 35.492, mean reward:  2.088 [-3.000, 32.402], mean action: 4.706 [0.000, 16.000],  loss: 0.020468, mae: 0.323752, mean_q: 0.501637, mean_eps: 0.000000
 3810/5000: episode: 167, duration: 0.362s, episode steps:  24, steps per second:  66, episode reward: -38.950, mean reward: -1.623 [-32.063,  2.910], mean action: 7.000 [0.000, 17.000],  loss: 0.019748, mae: 0.304958, mean_q: 0.515939, mean_eps: 0.000000
 3834/5000: episode: 168, duration: 0.353s, episode steps:  24, steps per second:  68, episode reward: 32.021, mean reward:  1.334 [-3.000, 32.310], mean action: 3.417 [0.000, 14.000],  loss: 0.020617, mae: 0.313294, mean_q: 0.534314, mean_eps: 0.000000
 3858/5000: episode: 169, duration: 0.350s, episode steps:  24, steps per second:  69, episode reward: 38.803, mean reward:  1.617 [-2.475, 32.041], mean action: 2.792 [0.000, 19.000],  loss: 0.023054, mae: 0.325235, mean_q: 0.527198, mean_eps: 0.000000
 3883/5000: episode: 170, duration: 0.371s, episode steps:  25, steps per second:  67, episode reward: 38.788, mean reward:  1.552 [-3.000, 32.340], mean action: 4.120 [0.000, 19.000],  loss: 0.019704, mae: 0.313588, mean_q: 0.544321, mean_eps: 0.000000
 3897/5000: episode: 171, duration: 0.220s, episode steps:  14, steps per second:  64, episode reward: 42.000, mean reward:  3.000 [-2.562, 32.360], mean action: 4.000 [0.000, 16.000],  loss: 0.014019, mae: 0.284648, mean_q: 0.545790, mean_eps: 0.000000
 3935/5000: episode: 172, duration: 0.542s, episode steps:  38, steps per second:  70, episode reward: 32.037, mean reward:  0.843 [-2.354, 31.786], mean action: 5.868 [1.000, 16.000],  loss: 0.023129, mae: 0.330046, mean_q: 0.552574, mean_eps: 0.000000
 3957/5000: episode: 173, duration: 0.330s, episode steps:  22, steps per second:  67, episode reward: 36.000, mean reward:  1.636 [-2.283, 32.130], mean action: 3.636 [0.000, 16.000],  loss: 0.016397, mae: 0.299018, mean_q: 0.543502, mean_eps: 0.000000
 3984/5000: episode: 174, duration: 0.395s, episode steps:  27, steps per second:  68, episode reward: 39.000, mean reward:  1.444 [-2.217, 32.070], mean action: 4.259 [0.000, 16.000],  loss: 0.020834, mae: 0.316330, mean_q: 0.542426, mean_eps: 0.000000
 4001/5000: episode: 175, duration: 0.251s, episode steps:  17, steps per second:  68, episode reward: 38.502, mean reward:  2.265 [-3.000, 32.210], mean action: 4.353 [0.000, 16.000],  loss: 0.020870, mae: 0.318423, mean_q: 0.523805, mean_eps: 0.000000
 4016/5000: episode: 176, duration: 0.227s, episode steps:  15, steps per second:  66, episode reward: 38.544, mean reward:  2.570 [-3.000, 32.600], mean action: 4.200 [0.000, 15.000],  loss: 0.017451, mae: 0.306891, mean_q: 0.542377, mean_eps: 0.000000
 4037/5000: episode: 177, duration: 0.313s, episode steps:  21, steps per second:  67, episode reward: 41.091, mean reward:  1.957 [-2.126, 32.032], mean action: 3.143 [0.000, 13.000],  loss: 0.024929, mae: 0.341419, mean_q: 0.487189, mean_eps: 0.000000
 4064/5000: episode: 178, duration: 0.458s, episode steps:  27, steps per second:  59, episode reward: 32.234, mean reward:  1.194 [-3.000, 31.675], mean action: 7.889 [0.000, 15.000],  loss: 0.019507, mae: 0.310964, mean_q: 0.504984, mean_eps: 0.000000
 4085/5000: episode: 179, duration: 0.369s, episode steps:  21, steps per second:  57, episode reward: 44.017, mean reward:  2.096 [-2.238, 32.090], mean action: 4.000 [1.000, 12.000],  loss: 0.017219, mae: 0.302466, mean_q: 0.521464, mean_eps: 0.000000
 4103/5000: episode: 180, duration: 0.262s, episode steps:  18, steps per second:  69, episode reward: -41.480, mean reward: -2.304 [-32.830,  2.020], mean action: 8.333 [0.000, 16.000],  loss: 0.019586, mae: 0.313270, mean_q: 0.537082, mean_eps: 0.000000
 4129/5000: episode: 181, duration: 0.394s, episode steps:  26, steps per second:  66, episode reward: 38.553, mean reward:  1.483 [-2.185, 32.021], mean action: 3.038 [0.000, 16.000],  loss: 0.022079, mae: 0.333073, mean_q: 0.557562, mean_eps: 0.000000
 4146/5000: episode: 182, duration: 0.265s, episode steps:  17, steps per second:  64, episode reward: 39.000, mean reward:  2.294 [-2.131, 29.723], mean action: 2.824 [0.000, 16.000],  loss: 0.024214, mae: 0.337446, mean_q: 0.540897, mean_eps: 0.000000
 4176/5000: episode: 183, duration: 0.440s, episode steps:  30, steps per second:  68, episode reward: 35.582, mean reward:  1.186 [-2.216, 32.100], mean action: 3.067 [0.000, 16.000],  loss: 0.022535, mae: 0.338313, mean_q: 0.568555, mean_eps: 0.000000
 4196/5000: episode: 184, duration: 0.299s, episode steps:  20, steps per second:  67, episode reward: 35.662, mean reward:  1.783 [-2.506, 32.320], mean action: 3.500 [0.000, 15.000],  loss: 0.022171, mae: 0.321450, mean_q: 0.575239, mean_eps: 0.000000
 4219/5000: episode: 185, duration: 0.368s, episode steps:  23, steps per second:  63, episode reward: 35.025, mean reward:  1.523 [-2.336, 32.499], mean action: 5.783 [0.000, 20.000],  loss: 0.020336, mae: 0.318153, mean_q: 0.485757, mean_eps: 0.000000
 4236/5000: episode: 186, duration: 0.323s, episode steps:  17, steps per second:  53, episode reward: 36.000, mean reward:  2.118 [-2.611, 30.081], mean action: 4.529 [1.000, 15.000],  loss: 0.018862, mae: 0.309829, mean_q: 0.551023, mean_eps: 0.000000
 4265/5000: episode: 187, duration: 0.443s, episode steps:  29, steps per second:  65, episode reward: -35.120, mean reward: -1.211 [-32.032,  2.436], mean action: 5.517 [0.000, 21.000],  loss: 0.019604, mae: 0.309550, mean_q: 0.525940, mean_eps: 0.000000
 4286/5000: episode: 188, duration: 0.315s, episode steps:  21, steps per second:  67, episode reward: 35.816, mean reward:  1.706 [-2.519, 32.816], mean action: 5.095 [0.000, 15.000],  loss: 0.016966, mae: 0.293368, mean_q: 0.465808, mean_eps: 0.000000
 4305/5000: episode: 189, duration: 0.283s, episode steps:  19, steps per second:  67, episode reward: 40.417, mean reward:  2.127 [-2.514, 31.957], mean action: 6.263 [0.000, 15.000],  loss: 0.024025, mae: 0.327190, mean_q: 0.431487, mean_eps: 0.000000
 4326/5000: episode: 190, duration: 0.305s, episode steps:  21, steps per second:  69, episode reward: 39.000, mean reward:  1.857 [-2.742, 33.000], mean action: 3.143 [0.000, 15.000],  loss: 0.024370, mae: 0.328721, mean_q: 0.452465, mean_eps: 0.000000
 4348/5000: episode: 191, duration: 0.332s, episode steps:  22, steps per second:  66, episode reward: 32.477, mean reward:  1.476 [-3.000, 32.698], mean action: 6.091 [0.000, 18.000],  loss: 0.020605, mae: 0.305057, mean_q: 0.476127, mean_eps: 0.000000
 4368/5000: episode: 192, duration: 0.289s, episode steps:  20, steps per second:  69, episode reward: -42.000, mean reward: -2.100 [-32.074,  2.569], mean action: 7.450 [0.000, 14.000],  loss: 0.017619, mae: 0.287724, mean_q: 0.528332, mean_eps: 0.000000
 4388/5000: episode: 193, duration: 0.300s, episode steps:  20, steps per second:  67, episode reward: 35.350, mean reward:  1.768 [-2.664, 32.080], mean action: 2.000 [0.000, 15.000],  loss: 0.020352, mae: 0.304440, mean_q: 0.552232, mean_eps: 0.000000
 4407/5000: episode: 194, duration: 0.283s, episode steps:  19, steps per second:  67, episode reward: 35.476, mean reward:  1.867 [-2.693, 32.386], mean action: 4.105 [0.000, 18.000],  loss: 0.017363, mae: 0.295184, mean_q: 0.494232, mean_eps: 0.000000
 4426/5000: episode: 195, duration: 0.296s, episode steps:  19, steps per second:  64, episode reward: 41.766, mean reward:  2.198 [-2.236, 32.268], mean action: 0.684 [0.000, 2.000],  loss: 0.019256, mae: 0.307577, mean_q: 0.454795, mean_eps: 0.000000
 4446/5000: episode: 196, duration: 0.305s, episode steps:  20, steps per second:  66, episode reward: 38.616, mean reward:  1.931 [-2.385, 31.914], mean action: 3.600 [0.000, 16.000],  loss: 0.020779, mae: 0.309218, mean_q: 0.481698, mean_eps: 0.000000
 4468/5000: episode: 197, duration: 0.320s, episode steps:  22, steps per second:  69, episode reward: 33.000, mean reward:  1.500 [-2.466, 32.420], mean action: 7.727 [0.000, 16.000],  loss: 0.022032, mae: 0.321185, mean_q: 0.498136, mean_eps: 0.000000
 4493/5000: episode: 198, duration: 0.366s, episode steps:  25, steps per second:  68, episode reward: 34.640, mean reward:  1.386 [-2.847, 32.270], mean action: 4.640 [0.000, 20.000],  loss: 0.019985, mae: 0.308808, mean_q: 0.539674, mean_eps: 0.000000
 4511/5000: episode: 199, duration: 0.266s, episode steps:  18, steps per second:  68, episode reward: 40.830, mean reward:  2.268 [-2.469, 31.867], mean action: 4.222 [0.000, 16.000],  loss: 0.019143, mae: 0.304160, mean_q: 0.544238, mean_eps: 0.000000
 4539/5000: episode: 200, duration: 0.404s, episode steps:  28, steps per second:  69, episode reward: 35.573, mean reward:  1.270 [-3.000, 32.063], mean action: 3.643 [0.000, 16.000],  loss: 0.019420, mae: 0.304107, mean_q: 0.561235, mean_eps: 0.000000
 4556/5000: episode: 201, duration: 0.288s, episode steps:  17, steps per second:  59, episode reward: 39.000, mean reward:  2.294 [-3.000, 32.500], mean action: 3.882 [0.000, 16.000],  loss: 0.020049, mae: 0.309187, mean_q: 0.594700, mean_eps: 0.000000
 4579/5000: episode: 202, duration: 0.349s, episode steps:  23, steps per second:  66, episode reward: -32.520, mean reward: -1.414 [-32.108,  3.000], mean action: 6.304 [0.000, 15.000],  loss: 0.023934, mae: 0.329642, mean_q: 0.579263, mean_eps: 0.000000
 4604/5000: episode: 203, duration: 0.367s, episode steps:  25, steps per second:  68, episode reward: 32.848, mean reward:  1.314 [-2.471, 32.068], mean action: 7.320 [0.000, 16.000],  loss: 0.017847, mae: 0.307869, mean_q: 0.573091, mean_eps: 0.000000
 4625/5000: episode: 204, duration: 0.317s, episode steps:  21, steps per second:  66, episode reward: 38.141, mean reward:  1.816 [-3.000, 32.230], mean action: 5.048 [0.000, 16.000],  loss: 0.020107, mae: 0.320329, mean_q: 0.512130, mean_eps: 0.000000
 4658/5000: episode: 205, duration: 0.543s, episode steps:  33, steps per second:  61, episode reward: 35.383, mean reward:  1.072 [-2.691, 32.058], mean action: 4.909 [0.000, 16.000],  loss: 0.023611, mae: 0.337109, mean_q: 0.526621, mean_eps: 0.000000
 4684/5000: episode: 206, duration: 0.376s, episode steps:  26, steps per second:  69, episode reward: 33.000, mean reward:  1.269 [-2.900, 32.390], mean action: 7.538 [0.000, 16.000],  loss: 0.021494, mae: 0.324410, mean_q: 0.573474, mean_eps: 0.000000
 4712/5000: episode: 207, duration: 0.404s, episode steps:  28, steps per second:  69, episode reward: 32.700, mean reward:  1.168 [-2.467, 32.540], mean action: 4.214 [0.000, 19.000],  loss: 0.020643, mae: 0.318175, mean_q: 0.615544, mean_eps: 0.000000
 4747/5000: episode: 208, duration: 0.492s, episode steps:  35, steps per second:  71, episode reward: -35.540, mean reward: -1.015 [-32.165,  2.550], mean action: 6.057 [0.000, 20.000],  loss: 0.026220, mae: 0.342575, mean_q: 0.581860, mean_eps: 0.000000
 4763/5000: episode: 209, duration: 0.247s, episode steps:  16, steps per second:  65, episode reward: 41.030, mean reward:  2.564 [-2.536, 32.830], mean action: 4.250 [0.000, 12.000],  loss: 0.026641, mae: 0.341752, mean_q: 0.586518, mean_eps: 0.000000
 4780/5000: episode: 210, duration: 0.253s, episode steps:  17, steps per second:  67, episode reward: 38.089, mean reward:  2.241 [-2.434, 32.314], mean action: 3.882 [2.000, 12.000],  loss: 0.020948, mae: 0.316771, mean_q: 0.595931, mean_eps: 0.000000
 4804/5000: episode: 211, duration: 0.348s, episode steps:  24, steps per second:  69, episode reward: 35.429, mean reward:  1.476 [-2.414, 32.240], mean action: 4.125 [0.000, 19.000],  loss: 0.021033, mae: 0.318385, mean_q: 0.549366, mean_eps: 0.000000
 4823/5000: episode: 212, duration: 0.289s, episode steps:  19, steps per second:  66, episode reward: 35.541, mean reward:  1.871 [-2.483, 32.410], mean action: 4.947 [0.000, 18.000],  loss: 0.022426, mae: 0.319923, mean_q: 0.512575, mean_eps: 0.000000
 4846/5000: episode: 213, duration: 0.332s, episode steps:  23, steps per second:  69, episode reward: -33.000, mean reward: -1.435 [-32.142,  2.232], mean action: 5.826 [0.000, 15.000],  loss: 0.016351, mae: 0.292792, mean_q: 0.516209, mean_eps: 0.000000
 4864/5000: episode: 214, duration: 0.275s, episode steps:  18, steps per second:  66, episode reward: 36.000, mean reward:  2.000 [-2.197, 33.000], mean action: 4.389 [0.000, 15.000],  loss: 0.018593, mae: 0.300137, mean_q: 0.563746, mean_eps: 0.000000
 4885/5000: episode: 215, duration: 0.309s, episode steps:  21, steps per second:  68, episode reward: -33.000, mean reward: -1.571 [-33.000,  2.612], mean action: 5.095 [0.000, 15.000],  loss: 0.019654, mae: 0.309662, mean_q: 0.476717, mean_eps: 0.000000
 4906/5000: episode: 216, duration: 0.322s, episode steps:  21, steps per second:  65, episode reward: 38.340, mean reward:  1.826 [-2.476, 32.315], mean action: 2.952 [0.000, 12.000],  loss: 0.022903, mae: 0.323257, mean_q: 0.585276, mean_eps: 0.000000
 4942/5000: episode: 217, duration: 0.518s, episode steps:  36, steps per second:  69, episode reward: 32.820, mean reward:  0.912 [-2.489, 32.130], mean action: 7.556 [0.000, 17.000],  loss: 0.022897, mae: 0.323554, mean_q: 0.602309, mean_eps: 0.000000
 4958/5000: episode: 218, duration: 0.242s, episode steps:  16, steps per second:  66, episode reward: 41.288, mean reward:  2.580 [-2.421, 32.676], mean action: 3.125 [0.000, 11.000],  loss: 0.017626, mae: 0.300908, mean_q: 0.558209, mean_eps: 0.000000
 4980/5000: episode: 219, duration: 0.322s, episode steps:  22, steps per second:  68, episode reward: 38.052, mean reward:  1.730 [-3.000, 32.340], mean action: 4.182 [0.000, 14.000],  loss: 0.020741, mae: 0.320019, mean_q: 0.553963, mean_eps: 0.000000
 4994/5000: episode: 220, duration: 0.213s, episode steps:  14, steps per second:  66, episode reward: 38.540, mean reward:  2.753 [-2.221, 33.000], mean action: 3.500 [0.000, 11.000],  loss: 0.026661, mae: 0.349267, mean_q: 0.613842, mean_eps: 0.000000
done, took 67.727 seconds
DQN Evaluation: 11619 victories out of 13554 episodes
Training for 5000 steps ...
   16/5000: episode: 1, duration: 0.185s, episode steps:  16, steps per second:  86, episode reward: 47.118, mean reward:  2.945 [-0.230, 32.160], mean action: 6.250 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   57/5000: episode: 2, duration: 0.287s, episode steps:  41, steps per second: 143, episode reward: 32.659, mean reward:  0.797 [-3.000, 29.901], mean action: 7.585 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   85/5000: episode: 3, duration: 0.186s, episode steps:  28, steps per second: 150, episode reward: 38.886, mean reward:  1.389 [-2.879, 32.186], mean action: 3.929 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  121/5000: episode: 4, duration: 0.250s, episode steps:  36, steps per second: 144, episode reward: 38.930, mean reward:  1.081 [-2.800, 32.090], mean action: 2.722 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  153/5000: episode: 5, duration: 0.213s, episode steps:  32, steps per second: 150, episode reward: 37.899, mean reward:  1.184 [-2.448, 32.236], mean action: 4.125 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  174/5000: episode: 6, duration: 0.161s, episode steps:  21, steps per second: 130, episode reward: 41.589, mean reward:  1.980 [-3.000, 32.110], mean action: 5.857 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  202/5000: episode: 7, duration: 0.186s, episode steps:  28, steps per second: 150, episode reward: 35.298, mean reward:  1.261 [-2.290, 32.702], mean action: 3.679 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  227/5000: episode: 8, duration: 0.169s, episode steps:  25, steps per second: 148, episode reward: 44.331, mean reward:  1.773 [-2.336, 33.000], mean action: 5.280 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  252/5000: episode: 9, duration: 0.165s, episode steps:  25, steps per second: 151, episode reward: 38.167, mean reward:  1.527 [-2.516, 31.993], mean action: 4.320 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  280/5000: episode: 10, duration: 0.181s, episode steps:  28, steps per second: 154, episode reward: 41.718, mean reward:  1.490 [-3.000, 32.380], mean action: 1.893 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  301/5000: episode: 11, duration: 0.154s, episode steps:  21, steps per second: 136, episode reward: 44.103, mean reward:  2.100 [-3.000, 32.340], mean action: 1.286 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  323/5000: episode: 12, duration: 0.152s, episode steps:  22, steps per second: 145, episode reward: 41.912, mean reward:  1.905 [-2.546, 32.090], mean action: 3.182 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  337/5000: episode: 13, duration: 0.107s, episode steps:  14, steps per second: 131, episode reward: 42.000, mean reward:  3.000 [-3.000, 32.580], mean action: 2.357 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  365/5000: episode: 14, duration: 0.189s, episode steps:  28, steps per second: 148, episode reward: 40.740, mean reward:  1.455 [-2.860, 32.080], mean action: 4.857 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  390/5000: episode: 15, duration: 0.174s, episode steps:  25, steps per second: 144, episode reward: 41.811, mean reward:  1.672 [-3.000, 32.620], mean action: 2.760 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  409/5000: episode: 16, duration: 0.144s, episode steps:  19, steps per second: 132, episode reward: 44.942, mean reward:  2.365 [-2.087, 32.280], mean action: 2.632 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  433/5000: episode: 17, duration: 0.161s, episode steps:  24, steps per second: 149, episode reward: 41.596, mean reward:  1.733 [-2.115, 32.090], mean action: 2.333 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  449/5000: episode: 18, duration: 0.120s, episode steps:  16, steps per second: 134, episode reward: 44.796, mean reward:  2.800 [-2.147, 32.078], mean action: 3.812 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  504/5000: episode: 19, duration: 0.337s, episode steps:  55, steps per second: 163, episode reward: 42.000, mean reward:  0.764 [-2.265, 32.530], mean action: 2.164 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  535/5000: episode: 20, duration: 0.205s, episode steps:  31, steps per second: 151, episode reward: 39.959, mean reward:  1.289 [-2.205, 32.270], mean action: 4.742 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  576/5000: episode: 21, duration: 0.252s, episode steps:  41, steps per second: 162, episode reward: 37.539, mean reward:  0.916 [-2.756, 32.097], mean action: 5.902 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  597/5000: episode: 22, duration: 0.145s, episode steps:  21, steps per second: 145, episode reward: 41.449, mean reward:  1.974 [-2.467, 31.724], mean action: 5.667 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  616/5000: episode: 23, duration: 0.133s, episode steps:  19, steps per second: 143, episode reward: 44.312, mean reward:  2.332 [-3.000, 32.150], mean action: 2.105 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  638/5000: episode: 24, duration: 0.151s, episode steps:  22, steps per second: 146, episode reward: 41.331, mean reward:  1.879 [-2.415, 32.331], mean action: 2.818 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  675/5000: episode: 25, duration: 0.225s, episode steps:  37, steps per second: 164, episode reward: 32.620, mean reward:  0.882 [-2.751, 32.064], mean action: 3.838 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  702/5000: episode: 26, duration: 0.176s, episode steps:  27, steps per second: 153, episode reward: 35.355, mean reward:  1.309 [-2.502, 33.283], mean action: 3.704 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  736/5000: episode: 27, duration: 0.219s, episode steps:  34, steps per second: 156, episode reward: 36.000, mean reward:  1.059 [-3.000, 32.180], mean action: 2.382 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  756/5000: episode: 28, duration: 0.244s, episode steps:  20, steps per second:  82, episode reward: 41.851, mean reward:  2.093 [-2.535, 32.120], mean action: 2.600 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  775/5000: episode: 29, duration: 0.273s, episode steps:  19, steps per second:  70, episode reward: 44.321, mean reward:  2.333 [-2.228, 32.470], mean action: 4.632 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  792/5000: episode: 30, duration: 0.128s, episode steps:  17, steps per second: 132, episode reward: 47.185, mean reward:  2.776 [-0.100, 33.000], mean action: 2.765 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  819/5000: episode: 31, duration: 0.191s, episode steps:  27, steps per second: 142, episode reward: 44.453, mean reward:  1.646 [-2.263, 32.143], mean action: 2.259 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  830/5000: episode: 32, duration: 0.091s, episode steps:  11, steps per second: 121, episode reward: 44.030, mean reward:  4.003 [-2.261, 32.903], mean action: 1.455 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  880/5000: episode: 33, duration: 0.311s, episode steps:  50, steps per second: 161, episode reward: -32.600, mean reward: -0.652 [-32.252,  2.455], mean action: 9.940 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  898/5000: episode: 34, duration: 0.134s, episode steps:  18, steps per second: 135, episode reward: 44.354, mean reward:  2.464 [-2.004, 32.070], mean action: 3.167 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  919/5000: episode: 35, duration: 0.146s, episode steps:  21, steps per second: 144, episode reward: 44.747, mean reward:  2.131 [-2.053, 32.139], mean action: 5.048 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  961/5000: episode: 36, duration: 0.271s, episode steps:  42, steps per second: 155, episode reward: 36.000, mean reward:  0.857 [-3.000, 32.030], mean action: 3.929 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  996/5000: episode: 37, duration: 0.214s, episode steps:  35, steps per second: 164, episode reward: 35.815, mean reward:  1.023 [-2.770, 32.010], mean action: 6.171 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1016/5000: episode: 38, duration: 0.285s, episode steps:  20, steps per second:  70, episode reward: 44.565, mean reward:  2.228 [-2.198, 32.100], mean action: 2.350 [0.000, 11.000],  loss: 0.020597, mae: 0.319534, mean_q: 0.589259, mean_eps: 0.000000
 1053/5000: episode: 39, duration: 0.527s, episode steps:  37, steps per second:  70, episode reward: 41.306, mean reward:  1.116 [-2.418, 32.140], mean action: 7.405 [0.000, 15.000],  loss: 0.022964, mae: 0.326774, mean_q: 0.595640, mean_eps: 0.000000
 1079/5000: episode: 40, duration: 0.373s, episode steps:  26, steps per second:  70, episode reward: 39.000, mean reward:  1.500 [-2.574, 32.050], mean action: 5.654 [0.000, 15.000],  loss: 0.021737, mae: 0.321224, mean_q: 0.546655, mean_eps: 0.000000
 1103/5000: episode: 41, duration: 0.542s, episode steps:  24, steps per second:  44, episode reward: 40.528, mean reward:  1.689 [-2.753, 32.080], mean action: 5.042 [0.000, 15.000],  loss: 0.022881, mae: 0.321710, mean_q: 0.539042, mean_eps: 0.000000
 1129/5000: episode: 42, duration: 0.574s, episode steps:  26, steps per second:  45, episode reward: 38.500, mean reward:  1.481 [-3.000, 32.910], mean action: 5.692 [0.000, 15.000],  loss: 0.021732, mae: 0.313208, mean_q: 0.590704, mean_eps: 0.000000
 1163/5000: episode: 43, duration: 0.503s, episode steps:  34, steps per second:  68, episode reward: 38.089, mean reward:  1.120 [-3.000, 31.619], mean action: 2.971 [0.000, 15.000],  loss: 0.020806, mae: 0.315024, mean_q: 0.588194, mean_eps: 0.000000
 1179/5000: episode: 44, duration: 0.252s, episode steps:  16, steps per second:  64, episode reward: 41.150, mean reward:  2.572 [-2.368, 32.153], mean action: 4.188 [0.000, 15.000],  loss: 0.022216, mae: 0.323028, mean_q: 0.599558, mean_eps: 0.000000
 1204/5000: episode: 45, duration: 0.486s, episode steps:  25, steps per second:  51, episode reward: 44.174, mean reward:  1.767 [-2.509, 32.130], mean action: 2.640 [0.000, 14.000],  loss: 0.019857, mae: 0.318352, mean_q: 0.539337, mean_eps: 0.000000
 1228/5000: episode: 46, duration: 0.357s, episode steps:  24, steps per second:  67, episode reward: 42.000, mean reward:  1.750 [-2.170, 32.200], mean action: 2.167 [0.000, 19.000],  loss: 0.020863, mae: 0.319015, mean_q: 0.581690, mean_eps: 0.000000
 1259/5000: episode: 47, duration: 0.439s, episode steps:  31, steps per second:  71, episode reward: 35.057, mean reward:  1.131 [-2.559, 31.582], mean action: 5.871 [0.000, 16.000],  loss: 0.017597, mae: 0.301187, mean_q: 0.577431, mean_eps: 0.000000
 1292/5000: episode: 48, duration: 0.462s, episode steps:  33, steps per second:  71, episode reward: 38.599, mean reward:  1.170 [-2.670, 32.450], mean action: 3.667 [0.000, 15.000],  loss: 0.023621, mae: 0.334747, mean_q: 0.518720, mean_eps: 0.000000
 1327/5000: episode: 49, duration: 0.498s, episode steps:  35, steps per second:  70, episode reward: 41.750, mean reward:  1.193 [-2.367, 32.226], mean action: 4.571 [0.000, 19.000],  loss: 0.021800, mae: 0.317847, mean_q: 0.552690, mean_eps: 0.000000
 1348/5000: episode: 50, duration: 0.311s, episode steps:  21, steps per second:  67, episode reward: 44.087, mean reward:  2.099 [-2.319, 31.909], mean action: 1.762 [0.000, 12.000],  loss: 0.019173, mae: 0.301904, mean_q: 0.565102, mean_eps: 0.000000
 1370/5000: episode: 51, duration: 0.326s, episode steps:  22, steps per second:  68, episode reward: 41.051, mean reward:  1.866 [-3.000, 31.656], mean action: 3.864 [1.000, 16.000],  loss: 0.016956, mae: 0.294565, mean_q: 0.541025, mean_eps: 0.000000
 1387/5000: episode: 52, duration: 0.255s, episode steps:  17, steps per second:  67, episode reward: 38.472, mean reward:  2.263 [-3.000, 32.569], mean action: 6.000 [0.000, 16.000],  loss: 0.019670, mae: 0.302679, mean_q: 0.529756, mean_eps: 0.000000
 1420/5000: episode: 53, duration: 0.490s, episode steps:  33, steps per second:  67, episode reward: 38.224, mean reward:  1.158 [-3.000, 32.300], mean action: 3.606 [0.000, 16.000],  loss: 0.022484, mae: 0.314452, mean_q: 0.460814, mean_eps: 0.000000
 1443/5000: episode: 54, duration: 0.331s, episode steps:  23, steps per second:  69, episode reward: 35.930, mean reward:  1.562 [-3.000, 32.540], mean action: 2.696 [0.000, 9.000],  loss: 0.022055, mae: 0.309200, mean_q: 0.532016, mean_eps: 0.000000
 1481/5000: episode: 55, duration: 0.537s, episode steps:  38, steps per second:  71, episode reward: 34.526, mean reward:  0.909 [-3.000, 32.340], mean action: 6.000 [0.000, 20.000],  loss: 0.019958, mae: 0.307294, mean_q: 0.519710, mean_eps: 0.000000
 1527/5000: episode: 56, duration: 0.636s, episode steps:  46, steps per second:  72, episode reward: 32.309, mean reward:  0.702 [-2.470, 29.803], mean action: 6.217 [0.000, 20.000],  loss: 0.023740, mae: 0.328837, mean_q: 0.522620, mean_eps: 0.000000
 1557/5000: episode: 57, duration: 0.428s, episode steps:  30, steps per second:  70, episode reward: 37.661, mean reward:  1.255 [-2.618, 32.088], mean action: 4.033 [0.000, 13.000],  loss: 0.021497, mae: 0.319369, mean_q: 0.472526, mean_eps: 0.000000
 1594/5000: episode: 58, duration: 0.525s, episode steps:  37, steps per second:  70, episode reward: -35.330, mean reward: -0.955 [-32.765,  2.690], mean action: 5.865 [0.000, 20.000],  loss: 0.021772, mae: 0.318647, mean_q: 0.517917, mean_eps: 0.000000
 1618/5000: episode: 59, duration: 0.346s, episode steps:  24, steps per second:  69, episode reward: 47.578, mean reward:  1.982 [-0.320, 32.010], mean action: 2.375 [0.000, 3.000],  loss: 0.019054, mae: 0.309876, mean_q: 0.539596, mean_eps: 0.000000
 1642/5000: episode: 60, duration: 0.358s, episode steps:  24, steps per second:  67, episode reward: 36.000, mean reward:  1.500 [-2.901, 32.010], mean action: 1.958 [0.000, 15.000],  loss: 0.020213, mae: 0.317017, mean_q: 0.586099, mean_eps: 0.000000
 1660/5000: episode: 61, duration: 0.267s, episode steps:  18, steps per second:  67, episode reward: 44.426, mean reward:  2.468 [-2.328, 32.414], mean action: 1.944 [0.000, 19.000],  loss: 0.020842, mae: 0.309546, mean_q: 0.575411, mean_eps: 0.000000
 1694/5000: episode: 62, duration: 0.485s, episode steps:  34, steps per second:  70, episode reward: 45.000, mean reward:  1.324 [-2.146, 32.450], mean action: 2.294 [1.000, 19.000],  loss: 0.024939, mae: 0.330728, mean_q: 0.556523, mean_eps: 0.000000
 1728/5000: episode: 63, duration: 0.483s, episode steps:  34, steps per second:  70, episode reward: 34.443, mean reward:  1.013 [-3.000, 31.701], mean action: 7.206 [0.000, 19.000],  loss: 0.021828, mae: 0.317215, mean_q: 0.538376, mean_eps: 0.000000
 1763/5000: episode: 64, duration: 0.501s, episode steps:  35, steps per second:  70, episode reward: 44.020, mean reward:  1.258 [-2.567, 32.450], mean action: 3.000 [0.000, 15.000],  loss: 0.022683, mae: 0.329978, mean_q: 0.550977, mean_eps: 0.000000
 1791/5000: episode: 65, duration: 0.403s, episode steps:  28, steps per second:  69, episode reward: 41.506, mean reward:  1.482 [-2.154, 32.084], mean action: 2.714 [0.000, 19.000],  loss: 0.021635, mae: 0.315488, mean_q: 0.553523, mean_eps: 0.000000
 1835/5000: episode: 66, duration: 0.630s, episode steps:  44, steps per second:  70, episode reward: 44.878, mean reward:  1.020 [-2.221, 32.110], mean action: 3.477 [1.000, 11.000],  loss: 0.022736, mae: 0.334546, mean_q: 0.513128, mean_eps: 0.000000
 1870/5000: episode: 67, duration: 0.503s, episode steps:  35, steps per second:  70, episode reward: 44.622, mean reward:  1.275 [-2.035, 32.180], mean action: 1.743 [0.000, 8.000],  loss: 0.018977, mae: 0.314751, mean_q: 0.627854, mean_eps: 0.000000
 1912/5000: episode: 68, duration: 0.594s, episode steps:  42, steps per second:  71, episode reward: 41.154, mean reward:  0.980 [-2.262, 32.236], mean action: 1.714 [0.000, 11.000],  loss: 0.020053, mae: 0.327849, mean_q: 0.538061, mean_eps: 0.000000
 1947/5000: episode: 69, duration: 0.491s, episode steps:  35, steps per second:  71, episode reward: 37.983, mean reward:  1.085 [-3.000, 32.300], mean action: 5.343 [0.000, 14.000],  loss: 0.019890, mae: 0.322238, mean_q: 0.501710, mean_eps: 0.000000
 1986/5000: episode: 70, duration: 0.554s, episode steps:  39, steps per second:  70, episode reward: 38.806, mean reward:  0.995 [-3.000, 32.080], mean action: 2.077 [0.000, 11.000],  loss: 0.021901, mae: 0.329597, mean_q: 0.590774, mean_eps: 0.000000
 2007/5000: episode: 71, duration: 0.312s, episode steps:  21, steps per second:  67, episode reward: 38.593, mean reward:  1.838 [-3.000, 31.734], mean action: 3.095 [0.000, 12.000],  loss: 0.019064, mae: 0.314998, mean_q: 0.576645, mean_eps: 0.000000
 2025/5000: episode: 72, duration: 0.270s, episode steps:  18, steps per second:  67, episode reward: 44.593, mean reward:  2.477 [-2.151, 32.110], mean action: 2.167 [0.000, 3.000],  loss: 0.018630, mae: 0.310779, mean_q: 0.539801, mean_eps: 0.000000
 2046/5000: episode: 73, duration: 0.303s, episode steps:  21, steps per second:  69, episode reward: 44.333, mean reward:  2.111 [-2.090, 32.480], mean action: 1.619 [0.000, 14.000],  loss: 0.020979, mae: 0.319369, mean_q: 0.546541, mean_eps: 0.000000
 2076/5000: episode: 74, duration: 0.489s, episode steps:  30, steps per second:  61, episode reward: 44.780, mean reward:  1.493 [-2.075, 32.080], mean action: 1.667 [0.000, 12.000],  loss: 0.023384, mae: 0.332899, mean_q: 0.528336, mean_eps: 0.000000
 2131/5000: episode: 75, duration: 0.753s, episode steps:  55, steps per second:  73, episode reward: -35.660, mean reward: -0.648 [-32.218,  2.634], mean action: 4.655 [0.000, 18.000],  loss: 0.020530, mae: 0.317777, mean_q: 0.507990, mean_eps: 0.000000
 2176/5000: episode: 76, duration: 0.644s, episode steps:  45, steps per second:  70, episode reward: 37.786, mean reward:  0.840 [-2.097, 31.981], mean action: 3.356 [0.000, 14.000],  loss: 0.018315, mae: 0.305355, mean_q: 0.566898, mean_eps: 0.000000
 2196/5000: episode: 77, duration: 0.303s, episode steps:  20, steps per second:  66, episode reward: 47.688, mean reward:  2.384 [-0.208, 32.100], mean action: 2.000 [0.000, 14.000],  loss: 0.019435, mae: 0.313136, mean_q: 0.550483, mean_eps: 0.000000
 2222/5000: episode: 78, duration: 0.373s, episode steps:  26, steps per second:  70, episode reward: 36.000, mean reward:  1.385 [-3.000, 32.300], mean action: 3.500 [0.000, 20.000],  loss: 0.020425, mae: 0.310198, mean_q: 0.548589, mean_eps: 0.000000
 2238/5000: episode: 79, duration: 0.267s, episode steps:  16, steps per second:  60, episode reward: 47.314, mean reward:  2.957 [-0.110, 33.000], mean action: 1.375 [0.000, 3.000],  loss: 0.021019, mae: 0.314648, mean_q: 0.546448, mean_eps: 0.000000
 2264/5000: episode: 80, duration: 0.377s, episode steps:  26, steps per second:  69, episode reward: 44.228, mean reward:  1.701 [-2.216, 32.120], mean action: 1.962 [0.000, 12.000],  loss: 0.022326, mae: 0.330543, mean_q: 0.524919, mean_eps: 0.000000
 2283/5000: episode: 81, duration: 0.279s, episode steps:  19, steps per second:  68, episode reward: 41.635, mean reward:  2.191 [-2.178, 32.113], mean action: 0.789 [0.000, 3.000],  loss: 0.020914, mae: 0.316413, mean_q: 0.524635, mean_eps: 0.000000
 2306/5000: episode: 82, duration: 0.333s, episode steps:  23, steps per second:  69, episode reward: 38.018, mean reward:  1.653 [-3.000, 31.702], mean action: 3.565 [0.000, 18.000],  loss: 0.015766, mae: 0.292932, mean_q: 0.525741, mean_eps: 0.000000
 2324/5000: episode: 83, duration: 0.269s, episode steps:  18, steps per second:  67, episode reward: 41.627, mean reward:  2.313 [-3.000, 32.370], mean action: 3.889 [0.000, 16.000],  loss: 0.027023, mae: 0.341219, mean_q: 0.510473, mean_eps: 0.000000
 2355/5000: episode: 84, duration: 0.453s, episode steps:  31, steps per second:  68, episode reward: 38.290, mean reward:  1.235 [-2.805, 32.231], mean action: 2.613 [0.000, 16.000],  loss: 0.020897, mae: 0.308824, mean_q: 0.517952, mean_eps: 0.000000
 2398/5000: episode: 85, duration: 0.606s, episode steps:  43, steps per second:  71, episode reward: 41.288, mean reward:  0.960 [-2.447, 32.160], mean action: 2.674 [0.000, 16.000],  loss: 0.020354, mae: 0.310032, mean_q: 0.520484, mean_eps: 0.000000
 2427/5000: episode: 86, duration: 0.412s, episode steps:  29, steps per second:  70, episode reward: 44.164, mean reward:  1.523 [-2.444, 32.070], mean action: 2.621 [1.000, 3.000],  loss: 0.023202, mae: 0.317206, mean_q: 0.498535, mean_eps: 0.000000
 2446/5000: episode: 87, duration: 0.278s, episode steps:  19, steps per second:  68, episode reward: 41.610, mean reward:  2.190 [-2.454, 32.903], mean action: 2.684 [1.000, 12.000],  loss: 0.021138, mae: 0.310911, mean_q: 0.501810, mean_eps: 0.000000
 2472/5000: episode: 88, duration: 0.385s, episode steps:  26, steps per second:  68, episode reward: 38.453, mean reward:  1.479 [-2.653, 32.009], mean action: 4.077 [0.000, 19.000],  loss: 0.017692, mae: 0.290762, mean_q: 0.508676, mean_eps: 0.000000
 2497/5000: episode: 89, duration: 0.359s, episode steps:  25, steps per second:  70, episode reward: 38.572, mean reward:  1.543 [-2.263, 32.210], mean action: 4.160 [0.000, 20.000],  loss: 0.024095, mae: 0.325458, mean_q: 0.551688, mean_eps: 0.000000
 2522/5000: episode: 90, duration: 0.359s, episode steps:  25, steps per second:  70, episode reward: 44.443, mean reward:  1.778 [-2.461, 31.985], mean action: 0.960 [0.000, 7.000],  loss: 0.022930, mae: 0.323750, mean_q: 0.599648, mean_eps: 0.000000
 2546/5000: episode: 91, duration: 0.346s, episode steps:  24, steps per second:  69, episode reward: 38.565, mean reward:  1.607 [-2.496, 31.885], mean action: 3.458 [0.000, 19.000],  loss: 0.020134, mae: 0.307925, mean_q: 0.594953, mean_eps: 0.000000
 2566/5000: episode: 92, duration: 0.292s, episode steps:  20, steps per second:  69, episode reward: 41.239, mean reward:  2.062 [-2.205, 32.180], mean action: 3.600 [1.000, 19.000],  loss: 0.019099, mae: 0.304384, mean_q: 0.572870, mean_eps: 0.000000
 2584/5000: episode: 93, duration: 0.265s, episode steps:  18, steps per second:  68, episode reward: 41.574, mean reward:  2.310 [-2.435, 32.055], mean action: 3.056 [0.000, 14.000],  loss: 0.022967, mae: 0.324548, mean_q: 0.562356, mean_eps: 0.000000
 2610/5000: episode: 94, duration: 0.370s, episode steps:  26, steps per second:  70, episode reward: 38.141, mean reward:  1.467 [-2.453, 32.458], mean action: 3.885 [0.000, 16.000],  loss: 0.022486, mae: 0.326334, mean_q: 0.586007, mean_eps: 0.000000
 2638/5000: episode: 95, duration: 0.403s, episode steps:  28, steps per second:  69, episode reward: 43.741, mean reward:  1.562 [-2.004, 32.480], mean action: 3.857 [0.000, 19.000],  loss: 0.019542, mae: 0.308434, mean_q: 0.551171, mean_eps: 0.000000
 2675/5000: episode: 96, duration: 0.516s, episode steps:  37, steps per second:  72, episode reward: 33.000, mean reward:  0.892 [-2.396, 32.350], mean action: 4.216 [0.000, 19.000],  loss: 0.020612, mae: 0.316869, mean_q: 0.552413, mean_eps: 0.000000
 2696/5000: episode: 97, duration: 0.307s, episode steps:  21, steps per second:  68, episode reward: 38.106, mean reward:  1.815 [-3.000, 32.340], mean action: 4.667 [0.000, 19.000],  loss: 0.020949, mae: 0.317732, mean_q: 0.514101, mean_eps: 0.000000
 2720/5000: episode: 98, duration: 0.348s, episode steps:  24, steps per second:  69, episode reward: 42.000, mean reward:  1.750 [-2.410, 32.050], mean action: 3.708 [1.000, 20.000],  loss: 0.022710, mae: 0.325136, mean_q: 0.560338, mean_eps: 0.000000
 2747/5000: episode: 99, duration: 0.390s, episode steps:  27, steps per second:  69, episode reward: 38.466, mean reward:  1.425 [-3.000, 32.621], mean action: 4.259 [0.000, 19.000],  loss: 0.019753, mae: 0.311941, mean_q: 0.548400, mean_eps: 0.000000
 2764/5000: episode: 100, duration: 0.263s, episode steps:  17, steps per second:  65, episode reward: 42.000, mean reward:  2.471 [-2.267, 32.460], mean action: 4.765 [0.000, 19.000],  loss: 0.020861, mae: 0.316439, mean_q: 0.538203, mean_eps: 0.000000
 2784/5000: episode: 101, duration: 0.299s, episode steps:  20, steps per second:  67, episode reward: 44.436, mean reward:  2.222 [-2.102, 32.458], mean action: 2.900 [0.000, 19.000],  loss: 0.022791, mae: 0.321300, mean_q: 0.568708, mean_eps: 0.000000
 2815/5000: episode: 102, duration: 0.441s, episode steps:  31, steps per second:  70, episode reward: 38.700, mean reward:  1.248 [-2.492, 32.450], mean action: 3.903 [0.000, 19.000],  loss: 0.023014, mae: 0.328487, mean_q: 0.549571, mean_eps: 0.000000
 2842/5000: episode: 103, duration: 0.385s, episode steps:  27, steps per second:  70, episode reward: 35.614, mean reward:  1.319 [-3.000, 32.140], mean action: 4.444 [0.000, 11.000],  loss: 0.022056, mae: 0.325889, mean_q: 0.571839, mean_eps: 0.000000
 2862/5000: episode: 104, duration: 0.290s, episode steps:  20, steps per second:  69, episode reward: 41.177, mean reward:  2.059 [-2.126, 32.240], mean action: 3.450 [0.000, 19.000],  loss: 0.018444, mae: 0.312674, mean_q: 0.481595, mean_eps: 0.000000
 2887/5000: episode: 105, duration: 0.357s, episode steps:  25, steps per second:  70, episode reward: 35.437, mean reward:  1.417 [-3.000, 32.120], mean action: 3.520 [0.000, 19.000],  loss: 0.016186, mae: 0.301624, mean_q: 0.502482, mean_eps: 0.000000
 2908/5000: episode: 106, duration: 0.305s, episode steps:  21, steps per second:  69, episode reward: 36.000, mean reward:  1.714 [-2.903, 32.370], mean action: 4.524 [0.000, 12.000],  loss: 0.018941, mae: 0.305675, mean_q: 0.501072, mean_eps: 0.000000
 2940/5000: episode: 107, duration: 0.448s, episode steps:  32, steps per second:  71, episode reward: 38.780, mean reward:  1.212 [-2.264, 32.352], mean action: 3.500 [0.000, 16.000],  loss: 0.021381, mae: 0.321102, mean_q: 0.550502, mean_eps: 0.000000
 2959/5000: episode: 108, duration: 0.278s, episode steps:  19, steps per second:  68, episode reward: 44.142, mean reward:  2.323 [-2.191, 32.240], mean action: 1.474 [0.000, 3.000],  loss: 0.020784, mae: 0.308988, mean_q: 0.485191, mean_eps: 0.000000
 3001/5000: episode: 109, duration: 0.585s, episode steps:  42, steps per second:  72, episode reward: 37.424, mean reward:  0.891 [-3.000, 32.140], mean action: 3.095 [0.000, 19.000],  loss: 0.020601, mae: 0.314780, mean_q: 0.504563, mean_eps: 0.000000
 3038/5000: episode: 110, duration: 0.558s, episode steps:  37, steps per second:  66, episode reward: 40.621, mean reward:  1.098 [-2.010, 29.931], mean action: 4.135 [0.000, 19.000],  loss: 0.021432, mae: 0.317231, mean_q: 0.537961, mean_eps: 0.000000
 3063/5000: episode: 111, duration: 0.363s, episode steps:  25, steps per second:  69, episode reward: 41.424, mean reward:  1.657 [-2.240, 32.080], mean action: 4.160 [0.000, 16.000],  loss: 0.022038, mae: 0.319758, mean_q: 0.513315, mean_eps: 0.000000
 3087/5000: episode: 112, duration: 0.342s, episode steps:  24, steps per second:  70, episode reward: 38.870, mean reward:  1.620 [-2.969, 32.210], mean action: 5.000 [0.000, 16.000],  loss: 0.021758, mae: 0.310610, mean_q: 0.468814, mean_eps: 0.000000
 3113/5000: episode: 113, duration: 0.374s, episode steps:  26, steps per second:  70, episode reward: 43.336, mean reward:  1.667 [-2.089, 31.813], mean action: 3.962 [0.000, 13.000],  loss: 0.023581, mae: 0.322803, mean_q: 0.541880, mean_eps: 0.000000
 3151/5000: episode: 114, duration: 0.535s, episode steps:  38, steps per second:  71, episode reward: 34.935, mean reward:  0.919 [-2.513, 31.863], mean action: 5.605 [0.000, 18.000],  loss: 0.021014, mae: 0.312692, mean_q: 0.512290, mean_eps: 0.000000
 3180/5000: episode: 115, duration: 0.422s, episode steps:  29, steps per second:  69, episode reward: 44.249, mean reward:  1.526 [-2.120, 32.150], mean action: 3.000 [0.000, 15.000],  loss: 0.020637, mae: 0.314798, mean_q: 0.496737, mean_eps: 0.000000
 3203/5000: episode: 116, duration: 0.343s, episode steps:  23, steps per second:  67, episode reward: 44.159, mean reward:  1.920 [-2.296, 32.440], mean action: 2.174 [0.000, 12.000],  loss: 0.020852, mae: 0.316081, mean_q: 0.460321, mean_eps: 0.000000
 3235/5000: episode: 117, duration: 0.543s, episode steps:  32, steps per second:  59, episode reward: 38.207, mean reward:  1.194 [-2.883, 31.577], mean action: 2.844 [0.000, 16.000],  loss: 0.023331, mae: 0.325261, mean_q: 0.493633, mean_eps: 0.000000
 3247/5000: episode: 118, duration: 0.193s, episode steps:  12, steps per second:  62, episode reward: 47.004, mean reward:  3.917 [-0.996, 33.000], mean action: 1.333 [0.000, 3.000],  loss: 0.021981, mae: 0.322904, mean_q: 0.501183, mean_eps: 0.000000
 3269/5000: episode: 119, duration: 0.320s, episode steps:  22, steps per second:  69, episode reward: 44.598, mean reward:  2.027 [-2.886, 32.600], mean action: 2.455 [0.000, 16.000],  loss: 0.020774, mae: 0.314957, mean_q: 0.530963, mean_eps: 0.000000
 3297/5000: episode: 120, duration: 0.399s, episode steps:  28, steps per second:  70, episode reward: 40.801, mean reward:  1.457 [-3.000, 31.870], mean action: 4.929 [0.000, 20.000],  loss: 0.017461, mae: 0.302739, mean_q: 0.480719, mean_eps: 0.000000
 3319/5000: episode: 121, duration: 0.320s, episode steps:  22, steps per second:  69, episode reward: 40.646, mean reward:  1.848 [-2.670, 32.810], mean action: 2.682 [0.000, 16.000],  loss: 0.016978, mae: 0.295579, mean_q: 0.482990, mean_eps: 0.000000
 3351/5000: episode: 122, duration: 0.466s, episode steps:  32, steps per second:  69, episode reward: 44.474, mean reward:  1.390 [-2.075, 32.083], mean action: 2.750 [2.000, 3.000],  loss: 0.022868, mae: 0.315491, mean_q: 0.510725, mean_eps: 0.000000
 3380/5000: episode: 123, duration: 0.421s, episode steps:  29, steps per second:  69, episode reward: 41.340, mean reward:  1.426 [-2.766, 32.070], mean action: 1.931 [0.000, 12.000],  loss: 0.023113, mae: 0.314674, mean_q: 0.504156, mean_eps: 0.000000
 3404/5000: episode: 124, duration: 0.343s, episode steps:  24, steps per second:  70, episode reward: 38.790, mean reward:  1.616 [-2.475, 32.070], mean action: 2.625 [0.000, 18.000],  loss: 0.019185, mae: 0.296369, mean_q: 0.486505, mean_eps: 0.000000
 3430/5000: episode: 125, duration: 0.378s, episode steps:  26, steps per second:  69, episode reward: 42.000, mean reward:  1.615 [-2.092, 32.740], mean action: 2.731 [0.000, 19.000],  loss: 0.021000, mae: 0.303083, mean_q: 0.464048, mean_eps: 0.000000
 3444/5000: episode: 126, duration: 0.223s, episode steps:  14, steps per second:  63, episode reward: 47.836, mean reward:  3.417 [ 0.059, 32.780], mean action: 2.500 [2.000, 3.000],  loss: 0.020284, mae: 0.295181, mean_q: 0.469194, mean_eps: 0.000000
 3477/5000: episode: 127, duration: 0.476s, episode steps:  33, steps per second:  69, episode reward: 41.214, mean reward:  1.249 [-2.295, 32.170], mean action: 2.818 [0.000, 12.000],  loss: 0.020940, mae: 0.303212, mean_q: 0.500284, mean_eps: 0.000000
 3493/5000: episode: 128, duration: 0.243s, episode steps:  16, steps per second:  66, episode reward: 47.343, mean reward:  2.959 [ 0.000, 31.898], mean action: 2.375 [1.000, 13.000],  loss: 0.021605, mae: 0.306091, mean_q: 0.495860, mean_eps: 0.000000
 3509/5000: episode: 129, duration: 0.243s, episode steps:  16, steps per second:  66, episode reward: 44.410, mean reward:  2.776 [-2.900, 32.300], mean action: 2.062 [0.000, 11.000],  loss: 0.020406, mae: 0.301624, mean_q: 0.478219, mean_eps: 0.000000
 3533/5000: episode: 130, duration: 0.341s, episode steps:  24, steps per second:  70, episode reward: 38.264, mean reward:  1.594 [-3.000, 31.834], mean action: 2.833 [0.000, 11.000],  loss: 0.020493, mae: 0.303286, mean_q: 0.470358, mean_eps: 0.000000
 3553/5000: episode: 131, duration: 0.297s, episode steps:  20, steps per second:  67, episode reward: 40.841, mean reward:  2.042 [-2.561, 32.290], mean action: 5.550 [0.000, 16.000],  loss: 0.017422, mae: 0.284674, mean_q: 0.485194, mean_eps: 0.000000
 3576/5000: episode: 132, duration: 0.335s, episode steps:  23, steps per second:  69, episode reward: 42.000, mean reward:  1.826 [-2.014, 30.269], mean action: 1.739 [0.000, 16.000],  loss: 0.022145, mae: 0.305816, mean_q: 0.503247, mean_eps: 0.000000
 3604/5000: episode: 133, duration: 0.404s, episode steps:  28, steps per second:  69, episode reward: 41.460, mean reward:  1.481 [-2.145, 32.440], mean action: 3.607 [0.000, 16.000],  loss: 0.020153, mae: 0.302042, mean_q: 0.551030, mean_eps: 0.000000
 3626/5000: episode: 134, duration: 0.319s, episode steps:  22, steps per second:  69, episode reward: 41.343, mean reward:  1.879 [-2.217, 32.813], mean action: 2.636 [0.000, 16.000],  loss: 0.017761, mae: 0.294569, mean_q: 0.542805, mean_eps: 0.000000
 3646/5000: episode: 135, duration: 0.295s, episode steps:  20, steps per second:  68, episode reward: 44.446, mean reward:  2.222 [-2.091, 32.240], mean action: 1.250 [0.000, 9.000],  loss: 0.020012, mae: 0.308330, mean_q: 0.585192, mean_eps: 0.000000
 3666/5000: episode: 136, duration: 0.296s, episode steps:  20, steps per second:  68, episode reward: 41.347, mean reward:  2.067 [-3.000, 32.170], mean action: 1.350 [0.000, 12.000],  loss: 0.024919, mae: 0.325235, mean_q: 0.575069, mean_eps: 0.000000
 3689/5000: episode: 137, duration: 0.334s, episode steps:  23, steps per second:  69, episode reward: 41.611, mean reward:  1.809 [-3.000, 32.083], mean action: 3.087 [0.000, 15.000],  loss: 0.018254, mae: 0.295845, mean_q: 0.530556, mean_eps: 0.000000
 3709/5000: episode: 138, duration: 0.503s, episode steps:  20, steps per second:  40, episode reward: 41.163, mean reward:  2.058 [-2.322, 31.809], mean action: 3.750 [0.000, 15.000],  loss: 0.017171, mae: 0.285404, mean_q: 0.550245, mean_eps: 0.000000
 3728/5000: episode: 139, duration: 0.293s, episode steps:  19, steps per second:  65, episode reward: 41.043, mean reward:  2.160 [-2.455, 32.184], mean action: 4.789 [0.000, 15.000],  loss: 0.022079, mae: 0.313963, mean_q: 0.521789, mean_eps: 0.000000
 3759/5000: episode: 140, duration: 0.447s, episode steps:  31, steps per second:  69, episode reward: 41.828, mean reward:  1.349 [-2.218, 32.788], mean action: 2.742 [0.000, 15.000],  loss: 0.020476, mae: 0.305742, mean_q: 0.522181, mean_eps: 0.000000
 3781/5000: episode: 141, duration: 0.315s, episode steps:  22, steps per second:  70, episode reward: 45.000, mean reward:  2.045 [-2.319, 32.300], mean action: 1.909 [0.000, 15.000],  loss: 0.018397, mae: 0.295966, mean_q: 0.550119, mean_eps: 0.000000
 3811/5000: episode: 142, duration: 0.427s, episode steps:  30, steps per second:  70, episode reward: 43.035, mean reward:  1.435 [-2.625, 31.435], mean action: 3.767 [1.000, 16.000],  loss: 0.021350, mae: 0.315087, mean_q: 0.476138, mean_eps: 0.000000
 3828/5000: episode: 143, duration: 0.265s, episode steps:  17, steps per second:  64, episode reward: 38.686, mean reward:  2.276 [-2.840, 32.242], mean action: 6.647 [0.000, 16.000],  loss: 0.019573, mae: 0.300128, mean_q: 0.477941, mean_eps: 0.000000
 3858/5000: episode: 144, duration: 0.509s, episode steps:  30, steps per second:  59, episode reward: 38.003, mean reward:  1.267 [-2.390, 32.102], mean action: 3.800 [0.000, 16.000],  loss: 0.018353, mae: 0.300411, mean_q: 0.527770, mean_eps: 0.000000
 3886/5000: episode: 145, duration: 0.403s, episode steps:  28, steps per second:  69, episode reward: 40.811, mean reward:  1.458 [-2.387, 32.330], mean action: 3.964 [0.000, 20.000],  loss: 0.019397, mae: 0.301593, mean_q: 0.564006, mean_eps: 0.000000
 3928/5000: episode: 146, duration: 0.590s, episode steps:  42, steps per second:  71, episode reward: 32.581, mean reward:  0.776 [-2.900, 32.050], mean action: 3.833 [0.000, 16.000],  loss: 0.018836, mae: 0.300464, mean_q: 0.501288, mean_eps: 0.000000
 3948/5000: episode: 147, duration: 0.289s, episode steps:  20, steps per second:  69, episode reward: 42.000, mean reward:  2.100 [-2.247, 32.670], mean action: 2.350 [0.000, 16.000],  loss: 0.020161, mae: 0.293765, mean_q: 0.491704, mean_eps: 0.000000
 3981/5000: episode: 148, duration: 0.471s, episode steps:  33, steps per second:  70, episode reward: 38.585, mean reward:  1.169 [-2.396, 32.739], mean action: 2.636 [0.000, 11.000],  loss: 0.019101, mae: 0.289312, mean_q: 0.527245, mean_eps: 0.000000
 4011/5000: episode: 149, duration: 0.432s, episode steps:  30, steps per second:  69, episode reward: 41.346, mean reward:  1.378 [-2.314, 32.210], mean action: 2.533 [0.000, 16.000],  loss: 0.022085, mae: 0.308102, mean_q: 0.509487, mean_eps: 0.000000
 4034/5000: episode: 150, duration: 0.332s, episode steps:  23, steps per second:  69, episode reward: 42.000, mean reward:  1.826 [-2.281, 32.340], mean action: 2.043 [1.000, 5.000],  loss: 0.023099, mae: 0.315453, mean_q: 0.524922, mean_eps: 0.000000
 4055/5000: episode: 151, duration: 0.314s, episode steps:  21, steps per second:  67, episode reward: 44.054, mean reward:  2.098 [-2.137, 32.340], mean action: 2.095 [0.000, 3.000],  loss: 0.022655, mae: 0.319514, mean_q: 0.509564, mean_eps: 0.000000
 4081/5000: episode: 152, duration: 0.383s, episode steps:  26, steps per second:  68, episode reward: 43.547, mean reward:  1.675 [-2.158, 31.867], mean action: 2.269 [0.000, 11.000],  loss: 0.017063, mae: 0.297571, mean_q: 0.511219, mean_eps: 0.000000
 4115/5000: episode: 153, duration: 0.478s, episode steps:  34, steps per second:  71, episode reward: 41.903, mean reward:  1.232 [-2.528, 32.593], mean action: 1.324 [0.000, 11.000],  loss: 0.020747, mae: 0.312994, mean_q: 0.512307, mean_eps: 0.000000
 4165/5000: episode: 154, duration: 0.698s, episode steps:  50, steps per second:  72, episode reward: -32.250, mean reward: -0.645 [-31.731,  3.000], mean action: 4.460 [0.000, 19.000],  loss: 0.021852, mae: 0.313961, mean_q: 0.477296, mean_eps: 0.000000
 4200/5000: episode: 155, duration: 0.499s, episode steps:  35, steps per second:  70, episode reward: 39.000, mean reward:  1.114 [-2.780, 32.290], mean action: 3.400 [0.000, 15.000],  loss: 0.020973, mae: 0.310011, mean_q: 0.515489, mean_eps: 0.000000
 4223/5000: episode: 156, duration: 0.338s, episode steps:  23, steps per second:  68, episode reward: 38.185, mean reward:  1.660 [-2.940, 32.260], mean action: 2.217 [0.000, 11.000],  loss: 0.023718, mae: 0.329646, mean_q: 0.527887, mean_eps: 0.000000
 4246/5000: episode: 157, duration: 0.329s, episode steps:  23, steps per second:  70, episode reward: 38.274, mean reward:  1.664 [-3.000, 32.350], mean action: 3.739 [0.000, 19.000],  loss: 0.019497, mae: 0.303928, mean_q: 0.543901, mean_eps: 0.000000
 4265/5000: episode: 158, duration: 0.340s, episode steps:  19, steps per second:  56, episode reward: 41.047, mean reward:  2.160 [-2.894, 32.082], mean action: 3.105 [0.000, 19.000],  loss: 0.024071, mae: 0.331868, mean_q: 0.562906, mean_eps: 0.000000
 4318/5000: episode: 159, duration: 0.731s, episode steps:  53, steps per second:  73, episode reward: 33.000, mean reward:  0.623 [-2.477, 32.140], mean action: 2.981 [0.000, 15.000],  loss: 0.020934, mae: 0.321683, mean_q: 0.537182, mean_eps: 0.000000
 4345/5000: episode: 160, duration: 0.388s, episode steps:  27, steps per second:  70, episode reward: 38.238, mean reward:  1.416 [-2.487, 31.876], mean action: 3.889 [1.000, 15.000],  loss: 0.019895, mae: 0.312695, mean_q: 0.487843, mean_eps: 0.000000
 4395/5000: episode: 161, duration: 0.697s, episode steps:  50, steps per second:  72, episode reward: 42.884, mean reward:  0.858 [-2.065, 32.191], mean action: 1.660 [0.000, 7.000],  loss: 0.019418, mae: 0.301780, mean_q: 0.488036, mean_eps: 0.000000
 4419/5000: episode: 162, duration: 0.342s, episode steps:  24, steps per second:  70, episode reward: 38.835, mean reward:  1.618 [-2.485, 32.155], mean action: 3.125 [0.000, 19.000],  loss: 0.020215, mae: 0.305743, mean_q: 0.522713, mean_eps: 0.000000
 4443/5000: episode: 163, duration: 0.352s, episode steps:  24, steps per second:  68, episode reward: 44.356, mean reward:  1.848 [-2.459, 32.360], mean action: 3.083 [0.000, 14.000],  loss: 0.020093, mae: 0.305182, mean_q: 0.507469, mean_eps: 0.000000
 4487/5000: episode: 164, duration: 0.607s, episode steps:  44, steps per second:  73, episode reward: 35.915, mean reward:  0.816 [-3.000, 32.915], mean action: 2.341 [0.000, 14.000],  loss: 0.022859, mae: 0.320334, mean_q: 0.513867, mean_eps: 0.000000
 4516/5000: episode: 165, duration: 0.411s, episode steps:  29, steps per second:  71, episode reward: 35.900, mean reward:  1.238 [-2.385, 32.280], mean action: 2.379 [0.000, 9.000],  loss: 0.018259, mae: 0.292175, mean_q: 0.532473, mean_eps: 0.000000
 4536/5000: episode: 166, duration: 0.300s, episode steps:  20, steps per second:  67, episode reward: 46.892, mean reward:  2.345 [-0.063, 32.250], mean action: 2.150 [0.000, 20.000],  loss: 0.022366, mae: 0.313262, mean_q: 0.514790, mean_eps: 0.000000
 4557/5000: episode: 167, duration: 0.311s, episode steps:  21, steps per second:  67, episode reward: 39.000, mean reward:  1.857 [-2.940, 32.340], mean action: 5.714 [0.000, 19.000],  loss: 0.023520, mae: 0.318848, mean_q: 0.535061, mean_eps: 0.000000
 4597/5000: episode: 168, duration: 0.560s, episode steps:  40, steps per second:  71, episode reward: 44.397, mean reward:  1.110 [-2.317, 32.050], mean action: 2.075 [0.000, 20.000],  loss: 0.021023, mae: 0.312242, mean_q: 0.539652, mean_eps: 0.000000
 4616/5000: episode: 169, duration: 0.281s, episode steps:  19, steps per second:  68, episode reward: 41.136, mean reward:  2.165 [-3.000, 32.350], mean action: 3.211 [0.000, 16.000],  loss: 0.021629, mae: 0.320031, mean_q: 0.489861, mean_eps: 0.000000
 4647/5000: episode: 170, duration: 0.528s, episode steps:  31, steps per second:  59, episode reward: 43.992, mean reward:  1.419 [-2.269, 31.964], mean action: 2.548 [0.000, 5.000],  loss: 0.022656, mae: 0.321308, mean_q: 0.485686, mean_eps: 0.000000
 4674/5000: episode: 171, duration: 0.387s, episode steps:  27, steps per second:  70, episode reward: 36.000, mean reward:  1.333 [-3.000, 32.470], mean action: 2.852 [0.000, 12.000],  loss: 0.021231, mae: 0.311592, mean_q: 0.543062, mean_eps: 0.000000
 4706/5000: episode: 172, duration: 0.454s, episode steps:  32, steps per second:  70, episode reward: 44.323, mean reward:  1.385 [-2.443, 32.330], mean action: 3.375 [0.000, 20.000],  loss: 0.020105, mae: 0.310747, mean_q: 0.582807, mean_eps: 0.000000
 4718/5000: episode: 173, duration: 0.203s, episode steps:  12, steps per second:  59, episode reward: 47.170, mean reward:  3.931 [-0.246, 31.971], mean action: 2.500 [0.000, 20.000],  loss: 0.025614, mae: 0.336130, mean_q: 0.580488, mean_eps: 0.000000
 4740/5000: episode: 174, duration: 0.327s, episode steps:  22, steps per second:  67, episode reward: 43.580, mean reward:  1.981 [-2.355, 32.205], mean action: 2.364 [0.000, 8.000],  loss: 0.019313, mae: 0.307007, mean_q: 0.566134, mean_eps: 0.000000
 4765/5000: episode: 175, duration: 0.361s, episode steps:  25, steps per second:  69, episode reward: 44.653, mean reward:  1.786 [-2.065, 32.180], mean action: 1.800 [1.000, 6.000],  loss: 0.021344, mae: 0.304233, mean_q: 0.542716, mean_eps: 0.000000
 4803/5000: episode: 176, duration: 0.532s, episode steps:  38, steps per second:  71, episode reward: 32.448, mean reward:  0.854 [-2.805, 32.040], mean action: 5.447 [0.000, 18.000],  loss: 0.019256, mae: 0.297754, mean_q: 0.518492, mean_eps: 0.000000
 4827/5000: episode: 177, duration: 0.349s, episode steps:  24, steps per second:  69, episode reward: 43.455, mean reward:  1.811 [-2.853, 32.010], mean action: 4.167 [0.000, 15.000],  loss: 0.019520, mae: 0.299359, mean_q: 0.524428, mean_eps: 0.000000
 4849/5000: episode: 178, duration: 0.318s, episode steps:  22, steps per second:  69, episode reward: 42.000, mean reward:  1.909 [-2.230, 32.120], mean action: 2.091 [0.000, 6.000],  loss: 0.023259, mae: 0.316121, mean_q: 0.510756, mean_eps: 0.000000
 4870/5000: episode: 179, duration: 0.308s, episode steps:  21, steps per second:  68, episode reward: 44.630, mean reward:  2.125 [-2.783, 32.060], mean action: 1.905 [0.000, 6.000],  loss: 0.021398, mae: 0.306247, mean_q: 0.505232, mean_eps: 0.000000
 4886/5000: episode: 180, duration: 0.244s, episode steps:  16, steps per second:  66, episode reward: 44.781, mean reward:  2.799 [-2.110, 32.110], mean action: 4.312 [0.000, 19.000],  loss: 0.024229, mae: 0.324331, mean_q: 0.514561, mean_eps: 0.000000
 4899/5000: episode: 181, duration: 0.201s, episode steps:  13, steps per second:  65, episode reward: 45.000, mean reward:  3.462 [-2.176, 32.420], mean action: 1.615 [0.000, 12.000],  loss: 0.020765, mae: 0.305887, mean_q: 0.533232, mean_eps: 0.000000
 4935/5000: episode: 182, duration: 0.508s, episode steps:  36, steps per second:  71, episode reward: 32.224, mean reward:  0.895 [-3.000, 31.993], mean action: 7.250 [0.000, 21.000],  loss: 0.023135, mae: 0.315262, mean_q: 0.536253, mean_eps: 0.000000
 4961/5000: episode: 183, duration: 0.369s, episode steps:  26, steps per second:  70, episode reward: 35.420, mean reward:  1.362 [-3.000, 32.103], mean action: 5.038 [0.000, 15.000],  loss: 0.020832, mae: 0.302441, mean_q: 0.519675, mean_eps: 0.000000
done, took 65.989 seconds
DQN Evaluation: 11799 victories out of 13738 episodes
Training for 5000 steps ...
   22/5000: episode: 1, duration: 0.194s, episode steps:  22, steps per second: 113, episode reward: -32.230, mean reward: -1.465 [-32.006,  2.670], mean action: 6.773 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   44/5000: episode: 2, duration: 0.162s, episode steps:  22, steps per second: 135, episode reward: 41.240, mean reward:  1.875 [-2.255, 32.410], mean action: 5.773 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   61/5000: episode: 3, duration: 0.127s, episode steps:  17, steps per second: 134, episode reward: 35.134, mean reward:  2.067 [-3.000, 32.903], mean action: 4.941 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   91/5000: episode: 4, duration: 0.199s, episode steps:  30, steps per second: 151, episode reward: -32.780, mean reward: -1.093 [-32.415,  2.400], mean action: 3.967 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  114/5000: episode: 5, duration: 0.162s, episode steps:  23, steps per second: 142, episode reward: 32.228, mean reward:  1.401 [-3.000, 32.186], mean action: 7.522 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  131/5000: episode: 6, duration: 0.139s, episode steps:  17, steps per second: 122, episode reward: 38.753, mean reward:  2.280 [-2.524, 32.951], mean action: 5.176 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  165/5000: episode: 7, duration: 0.223s, episode steps:  34, steps per second: 152, episode reward: 41.034, mean reward:  1.207 [-2.545, 31.882], mean action: 5.000 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  191/5000: episode: 8, duration: 0.177s, episode steps:  26, steps per second: 147, episode reward: -32.410, mean reward: -1.247 [-32.500,  2.673], mean action: 5.692 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  209/5000: episode: 9, duration: 0.129s, episode steps:  18, steps per second: 139, episode reward: 32.756, mean reward:  1.820 [-3.000, 32.756], mean action: 7.056 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  229/5000: episode: 10, duration: 0.143s, episode steps:  20, steps per second: 140, episode reward: 38.357, mean reward:  1.918 [-2.245, 31.647], mean action: 5.750 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  254/5000: episode: 11, duration: 0.171s, episode steps:  25, steps per second: 147, episode reward: 32.041, mean reward:  1.282 [-3.000, 31.844], mean action: 4.080 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  310/5000: episode: 12, duration: 0.380s, episode steps:  56, steps per second: 147, episode reward: -33.000, mean reward: -0.589 [-32.025,  2.900], mean action: 6.214 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  337/5000: episode: 13, duration: 0.182s, episode steps:  27, steps per second: 148, episode reward: 35.168, mean reward:  1.303 [-2.472, 32.284], mean action: 5.704 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  369/5000: episode: 14, duration: 0.204s, episode steps:  32, steps per second: 157, episode reward: -32.750, mean reward: -1.023 [-32.067,  2.780], mean action: 6.250 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  393/5000: episode: 15, duration: 0.166s, episode steps:  24, steps per second: 145, episode reward: 40.543, mean reward:  1.689 [-2.902, 31.987], mean action: 4.958 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  416/5000: episode: 16, duration: 0.152s, episode steps:  23, steps per second: 152, episode reward: -35.710, mean reward: -1.553 [-32.062,  2.340], mean action: 6.043 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  443/5000: episode: 17, duration: 0.176s, episode steps:  27, steps per second: 153, episode reward: 32.322, mean reward:  1.197 [-2.656, 31.740], mean action: 7.778 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  467/5000: episode: 18, duration: 0.161s, episode steps:  24, steps per second: 149, episode reward: 38.586, mean reward:  1.608 [-2.457, 32.301], mean action: 2.917 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  495/5000: episode: 19, duration: 0.190s, episode steps:  28, steps per second: 148, episode reward: 37.860, mean reward:  1.352 [-2.320, 32.664], mean action: 5.500 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  520/5000: episode: 20, duration: 0.163s, episode steps:  25, steps per second: 153, episode reward: 32.459, mean reward:  1.298 [-3.000, 32.310], mean action: 3.520 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  533/5000: episode: 21, duration: 0.107s, episode steps:  13, steps per second: 121, episode reward: 41.413, mean reward:  3.186 [-2.266, 31.913], mean action: 3.308 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  559/5000: episode: 22, duration: 0.173s, episode steps:  26, steps per second: 150, episode reward: 32.653, mean reward:  1.256 [-2.394, 32.053], mean action: 4.808 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  584/5000: episode: 23, duration: 0.171s, episode steps:  25, steps per second: 146, episode reward: -32.590, mean reward: -1.304 [-32.406,  2.310], mean action: 4.720 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  604/5000: episode: 24, duration: 0.141s, episode steps:  20, steps per second: 142, episode reward: 41.734, mean reward:  2.087 [-2.424, 32.150], mean action: 3.100 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  626/5000: episode: 25, duration: 0.150s, episode steps:  22, steps per second: 146, episode reward: 35.845, mean reward:  1.629 [-3.000, 32.550], mean action: 5.136 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  669/5000: episode: 26, duration: 0.277s, episode steps:  43, steps per second: 155, episode reward: -38.460, mean reward: -0.894 [-33.000,  2.601], mean action: 5.209 [2.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  686/5000: episode: 27, duration: 0.122s, episode steps:  17, steps per second: 139, episode reward: 38.117, mean reward:  2.242 [-3.000, 32.561], mean action: 5.647 [2.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  702/5000: episode: 28, duration: 0.115s, episode steps:  16, steps per second: 139, episode reward: 38.522, mean reward:  2.408 [-2.264, 32.432], mean action: 5.312 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  720/5000: episode: 29, duration: 0.127s, episode steps:  18, steps per second: 141, episode reward: 41.079, mean reward:  2.282 [-2.116, 32.140], mean action: 4.056 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  741/5000: episode: 30, duration: 0.142s, episode steps:  21, steps per second: 147, episode reward: 38.750, mean reward:  1.845 [-2.275, 32.251], mean action: 7.429 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  757/5000: episode: 31, duration: 0.125s, episode steps:  16, steps per second: 129, episode reward: 39.000, mean reward:  2.438 [-2.651, 33.000], mean action: 5.438 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  775/5000: episode: 32, duration: 0.128s, episode steps:  18, steps per second: 141, episode reward: 39.000, mean reward:  2.167 [-2.289, 33.000], mean action: 3.889 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  797/5000: episode: 33, duration: 0.153s, episode steps:  22, steps per second: 143, episode reward: -35.210, mean reward: -1.600 [-32.113,  2.532], mean action: 4.773 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  817/5000: episode: 34, duration: 0.135s, episode steps:  20, steps per second: 148, episode reward: 32.220, mean reward:  1.611 [-3.000, 32.473], mean action: 4.400 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  835/5000: episode: 35, duration: 0.231s, episode steps:  18, steps per second:  78, episode reward: -38.900, mean reward: -2.161 [-32.247,  2.411], mean action: 5.167 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  858/5000: episode: 36, duration: 0.317s, episode steps:  23, steps per second:  73, episode reward: 38.762, mean reward:  1.685 [-2.339, 32.430], mean action: 3.826 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  880/5000: episode: 37, duration: 0.145s, episode steps:  22, steps per second: 152, episode reward: 38.739, mean reward:  1.761 [-2.374, 32.240], mean action: 5.136 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  901/5000: episode: 38, duration: 0.148s, episode steps:  21, steps per second: 142, episode reward: 35.262, mean reward:  1.679 [-2.274, 32.070], mean action: 4.000 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  934/5000: episode: 39, duration: 0.211s, episode steps:  33, steps per second: 157, episode reward: -32.250, mean reward: -0.977 [-32.033,  2.903], mean action: 6.545 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  958/5000: episode: 40, duration: 0.163s, episode steps:  24, steps per second: 147, episode reward: 35.124, mean reward:  1.463 [-2.199, 32.413], mean action: 4.042 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  982/5000: episode: 41, duration: 0.159s, episode steps:  24, steps per second: 151, episode reward: 35.451, mean reward:  1.477 [-2.734, 33.000], mean action: 5.125 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1002/5000: episode: 42, duration: 0.155s, episode steps:  20, steps per second: 129, episode reward: 35.128, mean reward:  1.756 [-3.000, 32.226], mean action: 5.550 [0.000, 19.000],  loss: 0.008773, mae: 0.261904, mean_q: 0.507350, mean_eps: 0.000000
 1025/5000: episode: 43, duration: 0.332s, episode steps:  23, steps per second:  69, episode reward: 38.111, mean reward:  1.657 [-3.000, 32.034], mean action: 5.609 [0.000, 16.000],  loss: 0.019116, mae: 0.301435, mean_q: 0.521330, mean_eps: 0.000000
 1046/5000: episode: 44, duration: 0.318s, episode steps:  21, steps per second:  66, episode reward: 36.000, mean reward:  1.714 [-2.395, 32.200], mean action: 5.238 [0.000, 16.000],  loss: 0.018726, mae: 0.295542, mean_q: 0.508118, mean_eps: 0.000000
 1095/5000: episode: 45, duration: 0.670s, episode steps:  49, steps per second:  73, episode reward: 32.503, mean reward:  0.663 [-3.000, 32.157], mean action: 11.673 [1.000, 18.000],  loss: 0.017616, mae: 0.288850, mean_q: 0.497883, mean_eps: 0.000000
 1118/5000: episode: 46, duration: 0.332s, episode steps:  23, steps per second:  69, episode reward: 35.771, mean reward:  1.555 [-3.000, 32.020], mean action: 3.565 [0.000, 16.000],  loss: 0.020168, mae: 0.303013, mean_q: 0.474846, mean_eps: 0.000000
 1146/5000: episode: 47, duration: 0.399s, episode steps:  28, steps per second:  70, episode reward: 35.739, mean reward:  1.276 [-2.287, 32.040], mean action: 7.321 [0.000, 16.000],  loss: 0.020865, mae: 0.304297, mean_q: 0.478708, mean_eps: 0.000000
 1166/5000: episode: 48, duration: 0.301s, episode steps:  20, steps per second:  66, episode reward: 44.352, mean reward:  2.218 [-2.318, 32.250], mean action: 3.050 [2.000, 6.000],  loss: 0.018953, mae: 0.298305, mean_q: 0.498638, mean_eps: 0.000000
 1190/5000: episode: 49, duration: 0.345s, episode steps:  24, steps per second:  70, episode reward: 34.693, mean reward:  1.446 [-2.651, 32.130], mean action: 5.333 [0.000, 16.000],  loss: 0.018887, mae: 0.293902, mean_q: 0.489208, mean_eps: 0.000000
 1217/5000: episode: 50, duration: 0.387s, episode steps:  27, steps per second:  70, episode reward: 32.905, mean reward:  1.219 [-2.399, 31.975], mean action: 5.222 [0.000, 16.000],  loss: 0.019757, mae: 0.294869, mean_q: 0.468692, mean_eps: 0.000000
 1244/5000: episode: 51, duration: 0.384s, episode steps:  27, steps per second:  70, episode reward: -32.330, mean reward: -1.197 [-33.000,  3.061], mean action: 6.852 [1.000, 17.000],  loss: 0.018360, mae: 0.289501, mean_q: 0.550449, mean_eps: 0.000000
 1256/5000: episode: 52, duration: 0.183s, episode steps:  12, steps per second:  65, episode reward: 43.588, mean reward:  3.632 [-2.252, 32.356], mean action: 2.333 [0.000, 6.000],  loss: 0.021978, mae: 0.311797, mean_q: 0.550779, mean_eps: 0.000000
 1292/5000: episode: 53, duration: 0.499s, episode steps:  36, steps per second:  72, episode reward: -32.610, mean reward: -0.906 [-32.250,  2.300], mean action: 6.306 [1.000, 15.000],  loss: 0.017603, mae: 0.289775, mean_q: 0.505112, mean_eps: 0.000000
 1309/5000: episode: 54, duration: 0.251s, episode steps:  17, steps per second:  68, episode reward: 35.752, mean reward:  2.103 [-2.614, 33.000], mean action: 5.471 [0.000, 19.000],  loss: 0.020038, mae: 0.298551, mean_q: 0.482855, mean_eps: 0.000000
 1330/5000: episode: 55, duration: 0.305s, episode steps:  21, steps per second:  69, episode reward: 39.000, mean reward:  1.857 [-2.547, 32.500], mean action: 3.190 [0.000, 11.000],  loss: 0.018748, mae: 0.288919, mean_q: 0.539092, mean_eps: 0.000000
 1353/5000: episode: 56, duration: 0.327s, episode steps:  23, steps per second:  70, episode reward: -35.340, mean reward: -1.537 [-31.634,  2.814], mean action: 6.826 [0.000, 15.000],  loss: 0.019036, mae: 0.291179, mean_q: 0.491041, mean_eps: 0.000000
 1379/5000: episode: 57, duration: 0.376s, episode steps:  26, steps per second:  69, episode reward: 33.000, mean reward:  1.269 [-2.792, 33.000], mean action: 7.231 [0.000, 20.000],  loss: 0.016387, mae: 0.286184, mean_q: 0.536835, mean_eps: 0.000000
 1404/5000: episode: 58, duration: 0.357s, episode steps:  25, steps per second:  70, episode reward: 35.199, mean reward:  1.408 [-2.491, 33.000], mean action: 4.800 [0.000, 16.000],  loss: 0.019599, mae: 0.302844, mean_q: 0.556098, mean_eps: 0.000000
 1421/5000: episode: 59, duration: 0.247s, episode steps:  17, steps per second:  69, episode reward: -44.270, mean reward: -2.604 [-33.000,  1.726], mean action: 4.118 [0.000, 15.000],  loss: 0.017920, mae: 0.288791, mean_q: 0.530440, mean_eps: 0.000000
 1449/5000: episode: 60, duration: 0.402s, episode steps:  28, steps per second:  70, episode reward: 38.158, mean reward:  1.363 [-2.275, 32.110], mean action: 5.464 [0.000, 15.000],  loss: 0.022084, mae: 0.315019, mean_q: 0.505523, mean_eps: 0.000000
 1475/5000: episode: 61, duration: 0.378s, episode steps:  26, steps per second:  69, episode reward: 34.553, mean reward:  1.329 [-2.354, 32.069], mean action: 5.192 [0.000, 15.000],  loss: 0.023211, mae: 0.316674, mean_q: 0.524992, mean_eps: 0.000000
 1499/5000: episode: 62, duration: 0.348s, episode steps:  24, steps per second:  69, episode reward: 39.000, mean reward:  1.625 [-2.401, 32.470], mean action: 4.292 [0.000, 15.000],  loss: 0.019893, mae: 0.299543, mean_q: 0.523469, mean_eps: 0.000000
 1525/5000: episode: 63, duration: 0.380s, episode steps:  26, steps per second:  68, episode reward: 35.856, mean reward:  1.379 [-2.526, 32.520], mean action: 4.923 [0.000, 15.000],  loss: 0.018154, mae: 0.291302, mean_q: 0.580059, mean_eps: 0.000000
 1544/5000: episode: 64, duration: 0.281s, episode steps:  19, steps per second:  68, episode reward: 35.023, mean reward:  1.843 [-3.000, 29.826], mean action: 5.368 [0.000, 19.000],  loss: 0.018984, mae: 0.299833, mean_q: 0.509346, mean_eps: 0.000000
 1572/5000: episode: 65, duration: 0.406s, episode steps:  28, steps per second:  69, episode reward: 32.608, mean reward:  1.165 [-3.000, 31.718], mean action: 3.964 [0.000, 19.000],  loss: 0.016938, mae: 0.300385, mean_q: 0.546640, mean_eps: 0.000000
 1593/5000: episode: 66, duration: 0.300s, episode steps:  21, steps per second:  70, episode reward: 39.000, mean reward:  1.857 [-3.000, 32.100], mean action: 2.857 [0.000, 19.000],  loss: 0.020313, mae: 0.315325, mean_q: 0.523997, mean_eps: 0.000000
 1618/5000: episode: 67, duration: 0.353s, episode steps:  25, steps per second:  71, episode reward: -35.470, mean reward: -1.419 [-31.606,  2.400], mean action: 6.720 [0.000, 19.000],  loss: 0.021751, mae: 0.317092, mean_q: 0.513313, mean_eps: 0.000000
 1641/5000: episode: 68, duration: 0.331s, episode steps:  23, steps per second:  69, episode reward: 33.000, mean reward:  1.435 [-3.000, 32.020], mean action: 4.000 [0.000, 19.000],  loss: 0.024460, mae: 0.338756, mean_q: 0.506438, mean_eps: 0.000000
 1664/5000: episode: 69, duration: 0.328s, episode steps:  23, steps per second:  70, episode reward: 41.405, mean reward:  1.800 [-3.000, 31.645], mean action: 2.522 [0.000, 16.000],  loss: 0.019634, mae: 0.309040, mean_q: 0.544344, mean_eps: 0.000000
 1687/5000: episode: 70, duration: 0.332s, episode steps:  23, steps per second:  69, episode reward: 32.766, mean reward:  1.425 [-3.000, 32.027], mean action: 5.217 [0.000, 19.000],  loss: 0.021682, mae: 0.315266, mean_q: 0.593002, mean_eps: 0.000000
 1710/5000: episode: 71, duration: 0.332s, episode steps:  23, steps per second:  69, episode reward: 35.779, mean reward:  1.556 [-2.903, 32.209], mean action: 4.609 [0.000, 16.000],  loss: 0.024685, mae: 0.329268, mean_q: 0.520559, mean_eps: 0.000000
 1739/5000: episode: 72, duration: 0.431s, episode steps:  29, steps per second:  67, episode reward: -32.010, mean reward: -1.104 [-32.004,  3.000], mean action: 3.034 [0.000, 12.000],  loss: 0.018443, mae: 0.300890, mean_q: 0.533602, mean_eps: 0.000000
 1771/5000: episode: 73, duration: 0.440s, episode steps:  32, steps per second:  73, episode reward: -35.590, mean reward: -1.112 [-32.376,  2.302], mean action: 5.500 [0.000, 20.000],  loss: 0.017484, mae: 0.291048, mean_q: 0.532476, mean_eps: 0.000000
 1790/5000: episode: 74, duration: 0.276s, episode steps:  19, steps per second:  69, episode reward: 35.647, mean reward:  1.876 [-3.000, 32.720], mean action: 6.474 [0.000, 20.000],  loss: 0.021007, mae: 0.310789, mean_q: 0.574831, mean_eps: 0.000000
 1811/5000: episode: 75, duration: 0.297s, episode steps:  21, steps per second:  71, episode reward: -38.240, mean reward: -1.821 [-31.852,  2.775], mean action: 8.667 [0.000, 19.000],  loss: 0.021266, mae: 0.313122, mean_q: 0.572537, mean_eps: 0.000000
 1839/5000: episode: 76, duration: 0.405s, episode steps:  28, steps per second:  69, episode reward: -32.270, mean reward: -1.153 [-31.886,  2.246], mean action: 3.607 [0.000, 19.000],  loss: 0.021933, mae: 0.319222, mean_q: 0.579020, mean_eps: 0.000000
 1861/5000: episode: 77, duration: 0.315s, episode steps:  22, steps per second:  70, episode reward: -32.910, mean reward: -1.496 [-32.303,  3.000], mean action: 8.136 [0.000, 20.000],  loss: 0.023865, mae: 0.330147, mean_q: 0.562706, mean_eps: 0.000000
 1895/5000: episode: 78, duration: 0.491s, episode steps:  34, steps per second:  69, episode reward: 32.642, mean reward:  0.960 [-2.875, 32.090], mean action: 3.382 [0.000, 19.000],  loss: 0.021957, mae: 0.323770, mean_q: 0.579862, mean_eps: 0.000000
 1943/5000: episode: 79, duration: 0.656s, episode steps:  48, steps per second:  73, episode reward: 32.404, mean reward:  0.675 [-3.000, 31.925], mean action: 2.604 [0.000, 19.000],  loss: 0.022748, mae: 0.329427, mean_q: 0.578321, mean_eps: 0.000000
 1970/5000: episode: 80, duration: 0.388s, episode steps:  27, steps per second:  70, episode reward: 38.523, mean reward:  1.427 [-2.215, 32.290], mean action: 4.519 [0.000, 15.000],  loss: 0.019005, mae: 0.315906, mean_q: 0.540773, mean_eps: 0.000000
 2054/5000: episode: 81, duration: 1.139s, episode steps:  84, steps per second:  74, episode reward: 32.938, mean reward:  0.392 [-2.500, 33.000], mean action: 8.226 [0.000, 20.000],  loss: 0.021552, mae: 0.317750, mean_q: 0.549867, mean_eps: 0.000000
 2086/5000: episode: 82, duration: 0.448s, episode steps:  32, steps per second:  71, episode reward: 39.000, mean reward:  1.219 [-2.266, 32.280], mean action: 3.250 [0.000, 15.000],  loss: 0.019812, mae: 0.306761, mean_q: 0.537208, mean_eps: 0.000000
 2107/5000: episode: 83, duration: 0.298s, episode steps:  21, steps per second:  70, episode reward: -35.710, mean reward: -1.700 [-32.137,  2.134], mean action: 4.714 [0.000, 15.000],  loss: 0.022312, mae: 0.324023, mean_q: 0.471882, mean_eps: 0.000000
 2132/5000: episode: 84, duration: 0.354s, episode steps:  25, steps per second:  71, episode reward: -35.080, mean reward: -1.403 [-32.440,  2.632], mean action: 8.200 [0.000, 21.000],  loss: 0.024144, mae: 0.334171, mean_q: 0.497397, mean_eps: 0.000000
 2175/5000: episode: 85, duration: 0.594s, episode steps:  43, steps per second:  72, episode reward: 35.857, mean reward:  0.834 [-3.000, 32.610], mean action: 4.581 [0.000, 19.000],  loss: 0.019832, mae: 0.310855, mean_q: 0.535698, mean_eps: 0.000000
 2208/5000: episode: 86, duration: 0.457s, episode steps:  33, steps per second:  72, episode reward: -32.550, mean reward: -0.986 [-32.423,  2.849], mean action: 3.545 [0.000, 19.000],  loss: 0.018269, mae: 0.299998, mean_q: 0.555951, mean_eps: 0.000000
 2231/5000: episode: 87, duration: 0.331s, episode steps:  23, steps per second:  70, episode reward: -33.000, mean reward: -1.435 [-32.624,  2.897], mean action: 6.217 [1.000, 15.000],  loss: 0.023727, mae: 0.323730, mean_q: 0.483905, mean_eps: 0.000000
 2256/5000: episode: 88, duration: 0.357s, episode steps:  25, steps per second:  70, episode reward: 34.192, mean reward:  1.368 [-2.450, 32.465], mean action: 4.280 [0.000, 13.000],  loss: 0.021756, mae: 0.320215, mean_q: 0.513343, mean_eps: 0.000000
 2287/5000: episode: 89, duration: 0.436s, episode steps:  31, steps per second:  71, episode reward: 44.152, mean reward:  1.424 [-2.283, 32.050], mean action: 3.645 [0.000, 14.000],  loss: 0.019685, mae: 0.311021, mean_q: 0.540589, mean_eps: 0.000000
 2311/5000: episode: 90, duration: 0.344s, episode steps:  24, steps per second:  70, episode reward: -32.460, mean reward: -1.353 [-31.822,  3.000], mean action: 3.500 [0.000, 12.000],  loss: 0.018964, mae: 0.304887, mean_q: 0.555952, mean_eps: 0.000000
 2337/5000: episode: 91, duration: 0.370s, episode steps:  26, steps per second:  70, episode reward: 40.346, mean reward:  1.552 [-2.201, 32.090], mean action: 3.654 [0.000, 16.000],  loss: 0.018964, mae: 0.299180, mean_q: 0.518696, mean_eps: 0.000000
 2361/5000: episode: 92, duration: 0.345s, episode steps:  24, steps per second:  70, episode reward: 36.000, mean reward:  1.500 [-2.266, 32.290], mean action: 4.542 [0.000, 18.000],  loss: 0.019512, mae: 0.305446, mean_q: 0.518037, mean_eps: 0.000000
 2383/5000: episode: 93, duration: 0.318s, episode steps:  22, steps per second:  69, episode reward: 38.545, mean reward:  1.752 [-2.592, 33.000], mean action: 3.455 [0.000, 16.000],  loss: 0.024649, mae: 0.330224, mean_q: 0.548561, mean_eps: 0.000000
 2410/5000: episode: 94, duration: 0.385s, episode steps:  27, steps per second:  70, episode reward: -33.000, mean reward: -1.222 [-32.306,  2.530], mean action: 3.444 [0.000, 16.000],  loss: 0.016907, mae: 0.292526, mean_q: 0.490886, mean_eps: 0.000000
 2428/5000: episode: 95, duration: 0.265s, episode steps:  18, steps per second:  68, episode reward: 35.598, mean reward:  1.978 [-3.000, 32.198], mean action: 4.500 [1.000, 12.000],  loss: 0.023140, mae: 0.322933, mean_q: 0.473837, mean_eps: 0.000000
 2447/5000: episode: 96, duration: 0.285s, episode steps:  19, steps per second:  67, episode reward: 43.539, mean reward:  2.292 [-2.163, 32.087], mean action: 2.947 [1.000, 14.000],  loss: 0.022448, mae: 0.312753, mean_q: 0.522648, mean_eps: 0.000000
 2481/5000: episode: 97, duration: 0.483s, episode steps:  34, steps per second:  70, episode reward: 32.196, mean reward:  0.947 [-2.685, 31.913], mean action: 8.588 [0.000, 17.000],  loss: 0.020170, mae: 0.309055, mean_q: 0.556421, mean_eps: 0.000000
 2505/5000: episode: 98, duration: 0.349s, episode steps:  24, steps per second:  69, episode reward: -33.000, mean reward: -1.375 [-32.621,  2.123], mean action: 6.042 [0.000, 20.000],  loss: 0.025894, mae: 0.336523, mean_q: 0.512279, mean_eps: 0.000000
 2518/5000: episode: 99, duration: 0.202s, episode steps:  13, steps per second:  64, episode reward: 44.140, mean reward:  3.395 [-2.929, 31.871], mean action: 1.769 [0.000, 16.000],  loss: 0.019539, mae: 0.304599, mean_q: 0.498989, mean_eps: 0.000000
 2534/5000: episode: 100, duration: 0.234s, episode steps:  16, steps per second:  68, episode reward: 36.000, mean reward:  2.250 [-2.920, 33.000], mean action: 4.938 [0.000, 19.000],  loss: 0.020324, mae: 0.307860, mean_q: 0.514815, mean_eps: 0.000000
 2553/5000: episode: 101, duration: 0.276s, episode steps:  19, steps per second:  69, episode reward: 32.042, mean reward:  1.686 [-3.000, 32.100], mean action: 5.526 [0.000, 19.000],  loss: 0.020457, mae: 0.308425, mean_q: 0.560324, mean_eps: 0.000000
 2584/5000: episode: 102, duration: 0.437s, episode steps:  31, steps per second:  71, episode reward: 32.433, mean reward:  1.046 [-2.902, 32.380], mean action: 8.226 [0.000, 19.000],  loss: 0.018579, mae: 0.298731, mean_q: 0.539711, mean_eps: 0.000000
 2612/5000: episode: 103, duration: 0.402s, episode steps:  28, steps per second:  70, episode reward: -32.620, mean reward: -1.165 [-32.247,  2.685], mean action: 5.786 [0.000, 18.000],  loss: 0.019031, mae: 0.312366, mean_q: 0.495778, mean_eps: 0.000000
 2634/5000: episode: 104, duration: 0.315s, episode steps:  22, steps per second:  70, episode reward: 41.519, mean reward:  1.887 [-2.230, 32.100], mean action: 5.864 [1.000, 15.000],  loss: 0.019878, mae: 0.312446, mean_q: 0.568949, mean_eps: 0.000000
 2655/5000: episode: 105, duration: 0.309s, episode steps:  21, steps per second:  68, episode reward: 40.543, mean reward:  1.931 [-2.220, 32.190], mean action: 2.952 [0.000, 21.000],  loss: 0.022650, mae: 0.325179, mean_q: 0.583804, mean_eps: 0.000000
 2696/5000: episode: 106, duration: 0.563s, episode steps:  41, steps per second:  73, episode reward: 32.394, mean reward:  0.790 [-2.760, 32.470], mean action: 4.341 [1.000, 15.000],  loss: 0.018758, mae: 0.310224, mean_q: 0.581838, mean_eps: 0.000000
 2714/5000: episode: 107, duration: 0.252s, episode steps:  18, steps per second:  71, episode reward: -41.260, mean reward: -2.292 [-32.148,  2.133], mean action: 5.889 [0.000, 14.000],  loss: 0.025169, mae: 0.336592, mean_q: 0.516375, mean_eps: 0.000000
 2734/5000: episode: 108, duration: 0.286s, episode steps:  20, steps per second:  70, episode reward: 36.000, mean reward:  1.800 [-2.996, 32.300], mean action: 3.700 [0.000, 14.000],  loss: 0.016278, mae: 0.290013, mean_q: 0.542233, mean_eps: 0.000000
 2754/5000: episode: 109, duration: 0.289s, episode steps:  20, steps per second:  69, episode reward: 35.903, mean reward:  1.795 [-2.393, 32.623], mean action: 4.650 [0.000, 18.000],  loss: 0.018423, mae: 0.296794, mean_q: 0.538690, mean_eps: 0.000000
 2779/5000: episode: 110, duration: 0.351s, episode steps:  25, steps per second:  71, episode reward: -35.410, mean reward: -1.416 [-33.000,  2.925], mean action: 4.880 [0.000, 11.000],  loss: 0.020680, mae: 0.304868, mean_q: 0.584422, mean_eps: 0.000000
 2798/5000: episode: 111, duration: 0.284s, episode steps:  19, steps per second:  67, episode reward: 41.728, mean reward:  2.196 [-2.189, 32.380], mean action: 3.211 [0.000, 12.000],  loss: 0.021459, mae: 0.310082, mean_q: 0.605069, mean_eps: 0.000000
 2828/5000: episode: 112, duration: 0.420s, episode steps:  30, steps per second:  71, episode reward: 32.483, mean reward:  1.083 [-2.336, 32.945], mean action: 2.667 [0.000, 15.000],  loss: 0.022120, mae: 0.324509, mean_q: 0.483818, mean_eps: 0.000000
 2857/5000: episode: 113, duration: 0.417s, episode steps:  29, steps per second:  70, episode reward: 32.216, mean reward:  1.111 [-2.441, 32.030], mean action: 3.793 [0.000, 19.000],  loss: 0.019952, mae: 0.308578, mean_q: 0.545596, mean_eps: 0.000000
 2881/5000: episode: 114, duration: 0.343s, episode steps:  24, steps per second:  70, episode reward: 32.771, mean reward:  1.365 [-2.628, 32.091], mean action: 5.500 [0.000, 19.000],  loss: 0.021345, mae: 0.309168, mean_q: 0.539324, mean_eps: 0.000000
 2901/5000: episode: 115, duration: 0.292s, episode steps:  20, steps per second:  68, episode reward: 35.632, mean reward:  1.782 [-2.624, 31.978], mean action: 6.450 [1.000, 19.000],  loss: 0.019912, mae: 0.302815, mean_q: 0.552670, mean_eps: 0.000000
 2921/5000: episode: 116, duration: 0.295s, episode steps:  20, steps per second:  68, episode reward: 36.000, mean reward:  1.800 [-2.415, 32.180], mean action: 4.250 [0.000, 16.000],  loss: 0.021964, mae: 0.317455, mean_q: 0.534535, mean_eps: 0.000000
 2946/5000: episode: 117, duration: 0.350s, episode steps:  25, steps per second:  71, episode reward: -35.620, mean reward: -1.425 [-32.271,  2.555], mean action: 6.800 [0.000, 20.000],  loss: 0.018686, mae: 0.300464, mean_q: 0.471599, mean_eps: 0.000000
 2970/5000: episode: 118, duration: 0.354s, episode steps:  24, steps per second:  68, episode reward: 32.410, mean reward:  1.350 [-3.000, 31.710], mean action: 4.250 [1.000, 16.000],  loss: 0.017820, mae: 0.294059, mean_q: 0.512179, mean_eps: 0.000000
 2989/5000: episode: 119, duration: 0.364s, episode steps:  19, steps per second:  52, episode reward: 41.604, mean reward:  2.190 [-2.522, 32.020], mean action: 2.526 [0.000, 12.000],  loss: 0.020387, mae: 0.299165, mean_q: 0.498785, mean_eps: 0.000000
 3021/5000: episode: 120, duration: 0.445s, episode steps:  32, steps per second:  72, episode reward: 36.000, mean reward:  1.125 [-2.509, 33.000], mean action: 3.250 [0.000, 17.000],  loss: 0.024482, mae: 0.326134, mean_q: 0.468771, mean_eps: 0.000000
 3052/5000: episode: 121, duration: 0.443s, episode steps:  31, steps per second:  70, episode reward: 36.000, mean reward:  1.161 [-3.000, 32.380], mean action: 3.581 [0.000, 11.000],  loss: 0.018901, mae: 0.290258, mean_q: 0.484467, mean_eps: 0.000000
 3075/5000: episode: 122, duration: 0.328s, episode steps:  23, steps per second:  70, episode reward: 32.904, mean reward:  1.431 [-3.000, 32.104], mean action: 5.522 [0.000, 18.000],  loss: 0.024574, mae: 0.316606, mean_q: 0.494690, mean_eps: 0.000000
 3101/5000: episode: 123, duration: 0.368s, episode steps:  26, steps per second:  71, episode reward: -35.310, mean reward: -1.358 [-32.441,  2.200], mean action: 4.231 [0.000, 18.000],  loss: 0.019352, mae: 0.295761, mean_q: 0.549081, mean_eps: 0.000000
 3122/5000: episode: 124, duration: 0.304s, episode steps:  21, steps per second:  69, episode reward: 38.119, mean reward:  1.815 [-2.479, 33.000], mean action: 3.238 [0.000, 11.000],  loss: 0.018321, mae: 0.290324, mean_q: 0.529043, mean_eps: 0.000000
 3149/5000: episode: 125, duration: 0.387s, episode steps:  27, steps per second:  70, episode reward: 32.512, mean reward:  1.204 [-3.000, 31.892], mean action: 4.741 [0.000, 19.000],  loss: 0.021293, mae: 0.302889, mean_q: 0.488303, mean_eps: 0.000000
 3168/5000: episode: 126, duration: 0.279s, episode steps:  19, steps per second:  68, episode reward: 41.224, mean reward:  2.170 [-2.224, 32.070], mean action: 2.053 [0.000, 15.000],  loss: 0.019251, mae: 0.296553, mean_q: 0.517310, mean_eps: 0.000000
 3196/5000: episode: 127, duration: 0.401s, episode steps:  28, steps per second:  70, episode reward: 32.990, mean reward:  1.178 [-2.891, 32.400], mean action: 3.679 [0.000, 14.000],  loss: 0.021716, mae: 0.305496, mean_q: 0.514821, mean_eps: 0.000000
 3226/5000: episode: 128, duration: 0.417s, episode steps:  30, steps per second:  72, episode reward: 32.530, mean reward:  1.084 [-2.249, 31.930], mean action: 6.533 [0.000, 18.000],  loss: 0.019936, mae: 0.296832, mean_q: 0.528451, mean_eps: 0.000000
 3239/5000: episode: 129, duration: 0.200s, episode steps:  13, steps per second:  65, episode reward: 41.180, mean reward:  3.168 [-2.802, 32.690], mean action: 3.385 [0.000, 11.000],  loss: 0.017553, mae: 0.286595, mean_q: 0.537418, mean_eps: 0.000000
 3262/5000: episode: 130, duration: 0.338s, episode steps:  23, steps per second:  68, episode reward: 38.119, mean reward:  1.657 [-2.223, 32.540], mean action: 4.130 [0.000, 14.000],  loss: 0.019689, mae: 0.303020, mean_q: 0.562013, mean_eps: 0.000000
 3419/5000: episode: 131, duration: 2.129s, episode steps: 157, steps per second:  74, episode reward: 41.530, mean reward:  0.265 [-2.145, 29.783], mean action: 2.898 [0.000, 16.000],  loss: 0.019512, mae: 0.302795, mean_q: 0.526963, mean_eps: 0.000000
 3440/5000: episode: 132, duration: 0.302s, episode steps:  21, steps per second:  69, episode reward: 41.493, mean reward:  1.976 [-2.244, 32.305], mean action: 2.333 [0.000, 16.000],  loss: 0.019843, mae: 0.310725, mean_q: 0.522138, mean_eps: 0.000000
 3460/5000: episode: 133, duration: 0.286s, episode steps:  20, steps per second:  70, episode reward: -38.360, mean reward: -1.918 [-33.000,  2.760], mean action: 5.450 [0.000, 16.000],  loss: 0.021304, mae: 0.317907, mean_q: 0.543905, mean_eps: 0.000000
 3473/5000: episode: 134, duration: 0.198s, episode steps:  13, steps per second:  66, episode reward: 43.948, mean reward:  3.381 [-2.238, 32.910], mean action: 3.462 [0.000, 16.000],  loss: 0.026093, mae: 0.339137, mean_q: 0.527502, mean_eps: 0.000000
 3493/5000: episode: 135, duration: 0.584s, episode steps:  20, steps per second:  34, episode reward: 38.644, mean reward:  1.932 [-2.275, 32.110], mean action: 3.300 [0.000, 16.000],  loss: 0.019524, mae: 0.313953, mean_q: 0.518939, mean_eps: 0.000000
 3511/5000: episode: 136, duration: 0.275s, episode steps:  18, steps per second:  65, episode reward: 35.420, mean reward:  1.968 [-3.000, 32.420], mean action: 5.667 [0.000, 16.000],  loss: 0.020116, mae: 0.307109, mean_q: 0.511655, mean_eps: 0.000000
 3530/5000: episode: 137, duration: 0.278s, episode steps:  19, steps per second:  68, episode reward: 38.876, mean reward:  2.046 [-2.525, 32.180], mean action: 3.263 [0.000, 12.000],  loss: 0.021106, mae: 0.310445, mean_q: 0.545306, mean_eps: 0.000000
 3554/5000: episode: 138, duration: 0.347s, episode steps:  24, steps per second:  69, episode reward: 33.000, mean reward:  1.375 [-3.000, 32.080], mean action: 3.708 [0.000, 12.000],  loss: 0.015499, mae: 0.290456, mean_q: 0.477642, mean_eps: 0.000000
 3590/5000: episode: 139, duration: 0.512s, episode steps:  36, steps per second:  70, episode reward: 32.263, mean reward:  0.896 [-2.745, 32.110], mean action: 5.750 [0.000, 19.000],  loss: 0.019534, mae: 0.308861, mean_q: 0.445776, mean_eps: 0.000000
 3614/5000: episode: 140, duration: 0.342s, episode steps:  24, steps per second:  70, episode reward: -38.310, mean reward: -1.596 [-32.237,  2.250], mean action: 4.375 [0.000, 16.000],  loss: 0.023121, mae: 0.328278, mean_q: 0.487260, mean_eps: 0.000000
 3640/5000: episode: 141, duration: 0.375s, episode steps:  26, steps per second:  69, episode reward: 35.847, mean reward:  1.379 [-3.000, 33.000], mean action: 3.231 [0.000, 11.000],  loss: 0.022196, mae: 0.318939, mean_q: 0.479652, mean_eps: 0.000000
 3669/5000: episode: 142, duration: 0.407s, episode steps:  29, steps per second:  71, episode reward: 38.250, mean reward:  1.319 [-2.487, 32.150], mean action: 2.966 [0.000, 11.000],  loss: 0.019701, mae: 0.309291, mean_q: 0.497559, mean_eps: 0.000000
 3706/5000: episode: 143, duration: 0.529s, episode steps:  37, steps per second:  70, episode reward: 32.389, mean reward:  0.875 [-2.619, 32.390], mean action: 5.054 [0.000, 16.000],  loss: 0.019730, mae: 0.303438, mean_q: 0.551683, mean_eps: 0.000000
 3734/5000: episode: 144, duration: 0.414s, episode steps:  28, steps per second:  68, episode reward: 32.485, mean reward:  1.160 [-3.000, 32.669], mean action: 3.107 [0.000, 15.000],  loss: 0.020207, mae: 0.311250, mean_q: 0.588213, mean_eps: 0.000000
 3759/5000: episode: 145, duration: 0.357s, episode steps:  25, steps per second:  70, episode reward: 35.868, mean reward:  1.435 [-3.000, 32.010], mean action: 4.160 [0.000, 15.000],  loss: 0.019123, mae: 0.305016, mean_q: 0.547886, mean_eps: 0.000000
 3781/5000: episode: 146, duration: 0.392s, episode steps:  22, steps per second:  56, episode reward: -38.540, mean reward: -1.752 [-31.987,  2.402], mean action: 6.455 [0.000, 15.000],  loss: 0.022283, mae: 0.313253, mean_q: 0.473383, mean_eps: 0.000000
 3811/5000: episode: 147, duration: 0.425s, episode steps:  30, steps per second:  71, episode reward: 32.457, mean reward:  1.082 [-2.348, 32.530], mean action: 4.833 [0.000, 15.000],  loss: 0.019639, mae: 0.300367, mean_q: 0.498345, mean_eps: 0.000000
 3840/5000: episode: 148, duration: 0.410s, episode steps:  29, steps per second:  71, episode reward: 36.000, mean reward:  1.241 [-2.279, 32.210], mean action: 6.897 [0.000, 16.000],  loss: 0.018770, mae: 0.298360, mean_q: 0.546939, mean_eps: 0.000000
 3860/5000: episode: 149, duration: 0.290s, episode steps:  20, steps per second:  69, episode reward: 35.559, mean reward:  1.778 [-3.000, 32.250], mean action: 4.450 [0.000, 16.000],  loss: 0.019903, mae: 0.307999, mean_q: 0.539537, mean_eps: 0.000000
 3880/5000: episode: 150, duration: 0.291s, episode steps:  20, steps per second:  69, episode reward: 38.239, mean reward:  1.912 [-3.000, 31.971], mean action: 2.950 [0.000, 16.000],  loss: 0.019438, mae: 0.306790, mean_q: 0.548651, mean_eps: 0.000000
 3909/5000: episode: 151, duration: 0.411s, episode steps:  29, steps per second:  71, episode reward: 35.063, mean reward:  1.209 [-2.464, 32.230], mean action: 5.000 [0.000, 16.000],  loss: 0.019669, mae: 0.304286, mean_q: 0.504905, mean_eps: 0.000000
 3931/5000: episode: 152, duration: 0.392s, episode steps:  22, steps per second:  56, episode reward: 35.345, mean reward:  1.607 [-2.606, 32.383], mean action: 5.409 [0.000, 20.000],  loss: 0.021926, mae: 0.316557, mean_q: 0.540883, mean_eps: 0.000000
 3951/5000: episode: 153, duration: 0.329s, episode steps:  20, steps per second:  61, episode reward: 35.668, mean reward:  1.783 [-2.818, 32.478], mean action: 4.450 [0.000, 16.000],  loss: 0.021709, mae: 0.315506, mean_q: 0.525860, mean_eps: 0.000000
 3969/5000: episode: 154, duration: 0.275s, episode steps:  18, steps per second:  65, episode reward: 41.078, mean reward:  2.282 [-2.155, 31.278], mean action: 3.333 [0.000, 16.000],  loss: 0.019877, mae: 0.303215, mean_q: 0.535188, mean_eps: 0.000000
 3996/5000: episode: 155, duration: 0.401s, episode steps:  27, steps per second:  67, episode reward: 33.000, mean reward:  1.222 [-2.520, 30.094], mean action: 2.519 [0.000, 12.000],  loss: 0.019865, mae: 0.301247, mean_q: 0.527324, mean_eps: 0.000000
 4028/5000: episode: 156, duration: 0.496s, episode steps:  32, steps per second:  65, episode reward: -32.180, mean reward: -1.006 [-31.769,  2.241], mean action: 3.312 [0.000, 16.000],  loss: 0.020091, mae: 0.307829, mean_q: 0.552846, mean_eps: 0.000000
 4062/5000: episode: 157, duration: 0.483s, episode steps:  34, steps per second:  70, episode reward: -35.710, mean reward: -1.050 [-31.735,  2.410], mean action: 4.941 [0.000, 16.000],  loss: 0.019132, mae: 0.303196, mean_q: 0.535924, mean_eps: 0.000000
 4082/5000: episode: 158, duration: 0.315s, episode steps:  20, steps per second:  63, episode reward: 41.554, mean reward:  2.078 [-2.129, 32.380], mean action: 0.950 [0.000, 15.000],  loss: 0.024847, mae: 0.321705, mean_q: 0.536922, mean_eps: 0.000000
 4102/5000: episode: 159, duration: 0.346s, episode steps:  20, steps per second:  58, episode reward: 30.000, mean reward:  1.500 [-3.000, 30.298], mean action: 5.450 [0.000, 15.000],  loss: 0.022042, mae: 0.307901, mean_q: 0.510558, mean_eps: 0.000000
 4128/5000: episode: 160, duration: 0.374s, episode steps:  26, steps per second:  69, episode reward: -35.430, mean reward: -1.363 [-32.600,  2.300], mean action: 6.654 [0.000, 16.000],  loss: 0.019804, mae: 0.296825, mean_q: 0.493207, mean_eps: 0.000000
 4149/5000: episode: 161, duration: 0.309s, episode steps:  21, steps per second:  68, episode reward: 38.003, mean reward:  1.810 [-2.400, 32.461], mean action: 4.571 [0.000, 16.000],  loss: 0.022416, mae: 0.304869, mean_q: 0.497577, mean_eps: 0.000000
 4175/5000: episode: 162, duration: 0.368s, episode steps:  26, steps per second:  71, episode reward: 32.288, mean reward:  1.242 [-3.000, 32.180], mean action: 4.923 [0.000, 16.000],  loss: 0.025548, mae: 0.335901, mean_q: 0.606553, mean_eps: 0.000000
 4205/5000: episode: 163, duration: 0.422s, episode steps:  30, steps per second:  71, episode reward: -35.330, mean reward: -1.178 [-31.745,  2.540], mean action: 6.133 [0.000, 15.000],  loss: 0.024914, mae: 0.326746, mean_q: 0.564521, mean_eps: 0.000000
 4224/5000: episode: 164, duration: 0.279s, episode steps:  19, steps per second:  68, episode reward: 41.510, mean reward:  2.185 [-2.150, 32.367], mean action: 3.263 [0.000, 12.000],  loss: 0.017756, mae: 0.286072, mean_q: 0.506062, mean_eps: 0.000000
 4258/5000: episode: 165, duration: 0.480s, episode steps:  34, steps per second:  71, episode reward: 38.373, mean reward:  1.129 [-2.527, 32.296], mean action: 2.706 [0.000, 15.000],  loss: 0.021924, mae: 0.315097, mean_q: 0.462770, mean_eps: 0.000000
 4276/5000: episode: 166, duration: 0.294s, episode steps:  18, steps per second:  61, episode reward: 38.170, mean reward:  2.121 [-2.417, 32.003], mean action: 3.778 [0.000, 19.000],  loss: 0.023023, mae: 0.324365, mean_q: 0.464450, mean_eps: 0.000000
 4288/5000: episode: 167, duration: 0.185s, episode steps:  12, steps per second:  65, episode reward: 45.000, mean reward:  3.750 [-2.065, 32.240], mean action: 2.083 [1.000, 9.000],  loss: 0.017879, mae: 0.293184, mean_q: 0.509901, mean_eps: 0.000000
 4313/5000: episode: 168, duration: 0.353s, episode steps:  25, steps per second:  71, episode reward: 35.631, mean reward:  1.425 [-2.296, 32.541], mean action: 3.640 [0.000, 15.000],  loss: 0.023153, mae: 0.324428, mean_q: 0.565349, mean_eps: 0.000000
 4337/5000: episode: 169, duration: 0.340s, episode steps:  24, steps per second:  71, episode reward: -35.450, mean reward: -1.477 [-31.713,  2.510], mean action: 5.042 [0.000, 15.000],  loss: 0.020699, mae: 0.319510, mean_q: 0.557105, mean_eps: 0.000000
 4355/5000: episode: 170, duration: 0.264s, episode steps:  18, steps per second:  68, episode reward: 35.875, mean reward:  1.993 [-2.646, 32.235], mean action: 4.333 [0.000, 15.000],  loss: 0.021191, mae: 0.319530, mean_q: 0.537778, mean_eps: 0.000000
 4378/5000: episode: 171, duration: 0.334s, episode steps:  23, steps per second:  69, episode reward: 40.394, mean reward:  1.756 [-2.106, 32.300], mean action: 4.174 [1.000, 16.000],  loss: 0.022430, mae: 0.322254, mean_q: 0.555703, mean_eps: 0.000000
 4404/5000: episode: 172, duration: 0.375s, episode steps:  26, steps per second:  69, episode reward: -32.530, mean reward: -1.251 [-31.992,  2.616], mean action: 3.731 [0.000, 15.000],  loss: 0.023366, mae: 0.326704, mean_q: 0.540449, mean_eps: 0.000000
 4427/5000: episode: 173, duration: 0.343s, episode steps:  23, steps per second:  67, episode reward: 38.256, mean reward:  1.663 [-2.342, 32.589], mean action: 3.174 [0.000, 9.000],  loss: 0.020706, mae: 0.316091, mean_q: 0.558011, mean_eps: 0.000000
 4445/5000: episode: 174, duration: 0.265s, episode steps:  18, steps per second:  68, episode reward: 35.481, mean reward:  1.971 [-2.665, 32.220], mean action: 5.000 [0.000, 13.000],  loss: 0.025630, mae: 0.347771, mean_q: 0.585094, mean_eps: 0.000000
 4464/5000: episode: 175, duration: 0.287s, episode steps:  19, steps per second:  66, episode reward: 41.405, mean reward:  2.179 [-2.214, 33.000], mean action: 3.579 [0.000, 14.000],  loss: 0.018991, mae: 0.305459, mean_q: 0.580097, mean_eps: 0.000000
 4487/5000: episode: 176, duration: 0.328s, episode steps:  23, steps per second:  70, episode reward: -32.760, mean reward: -1.424 [-32.593,  3.000], mean action: 5.000 [0.000, 15.000],  loss: 0.019956, mae: 0.317907, mean_q: 0.566116, mean_eps: 0.000000
 4514/5000: episode: 177, duration: 0.390s, episode steps:  27, steps per second:  69, episode reward: 37.737, mean reward:  1.398 [-2.101, 32.226], mean action: 3.741 [0.000, 19.000],  loss: 0.025979, mae: 0.348167, mean_q: 0.497202, mean_eps: 0.000000
 4531/5000: episode: 178, duration: 0.253s, episode steps:  17, steps per second:  67, episode reward: 38.786, mean reward:  2.282 [-2.394, 32.061], mean action: 6.000 [0.000, 19.000],  loss: 0.019343, mae: 0.321142, mean_q: 0.489395, mean_eps: 0.000000
 4557/5000: episode: 179, duration: 0.366s, episode steps:  26, steps per second:  71, episode reward: 32.207, mean reward:  1.239 [-3.000, 33.000], mean action: 5.654 [0.000, 19.000],  loss: 0.020935, mae: 0.320160, mean_q: 0.551271, mean_eps: 0.000000
 4580/5000: episode: 180, duration: 0.328s, episode steps:  23, steps per second:  70, episode reward: 38.319, mean reward:  1.666 [-2.550, 33.000], mean action: 4.043 [0.000, 19.000],  loss: 0.023877, mae: 0.330636, mean_q: 0.545945, mean_eps: 0.000000
 4600/5000: episode: 181, duration: 0.302s, episode steps:  20, steps per second:  66, episode reward: 41.496, mean reward:  2.075 [-2.231, 32.470], mean action: 3.850 [0.000, 19.000],  loss: 0.021195, mae: 0.318108, mean_q: 0.560341, mean_eps: 0.000000
 4616/5000: episode: 182, duration: 0.241s, episode steps:  16, steps per second:  66, episode reward: 41.691, mean reward:  2.606 [-2.227, 32.121], mean action: 5.188 [0.000, 19.000],  loss: 0.019375, mae: 0.312926, mean_q: 0.538655, mean_eps: 0.000000
 4636/5000: episode: 183, duration: 0.292s, episode steps:  20, steps per second:  69, episode reward: 35.571, mean reward:  1.779 [-2.774, 32.290], mean action: 4.900 [0.000, 19.000],  loss: 0.023736, mae: 0.324233, mean_q: 0.497526, mean_eps: 0.000000
 4662/5000: episode: 184, duration: 0.377s, episode steps:  26, steps per second:  69, episode reward: 35.953, mean reward:  1.383 [-2.483, 29.995], mean action: 6.500 [0.000, 15.000],  loss: 0.021218, mae: 0.324706, mean_q: 0.499790, mean_eps: 0.000000
 4688/5000: episode: 185, duration: 0.367s, episode steps:  26, steps per second:  71, episode reward: 32.134, mean reward:  1.236 [-2.777, 31.796], mean action: 7.615 [0.000, 21.000],  loss: 0.021182, mae: 0.315619, mean_q: 0.457984, mean_eps: 0.000000
 4718/5000: episode: 186, duration: 0.425s, episode steps:  30, steps per second:  71, episode reward: 32.080, mean reward:  1.069 [-2.603, 31.090], mean action: 7.800 [0.000, 18.000],  loss: 0.021930, mae: 0.316142, mean_q: 0.512946, mean_eps: 0.000000
 4756/5000: episode: 187, duration: 0.526s, episode steps:  38, steps per second:  72, episode reward: -32.830, mean reward: -0.864 [-32.193,  3.062], mean action: 5.000 [0.000, 15.000],  loss: 0.021113, mae: 0.312578, mean_q: 0.534786, mean_eps: 0.000000
 4784/5000: episode: 188, duration: 0.407s, episode steps:  28, steps per second:  69, episode reward: 33.000, mean reward:  1.179 [-2.554, 32.560], mean action: 4.607 [0.000, 12.000],  loss: 0.019547, mae: 0.308483, mean_q: 0.568744, mean_eps: 0.000000
 4815/5000: episode: 189, duration: 0.439s, episode steps:  31, steps per second:  71, episode reward: 32.789, mean reward:  1.058 [-2.544, 32.089], mean action: 2.968 [0.000, 12.000],  loss: 0.017970, mae: 0.305494, mean_q: 0.524718, mean_eps: 0.000000
 4837/5000: episode: 190, duration: 0.318s, episode steps:  22, steps per second:  69, episode reward: -32.940, mean reward: -1.497 [-33.000,  2.370], mean action: 4.227 [0.000, 19.000],  loss: 0.021397, mae: 0.314694, mean_q: 0.497873, mean_eps: 0.000000
 4887/5000: episode: 191, duration: 0.694s, episode steps:  50, steps per second:  72, episode reward: 32.595, mean reward:  0.652 [-2.775, 32.880], mean action: 5.680 [0.000, 19.000],  loss: 0.021475, mae: 0.318342, mean_q: 0.559785, mean_eps: 0.000000
 4930/5000: episode: 192, duration: 0.589s, episode steps:  43, steps per second:  73, episode reward: 37.534, mean reward:  0.873 [-3.000, 32.828], mean action: 2.465 [0.000, 19.000],  loss: 0.020475, mae: 0.322833, mean_q: 0.520428, mean_eps: 0.000000
 4978/5000: episode: 193, duration: 0.878s, episode steps:  48, steps per second:  55, episode reward: -33.000, mean reward: -0.688 [-32.087,  2.200], mean action: 7.438 [0.000, 18.000],  loss: 0.020015, mae: 0.313346, mean_q: 0.521375, mean_eps: 0.000000
done, took 65.378 seconds
DQN Evaluation: 11946 victories out of 13932 episodes
Training for 5000 steps ...
   29/5000: episode: 1, duration: 0.229s, episode steps:  29, steps per second: 127, episode reward: 35.901, mean reward:  1.238 [-2.654, 32.901], mean action: 3.966 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   42/5000: episode: 2, duration: 0.115s, episode steps:  13, steps per second: 113, episode reward: 44.465, mean reward:  3.420 [-2.700, 33.000], mean action: 2.308 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   64/5000: episode: 3, duration: 0.159s, episode steps:  22, steps per second: 138, episode reward: 44.007, mean reward:  2.000 [-3.000, 32.242], mean action: 1.227 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   86/5000: episode: 4, duration: 0.164s, episode steps:  22, steps per second: 134, episode reward: 40.666, mean reward:  1.848 [-2.254, 31.681], mean action: 4.909 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  110/5000: episode: 5, duration: 0.172s, episode steps:  24, steps per second: 140, episode reward: 44.373, mean reward:  1.849 [-2.619, 32.270], mean action: 2.125 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  137/5000: episode: 6, duration: 0.192s, episode steps:  27, steps per second: 140, episode reward: 41.097, mean reward:  1.522 [-3.000, 32.270], mean action: 3.148 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  185/5000: episode: 7, duration: 0.296s, episode steps:  48, steps per second: 162, episode reward: 32.396, mean reward:  0.675 [-3.000, 32.020], mean action: 4.417 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  204/5000: episode: 8, duration: 0.137s, episode steps:  19, steps per second: 139, episode reward: 44.676, mean reward:  2.351 [-2.380, 32.600], mean action: 2.526 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  264/5000: episode: 9, duration: 0.348s, episode steps:  60, steps per second: 172, episode reward: -32.700, mean reward: -0.545 [-32.142,  2.730], mean action: 4.950 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  286/5000: episode: 10, duration: 0.156s, episode steps:  22, steps per second: 141, episode reward: 38.427, mean reward:  1.747 [-2.556, 32.370], mean action: 3.727 [1.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  308/5000: episode: 11, duration: 0.155s, episode steps:  22, steps per second: 142, episode reward: 38.878, mean reward:  1.767 [-2.889, 32.020], mean action: 3.318 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  334/5000: episode: 12, duration: 0.182s, episode steps:  26, steps per second: 143, episode reward: 44.901, mean reward:  1.727 [-2.209, 32.551], mean action: 1.038 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  433/5000: episode: 13, duration: 0.567s, episode steps:  99, steps per second: 175, episode reward: 42.683, mean reward:  0.431 [-2.100, 32.160], mean action: 1.152 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  466/5000: episode: 14, duration: 0.216s, episode steps:  33, steps per second: 152, episode reward: 45.000, mean reward:  1.364 [-2.444, 32.780], mean action: 0.364 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  483/5000: episode: 15, duration: 0.131s, episode steps:  17, steps per second: 130, episode reward: 41.188, mean reward:  2.423 [-2.176, 32.020], mean action: 2.824 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  513/5000: episode: 16, duration: 0.194s, episode steps:  30, steps per second: 155, episode reward: 41.739, mean reward:  1.391 [-2.514, 32.110], mean action: 2.933 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  534/5000: episode: 17, duration: 0.153s, episode steps:  21, steps per second: 137, episode reward: 47.088, mean reward:  2.242 [-0.040, 31.792], mean action: 1.381 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  548/5000: episode: 18, duration: 0.122s, episode steps:  14, steps per second: 115, episode reward: 47.363, mean reward:  3.383 [-0.266, 32.952], mean action: 2.357 [1.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  574/5000: episode: 19, duration: 0.176s, episode steps:  26, steps per second: 148, episode reward: 41.487, mean reward:  1.596 [-2.465, 31.847], mean action: 1.500 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  603/5000: episode: 20, duration: 0.189s, episode steps:  29, steps per second: 154, episode reward: 32.778, mean reward:  1.130 [-2.588, 32.108], mean action: 5.759 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  627/5000: episode: 21, duration: 0.164s, episode steps:  24, steps per second: 146, episode reward: 41.599, mean reward:  1.733 [-2.538, 32.530], mean action: 3.000 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  665/5000: episode: 22, duration: 0.246s, episode steps:  38, steps per second: 154, episode reward: 40.337, mean reward:  1.061 [-2.903, 31.743], mean action: 3.842 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  690/5000: episode: 23, duration: 0.168s, episode steps:  25, steps per second: 149, episode reward: 39.000, mean reward:  1.560 [-3.000, 32.150], mean action: 4.080 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  739/5000: episode: 24, duration: 0.295s, episode steps:  49, steps per second: 166, episode reward: 36.000, mean reward:  0.735 [-3.000, 32.190], mean action: 3.347 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  761/5000: episode: 25, duration: 0.166s, episode steps:  22, steps per second: 133, episode reward: 44.676, mean reward:  2.031 [-2.088, 32.200], mean action: 3.864 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  789/5000: episode: 26, duration: 0.177s, episode steps:  28, steps per second: 158, episode reward: 35.519, mean reward:  1.269 [-2.593, 31.719], mean action: 2.071 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  812/5000: episode: 27, duration: 0.158s, episode steps:  23, steps per second: 145, episode reward: 41.815, mean reward:  1.818 [-2.051, 32.020], mean action: 2.609 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  834/5000: episode: 28, duration: 0.151s, episode steps:  22, steps per second: 146, episode reward: 38.894, mean reward:  1.768 [-2.720, 32.024], mean action: 3.545 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  866/5000: episode: 29, duration: 0.204s, episode steps:  32, steps per second: 157, episode reward: 41.195, mean reward:  1.287 [-2.683, 32.203], mean action: 1.906 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  899/5000: episode: 30, duration: 0.211s, episode steps:  33, steps per second: 157, episode reward: 38.600, mean reward:  1.170 [-2.778, 31.970], mean action: 4.727 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  924/5000: episode: 31, duration: 0.188s, episode steps:  25, steps per second: 133, episode reward: 38.548, mean reward:  1.542 [-2.400, 31.578], mean action: 1.360 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  938/5000: episode: 32, duration: 0.107s, episode steps:  14, steps per second: 131, episode reward: 45.000, mean reward:  3.214 [-2.148, 32.620], mean action: 1.857 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  956/5000: episode: 33, duration: 0.126s, episode steps:  18, steps per second: 143, episode reward: 38.588, mean reward:  2.144 [-2.713, 32.637], mean action: 3.278 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  975/5000: episode: 34, duration: 0.131s, episode steps:  19, steps per second: 145, episode reward: 38.593, mean reward:  2.031 [-3.000, 32.190], mean action: 3.105 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  999/5000: episode: 35, duration: 0.162s, episode steps:  24, steps per second: 148, episode reward: 42.000, mean reward:  1.750 [-2.062, 32.420], mean action: 2.083 [0.000, 9.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1023/5000: episode: 36, duration: 0.342s, episode steps:  24, steps per second:  70, episode reward: 38.172, mean reward:  1.591 [-2.612, 32.203], mean action: 2.500 [0.000, 12.000],  loss: 0.019006, mae: 0.305868, mean_q: 0.512509, mean_eps: 0.000000
 1058/5000: episode: 37, duration: 0.502s, episode steps:  35, steps per second:  70, episode reward: 38.389, mean reward:  1.097 [-2.231, 32.330], mean action: 2.086 [0.000, 12.000],  loss: 0.020805, mae: 0.306704, mean_q: 0.536861, mean_eps: 0.000000
 1078/5000: episode: 38, duration: 0.293s, episode steps:  20, steps per second:  68, episode reward: 41.081, mean reward:  2.054 [-2.132, 32.180], mean action: 4.000 [0.000, 18.000],  loss: 0.016232, mae: 0.285650, mean_q: 0.552693, mean_eps: 0.000000
 1093/5000: episode: 39, duration: 0.228s, episode steps:  15, steps per second:  66, episode reward: 44.870, mean reward:  2.991 [-2.536, 32.220], mean action: 3.333 [0.000, 12.000],  loss: 0.023521, mae: 0.329804, mean_q: 0.494219, mean_eps: 0.000000
 1138/5000: episode: 40, duration: 0.637s, episode steps:  45, steps per second:  71, episode reward: 37.808, mean reward:  0.840 [-3.000, 32.200], mean action: 3.311 [0.000, 12.000],  loss: 0.022845, mae: 0.316398, mean_q: 0.480600, mean_eps: 0.000000
 1163/5000: episode: 41, duration: 0.551s, episode steps:  25, steps per second:  45, episode reward: 44.879, mean reward:  1.795 [-2.391, 32.060], mean action: 3.080 [0.000, 19.000],  loss: 0.021648, mae: 0.314405, mean_q: 0.509788, mean_eps: 0.000000
 1207/5000: episode: 42, duration: 0.627s, episode steps:  44, steps per second:  70, episode reward: 38.892, mean reward:  0.884 [-2.605, 32.276], mean action: 4.341 [0.000, 19.000],  loss: 0.021134, mae: 0.313864, mean_q: 0.496455, mean_eps: 0.000000
 1236/5000: episode: 43, duration: 0.418s, episode steps:  29, steps per second:  69, episode reward: 44.115, mean reward:  1.521 [-3.000, 31.195], mean action: 4.034 [0.000, 15.000],  loss: 0.022432, mae: 0.322591, mean_q: 0.525664, mean_eps: 0.000000
 1258/5000: episode: 44, duration: 0.321s, episode steps:  22, steps per second:  68, episode reward: 35.903, mean reward:  1.632 [-2.650, 32.813], mean action: 2.364 [0.000, 9.000],  loss: 0.021024, mae: 0.318282, mean_q: 0.557380, mean_eps: 0.000000
 1276/5000: episode: 45, duration: 0.269s, episode steps:  18, steps per second:  67, episode reward: 44.090, mean reward:  2.449 [-2.260, 32.460], mean action: 1.944 [0.000, 9.000],  loss: 0.018627, mae: 0.315166, mean_q: 0.554415, mean_eps: 0.000000
 1307/5000: episode: 46, duration: 0.440s, episode steps:  31, steps per second:  70, episode reward: 41.104, mean reward:  1.326 [-2.306, 32.270], mean action: 2.226 [0.000, 9.000],  loss: 0.019848, mae: 0.328349, mean_q: 0.596737, mean_eps: 0.000000
 1345/5000: episode: 47, duration: 0.528s, episode steps:  38, steps per second:  72, episode reward: -34.630, mean reward: -0.911 [-32.447,  2.127], mean action: 7.868 [0.000, 20.000],  loss: 0.022356, mae: 0.328903, mean_q: 0.534761, mean_eps: 0.000000
 1370/5000: episode: 48, duration: 0.363s, episode steps:  25, steps per second:  69, episode reward: 44.012, mean reward:  1.760 [-2.213, 32.440], mean action: 5.360 [1.000, 16.000],  loss: 0.020326, mae: 0.320917, mean_q: 0.539322, mean_eps: 0.000000
 1394/5000: episode: 49, duration: 0.349s, episode steps:  24, steps per second:  69, episode reward: 35.477, mean reward:  1.478 [-3.000, 32.260], mean action: 3.833 [0.000, 19.000],  loss: 0.021044, mae: 0.322178, mean_q: 0.526023, mean_eps: 0.000000
 1417/5000: episode: 50, duration: 0.328s, episode steps:  23, steps per second:  70, episode reward: 44.364, mean reward:  1.929 [-2.353, 32.120], mean action: 1.043 [0.000, 16.000],  loss: 0.017605, mae: 0.304307, mean_q: 0.499806, mean_eps: 0.000000
 1446/5000: episode: 51, duration: 0.414s, episode steps:  29, steps per second:  70, episode reward: 40.793, mean reward:  1.407 [-2.339, 32.260], mean action: 3.172 [0.000, 16.000],  loss: 0.019231, mae: 0.314339, mean_q: 0.563750, mean_eps: 0.000000
 1464/5000: episode: 52, duration: 0.265s, episode steps:  18, steps per second:  68, episode reward: 45.000, mean reward:  2.500 [-2.452, 32.040], mean action: 2.611 [1.000, 16.000],  loss: 0.019973, mae: 0.315341, mean_q: 0.538493, mean_eps: 0.000000
 1486/5000: episode: 53, duration: 0.324s, episode steps:  22, steps per second:  68, episode reward: 44.795, mean reward:  2.036 [-2.031, 32.300], mean action: 3.864 [1.000, 20.000],  loss: 0.021098, mae: 0.326398, mean_q: 0.570769, mean_eps: 0.000000
 1517/5000: episode: 54, duration: 0.434s, episode steps:  31, steps per second:  71, episode reward: 33.000, mean reward:  1.065 [-3.000, 32.400], mean action: 5.516 [0.000, 19.000],  loss: 0.021576, mae: 0.325868, mean_q: 0.582875, mean_eps: 0.000000
 1532/5000: episode: 55, duration: 0.231s, episode steps:  15, steps per second:  65, episode reward: 44.329, mean reward:  2.955 [-2.057, 32.240], mean action: 2.533 [1.000, 11.000],  loss: 0.017575, mae: 0.304449, mean_q: 0.595947, mean_eps: 0.000000
 1568/5000: episode: 56, duration: 0.504s, episode steps:  36, steps per second:  71, episode reward: 35.502, mean reward:  0.986 [-2.683, 31.692], mean action: 2.611 [0.000, 19.000],  loss: 0.023519, mae: 0.338552, mean_q: 0.589583, mean_eps: 0.000000
 1595/5000: episode: 57, duration: 0.382s, episode steps:  27, steps per second:  71, episode reward: 41.901, mean reward:  1.552 [-2.393, 32.241], mean action: 1.815 [0.000, 12.000],  loss: 0.018532, mae: 0.312011, mean_q: 0.539713, mean_eps: 0.000000
 1663/5000: episode: 58, duration: 0.929s, episode steps:  68, steps per second:  73, episode reward: -32.660, mean reward: -0.480 [-31.931,  2.542], mean action: 7.809 [0.000, 18.000],  loss: 0.020769, mae: 0.323675, mean_q: 0.532319, mean_eps: 0.000000
 1688/5000: episode: 59, duration: 0.362s, episode steps:  25, steps per second:  69, episode reward: 44.751, mean reward:  1.790 [-2.827, 32.420], mean action: 2.120 [0.000, 9.000],  loss: 0.021655, mae: 0.326783, mean_q: 0.502247, mean_eps: 0.000000
 1712/5000: episode: 60, duration: 0.342s, episode steps:  24, steps per second:  70, episode reward: 38.700, mean reward:  1.613 [-2.949, 32.870], mean action: 4.375 [1.000, 19.000],  loss: 0.019840, mae: 0.318460, mean_q: 0.492120, mean_eps: 0.000000
 1742/5000: episode: 61, duration: 0.434s, episode steps:  30, steps per second:  69, episode reward: 36.000, mean reward:  1.200 [-2.900, 32.520], mean action: 5.367 [0.000, 19.000],  loss: 0.021089, mae: 0.317971, mean_q: 0.509729, mean_eps: 0.000000
 1772/5000: episode: 62, duration: 0.423s, episode steps:  30, steps per second:  71, episode reward: 38.857, mean reward:  1.295 [-3.000, 32.720], mean action: 6.767 [0.000, 19.000],  loss: 0.021261, mae: 0.325561, mean_q: 0.570332, mean_eps: 0.000000
 1803/5000: episode: 63, duration: 0.445s, episode steps:  31, steps per second:  70, episode reward: 38.592, mean reward:  1.245 [-2.848, 32.204], mean action: 2.323 [0.000, 11.000],  loss: 0.021206, mae: 0.324911, mean_q: 0.539101, mean_eps: 0.000000
 1826/5000: episode: 64, duration: 0.353s, episode steps:  23, steps per second:  65, episode reward: 43.868, mean reward:  1.907 [-2.354, 32.140], mean action: 4.000 [0.000, 15.000],  loss: 0.021753, mae: 0.327495, mean_q: 0.548313, mean_eps: 0.000000
 1853/5000: episode: 65, duration: 0.394s, episode steps:  27, steps per second:  69, episode reward: 38.704, mean reward:  1.433 [-2.266, 31.982], mean action: 2.296 [0.000, 19.000],  loss: 0.019211, mae: 0.317944, mean_q: 0.554947, mean_eps: 0.000000
 1878/5000: episode: 66, duration: 0.360s, episode steps:  25, steps per second:  70, episode reward: 38.398, mean reward:  1.536 [-3.000, 32.180], mean action: 3.440 [0.000, 19.000],  loss: 0.018214, mae: 0.311465, mean_q: 0.559057, mean_eps: 0.000000
 1921/5000: episode: 67, duration: 0.603s, episode steps:  43, steps per second:  71, episode reward: -35.180, mean reward: -0.818 [-32.814,  3.000], mean action: 6.023 [0.000, 19.000],  loss: 0.018398, mae: 0.314067, mean_q: 0.572288, mean_eps: 0.000000
 1948/5000: episode: 68, duration: 0.383s, episode steps:  27, steps per second:  70, episode reward: 38.885, mean reward:  1.440 [-2.570, 32.195], mean action: 3.259 [0.000, 19.000],  loss: 0.019818, mae: 0.322129, mean_q: 0.514554, mean_eps: 0.000000
 1973/5000: episode: 69, duration: 0.361s, episode steps:  25, steps per second:  69, episode reward: 41.519, mean reward:  1.661 [-3.000, 31.872], mean action: 2.480 [0.000, 12.000],  loss: 0.021029, mae: 0.319477, mean_q: 0.511217, mean_eps: 0.000000
 2013/5000: episode: 70, duration: 0.553s, episode steps:  40, steps per second:  72, episode reward: 38.415, mean reward:  0.960 [-2.326, 32.300], mean action: 3.100 [0.000, 19.000],  loss: 0.019276, mae: 0.315066, mean_q: 0.547710, mean_eps: 0.000000
 2036/5000: episode: 71, duration: 0.337s, episode steps:  23, steps per second:  68, episode reward: 41.482, mean reward:  1.804 [-2.833, 32.030], mean action: 2.826 [0.000, 19.000],  loss: 0.020922, mae: 0.319405, mean_q: 0.562645, mean_eps: 0.000000
 2084/5000: episode: 72, duration: 0.722s, episode steps:  48, steps per second:  67, episode reward: 35.124, mean reward:  0.732 [-2.565, 32.030], mean action: 4.208 [0.000, 19.000],  loss: 0.021791, mae: 0.328413, mean_q: 0.542408, mean_eps: 0.000000
 2113/5000: episode: 73, duration: 0.409s, episode steps:  29, steps per second:  71, episode reward: 38.710, mean reward:  1.335 [-2.812, 32.080], mean action: 2.517 [0.000, 19.000],  loss: 0.017037, mae: 0.307719, mean_q: 0.581092, mean_eps: 0.000000
 2138/5000: episode: 74, duration: 0.363s, episode steps:  25, steps per second:  69, episode reward: 43.945, mean reward:  1.758 [-2.320, 32.117], mean action: 2.360 [0.000, 15.000],  loss: 0.021779, mae: 0.334321, mean_q: 0.582444, mean_eps: 0.000000
 2163/5000: episode: 75, duration: 0.388s, episode steps:  25, steps per second:  64, episode reward: 44.340, mean reward:  1.774 [-2.007, 31.910], mean action: 3.440 [0.000, 20.000],  loss: 0.022490, mae: 0.335053, mean_q: 0.555569, mean_eps: 0.000000
 2185/5000: episode: 76, duration: 0.381s, episode steps:  22, steps per second:  58, episode reward: 41.903, mean reward:  1.905 [-2.390, 32.363], mean action: 3.455 [0.000, 16.000],  loss: 0.019923, mae: 0.322321, mean_q: 0.585599, mean_eps: 0.000000
 2204/5000: episode: 77, duration: 0.283s, episode steps:  19, steps per second:  67, episode reward: 41.598, mean reward:  2.189 [-2.535, 32.334], mean action: 3.105 [0.000, 16.000],  loss: 0.019867, mae: 0.316838, mean_q: 0.552861, mean_eps: 0.000000
 2238/5000: episode: 78, duration: 0.471s, episode steps:  34, steps per second:  72, episode reward: -34.240, mean reward: -1.007 [-32.480,  2.440], mean action: 5.324 [0.000, 19.000],  loss: 0.018778, mae: 0.312658, mean_q: 0.554566, mean_eps: 0.000000
 2263/5000: episode: 79, duration: 0.363s, episode steps:  25, steps per second:  69, episode reward: 35.559, mean reward:  1.422 [-2.900, 32.065], mean action: 5.520 [0.000, 20.000],  loss: 0.023844, mae: 0.348439, mean_q: 0.509246, mean_eps: 0.000000
 2293/5000: episode: 80, duration: 0.423s, episode steps:  30, steps per second:  71, episode reward: 35.876, mean reward:  1.196 [-2.425, 32.380], mean action: 4.533 [0.000, 19.000],  loss: 0.023493, mae: 0.342233, mean_q: 0.539103, mean_eps: 0.000000
 2323/5000: episode: 81, duration: 0.424s, episode steps:  30, steps per second:  71, episode reward: 37.164, mean reward:  1.239 [-2.162, 32.156], mean action: 3.967 [0.000, 16.000],  loss: 0.018079, mae: 0.315187, mean_q: 0.613876, mean_eps: 0.000000
 2349/5000: episode: 82, duration: 0.377s, episode steps:  26, steps per second:  69, episode reward: 41.693, mean reward:  1.604 [-2.513, 32.100], mean action: 2.769 [0.000, 12.000],  loss: 0.020721, mae: 0.337307, mean_q: 0.566629, mean_eps: 0.000000
 2380/5000: episode: 83, duration: 0.441s, episode steps:  31, steps per second:  70, episode reward: 38.279, mean reward:  1.235 [-2.383, 32.453], mean action: 3.065 [0.000, 12.000],  loss: 0.022380, mae: 0.339294, mean_q: 0.556223, mean_eps: 0.000000
 2408/5000: episode: 84, duration: 0.398s, episode steps:  28, steps per second:  70, episode reward: 35.963, mean reward:  1.284 [-2.310, 32.523], mean action: 3.607 [0.000, 14.000],  loss: 0.018283, mae: 0.320402, mean_q: 0.536442, mean_eps: 0.000000
 2429/5000: episode: 85, duration: 0.521s, episode steps:  21, steps per second:  40, episode reward: 39.000, mean reward:  1.857 [-2.584, 32.770], mean action: 2.190 [0.000, 9.000],  loss: 0.018444, mae: 0.317312, mean_q: 0.519357, mean_eps: 0.000000
 2454/5000: episode: 86, duration: 0.358s, episode steps:  25, steps per second:  70, episode reward: 36.000, mean reward:  1.440 [-2.363, 32.450], mean action: 4.160 [0.000, 14.000],  loss: 0.016780, mae: 0.307046, mean_q: 0.511405, mean_eps: 0.000000
 2482/5000: episode: 87, duration: 0.400s, episode steps:  28, steps per second:  70, episode reward: 41.876, mean reward:  1.496 [-2.158, 32.130], mean action: 3.250 [0.000, 20.000],  loss: 0.020976, mae: 0.319699, mean_q: 0.506420, mean_eps: 0.000000
 2524/5000: episode: 88, duration: 0.601s, episode steps:  42, steps per second:  70, episode reward: 37.797, mean reward:  0.900 [-2.654, 31.610], mean action: 5.190 [0.000, 19.000],  loss: 0.020915, mae: 0.318529, mean_q: 0.555214, mean_eps: 0.000000
 2548/5000: episode: 89, duration: 0.349s, episode steps:  24, steps per second:  69, episode reward: 38.081, mean reward:  1.587 [-2.302, 31.348], mean action: 3.500 [0.000, 19.000],  loss: 0.019521, mae: 0.308903, mean_q: 0.464778, mean_eps: 0.000000
 2569/5000: episode: 90, duration: 0.315s, episode steps:  21, steps per second:  67, episode reward: 41.498, mean reward:  1.976 [-2.584, 31.738], mean action: 2.381 [0.000, 11.000],  loss: 0.020695, mae: 0.315742, mean_q: 0.523946, mean_eps: 0.000000
 2592/5000: episode: 91, duration: 0.331s, episode steps:  23, steps per second:  69, episode reward: 38.770, mean reward:  1.686 [-2.598, 31.865], mean action: 4.261 [0.000, 19.000],  loss: 0.020952, mae: 0.318459, mean_q: 0.569213, mean_eps: 0.000000
 2631/5000: episode: 92, duration: 0.544s, episode steps:  39, steps per second:  72, episode reward: 38.065, mean reward:  0.976 [-2.944, 32.450], mean action: 3.923 [0.000, 19.000],  loss: 0.021883, mae: 0.322802, mean_q: 0.563778, mean_eps: 0.000000
 2664/5000: episode: 93, duration: 0.476s, episode steps:  33, steps per second:  69, episode reward: -32.280, mean reward: -0.978 [-32.290,  2.630], mean action: 7.727 [0.000, 19.000],  loss: 0.019905, mae: 0.322729, mean_q: 0.572502, mean_eps: 0.000000
 2715/5000: episode: 94, duration: 0.697s, episode steps:  51, steps per second:  73, episode reward: 38.472, mean reward:  0.754 [-2.417, 32.090], mean action: 3.667 [0.000, 20.000],  loss: 0.018538, mae: 0.313978, mean_q: 0.557686, mean_eps: 0.000000
 2737/5000: episode: 95, duration: 0.335s, episode steps:  22, steps per second:  66, episode reward: 41.273, mean reward:  1.876 [-2.252, 32.080], mean action: 4.273 [0.000, 19.000],  loss: 0.020447, mae: 0.325716, mean_q: 0.590426, mean_eps: 0.000000
 2762/5000: episode: 96, duration: 0.362s, episode steps:  25, steps per second:  69, episode reward: 37.416, mean reward:  1.497 [-2.781, 32.184], mean action: 4.240 [0.000, 19.000],  loss: 0.023057, mae: 0.342571, mean_q: 0.624871, mean_eps: 0.000000
 2787/5000: episode: 97, duration: 0.367s, episode steps:  25, steps per second:  68, episode reward: 41.016, mean reward:  1.641 [-2.172, 31.830], mean action: 3.520 [0.000, 19.000],  loss: 0.017987, mae: 0.313711, mean_q: 0.641201, mean_eps: 0.000000
 2807/5000: episode: 98, duration: 0.301s, episode steps:  20, steps per second:  66, episode reward: 45.000, mean reward:  2.250 [-2.865, 32.190], mean action: 2.700 [0.000, 19.000],  loss: 0.016639, mae: 0.315629, mean_q: 0.586579, mean_eps: 0.000000
 2834/5000: episode: 99, duration: 0.390s, episode steps:  27, steps per second:  69, episode reward: 41.860, mean reward:  1.550 [-2.232, 32.113], mean action: 4.407 [0.000, 20.000],  loss: 0.019233, mae: 0.327933, mean_q: 0.598721, mean_eps: 0.000000
 2859/5000: episode: 100, duration: 0.357s, episode steps:  25, steps per second:  70, episode reward: 38.227, mean reward:  1.529 [-3.000, 32.190], mean action: 5.000 [0.000, 19.000],  loss: 0.023162, mae: 0.347442, mean_q: 0.562018, mean_eps: 0.000000
 2884/5000: episode: 101, duration: 0.367s, episode steps:  25, steps per second:  68, episode reward: 38.582, mean reward:  1.543 [-3.000, 31.902], mean action: 3.240 [0.000, 19.000],  loss: 0.019593, mae: 0.323828, mean_q: 0.612091, mean_eps: 0.000000
 2910/5000: episode: 102, duration: 0.377s, episode steps:  26, steps per second:  69, episode reward: 38.866, mean reward:  1.495 [-2.392, 32.230], mean action: 4.192 [1.000, 19.000],  loss: 0.020952, mae: 0.332427, mean_q: 0.581803, mean_eps: 0.000000
 2946/5000: episode: 103, duration: 0.506s, episode steps:  36, steps per second:  71, episode reward: 35.943, mean reward:  0.998 [-2.683, 32.530], mean action: 4.583 [0.000, 19.000],  loss: 0.020930, mae: 0.329055, mean_q: 0.522011, mean_eps: 0.000000
 2973/5000: episode: 104, duration: 0.390s, episode steps:  27, steps per second:  69, episode reward: 42.000, mean reward:  1.556 [-3.000, 32.230], mean action: 2.889 [0.000, 19.000],  loss: 0.019665, mae: 0.323279, mean_q: 0.582704, mean_eps: 0.000000
 2999/5000: episode: 105, duration: 0.378s, episode steps:  26, steps per second:  69, episode reward: 41.753, mean reward:  1.606 [-2.176, 32.020], mean action: 3.731 [1.000, 16.000],  loss: 0.020258, mae: 0.325966, mean_q: 0.520289, mean_eps: 0.000000
 3025/5000: episode: 106, duration: 0.371s, episode steps:  26, steps per second:  70, episode reward: 41.200, mean reward:  1.585 [-3.000, 32.450], mean action: 4.808 [0.000, 16.000],  loss: 0.018731, mae: 0.323648, mean_q: 0.478370, mean_eps: 0.000000
 3047/5000: episode: 107, duration: 0.323s, episode steps:  22, steps per second:  68, episode reward: 44.741, mean reward:  2.034 [-2.869, 32.360], mean action: 3.955 [1.000, 16.000],  loss: 0.020809, mae: 0.327009, mean_q: 0.506245, mean_eps: 0.000000
 3078/5000: episode: 108, duration: 0.438s, episode steps:  31, steps per second:  71, episode reward: 38.074, mean reward:  1.228 [-3.000, 32.084], mean action: 3.806 [0.000, 19.000],  loss: 0.020581, mae: 0.332829, mean_q: 0.497382, mean_eps: 0.000000
 3108/5000: episode: 109, duration: 0.424s, episode steps:  30, steps per second:  71, episode reward: 38.901, mean reward:  1.297 [-2.080, 32.161], mean action: 3.567 [0.000, 19.000],  loss: 0.021508, mae: 0.329582, mean_q: 0.565358, mean_eps: 0.000000
 3135/5000: episode: 110, duration: 0.390s, episode steps:  27, steps per second:  69, episode reward: 41.293, mean reward:  1.529 [-2.909, 32.820], mean action: 4.593 [0.000, 19.000],  loss: 0.022270, mae: 0.330882, mean_q: 0.523105, mean_eps: 0.000000
 3154/5000: episode: 111, duration: 0.283s, episode steps:  19, steps per second:  67, episode reward: 45.000, mean reward:  2.368 [-2.113, 32.510], mean action: 3.000 [0.000, 12.000],  loss: 0.024533, mae: 0.337090, mean_q: 0.516521, mean_eps: 0.000000
 3183/5000: episode: 112, duration: 0.422s, episode steps:  29, steps per second:  69, episode reward: 44.771, mean reward:  1.544 [-2.077, 32.006], mean action: 1.586 [0.000, 11.000],  loss: 0.018947, mae: 0.311431, mean_q: 0.504119, mean_eps: 0.000000
 3238/5000: episode: 113, duration: 0.748s, episode steps:  55, steps per second:  74, episode reward: 37.934, mean reward:  0.690 [-2.506, 32.230], mean action: 4.855 [0.000, 20.000],  loss: 0.019438, mae: 0.323245, mean_q: 0.518656, mean_eps: 0.000000
 3253/5000: episode: 114, duration: 0.233s, episode steps:  15, steps per second:  64, episode reward: 38.819, mean reward:  2.588 [-2.854, 32.729], mean action: 3.667 [0.000, 16.000],  loss: 0.020793, mae: 0.314857, mean_q: 0.530234, mean_eps: 0.000000
 3274/5000: episode: 115, duration: 0.393s, episode steps:  21, steps per second:  53, episode reward: 44.807, mean reward:  2.134 [-2.878, 32.330], mean action: 3.476 [0.000, 12.000],  loss: 0.018260, mae: 0.314518, mean_q: 0.528085, mean_eps: 0.000000
 3320/5000: episode: 116, duration: 0.670s, episode steps:  46, steps per second:  69, episode reward: 35.778, mean reward:  0.778 [-3.000, 32.150], mean action: 6.652 [0.000, 20.000],  loss: 0.020758, mae: 0.320240, mean_q: 0.485528, mean_eps: 0.000000
 3346/5000: episode: 117, duration: 0.377s, episode steps:  26, steps per second:  69, episode reward: 41.377, mean reward:  1.591 [-2.409, 31.870], mean action: 1.462 [0.000, 9.000],  loss: 0.019255, mae: 0.317760, mean_q: 0.516239, mean_eps: 0.000000
 3366/5000: episode: 118, duration: 0.297s, episode steps:  20, steps per second:  67, episode reward: 41.182, mean reward:  2.059 [-3.000, 32.080], mean action: 4.050 [0.000, 20.000],  loss: 0.020006, mae: 0.316723, mean_q: 0.501261, mean_eps: 0.000000
 3392/5000: episode: 119, duration: 0.374s, episode steps:  26, steps per second:  70, episode reward: 38.338, mean reward:  1.475 [-2.690, 32.650], mean action: 4.500 [0.000, 19.000],  loss: 0.018742, mae: 0.308835, mean_q: 0.507976, mean_eps: 0.000000
 3424/5000: episode: 120, duration: 0.448s, episode steps:  32, steps per second:  72, episode reward: 32.395, mean reward:  1.012 [-3.000, 32.093], mean action: 5.438 [0.000, 19.000],  loss: 0.021955, mae: 0.324288, mean_q: 0.491788, mean_eps: 0.000000
 3482/5000: episode: 121, duration: 0.797s, episode steps:  58, steps per second:  73, episode reward: 32.347, mean reward:  0.558 [-3.000, 32.099], mean action: 11.155 [0.000, 19.000],  loss: 0.020967, mae: 0.316161, mean_q: 0.478648, mean_eps: 0.000000
 3514/5000: episode: 122, duration: 0.453s, episode steps:  32, steps per second:  71, episode reward: 35.145, mean reward:  1.098 [-2.655, 32.020], mean action: 4.406 [0.000, 19.000],  loss: 0.020696, mae: 0.317706, mean_q: 0.542528, mean_eps: 0.000000
 3539/5000: episode: 123, duration: 0.358s, episode steps:  25, steps per second:  70, episode reward: 44.766, mean reward:  1.791 [-2.570, 32.160], mean action: 2.560 [0.000, 11.000],  loss: 0.022906, mae: 0.333253, mean_q: 0.538526, mean_eps: 0.000000
 3561/5000: episode: 124, duration: 0.318s, episode steps:  22, steps per second:  69, episode reward: 36.000, mean reward:  1.636 [-2.878, 29.697], mean action: 1.773 [0.000, 11.000],  loss: 0.021344, mae: 0.326064, mean_q: 0.586519, mean_eps: 0.000000
 3580/5000: episode: 125, duration: 0.277s, episode steps:  19, steps per second:  69, episode reward: 35.092, mean reward:  1.847 [-3.000, 32.130], mean action: 3.632 [0.000, 12.000],  loss: 0.022097, mae: 0.324300, mean_q: 0.618483, mean_eps: 0.000000
 3623/5000: episode: 126, duration: 0.602s, episode steps:  43, steps per second:  71, episode reward: 39.000, mean reward:  0.907 [-2.539, 32.200], mean action: 1.326 [0.000, 11.000],  loss: 0.019808, mae: 0.322494, mean_q: 0.591160, mean_eps: 0.000000
 3681/5000: episode: 127, duration: 0.792s, episode steps:  58, steps per second:  73, episode reward: -32.200, mean reward: -0.555 [-32.051,  2.210], mean action: 4.121 [0.000, 20.000],  loss: 0.023245, mae: 0.344320, mean_q: 0.579634, mean_eps: 0.000000
 3709/5000: episode: 128, duration: 0.397s, episode steps:  28, steps per second:  70, episode reward: 42.000, mean reward:  1.500 [-3.000, 32.230], mean action: 2.929 [0.000, 16.000],  loss: 0.018134, mae: 0.308677, mean_q: 0.588932, mean_eps: 0.000000
 3739/5000: episode: 129, duration: 0.426s, episode steps:  30, steps per second:  70, episode reward: 44.567, mean reward:  1.486 [-2.039, 32.060], mean action: 2.933 [0.000, 16.000],  loss: 0.019737, mae: 0.315406, mean_q: 0.536437, mean_eps: 0.000000
 3770/5000: episode: 130, duration: 0.436s, episode steps:  31, steps per second:  71, episode reward: 39.000, mean reward:  1.258 [-3.000, 32.390], mean action: 3.419 [0.000, 20.000],  loss: 0.020305, mae: 0.324316, mean_q: 0.579221, mean_eps: 0.000000
 3788/5000: episode: 131, duration: 0.276s, episode steps:  18, steps per second:  65, episode reward: 41.886, mean reward:  2.327 [-2.756, 32.226], mean action: 3.944 [0.000, 14.000],  loss: 0.023257, mae: 0.340496, mean_q: 0.621791, mean_eps: 0.000000
 3816/5000: episode: 132, duration: 0.421s, episode steps:  28, steps per second:  67, episode reward: 44.884, mean reward:  1.603 [-2.034, 32.444], mean action: 1.893 [0.000, 19.000],  loss: 0.019099, mae: 0.323828, mean_q: 0.613323, mean_eps: 0.000000
 3834/5000: episode: 133, duration: 0.269s, episode steps:  18, steps per second:  67, episode reward: 41.374, mean reward:  2.299 [-2.614, 32.041], mean action: 2.667 [0.000, 13.000],  loss: 0.019870, mae: 0.319141, mean_q: 0.540800, mean_eps: 0.000000
 3864/5000: episode: 134, duration: 0.421s, episode steps:  30, steps per second:  71, episode reward: 35.283, mean reward:  1.176 [-2.881, 32.546], mean action: 8.200 [0.000, 20.000],  loss: 0.021673, mae: 0.329049, mean_q: 0.548975, mean_eps: 0.000000
 3891/5000: episode: 135, duration: 0.391s, episode steps:  27, steps per second:  69, episode reward: -32.640, mean reward: -1.209 [-32.910,  2.463], mean action: 4.852 [0.000, 19.000],  loss: 0.021007, mae: 0.322079, mean_q: 0.571935, mean_eps: 0.000000
 3946/5000: episode: 136, duration: 0.758s, episode steps:  55, steps per second:  73, episode reward: 44.251, mean reward:  0.805 [-2.074, 32.313], mean action: 0.964 [0.000, 13.000],  loss: 0.020483, mae: 0.315418, mean_q: 0.477588, mean_eps: 0.000000
 3966/5000: episode: 137, duration: 0.292s, episode steps:  20, steps per second:  68, episode reward: 42.000, mean reward:  2.100 [-2.169, 32.100], mean action: 3.150 [0.000, 19.000],  loss: 0.019260, mae: 0.311396, mean_q: 0.526060, mean_eps: 0.000000
 3994/5000: episode: 138, duration: 0.405s, episode steps:  28, steps per second:  69, episode reward: 44.304, mean reward:  1.582 [-2.602, 32.020], mean action: 2.179 [0.000, 19.000],  loss: 0.023418, mae: 0.335556, mean_q: 0.584100, mean_eps: 0.000000
 4032/5000: episode: 139, duration: 0.702s, episode steps:  38, steps per second:  54, episode reward: 41.403, mean reward:  1.090 [-2.227, 32.480], mean action: 3.474 [0.000, 20.000],  loss: 0.022227, mae: 0.325926, mean_q: 0.518599, mean_eps: 0.000000
 4053/5000: episode: 140, duration: 0.328s, episode steps:  21, steps per second:  64, episode reward: 47.599, mean reward:  2.267 [-0.669, 32.270], mean action: 4.810 [1.000, 14.000],  loss: 0.020518, mae: 0.315470, mean_q: 0.501685, mean_eps: 0.000000
 4084/5000: episode: 141, duration: 0.438s, episode steps:  31, steps per second:  71, episode reward: 38.655, mean reward:  1.247 [-2.465, 32.055], mean action: 2.323 [0.000, 19.000],  loss: 0.021346, mae: 0.320507, mean_q: 0.532963, mean_eps: 0.000000
 4114/5000: episode: 142, duration: 0.428s, episode steps:  30, steps per second:  70, episode reward: 38.343, mean reward:  1.278 [-2.547, 32.162], mean action: 3.900 [0.000, 16.000],  loss: 0.018206, mae: 0.299714, mean_q: 0.564958, mean_eps: 0.000000
 4141/5000: episode: 143, duration: 0.383s, episode steps:  27, steps per second:  71, episode reward: 38.780, mean reward:  1.436 [-3.000, 32.191], mean action: 5.519 [0.000, 19.000],  loss: 0.022356, mae: 0.322547, mean_q: 0.530164, mean_eps: 0.000000
 4179/5000: episode: 144, duration: 0.546s, episode steps:  38, steps per second:  70, episode reward: 43.741, mean reward:  1.151 [-2.044, 32.020], mean action: 3.711 [1.000, 20.000],  loss: 0.017795, mae: 0.298963, mean_q: 0.528854, mean_eps: 0.000000
 4213/5000: episode: 145, duration: 0.514s, episode steps:  34, steps per second:  66, episode reward: 32.704, mean reward:  0.962 [-2.393, 32.201], mean action: 3.029 [0.000, 12.000],  loss: 0.019225, mae: 0.317026, mean_q: 0.511098, mean_eps: 0.000000
 4247/5000: episode: 146, duration: 0.476s, episode steps:  34, steps per second:  71, episode reward: 36.000, mean reward:  1.059 [-2.576, 32.250], mean action: 3.706 [0.000, 12.000],  loss: 0.021188, mae: 0.321061, mean_q: 0.543939, mean_eps: 0.000000
 4282/5000: episode: 147, duration: 0.499s, episode steps:  35, steps per second:  70, episode reward: 40.609, mean reward:  1.160 [-2.268, 32.380], mean action: 2.057 [0.000, 12.000],  loss: 0.018842, mae: 0.306588, mean_q: 0.585759, mean_eps: 0.000000
 4299/5000: episode: 148, duration: 0.256s, episode steps:  17, steps per second:  66, episode reward: 46.471, mean reward:  2.734 [-0.395, 32.904], mean action: 1.882 [0.000, 4.000],  loss: 0.019230, mae: 0.302842, mean_q: 0.567062, mean_eps: 0.000000
 4317/5000: episode: 149, duration: 0.338s, episode steps:  18, steps per second:  53, episode reward: 44.806, mean reward:  2.489 [-2.217, 32.133], mean action: 3.444 [3.000, 11.000],  loss: 0.025948, mae: 0.348762, mean_q: 0.512268, mean_eps: 0.000000
 4338/5000: episode: 150, duration: 0.308s, episode steps:  21, steps per second:  68, episode reward: 35.810, mean reward:  1.705 [-2.694, 32.250], mean action: 3.429 [1.000, 12.000],  loss: 0.019997, mae: 0.316696, mean_q: 0.499246, mean_eps: 0.000000
 4389/5000: episode: 151, duration: 0.707s, episode steps:  51, steps per second:  72, episode reward: 36.000, mean reward:  0.706 [-2.631, 32.070], mean action: 2.686 [0.000, 12.000],  loss: 0.021205, mae: 0.320510, mean_q: 0.482736, mean_eps: 0.000000
 4410/5000: episode: 152, duration: 0.310s, episode steps:  21, steps per second:  68, episode reward: 42.000, mean reward:  2.000 [-2.844, 32.880], mean action: 2.476 [0.000, 19.000],  loss: 0.021136, mae: 0.324760, mean_q: 0.435300, mean_eps: 0.000000
 4429/5000: episode: 153, duration: 0.289s, episode steps:  19, steps per second:  66, episode reward: 44.020, mean reward:  2.317 [-2.149, 32.495], mean action: 1.632 [0.000, 19.000],  loss: 0.017156, mae: 0.312832, mean_q: 0.463206, mean_eps: 0.000000
 4463/5000: episode: 154, duration: 0.484s, episode steps:  34, steps per second:  70, episode reward: 41.485, mean reward:  1.220 [-2.253, 32.220], mean action: 3.000 [0.000, 12.000],  loss: 0.021051, mae: 0.321726, mean_q: 0.485270, mean_eps: 0.000000
 4487/5000: episode: 155, duration: 0.368s, episode steps:  24, steps per second:  65, episode reward: 43.086, mean reward:  1.795 [-2.548, 32.220], mean action: 2.708 [0.000, 12.000],  loss: 0.018337, mae: 0.302393, mean_q: 0.553881, mean_eps: 0.000000
 4511/5000: episode: 156, duration: 0.344s, episode steps:  24, steps per second:  70, episode reward: 35.138, mean reward:  1.464 [-2.397, 32.373], mean action: 3.792 [0.000, 19.000],  loss: 0.018544, mae: 0.310823, mean_q: 0.508744, mean_eps: 0.000000
 4536/5000: episode: 157, duration: 0.387s, episode steps:  25, steps per second:  65, episode reward: 41.251, mean reward:  1.650 [-2.286, 32.140], mean action: 3.040 [0.000, 19.000],  loss: 0.022974, mae: 0.324545, mean_q: 0.535368, mean_eps: 0.000000
 4580/5000: episode: 158, duration: 0.616s, episode steps:  44, steps per second:  71, episode reward: 32.717, mean reward:  0.744 [-2.455, 32.140], mean action: 4.318 [0.000, 17.000],  loss: 0.023523, mae: 0.324754, mean_q: 0.589954, mean_eps: 0.000000
 4601/5000: episode: 159, duration: 0.305s, episode steps:  21, steps per second:  69, episode reward: 42.000, mean reward:  2.000 [-2.932, 32.770], mean action: 3.619 [1.000, 15.000],  loss: 0.018178, mae: 0.302738, mean_q: 0.492409, mean_eps: 0.000000
 4630/5000: episode: 160, duration: 0.413s, episode steps:  29, steps per second:  70, episode reward: 41.603, mean reward:  1.435 [-2.126, 32.471], mean action: 2.207 [0.000, 13.000],  loss: 0.017104, mae: 0.299950, mean_q: 0.517077, mean_eps: 0.000000
 4652/5000: episode: 161, duration: 0.319s, episode steps:  22, steps per second:  69, episode reward: 41.287, mean reward:  1.877 [-2.798, 32.680], mean action: 1.318 [0.000, 13.000],  loss: 0.019893, mae: 0.310917, mean_q: 0.495108, mean_eps: 0.000000
 4673/5000: episode: 162, duration: 0.315s, episode steps:  21, steps per second:  67, episode reward: 41.559, mean reward:  1.979 [-2.524, 32.142], mean action: 2.714 [0.000, 12.000],  loss: 0.022334, mae: 0.328622, mean_q: 0.456506, mean_eps: 0.000000
 4711/5000: episode: 163, duration: 0.549s, episode steps:  38, steps per second:  69, episode reward: 40.658, mean reward:  1.070 [-2.259, 32.110], mean action: 3.658 [1.000, 20.000],  loss: 0.021196, mae: 0.321365, mean_q: 0.536668, mean_eps: 0.000000
 4733/5000: episode: 164, duration: 0.319s, episode steps:  22, steps per second:  69, episode reward: 47.387, mean reward:  2.154 [-0.480, 32.610], mean action: 2.273 [0.000, 14.000],  loss: 0.022026, mae: 0.328065, mean_q: 0.600166, mean_eps: 0.000000
 4750/5000: episode: 165, duration: 0.253s, episode steps:  17, steps per second:  67, episode reward: 44.522, mean reward:  2.619 [-2.214, 32.894], mean action: 1.765 [0.000, 3.000],  loss: 0.023323, mae: 0.331528, mean_q: 0.622636, mean_eps: 0.000000
 4783/5000: episode: 166, duration: 0.471s, episode steps:  33, steps per second:  70, episode reward: 39.000, mean reward:  1.182 [-2.840, 32.940], mean action: 5.909 [0.000, 16.000],  loss: 0.021625, mae: 0.328169, mean_q: 0.624791, mean_eps: 0.000000
 4810/5000: episode: 167, duration: 0.386s, episode steps:  27, steps per second:  70, episode reward: 40.666, mean reward:  1.506 [-3.000, 32.144], mean action: 3.556 [0.000, 15.000],  loss: 0.018334, mae: 0.313184, mean_q: 0.518764, mean_eps: 0.000000
 4830/5000: episode: 168, duration: 0.298s, episode steps:  20, steps per second:  67, episode reward: 43.904, mean reward:  2.195 [-2.328, 32.131], mean action: 4.150 [0.000, 15.000],  loss: 0.020850, mae: 0.316538, mean_q: 0.530270, mean_eps: 0.000000
 4911/5000: episode: 169, duration: 1.100s, episode steps:  81, steps per second:  74, episode reward: 32.902, mean reward:  0.406 [-3.000, 32.246], mean action: 8.963 [0.000, 15.000],  loss: 0.019789, mae: 0.319277, mean_q: 0.526767, mean_eps: 0.000000
 4951/5000: episode: 170, duration: 0.557s, episode steps:  40, steps per second:  72, episode reward: 34.382, mean reward:  0.860 [-2.631, 32.080], mean action: 5.050 [1.000, 15.000],  loss: 0.021553, mae: 0.324419, mean_q: 0.549318, mean_eps: 0.000000
done, took 64.931 seconds
DQN Evaluation: 12108 victories out of 14103 episodes
Training for 5000 steps ...
   22/5000: episode: 1, duration: 0.852s, episode steps:  22, steps per second:  26, episode reward: 36.000, mean reward:  1.636 [-3.000, 32.910], mean action: 5.227 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   41/5000: episode: 2, duration: 0.215s, episode steps:  19, steps per second:  88, episode reward: 38.675, mean reward:  2.036 [-2.221, 33.000], mean action: 8.895 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   59/5000: episode: 3, duration: 0.307s, episode steps:  18, steps per second:  59, episode reward: 35.136, mean reward:  1.952 [-3.000, 32.340], mean action: 7.667 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   80/5000: episode: 4, duration: 27.071s, episode steps:  21, steps per second:   1, episode reward: 36.000, mean reward:  1.714 [-3.000, 32.090], mean action: 4.190 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  103/5000: episode: 5, duration: 0.180s, episode steps:  23, steps per second: 127, episode reward: 38.058, mean reward:  1.655 [-2.262, 33.000], mean action: 7.348 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  115/5000: episode: 6, duration: 0.115s, episode steps:  12, steps per second: 104, episode reward: 44.046, mean reward:  3.671 [-2.426, 32.783], mean action: 4.250 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  136/5000: episode: 7, duration: 0.162s, episode steps:  21, steps per second: 129, episode reward: 38.234, mean reward:  1.821 [-2.506, 32.160], mean action: 4.619 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  165/5000: episode: 8, duration: 0.207s, episode steps:  29, steps per second: 140, episode reward: 35.288, mean reward:  1.217 [-2.369, 32.030], mean action: 8.448 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  186/5000: episode: 9, duration: 0.150s, episode steps:  21, steps per second: 140, episode reward: 32.507, mean reward:  1.548 [-3.000, 31.767], mean action: 5.048 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  204/5000: episode: 10, duration: 0.141s, episode steps:  18, steps per second: 127, episode reward: 38.802, mean reward:  2.156 [-2.941, 32.280], mean action: 5.333 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  219/5000: episode: 11, duration: 0.125s, episode steps:  15, steps per second: 120, episode reward: 38.708, mean reward:  2.581 [-2.900, 32.321], mean action: 5.067 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  239/5000: episode: 12, duration: 0.165s, episode steps:  20, steps per second: 121, episode reward: 35.063, mean reward:  1.753 [-3.000, 32.756], mean action: 5.450 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  252/5000: episode: 13, duration: 0.124s, episode steps:  13, steps per second: 105, episode reward: 41.939, mean reward:  3.226 [-2.396, 33.000], mean action: 3.154 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  273/5000: episode: 14, duration: 0.157s, episode steps:  21, steps per second: 134, episode reward: -35.380, mean reward: -1.685 [-33.000,  2.222], mean action: 6.714 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  291/5000: episode: 15, duration: 0.162s, episode steps:  18, steps per second: 111, episode reward: 44.225, mean reward:  2.457 [-2.171, 33.000], mean action: 2.333 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  313/5000: episode: 16, duration: 0.177s, episode steps:  22, steps per second: 124, episode reward: -32.830, mean reward: -1.492 [-32.673,  2.840], mean action: 7.136 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  327/5000: episode: 17, duration: 0.127s, episode steps:  14, steps per second: 110, episode reward: 41.253, mean reward:  2.947 [-2.645, 32.360], mean action: 5.714 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  357/5000: episode: 18, duration: 0.226s, episode steps:  30, steps per second: 133, episode reward: 35.138, mean reward:  1.171 [-2.338, 32.035], mean action: 8.033 [1.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  367/5000: episode: 19, duration: 0.097s, episode steps:  10, steps per second: 104, episode reward: 44.348, mean reward:  4.435 [-2.133, 32.903], mean action: 5.600 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  384/5000: episode: 20, duration: 0.154s, episode steps:  17, steps per second: 111, episode reward: 41.682, mean reward:  2.452 [-2.289, 32.230], mean action: 3.765 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  410/5000: episode: 21, duration: 0.195s, episode steps:  26, steps per second: 133, episode reward: -32.340, mean reward: -1.244 [-32.523,  2.901], mean action: 6.654 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  425/5000: episode: 22, duration: 0.136s, episode steps:  15, steps per second: 110, episode reward: 43.506, mean reward:  2.900 [-2.237, 32.196], mean action: 0.733 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  437/5000: episode: 23, duration: 0.102s, episode steps:  12, steps per second: 118, episode reward: 44.171, mean reward:  3.681 [-2.084, 33.000], mean action: 2.417 [0.000, 6.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  465/5000: episode: 24, duration: 0.212s, episode steps:  28, steps per second: 132, episode reward: -32.160, mean reward: -1.149 [-31.593,  2.402], mean action: 7.321 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  491/5000: episode: 25, duration: 0.166s, episode steps:  26, steps per second: 157, episode reward: -35.210, mean reward: -1.354 [-32.283,  2.862], mean action: 7.115 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  538/5000: episode: 26, duration: 0.307s, episode steps:  47, steps per second: 153, episode reward: 32.848, mean reward:  0.699 [-2.875, 32.109], mean action: 5.383 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  556/5000: episode: 27, duration: 0.138s, episode steps:  18, steps per second: 131, episode reward: 41.358, mean reward:  2.298 [-2.149, 32.550], mean action: 3.167 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  583/5000: episode: 28, duration: 0.191s, episode steps:  27, steps per second: 141, episode reward: 35.352, mean reward:  1.309 [-2.174, 31.939], mean action: 4.889 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  610/5000: episode: 29, duration: 0.189s, episode steps:  27, steps per second: 143, episode reward: -35.630, mean reward: -1.320 [-32.204,  2.674], mean action: 10.074 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  633/5000: episode: 30, duration: 0.167s, episode steps:  23, steps per second: 138, episode reward: 32.904, mean reward:  1.431 [-2.590, 32.324], mean action: 6.261 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  666/5000: episode: 31, duration: 0.221s, episode steps:  33, steps per second: 149, episode reward: -32.050, mean reward: -0.971 [-32.110,  2.496], mean action: 5.697 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  696/5000: episode: 32, duration: 0.209s, episode steps:  30, steps per second: 144, episode reward: 32.430, mean reward:  1.081 [-3.000, 32.029], mean action: 5.367 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  729/5000: episode: 33, duration: 0.239s, episode steps:  33, steps per second: 138, episode reward: 33.000, mean reward:  1.000 [-2.603, 32.080], mean action: 10.091 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  754/5000: episode: 34, duration: 0.185s, episode steps:  25, steps per second: 135, episode reward: 37.087, mean reward:  1.483 [-2.256, 31.182], mean action: 6.160 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  771/5000: episode: 35, duration: 0.131s, episode steps:  17, steps per second: 129, episode reward: 44.198, mean reward:  2.600 [-2.203, 33.000], mean action: 4.118 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  789/5000: episode: 36, duration: 0.137s, episode steps:  18, steps per second: 131, episode reward: 35.733, mean reward:  1.985 [-3.000, 32.270], mean action: 5.056 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  833/5000: episode: 37, duration: 0.363s, episode steps:  44, steps per second: 121, episode reward: 32.401, mean reward:  0.736 [-2.451, 30.405], mean action: 3.705 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  852/5000: episode: 38, duration: 0.143s, episode steps:  19, steps per second: 133, episode reward: 38.409, mean reward:  2.022 [-2.294, 33.000], mean action: 6.895 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  875/5000: episode: 39, duration: 1.014s, episode steps:  23, steps per second:  23, episode reward: 36.000, mean reward:  1.565 [-3.000, 32.140], mean action: 4.304 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  897/5000: episode: 40, duration: 0.157s, episode steps:  22, steps per second: 140, episode reward: -41.430, mean reward: -1.883 [-32.188,  2.510], mean action: 8.818 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  918/5000: episode: 41, duration: 20.996s, episode steps:  21, steps per second:   1, episode reward: 41.232, mean reward:  1.963 [-2.270, 32.440], mean action: 3.905 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  939/5000: episode: 42, duration: 0.171s, episode steps:  21, steps per second: 123, episode reward: 40.633, mean reward:  1.935 [-2.168, 32.130], mean action: 4.905 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  958/5000: episode: 43, duration: 0.143s, episode steps:  19, steps per second: 133, episode reward: -38.550, mean reward: -2.029 [-33.000,  3.000], mean action: 9.000 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  981/5000: episode: 44, duration: 0.175s, episode steps:  23, steps per second: 131, episode reward: 41.020, mean reward:  1.783 [-2.197, 32.520], mean action: 4.130 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1002/5000: episode: 45, duration: 0.235s, episode steps:  21, steps per second:  89, episode reward: 35.187, mean reward:  1.676 [-3.000, 32.340], mean action: 7.619 [0.000, 19.000],  loss: 0.019673, mae: 0.332452, mean_q: 0.613758, mean_eps: 0.000000
 1059/5000: episode: 46, duration: 0.977s, episode steps:  57, steps per second:  58, episode reward: 33.000, mean reward:  0.579 [-2.900, 32.640], mean action: 12.684 [1.000, 20.000],  loss: 0.018971, mae: 0.304093, mean_q: 0.544682, mean_eps: 0.000000
 1080/5000: episode: 47, duration: 0.337s, episode steps:  21, steps per second:  62, episode reward: -35.690, mean reward: -1.700 [-32.088,  2.670], mean action: 3.476 [0.000, 16.000],  loss: 0.022501, mae: 0.321199, mean_q: 0.510949, mean_eps: 0.000000
 1101/5000: episode: 48, duration: 0.384s, episode steps:  21, steps per second:  55, episode reward: 36.000, mean reward:  1.714 [-2.488, 30.874], mean action: 3.333 [0.000, 16.000],  loss: 0.019746, mae: 0.301619, mean_q: 0.465186, mean_eps: 0.000000
 1122/5000: episode: 49, duration: 0.350s, episode steps:  21, steps per second:  60, episode reward: 32.645, mean reward:  1.555 [-2.472, 32.140], mean action: 3.714 [0.000, 12.000],  loss: 0.022129, mae: 0.316272, mean_q: 0.494802, mean_eps: 0.000000
 1143/5000: episode: 50, duration: 0.319s, episode steps:  21, steps per second:  66, episode reward: -36.000, mean reward: -1.714 [-29.695,  2.301], mean action: 4.667 [0.000, 14.000],  loss: 0.018559, mae: 0.307670, mean_q: 0.522638, mean_eps: 0.000000
 1167/5000: episode: 51, duration: 0.360s, episode steps:  24, steps per second:  67, episode reward: -32.570, mean reward: -1.357 [-31.598,  2.760], mean action: 3.375 [0.000, 11.000],  loss: 0.020983, mae: 0.308659, mean_q: 0.505716, mean_eps: 0.000000
 1195/5000: episode: 52, duration: 0.427s, episode steps:  28, steps per second:  66, episode reward: 32.904, mean reward:  1.175 [-3.000, 32.024], mean action: 7.286 [0.000, 20.000],  loss: 0.023164, mae: 0.323078, mean_q: 0.501595, mean_eps: 0.000000
 1211/5000: episode: 53, duration: 0.254s, episode steps:  16, steps per second:  63, episode reward: -41.780, mean reward: -2.611 [-32.780,  2.230], mean action: 6.312 [0.000, 16.000],  loss: 0.021801, mae: 0.312975, mean_q: 0.471527, mean_eps: 0.000000
 1228/5000: episode: 54, duration: 0.331s, episode steps:  17, steps per second:  51, episode reward: 38.474, mean reward:  2.263 [-2.286, 32.379], mean action: 2.941 [0.000, 16.000],  loss: 0.019639, mae: 0.308599, mean_q: 0.419608, mean_eps: 0.000000
 1247/5000: episode: 55, duration: 0.464s, episode steps:  19, steps per second:  41, episode reward: 36.000, mean reward:  1.895 [-2.758, 32.070], mean action: 3.947 [0.000, 16.000],  loss: 0.025730, mae: 0.334581, mean_q: 0.509560, mean_eps: 0.000000
 1268/5000: episode: 56, duration: 0.445s, episode steps:  21, steps per second:  47, episode reward: 35.378, mean reward:  1.685 [-3.000, 32.476], mean action: 4.238 [0.000, 16.000],  loss: 0.018827, mae: 0.307165, mean_q: 0.519814, mean_eps: 0.000000
 1290/5000: episode: 57, duration: 0.337s, episode steps:  22, steps per second:  65, episode reward: 35.651, mean reward:  1.620 [-2.672, 32.060], mean action: 4.773 [0.000, 16.000],  loss: 0.017837, mae: 0.300853, mean_q: 0.523955, mean_eps: 0.000000
 1308/5000: episode: 58, duration: 0.308s, episode steps:  18, steps per second:  58, episode reward: 43.520, mean reward:  2.418 [-2.998, 31.967], mean action: 2.000 [0.000, 16.000],  loss: 0.016219, mae: 0.295512, mean_q: 0.468914, mean_eps: 0.000000
 1326/5000: episode: 59, duration: 0.297s, episode steps:  18, steps per second:  61, episode reward: 38.478, mean reward:  2.138 [-2.465, 32.426], mean action: 4.389 [0.000, 16.000],  loss: 0.021816, mae: 0.319372, mean_q: 0.498991, mean_eps: 0.000000
 1352/5000: episode: 60, duration: 0.426s, episode steps:  26, steps per second:  61, episode reward: 32.754, mean reward:  1.260 [-2.900, 32.030], mean action: 4.154 [0.000, 16.000],  loss: 0.021716, mae: 0.318904, mean_q: 0.572586, mean_eps: 0.000000
 1375/5000: episode: 61, duration: 0.374s, episode steps:  23, steps per second:  61, episode reward: 35.254, mean reward:  1.533 [-3.000, 32.020], mean action: 3.957 [0.000, 15.000],  loss: 0.018325, mae: 0.306613, mean_q: 0.567259, mean_eps: 0.000000
 1397/5000: episode: 62, duration: 0.344s, episode steps:  22, steps per second:  64, episode reward: 35.901, mean reward:  1.632 [-2.408, 32.191], mean action: 3.636 [0.000, 12.000],  loss: 0.024116, mae: 0.331962, mean_q: 0.530904, mean_eps: 0.000000
 1421/5000: episode: 63, duration: 0.364s, episode steps:  24, steps per second:  66, episode reward: -33.000, mean reward: -1.375 [-32.268,  2.303], mean action: 3.875 [0.000, 14.000],  loss: 0.016898, mae: 0.300695, mean_q: 0.554575, mean_eps: 0.000000
 1453/5000: episode: 64, duration: 0.473s, episode steps:  32, steps per second:  68, episode reward: 36.000, mean reward:  1.125 [-2.134, 30.550], mean action: 2.219 [0.000, 18.000],  loss: 0.018046, mae: 0.307763, mean_q: 0.506299, mean_eps: 0.000000
 1468/5000: episode: 65, duration: 0.253s, episode steps:  15, steps per second:  59, episode reward: 38.258, mean reward:  2.551 [-3.000, 32.490], mean action: 3.867 [0.000, 12.000],  loss: 0.024532, mae: 0.334488, mean_q: 0.552943, mean_eps: 0.000000
 1499/5000: episode: 66, duration: 0.440s, episode steps:  31, steps per second:  70, episode reward: 35.751, mean reward:  1.153 [-2.333, 32.330], mean action: 4.000 [0.000, 9.000],  loss: 0.022024, mae: 0.324342, mean_q: 0.577361, mean_eps: 0.000000
 1519/5000: episode: 67, duration: 0.303s, episode steps:  20, steps per second:  66, episode reward: 33.000, mean reward:  1.650 [-3.000, 30.608], mean action: 3.850 [0.000, 9.000],  loss: 0.015454, mae: 0.295315, mean_q: 0.536437, mean_eps: 0.000000
 1539/5000: episode: 68, duration: 0.315s, episode steps:  20, steps per second:  64, episode reward: 36.000, mean reward:  1.800 [-2.750, 32.290], mean action: 3.350 [0.000, 14.000],  loss: 0.021827, mae: 0.325717, mean_q: 0.521873, mean_eps: 0.000000
 1560/5000: episode: 69, duration: 0.320s, episode steps:  21, steps per second:  66, episode reward: 38.122, mean reward:  1.815 [-2.998, 32.861], mean action: 4.048 [0.000, 14.000],  loss: 0.019746, mae: 0.312098, mean_q: 0.551919, mean_eps: 0.000000
 1582/5000: episode: 70, duration: 0.333s, episode steps:  22, steps per second:  66, episode reward: 36.000, mean reward:  1.636 [-2.443, 32.440], mean action: 2.636 [0.000, 12.000],  loss: 0.019069, mae: 0.310269, mean_q: 0.531087, mean_eps: 0.000000
 1601/5000: episode: 71, duration: 0.284s, episode steps:  19, steps per second:  67, episode reward: 38.011, mean reward:  2.001 [-2.428, 32.270], mean action: 5.158 [0.000, 14.000],  loss: 0.018310, mae: 0.302481, mean_q: 0.543136, mean_eps: 0.000000
 1620/5000: episode: 72, duration: 0.296s, episode steps:  19, steps per second:  64, episode reward: 41.004, mean reward:  2.158 [-2.608, 32.506], mean action: 3.105 [0.000, 12.000],  loss: 0.022044, mae: 0.322101, mean_q: 0.522655, mean_eps: 0.000000
 1637/5000: episode: 73, duration: 0.285s, episode steps:  17, steps per second:  60, episode reward: 40.903, mean reward:  2.406 [-2.118, 32.891], mean action: 4.647 [0.000, 14.000],  loss: 0.018620, mae: 0.317272, mean_q: 0.608565, mean_eps: 0.000000
 1667/5000: episode: 74, duration: 0.442s, episode steps:  30, steps per second:  68, episode reward: 32.903, mean reward:  1.097 [-2.360, 31.963], mean action: 2.900 [0.000, 12.000],  loss: 0.019901, mae: 0.316294, mean_q: 0.585496, mean_eps: 0.000000
 1689/5000: episode: 75, duration: 0.320s, episode steps:  22, steps per second:  69, episode reward: 32.902, mean reward:  1.496 [-3.000, 32.462], mean action: 3.364 [0.000, 12.000],  loss: 0.018337, mae: 0.310503, mean_q: 0.581864, mean_eps: 0.000000
 1721/5000: episode: 76, duration: 0.470s, episode steps:  32, steps per second:  68, episode reward: -33.000, mean reward: -1.031 [-32.234,  2.467], mean action: 5.906 [0.000, 17.000],  loss: 0.020015, mae: 0.323953, mean_q: 0.517397, mean_eps: 0.000000
 1752/5000: episode: 77, duration: 0.457s, episode steps:  31, steps per second:  68, episode reward: 32.429, mean reward:  1.046 [-2.474, 31.679], mean action: 7.161 [0.000, 20.000],  loss: 0.023075, mae: 0.335546, mean_q: 0.517642, mean_eps: 0.000000
 1767/5000: episode: 78, duration: 0.240s, episode steps:  15, steps per second:  62, episode reward: 38.789, mean reward:  2.586 [-2.475, 32.139], mean action: 4.267 [0.000, 16.000],  loss: 0.021494, mae: 0.327535, mean_q: 0.478189, mean_eps: 0.000000
 1806/5000: episode: 79, duration: 0.581s, episode steps:  39, steps per second:  67, episode reward: -32.820, mean reward: -0.842 [-32.117,  2.820], mean action: 7.179 [0.000, 19.000],  loss: 0.021190, mae: 0.316068, mean_q: 0.488567, mean_eps: 0.000000
 1829/5000: episode: 80, duration: 0.337s, episode steps:  23, steps per second:  68, episode reward: -32.680, mean reward: -1.421 [-31.734,  2.231], mean action: 2.826 [0.000, 9.000],  loss: 0.019737, mae: 0.309953, mean_q: 0.496155, mean_eps: 0.000000
 1847/5000: episode: 81, duration: 0.275s, episode steps:  18, steps per second:  65, episode reward: 38.901, mean reward:  2.161 [-2.336, 32.711], mean action: 4.278 [0.000, 21.000],  loss: 0.017042, mae: 0.297021, mean_q: 0.520056, mean_eps: 0.000000
 1866/5000: episode: 82, duration: 0.293s, episode steps:  19, steps per second:  65, episode reward: 41.058, mean reward:  2.161 [-2.481, 32.177], mean action: 2.263 [0.000, 9.000],  loss: 0.023593, mae: 0.326222, mean_q: 0.562968, mean_eps: 0.000000
 1904/5000: episode: 83, duration: 0.568s, episode steps:  38, steps per second:  67, episode reward: -32.040, mean reward: -0.843 [-32.022,  2.496], mean action: 8.974 [0.000, 18.000],  loss: 0.022083, mae: 0.327136, mean_q: 0.518954, mean_eps: 0.000000
 1919/5000: episode: 84, duration: 0.246s, episode steps:  15, steps per second:  61, episode reward: 41.313, mean reward:  2.754 [-2.196, 32.180], mean action: 3.400 [0.000, 13.000],  loss: 0.021722, mae: 0.318301, mean_q: 0.524081, mean_eps: 0.000000
 1942/5000: episode: 85, duration: 0.348s, episode steps:  23, steps per second:  66, episode reward: -36.000, mean reward: -1.565 [-30.571,  2.200], mean action: 4.478 [0.000, 14.000],  loss: 0.019028, mae: 0.311658, mean_q: 0.502968, mean_eps: 0.000000
 1956/5000: episode: 86, duration: 0.224s, episode steps:  14, steps per second:  62, episode reward: 41.208, mean reward:  2.943 [-2.263, 33.000], mean action: 4.786 [0.000, 13.000],  loss: 0.019768, mae: 0.307864, mean_q: 0.462912, mean_eps: 0.000000
 1982/5000: episode: 87, duration: 0.392s, episode steps:  26, steps per second:  66, episode reward: 40.551, mean reward:  1.560 [-2.138, 32.020], mean action: 3.846 [0.000, 13.000],  loss: 0.022947, mae: 0.332283, mean_q: 0.482764, mean_eps: 0.000000
 2005/5000: episode: 88, duration: 0.347s, episode steps:  23, steps per second:  66, episode reward: 33.000, mean reward:  1.435 [-3.000, 30.973], mean action: 2.870 [0.000, 18.000],  loss: 0.021594, mae: 0.322516, mean_q: 0.452286, mean_eps: 0.000000
 2047/5000: episode: 89, duration: 0.609s, episode steps:  42, steps per second:  69, episode reward: 33.000, mean reward:  0.786 [-2.435, 29.010], mean action: 3.595 [0.000, 15.000],  loss: 0.022216, mae: 0.322047, mean_q: 0.527450, mean_eps: 0.000000
 2066/5000: episode: 90, duration: 0.300s, episode steps:  19, steps per second:  63, episode reward: 38.064, mean reward:  2.003 [-2.558, 31.960], mean action: 3.526 [1.000, 15.000],  loss: 0.018749, mae: 0.308754, mean_q: 0.547712, mean_eps: 0.000000
 2085/5000: episode: 91, duration: 0.288s, episode steps:  19, steps per second:  66, episode reward: -35.670, mean reward: -1.877 [-32.152,  2.480], mean action: 4.158 [0.000, 15.000],  loss: 0.024154, mae: 0.336670, mean_q: 0.554117, mean_eps: 0.000000
 2108/5000: episode: 92, duration: 0.343s, episode steps:  23, steps per second:  67, episode reward: 36.000, mean reward:  1.565 [-2.779, 32.300], mean action: 4.087 [1.000, 15.000],  loss: 0.022233, mae: 0.322506, mean_q: 0.505481, mean_eps: 0.000000
 2152/5000: episode: 93, duration: 0.649s, episode steps:  44, steps per second:  68, episode reward: 36.000, mean reward:  0.818 [-2.260, 32.070], mean action: 6.136 [0.000, 15.000],  loss: 0.021591, mae: 0.329214, mean_q: 0.528234, mean_eps: 0.000000
 2164/5000: episode: 94, duration: 0.206s, episode steps:  12, steps per second:  58, episode reward: 47.253, mean reward:  3.938 [ 0.223, 32.210], mean action: 2.333 [2.000, 3.000],  loss: 0.015872, mae: 0.298268, mean_q: 0.556495, mean_eps: 0.000000
 2190/5000: episode: 95, duration: 0.385s, episode steps:  26, steps per second:  68, episode reward: 35.802, mean reward:  1.377 [-2.338, 32.901], mean action: 4.038 [0.000, 15.000],  loss: 0.018455, mae: 0.319208, mean_q: 0.594085, mean_eps: 0.000000
 2216/5000: episode: 96, duration: 0.400s, episode steps:  26, steps per second:  65, episode reward: 37.559, mean reward:  1.445 [-2.398, 31.950], mean action: 5.154 [0.000, 18.000],  loss: 0.015318, mae: 0.307234, mean_q: 0.580683, mean_eps: 0.000000
 2238/5000: episode: 97, duration: 0.422s, episode steps:  22, steps per second:  52, episode reward: 38.701, mean reward:  1.759 [-2.700, 32.400], mean action: 5.273 [2.000, 18.000],  loss: 0.020472, mae: 0.323667, mean_q: 0.496265, mean_eps: 0.000000
 2258/5000: episode: 98, duration: 0.388s, episode steps:  20, steps per second:  52, episode reward: 38.349, mean reward:  1.917 [-2.362, 32.330], mean action: 3.500 [0.000, 15.000],  loss: 0.022626, mae: 0.331684, mean_q: 0.521499, mean_eps: 0.000000
 2290/5000: episode: 99, duration: 0.485s, episode steps:  32, steps per second:  66, episode reward: 35.518, mean reward:  1.110 [-3.000, 32.040], mean action: 6.000 [0.000, 19.000],  loss: 0.020194, mae: 0.326844, mean_q: 0.521257, mean_eps: 0.000000
 2316/5000: episode: 100, duration: 0.420s, episode steps:  26, steps per second:  62, episode reward: 37.202, mean reward:  1.431 [-2.415, 32.410], mean action: 6.962 [0.000, 20.000],  loss: 0.022304, mae: 0.331464, mean_q: 0.537272, mean_eps: 0.000000
 2341/5000: episode: 101, duration: 0.380s, episode steps:  25, steps per second:  66, episode reward: 35.349, mean reward:  1.414 [-2.763, 32.070], mean action: 4.000 [0.000, 15.000],  loss: 0.023300, mae: 0.338104, mean_q: 0.524074, mean_eps: 0.000000
 2365/5000: episode: 102, duration: 0.349s, episode steps:  24, steps per second:  69, episode reward: -33.000, mean reward: -1.375 [-32.093,  2.110], mean action: 3.542 [0.000, 15.000],  loss: 0.021753, mae: 0.328719, mean_q: 0.484706, mean_eps: 0.000000
 2396/5000: episode: 103, duration: 0.444s, episode steps:  31, steps per second:  70, episode reward: -32.130, mean reward: -1.036 [-32.028,  2.229], mean action: 2.452 [0.000, 12.000],  loss: 0.021756, mae: 0.325397, mean_q: 0.495617, mean_eps: 0.000000
 2421/5000: episode: 104, duration: 0.367s, episode steps:  25, steps per second:  68, episode reward: -32.510, mean reward: -1.300 [-32.041,  2.252], mean action: 4.200 [0.000, 16.000],  loss: 0.017701, mae: 0.305857, mean_q: 0.435075, mean_eps: 0.000000
 2449/5000: episode: 105, duration: 0.404s, episode steps:  28, steps per second:  69, episode reward: -32.110, mean reward: -1.147 [-32.447,  2.390], mean action: 4.464 [0.000, 15.000],  loss: 0.019697, mae: 0.311349, mean_q: 0.494197, mean_eps: 0.000000
 2470/5000: episode: 106, duration: 0.342s, episode steps:  21, steps per second:  61, episode reward: 32.503, mean reward:  1.548 [-3.000, 31.753], mean action: 3.048 [0.000, 11.000],  loss: 0.022975, mae: 0.323376, mean_q: 0.570674, mean_eps: 0.000000
 2494/5000: episode: 107, duration: 0.359s, episode steps:  24, steps per second:  67, episode reward: 35.402, mean reward:  1.475 [-3.000, 32.154], mean action: 4.500 [0.000, 16.000],  loss: 0.019591, mae: 0.317714, mean_q: 0.554027, mean_eps: 0.000000
 2518/5000: episode: 108, duration: 0.363s, episode steps:  24, steps per second:  66, episode reward: -32.190, mean reward: -1.341 [-31.581,  2.320], mean action: 4.333 [0.000, 19.000],  loss: 0.019481, mae: 0.314018, mean_q: 0.484073, mean_eps: 0.000000
 2539/5000: episode: 109, duration: 0.312s, episode steps:  21, steps per second:  67, episode reward: 32.500, mean reward:  1.548 [-3.000, 31.540], mean action: 3.571 [0.000, 19.000],  loss: 0.020362, mae: 0.311662, mean_q: 0.504893, mean_eps: 0.000000
 2572/5000: episode: 110, duration: 0.469s, episode steps:  33, steps per second:  70, episode reward: -32.850, mean reward: -0.995 [-32.850,  4.000], mean action: 7.424 [0.000, 19.000],  loss: 0.022099, mae: 0.325744, mean_q: 0.585927, mean_eps: 0.000000
 2590/5000: episode: 111, duration: 0.267s, episode steps:  18, steps per second:  68, episode reward: -36.000, mean reward: -2.000 [-32.961,  2.571], mean action: 5.167 [0.000, 19.000],  loss: 0.019599, mae: 0.318277, mean_q: 0.561166, mean_eps: 0.000000
 2618/5000: episode: 112, duration: 0.409s, episode steps:  28, steps per second:  68, episode reward: 32.010, mean reward:  1.143 [-2.786, 31.450], mean action: 4.143 [0.000, 19.000],  loss: 0.019308, mae: 0.315331, mean_q: 0.528301, mean_eps: 0.000000
 2645/5000: episode: 113, duration: 0.399s, episode steps:  27, steps per second:  68, episode reward: -33.000, mean reward: -1.222 [-32.059,  2.502], mean action: 8.407 [0.000, 20.000],  loss: 0.023178, mae: 0.322722, mean_q: 0.461350, mean_eps: 0.000000
 2670/5000: episode: 114, duration: 0.374s, episode steps:  25, steps per second:  67, episode reward: 35.587, mean reward:  1.423 [-2.473, 32.052], mean action: 8.520 [0.000, 19.000],  loss: 0.020681, mae: 0.312530, mean_q: 0.525644, mean_eps: 0.000000
 2698/5000: episode: 115, duration: 0.405s, episode steps:  28, steps per second:  69, episode reward: 33.000, mean reward:  1.179 [-2.476, 30.468], mean action: 5.286 [0.000, 15.000],  loss: 0.019721, mae: 0.317644, mean_q: 0.548934, mean_eps: 0.000000
 2719/5000: episode: 116, duration: 0.320s, episode steps:  21, steps per second:  66, episode reward: 38.668, mean reward:  1.841 [-3.000, 32.220], mean action: 7.429 [0.000, 15.000],  loss: 0.016460, mae: 0.302148, mean_q: 0.522087, mean_eps: 0.000000
 2740/5000: episode: 117, duration: 0.311s, episode steps:  21, steps per second:  68, episode reward: 41.152, mean reward:  1.960 [-2.687, 31.900], mean action: 1.952 [1.000, 3.000],  loss: 0.019318, mae: 0.310083, mean_q: 0.529304, mean_eps: 0.000000
 2757/5000: episode: 118, duration: 0.277s, episode steps:  17, steps per second:  61, episode reward: 35.904, mean reward:  2.112 [-2.479, 33.309], mean action: 5.941 [0.000, 15.000],  loss: 0.019842, mae: 0.319432, mean_q: 0.525267, mean_eps: 0.000000
 2784/5000: episode: 119, duration: 0.399s, episode steps:  27, steps per second:  68, episode reward: -35.600, mean reward: -1.319 [-32.116,  2.424], mean action: 8.037 [2.000, 18.000],  loss: 0.023235, mae: 0.326447, mean_q: 0.551970, mean_eps: 0.000000
 2801/5000: episode: 120, duration: 0.262s, episode steps:  17, steps per second:  65, episode reward: 38.578, mean reward:  2.269 [-3.000, 32.490], mean action: 4.353 [0.000, 16.000],  loss: 0.027773, mae: 0.346526, mean_q: 0.530779, mean_eps: 0.000000
 2828/5000: episode: 121, duration: 0.392s, episode steps:  27, steps per second:  69, episode reward: 38.037, mean reward:  1.409 [-2.346, 32.012], mean action: 5.111 [0.000, 19.000],  loss: 0.020127, mae: 0.315793, mean_q: 0.537948, mean_eps: 0.000000
 2852/5000: episode: 122, duration: 0.363s, episode steps:  24, steps per second:  66, episode reward: 39.000, mean reward:  1.625 [-3.000, 32.160], mean action: 4.042 [0.000, 16.000],  loss: 0.022544, mae: 0.332758, mean_q: 0.559674, mean_eps: 0.000000
 2872/5000: episode: 123, duration: 0.299s, episode steps:  20, steps per second:  67, episode reward: 35.535, mean reward:  1.777 [-2.903, 31.615], mean action: 5.700 [2.000, 16.000],  loss: 0.021277, mae: 0.327380, mean_q: 0.546505, mean_eps: 0.000000
 2893/5000: episode: 124, duration: 0.306s, episode steps:  21, steps per second:  69, episode reward: 36.000, mean reward:  1.714 [-2.984, 32.740], mean action: 7.333 [0.000, 16.000],  loss: 0.018013, mae: 0.308895, mean_q: 0.558075, mean_eps: 0.000000
 2913/5000: episode: 125, duration: 0.298s, episode steps:  20, steps per second:  67, episode reward: 38.419, mean reward:  1.921 [-2.261, 32.030], mean action: 4.450 [0.000, 13.000],  loss: 0.020079, mae: 0.318101, mean_q: 0.557515, mean_eps: 0.000000
 2941/5000: episode: 126, duration: 0.408s, episode steps:  28, steps per second:  69, episode reward: -33.000, mean reward: -1.179 [-32.030,  2.200], mean action: 8.821 [0.000, 21.000],  loss: 0.022209, mae: 0.326191, mean_q: 0.543207, mean_eps: 0.000000
 2968/5000: episode: 127, duration: 0.383s, episode steps:  27, steps per second:  70, episode reward: -41.040, mean reward: -1.520 [-32.038,  2.113], mean action: 6.222 [0.000, 11.000],  loss: 0.019559, mae: 0.308702, mean_q: 0.486648, mean_eps: 0.000000
 2987/5000: episode: 128, duration: 0.282s, episode steps:  19, steps per second:  67, episode reward: 35.779, mean reward:  1.883 [-2.622, 32.541], mean action: 5.316 [0.000, 14.000],  loss: 0.016627, mae: 0.303213, mean_q: 0.492295, mean_eps: 0.000000
 3007/5000: episode: 129, duration: 0.306s, episode steps:  20, steps per second:  65, episode reward: -38.560, mean reward: -1.928 [-32.639,  2.409], mean action: 4.250 [0.000, 9.000],  loss: 0.021958, mae: 0.325177, mean_q: 0.492996, mean_eps: 0.000000
 3033/5000: episode: 130, duration: 0.374s, episode steps:  26, steps per second:  69, episode reward: 32.145, mean reward:  1.236 [-3.000, 32.364], mean action: 2.923 [0.000, 11.000],  loss: 0.023836, mae: 0.336153, mean_q: 0.526733, mean_eps: 0.000000
 3054/5000: episode: 131, duration: 0.304s, episode steps:  21, steps per second:  69, episode reward: 35.686, mean reward:  1.699 [-3.000, 32.050], mean action: 3.333 [0.000, 11.000],  loss: 0.025531, mae: 0.336890, mean_q: 0.549103, mean_eps: 0.000000
 3075/5000: episode: 132, duration: 0.325s, episode steps:  21, steps per second:  65, episode reward: 41.206, mean reward:  1.962 [-2.451, 32.903], mean action: 2.952 [0.000, 11.000],  loss: 0.023914, mae: 0.339640, mean_q: 0.613812, mean_eps: 0.000000
 3101/5000: episode: 133, duration: 0.370s, episode steps:  26, steps per second:  70, episode reward: 38.403, mean reward:  1.477 [-2.371, 31.876], mean action: 2.538 [0.000, 16.000],  loss: 0.019803, mae: 0.311553, mean_q: 0.611298, mean_eps: 0.000000
 3130/5000: episode: 134, duration: 0.407s, episode steps:  29, steps per second:  71, episode reward: 35.123, mean reward:  1.211 [-2.452, 32.247], mean action: 3.793 [0.000, 12.000],  loss: 0.020427, mae: 0.316426, mean_q: 0.582741, mean_eps: 0.000000
 3152/5000: episode: 135, duration: 0.314s, episode steps:  22, steps per second:  70, episode reward: 32.649, mean reward:  1.484 [-3.000, 32.600], mean action: 6.864 [0.000, 14.000],  loss: 0.018283, mae: 0.306320, mean_q: 0.577219, mean_eps: 0.000000
 3170/5000: episode: 136, duration: 0.276s, episode steps:  18, steps per second:  65, episode reward: 38.920, mean reward:  2.162 [-2.296, 32.110], mean action: 4.944 [0.000, 13.000],  loss: 0.020052, mae: 0.313213, mean_q: 0.565340, mean_eps: 0.000000
 3191/5000: episode: 137, duration: 0.308s, episode steps:  21, steps per second:  68, episode reward: -35.810, mean reward: -1.705 [-31.968,  2.288], mean action: 7.095 [0.000, 18.000],  loss: 0.019319, mae: 0.306527, mean_q: 0.518552, mean_eps: 0.000000
 3218/5000: episode: 138, duration: 0.390s, episode steps:  27, steps per second:  69, episode reward: 41.540, mean reward:  1.539 [-2.085, 32.060], mean action: 3.222 [0.000, 12.000],  loss: 0.022264, mae: 0.317820, mean_q: 0.524861, mean_eps: 0.000000
 3242/5000: episode: 139, duration: 0.351s, episode steps:  24, steps per second:  68, episode reward: 32.833, mean reward:  1.368 [-2.419, 32.025], mean action: 3.083 [0.000, 11.000],  loss: 0.020176, mae: 0.311239, mean_q: 0.469297, mean_eps: 0.000000
 3260/5000: episode: 140, duration: 0.278s, episode steps:  18, steps per second:  65, episode reward: 35.119, mean reward:  1.951 [-3.000, 29.527], mean action: 4.944 [0.000, 14.000],  loss: 0.020521, mae: 0.314573, mean_q: 0.465696, mean_eps: 0.000000
 3278/5000: episode: 141, duration: 0.269s, episode steps:  18, steps per second:  67, episode reward: 38.502, mean reward:  2.139 [-2.558, 32.310], mean action: 3.222 [0.000, 11.000],  loss: 0.022495, mae: 0.329106, mean_q: 0.528593, mean_eps: 0.000000
 3299/5000: episode: 142, duration: 0.306s, episode steps:  21, steps per second:  69, episode reward: 35.216, mean reward:  1.677 [-2.902, 32.450], mean action: 3.810 [0.000, 15.000],  loss: 0.026533, mae: 0.342567, mean_q: 0.568885, mean_eps: 0.000000
 3313/5000: episode: 143, duration: 0.233s, episode steps:  14, steps per second:  60, episode reward: 41.411, mean reward:  2.958 [-3.000, 31.943], mean action: 3.929 [2.000, 11.000],  loss: 0.014748, mae: 0.287577, mean_q: 0.576763, mean_eps: 0.000000
 3335/5000: episode: 144, duration: 0.321s, episode steps:  22, steps per second:  69, episode reward: 32.565, mean reward:  1.480 [-2.494, 32.370], mean action: 5.864 [0.000, 19.000],  loss: 0.021527, mae: 0.323986, mean_q: 0.582404, mean_eps: 0.000000
 3357/5000: episode: 145, duration: 0.316s, episode steps:  22, steps per second:  70, episode reward: 37.768, mean reward:  1.717 [-3.000, 32.824], mean action: 3.955 [0.000, 19.000],  loss: 0.023630, mae: 0.340453, mean_q: 0.542767, mean_eps: 0.000000
 3379/5000: episode: 146, duration: 0.341s, episode steps:  22, steps per second:  64, episode reward: 41.427, mean reward:  1.883 [-2.430, 33.000], mean action: 5.364 [0.000, 19.000],  loss: 0.022732, mae: 0.326120, mean_q: 0.474232, mean_eps: 0.000000
 3402/5000: episode: 147, duration: 0.340s, episode steps:  23, steps per second:  68, episode reward: -32.460, mean reward: -1.411 [-32.146,  2.486], mean action: 4.217 [0.000, 19.000],  loss: 0.024371, mae: 0.337090, mean_q: 0.548151, mean_eps: 0.000000
 3425/5000: episode: 148, duration: 0.333s, episode steps:  23, steps per second:  69, episode reward: 32.357, mean reward:  1.407 [-3.000, 32.002], mean action: 6.130 [0.000, 15.000],  loss: 0.020259, mae: 0.318522, mean_q: 0.593177, mean_eps: 0.000000
 3446/5000: episode: 149, duration: 0.309s, episode steps:  21, steps per second:  68, episode reward: -38.940, mean reward: -1.854 [-32.104,  2.320], mean action: 6.619 [0.000, 15.000],  loss: 0.019673, mae: 0.311740, mean_q: 0.545131, mean_eps: 0.000000
 3467/5000: episode: 150, duration: 0.315s, episode steps:  21, steps per second:  67, episode reward: 38.706, mean reward:  1.843 [-3.000, 32.080], mean action: 2.714 [0.000, 11.000],  loss: 0.020246, mae: 0.318705, mean_q: 0.493521, mean_eps: 0.000000
 3500/5000: episode: 151, duration: 0.472s, episode steps:  33, steps per second:  70, episode reward: -32.190, mean reward: -0.975 [-31.965,  3.062], mean action: 4.576 [0.000, 15.000],  loss: 0.022293, mae: 0.324569, mean_q: 0.515278, mean_eps: 0.000000
 3528/5000: episode: 152, duration: 0.403s, episode steps:  28, steps per second:  69, episode reward: 34.553, mean reward:  1.234 [-2.611, 31.639], mean action: 5.464 [0.000, 15.000],  loss: 0.018124, mae: 0.306755, mean_q: 0.475758, mean_eps: 0.000000
 3559/5000: episode: 153, duration: 0.434s, episode steps:  31, steps per second:  71, episode reward: 37.239, mean reward:  1.201 [-2.840, 32.362], mean action: 6.290 [0.000, 15.000],  loss: 0.022833, mae: 0.320424, mean_q: 0.462204, mean_eps: 0.000000
 3579/5000: episode: 154, duration: 0.295s, episode steps:  20, steps per second:  68, episode reward: 39.000, mean reward:  1.950 [-2.902, 32.570], mean action: 3.750 [0.000, 14.000],  loss: 0.016276, mae: 0.291635, mean_q: 0.462966, mean_eps: 0.000000
 3599/5000: episode: 155, duration: 0.292s, episode steps:  20, steps per second:  68, episode reward: 38.922, mean reward:  1.946 [-2.188, 32.032], mean action: 3.000 [0.000, 12.000],  loss: 0.022747, mae: 0.326322, mean_q: 0.465200, mean_eps: 0.000000
 3622/5000: episode: 156, duration: 0.344s, episode steps:  23, steps per second:  67, episode reward: 35.613, mean reward:  1.548 [-2.422, 32.023], mean action: 4.565 [0.000, 14.000],  loss: 0.020788, mae: 0.317648, mean_q: 0.509988, mean_eps: 0.000000
 3645/5000: episode: 157, duration: 0.330s, episode steps:  23, steps per second:  70, episode reward: 33.000, mean reward:  1.435 [-3.000, 32.330], mean action: 5.087 [0.000, 19.000],  loss: 0.020279, mae: 0.311301, mean_q: 0.530759, mean_eps: 0.000000
 3668/5000: episode: 158, duration: 0.342s, episode steps:  23, steps per second:  67, episode reward: 38.364, mean reward:  1.668 [-2.376, 32.160], mean action: 2.217 [0.000, 11.000],  loss: 0.024218, mae: 0.326963, mean_q: 0.620123, mean_eps: 0.000000
 3681/5000: episode: 159, duration: 0.202s, episode steps:  13, steps per second:  64, episode reward: 41.701, mean reward:  3.208 [-2.791, 32.701], mean action: 2.077 [0.000, 11.000],  loss: 0.022712, mae: 0.317577, mean_q: 0.567951, mean_eps: 0.000000
 3702/5000: episode: 160, duration: 0.320s, episode steps:  21, steps per second:  66, episode reward: 38.549, mean reward:  1.836 [-2.470, 32.400], mean action: 2.810 [0.000, 12.000],  loss: 0.021773, mae: 0.316443, mean_q: 0.557452, mean_eps: 0.000000
 3722/5000: episode: 161, duration: 0.292s, episode steps:  20, steps per second:  69, episode reward: 38.603, mean reward:  1.930 [-2.592, 33.000], mean action: 7.150 [0.000, 20.000],  loss: 0.022728, mae: 0.325573, mean_q: 0.583603, mean_eps: 0.000000
 3753/5000: episode: 162, duration: 0.592s, episode steps:  31, steps per second:  52, episode reward: 35.142, mean reward:  1.134 [-2.416, 32.048], mean action: 2.742 [0.000, 12.000],  loss: 0.018789, mae: 0.307806, mean_q: 0.552996, mean_eps: 0.000000
 3775/5000: episode: 163, duration: 0.362s, episode steps:  22, steps per second:  61, episode reward: 35.413, mean reward:  1.610 [-3.000, 31.933], mean action: 3.727 [0.000, 12.000],  loss: 0.016769, mae: 0.294819, mean_q: 0.559974, mean_eps: 0.000000
 3792/5000: episode: 164, duration: 0.255s, episode steps:  17, steps per second:  67, episode reward: 39.000, mean reward:  2.294 [-2.213, 33.000], mean action: 3.824 [0.000, 12.000],  loss: 0.016067, mae: 0.287926, mean_q: 0.554932, mean_eps: 0.000000
 3819/5000: episode: 165, duration: 0.400s, episode steps:  27, steps per second:  67, episode reward: -32.280, mean reward: -1.196 [-31.787,  2.276], mean action: 3.481 [0.000, 12.000],  loss: 0.019149, mae: 0.305271, mean_q: 0.522970, mean_eps: 0.000000
 3844/5000: episode: 166, duration: 0.372s, episode steps:  25, steps per second:  67, episode reward: 38.403, mean reward:  1.536 [-2.424, 32.270], mean action: 5.400 [0.000, 20.000],  loss: 0.017432, mae: 0.293502, mean_q: 0.486807, mean_eps: 0.000000
 3862/5000: episode: 167, duration: 0.269s, episode steps:  18, steps per second:  67, episode reward: 39.000, mean reward:  2.167 [-2.435, 32.260], mean action: 7.722 [0.000, 14.000],  loss: 0.023057, mae: 0.321276, mean_q: 0.503536, mean_eps: 0.000000
 3890/5000: episode: 168, duration: 0.423s, episode steps:  28, steps per second:  66, episode reward: 38.144, mean reward:  1.362 [-2.180, 32.203], mean action: 3.036 [0.000, 19.000],  loss: 0.022877, mae: 0.319167, mean_q: 0.527204, mean_eps: 0.000000
 3914/5000: episode: 169, duration: 0.339s, episode steps:  24, steps per second:  71, episode reward: 33.000, mean reward:  1.375 [-3.000, 32.690], mean action: 3.500 [0.000, 19.000],  loss: 0.023275, mae: 0.315613, mean_q: 0.537447, mean_eps: 0.000000
 3958/5000: episode: 170, duration: 0.614s, episode steps:  44, steps per second:  72, episode reward: -35.890, mean reward: -0.816 [-32.009,  2.470], mean action: 8.591 [0.000, 15.000],  loss: 0.020409, mae: 0.303358, mean_q: 0.546320, mean_eps: 0.000000
 3977/5000: episode: 171, duration: 0.288s, episode steps:  19, steps per second:  66, episode reward: 41.675, mean reward:  2.193 [-2.888, 32.160], mean action: 3.737 [0.000, 15.000],  loss: 0.023888, mae: 0.323599, mean_q: 0.479675, mean_eps: 0.000000
 4004/5000: episode: 172, duration: 0.398s, episode steps:  27, steps per second:  68, episode reward: 33.000, mean reward:  1.222 [-2.591, 32.500], mean action: 3.667 [0.000, 15.000],  loss: 0.020311, mae: 0.311907, mean_q: 0.492551, mean_eps: 0.000000
 4030/5000: episode: 173, duration: 0.385s, episode steps:  26, steps per second:  67, episode reward: 32.224, mean reward:  1.239 [-3.000, 32.336], mean action: 3.731 [0.000, 15.000],  loss: 0.022010, mae: 0.323597, mean_q: 0.508825, mean_eps: 0.000000
 4048/5000: episode: 174, duration: 0.269s, episode steps:  18, steps per second:  67, episode reward: 36.000, mean reward:  2.000 [-3.000, 32.290], mean action: 3.556 [0.000, 16.000],  loss: 0.019337, mae: 0.303682, mean_q: 0.511441, mean_eps: 0.000000
 4071/5000: episode: 175, duration: 0.336s, episode steps:  23, steps per second:  69, episode reward: 32.584, mean reward:  1.417 [-2.322, 31.714], mean action: 6.783 [0.000, 16.000],  loss: 0.019624, mae: 0.308096, mean_q: 0.519097, mean_eps: 0.000000
 4089/5000: episode: 176, duration: 0.301s, episode steps:  18, steps per second:  60, episode reward: 41.152, mean reward:  2.286 [-2.533, 32.420], mean action: 3.056 [0.000, 12.000],  loss: 0.015001, mae: 0.293553, mean_q: 0.437025, mean_eps: 0.000000
 4120/5000: episode: 177, duration: 0.439s, episode steps:  31, steps per second:  71, episode reward: 32.357, mean reward:  1.044 [-2.750, 32.220], mean action: 4.484 [0.000, 15.000],  loss: 0.021886, mae: 0.322599, mean_q: 0.466507, mean_eps: 0.000000
 4130/5000: episode: 178, duration: 0.448s, episode steps:  10, steps per second:  22, episode reward: 44.717, mean reward:  4.472 [-2.110, 33.000], mean action: 2.400 [0.000, 12.000],  loss: 0.022716, mae: 0.307766, mean_q: 0.490322, mean_eps: 0.000000
 4158/5000: episode: 179, duration: 0.436s, episode steps:  28, steps per second:  64, episode reward: 37.567, mean reward:  1.342 [-2.661, 32.039], mean action: 4.214 [1.000, 18.000],  loss: 0.023750, mae: 0.320699, mean_q: 0.535522, mean_eps: 0.000000
 4178/5000: episode: 180, duration: 0.299s, episode steps:  20, steps per second:  67, episode reward: 35.683, mean reward:  1.784 [-3.000, 32.683], mean action: 4.650 [0.000, 19.000],  loss: 0.022400, mae: 0.316083, mean_q: 0.601172, mean_eps: 0.000000
 4326/5000: episode: 181, duration: 2.060s, episode steps: 148, steps per second:  72, episode reward: 38.786, mean reward:  0.262 [-3.000, 32.006], mean action: 11.331 [0.000, 19.000],  loss: 0.020222, mae: 0.312775, mean_q: 0.555487, mean_eps: 0.000000
 4353/5000: episode: 182, duration: 0.401s, episode steps:  27, steps per second:  67, episode reward: 32.102, mean reward:  1.189 [-2.483, 31.911], mean action: 3.630 [0.000, 15.000],  loss: 0.016357, mae: 0.295783, mean_q: 0.508504, mean_eps: 0.000000
 4367/5000: episode: 183, duration: 0.215s, episode steps:  14, steps per second:  65, episode reward: 38.405, mean reward:  2.743 [-3.000, 32.796], mean action: 3.286 [0.000, 11.000],  loss: 0.020512, mae: 0.315172, mean_q: 0.513481, mean_eps: 0.000000
 4396/5000: episode: 184, duration: 0.430s, episode steps:  29, steps per second:  67, episode reward: -33.000, mean reward: -1.138 [-32.038,  2.490], mean action: 7.759 [0.000, 19.000],  loss: 0.020552, mae: 0.318430, mean_q: 0.574634, mean_eps: 0.000000
 4416/5000: episode: 185, duration: 0.287s, episode steps:  20, steps per second:  70, episode reward: 32.550, mean reward:  1.628 [-2.716, 32.030], mean action: 6.500 [0.000, 19.000],  loss: 0.017951, mae: 0.311923, mean_q: 0.548319, mean_eps: 0.000000
 4435/5000: episode: 186, duration: 0.292s, episode steps:  19, steps per second:  65, episode reward: 37.741, mean reward:  1.986 [-3.000, 32.061], mean action: 5.579 [1.000, 14.000],  loss: 0.017985, mae: 0.321090, mean_q: 0.548774, mean_eps: 0.000000
 4447/5000: episode: 187, duration: 0.188s, episode steps:  12, steps per second:  64, episode reward: 42.000, mean reward:  3.500 [-2.492, 32.400], mean action: 4.083 [0.000, 12.000],  loss: 0.019161, mae: 0.324106, mean_q: 0.522572, mean_eps: 0.000000
 4465/5000: episode: 188, duration: 0.265s, episode steps:  18, steps per second:  68, episode reward: 34.955, mean reward:  1.942 [-3.000, 31.734], mean action: 5.722 [0.000, 14.000],  loss: 0.022957, mae: 0.335025, mean_q: 0.512970, mean_eps: 0.000000
 4493/5000: episode: 189, duration: 0.407s, episode steps:  28, steps per second:  69, episode reward: 32.938, mean reward:  1.176 [-2.978, 32.050], mean action: 4.643 [0.000, 14.000],  loss: 0.017725, mae: 0.302105, mean_q: 0.536614, mean_eps: 0.000000
 4506/5000: episode: 190, duration: 0.212s, episode steps:  13, steps per second:  61, episode reward: 41.123, mean reward:  3.163 [-3.000, 32.476], mean action: 4.923 [0.000, 20.000],  loss: 0.015386, mae: 0.288501, mean_q: 0.578652, mean_eps: 0.000000
 4531/5000: episode: 191, duration: 0.360s, episode steps:  25, steps per second:  69, episode reward: 35.803, mean reward:  1.432 [-2.377, 32.154], mean action: 4.840 [0.000, 15.000],  loss: 0.018824, mae: 0.307794, mean_q: 0.570879, mean_eps: 0.000000
 4550/5000: episode: 192, duration: 0.279s, episode steps:  19, steps per second:  68, episode reward: 41.634, mean reward:  2.191 [-2.407, 32.090], mean action: 4.947 [0.000, 19.000],  loss: 0.019976, mae: 0.307835, mean_q: 0.565723, mean_eps: 0.000000
 4571/5000: episode: 193, duration: 0.309s, episode steps:  21, steps per second:  68, episode reward: 32.558, mean reward:  1.550 [-3.000, 31.808], mean action: 4.286 [0.000, 19.000],  loss: 0.019069, mae: 0.311800, mean_q: 0.579978, mean_eps: 0.000000
 4591/5000: episode: 194, duration: 0.294s, episode steps:  20, steps per second:  68, episode reward: 41.736, mean reward:  2.087 [-3.000, 32.070], mean action: 3.750 [1.000, 16.000],  loss: 0.019450, mae: 0.312346, mean_q: 0.562531, mean_eps: 0.000000
 4613/5000: episode: 195, duration: 0.320s, episode steps:  22, steps per second:  69, episode reward: 40.605, mean reward:  1.846 [-2.123, 32.260], mean action: 4.364 [0.000, 19.000],  loss: 0.023340, mae: 0.332035, mean_q: 0.533544, mean_eps: 0.000000
 4637/5000: episode: 196, duration: 0.351s, episode steps:  24, steps per second:  68, episode reward: 35.754, mean reward:  1.490 [-2.770, 32.390], mean action: 3.208 [0.000, 11.000],  loss: 0.018423, mae: 0.316358, mean_q: 0.516180, mean_eps: 0.000000
 4655/5000: episode: 197, duration: 0.265s, episode steps:  18, steps per second:  68, episode reward: 38.806, mean reward:  2.156 [-3.000, 32.903], mean action: 2.667 [0.000, 9.000],  loss: 0.015601, mae: 0.296794, mean_q: 0.497956, mean_eps: 0.000000
 4668/5000: episode: 198, duration: 0.212s, episode steps:  13, steps per second:  61, episode reward: 41.626, mean reward:  3.202 [-2.131, 32.626], mean action: 4.615 [0.000, 16.000],  loss: 0.016692, mae: 0.303853, mean_q: 0.493976, mean_eps: 0.000000
 4698/5000: episode: 199, duration: 0.428s, episode steps:  30, steps per second:  70, episode reward: 39.897, mean reward:  1.330 [-2.499, 32.354], mean action: 6.700 [0.000, 16.000],  loss: 0.019703, mae: 0.325071, mean_q: 0.478323, mean_eps: 0.000000
 4722/5000: episode: 200, duration: 0.356s, episode steps:  24, steps per second:  68, episode reward: -35.910, mean reward: -1.496 [-32.060,  2.222], mean action: 4.375 [0.000, 16.000],  loss: 0.016258, mae: 0.296750, mean_q: 0.474759, mean_eps: 0.000000
 4743/5000: episode: 201, duration: 0.301s, episode steps:  21, steps per second:  70, episode reward: -35.510, mean reward: -1.691 [-33.000,  2.581], mean action: 5.667 [0.000, 19.000],  loss: 0.022035, mae: 0.316694, mean_q: 0.527013, mean_eps: 0.000000
 4771/5000: episode: 202, duration: 0.413s, episode steps:  28, steps per second:  68, episode reward: 34.692, mean reward:  1.239 [-2.623, 32.020], mean action: 6.286 [0.000, 20.000],  loss: 0.019989, mae: 0.313305, mean_q: 0.522737, mean_eps: 0.000000
 4798/5000: episode: 203, duration: 0.379s, episode steps:  27, steps per second:  71, episode reward: -35.610, mean reward: -1.319 [-32.146,  2.681], mean action: 6.259 [0.000, 19.000],  loss: 0.020300, mae: 0.314208, mean_q: 0.479626, mean_eps: 0.000000
 4823/5000: episode: 204, duration: 0.369s, episode steps:  25, steps per second:  68, episode reward: 32.604, mean reward:  1.304 [-2.389, 32.050], mean action: 8.120 [0.000, 20.000],  loss: 0.016925, mae: 0.296343, mean_q: 0.444266, mean_eps: 0.000000
 4841/5000: episode: 205, duration: 0.280s, episode steps:  18, steps per second:  64, episode reward: 41.211, mean reward:  2.290 [-2.484, 31.908], mean action: 3.556 [0.000, 19.000],  loss: 0.021291, mae: 0.316854, mean_q: 0.495473, mean_eps: 0.000000
 4857/5000: episode: 206, duration: 0.263s, episode steps:  16, steps per second:  61, episode reward: 44.776, mean reward:  2.799 [-2.074, 32.460], mean action: 3.000 [0.000, 19.000],  loss: 0.016412, mae: 0.297635, mean_q: 0.485483, mean_eps: 0.000000
 4875/5000: episode: 207, duration: 0.284s, episode steps:  18, steps per second:  63, episode reward: 41.137, mean reward:  2.285 [-2.422, 33.000], mean action: 2.500 [0.000, 11.000],  loss: 0.020359, mae: 0.309599, mean_q: 0.479577, mean_eps: 0.000000
 4903/5000: episode: 208, duration: 0.388s, episode steps:  28, steps per second:  72, episode reward: -44.740, mean reward: -1.598 [-32.304,  2.046], mean action: 8.750 [2.000, 19.000],  loss: 0.021916, mae: 0.318305, mean_q: 0.496988, mean_eps: 0.000000
 4953/5000: episode: 209, duration: 0.694s, episode steps:  50, steps per second:  72, episode reward: -33.000, mean reward: -0.660 [-32.079,  2.672], mean action: 3.500 [0.000, 20.000],  loss: 0.019505, mae: 0.308749, mean_q: 0.563546, mean_eps: 0.000000
 4973/5000: episode: 210, duration: 0.297s, episode steps:  20, steps per second:  67, episode reward: 41.048, mean reward:  2.052 [-2.674, 32.420], mean action: 2.450 [0.000, 16.000],  loss: 0.019274, mae: 0.314501, mean_q: 0.537840, mean_eps: 0.000000
 4995/5000: episode: 211, duration: 0.325s, episode steps:  22, steps per second:  68, episode reward: -33.000, mean reward: -1.500 [-29.086,  2.011], mean action: 5.591 [0.000, 19.000],  loss: 0.017650, mae: 0.310641, mean_q: 0.489062, mean_eps: 0.000000
done, took 118.208 seconds
DQN Evaluation: 12274 victories out of 14315 episodes
Training for 5000 steps ...
   48/5000: episode: 1, duration: 0.393s, episode steps:  48, steps per second: 122, episode reward: 40.313, mean reward:  0.840 [-2.043, 32.140], mean action: 4.896 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   66/5000: episode: 2, duration: 0.147s, episode steps:  18, steps per second: 122, episode reward: 44.080, mean reward:  2.449 [-2.509, 32.110], mean action: 5.611 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   91/5000: episode: 3, duration: 0.200s, episode steps:  25, steps per second: 125, episode reward: 41.005, mean reward:  1.640 [-2.892, 31.895], mean action: 3.160 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  115/5000: episode: 4, duration: 0.177s, episode steps:  24, steps per second: 135, episode reward: 42.811, mean reward:  1.784 [-2.424, 32.130], mean action: 5.667 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  137/5000: episode: 5, duration: 0.165s, episode steps:  22, steps per second: 134, episode reward: 36.000, mean reward:  1.636 [-3.000, 32.570], mean action: 5.091 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  174/5000: episode: 6, duration: 0.265s, episode steps:  37, steps per second: 140, episode reward: 38.316, mean reward:  1.036 [-3.000, 31.927], mean action: 4.081 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  195/5000: episode: 7, duration: 0.155s, episode steps:  21, steps per second: 136, episode reward: 41.938, mean reward:  1.997 [-3.000, 32.210], mean action: 5.143 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  219/5000: episode: 8, duration: 0.161s, episode steps:  24, steps per second: 149, episode reward: 40.719, mean reward:  1.697 [-2.441, 32.090], mean action: 6.375 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  234/5000: episode: 9, duration: 0.129s, episode steps:  15, steps per second: 116, episode reward: 44.015, mean reward:  2.934 [-3.000, 32.250], mean action: 3.867 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  252/5000: episode: 10, duration: 0.131s, episode steps:  18, steps per second: 138, episode reward: 41.852, mean reward:  2.325 [-2.099, 29.366], mean action: 4.444 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  272/5000: episode: 11, duration: 0.147s, episode steps:  20, steps per second: 136, episode reward: 41.939, mean reward:  2.097 [-2.835, 32.020], mean action: 6.400 [1.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  292/5000: episode: 12, duration: 0.150s, episode steps:  20, steps per second: 133, episode reward: 44.510, mean reward:  2.225 [-2.077, 31.798], mean action: 4.200 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  314/5000: episode: 13, duration: 0.154s, episode steps:  22, steps per second: 143, episode reward: 41.575, mean reward:  1.890 [-3.000, 31.975], mean action: 5.773 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  331/5000: episode: 14, duration: 0.130s, episode steps:  17, steps per second: 131, episode reward: 38.471, mean reward:  2.263 [-3.000, 31.841], mean action: 5.000 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  370/5000: episode: 15, duration: 0.256s, episode steps:  39, steps per second: 152, episode reward: 33.000, mean reward:  0.846 [-3.000, 32.430], mean action: 4.615 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  392/5000: episode: 16, duration: 0.166s, episode steps:  22, steps per second: 132, episode reward: 44.152, mean reward:  2.007 [-2.676, 32.160], mean action: 5.227 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  421/5000: episode: 17, duration: 0.203s, episode steps:  29, steps per second: 143, episode reward: 43.431, mean reward:  1.498 [-2.454, 32.800], mean action: 3.172 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  438/5000: episode: 18, duration: 0.143s, episode steps:  17, steps per second: 119, episode reward: 45.000, mean reward:  2.647 [-2.232, 32.250], mean action: 3.412 [2.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  466/5000: episode: 19, duration: 0.183s, episode steps:  28, steps per second: 153, episode reward: 42.000, mean reward:  1.500 [-2.154, 32.170], mean action: 6.214 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  491/5000: episode: 20, duration: 0.186s, episode steps:  25, steps per second: 135, episode reward: 44.305, mean reward:  1.772 [-2.239, 32.396], mean action: 2.680 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  537/5000: episode: 21, duration: 0.309s, episode steps:  46, steps per second: 149, episode reward: 44.748, mean reward:  0.973 [-0.249, 30.146], mean action: 4.174 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  562/5000: episode: 22, duration: 0.181s, episode steps:  25, steps per second: 138, episode reward: 40.961, mean reward:  1.638 [-3.000, 32.240], mean action: 5.320 [2.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  592/5000: episode: 23, duration: 0.214s, episode steps:  30, steps per second: 140, episode reward: 43.632, mean reward:  1.454 [-2.734, 31.677], mean action: 6.433 [1.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  613/5000: episode: 24, duration: 0.178s, episode steps:  21, steps per second: 118, episode reward: 41.916, mean reward:  1.996 [-3.000, 32.090], mean action: 3.667 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  644/5000: episode: 25, duration: 0.207s, episode steps:  31, steps per second: 150, episode reward: 38.487, mean reward:  1.242 [-2.758, 32.120], mean action: 4.516 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  687/5000: episode: 26, duration: 0.275s, episode steps:  43, steps per second: 156, episode reward: 43.822, mean reward:  1.019 [-2.241, 32.180], mean action: 6.419 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  710/5000: episode: 27, duration: 0.159s, episode steps:  23, steps per second: 144, episode reward: 42.000, mean reward:  1.826 [-2.336, 32.010], mean action: 3.870 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  775/5000: episode: 28, duration: 0.406s, episode steps:  65, steps per second: 160, episode reward: 36.000, mean reward:  0.554 [-2.437, 32.140], mean action: 4.215 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  799/5000: episode: 29, duration: 0.168s, episode steps:  24, steps per second: 143, episode reward: 40.687, mean reward:  1.695 [-3.000, 32.330], mean action: 4.958 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  826/5000: episode: 30, duration: 0.189s, episode steps:  27, steps per second: 142, episode reward: 41.758, mean reward:  1.547 [-2.102, 32.110], mean action: 3.370 [1.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  852/5000: episode: 31, duration: 0.180s, episode steps:  26, steps per second: 144, episode reward: 37.066, mean reward:  1.426 [-3.000, 32.723], mean action: 4.885 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  877/5000: episode: 32, duration: 0.169s, episode steps:  25, steps per second: 148, episode reward: 38.533, mean reward:  1.541 [-2.473, 32.078], mean action: 3.960 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  903/5000: episode: 33, duration: 0.180s, episode steps:  26, steps per second: 144, episode reward: 35.878, mean reward:  1.380 [-2.901, 32.170], mean action: 4.962 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  942/5000: episode: 34, duration: 0.289s, episode steps:  39, steps per second: 135, episode reward: 38.447, mean reward:  0.986 [-2.435, 31.845], mean action: 5.436 [1.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  960/5000: episode: 35, duration: 0.133s, episode steps:  18, steps per second: 135, episode reward: 41.124, mean reward:  2.285 [-2.369, 31.937], mean action: 3.333 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  984/5000: episode: 36, duration: 0.177s, episode steps:  24, steps per second: 136, episode reward: 41.047, mean reward:  1.710 [-2.463, 32.200], mean action: 2.917 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1011/5000: episode: 37, duration: 0.287s, episode steps:  27, steps per second:  94, episode reward: 47.354, mean reward:  1.754 [-0.157, 32.180], mean action: 4.778 [2.000, 15.000],  loss: 0.024012, mae: 0.334588, mean_q: 0.552806, mean_eps: 0.000000
 1038/5000: episode: 38, duration: 0.682s, episode steps:  27, steps per second:  40, episode reward: 43.951, mean reward:  1.628 [-2.660, 32.070], mean action: 3.296 [1.000, 16.000],  loss: 0.017936, mae: 0.305544, mean_q: 0.550433, mean_eps: 0.000000
 1070/5000: episode: 39, duration: 0.476s, episode steps:  32, steps per second:  67, episode reward: 38.533, mean reward:  1.204 [-2.164, 32.140], mean action: 3.531 [0.000, 16.000],  loss: 0.020248, mae: 0.318233, mean_q: 0.541932, mean_eps: 0.000000
 1089/5000: episode: 40, duration: 0.302s, episode steps:  19, steps per second:  63, episode reward: 44.041, mean reward:  2.318 [-2.466, 32.150], mean action: 4.263 [0.000, 20.000],  loss: 0.021262, mae: 0.326683, mean_q: 0.569952, mean_eps: 0.000000
 1132/5000: episode: 41, duration: 0.630s, episode steps:  43, steps per second:  68, episode reward: 41.799, mean reward:  0.972 [-2.570, 32.421], mean action: 3.372 [0.000, 19.000],  loss: 0.022642, mae: 0.326369, mean_q: 0.540293, mean_eps: 0.000000
 1147/5000: episode: 42, duration: 0.239s, episode steps:  15, steps per second:  63, episode reward: 44.900, mean reward:  2.993 [-2.242, 32.900], mean action: 2.200 [1.000, 11.000],  loss: 0.020405, mae: 0.317285, mean_q: 0.516052, mean_eps: 0.000000
 1169/5000: episode: 43, duration: 0.350s, episode steps:  22, steps per second:  63, episode reward: 44.442, mean reward:  2.020 [-2.352, 31.915], mean action: 3.682 [2.000, 19.000],  loss: 0.018868, mae: 0.307067, mean_q: 0.494730, mean_eps: 0.000000
 1193/5000: episode: 44, duration: 0.362s, episode steps:  24, steps per second:  66, episode reward: 35.930, mean reward:  1.497 [-2.665, 32.110], mean action: 4.250 [0.000, 19.000],  loss: 0.023447, mae: 0.322171, mean_q: 0.495435, mean_eps: 0.000000
 1207/5000: episode: 45, duration: 0.232s, episode steps:  14, steps per second:  60, episode reward: 44.140, mean reward:  3.153 [-2.236, 32.115], mean action: 1.571 [0.000, 16.000],  loss: 0.020853, mae: 0.309235, mean_q: 0.509094, mean_eps: 0.000000
 1227/5000: episode: 46, duration: 0.309s, episode steps:  20, steps per second:  65, episode reward: 44.157, mean reward:  2.208 [-2.387, 32.060], mean action: 3.500 [0.000, 19.000],  loss: 0.020414, mae: 0.313023, mean_q: 0.517864, mean_eps: 0.000000
 1268/5000: episode: 47, duration: 0.590s, episode steps:  41, steps per second:  69, episode reward: 35.445, mean reward:  0.865 [-2.318, 32.043], mean action: 4.122 [0.000, 19.000],  loss: 0.020325, mae: 0.310397, mean_q: 0.512535, mean_eps: 0.000000
 1305/5000: episode: 48, duration: 0.545s, episode steps:  37, steps per second:  68, episode reward: 33.000, mean reward:  0.892 [-2.638, 32.090], mean action: 4.189 [0.000, 16.000],  loss: 0.019489, mae: 0.307697, mean_q: 0.535236, mean_eps: 0.000000
 1331/5000: episode: 49, duration: 0.375s, episode steps:  26, steps per second:  69, episode reward: 41.740, mean reward:  1.605 [-2.200, 31.820], mean action: 1.692 [0.000, 11.000],  loss: 0.018938, mae: 0.302560, mean_q: 0.550978, mean_eps: 0.000000
 1350/5000: episode: 50, duration: 0.293s, episode steps:  19, steps per second:  65, episode reward: 41.685, mean reward:  2.194 [-2.210, 32.230], mean action: 3.579 [0.000, 14.000],  loss: 0.018506, mae: 0.300055, mean_q: 0.523351, mean_eps: 0.000000
 1380/5000: episode: 51, duration: 0.437s, episode steps:  30, steps per second:  69, episode reward: 38.511, mean reward:  1.284 [-2.696, 32.031], mean action: 3.333 [0.000, 14.000],  loss: 0.022969, mae: 0.321233, mean_q: 0.566701, mean_eps: 0.000000
 1406/5000: episode: 52, duration: 0.381s, episode steps:  26, steps per second:  68, episode reward: 41.356, mean reward:  1.591 [-2.261, 32.100], mean action: 1.846 [0.000, 11.000],  loss: 0.024696, mae: 0.324185, mean_q: 0.590977, mean_eps: 0.000000
 1440/5000: episode: 53, duration: 0.502s, episode steps:  34, steps per second:  68, episode reward: 39.000, mean reward:  1.147 [-2.720, 32.330], mean action: 1.559 [0.000, 11.000],  loss: 0.020432, mae: 0.312479, mean_q: 0.599895, mean_eps: 0.000000
 1461/5000: episode: 54, duration: 0.330s, episode steps:  21, steps per second:  64, episode reward: 41.134, mean reward:  1.959 [-2.483, 32.250], mean action: 3.095 [0.000, 12.000],  loss: 0.019558, mae: 0.309665, mean_q: 0.551548, mean_eps: 0.000000
 1488/5000: episode: 55, duration: 0.392s, episode steps:  27, steps per second:  69, episode reward: 38.671, mean reward:  1.432 [-2.198, 32.292], mean action: 4.444 [0.000, 20.000],  loss: 0.016464, mae: 0.296523, mean_q: 0.487411, mean_eps: 0.000000
 1508/5000: episode: 56, duration: 0.309s, episode steps:  20, steps per second:  65, episode reward: 41.716, mean reward:  2.086 [-2.035, 32.691], mean action: 2.700 [0.000, 11.000],  loss: 0.020461, mae: 0.313737, mean_q: 0.525711, mean_eps: 0.000000
 1532/5000: episode: 57, duration: 0.358s, episode steps:  24, steps per second:  67, episode reward: 38.189, mean reward:  1.591 [-2.904, 32.095], mean action: 2.458 [0.000, 12.000],  loss: 0.024012, mae: 0.325988, mean_q: 0.514211, mean_eps: 0.000000
 1575/5000: episode: 58, duration: 0.645s, episode steps:  43, steps per second:  67, episode reward: 37.705, mean reward:  0.877 [-3.000, 32.004], mean action: 6.465 [1.000, 21.000],  loss: 0.020746, mae: 0.312243, mean_q: 0.496456, mean_eps: 0.000000
 1613/5000: episode: 59, duration: 0.563s, episode steps:  38, steps per second:  67, episode reward: 38.648, mean reward:  1.017 [-2.343, 32.130], mean action: 6.421 [0.000, 18.000],  loss: 0.021598, mae: 0.316750, mean_q: 0.465332, mean_eps: 0.000000
 1639/5000: episode: 60, duration: 0.384s, episode steps:  26, steps per second:  68, episode reward: 39.000, mean reward:  1.500 [-2.757, 32.290], mean action: 4.500 [1.000, 14.000],  loss: 0.023712, mae: 0.326992, mean_q: 0.468860, mean_eps: 0.000000
 1660/5000: episode: 61, duration: 0.314s, episode steps:  21, steps per second:  67, episode reward: 42.000, mean reward:  2.000 [-2.426, 32.490], mean action: 1.857 [0.000, 11.000],  loss: 0.018816, mae: 0.295719, mean_q: 0.538435, mean_eps: 0.000000
 1683/5000: episode: 62, duration: 0.340s, episode steps:  23, steps per second:  68, episode reward: 41.744, mean reward:  1.815 [-2.287, 31.944], mean action: 2.609 [0.000, 12.000],  loss: 0.017877, mae: 0.297466, mean_q: 0.517816, mean_eps: 0.000000
 1716/5000: episode: 63, duration: 0.483s, episode steps:  33, steps per second:  68, episode reward: 41.742, mean reward:  1.265 [-2.124, 32.560], mean action: 3.515 [0.000, 12.000],  loss: 0.022690, mae: 0.319279, mean_q: 0.483602, mean_eps: 0.000000
 1742/5000: episode: 64, duration: 0.390s, episode steps:  26, steps per second:  67, episode reward: 39.000, mean reward:  1.500 [-2.838, 32.560], mean action: 3.615 [0.000, 19.000],  loss: 0.020100, mae: 0.309511, mean_q: 0.482218, mean_eps: 0.000000
 1763/5000: episode: 65, duration: 0.333s, episode steps:  21, steps per second:  63, episode reward: 44.488, mean reward:  2.118 [-2.176, 32.100], mean action: 4.286 [0.000, 19.000],  loss: 0.021654, mae: 0.312955, mean_q: 0.494226, mean_eps: 0.000000
 1783/5000: episode: 66, duration: 0.301s, episode steps:  20, steps per second:  66, episode reward: 44.503, mean reward:  2.225 [-3.000, 32.250], mean action: 5.250 [0.000, 19.000],  loss: 0.017887, mae: 0.292682, mean_q: 0.478126, mean_eps: 0.000000
 1815/5000: episode: 67, duration: 0.450s, episode steps:  32, steps per second:  71, episode reward: 32.208, mean reward:  1.007 [-3.000, 31.579], mean action: 7.062 [0.000, 20.000],  loss: 0.020392, mae: 0.313086, mean_q: 0.529504, mean_eps: 0.000000
 1841/5000: episode: 68, duration: 0.381s, episode steps:  26, steps per second:  68, episode reward: 44.302, mean reward:  1.704 [-2.202, 32.190], mean action: 1.846 [0.000, 11.000],  loss: 0.020859, mae: 0.315890, mean_q: 0.509116, mean_eps: 0.000000
 1862/5000: episode: 69, duration: 0.327s, episode steps:  21, steps per second:  64, episode reward: 41.090, mean reward:  1.957 [-2.016, 31.850], mean action: 2.333 [0.000, 11.000],  loss: 0.019728, mae: 0.307312, mean_q: 0.524094, mean_eps: 0.000000
 1885/5000: episode: 70, duration: 0.345s, episode steps:  23, steps per second:  67, episode reward: 42.000, mean reward:  1.826 [-2.767, 32.270], mean action: 4.087 [0.000, 16.000],  loss: 0.020342, mae: 0.312635, mean_q: 0.532179, mean_eps: 0.000000
 1913/5000: episode: 71, duration: 0.407s, episode steps:  28, steps per second:  69, episode reward: 44.809, mean reward:  1.600 [-2.038, 32.110], mean action: 3.714 [2.000, 16.000],  loss: 0.018880, mae: 0.309292, mean_q: 0.516789, mean_eps: 0.000000
 1947/5000: episode: 72, duration: 0.496s, episode steps:  34, steps per second:  69, episode reward: 37.596, mean reward:  1.106 [-3.000, 32.040], mean action: 6.588 [0.000, 20.000],  loss: 0.019525, mae: 0.314018, mean_q: 0.455326, mean_eps: 0.000000
 2007/5000: episode: 73, duration: 0.832s, episode steps:  60, steps per second:  72, episode reward: 36.000, mean reward:  0.600 [-2.947, 32.040], mean action: 1.883 [0.000, 20.000],  loss: 0.019904, mae: 0.314239, mean_q: 0.525844, mean_eps: 0.000000
 2026/5000: episode: 74, duration: 0.285s, episode steps:  19, steps per second:  67, episode reward: 44.400, mean reward:  2.337 [-2.203, 32.270], mean action: 1.053 [0.000, 9.000],  loss: 0.021202, mae: 0.320138, mean_q: 0.501343, mean_eps: 0.000000
 2063/5000: episode: 75, duration: 0.543s, episode steps:  37, steps per second:  68, episode reward: 41.856, mean reward:  1.131 [-2.365, 32.210], mean action: 2.486 [0.000, 14.000],  loss: 0.022099, mae: 0.335034, mean_q: 0.539254, mean_eps: 0.000000
 2104/5000: episode: 76, duration: 0.691s, episode steps:  41, steps per second:  59, episode reward: 38.500, mean reward:  0.939 [-2.552, 32.200], mean action: 2.634 [0.000, 12.000],  loss: 0.021052, mae: 0.325504, mean_q: 0.517822, mean_eps: 0.000000
 2122/5000: episode: 77, duration: 0.276s, episode steps:  18, steps per second:  65, episode reward: 47.303, mean reward:  2.628 [-0.426, 32.809], mean action: 4.556 [0.000, 20.000],  loss: 0.016657, mae: 0.308398, mean_q: 0.467831, mean_eps: 0.000000
 2144/5000: episode: 78, duration: 0.335s, episode steps:  22, steps per second:  66, episode reward: 38.900, mean reward:  1.768 [-3.000, 32.050], mean action: 2.864 [0.000, 16.000],  loss: 0.020292, mae: 0.322361, mean_q: 0.488391, mean_eps: 0.000000
 2176/5000: episode: 79, duration: 0.462s, episode steps:  32, steps per second:  69, episode reward: 41.811, mean reward:  1.307 [-2.253, 32.211], mean action: 2.688 [0.000, 19.000],  loss: 0.019581, mae: 0.315402, mean_q: 0.522917, mean_eps: 0.000000
 2204/5000: episode: 80, duration: 0.409s, episode steps:  28, steps per second:  68, episode reward: 38.807, mean reward:  1.386 [-3.000, 32.463], mean action: 3.250 [0.000, 15.000],  loss: 0.018162, mae: 0.310632, mean_q: 0.511752, mean_eps: 0.000000
 2247/5000: episode: 81, duration: 0.604s, episode steps:  43, steps per second:  71, episode reward: 38.146, mean reward:  0.887 [-2.089, 32.520], mean action: 2.140 [0.000, 12.000],  loss: 0.022198, mae: 0.329893, mean_q: 0.491463, mean_eps: 0.000000
 2284/5000: episode: 82, duration: 0.535s, episode steps:  37, steps per second:  69, episode reward: 32.724, mean reward:  0.884 [-3.000, 32.510], mean action: 2.730 [0.000, 15.000],  loss: 0.020558, mae: 0.317770, mean_q: 0.586513, mean_eps: 0.000000
 2301/5000: episode: 83, duration: 0.263s, episode steps:  17, steps per second:  65, episode reward: 43.830, mean reward:  2.578 [-2.409, 32.261], mean action: 4.412 [1.000, 16.000],  loss: 0.018733, mae: 0.308029, mean_q: 0.507400, mean_eps: 0.000000
 2334/5000: episode: 84, duration: 0.469s, episode steps:  33, steps per second:  70, episode reward: 35.466, mean reward:  1.075 [-3.000, 32.100], mean action: 4.212 [0.000, 16.000],  loss: 0.018346, mae: 0.307805, mean_q: 0.484359, mean_eps: 0.000000
 2355/5000: episode: 85, duration: 0.312s, episode steps:  21, steps per second:  67, episode reward: 41.549, mean reward:  1.979 [-2.217, 32.379], mean action: 2.429 [0.000, 16.000],  loss: 0.019978, mae: 0.314471, mean_q: 0.457108, mean_eps: 0.000000
 2397/5000: episode: 86, duration: 0.585s, episode steps:  42, steps per second:  72, episode reward: 32.444, mean reward:  0.772 [-3.000, 32.120], mean action: 7.000 [0.000, 15.000],  loss: 0.021843, mae: 0.317960, mean_q: 0.530031, mean_eps: 0.000000
 2415/5000: episode: 87, duration: 0.283s, episode steps:  18, steps per second:  64, episode reward: 42.000, mean reward:  2.333 [-2.099, 33.000], mean action: 3.611 [0.000, 19.000],  loss: 0.018274, mae: 0.308613, mean_q: 0.563334, mean_eps: 0.000000
 2464/5000: episode: 88, duration: 0.697s, episode steps:  49, steps per second:  70, episode reward: -32.870, mean reward: -0.671 [-32.145,  2.693], mean action: 3.388 [0.000, 19.000],  loss: 0.017137, mae: 0.305623, mean_q: 0.513132, mean_eps: 0.000000
 2490/5000: episode: 89, duration: 0.384s, episode steps:  26, steps per second:  68, episode reward: 36.000, mean reward:  1.385 [-2.717, 30.310], mean action: 4.192 [1.000, 19.000],  loss: 0.020233, mae: 0.324253, mean_q: 0.447512, mean_eps: 0.000000
 2510/5000: episode: 90, duration: 0.314s, episode steps:  20, steps per second:  64, episode reward: 41.093, mean reward:  2.055 [-3.000, 32.060], mean action: 3.350 [0.000, 16.000],  loss: 0.023549, mae: 0.326017, mean_q: 0.457563, mean_eps: 0.000000
 2537/5000: episode: 91, duration: 0.393s, episode steps:  27, steps per second:  69, episode reward: 44.012, mean reward:  1.630 [-2.747, 32.310], mean action: 2.333 [0.000, 9.000],  loss: 0.017344, mae: 0.303892, mean_q: 0.503754, mean_eps: 0.000000
 2564/5000: episode: 92, duration: 0.393s, episode steps:  27, steps per second:  69, episode reward: 38.469, mean reward:  1.425 [-3.000, 32.280], mean action: 3.111 [0.000, 19.000],  loss: 0.019363, mae: 0.310278, mean_q: 0.523969, mean_eps: 0.000000
 2589/5000: episode: 93, duration: 0.370s, episode steps:  25, steps per second:  68, episode reward: 46.576, mean reward:  1.863 [-0.354, 32.360], mean action: 0.840 [0.000, 11.000],  loss: 0.019072, mae: 0.314342, mean_q: 0.563681, mean_eps: 0.000000
 2636/5000: episode: 94, duration: 0.673s, episode steps:  47, steps per second:  70, episode reward: 37.605, mean reward:  0.800 [-2.204, 31.642], mean action: 3.830 [0.000, 15.000],  loss: 0.022211, mae: 0.332250, mean_q: 0.530616, mean_eps: 0.000000
 2664/5000: episode: 95, duration: 0.415s, episode steps:  28, steps per second:  68, episode reward: 44.072, mean reward:  1.574 [-2.089, 32.062], mean action: 2.250 [0.000, 12.000],  loss: 0.020324, mae: 0.315944, mean_q: 0.537593, mean_eps: 0.000000
 2676/5000: episode: 96, duration: 0.193s, episode steps:  12, steps per second:  62, episode reward: 47.369, mean reward:  3.947 [ 0.000, 33.000], mean action: 2.083 [0.000, 14.000],  loss: 0.020150, mae: 0.314223, mean_q: 0.499914, mean_eps: 0.000000
 2703/5000: episode: 97, duration: 0.394s, episode steps:  27, steps per second:  68, episode reward: 41.901, mean reward:  1.552 [-2.194, 32.091], mean action: 2.926 [0.000, 14.000],  loss: 0.023942, mae: 0.326614, mean_q: 0.541573, mean_eps: 0.000000
 2724/5000: episode: 98, duration: 0.318s, episode steps:  21, steps per second:  66, episode reward: 38.427, mean reward:  1.830 [-2.477, 32.580], mean action: 3.286 [0.000, 19.000],  loss: 0.021452, mae: 0.317891, mean_q: 0.492425, mean_eps: 0.000000
 2741/5000: episode: 99, duration: 0.274s, episode steps:  17, steps per second:  62, episode reward: 41.360, mean reward:  2.433 [-2.548, 32.040], mean action: 3.706 [0.000, 19.000],  loss: 0.023500, mae: 0.328196, mean_q: 0.513041, mean_eps: 0.000000
 2770/5000: episode: 100, duration: 0.430s, episode steps:  29, steps per second:  67, episode reward: 38.003, mean reward:  1.310 [-2.324, 32.150], mean action: 5.414 [0.000, 19.000],  loss: 0.021448, mae: 0.320883, mean_q: 0.541916, mean_eps: 0.000000
 2792/5000: episode: 101, duration: 0.322s, episode steps:  22, steps per second:  68, episode reward: 39.973, mean reward:  1.817 [-2.348, 32.200], mean action: 3.818 [0.000, 14.000],  loss: 0.016920, mae: 0.297698, mean_q: 0.503335, mean_eps: 0.000000
 2810/5000: episode: 102, duration: 0.271s, episode steps:  18, steps per second:  66, episode reward: 41.843, mean reward:  2.325 [-2.353, 32.490], mean action: 7.889 [0.000, 20.000],  loss: 0.017984, mae: 0.301911, mean_q: 0.503270, mean_eps: 0.000000
 2832/5000: episode: 103, duration: 0.338s, episode steps:  22, steps per second:  65, episode reward: 41.030, mean reward:  1.865 [-3.000, 32.610], mean action: 4.773 [0.000, 16.000],  loss: 0.019782, mae: 0.318015, mean_q: 0.535839, mean_eps: 0.000000
 2858/5000: episode: 104, duration: 0.397s, episode steps:  26, steps per second:  66, episode reward: 41.218, mean reward:  1.585 [-2.519, 32.080], mean action: 2.692 [0.000, 11.000],  loss: 0.020877, mae: 0.318925, mean_q: 0.524067, mean_eps: 0.000000
 2887/5000: episode: 105, duration: 0.442s, episode steps:  29, steps per second:  66, episode reward: 44.587, mean reward:  1.537 [-2.134, 32.663], mean action: 2.103 [0.000, 11.000],  loss: 0.022001, mae: 0.326319, mean_q: 0.604175, mean_eps: 0.000000
 2907/5000: episode: 106, duration: 0.314s, episode steps:  20, steps per second:  64, episode reward: 38.822, mean reward:  1.941 [-3.000, 32.250], mean action: 2.950 [0.000, 11.000],  loss: 0.022926, mae: 0.331207, mean_q: 0.581433, mean_eps: 0.000000
 2943/5000: episode: 107, duration: 0.545s, episode steps:  36, steps per second:  66, episode reward: 38.878, mean reward:  1.080 [-2.876, 32.060], mean action: 3.417 [0.000, 11.000],  loss: 0.024067, mae: 0.336806, mean_q: 0.515116, mean_eps: 0.000000
 2975/5000: episode: 108, duration: 0.464s, episode steps:  32, steps per second:  69, episode reward: 40.754, mean reward:  1.274 [-2.797, 32.282], mean action: 3.125 [0.000, 20.000],  loss: 0.020973, mae: 0.323583, mean_q: 0.540423, mean_eps: 0.000000
 2987/5000: episode: 109, duration: 0.194s, episode steps:  12, steps per second:  62, episode reward: 47.974, mean reward:  3.998 [-0.171, 32.580], mean action: 0.750 [0.000, 1.000],  loss: 0.025553, mae: 0.341673, mean_q: 0.585757, mean_eps: 0.000000
 3023/5000: episode: 110, duration: 0.540s, episode steps:  36, steps per second:  67, episode reward: 34.350, mean reward:  0.954 [-2.576, 32.350], mean action: 5.972 [0.000, 16.000],  loss: 0.022161, mae: 0.329937, mean_q: 0.591555, mean_eps: 0.000000
 3050/5000: episode: 111, duration: 0.416s, episode steps:  27, steps per second:  65, episode reward: 42.000, mean reward:  1.556 [-2.183, 32.400], mean action: 4.296 [1.000, 16.000],  loss: 0.021804, mae: 0.327059, mean_q: 0.578231, mean_eps: 0.000000
 3091/5000: episode: 112, duration: 0.622s, episode steps:  41, steps per second:  66, episode reward: 39.000, mean reward:  0.951 [-2.503, 32.510], mean action: 2.537 [0.000, 16.000],  loss: 0.019253, mae: 0.314170, mean_q: 0.556990, mean_eps: 0.000000
 3118/5000: episode: 113, duration: 0.391s, episode steps:  27, steps per second:  69, episode reward: 41.626, mean reward:  1.542 [-2.190, 32.070], mean action: 2.889 [0.000, 19.000],  loss: 0.022456, mae: 0.321789, mean_q: 0.591932, mean_eps: 0.000000
 3143/5000: episode: 114, duration: 0.405s, episode steps:  25, steps per second:  62, episode reward: 40.757, mean reward:  1.630 [-2.772, 31.975], mean action: 5.600 [0.000, 19.000],  loss: 0.020019, mae: 0.322497, mean_q: 0.561749, mean_eps: 0.000000
 3160/5000: episode: 115, duration: 0.258s, episode steps:  17, steps per second:  66, episode reward: 44.714, mean reward:  2.630 [-2.207, 32.130], mean action: 3.588 [0.000, 14.000],  loss: 0.021590, mae: 0.333402, mean_q: 0.536137, mean_eps: 0.000000
 3183/5000: episode: 116, duration: 0.349s, episode steps:  23, steps per second:  66, episode reward: 41.923, mean reward:  1.823 [-2.776, 32.110], mean action: 2.087 [0.000, 16.000],  loss: 0.028541, mae: 0.350529, mean_q: 0.508420, mean_eps: 0.000000
 3210/5000: episode: 117, duration: 0.388s, episode steps:  27, steps per second:  70, episode reward: 41.436, mean reward:  1.535 [-2.380, 32.420], mean action: 3.963 [0.000, 9.000],  loss: 0.020388, mae: 0.319590, mean_q: 0.523951, mean_eps: 0.000000
 3235/5000: episode: 118, duration: 0.449s, episode steps:  25, steps per second:  56, episode reward: 40.610, mean reward:  1.624 [-2.219, 31.700], mean action: 2.040 [0.000, 9.000],  loss: 0.022287, mae: 0.327031, mean_q: 0.565273, mean_eps: 0.000000
 3266/5000: episode: 119, duration: 0.476s, episode steps:  31, steps per second:  65, episode reward: 36.000, mean reward:  1.161 [-2.756, 32.090], mean action: 2.032 [0.000, 9.000],  loss: 0.021481, mae: 0.323403, mean_q: 0.587311, mean_eps: 0.000000
 3287/5000: episode: 120, duration: 0.312s, episode steps:  21, steps per second:  67, episode reward: 41.174, mean reward:  1.961 [-3.000, 32.350], mean action: 3.810 [0.000, 16.000],  loss: 0.017432, mae: 0.309218, mean_q: 0.564367, mean_eps: 0.000000
 3322/5000: episode: 121, duration: 0.542s, episode steps:  35, steps per second:  65, episode reward: 35.753, mean reward:  1.022 [-2.571, 32.640], mean action: 3.171 [0.000, 19.000],  loss: 0.019708, mae: 0.321842, mean_q: 0.533549, mean_eps: 0.000000
 3345/5000: episode: 122, duration: 0.474s, episode steps:  23, steps per second:  49, episode reward: 41.579, mean reward:  1.808 [-2.270, 32.276], mean action: 5.043 [1.000, 20.000],  loss: 0.022222, mae: 0.331182, mean_q: 0.498294, mean_eps: 0.000000
 3369/5000: episode: 123, duration: 0.360s, episode steps:  24, steps per second:  67, episode reward: 38.761, mean reward:  1.615 [-2.194, 32.001], mean action: 3.667 [0.000, 19.000],  loss: 0.017954, mae: 0.303828, mean_q: 0.542946, mean_eps: 0.000000
 3382/5000: episode: 124, duration: 0.212s, episode steps:  13, steps per second:  61, episode reward: 41.419, mean reward:  3.186 [-3.000, 32.359], mean action: 2.000 [0.000, 11.000],  loss: 0.023331, mae: 0.336915, mean_q: 0.502661, mean_eps: 0.000000
 3408/5000: episode: 125, duration: 0.384s, episode steps:  26, steps per second:  68, episode reward: 46.753, mean reward:  1.798 [-0.358, 32.130], mean action: 1.885 [1.000, 8.000],  loss: 0.019593, mae: 0.323830, mean_q: 0.477191, mean_eps: 0.000000
 3426/5000: episode: 126, duration: 0.268s, episode steps:  18, steps per second:  67, episode reward: 41.513, mean reward:  2.306 [-2.751, 32.010], mean action: 2.333 [0.000, 9.000],  loss: 0.017788, mae: 0.306927, mean_q: 0.542138, mean_eps: 0.000000
 3446/5000: episode: 127, duration: 0.316s, episode steps:  20, steps per second:  63, episode reward: 45.000, mean reward:  2.250 [-2.281, 32.700], mean action: 2.500 [0.000, 12.000],  loss: 0.017684, mae: 0.312598, mean_q: 0.557572, mean_eps: 0.000000
 3480/5000: episode: 128, duration: 0.590s, episode steps:  34, steps per second:  58, episode reward: 37.770, mean reward:  1.111 [-2.531, 32.380], mean action: 3.147 [0.000, 14.000],  loss: 0.021963, mae: 0.333718, mean_q: 0.535074, mean_eps: 0.000000
 3506/5000: episode: 129, duration: 0.387s, episode steps:  26, steps per second:  67, episode reward: 32.477, mean reward:  1.249 [-2.722, 31.857], mean action: 3.923 [0.000, 16.000],  loss: 0.021760, mae: 0.327776, mean_q: 0.562267, mean_eps: 0.000000
 3546/5000: episode: 130, duration: 0.585s, episode steps:  40, steps per second:  68, episode reward: 35.301, mean reward:  0.883 [-2.354, 32.110], mean action: 3.275 [0.000, 14.000],  loss: 0.021675, mae: 0.332988, mean_q: 0.567076, mean_eps: 0.000000
 3564/5000: episode: 131, duration: 0.274s, episode steps:  18, steps per second:  66, episode reward: 40.850, mean reward:  2.269 [-2.095, 31.705], mean action: 2.167 [0.000, 15.000],  loss: 0.024086, mae: 0.351691, mean_q: 0.624571, mean_eps: 0.000000
 3585/5000: episode: 132, duration: 0.324s, episode steps:  21, steps per second:  65, episode reward: 44.471, mean reward:  2.118 [-2.257, 32.110], mean action: 1.333 [0.000, 15.000],  loss: 0.020082, mae: 0.344269, mean_q: 0.588407, mean_eps: 0.000000
 3607/5000: episode: 133, duration: 0.348s, episode steps:  22, steps per second:  63, episode reward: 37.356, mean reward:  1.698 [-3.000, 31.969], mean action: 4.682 [0.000, 19.000],  loss: 0.021547, mae: 0.342436, mean_q: 0.547818, mean_eps: 0.000000
 3630/5000: episode: 134, duration: 0.365s, episode steps:  23, steps per second:  63, episode reward: 43.844, mean reward:  1.906 [-2.599, 32.091], mean action: 2.957 [0.000, 15.000],  loss: 0.019996, mae: 0.328650, mean_q: 0.517324, mean_eps: 0.000000
 3659/5000: episode: 135, duration: 0.427s, episode steps:  29, steps per second:  68, episode reward: 41.925, mean reward:  1.446 [-3.000, 32.590], mean action: 3.000 [0.000, 19.000],  loss: 0.021895, mae: 0.333293, mean_q: 0.540570, mean_eps: 0.000000
 3681/5000: episode: 136, duration: 0.324s, episode steps:  22, steps per second:  68, episode reward: 42.000, mean reward:  1.909 [-2.362, 32.170], mean action: 2.818 [0.000, 19.000],  loss: 0.023292, mae: 0.344610, mean_q: 0.537150, mean_eps: 0.000000
 3708/5000: episode: 137, duration: 0.383s, episode steps:  27, steps per second:  70, episode reward: 41.614, mean reward:  1.541 [-3.000, 32.210], mean action: 1.741 [0.000, 19.000],  loss: 0.018853, mae: 0.320746, mean_q: 0.594075, mean_eps: 0.000000
 3726/5000: episode: 138, duration: 0.281s, episode steps:  18, steps per second:  64, episode reward: 44.372, mean reward:  2.465 [-2.459, 32.150], mean action: 2.056 [0.000, 12.000],  loss: 0.022555, mae: 0.332696, mean_q: 0.540590, mean_eps: 0.000000
 3753/5000: episode: 139, duration: 0.405s, episode steps:  27, steps per second:  67, episode reward: 42.000, mean reward:  1.556 [-2.536, 32.200], mean action: 1.556 [0.000, 16.000],  loss: 0.022398, mae: 0.325615, mean_q: 0.572435, mean_eps: 0.000000
 3780/5000: episode: 140, duration: 0.390s, episode steps:  27, steps per second:  69, episode reward: -32.630, mean reward: -1.209 [-32.327,  2.550], mean action: 4.519 [0.000, 19.000],  loss: 0.021063, mae: 0.328108, mean_q: 0.550270, mean_eps: 0.000000
 3800/5000: episode: 141, duration: 0.315s, episode steps:  20, steps per second:  63, episode reward: 42.000, mean reward:  2.100 [-2.125, 32.090], mean action: 2.450 [0.000, 16.000],  loss: 0.020141, mae: 0.323826, mean_q: 0.533321, mean_eps: 0.000000
 3822/5000: episode: 142, duration: 0.321s, episode steps:  22, steps per second:  68, episode reward: 47.213, mean reward:  2.146 [-0.127, 32.300], mean action: 4.455 [0.000, 13.000],  loss: 0.022285, mae: 0.331901, mean_q: 0.560847, mean_eps: 0.000000
 3844/5000: episode: 143, duration: 0.318s, episode steps:  22, steps per second:  69, episode reward: 47.500, mean reward:  2.159 [ 0.000, 32.660], mean action: 2.182 [0.000, 3.000],  loss: 0.022772, mae: 0.334799, mean_q: 0.533736, mean_eps: 0.000000
 3868/5000: episode: 144, duration: 0.359s, episode steps:  24, steps per second:  67, episode reward: 44.477, mean reward:  1.853 [-2.076, 32.200], mean action: 3.500 [0.000, 16.000],  loss: 0.025438, mae: 0.337412, mean_q: 0.525989, mean_eps: 0.000000
 3898/5000: episode: 145, duration: 0.535s, episode steps:  30, steps per second:  56, episode reward: 41.771, mean reward:  1.392 [-2.620, 32.310], mean action: 2.733 [0.000, 16.000],  loss: 0.018800, mae: 0.308797, mean_q: 0.519671, mean_eps: 0.000000
 3922/5000: episode: 146, duration: 0.380s, episode steps:  24, steps per second:  63, episode reward: 41.709, mean reward:  1.738 [-2.306, 31.963], mean action: 3.375 [0.000, 16.000],  loss: 0.018439, mae: 0.301643, mean_q: 0.494628, mean_eps: 0.000000
 3941/5000: episode: 147, duration: 0.290s, episode steps:  19, steps per second:  66, episode reward: 38.667, mean reward:  2.035 [-2.576, 32.540], mean action: 4.737 [1.000, 19.000],  loss: 0.017482, mae: 0.292195, mean_q: 0.514049, mean_eps: 0.000000
 3968/5000: episode: 148, duration: 0.388s, episode steps:  27, steps per second:  70, episode reward: 41.709, mean reward:  1.545 [-2.233, 32.240], mean action: 3.556 [0.000, 19.000],  loss: 0.019158, mae: 0.303765, mean_q: 0.520783, mean_eps: 0.000000
 3988/5000: episode: 149, duration: 0.294s, episode steps:  20, steps per second:  68, episode reward: 39.000, mean reward:  1.950 [-2.402, 32.260], mean action: 3.250 [0.000, 16.000],  loss: 0.020453, mae: 0.309745, mean_q: 0.546158, mean_eps: 0.000000
 4013/5000: episode: 150, duration: 0.365s, episode steps:  25, steps per second:  69, episode reward: 38.953, mean reward:  1.558 [-2.291, 32.010], mean action: 4.000 [0.000, 19.000],  loss: 0.018394, mae: 0.304470, mean_q: 0.531217, mean_eps: 0.000000
 4042/5000: episode: 151, duration: 0.422s, episode steps:  29, steps per second:  69, episode reward: 43.026, mean reward:  1.484 [-2.467, 32.700], mean action: 2.793 [0.000, 13.000],  loss: 0.020703, mae: 0.314460, mean_q: 0.547630, mean_eps: 0.000000
 4072/5000: episode: 152, duration: 0.427s, episode steps:  30, steps per second:  70, episode reward: 41.189, mean reward:  1.373 [-3.000, 32.094], mean action: 6.067 [0.000, 19.000],  loss: 0.022403, mae: 0.330445, mean_q: 0.543285, mean_eps: 0.000000
 4091/5000: episode: 153, duration: 0.276s, episode steps:  19, steps per second:  69, episode reward: 41.674, mean reward:  2.193 [-2.869, 32.250], mean action: 5.737 [0.000, 16.000],  loss: 0.020443, mae: 0.328116, mean_q: 0.574581, mean_eps: 0.000000
 4115/5000: episode: 154, duration: 0.345s, episode steps:  24, steps per second:  70, episode reward: 38.767, mean reward:  1.615 [-2.433, 32.048], mean action: 3.250 [0.000, 14.000],  loss: 0.019258, mae: 0.312298, mean_q: 0.551148, mean_eps: 0.000000
 4145/5000: episode: 155, duration: 0.428s, episode steps:  30, steps per second:  70, episode reward: 40.455, mean reward:  1.348 [-2.414, 32.603], mean action: 2.900 [0.000, 16.000],  loss: 0.023752, mae: 0.338750, mean_q: 0.578459, mean_eps: 0.000000
 4194/5000: episode: 156, duration: 0.723s, episode steps:  49, steps per second:  68, episode reward: 44.292, mean reward:  0.904 [-2.278, 32.050], mean action: 1.959 [0.000, 13.000],  loss: 0.019850, mae: 0.322186, mean_q: 0.579104, mean_eps: 0.000000
 4218/5000: episode: 157, duration: 0.363s, episode steps:  24, steps per second:  66, episode reward: 44.857, mean reward:  1.869 [-2.504, 32.300], mean action: 0.917 [0.000, 12.000],  loss: 0.024533, mae: 0.345045, mean_q: 0.602777, mean_eps: 0.000000
 4240/5000: episode: 158, duration: 0.324s, episode steps:  22, steps per second:  68, episode reward: 41.319, mean reward:  1.878 [-2.301, 31.608], mean action: 2.909 [0.000, 16.000],  loss: 0.019276, mae: 0.322261, mean_q: 0.595122, mean_eps: 0.000000
 4260/5000: episode: 159, duration: 0.296s, episode steps:  20, steps per second:  68, episode reward: 38.681, mean reward:  1.934 [-3.000, 31.962], mean action: 3.800 [1.000, 16.000],  loss: 0.019311, mae: 0.327258, mean_q: 0.552684, mean_eps: 0.000000
 4282/5000: episode: 160, duration: 0.330s, episode steps:  22, steps per second:  67, episode reward: 44.960, mean reward:  2.044 [-2.012, 32.440], mean action: 3.455 [0.000, 16.000],  loss: 0.015958, mae: 0.310972, mean_q: 0.576011, mean_eps: 0.000000
 4298/5000: episode: 161, duration: 0.250s, episode steps:  16, steps per second:  64, episode reward: 44.179, mean reward:  2.761 [-2.180, 32.903], mean action: 4.688 [0.000, 16.000],  loss: 0.018506, mae: 0.324892, mean_q: 0.564578, mean_eps: 0.000000
 4328/5000: episode: 162, duration: 0.434s, episode steps:  30, steps per second:  69, episode reward: 42.000, mean reward:  1.400 [-2.209, 32.060], mean action: 2.467 [1.000, 16.000],  loss: 0.022175, mae: 0.344493, mean_q: 0.515034, mean_eps: 0.000000
 4357/5000: episode: 163, duration: 0.426s, episode steps:  29, steps per second:  68, episode reward: -32.070, mean reward: -1.106 [-32.180,  2.939], mean action: 4.690 [0.000, 19.000],  loss: 0.023500, mae: 0.342328, mean_q: 0.512430, mean_eps: 0.000000
 4389/5000: episode: 164, duration: 0.453s, episode steps:  32, steps per second:  71, episode reward: 38.148, mean reward:  1.192 [-3.000, 32.150], mean action: 2.375 [0.000, 11.000],  loss: 0.018351, mae: 0.313653, mean_q: 0.531228, mean_eps: 0.000000
 4446/5000: episode: 165, duration: 0.914s, episode steps:  57, steps per second:  62, episode reward: -32.850, mean reward: -0.576 [-33.154,  2.196], mean action: 2.947 [0.000, 19.000],  loss: 0.021831, mae: 0.335772, mean_q: 0.551105, mean_eps: 0.000000
 4467/5000: episode: 166, duration: 0.329s, episode steps:  21, steps per second:  64, episode reward: 38.619, mean reward:  1.839 [-2.356, 31.783], mean action: 1.952 [0.000, 15.000],  loss: 0.024361, mae: 0.346296, mean_q: 0.587384, mean_eps: 0.000000
 4478/5000: episode: 167, duration: 0.173s, episode steps:  11, steps per second:  64, episode reward: 44.839, mean reward:  4.076 [-2.004, 32.900], mean action: 1.636 [1.000, 3.000],  loss: 0.017414, mae: 0.306348, mean_q: 0.596041, mean_eps: 0.000000
 4506/5000: episode: 168, duration: 0.397s, episode steps:  28, steps per second:  70, episode reward: 38.890, mean reward:  1.389 [-3.000, 33.000], mean action: 4.500 [0.000, 15.000],  loss: 0.023420, mae: 0.342282, mean_q: 0.564521, mean_eps: 0.000000
 4538/5000: episode: 169, duration: 0.462s, episode steps:  32, steps per second:  69, episode reward: 38.679, mean reward:  1.209 [-3.000, 32.320], mean action: 4.156 [0.000, 15.000],  loss: 0.022169, mae: 0.330824, mean_q: 0.556874, mean_eps: 0.000000
 4553/5000: episode: 170, duration: 0.235s, episode steps:  15, steps per second:  64, episode reward: 44.903, mean reward:  2.994 [-2.494, 32.143], mean action: 0.733 [0.000, 3.000],  loss: 0.016607, mae: 0.313073, mean_q: 0.536006, mean_eps: 0.000000
 4573/5000: episode: 171, duration: 0.313s, episode steps:  20, steps per second:  64, episode reward: 46.662, mean reward:  2.333 [-0.327, 32.374], mean action: 2.050 [0.000, 8.000],  loss: 0.021069, mae: 0.327582, mean_q: 0.489865, mean_eps: 0.000000
 4595/5000: episode: 172, duration: 0.327s, episode steps:  22, steps per second:  67, episode reward: 45.000, mean reward:  2.045 [-2.269, 32.180], mean action: 1.364 [0.000, 3.000],  loss: 0.022451, mae: 0.325168, mean_q: 0.537460, mean_eps: 0.000000
 4617/5000: episode: 173, duration: 0.322s, episode steps:  22, steps per second:  68, episode reward: 38.104, mean reward:  1.732 [-3.000, 32.140], mean action: 5.045 [0.000, 16.000],  loss: 0.018234, mae: 0.302814, mean_q: 0.499123, mean_eps: 0.000000
 4646/5000: episode: 174, duration: 0.416s, episode steps:  29, steps per second:  70, episode reward: 38.359, mean reward:  1.323 [-2.563, 32.060], mean action: 3.310 [0.000, 16.000],  loss: 0.019267, mae: 0.311639, mean_q: 0.514060, mean_eps: 0.000000
 4667/5000: episode: 175, duration: 0.322s, episode steps:  21, steps per second:  65, episode reward: 41.978, mean reward:  1.999 [-2.074, 32.550], mean action: 4.476 [0.000, 15.000],  loss: 0.026869, mae: 0.344047, mean_q: 0.523542, mean_eps: 0.000000
 4702/5000: episode: 176, duration: 0.517s, episode steps:  35, steps per second:  68, episode reward: 44.103, mean reward:  1.260 [-2.901, 32.130], mean action: 3.714 [0.000, 16.000],  loss: 0.019441, mae: 0.309651, mean_q: 0.520481, mean_eps: 0.000000
 4721/5000: episode: 177, duration: 0.298s, episode steps:  19, steps per second:  64, episode reward: 44.587, mean reward:  2.347 [-2.297, 32.030], mean action: 3.105 [2.000, 16.000],  loss: 0.020425, mae: 0.310884, mean_q: 0.522809, mean_eps: 0.000000
 4743/5000: episode: 178, duration: 0.323s, episode steps:  22, steps per second:  68, episode reward: 43.631, mean reward:  1.983 [-2.281, 31.863], mean action: 3.091 [0.000, 16.000],  loss: 0.020045, mae: 0.310198, mean_q: 0.516705, mean_eps: 0.000000
 4761/5000: episode: 179, duration: 0.268s, episode steps:  18, steps per second:  67, episode reward: 44.696, mean reward:  2.483 [-2.360, 32.280], mean action: 2.111 [0.000, 15.000],  loss: 0.019567, mae: 0.302945, mean_q: 0.479161, mean_eps: 0.000000
 4793/5000: episode: 180, duration: 0.464s, episode steps:  32, steps per second:  69, episode reward: 42.000, mean reward:  1.312 [-2.165, 32.160], mean action: 4.031 [1.000, 16.000],  loss: 0.019657, mae: 0.306798, mean_q: 0.522281, mean_eps: 0.000000
 4821/5000: episode: 181, duration: 0.415s, episode steps:  28, steps per second:  67, episode reward: 38.508, mean reward:  1.375 [-2.251, 31.638], mean action: 4.429 [0.000, 18.000],  loss: 0.019883, mae: 0.314793, mean_q: 0.568429, mean_eps: 0.000000
 4843/5000: episode: 182, duration: 0.336s, episode steps:  22, steps per second:  65, episode reward: 44.447, mean reward:  2.020 [-2.232, 31.896], mean action: 3.273 [2.000, 9.000],  loss: 0.020058, mae: 0.316436, mean_q: 0.528920, mean_eps: 0.000000
 4875/5000: episode: 183, duration: 0.456s, episode steps:  32, steps per second:  70, episode reward: 39.000, mean reward:  1.219 [-2.873, 32.310], mean action: 3.500 [0.000, 19.000],  loss: 0.017805, mae: 0.313048, mean_q: 0.518428, mean_eps: 0.000000
 4900/5000: episode: 184, duration: 0.379s, episode steps:  25, steps per second:  66, episode reward: 41.833, mean reward:  1.673 [-2.313, 33.000], mean action: 4.960 [1.000, 19.000],  loss: 0.020325, mae: 0.321065, mean_q: 0.550926, mean_eps: 0.000000
 4923/5000: episode: 185, duration: 0.337s, episode steps:  23, steps per second:  68, episode reward: 38.734, mean reward:  1.684 [-3.000, 32.094], mean action: 3.174 [0.000, 19.000],  loss: 0.019803, mae: 0.314337, mean_q: 0.614199, mean_eps: 0.000000
 4938/5000: episode: 186, duration: 0.236s, episode steps:  15, steps per second:  64, episode reward: 44.428, mean reward:  2.962 [-2.356, 32.280], mean action: 4.533 [1.000, 19.000],  loss: 0.022349, mae: 0.321630, mean_q: 0.500457, mean_eps: 0.000000
 4981/5000: episode: 187, duration: 0.612s, episode steps:  43, steps per second:  70, episode reward: 41.034, mean reward:  0.954 [-2.525, 32.220], mean action: 2.372 [0.000, 16.000],  loss: 0.019968, mae: 0.316217, mean_q: 0.507029, mean_eps: 0.000000
done, took 67.535 seconds
DQN Evaluation: 12458 victories out of 14503 episodes
Training for 5000 steps ...
   16/5000: episode: 1, duration: 0.182s, episode steps:  16, steps per second:  88, episode reward: 44.127, mean reward:  2.758 [-2.737, 32.441], mean action: 4.062 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   48/5000: episode: 2, duration: 0.227s, episode steps:  32, steps per second: 141, episode reward: 32.901, mean reward:  1.028 [-2.581, 32.461], mean action: 4.094 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   69/5000: episode: 3, duration: 0.149s, episode steps:  21, steps per second: 141, episode reward: 32.901, mean reward:  1.567 [-2.603, 32.251], mean action: 5.905 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   89/5000: episode: 4, duration: 0.150s, episode steps:  20, steps per second: 133, episode reward: 38.082, mean reward:  1.904 [-2.278, 32.689], mean action: 3.550 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  110/5000: episode: 5, duration: 0.164s, episode steps:  21, steps per second: 128, episode reward: 32.805, mean reward:  1.562 [-3.000, 32.100], mean action: 4.619 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  127/5000: episode: 6, duration: 0.124s, episode steps:  17, steps per second: 137, episode reward: 36.000, mean reward:  2.118 [-2.562, 32.420], mean action: 5.000 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  145/5000: episode: 7, duration: 0.132s, episode steps:  18, steps per second: 136, episode reward: 42.000, mean reward:  2.333 [-2.759, 32.080], mean action: 3.000 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  168/5000: episode: 8, duration: 0.158s, episode steps:  23, steps per second: 146, episode reward: 34.254, mean reward:  1.489 [-2.470, 32.647], mean action: 5.609 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  184/5000: episode: 9, duration: 0.115s, episode steps:  16, steps per second: 139, episode reward: 38.178, mean reward:  2.386 [-2.607, 32.180], mean action: 4.812 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  202/5000: episode: 10, duration: 0.137s, episode steps:  18, steps per second: 131, episode reward: 38.248, mean reward:  2.125 [-2.745, 31.941], mean action: 4.056 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  229/5000: episode: 11, duration: 0.177s, episode steps:  27, steps per second: 153, episode reward: 34.435, mean reward:  1.275 [-2.314, 32.100], mean action: 5.556 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  254/5000: episode: 12, duration: 0.174s, episode steps:  25, steps per second: 143, episode reward: 34.490, mean reward:  1.380 [-2.642, 32.170], mean action: 4.880 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  269/5000: episode: 13, duration: 0.133s, episode steps:  15, steps per second: 113, episode reward: 39.000, mean reward:  2.600 [-2.903, 32.130], mean action: 5.067 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  292/5000: episode: 14, duration: 0.172s, episode steps:  23, steps per second: 134, episode reward: 43.524, mean reward:  1.892 [-2.082, 32.350], mean action: 3.522 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  309/5000: episode: 15, duration: 0.123s, episode steps:  17, steps per second: 138, episode reward: 38.711, mean reward:  2.277 [-2.998, 32.535], mean action: 5.647 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  339/5000: episode: 16, duration: 0.199s, episode steps:  30, steps per second: 151, episode reward: 32.903, mean reward:  1.097 [-2.903, 32.193], mean action: 4.400 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  360/5000: episode: 17, duration: 0.164s, episode steps:  21, steps per second: 128, episode reward: 40.461, mean reward:  1.927 [-2.381, 32.212], mean action: 4.762 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  379/5000: episode: 18, duration: 0.145s, episode steps:  19, steps per second: 131, episode reward: 41.480, mean reward:  2.183 [-2.430, 31.953], mean action: 3.632 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  406/5000: episode: 19, duration: 0.185s, episode steps:  27, steps per second: 146, episode reward: 34.922, mean reward:  1.293 [-2.226, 32.090], mean action: 5.037 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  425/5000: episode: 20, duration: 0.142s, episode steps:  19, steps per second: 134, episode reward: 38.665, mean reward:  2.035 [-2.651, 32.420], mean action: 5.211 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  442/5000: episode: 21, duration: 0.123s, episode steps:  17, steps per second: 139, episode reward: 38.900, mean reward:  2.288 [-2.615, 32.900], mean action: 3.588 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  465/5000: episode: 22, duration: 0.162s, episode steps:  23, steps per second: 142, episode reward: 38.794, mean reward:  1.687 [-2.725, 32.230], mean action: 4.304 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  486/5000: episode: 23, duration: 0.156s, episode steps:  21, steps per second: 134, episode reward: -35.130, mean reward: -1.673 [-32.066,  2.710], mean action: 5.714 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  512/5000: episode: 24, duration: 0.178s, episode steps:  26, steps per second: 146, episode reward: 44.494, mean reward:  1.711 [-2.401, 32.120], mean action: 3.269 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  529/5000: episode: 25, duration: 0.133s, episode steps:  17, steps per second: 128, episode reward: 40.781, mean reward:  2.399 [-3.000, 32.410], mean action: 3.824 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  557/5000: episode: 26, duration: 0.199s, episode steps:  28, steps per second: 140, episode reward: 32.636, mean reward:  1.166 [-3.000, 31.766], mean action: 4.214 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  574/5000: episode: 27, duration: 0.132s, episode steps:  17, steps per second: 129, episode reward: 38.037, mean reward:  2.237 [-3.000, 32.560], mean action: 5.471 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  613/5000: episode: 28, duration: 0.477s, episode steps:  39, steps per second:  82, episode reward: -35.340, mean reward: -0.906 [-32.051,  2.670], mean action: 11.590 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  637/5000: episode: 29, duration: 0.192s, episode steps:  24, steps per second: 125, episode reward: 44.122, mean reward:  1.838 [-2.145, 32.133], mean action: 3.458 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  648/5000: episode: 30, duration: 0.106s, episode steps:  11, steps per second: 104, episode reward: 44.199, mean reward:  4.018 [-2.004, 32.120], mean action: 2.727 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  674/5000: episode: 31, duration: 0.194s, episode steps:  26, steps per second: 134, episode reward: 35.803, mean reward:  1.377 [-3.000, 32.321], mean action: 4.500 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  685/5000: episode: 32, duration: 0.094s, episode steps:  11, steps per second: 117, episode reward: 41.219, mean reward:  3.747 [-3.000, 33.000], mean action: 4.364 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  712/5000: episode: 33, duration: 0.192s, episode steps:  27, steps per second: 141, episode reward: 35.901, mean reward:  1.330 [-2.260, 32.211], mean action: 5.037 [1.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  744/5000: episode: 34, duration: 0.205s, episode steps:  32, steps per second: 156, episode reward: -42.000, mean reward: -1.312 [-32.628,  1.839], mean action: 8.281 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  773/5000: episode: 35, duration: 0.208s, episode steps:  29, steps per second: 139, episode reward: 37.973, mean reward:  1.309 [-2.485, 31.985], mean action: 4.241 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  798/5000: episode: 36, duration: 0.185s, episode steps:  25, steps per second: 135, episode reward: 41.318, mean reward:  1.653 [-2.366, 33.000], mean action: 3.080 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  823/5000: episode: 37, duration: 0.167s, episode steps:  25, steps per second: 150, episode reward: 32.597, mean reward:  1.304 [-2.418, 32.100], mean action: 4.720 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  847/5000: episode: 38, duration: 0.170s, episode steps:  24, steps per second: 141, episode reward: 32.117, mean reward:  1.338 [-3.000, 31.956], mean action: 6.917 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  866/5000: episode: 39, duration: 0.133s, episode steps:  19, steps per second: 142, episode reward: 35.918, mean reward:  1.890 [-3.000, 32.918], mean action: 6.368 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  907/5000: episode: 40, duration: 0.273s, episode steps:  41, steps per second: 150, episode reward: -32.460, mean reward: -0.792 [-32.460,  2.360], mean action: 10.366 [1.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  925/5000: episode: 41, duration: 0.130s, episode steps:  18, steps per second: 138, episode reward: 35.596, mean reward:  1.978 [-3.000, 32.040], mean action: 4.444 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  948/5000: episode: 42, duration: 0.162s, episode steps:  23, steps per second: 142, episode reward: 44.112, mean reward:  1.918 [-2.907, 32.220], mean action: 3.522 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  983/5000: episode: 43, duration: 0.224s, episode steps:  35, steps per second: 157, episode reward: 32.195, mean reward:  0.920 [-2.591, 32.160], mean action: 11.200 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1007/5000: episode: 44, duration: 0.225s, episode steps:  24, steps per second: 107, episode reward: 38.901, mean reward:  1.621 [-3.000, 32.071], mean action: 4.083 [1.000, 19.000],  loss: 0.018372, mae: 0.301012, mean_q: 0.513493, mean_eps: 0.000000
 1029/5000: episode: 45, duration: 0.332s, episode steps:  22, steps per second:  66, episode reward: -36.000, mean reward: -1.636 [-32.471,  2.600], mean action: 8.955 [0.000, 19.000],  loss: 0.021569, mae: 0.312093, mean_q: 0.516215, mean_eps: 0.000000
 1056/5000: episode: 46, duration: 0.420s, episode steps:  27, steps per second:  64, episode reward: 36.000, mean reward:  1.333 [-2.504, 29.865], mean action: 4.889 [1.000, 20.000],  loss: 0.019975, mae: 0.306557, mean_q: 0.598038, mean_eps: 0.000000
 1084/5000: episode: 47, duration: 0.413s, episode steps:  28, steps per second:  68, episode reward: 33.000, mean reward:  1.179 [-2.757, 33.259], mean action: 4.821 [0.000, 18.000],  loss: 0.021198, mae: 0.321404, mean_q: 0.590483, mean_eps: 0.000000
 1120/5000: episode: 48, duration: 0.517s, episode steps:  36, steps per second:  70, episode reward: -32.100, mean reward: -0.892 [-32.096,  2.160], mean action: 3.667 [0.000, 21.000],  loss: 0.020806, mae: 0.313517, mean_q: 0.568093, mean_eps: 0.000000
 1145/5000: episode: 49, duration: 0.365s, episode steps:  25, steps per second:  68, episode reward: 38.234, mean reward:  1.529 [-2.643, 32.177], mean action: 6.040 [0.000, 19.000],  loss: 0.019464, mae: 0.308298, mean_q: 0.572524, mean_eps: 0.000000
 1171/5000: episode: 50, duration: 0.384s, episode steps:  26, steps per second:  68, episode reward: 35.899, mean reward:  1.381 [-2.291, 33.000], mean action: 5.731 [0.000, 19.000],  loss: 0.017133, mae: 0.289764, mean_q: 0.545388, mean_eps: 0.000000
 1204/5000: episode: 51, duration: 0.481s, episode steps:  33, steps per second:  69, episode reward: 39.964, mean reward:  1.211 [-2.234, 32.110], mean action: 7.879 [0.000, 18.000],  loss: 0.021623, mae: 0.320335, mean_q: 0.583491, mean_eps: 0.000000
 1225/5000: episode: 52, duration: 0.307s, episode steps:  21, steps per second:  68, episode reward: -33.000, mean reward: -1.571 [-32.881,  3.000], mean action: 7.000 [0.000, 15.000],  loss: 0.021571, mae: 0.316083, mean_q: 0.594608, mean_eps: 0.000000
 1250/5000: episode: 53, duration: 0.368s, episode steps:  25, steps per second:  68, episode reward: 32.417, mean reward:  1.297 [-2.860, 33.000], mean action: 7.160 [0.000, 18.000],  loss: 0.017749, mae: 0.300152, mean_q: 0.550905, mean_eps: 0.000000
 1280/5000: episode: 54, duration: 0.432s, episode steps:  30, steps per second:  69, episode reward: -35.720, mean reward: -1.191 [-32.092,  2.664], mean action: 12.200 [0.000, 19.000],  loss: 0.021095, mae: 0.309698, mean_q: 0.548109, mean_eps: 0.000000
 1302/5000: episode: 55, duration: 0.328s, episode steps:  22, steps per second:  67, episode reward: -35.510, mean reward: -1.614 [-32.092,  2.650], mean action: 4.682 [0.000, 19.000],  loss: 0.022908, mae: 0.320428, mean_q: 0.560395, mean_eps: 0.000000
 1325/5000: episode: 56, duration: 0.445s, episode steps:  23, steps per second:  52, episode reward: 33.000, mean reward:  1.435 [-2.903, 32.040], mean action: 6.609 [0.000, 19.000],  loss: 0.019567, mae: 0.311192, mean_q: 0.523625, mean_eps: 0.000000
 1349/5000: episode: 57, duration: 0.351s, episode steps:  24, steps per second:  68, episode reward: 41.500, mean reward:  1.729 [-3.000, 32.230], mean action: 2.750 [0.000, 19.000],  loss: 0.022064, mae: 0.315327, mean_q: 0.576813, mean_eps: 0.000000
 1373/5000: episode: 58, duration: 0.347s, episode steps:  24, steps per second:  69, episode reward: 35.112, mean reward:  1.463 [-2.354, 32.150], mean action: 2.417 [0.000, 12.000],  loss: 0.017158, mae: 0.295951, mean_q: 0.532952, mean_eps: 0.000000
 1387/5000: episode: 59, duration: 0.226s, episode steps:  14, steps per second:  62, episode reward: 41.903, mean reward:  2.993 [-2.358, 32.113], mean action: 2.714 [0.000, 16.000],  loss: 0.022734, mae: 0.326008, mean_q: 0.562470, mean_eps: 0.000000
 1402/5000: episode: 60, duration: 0.251s, episode steps:  15, steps per second:  60, episode reward: 41.134, mean reward:  2.742 [-2.901, 32.260], mean action: 3.133 [1.000, 12.000],  loss: 0.024172, mae: 0.335489, mean_q: 0.522545, mean_eps: 0.000000
 1425/5000: episode: 61, duration: 0.353s, episode steps:  23, steps per second:  65, episode reward: 32.464, mean reward:  1.411 [-3.000, 32.224], mean action: 3.391 [0.000, 12.000],  loss: 0.022778, mae: 0.332494, mean_q: 0.486839, mean_eps: 0.000000
 1450/5000: episode: 62, duration: 0.365s, episode steps:  25, steps per second:  68, episode reward: 35.217, mean reward:  1.409 [-2.514, 29.473], mean action: 7.920 [1.000, 21.000],  loss: 0.019508, mae: 0.318940, mean_q: 0.520837, mean_eps: 0.000000
 1467/5000: episode: 63, duration: 0.260s, episode steps:  17, steps per second:  65, episode reward: 38.073, mean reward:  2.240 [-2.568, 31.785], mean action: 3.647 [0.000, 16.000],  loss: 0.019801, mae: 0.322853, mean_q: 0.565087, mean_eps: 0.000000
 1485/5000: episode: 64, duration: 0.279s, episode steps:  18, steps per second:  65, episode reward: 38.136, mean reward:  2.119 [-2.826, 32.370], mean action: 5.556 [0.000, 16.000],  loss: 0.021795, mae: 0.321481, mean_q: 0.505136, mean_eps: 0.000000
 1507/5000: episode: 65, duration: 0.335s, episode steps:  22, steps per second:  66, episode reward: 38.628, mean reward:  1.756 [-2.396, 32.368], mean action: 5.636 [0.000, 20.000],  loss: 0.018998, mae: 0.311073, mean_q: 0.463013, mean_eps: 0.000000
 1534/5000: episode: 66, duration: 0.455s, episode steps:  27, steps per second:  59, episode reward: 32.214, mean reward:  1.193 [-2.400, 32.100], mean action: 6.111 [0.000, 20.000],  loss: 0.027610, mae: 0.348974, mean_q: 0.479495, mean_eps: 0.000000
 1562/5000: episode: 67, duration: 0.403s, episode steps:  28, steps per second:  69, episode reward: -35.220, mean reward: -1.258 [-32.500,  2.416], mean action: 4.893 [0.000, 20.000],  loss: 0.022403, mae: 0.318348, mean_q: 0.522441, mean_eps: 0.000000
 1597/5000: episode: 68, duration: 0.499s, episode steps:  35, steps per second:  70, episode reward: -33.000, mean reward: -0.943 [-32.408,  2.810], mean action: 5.086 [0.000, 20.000],  loss: 0.019926, mae: 0.309013, mean_q: 0.532557, mean_eps: 0.000000
 1617/5000: episode: 69, duration: 0.298s, episode steps:  20, steps per second:  67, episode reward: 33.000, mean reward:  1.650 [-2.646, 32.060], mean action: 6.100 [0.000, 15.000],  loss: 0.022442, mae: 0.320595, mean_q: 0.492565, mean_eps: 0.000000
 1639/5000: episode: 70, duration: 0.336s, episode steps:  22, steps per second:  65, episode reward: 43.518, mean reward:  1.978 [-2.125, 32.042], mean action: 4.182 [1.000, 15.000],  loss: 0.021496, mae: 0.316657, mean_q: 0.522530, mean_eps: 0.000000
 1664/5000: episode: 71, duration: 0.371s, episode steps:  25, steps per second:  67, episode reward: 39.000, mean reward:  1.560 [-2.177, 32.610], mean action: 6.000 [0.000, 15.000],  loss: 0.022091, mae: 0.319546, mean_q: 0.577859, mean_eps: 0.000000
 1698/5000: episode: 72, duration: 0.495s, episode steps:  34, steps per second:  69, episode reward: 40.213, mean reward:  1.183 [-2.530, 32.430], mean action: 3.971 [0.000, 18.000],  loss: 0.023583, mae: 0.328846, mean_q: 0.615426, mean_eps: 0.000000
 1713/5000: episode: 73, duration: 0.245s, episode steps:  15, steps per second:  61, episode reward: 41.717, mean reward:  2.781 [-2.807, 32.007], mean action: 3.267 [1.000, 15.000],  loss: 0.024707, mae: 0.341634, mean_q: 0.636811, mean_eps: 0.000000
 1739/5000: episode: 74, duration: 0.381s, episode steps:  26, steps per second:  68, episode reward: 35.241, mean reward:  1.355 [-2.553, 33.000], mean action: 5.500 [0.000, 20.000],  loss: 0.019275, mae: 0.316426, mean_q: 0.627582, mean_eps: 0.000000
 1762/5000: episode: 75, duration: 0.326s, episode steps:  23, steps per second:  71, episode reward: -35.200, mean reward: -1.530 [-31.684,  2.630], mean action: 7.957 [0.000, 20.000],  loss: 0.020130, mae: 0.332719, mean_q: 0.588519, mean_eps: 0.000000
 1805/5000: episode: 76, duration: 0.641s, episode steps:  43, steps per second:  67, episode reward: 38.140, mean reward:  0.887 [-2.639, 32.060], mean action: 2.977 [0.000, 20.000],  loss: 0.022275, mae: 0.331602, mean_q: 0.581435, mean_eps: 0.000000
 1822/5000: episode: 77, duration: 0.281s, episode steps:  17, steps per second:  60, episode reward: 35.868, mean reward:  2.110 [-2.903, 32.418], mean action: 4.059 [0.000, 11.000],  loss: 0.019261, mae: 0.310144, mean_q: 0.561633, mean_eps: 0.000000
 1849/5000: episode: 78, duration: 0.406s, episode steps:  27, steps per second:  67, episode reward: -32.200, mean reward: -1.193 [-32.037,  2.438], mean action: 7.630 [0.000, 19.000],  loss: 0.016722, mae: 0.293289, mean_q: 0.500758, mean_eps: 0.000000
 1866/5000: episode: 79, duration: 0.267s, episode steps:  17, steps per second:  64, episode reward: 38.042, mean reward:  2.238 [-2.405, 33.000], mean action: 4.882 [0.000, 21.000],  loss: 0.020999, mae: 0.311054, mean_q: 0.492070, mean_eps: 0.000000
 1889/5000: episode: 80, duration: 0.336s, episode steps:  23, steps per second:  68, episode reward: 35.712, mean reward:  1.553 [-2.441, 31.889], mean action: 4.783 [0.000, 16.000],  loss: 0.017055, mae: 0.295003, mean_q: 0.512748, mean_eps: 0.000000
 1922/5000: episode: 81, duration: 0.503s, episode steps:  33, steps per second:  66, episode reward: 35.718, mean reward:  1.082 [-2.486, 32.360], mean action: 3.879 [0.000, 16.000],  loss: 0.023146, mae: 0.327786, mean_q: 0.552224, mean_eps: 0.000000
 1948/5000: episode: 82, duration: 0.380s, episode steps:  26, steps per second:  68, episode reward: 35.887, mean reward:  1.380 [-2.277, 32.427], mean action: 3.654 [0.000, 16.000],  loss: 0.029172, mae: 0.360205, mean_q: 0.579651, mean_eps: 0.000000
 1967/5000: episode: 83, duration: 0.284s, episode steps:  19, steps per second:  67, episode reward: 41.926, mean reward:  2.207 [-2.182, 32.926], mean action: 3.474 [0.000, 19.000],  loss: 0.022213, mae: 0.326931, mean_q: 0.596922, mean_eps: 0.000000
 1983/5000: episode: 84, duration: 0.244s, episode steps:  16, steps per second:  66, episode reward: 38.631, mean reward:  2.414 [-2.507, 32.670], mean action: 3.438 [0.000, 12.000],  loss: 0.026630, mae: 0.353601, mean_q: 0.576948, mean_eps: 0.000000
 2007/5000: episode: 85, duration: 0.349s, episode steps:  24, steps per second:  69, episode reward: 35.037, mean reward:  1.460 [-3.000, 32.151], mean action: 5.042 [0.000, 20.000],  loss: 0.020111, mae: 0.317641, mean_q: 0.546912, mean_eps: 0.000000
 2037/5000: episode: 86, duration: 0.479s, episode steps:  30, steps per second:  63, episode reward: 35.152, mean reward:  1.172 [-3.000, 31.867], mean action: 6.267 [0.000, 14.000],  loss: 0.019715, mae: 0.312133, mean_q: 0.517260, mean_eps: 0.000000
 2063/5000: episode: 87, duration: 0.396s, episode steps:  26, steps per second:  66, episode reward: 35.466, mean reward:  1.364 [-3.000, 31.776], mean action: 1.962 [0.000, 12.000],  loss: 0.017164, mae: 0.295161, mean_q: 0.518960, mean_eps: 0.000000
 2082/5000: episode: 88, duration: 0.281s, episode steps:  19, steps per second:  68, episode reward: 38.075, mean reward:  2.004 [-2.331, 32.044], mean action: 4.842 [0.000, 19.000],  loss: 0.023707, mae: 0.325954, mean_q: 0.543417, mean_eps: 0.000000
 2132/5000: episode: 89, duration: 0.700s, episode steps:  50, steps per second:  71, episode reward: 32.018, mean reward:  0.640 [-3.000, 32.510], mean action: 9.240 [0.000, 19.000],  loss: 0.020605, mae: 0.306874, mean_q: 0.523358, mean_eps: 0.000000
 2160/5000: episode: 90, duration: 0.396s, episode steps:  28, steps per second:  71, episode reward: -32.370, mean reward: -1.156 [-32.150,  3.000], mean action: 4.893 [0.000, 19.000],  loss: 0.020328, mae: 0.310933, mean_q: 0.498151, mean_eps: 0.000000
 2175/5000: episode: 91, duration: 0.223s, episode steps:  15, steps per second:  67, episode reward: -41.040, mean reward: -2.736 [-32.253,  2.370], mean action: 9.133 [0.000, 18.000],  loss: 0.021587, mae: 0.310790, mean_q: 0.504580, mean_eps: 0.000000
 2197/5000: episode: 92, duration: 0.331s, episode steps:  22, steps per second:  66, episode reward: -37.720, mean reward: -1.715 [-31.781,  2.761], mean action: 6.409 [0.000, 16.000],  loss: 0.024529, mae: 0.326068, mean_q: 0.536481, mean_eps: 0.000000
 2218/5000: episode: 93, duration: 0.307s, episode steps:  21, steps per second:  68, episode reward: 36.000, mean reward:  1.714 [-2.332, 30.319], mean action: 3.619 [0.000, 15.000],  loss: 0.023178, mae: 0.318207, mean_q: 0.498305, mean_eps: 0.000000
 2237/5000: episode: 94, duration: 0.290s, episode steps:  19, steps per second:  65, episode reward: 36.000, mean reward:  1.895 [-2.619, 33.011], mean action: 6.000 [0.000, 20.000],  loss: 0.020419, mae: 0.310837, mean_q: 0.544449, mean_eps: 0.000000
 2267/5000: episode: 95, duration: 0.433s, episode steps:  30, steps per second:  69, episode reward: 38.884, mean reward:  1.296 [-2.846, 32.164], mean action: 3.033 [1.000, 11.000],  loss: 0.020686, mae: 0.314329, mean_q: 0.585386, mean_eps: 0.000000
 2280/5000: episode: 96, duration: 0.203s, episode steps:  13, steps per second:  64, episode reward: 44.295, mean reward:  3.407 [-2.227, 32.280], mean action: 2.308 [0.000, 11.000],  loss: 0.020390, mae: 0.314730, mean_q: 0.551847, mean_eps: 0.000000
 2304/5000: episode: 97, duration: 0.342s, episode steps:  24, steps per second:  70, episode reward: -30.000, mean reward: -1.250 [-29.811,  3.000], mean action: 6.042 [0.000, 18.000],  loss: 0.020268, mae: 0.313451, mean_q: 0.477180, mean_eps: 0.000000
 2325/5000: episode: 98, duration: 0.351s, episode steps:  21, steps per second:  60, episode reward: 36.000, mean reward:  1.714 [-2.657, 33.000], mean action: 4.810 [0.000, 19.000],  loss: 0.020813, mae: 0.312470, mean_q: 0.511926, mean_eps: 0.000000
 2348/5000: episode: 99, duration: 0.341s, episode steps:  23, steps per second:  67, episode reward: 37.891, mean reward:  1.647 [-2.363, 32.449], mean action: 4.870 [0.000, 19.000],  loss: 0.017666, mae: 0.297477, mean_q: 0.523483, mean_eps: 0.000000
 2365/5000: episode: 100, duration: 0.267s, episode steps:  17, steps per second:  64, episode reward: 38.538, mean reward:  2.267 [-2.476, 31.879], mean action: 4.824 [0.000, 19.000],  loss: 0.018580, mae: 0.302904, mean_q: 0.538290, mean_eps: 0.000000
 2387/5000: episode: 101, duration: 0.334s, episode steps:  22, steps per second:  66, episode reward: 33.000, mean reward:  1.500 [-2.628, 32.430], mean action: 4.773 [0.000, 19.000],  loss: 0.018747, mae: 0.297445, mean_q: 0.492187, mean_eps: 0.000000
 2413/5000: episode: 102, duration: 0.371s, episode steps:  26, steps per second:  70, episode reward: 32.666, mean reward:  1.256 [-2.317, 32.256], mean action: 4.308 [0.000, 19.000],  loss: 0.023789, mae: 0.324253, mean_q: 0.533082, mean_eps: 0.000000
 2435/5000: episode: 103, duration: 0.319s, episode steps:  22, steps per second:  69, episode reward: 38.632, mean reward:  1.756 [-2.777, 33.000], mean action: 3.864 [0.000, 19.000],  loss: 0.020829, mae: 0.309453, mean_q: 0.463577, mean_eps: 0.000000
 2453/5000: episode: 104, duration: 0.274s, episode steps:  18, steps per second:  66, episode reward: 32.738, mean reward:  1.819 [-3.000, 32.738], mean action: 6.667 [0.000, 19.000],  loss: 0.018929, mae: 0.297316, mean_q: 0.483522, mean_eps: 0.000000
 2481/5000: episode: 105, duration: 0.427s, episode steps:  28, steps per second:  66, episode reward: 35.694, mean reward:  1.275 [-2.902, 32.163], mean action: 5.036 [0.000, 19.000],  loss: 0.018615, mae: 0.293663, mean_q: 0.497604, mean_eps: 0.000000
 2503/5000: episode: 106, duration: 0.331s, episode steps:  22, steps per second:  67, episode reward: 35.627, mean reward:  1.619 [-2.380, 32.097], mean action: 4.000 [0.000, 19.000],  loss: 0.023454, mae: 0.315586, mean_q: 0.469080, mean_eps: 0.000000
 2523/5000: episode: 107, duration: 0.307s, episode steps:  20, steps per second:  65, episode reward: 41.179, mean reward:  2.059 [-3.000, 32.108], mean action: 3.500 [0.000, 19.000],  loss: 0.017673, mae: 0.303307, mean_q: 0.549761, mean_eps: 0.000000
 2540/5000: episode: 108, duration: 0.263s, episode steps:  17, steps per second:  65, episode reward: 38.756, mean reward:  2.280 [-2.487, 32.756], mean action: 3.882 [0.000, 19.000],  loss: 0.019368, mae: 0.306379, mean_q: 0.527460, mean_eps: 0.000000
 2615/5000: episode: 109, duration: 1.045s, episode steps:  75, steps per second:  72, episode reward: 32.020, mean reward:  0.427 [-2.757, 32.167], mean action: 6.707 [0.000, 19.000],  loss: 0.018632, mae: 0.301370, mean_q: 0.534186, mean_eps: 0.000000
 2633/5000: episode: 110, duration: 0.275s, episode steps:  18, steps per second:  65, episode reward: 35.410, mean reward:  1.967 [-3.000, 32.292], mean action: 4.833 [0.000, 12.000],  loss: 0.023928, mae: 0.322013, mean_q: 0.526250, mean_eps: 0.000000
 2660/5000: episode: 111, duration: 0.400s, episode steps:  27, steps per second:  68, episode reward: -35.030, mean reward: -1.297 [-32.059,  2.910], mean action: 5.852 [0.000, 18.000],  loss: 0.021622, mae: 0.322045, mean_q: 0.521976, mean_eps: 0.000000
 2682/5000: episode: 112, duration: 0.344s, episode steps:  22, steps per second:  64, episode reward: 32.532, mean reward:  1.479 [-2.901, 32.062], mean action: 4.273 [0.000, 12.000],  loss: 0.020962, mae: 0.319026, mean_q: 0.514057, mean_eps: 0.000000
 2708/5000: episode: 113, duration: 0.360s, episode steps:  26, steps per second:  72, episode reward: -41.490, mean reward: -1.596 [-32.185,  2.763], mean action: 9.423 [0.000, 20.000],  loss: 0.020167, mae: 0.320863, mean_q: 0.543456, mean_eps: 0.000000
 2732/5000: episode: 114, duration: 0.364s, episode steps:  24, steps per second:  66, episode reward: 37.517, mean reward:  1.563 [-3.000, 32.380], mean action: 7.583 [0.000, 19.000],  loss: 0.022520, mae: 0.328591, mean_q: 0.564303, mean_eps: 0.000000
 2755/5000: episode: 115, duration: 0.339s, episode steps:  23, steps per second:  68, episode reward: 32.798, mean reward:  1.426 [-3.000, 31.828], mean action: 5.913 [0.000, 19.000],  loss: 0.022580, mae: 0.328082, mean_q: 0.589888, mean_eps: 0.000000
 2787/5000: episode: 116, duration: 0.463s, episode steps:  32, steps per second:  69, episode reward: -35.280, mean reward: -1.103 [-32.221,  2.140], mean action: 5.156 [0.000, 15.000],  loss: 0.019862, mae: 0.315046, mean_q: 0.591911, mean_eps: 0.000000
 2810/5000: episode: 117, duration: 0.336s, episode steps:  23, steps per second:  68, episode reward: -36.000, mean reward: -1.565 [-32.653,  2.653], mean action: 4.348 [0.000, 19.000],  loss: 0.021643, mae: 0.325715, mean_q: 0.555962, mean_eps: 0.000000
 2846/5000: episode: 118, duration: 0.504s, episode steps:  36, steps per second:  71, episode reward: -33.000, mean reward: -0.917 [-32.202,  2.480], mean action: 7.500 [0.000, 15.000],  loss: 0.019453, mae: 0.314325, mean_q: 0.548731, mean_eps: 0.000000
 2870/5000: episode: 119, duration: 0.346s, episode steps:  24, steps per second:  69, episode reward: 35.432, mean reward:  1.476 [-2.303, 33.000], mean action: 4.833 [0.000, 19.000],  loss: 0.020999, mae: 0.318382, mean_q: 0.564951, mean_eps: 0.000000
 2904/5000: episode: 120, duration: 0.489s, episode steps:  34, steps per second:  69, episode reward: 32.577, mean reward:  0.958 [-3.000, 31.827], mean action: 6.265 [0.000, 20.000],  loss: 0.017065, mae: 0.299093, mean_q: 0.487686, mean_eps: 0.000000
 2927/5000: episode: 121, duration: 0.358s, episode steps:  23, steps per second:  64, episode reward: 38.009, mean reward:  1.653 [-2.383, 32.560], mean action: 5.826 [0.000, 19.000],  loss: 0.018422, mae: 0.303579, mean_q: 0.509894, mean_eps: 0.000000
 2951/5000: episode: 122, duration: 0.350s, episode steps:  24, steps per second:  68, episode reward: 35.161, mean reward:  1.465 [-2.462, 30.294], mean action: 7.250 [0.000, 20.000],  loss: 0.021511, mae: 0.323969, mean_q: 0.579946, mean_eps: 0.000000
 2973/5000: episode: 123, duration: 0.313s, episode steps:  22, steps per second:  70, episode reward: 35.798, mean reward:  1.627 [-2.508, 32.080], mean action: 3.227 [0.000, 16.000],  loss: 0.017150, mae: 0.302046, mean_q: 0.546426, mean_eps: 0.000000
 2987/5000: episode: 124, duration: 0.220s, episode steps:  14, steps per second:  64, episode reward: 44.272, mean reward:  3.162 [-2.141, 32.370], mean action: 3.071 [1.000, 16.000],  loss: 0.020109, mae: 0.314692, mean_q: 0.566668, mean_eps: 0.000000
 3011/5000: episode: 125, duration: 0.365s, episode steps:  24, steps per second:  66, episode reward: 33.000, mean reward:  1.375 [-2.296, 32.300], mean action: 4.333 [0.000, 15.000],  loss: 0.020235, mae: 0.313394, mean_q: 0.552717, mean_eps: 0.000000
 3028/5000: episode: 126, duration: 0.259s, episode steps:  17, steps per second:  66, episode reward: 38.099, mean reward:  2.241 [-3.000, 32.151], mean action: 3.235 [0.000, 16.000],  loss: 0.016219, mae: 0.299095, mean_q: 0.499695, mean_eps: 0.000000
 3049/5000: episode: 127, duration: 0.314s, episode steps:  21, steps per second:  67, episode reward: 38.285, mean reward:  1.823 [-2.455, 32.070], mean action: 4.000 [1.000, 16.000],  loss: 0.020720, mae: 0.313821, mean_q: 0.498463, mean_eps: 0.000000
 3069/5000: episode: 128, duration: 0.311s, episode steps:  20, steps per second:  64, episode reward: 41.581, mean reward:  2.079 [-2.234, 31.947], mean action: 1.900 [0.000, 18.000],  loss: 0.020049, mae: 0.309225, mean_q: 0.547993, mean_eps: 0.000000
 3095/5000: episode: 129, duration: 0.378s, episode steps:  26, steps per second:  69, episode reward: 35.141, mean reward:  1.352 [-2.321, 32.050], mean action: 5.769 [0.000, 16.000],  loss: 0.022641, mae: 0.326478, mean_q: 0.545617, mean_eps: 0.000000
 3118/5000: episode: 130, duration: 0.330s, episode steps:  23, steps per second:  70, episode reward: 32.478, mean reward:  1.412 [-3.000, 31.926], mean action: 5.261 [0.000, 16.000],  loss: 0.021953, mae: 0.320628, mean_q: 0.545386, mean_eps: 0.000000
 3140/5000: episode: 131, duration: 0.330s, episode steps:  22, steps per second:  67, episode reward: 33.000, mean reward:  1.500 [-3.000, 32.350], mean action: 5.364 [0.000, 19.000],  loss: 0.020802, mae: 0.320324, mean_q: 0.511525, mean_eps: 0.000000
 3178/5000: episode: 132, duration: 0.565s, episode steps:  38, steps per second:  67, episode reward: -32.190, mean reward: -0.847 [-31.941,  2.942], mean action: 9.789 [0.000, 19.000],  loss: 0.021623, mae: 0.320843, mean_q: 0.490975, mean_eps: 0.000000
 3200/5000: episode: 133, duration: 0.503s, episode steps:  22, steps per second:  44, episode reward: 41.040, mean reward:  1.865 [-2.463, 32.903], mean action: 4.227 [0.000, 14.000],  loss: 0.021074, mae: 0.309874, mean_q: 0.519617, mean_eps: 0.000000
 3222/5000: episode: 134, duration: 0.465s, episode steps:  22, steps per second:  47, episode reward: 37.789, mean reward:  1.718 [-2.401, 33.000], mean action: 3.955 [0.000, 19.000],  loss: 0.020706, mae: 0.316466, mean_q: 0.478399, mean_eps: 0.000000
 3242/5000: episode: 135, duration: 0.312s, episode steps:  20, steps per second:  64, episode reward: 41.138, mean reward:  2.057 [-2.528, 31.861], mean action: 3.650 [0.000, 19.000],  loss: 0.018590, mae: 0.296979, mean_q: 0.471425, mean_eps: 0.000000
 3260/5000: episode: 136, duration: 0.302s, episode steps:  18, steps per second:  60, episode reward: 38.653, mean reward:  2.147 [-2.514, 32.170], mean action: 4.222 [0.000, 19.000],  loss: 0.023661, mae: 0.330431, mean_q: 0.454190, mean_eps: 0.000000
 3290/5000: episode: 137, duration: 0.537s, episode steps:  30, steps per second:  56, episode reward: -35.710, mean reward: -1.190 [-31.913,  2.760], mean action: 5.667 [0.000, 16.000],  loss: 0.021790, mae: 0.313356, mean_q: 0.521164, mean_eps: 0.000000
 3311/5000: episode: 138, duration: 0.336s, episode steps:  21, steps per second:  63, episode reward: -36.000, mean reward: -1.714 [-30.000,  2.211], mean action: 5.905 [0.000, 20.000],  loss: 0.020822, mae: 0.318146, mean_q: 0.572105, mean_eps: 0.000000
 3335/5000: episode: 139, duration: 0.369s, episode steps:  24, steps per second:  65, episode reward: 36.000, mean reward:  1.500 [-3.000, 32.030], mean action: 4.625 [0.000, 15.000],  loss: 0.019171, mae: 0.310528, mean_q: 0.500451, mean_eps: 0.000000
 3358/5000: episode: 140, duration: 0.354s, episode steps:  23, steps per second:  65, episode reward: 38.799, mean reward:  1.687 [-2.583, 32.159], mean action: 6.261 [0.000, 20.000],  loss: 0.024813, mae: 0.330166, mean_q: 0.514061, mean_eps: 0.000000
 3372/5000: episode: 141, duration: 0.228s, episode steps:  14, steps per second:  61, episode reward: 41.246, mean reward:  2.946 [-2.314, 32.305], mean action: 4.357 [2.000, 15.000],  loss: 0.024323, mae: 0.335036, mean_q: 0.483124, mean_eps: 0.000000
 3390/5000: episode: 142, duration: 0.290s, episode steps:  18, steps per second:  62, episode reward: 38.698, mean reward:  2.150 [-2.344, 33.000], mean action: 5.222 [0.000, 20.000],  loss: 0.017532, mae: 0.298711, mean_q: 0.482147, mean_eps: 0.000000
 3412/5000: episode: 143, duration: 0.346s, episode steps:  22, steps per second:  64, episode reward: 37.998, mean reward:  1.727 [-2.843, 33.000], mean action: 7.773 [0.000, 19.000],  loss: 0.023528, mae: 0.329060, mean_q: 0.469244, mean_eps: 0.000000
 3445/5000: episode: 144, duration: 0.500s, episode steps:  33, steps per second:  66, episode reward: 32.244, mean reward:  0.977 [-2.399, 32.390], mean action: 4.000 [0.000, 19.000],  loss: 0.018216, mae: 0.304513, mean_q: 0.458626, mean_eps: 0.000000
 3472/5000: episode: 145, duration: 0.404s, episode steps:  27, steps per second:  67, episode reward: 38.769, mean reward:  1.436 [-2.105, 32.769], mean action: 2.815 [0.000, 15.000],  loss: 0.020501, mae: 0.318243, mean_q: 0.535088, mean_eps: 0.000000
 3488/5000: episode: 146, duration: 0.248s, episode steps:  16, steps per second:  65, episode reward: 38.325, mean reward:  2.395 [-2.237, 31.605], mean action: 3.438 [0.000, 15.000],  loss: 0.017788, mae: 0.309565, mean_q: 0.585526, mean_eps: 0.000000
 3504/5000: episode: 147, duration: 0.256s, episode steps:  16, steps per second:  63, episode reward: 36.000, mean reward:  2.250 [-2.444, 32.110], mean action: 5.188 [1.000, 19.000],  loss: 0.021920, mae: 0.330300, mean_q: 0.584760, mean_eps: 0.000000
 3522/5000: episode: 148, duration: 0.275s, episode steps:  18, steps per second:  65, episode reward: 35.271, mean reward:  1.960 [-3.000, 32.501], mean action: 5.222 [0.000, 19.000],  loss: 0.019705, mae: 0.316852, mean_q: 0.589932, mean_eps: 0.000000
 3555/5000: episode: 149, duration: 0.482s, episode steps:  33, steps per second:  69, episode reward: -35.740, mean reward: -1.083 [-32.081,  2.331], mean action: 4.667 [0.000, 18.000],  loss: 0.021443, mae: 0.325693, mean_q: 0.492953, mean_eps: 0.000000
 3567/5000: episode: 150, duration: 0.204s, episode steps:  12, steps per second:  59, episode reward: 44.196, mean reward:  3.683 [-2.388, 32.630], mean action: 2.583 [0.000, 16.000],  loss: 0.018507, mae: 0.314820, mean_q: 0.593602, mean_eps: 0.000000
 3587/5000: episode: 151, duration: 0.311s, episode steps:  20, steps per second:  64, episode reward: 35.089, mean reward:  1.754 [-3.000, 32.310], mean action: 4.550 [0.000, 16.000],  loss: 0.025738, mae: 0.342345, mean_q: 0.602373, mean_eps: 0.000000
 3611/5000: episode: 152, duration: 0.354s, episode steps:  24, steps per second:  68, episode reward: 41.052, mean reward:  1.710 [-2.331, 33.000], mean action: 3.125 [0.000, 19.000],  loss: 0.019531, mae: 0.320174, mean_q: 0.554845, mean_eps: 0.000000
 3637/5000: episode: 153, duration: 0.392s, episode steps:  26, steps per second:  66, episode reward: 32.521, mean reward:  1.251 [-2.645, 31.541], mean action: 7.385 [0.000, 21.000],  loss: 0.020227, mae: 0.328377, mean_q: 0.568232, mean_eps: 0.000000
 3653/5000: episode: 154, duration: 0.244s, episode steps:  16, steps per second:  65, episode reward: 36.000, mean reward:  2.250 [-3.000, 30.421], mean action: 4.500 [0.000, 19.000],  loss: 0.024625, mae: 0.351961, mean_q: 0.611641, mean_eps: 0.000000
 3681/5000: episode: 155, duration: 0.417s, episode steps:  28, steps per second:  67, episode reward: -35.510, mean reward: -1.268 [-32.159,  2.520], mean action: 6.393 [0.000, 19.000],  loss: 0.020592, mae: 0.326635, mean_q: 0.606590, mean_eps: 0.000000
 3697/5000: episode: 156, duration: 0.245s, episode steps:  16, steps per second:  65, episode reward: 41.639, mean reward:  2.602 [-2.065, 31.799], mean action: 4.062 [0.000, 16.000],  loss: 0.017827, mae: 0.314902, mean_q: 0.619910, mean_eps: 0.000000
 3723/5000: episode: 157, duration: 0.370s, episode steps:  26, steps per second:  70, episode reward: 32.421, mean reward:  1.247 [-3.000, 32.180], mean action: 8.038 [0.000, 19.000],  loss: 0.021645, mae: 0.338648, mean_q: 0.583863, mean_eps: 0.000000
 3766/5000: episode: 158, duration: 0.608s, episode steps:  43, steps per second:  71, episode reward: -35.380, mean reward: -0.823 [-32.591,  2.380], mean action: 10.209 [1.000, 20.000],  loss: 0.020330, mae: 0.323228, mean_q: 0.460598, mean_eps: 0.000000
 3795/5000: episode: 159, duration: 0.419s, episode steps:  29, steps per second:  69, episode reward: 32.615, mean reward:  1.125 [-2.397, 31.843], mean action: 4.862 [0.000, 20.000],  loss: 0.019621, mae: 0.311230, mean_q: 0.513922, mean_eps: 0.000000
 3824/5000: episode: 160, duration: 0.422s, episode steps:  29, steps per second:  69, episode reward: -35.160, mean reward: -1.212 [-32.075,  2.390], mean action: 6.310 [0.000, 15.000],  loss: 0.020478, mae: 0.321092, mean_q: 0.502417, mean_eps: 0.000000
 3850/5000: episode: 161, duration: 0.376s, episode steps:  26, steps per second:  69, episode reward: 32.109, mean reward:  1.235 [-2.464, 32.180], mean action: 4.038 [0.000, 15.000],  loss: 0.016920, mae: 0.302837, mean_q: 0.504146, mean_eps: 0.000000
 3889/5000: episode: 162, duration: 0.546s, episode steps:  39, steps per second:  71, episode reward: -36.000, mean reward: -0.923 [-32.089,  2.685], mean action: 3.795 [0.000, 16.000],  loss: 0.020655, mae: 0.323213, mean_q: 0.565185, mean_eps: 0.000000
 3913/5000: episode: 163, duration: 0.361s, episode steps:  24, steps per second:  67, episode reward: 40.877, mean reward:  1.703 [-2.365, 32.443], mean action: 5.208 [0.000, 16.000],  loss: 0.021417, mae: 0.325277, mean_q: 0.509409, mean_eps: 0.000000
 3942/5000: episode: 164, duration: 0.417s, episode steps:  29, steps per second:  70, episode reward: 41.180, mean reward:  1.420 [-2.300, 31.718], mean action: 2.310 [1.000, 16.000],  loss: 0.023944, mae: 0.341381, mean_q: 0.552936, mean_eps: 0.000000
 3972/5000: episode: 165, duration: 0.434s, episode steps:  30, steps per second:  69, episode reward: 32.827, mean reward:  1.094 [-3.000, 32.160], mean action: 8.133 [0.000, 20.000],  loss: 0.019510, mae: 0.319098, mean_q: 0.540175, mean_eps: 0.000000
 3998/5000: episode: 166, duration: 0.376s, episode steps:  26, steps per second:  69, episode reward: 35.669, mean reward:  1.372 [-2.901, 32.460], mean action: 4.885 [0.000, 19.000],  loss: 0.020566, mae: 0.329231, mean_q: 0.540489, mean_eps: 0.000000
 4025/5000: episode: 167, duration: 0.391s, episode steps:  27, steps per second:  69, episode reward: 36.000, mean reward:  1.333 [-2.429, 32.180], mean action: 5.778 [1.000, 19.000],  loss: 0.019186, mae: 0.314012, mean_q: 0.551009, mean_eps: 0.000000
 4046/5000: episode: 168, duration: 0.319s, episode steps:  21, steps per second:  66, episode reward: 41.415, mean reward:  1.972 [-2.590, 32.345], mean action: 3.381 [0.000, 14.000],  loss: 0.021893, mae: 0.335828, mean_q: 0.557892, mean_eps: 0.000000
 4065/5000: episode: 169, duration: 0.280s, episode steps:  19, steps per second:  68, episode reward: 38.393, mean reward:  2.021 [-2.498, 33.000], mean action: 2.368 [0.000, 9.000],  loss: 0.023293, mae: 0.336055, mean_q: 0.541394, mean_eps: 0.000000
 4092/5000: episode: 170, duration: 0.403s, episode steps:  27, steps per second:  67, episode reward: 36.000, mean reward:  1.333 [-2.220, 32.560], mean action: 2.296 [0.000, 9.000],  loss: 0.022214, mae: 0.328813, mean_q: 0.550757, mean_eps: 0.000000
 4183/5000: episode: 171, duration: 1.285s, episode steps:  91, steps per second:  71, episode reward: 34.557, mean reward:  0.380 [-2.276, 32.430], mean action: 4.901 [0.000, 21.000],  loss: 0.020051, mae: 0.314377, mean_q: 0.590172, mean_eps: 0.000000
 4206/5000: episode: 172, duration: 0.330s, episode steps:  23, steps per second:  70, episode reward: 32.525, mean reward:  1.414 [-3.000, 32.025], mean action: 7.826 [0.000, 20.000],  loss: 0.022375, mae: 0.329448, mean_q: 0.509519, mean_eps: 0.000000
 4224/5000: episode: 173, duration: 0.286s, episode steps:  18, steps per second:  63, episode reward: 41.358, mean reward:  2.298 [-2.484, 31.967], mean action: 2.889 [0.000, 9.000],  loss: 0.019364, mae: 0.322876, mean_q: 0.520438, mean_eps: 0.000000
 4238/5000: episode: 174, duration: 0.219s, episode steps:  14, steps per second:  64, episode reward: 42.000, mean reward:  3.000 [-2.498, 32.950], mean action: 3.214 [0.000, 12.000],  loss: 0.021331, mae: 0.330799, mean_q: 0.508237, mean_eps: 0.000000
 4428/5000: episode: 175, duration: 2.429s, episode steps: 190, steps per second:  78, episode reward: 33.000, mean reward:  0.174 [-2.494, 32.580], mean action: 9.226 [0.000, 16.000],  loss: 0.021654, mae: 0.328414, mean_q: 0.516469, mean_eps: 0.000000
 4449/5000: episode: 176, duration: 0.306s, episode steps:  21, steps per second:  69, episode reward: 37.745, mean reward:  1.797 [-2.084, 32.303], mean action: 5.857 [0.000, 16.000],  loss: 0.022549, mae: 0.331274, mean_q: 0.480363, mean_eps: 0.000000
 4471/5000: episode: 177, duration: 0.316s, episode steps:  22, steps per second:  70, episode reward: -35.750, mean reward: -1.625 [-32.371,  2.480], mean action: 5.955 [0.000, 16.000],  loss: 0.021567, mae: 0.328658, mean_q: 0.511427, mean_eps: 0.000000
 4488/5000: episode: 178, duration: 0.268s, episode steps:  17, steps per second:  63, episode reward: 38.853, mean reward:  2.285 [-3.000, 32.853], mean action: 4.882 [0.000, 20.000],  loss: 0.015664, mae: 0.298488, mean_q: 0.520608, mean_eps: 0.000000
 4507/5000: episode: 179, duration: 0.274s, episode steps:  19, steps per second:  69, episode reward: 38.610, mean reward:  2.032 [-2.717, 32.550], mean action: 2.789 [0.000, 11.000],  loss: 0.019339, mae: 0.312245, mean_q: 0.519023, mean_eps: 0.000000
 4530/5000: episode: 180, duration: 0.337s, episode steps:  23, steps per second:  68, episode reward: 35.534, mean reward:  1.545 [-2.698, 31.921], mean action: 4.348 [0.000, 14.000],  loss: 0.019142, mae: 0.320931, mean_q: 0.566079, mean_eps: 0.000000
 4550/5000: episode: 181, duration: 0.298s, episode steps:  20, steps per second:  67, episode reward: 35.902, mean reward:  1.795 [-3.000, 32.902], mean action: 4.650 [0.000, 16.000],  loss: 0.021229, mae: 0.325791, mean_q: 0.539680, mean_eps: 0.000000
 4578/5000: episode: 182, duration: 0.411s, episode steps:  28, steps per second:  68, episode reward: 41.467, mean reward:  1.481 [-2.118, 32.020], mean action: 2.964 [1.000, 16.000],  loss: 0.023691, mae: 0.337806, mean_q: 0.525042, mean_eps: 0.000000
 4601/5000: episode: 183, duration: 0.343s, episode steps:  23, steps per second:  67, episode reward: 41.901, mean reward:  1.822 [-2.157, 32.061], mean action: 2.565 [0.000, 12.000],  loss: 0.022469, mae: 0.330866, mean_q: 0.551123, mean_eps: 0.000000
 4626/5000: episode: 184, duration: 0.357s, episode steps:  25, steps per second:  70, episode reward: 32.408, mean reward:  1.296 [-3.000, 32.080], mean action: 4.240 [0.000, 16.000],  loss: 0.019473, mae: 0.320587, mean_q: 0.551550, mean_eps: 0.000000
 4650/5000: episode: 185, duration: 0.356s, episode steps:  24, steps per second:  67, episode reward: 41.236, mean reward:  1.718 [-2.335, 32.180], mean action: 3.958 [0.000, 16.000],  loss: 0.018138, mae: 0.312504, mean_q: 0.540898, mean_eps: 0.000000
 4670/5000: episode: 186, duration: 0.293s, episode steps:  20, steps per second:  68, episode reward: 41.069, mean reward:  2.053 [-2.236, 31.821], mean action: 1.950 [0.000, 16.000],  loss: 0.019013, mae: 0.323073, mean_q: 0.526826, mean_eps: 0.000000
 4695/5000: episode: 187, duration: 0.361s, episode steps:  25, steps per second:  69, episode reward: 39.000, mean reward:  1.560 [-3.000, 32.180], mean action: 3.200 [0.000, 16.000],  loss: 0.020673, mae: 0.320998, mean_q: 0.544026, mean_eps: 0.000000
 4732/5000: episode: 188, duration: 0.522s, episode steps:  37, steps per second:  71, episode reward: -32.230, mean reward: -0.871 [-32.091,  2.592], mean action: 5.351 [0.000, 21.000],  loss: 0.021894, mae: 0.328798, mean_q: 0.562754, mean_eps: 0.000000
 4754/5000: episode: 189, duration: 0.332s, episode steps:  22, steps per second:  66, episode reward: 32.162, mean reward:  1.462 [-2.304, 31.472], mean action: 5.682 [0.000, 16.000],  loss: 0.022780, mae: 0.335774, mean_q: 0.572278, mean_eps: 0.000000
 4798/5000: episode: 190, duration: 0.619s, episode steps:  44, steps per second:  71, episode reward: 32.831, mean reward:  0.746 [-2.294, 32.330], mean action: 9.591 [0.000, 19.000],  loss: 0.020328, mae: 0.325550, mean_q: 0.510248, mean_eps: 0.000000
 4819/5000: episode: 191, duration: 0.324s, episode steps:  21, steps per second:  65, episode reward: 38.555, mean reward:  1.836 [-2.412, 33.000], mean action: 5.000 [0.000, 19.000],  loss: 0.026916, mae: 0.347128, mean_q: 0.492612, mean_eps: 0.000000
 4849/5000: episode: 192, duration: 0.449s, episode steps:  30, steps per second:  67, episode reward: 33.000, mean reward:  1.100 [-2.940, 32.170], mean action: 4.967 [0.000, 19.000],  loss: 0.019209, mae: 0.313966, mean_q: 0.560505, mean_eps: 0.000000
 4874/5000: episode: 193, duration: 0.358s, episode steps:  25, steps per second:  70, episode reward: 35.748, mean reward:  1.430 [-2.375, 32.058], mean action: 3.080 [0.000, 15.000],  loss: 0.022032, mae: 0.325721, mean_q: 0.614857, mean_eps: 0.000000
 4894/5000: episode: 194, duration: 0.310s, episode steps:  20, steps per second:  65, episode reward: -38.180, mean reward: -1.909 [-32.095,  3.000], mean action: 5.200 [0.000, 15.000],  loss: 0.018938, mae: 0.307676, mean_q: 0.569186, mean_eps: 0.000000
 4922/5000: episode: 195, duration: 0.409s, episode steps:  28, steps per second:  69, episode reward: 40.315, mean reward:  1.440 [-2.254, 32.210], mean action: 2.929 [0.000, 16.000],  loss: 0.019458, mae: 0.313838, mean_q: 0.503509, mean_eps: 0.000000
 4943/5000: episode: 196, duration: 0.319s, episode steps:  21, steps per second:  66, episode reward: -35.450, mean reward: -1.688 [-32.174,  2.639], mean action: 6.381 [0.000, 16.000],  loss: 0.021567, mae: 0.318139, mean_q: 0.545517, mean_eps: 0.000000
 4969/5000: episode: 197, duration: 0.380s, episode steps:  26, steps per second:  68, episode reward: 36.000, mean reward:  1.385 [-3.000, 32.280], mean action: 5.346 [0.000, 20.000],  loss: 0.023928, mae: 0.332055, mean_q: 0.566961, mean_eps: 0.000000
 4993/5000: episode: 198, duration: 0.348s, episode steps:  24, steps per second:  69, episode reward: 35.752, mean reward:  1.490 [-2.891, 32.122], mean action: 3.333 [0.000, 11.000],  loss: 0.021403, mae: 0.315495, mean_q: 0.485850, mean_eps: 0.000000
done, took 67.184 seconds
DQN Evaluation: 12622 victories out of 14702 episodes
Training for 5000 steps ...
   25/5000: episode: 1, duration: 0.237s, episode steps:  25, steps per second: 105, episode reward: 41.191, mean reward:  1.648 [-2.100, 32.130], mean action: 1.360 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   46/5000: episode: 2, duration: 0.172s, episode steps:  21, steps per second: 122, episode reward: 41.434, mean reward:  1.973 [-2.446, 31.624], mean action: 2.381 [1.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   75/5000: episode: 3, duration: 0.225s, episode steps:  29, steps per second: 129, episode reward: 40.698, mean reward:  1.403 [-2.271, 32.423], mean action: 4.690 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   98/5000: episode: 4, duration: 0.165s, episode steps:  23, steps per second: 139, episode reward: 43.723, mean reward:  1.901 [-2.389, 32.590], mean action: 3.696 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  159/5000: episode: 5, duration: 0.388s, episode steps:  61, steps per second: 157, episode reward: -32.900, mean reward: -0.539 [-32.960,  2.650], mean action: 2.984 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  188/5000: episode: 6, duration: 0.207s, episode steps:  29, steps per second: 140, episode reward: 40.834, mean reward:  1.408 [-2.275, 30.019], mean action: 2.586 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  214/5000: episode: 7, duration: 0.179s, episode steps:  26, steps per second: 145, episode reward: 36.000, mean reward:  1.385 [-3.000, 32.130], mean action: 2.731 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  246/5000: episode: 8, duration: 0.215s, episode steps:  32, steps per second: 149, episode reward: 38.720, mean reward:  1.210 [-2.715, 32.190], mean action: 2.406 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  275/5000: episode: 9, duration: 0.201s, episode steps:  29, steps per second: 144, episode reward: 38.802, mean reward:  1.338 [-3.000, 32.780], mean action: 3.621 [2.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  293/5000: episode: 10, duration: 0.142s, episode steps:  18, steps per second: 127, episode reward: 44.322, mean reward:  2.462 [-2.633, 32.100], mean action: 1.667 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  326/5000: episode: 11, duration: 0.216s, episode steps:  33, steps per second: 153, episode reward: 44.355, mean reward:  1.344 [-2.127, 32.270], mean action: 3.030 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  343/5000: episode: 12, duration: 0.130s, episode steps:  17, steps per second: 130, episode reward: 47.634, mean reward:  2.802 [-0.067, 32.440], mean action: 2.706 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  368/5000: episode: 13, duration: 0.177s, episode steps:  25, steps per second: 141, episode reward: 38.470, mean reward:  1.539 [-2.712, 32.230], mean action: 4.920 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  387/5000: episode: 14, duration: 0.153s, episode steps:  19, steps per second: 124, episode reward: 44.393, mean reward:  2.336 [-2.041, 32.143], mean action: 2.105 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  417/5000: episode: 15, duration: 0.192s, episode steps:  30, steps per second: 156, episode reward: 38.345, mean reward:  1.278 [-2.411, 32.040], mean action: 2.500 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  445/5000: episode: 16, duration: 0.194s, episode steps:  28, steps per second: 144, episode reward: 39.000, mean reward:  1.393 [-2.551, 32.130], mean action: 3.179 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  490/5000: episode: 17, duration: 0.295s, episode steps:  45, steps per second: 153, episode reward: 35.648, mean reward:  0.792 [-2.941, 32.113], mean action: 2.467 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  526/5000: episode: 18, duration: 0.234s, episode steps:  36, steps per second: 154, episode reward: 41.283, mean reward:  1.147 [-2.361, 32.050], mean action: 2.861 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  553/5000: episode: 19, duration: 0.182s, episode steps:  27, steps per second: 148, episode reward: 41.803, mean reward:  1.548 [-2.411, 32.341], mean action: 1.407 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  569/5000: episode: 20, duration: 0.126s, episode steps:  16, steps per second: 127, episode reward: 44.301, mean reward:  2.769 [-2.175, 32.185], mean action: 2.188 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  598/5000: episode: 21, duration: 0.198s, episode steps:  29, steps per second: 147, episode reward: 38.511, mean reward:  1.328 [-2.104, 32.070], mean action: 2.690 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  627/5000: episode: 22, duration: 0.189s, episode steps:  29, steps per second: 153, episode reward: 32.604, mean reward:  1.124 [-3.000, 31.754], mean action: 3.276 [1.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  658/5000: episode: 23, duration: 0.210s, episode steps:  31, steps per second: 148, episode reward: 47.314, mean reward:  1.526 [-0.333, 32.390], mean action: 2.355 [0.000, 7.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  688/5000: episode: 24, duration: 0.208s, episode steps:  30, steps per second: 144, episode reward: 41.766, mean reward:  1.392 [-2.683, 32.060], mean action: 2.400 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  705/5000: episode: 25, duration: 0.125s, episode steps:  17, steps per second: 136, episode reward: 41.644, mean reward:  2.450 [-2.153, 33.000], mean action: 2.412 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  729/5000: episode: 26, duration: 0.169s, episode steps:  24, steps per second: 142, episode reward: 35.684, mean reward:  1.487 [-3.000, 32.330], mean action: 3.667 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  740/5000: episode: 27, duration: 0.107s, episode steps:  11, steps per second: 102, episode reward: 48.000, mean reward:  4.364 [ 0.279, 32.470], mean action: 3.909 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  780/5000: episode: 28, duration: 0.262s, episode steps:  40, steps per second: 152, episode reward: 35.670, mean reward:  0.892 [-2.460, 32.140], mean action: 7.875 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  793/5000: episode: 29, duration: 0.102s, episode steps:  13, steps per second: 128, episode reward: 41.234, mean reward:  3.172 [-2.489, 32.710], mean action: 1.923 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  812/5000: episode: 30, duration: 0.143s, episode steps:  19, steps per second: 133, episode reward: 42.000, mean reward:  2.211 [-2.467, 33.254], mean action: 3.316 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  835/5000: episode: 31, duration: 0.173s, episode steps:  23, steps per second: 133, episode reward: 44.191, mean reward:  1.921 [-2.468, 31.725], mean action: 2.261 [0.000, 11.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  871/5000: episode: 32, duration: 0.237s, episode steps:  36, steps per second: 152, episode reward: 38.715, mean reward:  1.075 [-2.378, 32.200], mean action: 2.139 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  896/5000: episode: 33, duration: 0.176s, episode steps:  25, steps per second: 142, episode reward: 45.000, mean reward:  1.800 [-2.115, 32.440], mean action: 2.040 [1.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  917/5000: episode: 34, duration: 0.155s, episode steps:  21, steps per second: 136, episode reward: 41.371, mean reward:  1.970 [-2.602, 32.500], mean action: 3.571 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  953/5000: episode: 35, duration: 0.244s, episode steps:  36, steps per second: 148, episode reward: -32.230, mean reward: -0.895 [-32.048,  3.000], mean action: 10.722 [0.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  979/5000: episode: 36, duration: 0.176s, episode steps:  26, steps per second: 148, episode reward: 44.094, mean reward:  1.696 [-2.054, 32.130], mean action: 2.962 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1000/5000: episode: 37, duration: 0.147s, episode steps:  21, steps per second: 142, episode reward: 39.000, mean reward:  1.857 [-2.494, 32.760], mean action: 6.619 [1.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1026/5000: episode: 38, duration: 0.388s, episode steps:  26, steps per second:  67, episode reward: 38.417, mean reward:  1.478 [-2.566, 32.030], mean action: 5.154 [0.000, 21.000],  loss: 0.015851, mae: 0.289345, mean_q: 0.522906, mean_eps: 0.000000
 1057/5000: episode: 39, duration: 0.449s, episode steps:  31, steps per second:  69, episode reward: 35.047, mean reward:  1.131 [-3.000, 32.160], mean action: 3.710 [0.000, 19.000],  loss: 0.020472, mae: 0.309258, mean_q: 0.583700, mean_eps: 0.000000
 1074/5000: episode: 40, duration: 0.306s, episode steps:  17, steps per second:  56, episode reward: 44.830, mean reward:  2.637 [-3.000, 32.250], mean action: 2.706 [0.000, 13.000],  loss: 0.022645, mae: 0.325643, mean_q: 0.554027, mean_eps: 0.000000
 1100/5000: episode: 41, duration: 0.533s, episode steps:  26, steps per second:  49, episode reward: 38.861, mean reward:  1.495 [-2.904, 32.340], mean action: 2.462 [0.000, 11.000],  loss: 0.025732, mae: 0.332695, mean_q: 0.564786, mean_eps: 0.000000
 1125/5000: episode: 42, duration: 0.368s, episode steps:  25, steps per second:  68, episode reward: 36.000, mean reward:  1.440 [-2.875, 32.450], mean action: 2.840 [0.000, 11.000],  loss: 0.024380, mae: 0.331331, mean_q: 0.586485, mean_eps: 0.000000
 1170/5000: episode: 43, duration: 0.631s, episode steps:  45, steps per second:  71, episode reward: 44.227, mean reward:  0.983 [-2.311, 32.614], mean action: 1.911 [0.000, 12.000],  loss: 0.022148, mae: 0.326667, mean_q: 0.592993, mean_eps: 0.000000
 1219/5000: episode: 44, duration: 0.694s, episode steps:  49, steps per second:  71, episode reward: 44.303, mean reward:  0.904 [-2.201, 32.084], mean action: 2.408 [1.000, 16.000],  loss: 0.024246, mae: 0.339844, mean_q: 0.616797, mean_eps: 0.000000
 1232/5000: episode: 45, duration: 0.204s, episode steps:  13, steps per second:  64, episode reward: 44.739, mean reward:  3.441 [-2.409, 32.470], mean action: 2.692 [0.000, 15.000],  loss: 0.026174, mae: 0.341562, mean_q: 0.555400, mean_eps: 0.000000
 1257/5000: episode: 46, duration: 0.381s, episode steps:  25, steps per second:  66, episode reward: 39.000, mean reward:  1.560 [-2.282, 32.230], mean action: 2.920 [0.000, 15.000],  loss: 0.019243, mae: 0.313375, mean_q: 0.525632, mean_eps: 0.000000
 1279/5000: episode: 47, duration: 0.332s, episode steps:  22, steps per second:  66, episode reward: 41.206, mean reward:  1.873 [-2.459, 32.080], mean action: 3.227 [0.000, 15.000],  loss: 0.017644, mae: 0.301203, mean_q: 0.514746, mean_eps: 0.000000
 1293/5000: episode: 48, duration: 0.222s, episode steps:  14, steps per second:  63, episode reward: 44.008, mean reward:  3.143 [-2.132, 32.260], mean action: 2.286 [0.000, 15.000],  loss: 0.022594, mae: 0.325001, mean_q: 0.513841, mean_eps: 0.000000
 1330/5000: episode: 49, duration: 0.521s, episode steps:  37, steps per second:  71, episode reward: 43.844, mean reward:  1.185 [-2.298, 32.064], mean action: 2.162 [0.000, 15.000],  loss: 0.020940, mae: 0.323358, mean_q: 0.582138, mean_eps: 0.000000
 1360/5000: episode: 50, duration: 0.439s, episode steps:  30, steps per second:  68, episode reward: 35.277, mean reward:  1.176 [-3.000, 32.148], mean action: 3.033 [0.000, 16.000],  loss: 0.018940, mae: 0.312416, mean_q: 0.560476, mean_eps: 0.000000
 1392/5000: episode: 51, duration: 0.482s, episode steps:  32, steps per second:  66, episode reward: -32.060, mean reward: -1.002 [-32.065,  3.000], mean action: 6.438 [0.000, 20.000],  loss: 0.020160, mae: 0.324674, mean_q: 0.530178, mean_eps: 0.000000
 1416/5000: episode: 52, duration: 0.354s, episode steps:  24, steps per second:  68, episode reward: 35.207, mean reward:  1.467 [-3.000, 32.074], mean action: 5.833 [0.000, 15.000],  loss: 0.018929, mae: 0.317624, mean_q: 0.526440, mean_eps: 0.000000
 1433/5000: episode: 53, duration: 0.263s, episode steps:  17, steps per second:  65, episode reward: 43.534, mean reward:  2.561 [-2.614, 31.932], mean action: 2.588 [0.000, 13.000],  loss: 0.020889, mae: 0.330540, mean_q: 0.495532, mean_eps: 0.000000
 1474/5000: episode: 54, duration: 0.602s, episode steps:  41, steps per second:  68, episode reward: 42.895, mean reward:  1.046 [-2.522, 32.280], mean action: 4.024 [0.000, 14.000],  loss: 0.023723, mae: 0.328138, mean_q: 0.511839, mean_eps: 0.000000
 1494/5000: episode: 55, duration: 0.295s, episode steps:  20, steps per second:  68, episode reward: 39.000, mean reward:  1.950 [-2.373, 32.740], mean action: 4.550 [0.000, 13.000],  loss: 0.018122, mae: 0.303077, mean_q: 0.532877, mean_eps: 0.000000
 1522/5000: episode: 56, duration: 0.405s, episode steps:  28, steps per second:  69, episode reward: 41.576, mean reward:  1.485 [-2.097, 32.480], mean action: 2.643 [0.000, 11.000],  loss: 0.019763, mae: 0.312621, mean_q: 0.558320, mean_eps: 0.000000
 1550/5000: episode: 57, duration: 0.414s, episode steps:  28, steps per second:  68, episode reward: 46.627, mean reward:  1.665 [-0.542, 32.654], mean action: 2.429 [0.000, 12.000],  loss: 0.019298, mae: 0.316659, mean_q: 0.548862, mean_eps: 0.000000
 1566/5000: episode: 58, duration: 0.250s, episode steps:  16, steps per second:  64, episode reward: 41.605, mean reward:  2.600 [-2.817, 31.971], mean action: 2.562 [0.000, 11.000],  loss: 0.016993, mae: 0.298086, mean_q: 0.493537, mean_eps: 0.000000
 1601/5000: episode: 59, duration: 0.503s, episode steps:  35, steps per second:  70, episode reward: 41.533, mean reward:  1.187 [-2.149, 32.346], mean action: 2.229 [0.000, 14.000],  loss: 0.021583, mae: 0.316979, mean_q: 0.526158, mean_eps: 0.000000
 1625/5000: episode: 60, duration: 0.351s, episode steps:  24, steps per second:  68, episode reward: 38.017, mean reward:  1.584 [-3.000, 32.403], mean action: 6.208 [0.000, 21.000],  loss: 0.021837, mae: 0.316703, mean_q: 0.541549, mean_eps: 0.000000
 1654/5000: episode: 61, duration: 0.423s, episode steps:  29, steps per second:  69, episode reward: 38.808, mean reward:  1.338 [-2.381, 32.224], mean action: 2.690 [0.000, 11.000],  loss: 0.019243, mae: 0.305180, mean_q: 0.515657, mean_eps: 0.000000
 1690/5000: episode: 62, duration: 0.519s, episode steps:  36, steps per second:  69, episode reward: 44.350, mean reward:  1.232 [-2.394, 32.420], mean action: 3.944 [0.000, 13.000],  loss: 0.021049, mae: 0.312753, mean_q: 0.532793, mean_eps: 0.000000
 1711/5000: episode: 63, duration: 0.342s, episode steps:  21, steps per second:  61, episode reward: 44.135, mean reward:  2.102 [-2.214, 33.190], mean action: 3.048 [0.000, 14.000],  loss: 0.022695, mae: 0.323360, mean_q: 0.559927, mean_eps: 0.000000
 1731/5000: episode: 64, duration: 0.304s, episode steps:  20, steps per second:  66, episode reward: 44.535, mean reward:  2.227 [-2.363, 32.480], mean action: 2.350 [0.000, 12.000],  loss: 0.023298, mae: 0.328560, mean_q: 0.495376, mean_eps: 0.000000
 1764/5000: episode: 65, duration: 0.480s, episode steps:  33, steps per second:  69, episode reward: 41.424, mean reward:  1.255 [-2.404, 32.170], mean action: 4.152 [0.000, 16.000],  loss: 0.020183, mae: 0.314161, mean_q: 0.485334, mean_eps: 0.000000
 1806/5000: episode: 66, duration: 0.604s, episode steps:  42, steps per second:  70, episode reward: 38.414, mean reward:  0.915 [-2.231, 32.051], mean action: 4.429 [0.000, 20.000],  loss: 0.023797, mae: 0.324003, mean_q: 0.534359, mean_eps: 0.000000
 1850/5000: episode: 67, duration: 0.637s, episode steps:  44, steps per second:  69, episode reward: 42.000, mean reward:  0.955 [-2.859, 32.060], mean action: 2.227 [0.000, 16.000],  loss: 0.019544, mae: 0.303627, mean_q: 0.533048, mean_eps: 0.000000
 1887/5000: episode: 68, duration: 0.533s, episode steps:  37, steps per second:  69, episode reward: 41.724, mean reward:  1.128 [-2.685, 32.170], mean action: 3.027 [0.000, 16.000],  loss: 0.021102, mae: 0.315599, mean_q: 0.568684, mean_eps: 0.000000
 1905/5000: episode: 69, duration: 0.288s, episode steps:  18, steps per second:  62, episode reward: 44.752, mean reward:  2.486 [-2.514, 33.000], mean action: 2.111 [0.000, 12.000],  loss: 0.019717, mae: 0.297823, mean_q: 0.490947, mean_eps: 0.000000
 1930/5000: episode: 70, duration: 0.373s, episode steps:  25, steps per second:  67, episode reward: 41.089, mean reward:  1.644 [-2.880, 32.310], mean action: 2.840 [0.000, 19.000],  loss: 0.021842, mae: 0.313144, mean_q: 0.478275, mean_eps: 0.000000
 1960/5000: episode: 71, duration: 0.434s, episode steps:  30, steps per second:  69, episode reward: 41.186, mean reward:  1.373 [-2.363, 32.320], mean action: 2.933 [0.000, 19.000],  loss: 0.022805, mae: 0.319566, mean_q: 0.495220, mean_eps: 0.000000
 2002/5000: episode: 72, duration: 0.595s, episode steps:  42, steps per second:  71, episode reward: 37.284, mean reward:  0.888 [-2.880, 32.590], mean action: 4.190 [1.000, 19.000],  loss: 0.020048, mae: 0.303180, mean_q: 0.535184, mean_eps: 0.000000
 2030/5000: episode: 73, duration: 0.405s, episode steps:  28, steps per second:  69, episode reward: 38.835, mean reward:  1.387 [-2.148, 32.075], mean action: 3.429 [0.000, 19.000],  loss: 0.019259, mae: 0.299086, mean_q: 0.495082, mean_eps: 0.000000
 2050/5000: episode: 74, duration: 0.310s, episode steps:  20, steps per second:  65, episode reward: 42.000, mean reward:  2.100 [-2.472, 32.450], mean action: 4.050 [1.000, 19.000],  loss: 0.021325, mae: 0.300503, mean_q: 0.497002, mean_eps: 0.000000
 2072/5000: episode: 75, duration: 0.333s, episode steps:  22, steps per second:  66, episode reward: 41.599, mean reward:  1.891 [-2.030, 32.250], mean action: 6.636 [0.000, 19.000],  loss: 0.021158, mae: 0.316623, mean_q: 0.469642, mean_eps: 0.000000
 2100/5000: episode: 76, duration: 0.414s, episode steps:  28, steps per second:  68, episode reward: 41.805, mean reward:  1.493 [-2.281, 32.193], mean action: 2.286 [0.000, 12.000],  loss: 0.016774, mae: 0.295359, mean_q: 0.455239, mean_eps: 0.000000
 2152/5000: episode: 77, duration: 0.745s, episode steps:  52, steps per second:  70, episode reward: 32.878, mean reward:  0.632 [-2.188, 32.280], mean action: 3.769 [0.000, 14.000],  loss: 0.021888, mae: 0.311525, mean_q: 0.483487, mean_eps: 0.000000
 2188/5000: episode: 78, duration: 0.505s, episode steps:  36, steps per second:  71, episode reward: 41.407, mean reward:  1.150 [-2.256, 31.742], mean action: 4.500 [0.000, 20.000],  loss: 0.021122, mae: 0.308388, mean_q: 0.516058, mean_eps: 0.000000
 2259/5000: episode: 79, duration: 0.985s, episode steps:  71, steps per second:  72, episode reward: 32.936, mean reward:  0.464 [-3.000, 32.100], mean action: 12.324 [0.000, 21.000],  loss: 0.021245, mae: 0.309218, mean_q: 0.586480, mean_eps: 0.000000
 2281/5000: episode: 80, duration: 0.338s, episode steps:  22, steps per second:  65, episode reward: 36.000, mean reward:  1.636 [-2.564, 29.481], mean action: 3.455 [0.000, 16.000],  loss: 0.020092, mae: 0.303566, mean_q: 0.602830, mean_eps: 0.000000
 2309/5000: episode: 81, duration: 0.409s, episode steps:  28, steps per second:  68, episode reward: 41.074, mean reward:  1.467 [-2.243, 32.067], mean action: 2.679 [0.000, 14.000],  loss: 0.018909, mae: 0.300514, mean_q: 0.557915, mean_eps: 0.000000
 2332/5000: episode: 82, duration: 0.343s, episode steps:  23, steps per second:  67, episode reward: 44.394, mean reward:  1.930 [-2.101, 32.490], mean action: 2.261 [0.000, 11.000],  loss: 0.020084, mae: 0.309320, mean_q: 0.539066, mean_eps: 0.000000
 2364/5000: episode: 83, duration: 0.479s, episode steps:  32, steps per second:  67, episode reward: 40.612, mean reward:  1.269 [-2.341, 31.978], mean action: 2.812 [0.000, 12.000],  loss: 0.020628, mae: 0.305099, mean_q: 0.512876, mean_eps: 0.000000
 2394/5000: episode: 84, duration: 0.444s, episode steps:  30, steps per second:  68, episode reward: 35.226, mean reward:  1.174 [-2.826, 32.438], mean action: 3.233 [0.000, 12.000],  loss: 0.022127, mae: 0.312984, mean_q: 0.507691, mean_eps: 0.000000
 2411/5000: episode: 85, duration: 0.262s, episode steps:  17, steps per second:  65, episode reward: 42.000, mean reward:  2.471 [-2.271, 32.340], mean action: 3.235 [0.000, 12.000],  loss: 0.022764, mae: 0.313872, mean_q: 0.454328, mean_eps: 0.000000
 2443/5000: episode: 86, duration: 0.460s, episode steps:  32, steps per second:  70, episode reward: 41.320, mean reward:  1.291 [-2.083, 32.040], mean action: 2.250 [1.000, 12.000],  loss: 0.021959, mae: 0.311731, mean_q: 0.517041, mean_eps: 0.000000
 2465/5000: episode: 87, duration: 0.326s, episode steps:  22, steps per second:  68, episode reward: 41.125, mean reward:  1.869 [-2.078, 31.683], mean action: 3.591 [0.000, 12.000],  loss: 0.018043, mae: 0.292165, mean_q: 0.513309, mean_eps: 0.000000
 2489/5000: episode: 88, duration: 0.348s, episode steps:  24, steps per second:  69, episode reward: 38.138, mean reward:  1.589 [-2.651, 31.664], mean action: 2.625 [0.000, 11.000],  loss: 0.020454, mae: 0.309592, mean_q: 0.556139, mean_eps: 0.000000
 2515/5000: episode: 89, duration: 0.393s, episode steps:  26, steps per second:  66, episode reward: 39.000, mean reward:  1.500 [-2.635, 29.836], mean action: 1.731 [0.000, 11.000],  loss: 0.020585, mae: 0.304296, mean_q: 0.543466, mean_eps: 0.000000
 2532/5000: episode: 90, duration: 0.258s, episode steps:  17, steps per second:  66, episode reward: 41.177, mean reward:  2.422 [-2.495, 31.803], mean action: 3.118 [0.000, 12.000],  loss: 0.017950, mae: 0.284289, mean_q: 0.574968, mean_eps: 0.000000
 2558/5000: episode: 91, duration: 0.374s, episode steps:  26, steps per second:  69, episode reward: 42.000, mean reward:  1.615 [-2.118, 32.150], mean action: 3.308 [0.000, 12.000],  loss: 0.021657, mae: 0.310910, mean_q: 0.521098, mean_eps: 0.000000
 2583/5000: episode: 92, duration: 0.356s, episode steps:  25, steps per second:  70, episode reward: 33.000, mean reward:  1.320 [-2.903, 32.180], mean action: 3.840 [0.000, 12.000],  loss: 0.020792, mae: 0.308268, mean_q: 0.501239, mean_eps: 0.000000
 2609/5000: episode: 93, duration: 0.388s, episode steps:  26, steps per second:  67, episode reward: 44.090, mean reward:  1.696 [-2.255, 32.380], mean action: 2.308 [0.000, 20.000],  loss: 0.018809, mae: 0.304930, mean_q: 0.496179, mean_eps: 0.000000
 2633/5000: episode: 94, duration: 0.351s, episode steps:  24, steps per second:  68, episode reward: 38.695, mean reward:  1.612 [-3.000, 32.190], mean action: 3.333 [0.000, 12.000],  loss: 0.021222, mae: 0.320435, mean_q: 0.526517, mean_eps: 0.000000
 2661/5000: episode: 95, duration: 0.411s, episode steps:  28, steps per second:  68, episode reward: 44.347, mean reward:  1.584 [-2.384, 32.068], mean action: 2.250 [0.000, 20.000],  loss: 0.023933, mae: 0.323499, mean_q: 0.487904, mean_eps: 0.000000
 2688/5000: episode: 96, duration: 0.394s, episode steps:  27, steps per second:  69, episode reward: 41.357, mean reward:  1.532 [-2.451, 32.170], mean action: 3.037 [0.000, 15.000],  loss: 0.021476, mae: 0.320254, mean_q: 0.543586, mean_eps: 0.000000
 2715/5000: episode: 97, duration: 0.393s, episode steps:  27, steps per second:  69, episode reward: 44.446, mean reward:  1.646 [-2.067, 31.674], mean action: 1.963 [0.000, 16.000],  loss: 0.020019, mae: 0.310834, mean_q: 0.527769, mean_eps: 0.000000
 2739/5000: episode: 98, duration: 0.361s, episode steps:  24, steps per second:  67, episode reward: 44.628, mean reward:  1.860 [-2.194, 32.360], mean action: 3.167 [0.000, 16.000],  loss: 0.021244, mae: 0.302670, mean_q: 0.498369, mean_eps: 0.000000
 2761/5000: episode: 99, duration: 0.321s, episode steps:  22, steps per second:  68, episode reward: 41.707, mean reward:  1.896 [-2.416, 32.210], mean action: 4.455 [1.000, 18.000],  loss: 0.022839, mae: 0.317901, mean_q: 0.502573, mean_eps: 0.000000
 2787/5000: episode: 100, duration: 0.390s, episode steps:  26, steps per second:  67, episode reward: 41.275, mean reward:  1.587 [-2.705, 32.181], mean action: 3.654 [0.000, 16.000],  loss: 0.017440, mae: 0.290761, mean_q: 0.516276, mean_eps: 0.000000
 2811/5000: episode: 101, duration: 0.369s, episode steps:  24, steps per second:  65, episode reward: 44.494, mean reward:  1.854 [-2.226, 32.210], mean action: 3.000 [0.000, 16.000],  loss: 0.022879, mae: 0.314736, mean_q: 0.512793, mean_eps: 0.000000
 2831/5000: episode: 102, duration: 0.301s, episode steps:  20, steps per second:  66, episode reward: 44.227, mean reward:  2.211 [-2.365, 32.170], mean action: 0.850 [0.000, 9.000],  loss: 0.020107, mae: 0.309489, mean_q: 0.562327, mean_eps: 0.000000
 2859/5000: episode: 103, duration: 0.400s, episode steps:  28, steps per second:  70, episode reward: 36.000, mean reward:  1.286 [-3.000, 32.620], mean action: 3.964 [0.000, 20.000],  loss: 0.021030, mae: 0.308597, mean_q: 0.547150, mean_eps: 0.000000
 2897/5000: episode: 104, duration: 0.557s, episode steps:  38, steps per second:  68, episode reward: 35.807, mean reward:  0.942 [-3.000, 32.270], mean action: 5.868 [0.000, 19.000],  loss: 0.019576, mae: 0.315525, mean_q: 0.525218, mean_eps: 0.000000
 2918/5000: episode: 105, duration: 0.313s, episode steps:  21, steps per second:  67, episode reward: 38.351, mean reward:  1.826 [-3.000, 32.082], mean action: 4.857 [0.000, 19.000],  loss: 0.023433, mae: 0.330160, mean_q: 0.523823, mean_eps: 0.000000
 2978/5000: episode: 106, duration: 0.996s, episode steps:  60, steps per second:  60, episode reward: 36.000, mean reward:  0.600 [-2.302, 32.310], mean action: 2.517 [0.000, 19.000],  loss: 0.021309, mae: 0.321107, mean_q: 0.517448, mean_eps: 0.000000
 3012/5000: episode: 107, duration: 0.709s, episode steps:  34, steps per second:  48, episode reward: 42.000, mean reward:  1.235 [-2.566, 32.220], mean action: 1.735 [0.000, 9.000],  loss: 0.020100, mae: 0.307922, mean_q: 0.505496, mean_eps: 0.000000
 3038/5000: episode: 108, duration: 0.393s, episode steps:  26, steps per second:  66, episode reward: 39.000, mean reward:  1.500 [-3.000, 32.330], mean action: 2.808 [0.000, 15.000],  loss: 0.017408, mae: 0.292379, mean_q: 0.527555, mean_eps: 0.000000
 3063/5000: episode: 109, duration: 0.389s, episode steps:  25, steps per second:  64, episode reward: 41.587, mean reward:  1.663 [-2.140, 32.003], mean action: 2.360 [0.000, 20.000],  loss: 0.022216, mae: 0.315400, mean_q: 0.525048, mean_eps: 0.000000
 3097/5000: episode: 110, duration: 0.477s, episode steps:  34, steps per second:  71, episode reward: 41.961, mean reward:  1.234 [-2.285, 32.200], mean action: 1.676 [0.000, 12.000],  loss: 0.024462, mae: 0.327177, mean_q: 0.502563, mean_eps: 0.000000
 3119/5000: episode: 111, duration: 0.339s, episode steps:  22, steps per second:  65, episode reward: 41.281, mean reward:  1.876 [-2.179, 31.495], mean action: 3.136 [1.000, 12.000],  loss: 0.021347, mae: 0.317825, mean_q: 0.543496, mean_eps: 0.000000
 3159/5000: episode: 112, duration: 0.606s, episode steps:  40, steps per second:  66, episode reward: 38.556, mean reward:  0.964 [-2.295, 32.580], mean action: 6.525 [0.000, 20.000],  loss: 0.019423, mae: 0.304401, mean_q: 0.532456, mean_eps: 0.000000
 3182/5000: episode: 113, duration: 0.335s, episode steps:  23, steps per second:  69, episode reward: 43.652, mean reward:  1.898 [-2.036, 32.140], mean action: 1.565 [0.000, 9.000],  loss: 0.022513, mae: 0.319739, mean_q: 0.463964, mean_eps: 0.000000
 3224/5000: episode: 114, duration: 0.596s, episode steps:  42, steps per second:  71, episode reward: 46.306, mean reward:  1.103 [-0.151, 31.552], mean action: 2.429 [0.000, 20.000],  loss: 0.018913, mae: 0.306602, mean_q: 0.494929, mean_eps: 0.000000
 3247/5000: episode: 115, duration: 0.337s, episode steps:  23, steps per second:  68, episode reward: 46.270, mean reward:  2.012 [-0.687, 32.170], mean action: 3.435 [0.000, 12.000],  loss: 0.018528, mae: 0.304889, mean_q: 0.510367, mean_eps: 0.000000
 3275/5000: episode: 116, duration: 0.505s, episode steps:  28, steps per second:  55, episode reward: 38.256, mean reward:  1.366 [-2.717, 32.350], mean action: 3.679 [0.000, 12.000],  loss: 0.019399, mae: 0.308271, mean_q: 0.570839, mean_eps: 0.000000
 3304/5000: episode: 117, duration: 0.425s, episode steps:  29, steps per second:  68, episode reward: 38.654, mean reward:  1.333 [-3.000, 32.480], mean action: 2.690 [0.000, 12.000],  loss: 0.021052, mae: 0.311469, mean_q: 0.551054, mean_eps: 0.000000
 3341/5000: episode: 118, duration: 0.536s, episode steps:  37, steps per second:  69, episode reward: 41.665, mean reward:  1.126 [-2.444, 32.015], mean action: 2.757 [0.000, 19.000],  loss: 0.022613, mae: 0.322122, mean_q: 0.558292, mean_eps: 0.000000
 3373/5000: episode: 119, duration: 0.454s, episode steps:  32, steps per second:  71, episode reward: 38.741, mean reward:  1.211 [-3.000, 31.991], mean action: 2.875 [0.000, 16.000],  loss: 0.018932, mae: 0.301863, mean_q: 0.484613, mean_eps: 0.000000
 3408/5000: episode: 120, duration: 0.498s, episode steps:  35, steps per second:  70, episode reward: 33.000, mean reward:  0.943 [-3.000, 32.460], mean action: 4.829 [0.000, 16.000],  loss: 0.020553, mae: 0.316172, mean_q: 0.526250, mean_eps: 0.000000
 3436/5000: episode: 121, duration: 0.409s, episode steps:  28, steps per second:  68, episode reward: 41.169, mean reward:  1.470 [-3.000, 32.570], mean action: 3.286 [0.000, 18.000],  loss: 0.020349, mae: 0.312264, mean_q: 0.524686, mean_eps: 0.000000
 3466/5000: episode: 122, duration: 0.431s, episode steps:  30, steps per second:  70, episode reward: 35.490, mean reward:  1.183 [-3.000, 31.590], mean action: 4.000 [0.000, 16.000],  loss: 0.016945, mae: 0.294708, mean_q: 0.575207, mean_eps: 0.000000
 3475/5000: episode: 123, duration: 0.150s, episode steps:   9, steps per second:  60, episode reward: 47.455, mean reward:  5.273 [-0.153, 32.607], mean action: 0.222 [0.000, 2.000],  loss: 0.013712, mae: 0.285308, mean_q: 0.607361, mean_eps: 0.000000
 3515/5000: episode: 124, duration: 0.577s, episode steps:  40, steps per second:  69, episode reward: 40.426, mean reward:  1.011 [-2.625, 32.903], mean action: 2.325 [0.000, 16.000],  loss: 0.022519, mae: 0.325118, mean_q: 0.566081, mean_eps: 0.000000
 3531/5000: episode: 125, duration: 0.288s, episode steps:  16, steps per second:  56, episode reward: 44.895, mean reward:  2.806 [-2.020, 32.195], mean action: 1.312 [0.000, 11.000],  loss: 0.022081, mae: 0.323823, mean_q: 0.515390, mean_eps: 0.000000
 3578/5000: episode: 126, duration: 0.705s, episode steps:  47, steps per second:  67, episode reward: 37.801, mean reward:  0.804 [-2.998, 32.562], mean action: 3.277 [1.000, 16.000],  loss: 0.021591, mae: 0.325665, mean_q: 0.548365, mean_eps: 0.000000
 3600/5000: episode: 127, duration: 0.325s, episode steps:  22, steps per second:  68, episode reward: 41.109, mean reward:  1.869 [-2.658, 32.230], mean action: 5.045 [0.000, 19.000],  loss: 0.018524, mae: 0.307350, mean_q: 0.496623, mean_eps: 0.000000
 3620/5000: episode: 128, duration: 0.306s, episode steps:  20, steps per second:  65, episode reward: 41.243, mean reward:  2.062 [-2.919, 31.935], mean action: 2.200 [0.000, 16.000],  loss: 0.020089, mae: 0.310483, mean_q: 0.550100, mean_eps: 0.000000
 3650/5000: episode: 129, duration: 0.464s, episode steps:  30, steps per second:  65, episode reward: 44.973, mean reward:  1.499 [-2.489, 32.237], mean action: 2.833 [1.000, 19.000],  loss: 0.019632, mae: 0.305351, mean_q: 0.545285, mean_eps: 0.000000
 3672/5000: episode: 130, duration: 0.329s, episode steps:  22, steps per second:  67, episode reward: 41.393, mean reward:  1.882 [-2.532, 32.287], mean action: 2.500 [0.000, 19.000],  loss: 0.022192, mae: 0.310660, mean_q: 0.531914, mean_eps: 0.000000
 3695/5000: episode: 131, duration: 0.405s, episode steps:  23, steps per second:  57, episode reward: 41.145, mean reward:  1.789 [-2.361, 32.870], mean action: 2.957 [1.000, 19.000],  loss: 0.021843, mae: 0.305718, mean_q: 0.521932, mean_eps: 0.000000
 3721/5000: episode: 132, duration: 0.381s, episode steps:  26, steps per second:  68, episode reward: 37.622, mean reward:  1.447 [-3.000, 31.930], mean action: 5.923 [0.000, 20.000],  loss: 0.022508, mae: 0.309292, mean_q: 0.513494, mean_eps: 0.000000
 3745/5000: episode: 133, duration: 0.354s, episode steps:  24, steps per second:  68, episode reward: 44.783, mean reward:  1.866 [-2.186, 31.923], mean action: 1.083 [0.000, 12.000],  loss: 0.016003, mae: 0.284201, mean_q: 0.485992, mean_eps: 0.000000
 3768/5000: episode: 134, duration: 0.338s, episode steps:  23, steps per second:  68, episode reward: 41.267, mean reward:  1.794 [-2.664, 32.460], mean action: 4.217 [0.000, 19.000],  loss: 0.022335, mae: 0.319358, mean_q: 0.558483, mean_eps: 0.000000
 3804/5000: episode: 135, duration: 0.512s, episode steps:  36, steps per second:  70, episode reward: 37.728, mean reward:  1.048 [-2.770, 32.322], mean action: 3.417 [0.000, 19.000],  loss: 0.023068, mae: 0.314365, mean_q: 0.544086, mean_eps: 0.000000
 3826/5000: episode: 136, duration: 0.325s, episode steps:  22, steps per second:  68, episode reward: 38.307, mean reward:  1.741 [-2.614, 29.110], mean action: 3.591 [0.000, 14.000],  loss: 0.021336, mae: 0.308395, mean_q: 0.524469, mean_eps: 0.000000
 3852/5000: episode: 137, duration: 0.382s, episode steps:  26, steps per second:  68, episode reward: 41.330, mean reward:  1.590 [-2.556, 32.220], mean action: 3.885 [0.000, 14.000],  loss: 0.019457, mae: 0.305098, mean_q: 0.526764, mean_eps: 0.000000
 3873/5000: episode: 138, duration: 0.314s, episode steps:  21, steps per second:  67, episode reward: 45.000, mean reward:  2.143 [-2.030, 32.160], mean action: 1.095 [0.000, 12.000],  loss: 0.015627, mae: 0.287913, mean_q: 0.451019, mean_eps: 0.000000
 3903/5000: episode: 139, duration: 0.431s, episode steps:  30, steps per second:  70, episode reward: 38.902, mean reward:  1.297 [-2.283, 32.562], mean action: 2.433 [0.000, 12.000],  loss: 0.017148, mae: 0.293471, mean_q: 0.463027, mean_eps: 0.000000
 3932/5000: episode: 140, duration: 0.418s, episode steps:  29, steps per second:  69, episode reward: 36.000, mean reward:  1.241 [-2.606, 32.460], mean action: 2.793 [0.000, 12.000],  loss: 0.020326, mae: 0.308952, mean_q: 0.513559, mean_eps: 0.000000
 3953/5000: episode: 141, duration: 0.315s, episode steps:  21, steps per second:  67, episode reward: 44.680, mean reward:  2.128 [-2.054, 32.199], mean action: 2.238 [0.000, 11.000],  loss: 0.021921, mae: 0.322987, mean_q: 0.502111, mean_eps: 0.000000
 3980/5000: episode: 142, duration: 0.387s, episode steps:  27, steps per second:  70, episode reward: 44.734, mean reward:  1.657 [-2.253, 32.140], mean action: 2.037 [0.000, 11.000],  loss: 0.018816, mae: 0.308152, mean_q: 0.446408, mean_eps: 0.000000
 4003/5000: episode: 143, duration: 0.347s, episode steps:  23, steps per second:  66, episode reward: 47.357, mean reward:  2.059 [-0.472, 32.158], mean action: 2.261 [0.000, 3.000],  loss: 0.020457, mae: 0.313150, mean_q: 0.543123, mean_eps: 0.000000
 4028/5000: episode: 144, duration: 0.367s, episode steps:  25, steps per second:  68, episode reward: 43.810, mean reward:  1.752 [-2.765, 31.769], mean action: 4.200 [0.000, 19.000],  loss: 0.023064, mae: 0.330536, mean_q: 0.493141, mean_eps: 0.000000
 4051/5000: episode: 145, duration: 0.338s, episode steps:  23, steps per second:  68, episode reward: 44.610, mean reward:  1.940 [-2.039, 32.250], mean action: 2.391 [0.000, 14.000],  loss: 0.021249, mae: 0.318632, mean_q: 0.458508, mean_eps: 0.000000
 4083/5000: episode: 146, duration: 0.465s, episode steps:  32, steps per second:  69, episode reward: 36.000, mean reward:  1.125 [-2.442, 32.070], mean action: 4.688 [0.000, 14.000],  loss: 0.015882, mae: 0.288858, mean_q: 0.505119, mean_eps: 0.000000
 4109/5000: episode: 147, duration: 0.386s, episode steps:  26, steps per second:  67, episode reward: 44.669, mean reward:  1.718 [-2.267, 32.035], mean action: 1.000 [0.000, 11.000],  loss: 0.024197, mae: 0.320978, mean_q: 0.524680, mean_eps: 0.000000
 4143/5000: episode: 148, duration: 0.499s, episode steps:  34, steps per second:  68, episode reward: -35.010, mean reward: -1.030 [-32.075,  2.158], mean action: 5.000 [0.000, 19.000],  loss: 0.018027, mae: 0.296541, mean_q: 0.510762, mean_eps: 0.000000
 4170/5000: episode: 149, duration: 0.412s, episode steps:  27, steps per second:  66, episode reward: 32.150, mean reward:  1.191 [-3.000, 32.360], mean action: 8.630 [0.000, 19.000],  loss: 0.015756, mae: 0.289696, mean_q: 0.596269, mean_eps: 0.000000
 4184/5000: episode: 150, duration: 0.224s, episode steps:  14, steps per second:  62, episode reward: 44.209, mean reward:  3.158 [-2.322, 33.000], mean action: 2.571 [0.000, 19.000],  loss: 0.017338, mae: 0.292506, mean_q: 0.547965, mean_eps: 0.000000
 4215/5000: episode: 151, duration: 0.458s, episode steps:  31, steps per second:  68, episode reward: 33.000, mean reward:  1.065 [-3.000, 32.230], mean action: 6.323 [0.000, 19.000],  loss: 0.020623, mae: 0.313785, mean_q: 0.507529, mean_eps: 0.000000
 4279/5000: episode: 152, duration: 0.883s, episode steps:  64, steps per second:  73, episode reward: 35.963, mean reward:  0.562 [-2.224, 32.103], mean action: 3.422 [0.000, 19.000],  loss: 0.020928, mae: 0.312053, mean_q: 0.522464, mean_eps: 0.000000
 4350/5000: episode: 153, duration: 1.012s, episode steps:  71, steps per second:  70, episode reward: 40.046, mean reward:  0.564 [-2.313, 32.420], mean action: 4.042 [0.000, 19.000],  loss: 0.020872, mae: 0.308529, mean_q: 0.544033, mean_eps: 0.000000
 4381/5000: episode: 154, duration: 0.458s, episode steps:  31, steps per second:  68, episode reward: 41.749, mean reward:  1.347 [-2.328, 32.160], mean action: 3.935 [0.000, 15.000],  loss: 0.020083, mae: 0.306656, mean_q: 0.564368, mean_eps: 0.000000
 4401/5000: episode: 155, duration: 0.312s, episode steps:  20, steps per second:  64, episode reward: 44.946, mean reward:  2.247 [-2.197, 32.400], mean action: 2.100 [0.000, 9.000],  loss: 0.019952, mae: 0.302162, mean_q: 0.535349, mean_eps: 0.000000
 4430/5000: episode: 156, duration: 0.417s, episode steps:  29, steps per second:  70, episode reward: 36.000, mean reward:  1.241 [-2.620, 32.200], mean action: 3.310 [0.000, 15.000],  loss: 0.019294, mae: 0.303224, mean_q: 0.524812, mean_eps: 0.000000
 4453/5000: episode: 157, duration: 0.340s, episode steps:  23, steps per second:  68, episode reward: 43.862, mean reward:  1.907 [-2.187, 32.223], mean action: 4.522 [0.000, 20.000],  loss: 0.022017, mae: 0.321161, mean_q: 0.533443, mean_eps: 0.000000
 4485/5000: episode: 158, duration: 0.665s, episode steps:  32, steps per second:  48, episode reward: 43.526, mean reward:  1.360 [-2.581, 32.270], mean action: 3.406 [0.000, 12.000],  loss: 0.020379, mae: 0.314530, mean_q: 0.610193, mean_eps: 0.000000
 4511/5000: episode: 159, duration: 0.394s, episode steps:  26, steps per second:  66, episode reward: 44.571, mean reward:  1.714 [-2.227, 32.900], mean action: 2.654 [0.000, 11.000],  loss: 0.018920, mae: 0.311728, mean_q: 0.627757, mean_eps: 0.000000
 4541/5000: episode: 160, duration: 0.437s, episode steps:  30, steps per second:  69, episode reward: 38.814, mean reward:  1.294 [-2.486, 32.520], mean action: 2.667 [0.000, 12.000],  loss: 0.022225, mae: 0.320474, mean_q: 0.600702, mean_eps: 0.000000
 4562/5000: episode: 161, duration: 0.311s, episode steps:  21, steps per second:  67, episode reward: 44.623, mean reward:  2.125 [-2.349, 32.090], mean action: 4.000 [0.000, 14.000],  loss: 0.020195, mae: 0.313508, mean_q: 0.580280, mean_eps: 0.000000
 4581/5000: episode: 162, duration: 0.298s, episode steps:  19, steps per second:  64, episode reward: 44.332, mean reward:  2.333 [-2.031, 32.410], mean action: 2.000 [0.000, 11.000],  loss: 0.019490, mae: 0.305831, mean_q: 0.568426, mean_eps: 0.000000
 4616/5000: episode: 163, duration: 0.509s, episode steps:  35, steps per second:  69, episode reward: 37.998, mean reward:  1.086 [-2.588, 31.802], mean action: 7.114 [0.000, 20.000],  loss: 0.020320, mae: 0.320406, mean_q: 0.559853, mean_eps: 0.000000
 4630/5000: episode: 164, duration: 0.218s, episode steps:  14, steps per second:  64, episode reward: 47.305, mean reward:  3.379 [-0.061, 32.010], mean action: 4.857 [0.000, 14.000],  loss: 0.025016, mae: 0.333769, mean_q: 0.540708, mean_eps: 0.000000
 4656/5000: episode: 165, duration: 0.376s, episode steps:  26, steps per second:  69, episode reward: 41.013, mean reward:  1.577 [-2.245, 32.438], mean action: 3.385 [0.000, 12.000],  loss: 0.019143, mae: 0.305850, mean_q: 0.539080, mean_eps: 0.000000
 4679/5000: episode: 166, duration: 0.347s, episode steps:  23, steps per second:  66, episode reward: 41.858, mean reward:  1.820 [-2.503, 32.210], mean action: 1.957 [0.000, 12.000],  loss: 0.016564, mae: 0.299674, mean_q: 0.538156, mean_eps: 0.000000
 4710/5000: episode: 167, duration: 0.443s, episode steps:  31, steps per second:  70, episode reward: 41.302, mean reward:  1.332 [-2.357, 32.430], mean action: 6.387 [0.000, 20.000],  loss: 0.022489, mae: 0.328176, mean_q: 0.470405, mean_eps: 0.000000
 4733/5000: episode: 168, duration: 0.338s, episode steps:  23, steps per second:  68, episode reward: 41.278, mean reward:  1.795 [-2.528, 32.033], mean action: 3.870 [1.000, 14.000],  loss: 0.024215, mae: 0.326833, mean_q: 0.532570, mean_eps: 0.000000
 4776/5000: episode: 169, duration: 0.607s, episode steps:  43, steps per second:  71, episode reward: 41.506, mean reward:  0.965 [-2.173, 32.360], mean action: 2.512 [0.000, 20.000],  loss: 0.022604, mae: 0.321395, mean_q: 0.578346, mean_eps: 0.000000
 4796/5000: episode: 170, duration: 0.297s, episode steps:  20, steps per second:  67, episode reward: 42.000, mean reward:  2.100 [-3.000, 32.470], mean action: 1.250 [0.000, 11.000],  loss: 0.019207, mae: 0.301376, mean_q: 0.493999, mean_eps: 0.000000
 4818/5000: episode: 171, duration: 0.321s, episode steps:  22, steps per second:  69, episode reward: 46.445, mean reward:  2.111 [-0.060, 32.060], mean action: 4.455 [0.000, 20.000],  loss: 0.015690, mae: 0.290623, mean_q: 0.428951, mean_eps: 0.000000
 4845/5000: episode: 172, duration: 0.391s, episode steps:  27, steps per second:  69, episode reward: 40.925, mean reward:  1.516 [-2.448, 32.472], mean action: 6.148 [0.000, 14.000],  loss: 0.020081, mae: 0.308617, mean_q: 0.511165, mean_eps: 0.000000
 4895/5000: episode: 173, duration: 0.704s, episode steps:  50, steps per second:  71, episode reward: 33.000, mean reward:  0.660 [-2.570, 32.860], mean action: 6.700 [1.000, 20.000],  loss: 0.017908, mae: 0.300183, mean_q: 0.554602, mean_eps: 0.000000
 4927/5000: episode: 174, duration: 0.458s, episode steps:  32, steps per second:  70, episode reward: 35.922, mean reward:  1.123 [-3.000, 32.410], mean action: 4.000 [0.000, 16.000],  loss: 0.020979, mae: 0.316185, mean_q: 0.581285, mean_eps: 0.000000
 4953/5000: episode: 175, duration: 0.374s, episode steps:  26, steps per second:  70, episode reward: 38.188, mean reward:  1.469 [-3.000, 32.790], mean action: 4.808 [0.000, 16.000],  loss: 0.019011, mae: 0.304140, mean_q: 0.546366, mean_eps: 0.000000
 4990/5000: episode: 176, duration: 0.537s, episode steps:  37, steps per second:  69, episode reward: 45.000, mean reward:  1.216 [-2.118, 32.030], mean action: 1.811 [0.000, 16.000],  loss: 0.022284, mae: 0.318082, mean_q: 0.559216, mean_eps: 0.000000
done, took 66.743 seconds
DQN Evaluation: 12795 victories out of 14879 episodes
Training for 5000 steps ...
   12/5000: episode: 1, duration: 0.162s, episode steps:  12, steps per second:  74, episode reward: 44.626, mean reward:  3.719 [-2.206, 32.290], mean action: 3.083 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   30/5000: episode: 2, duration: 0.141s, episode steps:  18, steps per second: 128, episode reward: -36.000, mean reward: -2.000 [-32.132,  2.510], mean action: 5.611 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   54/5000: episode: 3, duration: 0.197s, episode steps:  24, steps per second: 122, episode reward: 37.917, mean reward:  1.580 [-2.460, 32.340], mean action: 3.625 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   79/5000: episode: 4, duration: 0.185s, episode steps:  25, steps per second: 135, episode reward: 33.000, mean reward:  1.320 [-3.000, 32.240], mean action: 4.840 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  105/5000: episode: 5, duration: 0.185s, episode steps:  26, steps per second: 140, episode reward: 32.805, mean reward:  1.262 [-2.373, 32.903], mean action: 6.654 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  127/5000: episode: 6, duration: 0.166s, episode steps:  22, steps per second: 132, episode reward: 41.620, mean reward:  1.892 [-2.037, 32.230], mean action: 2.773 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  168/5000: episode: 7, duration: 0.271s, episode steps:  41, steps per second: 151, episode reward: 35.574, mean reward:  0.868 [-2.418, 32.220], mean action: 4.610 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  187/5000: episode: 8, duration: 0.139s, episode steps:  19, steps per second: 137, episode reward: 33.000, mean reward:  1.737 [-3.000, 32.230], mean action: 5.053 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  200/5000: episode: 9, duration: 0.121s, episode steps:  13, steps per second: 107, episode reward: 41.553, mean reward:  3.196 [-2.007, 32.422], mean action: 5.154 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  227/5000: episode: 10, duration: 0.187s, episode steps:  27, steps per second: 144, episode reward: 38.109, mean reward:  1.411 [-2.617, 32.040], mean action: 3.815 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  251/5000: episode: 11, duration: 0.169s, episode steps:  24, steps per second: 142, episode reward: 38.761, mean reward:  1.615 [-2.475, 33.000], mean action: 3.958 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  269/5000: episode: 12, duration: 0.137s, episode steps:  18, steps per second: 132, episode reward: 44.631, mean reward:  2.480 [-2.099, 32.310], mean action: 4.111 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  284/5000: episode: 13, duration: 0.118s, episode steps:  15, steps per second: 127, episode reward: 38.811, mean reward:  2.587 [-2.475, 30.717], mean action: 5.667 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  304/5000: episode: 14, duration: 0.185s, episode steps:  20, steps per second: 108, episode reward: 41.036, mean reward:  2.052 [-2.516, 32.320], mean action: 2.750 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  327/5000: episode: 15, duration: 0.180s, episode steps:  23, steps per second: 127, episode reward: 36.000, mean reward:  1.565 [-2.398, 32.060], mean action: 4.478 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  355/5000: episode: 16, duration: 0.188s, episode steps:  28, steps per second: 149, episode reward: 35.607, mean reward:  1.272 [-3.000, 31.976], mean action: 4.107 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  380/5000: episode: 17, duration: 0.170s, episode steps:  25, steps per second: 147, episode reward: 35.572, mean reward:  1.423 [-2.566, 32.151], mean action: 3.560 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  403/5000: episode: 18, duration: 0.185s, episode steps:  23, steps per second: 124, episode reward: 32.318, mean reward:  1.405 [-2.583, 31.378], mean action: 5.913 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  438/5000: episode: 19, duration: 0.238s, episode steps:  35, steps per second: 147, episode reward: 32.104, mean reward:  0.917 [-2.486, 32.058], mean action: 3.943 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  468/5000: episode: 20, duration: 0.213s, episode steps:  30, steps per second: 141, episode reward: 38.219, mean reward:  1.274 [-2.960, 32.008], mean action: 3.100 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  491/5000: episode: 21, duration: 0.156s, episode steps:  23, steps per second: 148, episode reward: 32.746, mean reward:  1.424 [-3.000, 32.182], mean action: 6.304 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  510/5000: episode: 22, duration: 0.138s, episode steps:  19, steps per second: 138, episode reward: 35.698, mean reward:  1.879 [-3.000, 31.788], mean action: 4.263 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  531/5000: episode: 23, duration: 0.157s, episode steps:  21, steps per second: 134, episode reward: 39.799, mean reward:  1.895 [-2.553, 32.110], mean action: 5.000 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  565/5000: episode: 24, duration: 0.246s, episode steps:  34, steps per second: 138, episode reward: 34.661, mean reward:  1.019 [-2.498, 32.030], mean action: 5.971 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  591/5000: episode: 25, duration: 0.174s, episode steps:  26, steps per second: 149, episode reward: 32.661, mean reward:  1.256 [-3.000, 31.961], mean action: 6.077 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  611/5000: episode: 26, duration: 0.140s, episode steps:  20, steps per second: 142, episode reward: 41.475, mean reward:  2.074 [-2.317, 32.180], mean action: 3.450 [1.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  639/5000: episode: 27, duration: 0.189s, episode steps:  28, steps per second: 148, episode reward: -32.560, mean reward: -1.163 [-32.849,  2.620], mean action: 7.214 [0.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  653/5000: episode: 28, duration: 0.106s, episode steps:  14, steps per second: 132, episode reward: 38.528, mean reward:  2.752 [-3.000, 32.188], mean action: 5.500 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  673/5000: episode: 29, duration: 0.161s, episode steps:  20, steps per second: 124, episode reward: 41.388, mean reward:  2.069 [-2.175, 32.156], mean action: 2.900 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  696/5000: episode: 30, duration: 0.171s, episode steps:  23, steps per second: 135, episode reward: 37.812, mean reward:  1.644 [-2.509, 32.310], mean action: 6.348 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  728/5000: episode: 31, duration: 0.212s, episode steps:  32, steps per second: 151, episode reward: 41.216, mean reward:  1.288 [-2.192, 32.100], mean action: 3.688 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  751/5000: episode: 32, duration: 0.163s, episode steps:  23, steps per second: 141, episode reward: -35.490, mean reward: -1.543 [-32.007,  2.440], mean action: 5.783 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  773/5000: episode: 33, duration: 0.152s, episode steps:  22, steps per second: 145, episode reward: 36.000, mean reward:  1.636 [-2.693, 32.390], mean action: 4.682 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  806/5000: episode: 34, duration: 0.213s, episode steps:  33, steps per second: 155, episode reward: 38.320, mean reward:  1.161 [-2.072, 32.170], mean action: 3.424 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  831/5000: episode: 35, duration: 0.168s, episode steps:  25, steps per second: 149, episode reward: 32.833, mean reward:  1.313 [-2.609, 33.000], mean action: 4.760 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  850/5000: episode: 36, duration: 0.138s, episode steps:  19, steps per second: 138, episode reward: 32.514, mean reward:  1.711 [-3.000, 31.780], mean action: 6.316 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  878/5000: episode: 37, duration: 0.183s, episode steps:  28, steps per second: 153, episode reward: -35.850, mean reward: -1.280 [-31.886,  2.370], mean action: 5.750 [0.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  908/5000: episode: 38, duration: 0.208s, episode steps:  30, steps per second: 144, episode reward: 32.417, mean reward:  1.081 [-3.000, 32.464], mean action: 5.800 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  935/5000: episode: 39, duration: 0.199s, episode steps:  27, steps per second: 136, episode reward: 41.221, mean reward:  1.527 [-2.801, 32.320], mean action: 2.593 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  964/5000: episode: 40, duration: 0.190s, episode steps:  29, steps per second: 153, episode reward: 32.064, mean reward:  1.106 [-2.462, 31.962], mean action: 4.552 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  979/5000: episode: 41, duration: 0.118s, episode steps:  15, steps per second: 127, episode reward: 41.216, mean reward:  2.748 [-3.000, 32.330], mean action: 3.533 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  999/5000: episode: 42, duration: 0.144s, episode steps:  20, steps per second: 139, episode reward: 36.000, mean reward:  1.800 [-2.902, 32.290], mean action: 5.850 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1017/5000: episode: 43, duration: 0.265s, episode steps:  18, steps per second:  68, episode reward: 38.054, mean reward:  2.114 [-3.000, 32.650], mean action: 5.944 [0.000, 16.000],  loss: 0.026011, mae: 0.333977, mean_q: 0.539631, mean_eps: 0.000000
 1034/5000: episode: 44, duration: 0.260s, episode steps:  17, steps per second:  65, episode reward: 42.000, mean reward:  2.471 [-2.341, 33.000], mean action: 3.412 [0.000, 16.000],  loss: 0.021504, mae: 0.313874, mean_q: 0.557515, mean_eps: 0.000000
 1049/5000: episode: 45, duration: 0.235s, episode steps:  15, steps per second:  64, episode reward: 37.563, mean reward:  2.504 [-2.410, 32.483], mean action: 4.733 [0.000, 16.000],  loss: 0.026055, mae: 0.338953, mean_q: 0.546630, mean_eps: 0.000000
 1081/5000: episode: 46, duration: 0.459s, episode steps:  32, steps per second:  70, episode reward: -33.000, mean reward: -1.031 [-33.000,  2.940], mean action: 3.500 [0.000, 16.000],  loss: 0.021490, mae: 0.310776, mean_q: 0.524964, mean_eps: 0.000000
 1118/5000: episode: 47, duration: 0.546s, episode steps:  37, steps per second:  68, episode reward: -32.310, mean reward: -0.873 [-33.000,  2.240], mean action: 5.297 [0.000, 16.000],  loss: 0.017910, mae: 0.295756, mean_q: 0.475652, mean_eps: 0.000000
 1133/5000: episode: 48, duration: 0.290s, episode steps:  15, steps per second:  52, episode reward: 41.195, mean reward:  2.746 [-2.257, 32.228], mean action: 1.800 [0.000, 9.000],  loss: 0.026966, mae: 0.348593, mean_q: 0.455860, mean_eps: 0.000000
 1148/5000: episode: 49, duration: 0.386s, episode steps:  15, steps per second:  39, episode reward: 44.478, mean reward:  2.965 [-2.055, 32.380], mean action: 1.467 [0.000, 6.000],  loss: 0.023447, mae: 0.325207, mean_q: 0.500333, mean_eps: 0.000000
 1167/5000: episode: 50, duration: 0.287s, episode steps:  19, steps per second:  66, episode reward: 35.102, mean reward:  1.847 [-3.000, 31.776], mean action: 4.842 [0.000, 15.000],  loss: 0.025204, mae: 0.342450, mean_q: 0.546615, mean_eps: 0.000000
 1196/5000: episode: 51, duration: 0.413s, episode steps:  29, steps per second:  70, episode reward: -32.520, mean reward: -1.121 [-31.926,  2.477], mean action: 6.034 [0.000, 15.000],  loss: 0.019807, mae: 0.313569, mean_q: 0.539141, mean_eps: 0.000000
 1223/5000: episode: 52, duration: 0.405s, episode steps:  27, steps per second:  67, episode reward: 32.313, mean reward:  1.197 [-2.669, 31.964], mean action: 3.963 [0.000, 19.000],  loss: 0.021813, mae: 0.317111, mean_q: 0.580212, mean_eps: 0.000000
 1245/5000: episode: 53, duration: 0.326s, episode steps:  22, steps per second:  68, episode reward: 38.739, mean reward:  1.761 [-2.964, 32.550], mean action: 3.364 [0.000, 20.000],  loss: 0.021142, mae: 0.315876, mean_q: 0.555711, mean_eps: 0.000000
 1268/5000: episode: 54, duration: 0.342s, episode steps:  23, steps per second:  67, episode reward: 35.915, mean reward:  1.562 [-2.339, 32.190], mean action: 3.565 [0.000, 19.000],  loss: 0.019507, mae: 0.322306, mean_q: 0.560442, mean_eps: 0.000000
 1288/5000: episode: 55, duration: 0.322s, episode steps:  20, steps per second:  62, episode reward: 43.107, mean reward:  2.155 [-2.023, 32.270], mean action: 2.800 [0.000, 16.000],  loss: 0.023220, mae: 0.334541, mean_q: 0.625475, mean_eps: 0.000000
 1311/5000: episode: 56, duration: 0.348s, episode steps:  23, steps per second:  66, episode reward: 33.000, mean reward:  1.435 [-3.000, 32.100], mean action: 10.000 [0.000, 20.000],  loss: 0.019860, mae: 0.320604, mean_q: 0.609310, mean_eps: 0.000000
 1329/5000: episode: 57, duration: 0.281s, episode steps:  18, steps per second:  64, episode reward: 41.127, mean reward:  2.285 [-2.232, 32.420], mean action: 2.833 [0.000, 16.000],  loss: 0.016365, mae: 0.298074, mean_q: 0.580518, mean_eps: 0.000000
 1353/5000: episode: 58, duration: 0.354s, episode steps:  24, steps per second:  68, episode reward: -32.010, mean reward: -1.334 [-32.155,  2.695], mean action: 3.458 [1.000, 9.000],  loss: 0.022480, mae: 0.333971, mean_q: 0.570196, mean_eps: 0.000000
 1371/5000: episode: 59, duration: 0.275s, episode steps:  18, steps per second:  65, episode reward: 35.989, mean reward:  1.999 [-2.408, 32.550], mean action: 4.000 [0.000, 9.000],  loss: 0.024280, mae: 0.343222, mean_q: 0.559221, mean_eps: 0.000000
 1396/5000: episode: 60, duration: 0.370s, episode steps:  25, steps per second:  68, episode reward: 32.016, mean reward:  1.281 [-3.000, 32.860], mean action: 6.440 [0.000, 17.000],  loss: 0.018304, mae: 0.311297, mean_q: 0.521149, mean_eps: 0.000000
 1423/5000: episode: 61, duration: 0.405s, episode steps:  27, steps per second:  67, episode reward: 40.891, mean reward:  1.514 [-2.292, 32.530], mean action: 8.556 [0.000, 15.000],  loss: 0.021642, mae: 0.331449, mean_q: 0.493524, mean_eps: 0.000000
 1452/5000: episode: 62, duration: 0.455s, episode steps:  29, steps per second:  64, episode reward: -33.000, mean reward: -1.138 [-32.213,  2.878], mean action: 5.586 [0.000, 12.000],  loss: 0.018418, mae: 0.309467, mean_q: 0.522158, mean_eps: 0.000000
 1475/5000: episode: 63, duration: 0.405s, episode steps:  23, steps per second:  57, episode reward: 32.671, mean reward:  1.420 [-3.000, 32.241], mean action: 4.826 [0.000, 16.000],  loss: 0.021936, mae: 0.334052, mean_q: 0.489550, mean_eps: 0.000000
 1499/5000: episode: 64, duration: 0.361s, episode steps:  24, steps per second:  66, episode reward: 32.902, mean reward:  1.371 [-3.000, 32.902], mean action: 5.708 [0.000, 16.000],  loss: 0.021648, mae: 0.330356, mean_q: 0.510896, mean_eps: 0.000000
 1507/5000: episode: 65, duration: 0.179s, episode steps:   8, steps per second:  45, episode reward: 47.025, mean reward:  5.878 [ 0.085, 32.430], mean action: 2.000 [0.000, 14.000],  loss: 0.015675, mae: 0.298666, mean_q: 0.481646, mean_eps: 0.000000
 1524/5000: episode: 66, duration: 0.269s, episode steps:  17, steps per second:  63, episode reward: 38.065, mean reward:  2.239 [-3.000, 31.816], mean action: 3.882 [0.000, 15.000],  loss: 0.018638, mae: 0.312830, mean_q: 0.473315, mean_eps: 0.000000
 1557/5000: episode: 67, duration: 0.479s, episode steps:  33, steps per second:  69, episode reward: 32.624, mean reward:  0.989 [-3.000, 32.124], mean action: 4.909 [0.000, 16.000],  loss: 0.022569, mae: 0.324508, mean_q: 0.530573, mean_eps: 0.000000
 1572/5000: episode: 68, duration: 0.242s, episode steps:  15, steps per second:  62, episode reward: 41.392, mean reward:  2.759 [-2.047, 32.904], mean action: 2.467 [0.000, 16.000],  loss: 0.022361, mae: 0.329868, mean_q: 0.535512, mean_eps: 0.000000
 1601/5000: episode: 69, duration: 0.412s, episode steps:  29, steps per second:  70, episode reward: -38.230, mean reward: -1.318 [-32.127,  2.940], mean action: 6.414 [0.000, 16.000],  loss: 0.019435, mae: 0.312750, mean_q: 0.520177, mean_eps: 0.000000
 1627/5000: episode: 70, duration: 0.376s, episode steps:  26, steps per second:  69, episode reward: -35.670, mean reward: -1.372 [-32.422,  2.270], mean action: 5.846 [0.000, 16.000],  loss: 0.016471, mae: 0.305002, mean_q: 0.462676, mean_eps: 0.000000
 1653/5000: episode: 71, duration: 0.374s, episode steps:  26, steps per second:  70, episode reward: 35.021, mean reward:  1.347 [-2.376, 32.110], mean action: 4.115 [0.000, 16.000],  loss: 0.020701, mae: 0.319676, mean_q: 0.478277, mean_eps: 0.000000
 1670/5000: episode: 72, duration: 0.267s, episode steps:  17, steps per second:  64, episode reward: 42.000, mean reward:  2.471 [-2.107, 32.410], mean action: 3.000 [0.000, 9.000],  loss: 0.017708, mae: 0.306829, mean_q: 0.543750, mean_eps: 0.000000
 1689/5000: episode: 73, duration: 0.290s, episode steps:  19, steps per second:  65, episode reward: 41.333, mean reward:  2.175 [-2.331, 32.160], mean action: 2.632 [0.000, 16.000],  loss: 0.021798, mae: 0.322612, mean_q: 0.525486, mean_eps: 0.000000
 1705/5000: episode: 74, duration: 0.249s, episode steps:  16, steps per second:  64, episode reward: 40.852, mean reward:  2.553 [-3.000, 31.743], mean action: 4.688 [0.000, 15.000],  loss: 0.021445, mae: 0.326153, mean_q: 0.514429, mean_eps: 0.000000
 1724/5000: episode: 75, duration: 0.282s, episode steps:  19, steps per second:  67, episode reward: 38.032, mean reward:  2.002 [-2.830, 31.869], mean action: 5.737 [0.000, 15.000],  loss: 0.023798, mae: 0.332992, mean_q: 0.536550, mean_eps: 0.000000
 1753/5000: episode: 76, duration: 0.463s, episode steps:  29, steps per second:  63, episode reward: 32.183, mean reward:  1.110 [-2.458, 31.841], mean action: 6.276 [0.000, 16.000],  loss: 0.021011, mae: 0.319842, mean_q: 0.538937, mean_eps: 0.000000
 1782/5000: episode: 77, duration: 0.413s, episode steps:  29, steps per second:  70, episode reward: -41.130, mean reward: -1.418 [-31.907,  2.177], mean action: 9.034 [1.000, 15.000],  loss: 0.022371, mae: 0.326158, mean_q: 0.553143, mean_eps: 0.000000
 1808/5000: episode: 78, duration: 0.379s, episode steps:  26, steps per second:  69, episode reward: -35.680, mean reward: -1.372 [-32.680,  2.180], mean action: 8.769 [0.000, 15.000],  loss: 0.019546, mae: 0.308233, mean_q: 0.575438, mean_eps: 0.000000
 1832/5000: episode: 79, duration: 0.368s, episode steps:  24, steps per second:  65, episode reward: -35.910, mean reward: -1.496 [-32.214,  2.290], mean action: 5.708 [1.000, 21.000],  loss: 0.019858, mae: 0.311742, mean_q: 0.554316, mean_eps: 0.000000
 1853/5000: episode: 80, duration: 0.384s, episode steps:  21, steps per second:  55, episode reward: 37.611, mean reward:  1.791 [-2.615, 32.262], mean action: 6.429 [0.000, 15.000],  loss: 0.022960, mae: 0.325203, mean_q: 0.560815, mean_eps: 0.000000
 1883/5000: episode: 81, duration: 0.441s, episode steps:  30, steps per second:  68, episode reward: 34.730, mean reward:  1.158 [-2.357, 32.317], mean action: 5.067 [0.000, 19.000],  loss: 0.020034, mae: 0.317392, mean_q: 0.532265, mean_eps: 0.000000
 1900/5000: episode: 82, duration: 0.250s, episode steps:  17, steps per second:  68, episode reward: 40.868, mean reward:  2.404 [-2.635, 32.190], mean action: 5.941 [1.000, 19.000],  loss: 0.023045, mae: 0.326312, mean_q: 0.539275, mean_eps: 0.000000
 1919/5000: episode: 83, duration: 0.292s, episode steps:  19, steps per second:  65, episode reward: 38.523, mean reward:  2.028 [-2.230, 33.000], mean action: 4.684 [0.000, 19.000],  loss: 0.015997, mae: 0.293474, mean_q: 0.545393, mean_eps: 0.000000
 1933/5000: episode: 84, duration: 0.223s, episode steps:  14, steps per second:  63, episode reward: 41.178, mean reward:  2.941 [-2.464, 32.710], mean action: 6.143 [0.000, 15.000],  loss: 0.020691, mae: 0.319765, mean_q: 0.493210, mean_eps: 0.000000
 1971/5000: episode: 85, duration: 0.533s, episode steps:  38, steps per second:  71, episode reward: -36.000, mean reward: -0.947 [-32.249,  2.370], mean action: 10.605 [0.000, 21.000],  loss: 0.022801, mae: 0.326033, mean_q: 0.523770, mean_eps: 0.000000
 2000/5000: episode: 86, duration: 0.419s, episode steps:  29, steps per second:  69, episode reward: -34.610, mean reward: -1.193 [-32.171,  2.378], mean action: 4.621 [0.000, 15.000],  loss: 0.022022, mae: 0.323933, mean_q: 0.557555, mean_eps: 0.000000
 2023/5000: episode: 87, duration: 0.347s, episode steps:  23, steps per second:  66, episode reward: 38.545, mean reward:  1.676 [-2.492, 31.991], mean action: 5.913 [0.000, 19.000],  loss: 0.019181, mae: 0.319311, mean_q: 0.523102, mean_eps: 0.000000
 2046/5000: episode: 88, duration: 0.338s, episode steps:  23, steps per second:  68, episode reward: 35.879, mean reward:  1.560 [-2.615, 32.339], mean action: 3.652 [0.000, 15.000],  loss: 0.021981, mae: 0.330335, mean_q: 0.476843, mean_eps: 0.000000
 2060/5000: episode: 89, duration: 0.210s, episode steps:  14, steps per second:  67, episode reward: 38.028, mean reward:  2.716 [-3.000, 33.000], mean action: 3.857 [0.000, 12.000],  loss: 0.023286, mae: 0.324603, mean_q: 0.532618, mean_eps: 0.000000
 2084/5000: episode: 90, duration: 0.394s, episode steps:  24, steps per second:  61, episode reward: 35.279, mean reward:  1.470 [-2.875, 32.210], mean action: 9.125 [0.000, 20.000],  loss: 0.020886, mae: 0.311953, mean_q: 0.591656, mean_eps: 0.000000
 2102/5000: episode: 91, duration: 0.322s, episode steps:  18, steps per second:  56, episode reward: 41.754, mean reward:  2.320 [-2.254, 32.044], mean action: 1.389 [0.000, 11.000],  loss: 0.022260, mae: 0.326697, mean_q: 0.585505, mean_eps: 0.000000
 2112/5000: episode: 92, duration: 0.160s, episode steps:  10, steps per second:  62, episode reward: 42.000, mean reward:  4.200 [-2.669, 30.911], mean action: 5.900 [0.000, 14.000],  loss: 0.018299, mae: 0.306101, mean_q: 0.573806, mean_eps: 0.000000
 2129/5000: episode: 93, duration: 0.263s, episode steps:  17, steps per second:  65, episode reward: 32.632, mean reward:  1.920 [-3.000, 32.182], mean action: 4.882 [0.000, 12.000],  loss: 0.021028, mae: 0.330814, mean_q: 0.612626, mean_eps: 0.000000
 2153/5000: episode: 94, duration: 0.347s, episode steps:  24, steps per second:  69, episode reward: -32.800, mean reward: -1.367 [-33.000,  2.730], mean action: 5.792 [0.000, 14.000],  loss: 0.023531, mae: 0.335749, mean_q: 0.579733, mean_eps: 0.000000
 2177/5000: episode: 95, duration: 0.372s, episode steps:  24, steps per second:  64, episode reward: 34.836, mean reward:  1.452 [-2.653, 32.012], mean action: 5.042 [0.000, 19.000],  loss: 0.020873, mae: 0.328328, mean_q: 0.566493, mean_eps: 0.000000
 2204/5000: episode: 96, duration: 0.401s, episode steps:  27, steps per second:  67, episode reward: -36.000, mean reward: -1.333 [-32.235,  2.910], mean action: 6.926 [0.000, 19.000],  loss: 0.020941, mae: 0.319690, mean_q: 0.497580, mean_eps: 0.000000
 2222/5000: episode: 97, duration: 0.284s, episode steps:  18, steps per second:  63, episode reward: 38.074, mean reward:  2.115 [-2.615, 32.139], mean action: 4.833 [1.000, 19.000],  loss: 0.023975, mae: 0.340315, mean_q: 0.506977, mean_eps: 0.000000
 2245/5000: episode: 98, duration: 0.334s, episode steps:  23, steps per second:  69, episode reward: 32.025, mean reward:  1.392 [-2.727, 32.041], mean action: 4.043 [1.000, 12.000],  loss: 0.022828, mae: 0.325353, mean_q: 0.518251, mean_eps: 0.000000
 2261/5000: episode: 99, duration: 0.247s, episode steps:  16, steps per second:  65, episode reward: 41.230, mean reward:  2.577 [-2.502, 31.810], mean action: 2.938 [0.000, 12.000],  loss: 0.020729, mae: 0.311336, mean_q: 0.506963, mean_eps: 0.000000
 2287/5000: episode: 100, duration: 0.372s, episode steps:  26, steps per second:  70, episode reward: -35.210, mean reward: -1.354 [-31.821,  2.423], mean action: 4.423 [0.000, 19.000],  loss: 0.017524, mae: 0.303565, mean_q: 0.550471, mean_eps: 0.000000
 2305/5000: episode: 101, duration: 0.276s, episode steps:  18, steps per second:  65, episode reward: 38.048, mean reward:  2.114 [-3.000, 32.160], mean action: 2.944 [0.000, 12.000],  loss: 0.018454, mae: 0.315862, mean_q: 0.570001, mean_eps: 0.000000
 2318/5000: episode: 102, duration: 0.209s, episode steps:  13, steps per second:  62, episode reward: 43.382, mean reward:  3.337 [-3.000, 32.136], mean action: 3.615 [0.000, 11.000],  loss: 0.020921, mae: 0.318209, mean_q: 0.520072, mean_eps: 0.000000
 2342/5000: episode: 103, duration: 0.372s, episode steps:  24, steps per second:  64, episode reward: 36.000, mean reward:  1.500 [-2.346, 32.480], mean action: 5.167 [0.000, 19.000],  loss: 0.020214, mae: 0.313861, mean_q: 0.538977, mean_eps: 0.000000
 2358/5000: episode: 104, duration: 0.264s, episode steps:  16, steps per second:  61, episode reward: 36.000, mean reward:  2.250 [-3.000, 30.733], mean action: 5.062 [0.000, 19.000],  loss: 0.020792, mae: 0.315627, mean_q: 0.492705, mean_eps: 0.000000
 2385/5000: episode: 105, duration: 0.557s, episode steps:  27, steps per second:  48, episode reward: 32.890, mean reward:  1.218 [-2.511, 32.090], mean action: 4.704 [0.000, 19.000],  loss: 0.024451, mae: 0.330937, mean_q: 0.524935, mean_eps: 0.000000
 2412/5000: episode: 106, duration: 0.411s, episode steps:  27, steps per second:  66, episode reward: -35.340, mean reward: -1.309 [-32.103,  2.901], mean action: 4.407 [0.000, 14.000],  loss: 0.024795, mae: 0.336917, mean_q: 0.562535, mean_eps: 0.000000
 2433/5000: episode: 107, duration: 0.313s, episode steps:  21, steps per second:  67, episode reward: 38.280, mean reward:  1.823 [-2.331, 32.080], mean action: 3.238 [0.000, 11.000],  loss: 0.019534, mae: 0.314805, mean_q: 0.615509, mean_eps: 0.000000
 2457/5000: episode: 108, duration: 0.374s, episode steps:  24, steps per second:  64, episode reward: 39.000, mean reward:  1.625 [-2.383, 32.820], mean action: 2.792 [0.000, 19.000],  loss: 0.023940, mae: 0.338880, mean_q: 0.522363, mean_eps: 0.000000
 2486/5000: episode: 109, duration: 0.474s, episode steps:  29, steps per second:  61, episode reward: 38.101, mean reward:  1.314 [-2.747, 32.360], mean action: 4.069 [0.000, 19.000],  loss: 0.018447, mae: 0.307446, mean_q: 0.529317, mean_eps: 0.000000
 2504/5000: episode: 110, duration: 0.354s, episode steps:  18, steps per second:  51, episode reward: 35.649, mean reward:  1.980 [-2.269, 32.559], mean action: 5.944 [0.000, 19.000],  loss: 0.023085, mae: 0.333054, mean_q: 0.540750, mean_eps: 0.000000
 2524/5000: episode: 111, duration: 0.336s, episode steps:  20, steps per second:  60, episode reward: 35.734, mean reward:  1.787 [-2.902, 31.955], mean action: 5.050 [0.000, 19.000],  loss: 0.016789, mae: 0.299089, mean_q: 0.592738, mean_eps: 0.000000
 2542/5000: episode: 112, duration: 0.294s, episode steps:  18, steps per second:  61, episode reward: 38.393, mean reward:  2.133 [-2.383, 32.420], mean action: 4.500 [0.000, 19.000],  loss: 0.024721, mae: 0.338678, mean_q: 0.510096, mean_eps: 0.000000
 2554/5000: episode: 113, duration: 0.194s, episode steps:  12, steps per second:  62, episode reward: 42.764, mean reward:  3.564 [-2.467, 32.611], mean action: 4.917 [0.000, 19.000],  loss: 0.018392, mae: 0.300737, mean_q: 0.450186, mean_eps: 0.000000
 2576/5000: episode: 114, duration: 0.350s, episode steps:  22, steps per second:  63, episode reward: 35.889, mean reward:  1.631 [-2.622, 32.090], mean action: 4.409 [0.000, 19.000],  loss: 0.019832, mae: 0.309187, mean_q: 0.498857, mean_eps: 0.000000
 2598/5000: episode: 115, duration: 0.352s, episode steps:  22, steps per second:  62, episode reward: -35.270, mean reward: -1.603 [-32.186,  2.125], mean action: 5.682 [0.000, 19.000],  loss: 0.019635, mae: 0.308048, mean_q: 0.525706, mean_eps: 0.000000
 2616/5000: episode: 116, duration: 0.292s, episode steps:  18, steps per second:  62, episode reward: 39.000, mean reward:  2.167 [-2.267, 32.130], mean action: 4.167 [0.000, 19.000],  loss: 0.022474, mae: 0.329680, mean_q: 0.469064, mean_eps: 0.000000
 2648/5000: episode: 117, duration: 0.483s, episode steps:  32, steps per second:  66, episode reward: -32.730, mean reward: -1.023 [-31.842,  2.584], mean action: 8.719 [1.000, 19.000],  loss: 0.021956, mae: 0.329275, mean_q: 0.536093, mean_eps: 0.000000
 2667/5000: episode: 118, duration: 0.278s, episode steps:  19, steps per second:  68, episode reward: -33.000, mean reward: -1.737 [-30.900,  2.360], mean action: 5.579 [0.000, 19.000],  loss: 0.021507, mae: 0.331698, mean_q: 0.601691, mean_eps: 0.000000
 2699/5000: episode: 119, duration: 0.502s, episode steps:  32, steps per second:  64, episode reward: -32.990, mean reward: -1.031 [-32.040,  2.281], mean action: 9.438 [0.000, 20.000],  loss: 0.020089, mae: 0.318899, mean_q: 0.558048, mean_eps: 0.000000
 2738/5000: episode: 120, duration: 0.549s, episode steps:  39, steps per second:  71, episode reward: 38.401, mean reward:  0.985 [-2.457, 32.599], mean action: 3.282 [0.000, 16.000],  loss: 0.021660, mae: 0.325536, mean_q: 0.597518, mean_eps: 0.000000
 2761/5000: episode: 121, duration: 0.368s, episode steps:  23, steps per second:  63, episode reward: 44.245, mean reward:  1.924 [-2.374, 33.000], mean action: 4.435 [0.000, 16.000],  loss: 0.021887, mae: 0.325466, mean_q: 0.521143, mean_eps: 0.000000
 2778/5000: episode: 122, duration: 0.252s, episode steps:  17, steps per second:  67, episode reward: 35.769, mean reward:  2.104 [-3.000, 33.000], mean action: 4.882 [0.000, 16.000],  loss: 0.020747, mae: 0.318228, mean_q: 0.574249, mean_eps: 0.000000
 2797/5000: episode: 123, duration: 0.282s, episode steps:  19, steps per second:  67, episode reward: -36.000, mean reward: -1.895 [-32.390,  3.000], mean action: 5.684 [0.000, 16.000],  loss: 0.016295, mae: 0.303071, mean_q: 0.523799, mean_eps: 0.000000
 2821/5000: episode: 124, duration: 0.360s, episode steps:  24, steps per second:  67, episode reward: 32.356, mean reward:  1.348 [-3.000, 31.566], mean action: 7.042 [0.000, 16.000],  loss: 0.019698, mae: 0.316174, mean_q: 0.492429, mean_eps: 0.000000
 2846/5000: episode: 125, duration: 0.367s, episode steps:  25, steps per second:  68, episode reward: -32.620, mean reward: -1.305 [-32.229,  2.559], mean action: 5.600 [0.000, 14.000],  loss: 0.020768, mae: 0.323105, mean_q: 0.447769, mean_eps: 0.000000
 2864/5000: episode: 126, duration: 0.266s, episode steps:  18, steps per second:  68, episode reward: 35.597, mean reward:  1.978 [-2.455, 32.120], mean action: 3.444 [0.000, 12.000],  loss: 0.018661, mae: 0.314320, mean_q: 0.568524, mean_eps: 0.000000
 2896/5000: episode: 127, duration: 0.466s, episode steps:  32, steps per second:  69, episode reward: 32.226, mean reward:  1.007 [-2.807, 32.064], mean action: 3.906 [0.000, 19.000],  loss: 0.024088, mae: 0.330269, mean_q: 0.517240, mean_eps: 0.000000
 2917/5000: episode: 128, duration: 0.301s, episode steps:  21, steps per second:  70, episode reward: 38.679, mean reward:  1.842 [-2.230, 33.000], mean action: 2.524 [0.000, 15.000],  loss: 0.024346, mae: 0.330423, mean_q: 0.538893, mean_eps: 0.000000
 2940/5000: episode: 129, duration: 0.370s, episode steps:  23, steps per second:  62, episode reward: 38.623, mean reward:  1.679 [-2.185, 32.093], mean action: 2.870 [0.000, 15.000],  loss: 0.021159, mae: 0.318114, mean_q: 0.535157, mean_eps: 0.000000
 2970/5000: episode: 130, duration: 0.459s, episode steps:  30, steps per second:  65, episode reward: 42.000, mean reward:  1.400 [-2.262, 32.060], mean action: 1.800 [0.000, 9.000],  loss: 0.022177, mae: 0.323409, mean_q: 0.511639, mean_eps: 0.000000
 3020/5000: episode: 131, duration: 0.715s, episode steps:  50, steps per second:  70, episode reward: -36.000, mean reward: -0.720 [-33.000,  2.340], mean action: 4.820 [0.000, 15.000],  loss: 0.021467, mae: 0.319125, mean_q: 0.497618, mean_eps: 0.000000
 3046/5000: episode: 132, duration: 0.373s, episode steps:  26, steps per second:  70, episode reward: 38.796, mean reward:  1.492 [-2.316, 32.296], mean action: 2.923 [0.000, 19.000],  loss: 0.021842, mae: 0.318304, mean_q: 0.561492, mean_eps: 0.000000
 3073/5000: episode: 133, duration: 0.392s, episode steps:  27, steps per second:  69, episode reward: 34.410, mean reward:  1.274 [-3.000, 32.481], mean action: 4.037 [0.000, 19.000],  loss: 0.022211, mae: 0.321560, mean_q: 0.555467, mean_eps: 0.000000
 3099/5000: episode: 134, duration: 0.386s, episode steps:  26, steps per second:  67, episode reward: 36.000, mean reward:  1.385 [-2.375, 32.250], mean action: 4.885 [0.000, 19.000],  loss: 0.020752, mae: 0.327668, mean_q: 0.515522, mean_eps: 0.000000
 3123/5000: episode: 135, duration: 0.353s, episode steps:  24, steps per second:  68, episode reward: 38.114, mean reward:  1.588 [-2.807, 31.626], mean action: 4.250 [0.000, 19.000],  loss: 0.021511, mae: 0.325425, mean_q: 0.504639, mean_eps: 0.000000
 3149/5000: episode: 136, duration: 0.405s, episode steps:  26, steps per second:  64, episode reward: 40.794, mean reward:  1.569 [-2.191, 32.240], mean action: 1.654 [0.000, 19.000],  loss: 0.022408, mae: 0.320779, mean_q: 0.548868, mean_eps: 0.000000
 3169/5000: episode: 137, duration: 0.298s, episode steps:  20, steps per second:  67, episode reward: 38.586, mean reward:  1.929 [-3.000, 32.230], mean action: 3.800 [0.000, 19.000],  loss: 0.023167, mae: 0.321549, mean_q: 0.585132, mean_eps: 0.000000
 3187/5000: episode: 138, duration: 0.291s, episode steps:  18, steps per second:  62, episode reward: 37.943, mean reward:  2.108 [-2.904, 32.730], mean action: 5.167 [0.000, 19.000],  loss: 0.018961, mae: 0.304417, mean_q: 0.554165, mean_eps: 0.000000
 3210/5000: episode: 139, duration: 0.343s, episode steps:  23, steps per second:  67, episode reward: 38.311, mean reward:  1.666 [-2.430, 32.371], mean action: 5.522 [0.000, 19.000],  loss: 0.022023, mae: 0.320665, mean_q: 0.527404, mean_eps: 0.000000
 3233/5000: episode: 140, duration: 0.393s, episode steps:  23, steps per second:  58, episode reward: -39.000, mean reward: -1.696 [-33.000,  2.667], mean action: 10.304 [1.000, 19.000],  loss: 0.019864, mae: 0.314182, mean_q: 0.493272, mean_eps: 0.000000
 3254/5000: episode: 141, duration: 0.351s, episode steps:  21, steps per second:  60, episode reward: 38.354, mean reward:  1.826 [-2.494, 32.250], mean action: 5.714 [0.000, 19.000],  loss: 0.019542, mae: 0.308415, mean_q: 0.507549, mean_eps: 0.000000
 3277/5000: episode: 142, duration: 0.342s, episode steps:  23, steps per second:  67, episode reward: -35.010, mean reward: -1.522 [-32.146,  2.630], mean action: 7.130 [0.000, 20.000],  loss: 0.020030, mae: 0.309556, mean_q: 0.547674, mean_eps: 0.000000
 3297/5000: episode: 143, duration: 0.306s, episode steps:  20, steps per second:  65, episode reward: 33.000, mean reward:  1.650 [-2.488, 32.140], mean action: 4.850 [0.000, 19.000],  loss: 0.020231, mae: 0.315856, mean_q: 0.513074, mean_eps: 0.000000
 3317/5000: episode: 144, duration: 0.309s, episode steps:  20, steps per second:  65, episode reward: 38.665, mean reward:  1.933 [-2.563, 32.100], mean action: 2.450 [0.000, 12.000],  loss: 0.022718, mae: 0.331028, mean_q: 0.562344, mean_eps: 0.000000
 3337/5000: episode: 145, duration: 0.325s, episode steps:  20, steps per second:  62, episode reward: 35.938, mean reward:  1.797 [-2.753, 32.090], mean action: 4.200 [1.000, 19.000],  loss: 0.017764, mae: 0.313583, mean_q: 0.537599, mean_eps: 0.000000
 3354/5000: episode: 146, duration: 0.273s, episode steps:  17, steps per second:  62, episode reward: 39.000, mean reward:  2.294 [-2.464, 32.310], mean action: 3.059 [0.000, 9.000],  loss: 0.021874, mae: 0.338597, mean_q: 0.534926, mean_eps: 0.000000
 3374/5000: episode: 147, duration: 0.297s, episode steps:  20, steps per second:  67, episode reward: 39.000, mean reward:  1.950 [-2.368, 33.000], mean action: 4.350 [0.000, 15.000],  loss: 0.022179, mae: 0.329184, mean_q: 0.536972, mean_eps: 0.000000
 3397/5000: episode: 148, duration: 11.159s, episode steps:  23, steps per second:   2, episode reward: 41.147, mean reward:  1.789 [-2.283, 32.131], mean action: 4.043 [0.000, 15.000],  loss: 0.020447, mae: 0.318872, mean_q: 0.564244, mean_eps: 0.000000
 3414/5000: episode: 149, duration: 0.262s, episode steps:  17, steps per second:  65, episode reward: 44.094, mean reward:  2.594 [-2.239, 31.793], mean action: 1.706 [0.000, 11.000],  loss: 0.020075, mae: 0.323056, mean_q: 0.587904, mean_eps: 0.000000
 3432/5000: episode: 150, duration: 0.278s, episode steps:  18, steps per second:  65, episode reward: -35.350, mean reward: -1.964 [-31.875,  3.000], mean action: 3.667 [0.000, 11.000],  loss: 0.016649, mae: 0.297570, mean_q: 0.536082, mean_eps: 0.000000
 3487/5000: episode: 151, duration: 0.775s, episode steps:  55, steps per second:  71, episode reward: -33.000, mean reward: -0.600 [-32.098,  2.360], mean action: 7.945 [0.000, 20.000],  loss: 0.019284, mae: 0.313047, mean_q: 0.552221, mean_eps: 0.000000
 3505/5000: episode: 152, duration: 0.283s, episode steps:  18, steps per second:  64, episode reward: 38.661, mean reward:  2.148 [-3.000, 32.120], mean action: 5.389 [0.000, 12.000],  loss: 0.020600, mae: 0.327741, mean_q: 0.609346, mean_eps: 0.000000
 3527/5000: episode: 153, duration: 0.355s, episode steps:  22, steps per second:  62, episode reward: 38.455, mean reward:  1.748 [-2.594, 33.000], mean action: 2.636 [0.000, 11.000],  loss: 0.021585, mae: 0.335260, mean_q: 0.558463, mean_eps: 0.000000
 3542/5000: episode: 154, duration: 0.227s, episode steps:  15, steps per second:  66, episode reward: 47.862, mean reward:  3.191 [ 0.000, 33.000], mean action: 3.667 [1.000, 15.000],  loss: 0.021718, mae: 0.327249, mean_q: 0.586780, mean_eps: 0.000000
 3562/5000: episode: 155, duration: 0.316s, episode steps:  20, steps per second:  63, episode reward: 40.568, mean reward:  2.028 [-2.402, 32.110], mean action: 4.450 [0.000, 15.000],  loss: 0.018748, mae: 0.309180, mean_q: 0.629526, mean_eps: 0.000000
 3587/5000: episode: 156, duration: 0.388s, episode steps:  25, steps per second:  64, episode reward: 32.582, mean reward:  1.303 [-2.515, 31.858], mean action: 3.360 [0.000, 12.000],  loss: 0.023026, mae: 0.334359, mean_q: 0.625363, mean_eps: 0.000000
 3605/5000: episode: 157, duration: 0.264s, episode steps:  18, steps per second:  68, episode reward: 39.000, mean reward:  2.167 [-2.823, 32.100], mean action: 4.444 [2.000, 10.000],  loss: 0.022657, mae: 0.336560, mean_q: 0.623621, mean_eps: 0.000000
 3622/5000: episode: 158, duration: 0.251s, episode steps:  17, steps per second:  68, episode reward: 36.000, mean reward:  2.118 [-2.328, 32.720], mean action: 5.412 [0.000, 16.000],  loss: 0.023661, mae: 0.344524, mean_q: 0.568003, mean_eps: 0.000000
 3642/5000: episode: 159, duration: 0.295s, episode steps:  20, steps per second:  68, episode reward: 36.000, mean reward:  1.800 [-2.428, 33.000], mean action: 6.350 [0.000, 16.000],  loss: 0.020997, mae: 0.330207, mean_q: 0.536361, mean_eps: 0.000000
 3677/5000: episode: 160, duration: 0.504s, episode steps:  35, steps per second:  69, episode reward: 32.678, mean reward:  0.934 [-2.309, 31.941], mean action: 3.886 [0.000, 16.000],  loss: 0.019914, mae: 0.315262, mean_q: 0.547774, mean_eps: 0.000000
 3715/5000: episode: 161, duration: 0.541s, episode steps:  38, steps per second:  70, episode reward: 33.000, mean reward:  0.868 [-2.361, 32.250], mean action: 6.816 [0.000, 14.000],  loss: 0.019539, mae: 0.308262, mean_q: 0.524142, mean_eps: 0.000000
 3749/5000: episode: 162, duration: 0.476s, episode steps:  34, steps per second:  71, episode reward: 38.362, mean reward:  1.128 [-3.000, 32.239], mean action: 3.235 [0.000, 18.000],  loss: 0.017109, mae: 0.296283, mean_q: 0.549621, mean_eps: 0.000000
 3763/5000: episode: 163, duration: 0.238s, episode steps:  14, steps per second:  59, episode reward: 44.312, mean reward:  3.165 [-2.243, 31.886], mean action: 1.429 [0.000, 9.000],  loss: 0.022884, mae: 0.324292, mean_q: 0.505538, mean_eps: 0.000000
 3792/5000: episode: 164, duration: 0.427s, episode steps:  29, steps per second:  68, episode reward: 38.205, mean reward:  1.317 [-2.412, 32.130], mean action: 2.828 [0.000, 12.000],  loss: 0.025297, mae: 0.339102, mean_q: 0.456226, mean_eps: 0.000000
 3812/5000: episode: 165, duration: 0.306s, episode steps:  20, steps per second:  65, episode reward: 44.403, mean reward:  2.220 [-2.471, 32.150], mean action: 1.750 [1.000, 12.000],  loss: 0.017210, mae: 0.298954, mean_q: 0.533185, mean_eps: 0.000000
 3830/5000: episode: 166, duration: 0.274s, episode steps:  18, steps per second:  66, episode reward: 39.000, mean reward:  2.167 [-3.000, 32.800], mean action: 3.833 [0.000, 14.000],  loss: 0.023284, mae: 0.333719, mean_q: 0.567369, mean_eps: 0.000000
 3848/5000: episode: 167, duration: 0.269s, episode steps:  18, steps per second:  67, episode reward: 38.494, mean reward:  2.139 [-2.486, 32.080], mean action: 3.611 [0.000, 12.000],  loss: 0.021572, mae: 0.324113, mean_q: 0.492282, mean_eps: 0.000000
 3892/5000: episode: 168, duration: 0.619s, episode steps:  44, steps per second:  71, episode reward: 32.735, mean reward:  0.744 [-3.000, 32.150], mean action: 3.591 [0.000, 19.000],  loss: 0.021198, mae: 0.319398, mean_q: 0.555909, mean_eps: 0.000000
 3918/5000: episode: 169, duration: 0.407s, episode steps:  26, steps per second:  64, episode reward: 32.419, mean reward:  1.247 [-3.000, 32.894], mean action: 5.231 [0.000, 19.000],  loss: 0.022755, mae: 0.329337, mean_q: 0.552890, mean_eps: 0.000000
 3932/5000: episode: 170, duration: 0.214s, episode steps:  14, steps per second:  65, episode reward: 39.000, mean reward:  2.786 [-2.315, 32.290], mean action: 5.286 [0.000, 19.000],  loss: 0.021257, mae: 0.326080, mean_q: 0.503370, mean_eps: 0.000000
 3959/5000: episode: 171, duration: 0.404s, episode steps:  27, steps per second:  67, episode reward: -38.220, mean reward: -1.416 [-32.260,  2.591], mean action: 7.222 [0.000, 19.000],  loss: 0.019491, mae: 0.312301, mean_q: 0.533380, mean_eps: 0.000000
 3979/5000: episode: 172, duration: 0.303s, episode steps:  20, steps per second:  66, episode reward: 36.000, mean reward:  1.800 [-3.000, 30.143], mean action: 3.450 [0.000, 19.000],  loss: 0.021419, mae: 0.327790, mean_q: 0.543285, mean_eps: 0.000000
 4008/5000: episode: 173, duration: 0.440s, episode steps:  29, steps per second:  66, episode reward: 38.388, mean reward:  1.324 [-3.000, 32.141], mean action: 3.621 [1.000, 12.000],  loss: 0.020785, mae: 0.324866, mean_q: 0.524977, mean_eps: 0.000000
 4040/5000: episode: 174, duration: 0.460s, episode steps:  32, steps per second:  70, episode reward: 38.650, mean reward:  1.208 [-2.420, 32.100], mean action: 2.656 [0.000, 9.000],  loss: 0.020646, mae: 0.322660, mean_q: 0.541047, mean_eps: 0.000000
 4062/5000: episode: 175, duration: 0.320s, episode steps:  22, steps per second:  69, episode reward: 33.000, mean reward:  1.500 [-2.314, 33.000], mean action: 3.318 [0.000, 16.000],  loss: 0.018402, mae: 0.308471, mean_q: 0.537919, mean_eps: 0.000000
 4076/5000: episode: 176, duration: 0.326s, episode steps:  14, steps per second:  43, episode reward: 43.402, mean reward:  3.100 [-2.221, 33.000], mean action: 4.071 [0.000, 14.000],  loss: 0.027276, mae: 0.350841, mean_q: 0.515356, mean_eps: 0.000000
 4094/5000: episode: 177, duration: 0.388s, episode steps:  18, steps per second:  46, episode reward: 38.325, mean reward:  2.129 [-2.509, 32.220], mean action: 3.889 [0.000, 14.000],  loss: 0.022883, mae: 0.327573, mean_q: 0.536516, mean_eps: 0.000000
 4108/5000: episode: 178, duration: 0.240s, episode steps:  14, steps per second:  58, episode reward: 41.489, mean reward:  2.964 [-3.000, 31.744], mean action: 3.500 [0.000, 11.000],  loss: 0.021578, mae: 0.317944, mean_q: 0.502771, mean_eps: 0.000000
 4127/5000: episode: 179, duration: 0.283s, episode steps:  19, steps per second:  67, episode reward: -38.180, mean reward: -2.009 [-32.206,  2.310], mean action: 4.421 [0.000, 16.000],  loss: 0.017378, mae: 0.299552, mean_q: 0.507535, mean_eps: 0.000000
 4148/5000: episode: 180, duration: 0.311s, episode steps:  21, steps per second:  68, episode reward: 36.000, mean reward:  1.714 [-3.000, 32.460], mean action: 4.286 [0.000, 16.000],  loss: 0.018063, mae: 0.301857, mean_q: 0.510723, mean_eps: 0.000000
 4173/5000: episode: 181, duration: 0.364s, episode steps:  25, steps per second:  69, episode reward: 35.806, mean reward:  1.432 [-2.728, 33.000], mean action: 7.280 [0.000, 20.000],  loss: 0.020278, mae: 0.312673, mean_q: 0.519133, mean_eps: 0.000000
 4202/5000: episode: 182, duration: 0.410s, episode steps:  29, steps per second:  71, episode reward: 35.852, mean reward:  1.236 [-2.171, 32.260], mean action: 4.000 [0.000, 18.000],  loss: 0.022207, mae: 0.322979, mean_q: 0.506885, mean_eps: 0.000000
 4225/5000: episode: 183, duration: 0.343s, episode steps:  23, steps per second:  67, episode reward: 37.663, mean reward:  1.638 [-2.247, 32.739], mean action: 5.826 [0.000, 15.000],  loss: 0.019336, mae: 0.319544, mean_q: 0.480150, mean_eps: 0.000000
 4246/5000: episode: 184, duration: 0.338s, episode steps:  21, steps per second:  62, episode reward: -41.250, mean reward: -1.964 [-32.149,  2.490], mean action: 8.286 [0.000, 16.000],  loss: 0.021300, mae: 0.326106, mean_q: 0.487961, mean_eps: 0.000000
 4270/5000: episode: 185, duration: 0.353s, episode steps:  24, steps per second:  68, episode reward: 41.444, mean reward:  1.727 [-2.941, 31.997], mean action: 6.875 [1.000, 15.000],  loss: 0.017712, mae: 0.305198, mean_q: 0.444889, mean_eps: 0.000000
 4289/5000: episode: 186, duration: 0.305s, episode steps:  19, steps per second:  62, episode reward: 41.292, mean reward:  2.173 [-2.448, 32.221], mean action: 3.158 [0.000, 12.000],  loss: 0.021104, mae: 0.316350, mean_q: 0.466067, mean_eps: 0.000000
 4312/5000: episode: 187, duration: 0.334s, episode steps:  23, steps per second:  69, episode reward: 32.144, mean reward:  1.398 [-3.000, 32.311], mean action: 5.957 [0.000, 18.000],  loss: 0.023018, mae: 0.321444, mean_q: 0.503347, mean_eps: 0.000000
 4331/5000: episode: 188, duration: 0.295s, episode steps:  19, steps per second:  64, episode reward: 41.163, mean reward:  2.166 [-2.169, 32.270], mean action: 2.526 [0.000, 16.000],  loss: 0.018356, mae: 0.294206, mean_q: 0.509527, mean_eps: 0.000000
 4344/5000: episode: 189, duration: 0.208s, episode steps:  13, steps per second:  63, episode reward: 47.577, mean reward:  3.660 [ 0.000, 33.059], mean action: 2.231 [1.000, 3.000],  loss: 0.021020, mae: 0.309197, mean_q: 0.526457, mean_eps: 0.000000
 4364/5000: episode: 190, duration: 0.300s, episode steps:  20, steps per second:  67, episode reward: 36.000, mean reward:  1.800 [-2.860, 32.440], mean action: 5.100 [0.000, 15.000],  loss: 0.019010, mae: 0.301424, mean_q: 0.490799, mean_eps: 0.000000
 4397/5000: episode: 191, duration: 0.468s, episode steps:  33, steps per second:  71, episode reward: 35.005, mean reward:  1.061 [-2.276, 31.869], mean action: 3.970 [0.000, 14.000],  loss: 0.023497, mae: 0.320739, mean_q: 0.548204, mean_eps: 0.000000
 4413/5000: episode: 192, duration: 0.248s, episode steps:  16, steps per second:  65, episode reward: 38.904, mean reward:  2.431 [-3.000, 32.604], mean action: 3.250 [0.000, 11.000],  loss: 0.020009, mae: 0.311875, mean_q: 0.590583, mean_eps: 0.000000
 4437/5000: episode: 193, duration: 0.345s, episode steps:  24, steps per second:  70, episode reward: 33.000, mean reward:  1.375 [-2.192, 32.010], mean action: 3.125 [0.000, 12.000],  loss: 0.020339, mae: 0.310883, mean_q: 0.568786, mean_eps: 0.000000
 4457/5000: episode: 194, duration: 0.309s, episode steps:  20, steps per second:  65, episode reward: 35.628, mean reward:  1.781 [-2.904, 32.020], mean action: 5.050 [0.000, 14.000],  loss: 0.020185, mae: 0.312982, mean_q: 0.517866, mean_eps: 0.000000
 4506/5000: episode: 195, duration: 0.717s, episode steps:  49, steps per second:  68, episode reward: -33.000, mean reward: -0.673 [-30.137,  2.391], mean action: 3.633 [0.000, 16.000],  loss: 0.019365, mae: 0.311623, mean_q: 0.475861, mean_eps: 0.000000
 4565/5000: episode: 196, duration: 0.817s, episode steps:  59, steps per second:  72, episode reward: 33.000, mean reward:  0.559 [-2.385, 32.130], mean action: 10.983 [1.000, 21.000],  loss: 0.020845, mae: 0.314968, mean_q: 0.489989, mean_eps: 0.000000
 4592/5000: episode: 197, duration: 0.399s, episode steps:  27, steps per second:  68, episode reward: 32.491, mean reward:  1.203 [-3.000, 32.160], mean action: 6.778 [0.000, 19.000],  loss: 0.021478, mae: 0.316558, mean_q: 0.540606, mean_eps: 0.000000
 4617/5000: episode: 198, duration: 0.372s, episode steps:  25, steps per second:  67, episode reward: -32.580, mean reward: -1.303 [-33.000,  3.000], mean action: 6.480 [0.000, 19.000],  loss: 0.019795, mae: 0.315729, mean_q: 0.588931, mean_eps: 0.000000
 4633/5000: episode: 199, duration: 0.249s, episode steps:  16, steps per second:  64, episode reward: 44.375, mean reward:  2.773 [-2.443, 32.080], mean action: 2.625 [0.000, 15.000],  loss: 0.021490, mae: 0.317305, mean_q: 0.530577, mean_eps: 0.000000
 4647/5000: episode: 200, duration: 0.216s, episode steps:  14, steps per second:  65, episode reward: 42.000, mean reward:  3.000 [-2.313, 32.510], mean action: 4.714 [1.000, 15.000],  loss: 0.019292, mae: 0.308801, mean_q: 0.502442, mean_eps: 0.000000
 4669/5000: episode: 201, duration: 0.323s, episode steps:  22, steps per second:  68, episode reward: -32.420, mean reward: -1.474 [-32.203,  2.666], mean action: 6.773 [0.000, 15.000],  loss: 0.019568, mae: 0.312302, mean_q: 0.480637, mean_eps: 0.000000
 4686/5000: episode: 202, duration: 0.263s, episode steps:  17, steps per second:  65, episode reward: 38.176, mean reward:  2.246 [-2.770, 32.340], mean action: 6.235 [0.000, 15.000],  loss: 0.023455, mae: 0.325309, mean_q: 0.506667, mean_eps: 0.000000
 4719/5000: episode: 203, duration: 0.499s, episode steps:  33, steps per second:  66, episode reward: 32.167, mean reward:  0.975 [-2.904, 32.390], mean action: 9.788 [0.000, 21.000],  loss: 0.021159, mae: 0.319128, mean_q: 0.543409, mean_eps: 0.000000
 4742/5000: episode: 204, duration: 0.350s, episode steps:  23, steps per second:  66, episode reward: -32.580, mean reward: -1.417 [-32.580,  2.382], mean action: 6.435 [0.000, 15.000],  loss: 0.019594, mae: 0.309629, mean_q: 0.505723, mean_eps: 0.000000
 4769/5000: episode: 205, duration: 0.417s, episode steps:  27, steps per second:  65, episode reward: 35.609, mean reward:  1.319 [-2.461, 33.000], mean action: 4.963 [0.000, 15.000],  loss: 0.020493, mae: 0.307442, mean_q: 0.518833, mean_eps: 0.000000
 4783/5000: episode: 206, duration: 0.225s, episode steps:  14, steps per second:  62, episode reward: 44.901, mean reward:  3.207 [-2.836, 32.611], mean action: 2.571 [0.000, 3.000],  loss: 0.019364, mae: 0.297507, mean_q: 0.520856, mean_eps: 0.000000
 4811/5000: episode: 207, duration: 0.399s, episode steps:  28, steps per second:  70, episode reward: 32.394, mean reward:  1.157 [-3.000, 32.130], mean action: 6.500 [0.000, 21.000],  loss: 0.021284, mae: 0.310676, mean_q: 0.504445, mean_eps: 0.000000
 4829/5000: episode: 208, duration: 0.281s, episode steps:  18, steps per second:  64, episode reward: 41.898, mean reward:  2.328 [-2.178, 31.982], mean action: 2.944 [0.000, 15.000],  loss: 0.022963, mae: 0.317302, mean_q: 0.566460, mean_eps: 0.000000
 4863/5000: episode: 209, duration: 0.483s, episode steps:  34, steps per second:  70, episode reward: 33.000, mean reward:  0.971 [-2.702, 32.150], mean action: 3.765 [0.000, 15.000],  loss: 0.018073, mae: 0.302608, mean_q: 0.548153, mean_eps: 0.000000
 4884/5000: episode: 210, duration: 0.329s, episode steps:  21, steps per second:  64, episode reward: 36.000, mean reward:  1.714 [-2.475, 33.000], mean action: 3.667 [0.000, 15.000],  loss: 0.018898, mae: 0.309480, mean_q: 0.491909, mean_eps: 0.000000
 4901/5000: episode: 211, duration: 0.269s, episode steps:  17, steps per second:  63, episode reward: 44.278, mean reward:  2.605 [-2.090, 32.310], mean action: 1.941 [0.000, 15.000],  loss: 0.019112, mae: 0.302225, mean_q: 0.532474, mean_eps: 0.000000
 4923/5000: episode: 212, duration: 0.330s, episode steps:  22, steps per second:  67, episode reward: 33.000, mean reward:  1.500 [-2.446, 29.994], mean action: 3.818 [0.000, 19.000],  loss: 0.022481, mae: 0.317839, mean_q: 0.507966, mean_eps: 0.000000
 4943/5000: episode: 213, duration: 0.299s, episode steps:  20, steps per second:  67, episode reward: 44.236, mean reward:  2.212 [-2.088, 32.350], mean action: 1.250 [0.000, 15.000],  loss: 0.019264, mae: 0.305997, mean_q: 0.504944, mean_eps: 0.000000
 4967/5000: episode: 214, duration: 0.364s, episode steps:  24, steps per second:  66, episode reward: 38.352, mean reward:  1.598 [-2.513, 32.220], mean action: 4.708 [0.000, 15.000],  loss: 0.018794, mae: 0.299996, mean_q: 0.501303, mean_eps: 0.000000
 4991/5000: episode: 215, duration: 0.355s, episode steps:  24, steps per second:  68, episode reward: 35.779, mean reward:  1.491 [-2.319, 32.499], mean action: 3.250 [1.000, 15.000],  loss: 0.019582, mae: 0.305684, mean_q: 0.474284, mean_eps: 0.000000
done, took 79.314 seconds
DQN Evaluation: 12972 victories out of 15095 episodes
Training for 5000 steps ...
   34/5000: episode: 1, duration: 0.294s, episode steps:  34, steps per second: 116, episode reward: 43.965, mean reward:  1.293 [-2.031, 32.230], mean action: 3.735 [1.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   57/5000: episode: 2, duration: 0.182s, episode steps:  23, steps per second: 127, episode reward: 44.374, mean reward:  1.929 [-2.060, 32.340], mean action: 3.696 [1.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  107/5000: episode: 3, duration: 0.328s, episode steps:  50, steps per second: 152, episode reward: -36.450, mean reward: -0.729 [-32.013,  2.183], mean action: 3.520 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  134/5000: episode: 4, duration: 0.193s, episode steps:  27, steps per second: 140, episode reward: 42.000, mean reward:  1.556 [-2.465, 32.440], mean action: 2.926 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  156/5000: episode: 5, duration: 0.164s, episode steps:  22, steps per second: 134, episode reward: 44.723, mean reward:  2.033 [-2.265, 32.340], mean action: 2.318 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  176/5000: episode: 6, duration: 0.150s, episode steps:  20, steps per second: 133, episode reward: 44.345, mean reward:  2.217 [-2.201, 31.901], mean action: 1.850 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  197/5000: episode: 7, duration: 0.163s, episode steps:  21, steps per second: 129, episode reward: 44.471, mean reward:  2.118 [-2.372, 32.150], mean action: 1.810 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  220/5000: episode: 8, duration: 0.179s, episode steps:  23, steps per second: 128, episode reward: 39.000, mean reward:  1.696 [-3.000, 32.340], mean action: 3.739 [1.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  271/5000: episode: 9, duration: 0.330s, episode steps:  51, steps per second: 154, episode reward: 38.670, mean reward:  0.758 [-2.326, 32.940], mean action: 5.059 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  294/5000: episode: 10, duration: 0.164s, episode steps:  23, steps per second: 140, episode reward: 41.843, mean reward:  1.819 [-2.149, 32.843], mean action: 3.000 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  320/5000: episode: 11, duration: 0.189s, episode steps:  26, steps per second: 138, episode reward: 38.315, mean reward:  1.474 [-3.000, 32.042], mean action: 3.462 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  341/5000: episode: 12, duration: 0.168s, episode steps:  21, steps per second: 125, episode reward: 44.066, mean reward:  2.098 [-2.442, 32.100], mean action: 2.476 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  370/5000: episode: 13, duration: 0.197s, episode steps:  29, steps per second: 147, episode reward: 37.360, mean reward:  1.288 [-3.000, 31.542], mean action: 5.586 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  382/5000: episode: 14, duration: 0.115s, episode steps:  12, steps per second: 104, episode reward: 45.000, mean reward:  3.750 [-3.000, 32.420], mean action: 1.667 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  413/5000: episode: 15, duration: 0.215s, episode steps:  31, steps per second: 144, episode reward: 47.140, mean reward:  1.521 [-0.527, 32.500], mean action: 2.161 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  447/5000: episode: 16, duration: 0.222s, episode steps:  34, steps per second: 153, episode reward: 42.000, mean reward:  1.235 [-2.233, 32.410], mean action: 3.235 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  473/5000: episode: 17, duration: 0.195s, episode steps:  26, steps per second: 134, episode reward: 44.449, mean reward:  1.710 [-2.187, 32.530], mean action: 2.231 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  493/5000: episode: 18, duration: 0.163s, episode steps:  20, steps per second: 123, episode reward: 47.364, mean reward:  2.368 [-0.105, 32.060], mean action: 2.000 [1.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  523/5000: episode: 19, duration: 0.202s, episode steps:  30, steps per second: 149, episode reward: 47.781, mean reward:  1.593 [-0.050, 32.290], mean action: 2.267 [1.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  545/5000: episode: 20, duration: 0.170s, episode steps:  22, steps per second: 130, episode reward: 44.629, mean reward:  2.029 [-2.212, 32.347], mean action: 2.500 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  571/5000: episode: 21, duration: 0.191s, episode steps:  26, steps per second: 136, episode reward: 41.564, mean reward:  1.599 [-2.622, 32.058], mean action: 1.462 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  603/5000: episode: 22, duration: 0.212s, episode steps:  32, steps per second: 151, episode reward: 38.244, mean reward:  1.195 [-2.666, 32.085], mean action: 4.250 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  634/5000: episode: 23, duration: 0.208s, episode steps:  31, steps per second: 149, episode reward: 38.298, mean reward:  1.235 [-2.467, 31.991], mean action: 5.129 [1.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  658/5000: episode: 24, duration: 0.175s, episode steps:  24, steps per second: 137, episode reward: 38.549, mean reward:  1.606 [-3.000, 31.942], mean action: 3.917 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  682/5000: episode: 25, duration: 0.183s, episode steps:  24, steps per second: 131, episode reward: 44.121, mean reward:  1.838 [-2.165, 32.240], mean action: 3.875 [2.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  712/5000: episode: 26, duration: 0.211s, episode steps:  30, steps per second: 142, episode reward: 37.782, mean reward:  1.259 [-2.416, 31.891], mean action: 4.933 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  734/5000: episode: 27, duration: 0.172s, episode steps:  22, steps per second: 128, episode reward: 41.040, mean reward:  1.865 [-3.000, 31.878], mean action: 3.136 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  758/5000: episode: 28, duration: 0.171s, episode steps:  24, steps per second: 140, episode reward: 41.940, mean reward:  1.747 [-2.363, 32.310], mean action: 2.833 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  779/5000: episode: 29, duration: 0.156s, episode steps:  21, steps per second: 134, episode reward: 41.997, mean reward:  2.000 [-2.282, 32.110], mean action: 2.952 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  808/5000: episode: 30, duration: 0.217s, episode steps:  29, steps per second: 134, episode reward: 45.460, mean reward:  1.568 [-0.773, 31.636], mean action: 4.414 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  823/5000: episode: 31, duration: 0.116s, episode steps:  15, steps per second: 129, episode reward: 44.858, mean reward:  2.991 [-2.876, 32.060], mean action: 2.333 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  848/5000: episode: 32, duration: 0.329s, episode steps:  25, steps per second:  76, episode reward: 44.610, mean reward:  1.784 [-2.636, 32.260], mean action: 2.920 [1.000, 7.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  901/5000: episode: 33, duration: 0.395s, episode steps:  53, steps per second: 134, episode reward: 39.000, mean reward:  0.736 [-3.000, 32.160], mean action: 2.566 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  938/5000: episode: 34, duration: 0.255s, episode steps:  37, steps per second: 145, episode reward: 38.621, mean reward:  1.044 [-3.000, 32.540], mean action: 4.541 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  958/5000: episode: 35, duration: 0.164s, episode steps:  20, steps per second: 122, episode reward: 44.512, mean reward:  2.226 [-2.198, 32.210], mean action: 2.250 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  967/5000: episode: 36, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward: 47.710, mean reward:  5.301 [ 0.350, 33.000], mean action: 1.667 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  988/5000: episode: 37, duration: 0.153s, episode steps:  21, steps per second: 138, episode reward: 47.795, mean reward:  2.276 [ 0.000, 32.240], mean action: 3.333 [0.000, 13.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1009/5000: episode: 38, duration: 0.247s, episode steps:  21, steps per second:  85, episode reward: 41.296, mean reward:  1.966 [-2.340, 32.230], mean action: 3.857 [1.000, 16.000],  loss: 0.021848, mae: 0.315107, mean_q: 0.490894, mean_eps: 0.000000
 1019/5000: episode: 39, duration: 0.172s, episode steps:  10, steps per second:  58, episode reward: 48.000, mean reward:  4.800 [ 0.000, 33.000], mean action: 2.800 [0.000, 14.000],  loss: 0.015400, mae: 0.275516, mean_q: 0.465664, mean_eps: 0.000000
 1034/5000: episode: 40, duration: 0.234s, episode steps:  15, steps per second:  64, episode reward: 42.000, mean reward:  2.800 [-2.530, 32.310], mean action: 2.133 [0.000, 3.000],  loss: 0.022772, mae: 0.313908, mean_q: 0.553078, mean_eps: 0.000000
 1057/5000: episode: 41, duration: 0.342s, episode steps:  23, steps per second:  67, episode reward: 39.000, mean reward:  1.696 [-2.398, 32.600], mean action: 1.609 [0.000, 12.000],  loss: 0.019974, mae: 0.303761, mean_q: 0.524417, mean_eps: 0.000000
 1087/5000: episode: 42, duration: 0.455s, episode steps:  30, steps per second:  66, episode reward: 41.651, mean reward:  1.388 [-2.363, 31.821], mean action: 2.067 [0.000, 12.000],  loss: 0.020878, mae: 0.306690, mean_q: 0.581919, mean_eps: 0.000000
 1121/5000: episode: 43, duration: 0.501s, episode steps:  34, steps per second:  68, episode reward: 40.955, mean reward:  1.205 [-2.630, 31.845], mean action: 3.706 [0.000, 12.000],  loss: 0.021669, mae: 0.312529, mean_q: 0.519192, mean_eps: 0.000000
 1143/5000: episode: 44, duration: 0.328s, episode steps:  22, steps per second:  67, episode reward: 47.547, mean reward:  2.161 [ 0.000, 32.240], mean action: 1.727 [1.000, 3.000],  loss: 0.015705, mae: 0.285384, mean_q: 0.527823, mean_eps: 0.000000
 1160/5000: episode: 45, duration: 0.281s, episode steps:  17, steps per second:  61, episode reward: 44.047, mean reward:  2.591 [-2.293, 31.634], mean action: 2.471 [0.000, 19.000],  loss: 0.017308, mae: 0.290683, mean_q: 0.486711, mean_eps: 0.000000
 1184/5000: episode: 46, duration: 0.359s, episode steps:  24, steps per second:  67, episode reward: 41.036, mean reward:  1.710 [-2.903, 31.979], mean action: 3.083 [0.000, 19.000],  loss: 0.021781, mae: 0.317222, mean_q: 0.573708, mean_eps: 0.000000
 1195/5000: episode: 47, duration: 0.187s, episode steps:  11, steps per second:  59, episode reward: 47.141, mean reward:  4.286 [-0.210, 33.000], mean action: 1.182 [1.000, 2.000],  loss: 0.018165, mae: 0.307014, mean_q: 0.598238, mean_eps: 0.000000
 1221/5000: episode: 48, duration: 0.386s, episode steps:  26, steps per second:  67, episode reward: 40.415, mean reward:  1.554 [-2.325, 32.525], mean action: 3.115 [0.000, 13.000],  loss: 0.021243, mae: 0.312220, mean_q: 0.546905, mean_eps: 0.000000
 1246/5000: episode: 49, duration: 0.383s, episode steps:  25, steps per second:  65, episode reward: 41.412, mean reward:  1.656 [-2.571, 32.072], mean action: 1.880 [0.000, 16.000],  loss: 0.021344, mae: 0.300437, mean_q: 0.513071, mean_eps: 0.000000
 1273/5000: episode: 50, duration: 0.401s, episode steps:  27, steps per second:  67, episode reward: 38.872, mean reward:  1.440 [-2.179, 32.062], mean action: 4.926 [0.000, 16.000],  loss: 0.018562, mae: 0.290404, mean_q: 0.508430, mean_eps: 0.000000
 1297/5000: episode: 51, duration: 0.351s, episode steps:  24, steps per second:  68, episode reward: 43.466, mean reward:  1.811 [-2.309, 32.330], mean action: 2.833 [0.000, 13.000],  loss: 0.024840, mae: 0.318573, mean_q: 0.520516, mean_eps: 0.000000
 1325/5000: episode: 52, duration: 0.412s, episode steps:  28, steps per second:  68, episode reward: 38.280, mean reward:  1.367 [-2.356, 32.040], mean action: 4.429 [0.000, 15.000],  loss: 0.020750, mae: 0.301579, mean_q: 0.461669, mean_eps: 0.000000
 1349/5000: episode: 53, duration: 0.352s, episode steps:  24, steps per second:  68, episode reward: 36.000, mean reward:  1.500 [-3.000, 32.340], mean action: 2.833 [0.000, 15.000],  loss: 0.018593, mae: 0.288394, mean_q: 0.499009, mean_eps: 0.000000
 1374/5000: episode: 54, duration: 0.397s, episode steps:  25, steps per second:  63, episode reward: 42.615, mean reward:  1.705 [-2.057, 32.180], mean action: 2.680 [0.000, 9.000],  loss: 0.019286, mae: 0.294181, mean_q: 0.529805, mean_eps: 0.000000
 1390/5000: episode: 55, duration: 0.245s, episode steps:  16, steps per second:  65, episode reward: 44.401, mean reward:  2.775 [-2.553, 32.390], mean action: 2.812 [0.000, 9.000],  loss: 0.023146, mae: 0.314121, mean_q: 0.541432, mean_eps: 0.000000
 1497/5000: episode: 56, duration: 1.438s, episode steps: 107, steps per second:  74, episode reward: 33.000, mean reward:  0.308 [-3.000, 32.070], mean action: 14.364 [0.000, 20.000],  loss: 0.021365, mae: 0.304818, mean_q: 0.553381, mean_eps: 0.000000
 1514/5000: episode: 57, duration: 0.272s, episode steps:  17, steps per second:  63, episode reward: 45.000, mean reward:  2.647 [-2.110, 32.290], mean action: 1.059 [0.000, 9.000],  loss: 0.020711, mae: 0.301358, mean_q: 0.485130, mean_eps: 0.000000
 1558/5000: episode: 58, duration: 0.623s, episode steps:  44, steps per second:  71, episode reward: 42.000, mean reward:  0.955 [-2.229, 32.500], mean action: 1.409 [0.000, 16.000],  loss: 0.019057, mae: 0.298226, mean_q: 0.539123, mean_eps: 0.000000
 1587/5000: episode: 59, duration: 0.414s, episode steps:  29, steps per second:  70, episode reward: 32.589, mean reward:  1.124 [-2.803, 32.222], mean action: 4.828 [0.000, 19.000],  loss: 0.019157, mae: 0.299841, mean_q: 0.520742, mean_eps: 0.000000
 1622/5000: episode: 60, duration: 0.508s, episode steps:  35, steps per second:  69, episode reward: 38.618, mean reward:  1.103 [-3.000, 32.074], mean action: 4.600 [0.000, 19.000],  loss: 0.020304, mae: 0.310331, mean_q: 0.559882, mean_eps: 0.000000
 1651/5000: episode: 61, duration: 0.428s, episode steps:  29, steps per second:  68, episode reward: 35.806, mean reward:  1.235 [-2.939, 32.560], mean action: 3.310 [0.000, 16.000],  loss: 0.021074, mae: 0.314792, mean_q: 0.505108, mean_eps: 0.000000
 1668/5000: episode: 62, duration: 0.262s, episode steps:  17, steps per second:  65, episode reward: 44.086, mean reward:  2.593 [-2.024, 31.096], mean action: 2.294 [0.000, 13.000],  loss: 0.018983, mae: 0.294656, mean_q: 0.510709, mean_eps: 0.000000
 1696/5000: episode: 63, duration: 0.411s, episode steps:  28, steps per second:  68, episode reward: 41.876, mean reward:  1.496 [-2.790, 32.070], mean action: 3.321 [0.000, 16.000],  loss: 0.020044, mae: 0.305724, mean_q: 0.518125, mean_eps: 0.000000
 1716/5000: episode: 64, duration: 0.304s, episode steps:  20, steps per second:  66, episode reward: 43.980, mean reward:  2.199 [-2.138, 32.220], mean action: 4.850 [0.000, 16.000],  loss: 0.018370, mae: 0.305592, mean_q: 0.517436, mean_eps: 0.000000
 1774/5000: episode: 65, duration: 0.821s, episode steps:  58, steps per second:  71, episode reward: -33.000, mean reward: -0.569 [-32.761,  2.820], mean action: 6.948 [0.000, 14.000],  loss: 0.023588, mae: 0.327367, mean_q: 0.545149, mean_eps: 0.000000
 1792/5000: episode: 66, duration: 0.280s, episode steps:  18, steps per second:  64, episode reward: 44.838, mean reward:  2.491 [-2.136, 32.098], mean action: 3.167 [2.000, 15.000],  loss: 0.017254, mae: 0.298736, mean_q: 0.593420, mean_eps: 0.000000
 1812/5000: episode: 67, duration: 0.314s, episode steps:  20, steps per second:  64, episode reward: 44.703, mean reward:  2.235 [-2.238, 32.620], mean action: 5.000 [0.000, 15.000],  loss: 0.019757, mae: 0.302017, mean_q: 0.592227, mean_eps: 0.000000
 1833/5000: episode: 68, duration: 0.335s, episode steps:  21, steps per second:  63, episode reward: 44.219, mean reward:  2.106 [-2.723, 32.100], mean action: 1.381 [0.000, 11.000],  loss: 0.021197, mae: 0.310051, mean_q: 0.555418, mean_eps: 0.000000
 1869/5000: episode: 69, duration: 0.530s, episode steps:  36, steps per second:  68, episode reward: 35.466, mean reward:  0.985 [-2.559, 32.139], mean action: 4.722 [0.000, 20.000],  loss: 0.020001, mae: 0.313549, mean_q: 0.508188, mean_eps: 0.000000
 1904/5000: episode: 70, duration: 0.503s, episode steps:  35, steps per second:  70, episode reward: 45.000, mean reward:  1.286 [-2.304, 32.110], mean action: 4.057 [2.000, 14.000],  loss: 0.022164, mae: 0.319339, mean_q: 0.513826, mean_eps: 0.000000
 1926/5000: episode: 71, duration: 0.359s, episode steps:  22, steps per second:  61, episode reward: 43.938, mean reward:  1.997 [-3.000, 32.370], mean action: 3.818 [0.000, 14.000],  loss: 0.020148, mae: 0.310295, mean_q: 0.533936, mean_eps: 0.000000
 1962/5000: episode: 72, duration: 0.518s, episode steps:  36, steps per second:  70, episode reward: 40.519, mean reward:  1.126 [-2.471, 32.080], mean action: 4.444 [0.000, 16.000],  loss: 0.022123, mae: 0.312518, mean_q: 0.516544, mean_eps: 0.000000
 1984/5000: episode: 73, duration: 0.324s, episode steps:  22, steps per second:  68, episode reward: 39.000, mean reward:  1.773 [-2.758, 32.400], mean action: 3.182 [0.000, 12.000],  loss: 0.026115, mae: 0.325869, mean_q: 0.547047, mean_eps: 0.000000
 2016/5000: episode: 74, duration: 0.458s, episode steps:  32, steps per second:  70, episode reward: 41.429, mean reward:  1.295 [-3.000, 32.050], mean action: 2.625 [0.000, 12.000],  loss: 0.022095, mae: 0.309623, mean_q: 0.545025, mean_eps: 0.000000
 2041/5000: episode: 75, duration: 0.374s, episode steps:  25, steps per second:  67, episode reward: 43.154, mean reward:  1.726 [-2.370, 32.200], mean action: 3.160 [0.000, 19.000],  loss: 0.018639, mae: 0.300793, mean_q: 0.493453, mean_eps: 0.000000
 2082/5000: episode: 76, duration: 0.591s, episode steps:  41, steps per second:  69, episode reward: -32.250, mean reward: -0.787 [-32.553,  2.620], mean action: 6.439 [0.000, 19.000],  loss: 0.020963, mae: 0.314274, mean_q: 0.491790, mean_eps: 0.000000
 2115/5000: episode: 77, duration: 0.466s, episode steps:  33, steps per second:  71, episode reward: 41.889, mean reward:  1.269 [-2.494, 32.010], mean action: 2.818 [0.000, 14.000],  loss: 0.019866, mae: 0.304610, mean_q: 0.533691, mean_eps: 0.000000
 2143/5000: episode: 78, duration: 0.401s, episode steps:  28, steps per second:  70, episode reward: 35.543, mean reward:  1.269 [-3.000, 32.300], mean action: 3.786 [0.000, 14.000],  loss: 0.023257, mae: 0.321825, mean_q: 0.573777, mean_eps: 0.000000
 2176/5000: episode: 79, duration: 0.551s, episode steps:  33, steps per second:  60, episode reward: 38.549, mean reward:  1.168 [-2.403, 29.853], mean action: 5.242 [0.000, 19.000],  loss: 0.021826, mae: 0.309422, mean_q: 0.530550, mean_eps: 0.000000
 2198/5000: episode: 80, duration: 0.339s, episode steps:  22, steps per second:  65, episode reward: 41.314, mean reward:  1.878 [-2.402, 32.400], mean action: 3.000 [1.000, 11.000],  loss: 0.019265, mae: 0.301655, mean_q: 0.491313, mean_eps: 0.000000
 2237/5000: episode: 81, duration: 0.560s, episode steps:  39, steps per second:  70, episode reward: 38.896, mean reward:  0.997 [-2.940, 32.236], mean action: 3.000 [0.000, 15.000],  loss: 0.019889, mae: 0.310974, mean_q: 0.567676, mean_eps: 0.000000
 2247/5000: episode: 82, duration: 0.168s, episode steps:  10, steps per second:  60, episode reward: 45.000, mean reward:  4.500 [-2.286, 33.000], mean action: 3.000 [0.000, 9.000],  loss: 0.023468, mae: 0.325435, mean_q: 0.518629, mean_eps: 0.000000
 2270/5000: episode: 83, duration: 0.335s, episode steps:  23, steps per second:  69, episode reward: 41.938, mean reward:  1.823 [-2.121, 32.270], mean action: 2.043 [0.000, 15.000],  loss: 0.020391, mae: 0.314736, mean_q: 0.561622, mean_eps: 0.000000
 2300/5000: episode: 84, duration: 0.433s, episode steps:  30, steps per second:  69, episode reward: 41.123, mean reward:  1.371 [-2.761, 31.923], mean action: 4.967 [0.000, 20.000],  loss: 0.021502, mae: 0.313338, mean_q: 0.555249, mean_eps: 0.000000
 2327/5000: episode: 85, duration: 0.415s, episode steps:  27, steps per second:  65, episode reward: 38.439, mean reward:  1.424 [-3.000, 32.440], mean action: 3.370 [0.000, 19.000],  loss: 0.017064, mae: 0.295698, mean_q: 0.527024, mean_eps: 0.000000
 2356/5000: episode: 86, duration: 0.428s, episode steps:  29, steps per second:  68, episode reward: 44.562, mean reward:  1.537 [-2.567, 32.200], mean action: 2.207 [0.000, 15.000],  loss: 0.019857, mae: 0.310272, mean_q: 0.526883, mean_eps: 0.000000
 2379/5000: episode: 87, duration: 0.346s, episode steps:  23, steps per second:  66, episode reward: 35.356, mean reward:  1.537 [-3.000, 31.596], mean action: 5.000 [0.000, 15.000],  loss: 0.025166, mae: 0.334267, mean_q: 0.551980, mean_eps: 0.000000
 2430/5000: episode: 88, duration: 0.727s, episode steps:  51, steps per second:  70, episode reward: -34.080, mean reward: -0.668 [-33.000,  2.250], mean action: 3.373 [0.000, 15.000],  loss: 0.021934, mae: 0.316319, mean_q: 0.600157, mean_eps: 0.000000
 2455/5000: episode: 89, duration: 0.379s, episode steps:  25, steps per second:  66, episode reward: 41.906, mean reward:  1.676 [-2.637, 32.570], mean action: 4.800 [0.000, 15.000],  loss: 0.019508, mae: 0.311227, mean_q: 0.547944, mean_eps: 0.000000
 2472/5000: episode: 90, duration: 0.281s, episode steps:  17, steps per second:  60, episode reward: 44.667, mean reward:  2.627 [-2.014, 32.350], mean action: 4.118 [0.000, 15.000],  loss: 0.023882, mae: 0.325227, mean_q: 0.549611, mean_eps: 0.000000
 2497/5000: episode: 91, duration: 0.365s, episode steps:  25, steps per second:  68, episode reward: 35.901, mean reward:  1.436 [-3.000, 32.701], mean action: 4.000 [0.000, 15.000],  loss: 0.020195, mae: 0.309913, mean_q: 0.534289, mean_eps: 0.000000
 2523/5000: episode: 92, duration: 0.391s, episode steps:  26, steps per second:  67, episode reward: 39.000, mean reward:  1.500 [-2.338, 32.150], mean action: 2.846 [0.000, 15.000],  loss: 0.020743, mae: 0.305274, mean_q: 0.563229, mean_eps: 0.000000
 2556/5000: episode: 93, duration: 0.468s, episode steps:  33, steps per second:  71, episode reward: 39.000, mean reward:  1.182 [-2.506, 32.310], mean action: 3.061 [0.000, 15.000],  loss: 0.022881, mae: 0.316198, mean_q: 0.550536, mean_eps: 0.000000
 2586/5000: episode: 94, duration: 0.455s, episode steps:  30, steps per second:  66, episode reward: 39.000, mean reward:  1.300 [-2.347, 32.140], mean action: 2.900 [0.000, 19.000],  loss: 0.024492, mae: 0.327841, mean_q: 0.474802, mean_eps: 0.000000
 2604/5000: episode: 95, duration: 0.275s, episode steps:  18, steps per second:  65, episode reward: 44.042, mean reward:  2.447 [-2.595, 32.360], mean action: 3.944 [0.000, 19.000],  loss: 0.018391, mae: 0.300680, mean_q: 0.477835, mean_eps: 0.000000
 2632/5000: episode: 96, duration: 0.407s, episode steps:  28, steps per second:  69, episode reward: 44.790, mean reward:  1.600 [-2.473, 32.200], mean action: 3.500 [1.000, 19.000],  loss: 0.024201, mae: 0.325562, mean_q: 0.556546, mean_eps: 0.000000
 2685/5000: episode: 97, duration: 0.737s, episode steps:  53, steps per second:  72, episode reward: -35.130, mean reward: -0.663 [-31.914,  2.430], mean action: 6.736 [0.000, 21.000],  loss: 0.019215, mae: 0.299282, mean_q: 0.545014, mean_eps: 0.000000
 2710/5000: episode: 98, duration: 0.362s, episode steps:  25, steps per second:  69, episode reward: 35.951, mean reward:  1.438 [-3.000, 32.001], mean action: 5.160 [0.000, 20.000],  loss: 0.024129, mae: 0.323089, mean_q: 0.495257, mean_eps: 0.000000
 2734/5000: episode: 99, duration: 0.374s, episode steps:  24, steps per second:  64, episode reward: 44.397, mean reward:  1.850 [-2.806, 34.468], mean action: 4.250 [0.000, 13.000],  loss: 0.018897, mae: 0.300879, mean_q: 0.491976, mean_eps: 0.000000
 2758/5000: episode: 100, duration: 0.367s, episode steps:  24, steps per second:  65, episode reward: 40.073, mean reward:  1.670 [-3.000, 32.150], mean action: 2.417 [0.000, 16.000],  loss: 0.021836, mae: 0.310860, mean_q: 0.477730, mean_eps: 0.000000
 2777/5000: episode: 101, duration: 0.282s, episode steps:  19, steps per second:  67, episode reward: 38.741, mean reward:  2.039 [-2.426, 33.000], mean action: 3.526 [1.000, 12.000],  loss: 0.023403, mae: 0.313457, mean_q: 0.460470, mean_eps: 0.000000
 2809/5000: episode: 102, duration: 0.469s, episode steps:  32, steps per second:  68, episode reward: 33.000, mean reward:  1.031 [-3.000, 32.240], mean action: 3.125 [0.000, 12.000],  loss: 0.019332, mae: 0.301535, mean_q: 0.491784, mean_eps: 0.000000
 2841/5000: episode: 103, duration: 0.466s, episode steps:  32, steps per second:  69, episode reward: 35.754, mean reward:  1.117 [-3.000, 32.220], mean action: 3.406 [0.000, 16.000],  loss: 0.020818, mae: 0.306828, mean_q: 0.454845, mean_eps: 0.000000
 2885/5000: episode: 104, duration: 0.623s, episode steps:  44, steps per second:  71, episode reward: 41.545, mean reward:  0.944 [-2.161, 32.671], mean action: 1.659 [0.000, 9.000],  loss: 0.017635, mae: 0.294852, mean_q: 0.441223, mean_eps: 0.000000
 2905/5000: episode: 105, duration: 0.300s, episode steps:  20, steps per second:  67, episode reward: 37.991, mean reward:  1.900 [-3.000, 34.522], mean action: 5.700 [0.000, 13.000],  loss: 0.023025, mae: 0.317186, mean_q: 0.526165, mean_eps: 0.000000
 2941/5000: episode: 106, duration: 0.523s, episode steps:  36, steps per second:  69, episode reward: 35.089, mean reward:  0.975 [-2.427, 31.691], mean action: 4.500 [0.000, 15.000],  loss: 0.017782, mae: 0.292083, mean_q: 0.516970, mean_eps: 0.000000
 2962/5000: episode: 107, duration: 0.308s, episode steps:  21, steps per second:  68, episode reward: 35.795, mean reward:  1.705 [-3.000, 32.580], mean action: 4.286 [0.000, 16.000],  loss: 0.026586, mae: 0.330906, mean_q: 0.528013, mean_eps: 0.000000
 2993/5000: episode: 108, duration: 0.444s, episode steps:  31, steps per second:  70, episode reward: 38.642, mean reward:  1.247 [-2.434, 32.050], mean action: 3.000 [0.000, 19.000],  loss: 0.021377, mae: 0.306848, mean_q: 0.500418, mean_eps: 0.000000
 3035/5000: episode: 109, duration: 0.584s, episode steps:  42, steps per second:  72, episode reward: 38.237, mean reward:  0.910 [-3.000, 32.193], mean action: 4.905 [0.000, 19.000],  loss: 0.019596, mae: 0.300025, mean_q: 0.528434, mean_eps: 0.000000
 3060/5000: episode: 110, duration: 0.372s, episode steps:  25, steps per second:  67, episode reward: 41.903, mean reward:  1.676 [-2.147, 32.003], mean action: 2.680 [1.000, 19.000],  loss: 0.021529, mae: 0.310133, mean_q: 0.566546, mean_eps: 0.000000
 3077/5000: episode: 111, duration: 0.279s, episode steps:  17, steps per second:  61, episode reward: 41.131, mean reward:  2.419 [-2.122, 32.533], mean action: 3.471 [0.000, 19.000],  loss: 0.023439, mae: 0.315722, mean_q: 0.572393, mean_eps: 0.000000
 3093/5000: episode: 112, duration: 0.241s, episode steps:  16, steps per second:  66, episode reward: 40.950, mean reward:  2.559 [-2.517, 32.143], mean action: 4.938 [0.000, 19.000],  loss: 0.019318, mae: 0.299385, mean_q: 0.548278, mean_eps: 0.000000
 3117/5000: episode: 113, duration: 0.356s, episode steps:  24, steps per second:  67, episode reward: 37.594, mean reward:  1.566 [-2.586, 32.120], mean action: 6.625 [0.000, 19.000],  loss: 0.017621, mae: 0.290038, mean_q: 0.510552, mean_eps: 0.000000
 3134/5000: episode: 114, duration: 0.268s, episode steps:  17, steps per second:  63, episode reward: 38.186, mean reward:  2.246 [-3.000, 32.012], mean action: 4.941 [0.000, 15.000],  loss: 0.021192, mae: 0.312333, mean_q: 0.581738, mean_eps: 0.000000
 3153/5000: episode: 115, duration: 0.279s, episode steps:  19, steps per second:  68, episode reward: 44.003, mean reward:  2.316 [-2.143, 32.314], mean action: 2.684 [1.000, 15.000],  loss: 0.019749, mae: 0.305939, mean_q: 0.604858, mean_eps: 0.000000
 3179/5000: episode: 116, duration: 0.425s, episode steps:  26, steps per second:  61, episode reward: 41.861, mean reward:  1.610 [-2.332, 32.280], mean action: 3.038 [0.000, 15.000],  loss: 0.023238, mae: 0.325599, mean_q: 0.578315, mean_eps: 0.000000
 3211/5000: episode: 117, duration: 0.581s, episode steps:  32, steps per second:  55, episode reward: 41.004, mean reward:  1.281 [-3.000, 31.887], mean action: 3.125 [0.000, 15.000],  loss: 0.018495, mae: 0.301181, mean_q: 0.513137, mean_eps: 0.000000
 3243/5000: episode: 118, duration: 0.501s, episode steps:  32, steps per second:  64, episode reward: 40.763, mean reward:  1.274 [-3.000, 32.440], mean action: 4.906 [0.000, 15.000],  loss: 0.020218, mae: 0.307567, mean_q: 0.563972, mean_eps: 0.000000
 3270/5000: episode: 119, duration: 0.397s, episode steps:  27, steps per second:  68, episode reward: 44.034, mean reward:  1.631 [-2.084, 32.570], mean action: 1.519 [0.000, 9.000],  loss: 0.018712, mae: 0.298776, mean_q: 0.568678, mean_eps: 0.000000
 3297/5000: episode: 120, duration: 0.395s, episode steps:  27, steps per second:  68, episode reward: 36.000, mean reward:  1.333 [-3.000, 32.330], mean action: 3.519 [0.000, 19.000],  loss: 0.023922, mae: 0.310103, mean_q: 0.563005, mean_eps: 0.000000
 3323/5000: episode: 121, duration: 0.382s, episode steps:  26, steps per second:  68, episode reward: 38.697, mean reward:  1.488 [-2.208, 32.470], mean action: 3.923 [0.000, 19.000],  loss: 0.019376, mae: 0.288502, mean_q: 0.506199, mean_eps: 0.000000
 3345/5000: episode: 122, duration: 0.446s, episode steps:  22, steps per second:  49, episode reward: 38.245, mean reward:  1.738 [-2.522, 32.351], mean action: 4.409 [1.000, 19.000],  loss: 0.021412, mae: 0.295120, mean_q: 0.537761, mean_eps: 0.000000
 3358/5000: episode: 123, duration: 0.269s, episode steps:  13, steps per second:  48, episode reward: 44.088, mean reward:  3.391 [-2.600, 33.000], mean action: 4.231 [0.000, 19.000],  loss: 0.025262, mae: 0.317128, mean_q: 0.565516, mean_eps: 0.000000
 3379/5000: episode: 124, duration: 0.311s, episode steps:  21, steps per second:  68, episode reward: 41.300, mean reward:  1.967 [-2.056, 31.934], mean action: 3.905 [0.000, 19.000],  loss: 0.022792, mae: 0.314793, mean_q: 0.565035, mean_eps: 0.000000
 3405/5000: episode: 125, duration: 0.376s, episode steps:  26, steps per second:  69, episode reward: 35.627, mean reward:  1.370 [-2.498, 32.600], mean action: 3.308 [0.000, 16.000],  loss: 0.019153, mae: 0.294042, mean_q: 0.552861, mean_eps: 0.000000
 3422/5000: episode: 126, duration: 0.253s, episode steps:  17, steps per second:  67, episode reward: 42.000, mean reward:  2.471 [-2.805, 33.000], mean action: 3.765 [0.000, 16.000],  loss: 0.019203, mae: 0.300803, mean_q: 0.558367, mean_eps: 0.000000
 3461/5000: episode: 127, duration: 0.562s, episode steps:  39, steps per second:  69, episode reward: 34.865, mean reward:  0.894 [-2.234, 33.000], mean action: 5.769 [0.000, 18.000],  loss: 0.020106, mae: 0.308839, mean_q: 0.545036, mean_eps: 0.000000
 3489/5000: episode: 128, duration: 0.406s, episode steps:  28, steps per second:  69, episode reward: 39.000, mean reward:  1.393 [-3.000, 32.110], mean action: 4.893 [0.000, 16.000],  loss: 0.020061, mae: 0.307195, mean_q: 0.560598, mean_eps: 0.000000
 3508/5000: episode: 129, duration: 0.292s, episode steps:  19, steps per second:  65, episode reward: 41.140, mean reward:  2.165 [-2.277, 32.130], mean action: 2.474 [0.000, 16.000],  loss: 0.021139, mae: 0.307583, mean_q: 0.612836, mean_eps: 0.000000
 3533/5000: episode: 130, duration: 0.412s, episode steps:  25, steps per second:  61, episode reward: 40.505, mean reward:  1.620 [-2.622, 32.009], mean action: 3.800 [0.000, 19.000],  loss: 0.020533, mae: 0.303716, mean_q: 0.584019, mean_eps: 0.000000
 3557/5000: episode: 131, duration: 0.366s, episode steps:  24, steps per second:  66, episode reward: 44.542, mean reward:  1.856 [-2.250, 32.070], mean action: 3.000 [1.000, 19.000],  loss: 0.019904, mae: 0.301582, mean_q: 0.549744, mean_eps: 0.000000
 3585/5000: episode: 132, duration: 0.411s, episode steps:  28, steps per second:  68, episode reward: 44.512, mean reward:  1.590 [-2.150, 32.420], mean action: 2.357 [1.000, 19.000],  loss: 0.019246, mae: 0.302591, mean_q: 0.517780, mean_eps: 0.000000
 3606/5000: episode: 133, duration: 0.307s, episode steps:  21, steps per second:  68, episode reward: 41.935, mean reward:  1.997 [-2.361, 32.410], mean action: 3.476 [0.000, 19.000],  loss: 0.022492, mae: 0.314192, mean_q: 0.485918, mean_eps: 0.000000
 3632/5000: episode: 134, duration: 0.397s, episode steps:  26, steps per second:  65, episode reward: 40.980, mean reward:  1.576 [-3.000, 32.070], mean action: 3.115 [0.000, 19.000],  loss: 0.017364, mae: 0.286467, mean_q: 0.539015, mean_eps: 0.000000
 3659/5000: episode: 135, duration: 0.406s, episode steps:  27, steps per second:  66, episode reward: 35.072, mean reward:  1.299 [-3.000, 32.232], mean action: 4.963 [0.000, 19.000],  loss: 0.023257, mae: 0.313170, mean_q: 0.537108, mean_eps: 0.000000
 3685/5000: episode: 136, duration: 0.379s, episode steps:  26, steps per second:  69, episode reward: 41.715, mean reward:  1.604 [-2.254, 32.783], mean action: 3.000 [0.000, 19.000],  loss: 0.017843, mae: 0.290998, mean_q: 0.561603, mean_eps: 0.000000
 3716/5000: episode: 137, duration: 0.453s, episode steps:  31, steps per second:  68, episode reward: 33.000, mean reward:  1.065 [-2.535, 32.170], mean action: 6.355 [0.000, 19.000],  loss: 0.021476, mae: 0.307147, mean_q: 0.577049, mean_eps: 0.000000
 3757/5000: episode: 138, duration: 0.579s, episode steps:  41, steps per second:  71, episode reward: 38.784, mean reward:  0.946 [-3.000, 32.431], mean action: 5.829 [0.000, 20.000],  loss: 0.020761, mae: 0.302856, mean_q: 0.554473, mean_eps: 0.000000
 3788/5000: episode: 139, duration: 0.466s, episode steps:  31, steps per second:  67, episode reward: 41.444, mean reward:  1.337 [-2.807, 32.280], mean action: 4.097 [0.000, 16.000],  loss: 0.018761, mae: 0.302181, mean_q: 0.518946, mean_eps: 0.000000
 3814/5000: episode: 140, duration: 0.391s, episode steps:  26, steps per second:  67, episode reward: 39.000, mean reward:  1.500 [-2.438, 32.190], mean action: 1.885 [0.000, 12.000],  loss: 0.020904, mae: 0.312365, mean_q: 0.525654, mean_eps: 0.000000
 3838/5000: episode: 141, duration: 0.351s, episode steps:  24, steps per second:  68, episode reward: 38.327, mean reward:  1.597 [-3.000, 33.000], mean action: 3.750 [0.000, 16.000],  loss: 0.019029, mae: 0.311197, mean_q: 0.554864, mean_eps: 0.000000
 3859/5000: episode: 142, duration: 0.309s, episode steps:  21, steps per second:  68, episode reward: 39.000, mean reward:  1.857 [-3.000, 33.000], mean action: 2.571 [0.000, 11.000],  loss: 0.022886, mae: 0.332431, mean_q: 0.501275, mean_eps: 0.000000
 3883/5000: episode: 143, duration: 0.358s, episode steps:  24, steps per second:  67, episode reward: 35.420, mean reward:  1.476 [-2.756, 32.017], mean action: 6.417 [0.000, 20.000],  loss: 0.018537, mae: 0.310177, mean_q: 0.512533, mean_eps: 0.000000
 3916/5000: episode: 144, duration: 0.481s, episode steps:  33, steps per second:  69, episode reward: 38.057, mean reward:  1.153 [-3.000, 31.436], mean action: 3.333 [1.000, 15.000],  loss: 0.021441, mae: 0.322059, mean_q: 0.529462, mean_eps: 0.000000
 3955/5000: episode: 145, duration: 0.582s, episode steps:  39, steps per second:  67, episode reward: 36.000, mean reward:  0.923 [-2.429, 30.120], mean action: 3.282 [0.000, 20.000],  loss: 0.019755, mae: 0.313151, mean_q: 0.532607, mean_eps: 0.000000
 3975/5000: episode: 146, duration: 0.307s, episode steps:  20, steps per second:  65, episode reward: 44.243, mean reward:  2.212 [-2.618, 32.530], mean action: 4.200 [0.000, 19.000],  loss: 0.018519, mae: 0.297456, mean_q: 0.496108, mean_eps: 0.000000
 3987/5000: episode: 147, duration: 0.206s, episode steps:  12, steps per second:  58, episode reward: 47.112, mean reward:  3.926 [-0.089, 31.692], mean action: 2.583 [2.000, 3.000],  loss: 0.022610, mae: 0.325139, mean_q: 0.460672, mean_eps: 0.000000
 4011/5000: episode: 148, duration: 0.359s, episode steps:  24, steps per second:  67, episode reward: 45.000, mean reward:  1.875 [-2.126, 32.690], mean action: 3.083 [0.000, 14.000],  loss: 0.020814, mae: 0.306486, mean_q: 0.446869, mean_eps: 0.000000
 4033/5000: episode: 149, duration: 0.331s, episode steps:  22, steps per second:  66, episode reward: 36.000, mean reward:  1.636 [-2.639, 32.230], mean action: 3.182 [0.000, 12.000],  loss: 0.022384, mae: 0.317317, mean_q: 0.532877, mean_eps: 0.000000
 4058/5000: episode: 150, duration: 0.379s, episode steps:  25, steps per second:  66, episode reward: 41.027, mean reward:  1.641 [-2.161, 32.110], mean action: 2.840 [0.000, 19.000],  loss: 0.022077, mae: 0.318738, mean_q: 0.522337, mean_eps: 0.000000
 4086/5000: episode: 151, duration: 0.414s, episode steps:  28, steps per second:  68, episode reward: 38.753, mean reward:  1.384 [-2.953, 31.843], mean action: 4.393 [0.000, 19.000],  loss: 0.019351, mae: 0.297586, mean_q: 0.544065, mean_eps: 0.000000
 4106/5000: episode: 152, duration: 0.312s, episode steps:  20, steps per second:  64, episode reward: 42.000, mean reward:  2.100 [-2.628, 32.080], mean action: 2.000 [0.000, 9.000],  loss: 0.024074, mae: 0.320750, mean_q: 0.503171, mean_eps: 0.000000
 4130/5000: episode: 153, duration: 0.373s, episode steps:  24, steps per second:  64, episode reward: 35.875, mean reward:  1.495 [-3.000, 32.240], mean action: 4.083 [0.000, 19.000],  loss: 0.018621, mae: 0.289427, mean_q: 0.484769, mean_eps: 0.000000
 4173/5000: episode: 154, duration: 0.615s, episode steps:  43, steps per second:  70, episode reward: 37.534, mean reward:  0.873 [-2.391, 32.031], mean action: 7.884 [0.000, 20.000],  loss: 0.020507, mae: 0.301704, mean_q: 0.458552, mean_eps: 0.000000
 4203/5000: episode: 155, duration: 0.434s, episode steps:  30, steps per second:  69, episode reward: 41.525, mean reward:  1.384 [-2.502, 32.080], mean action: 1.467 [0.000, 15.000],  loss: 0.020729, mae: 0.301164, mean_q: 0.507030, mean_eps: 0.000000
 4230/5000: episode: 156, duration: 0.408s, episode steps:  27, steps per second:  66, episode reward: 43.555, mean reward:  1.613 [-2.123, 32.270], mean action: 4.741 [0.000, 16.000],  loss: 0.020280, mae: 0.303720, mean_q: 0.509431, mean_eps: 0.000000
 4347/5000: episode: 157, duration: 1.608s, episode steps: 117, steps per second:  73, episode reward: 46.853, mean reward:  0.400 [-0.420, 32.100], mean action: 1.325 [0.000, 14.000],  loss: 0.020418, mae: 0.305944, mean_q: 0.512651, mean_eps: 0.000000
 4366/5000: episode: 158, duration: 0.296s, episode steps:  19, steps per second:  64, episode reward: 38.440, mean reward:  2.023 [-2.265, 32.340], mean action: 4.053 [0.000, 11.000],  loss: 0.021761, mae: 0.314904, mean_q: 0.494317, mean_eps: 0.000000
 4404/5000: episode: 159, duration: 0.561s, episode steps:  38, steps per second:  68, episode reward: 38.222, mean reward:  1.006 [-2.313, 31.714], mean action: 2.026 [0.000, 16.000],  loss: 0.020931, mae: 0.308607, mean_q: 0.458184, mean_eps: 0.000000
 4426/5000: episode: 160, duration: 0.329s, episode steps:  22, steps per second:  67, episode reward: 47.111, mean reward:  2.141 [-0.451, 32.490], mean action: 3.000 [3.000, 3.000],  loss: 0.020823, mae: 0.304789, mean_q: 0.531160, mean_eps: 0.000000
 4470/5000: episode: 161, duration: 0.648s, episode steps:  44, steps per second:  68, episode reward: 38.556, mean reward:  0.876 [-2.708, 31.706], mean action: 2.932 [0.000, 16.000],  loss: 0.020909, mae: 0.311460, mean_q: 0.531101, mean_eps: 0.000000
 4503/5000: episode: 162, duration: 0.486s, episode steps:  33, steps per second:  68, episode reward: 35.947, mean reward:  1.089 [-3.000, 32.187], mean action: 2.515 [0.000, 18.000],  loss: 0.019724, mae: 0.299541, mean_q: 0.507234, mean_eps: 0.000000
 4553/5000: episode: 163, duration: 0.705s, episode steps:  50, steps per second:  71, episode reward: -32.680, mean reward: -0.654 [-32.380,  2.460], mean action: 7.880 [0.000, 20.000],  loss: 0.019026, mae: 0.301403, mean_q: 0.478781, mean_eps: 0.000000
 4572/5000: episode: 164, duration: 0.283s, episode steps:  19, steps per second:  67, episode reward: 44.689, mean reward:  2.352 [-2.290, 32.100], mean action: 1.947 [0.000, 15.000],  loss: 0.017685, mae: 0.303046, mean_q: 0.453422, mean_eps: 0.000000
 4599/5000: episode: 165, duration: 0.396s, episode steps:  27, steps per second:  68, episode reward: 38.870, mean reward:  1.440 [-2.892, 32.190], mean action: 3.556 [0.000, 15.000],  loss: 0.021112, mae: 0.308206, mean_q: 0.499194, mean_eps: 0.000000
 4641/5000: episode: 166, duration: 0.619s, episode steps:  42, steps per second:  68, episode reward: 40.168, mean reward:  0.956 [-2.411, 32.223], mean action: 2.929 [0.000, 13.000],  loss: 0.021378, mae: 0.310264, mean_q: 0.483404, mean_eps: 0.000000
 4665/5000: episode: 167, duration: 0.384s, episode steps:  24, steps per second:  62, episode reward: 42.000, mean reward:  1.750 [-2.345, 29.110], mean action: 2.458 [1.000, 3.000],  loss: 0.018658, mae: 0.300479, mean_q: 0.513472, mean_eps: 0.000000
 4752/5000: episode: 168, duration: 1.455s, episode steps:  87, steps per second:  60, episode reward: 38.421, mean reward:  0.442 [-2.186, 32.240], mean action: 1.897 [0.000, 16.000],  loss: 0.019043, mae: 0.305308, mean_q: 0.534837, mean_eps: 0.000000
 4785/5000: episode: 169, duration: 0.531s, episode steps:  33, steps per second:  62, episode reward: 41.139, mean reward:  1.247 [-2.529, 32.345], mean action: 6.152 [0.000, 16.000],  loss: 0.017104, mae: 0.297529, mean_q: 0.529198, mean_eps: 0.000000
 4810/5000: episode: 170, duration: 0.393s, episode steps:  25, steps per second:  64, episode reward: 41.679, mean reward:  1.667 [-2.147, 32.169], mean action: 3.520 [0.000, 19.000],  loss: 0.018148, mae: 0.300746, mean_q: 0.549494, mean_eps: 0.000000
 4833/5000: episode: 171, duration: 0.390s, episode steps:  23, steps per second:  59, episode reward: 40.748, mean reward:  1.772 [-2.500, 32.207], mean action: 3.304 [0.000, 19.000],  loss: 0.019581, mae: 0.307939, mean_q: 0.542506, mean_eps: 0.000000
 4891/5000: episode: 172, duration: 1.125s, episode steps:  58, steps per second:  52, episode reward: -32.510, mean reward: -0.561 [-32.452,  3.000], mean action: 9.914 [0.000, 19.000],  loss: 0.022067, mae: 0.320773, mean_q: 0.585607, mean_eps: 0.000000
 4922/5000: episode: 173, duration: 0.476s, episode steps:  31, steps per second:  65, episode reward: 32.481, mean reward:  1.048 [-2.704, 31.721], mean action: 4.484 [1.000, 19.000],  loss: 0.019672, mae: 0.308237, mean_q: 0.555832, mean_eps: 0.000000
 4961/5000: episode: 174, duration: 0.573s, episode steps:  39, steps per second:  68, episode reward: 34.708, mean reward:  0.890 [-2.875, 32.240], mean action: 5.923 [1.000, 20.000],  loss: 0.024466, mae: 0.330307, mean_q: 0.550319, mean_eps: 0.000000
 4994/5000: episode: 175, duration: 0.500s, episode steps:  33, steps per second:  66, episode reward: 38.546, mean reward:  1.168 [-3.000, 32.244], mean action: 3.758 [0.000, 16.000],  loss: 0.019868, mae: 0.301976, mean_q: 0.539779, mean_eps: 0.000000
done, took 67.622 seconds
DQN Evaluation: 13140 victories out of 15271 episodes
Training for 5000 steps ...
   17/5000: episode: 1, duration: 0.177s, episode steps:  17, steps per second:  96, episode reward: 35.777, mean reward:  2.105 [-2.704, 32.137], mean action: 3.824 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   43/5000: episode: 2, duration: 0.195s, episode steps:  26, steps per second: 134, episode reward: 33.000, mean reward:  1.269 [-3.000, 32.120], mean action: 4.462 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   65/5000: episode: 3, duration: 0.158s, episode steps:  22, steps per second: 139, episode reward: 41.449, mean reward:  1.884 [-2.136, 32.918], mean action: 5.045 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   83/5000: episode: 4, duration: 0.133s, episode steps:  18, steps per second: 135, episode reward: -36.000, mean reward: -2.000 [-30.910,  2.290], mean action: 5.389 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  107/5000: episode: 5, duration: 0.167s, episode steps:  24, steps per second: 144, episode reward: 38.569, mean reward:  1.607 [-2.741, 32.100], mean action: 3.917 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  136/5000: episode: 6, duration: 0.207s, episode steps:  29, steps per second: 140, episode reward: 41.105, mean reward:  1.417 [-2.211, 31.891], mean action: 2.241 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  153/5000: episode: 7, duration: 0.128s, episode steps:  17, steps per second: 133, episode reward: 38.975, mean reward:  2.293 [-2.359, 32.215], mean action: 4.765 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  177/5000: episode: 8, duration: 0.172s, episode steps:  24, steps per second: 140, episode reward: 37.715, mean reward:  1.571 [-2.470, 32.318], mean action: 6.250 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  212/5000: episode: 9, duration: 0.243s, episode steps:  35, steps per second: 144, episode reward: 32.464, mean reward:  0.928 [-3.000, 32.160], mean action: 4.686 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  228/5000: episode: 10, duration: 0.125s, episode steps:  16, steps per second: 128, episode reward: 38.109, mean reward:  2.382 [-2.569, 31.559], mean action: 4.875 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  259/5000: episode: 11, duration: 0.220s, episode steps:  31, steps per second: 141, episode reward: 38.571, mean reward:  1.244 [-2.266, 31.997], mean action: 3.871 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  284/5000: episode: 12, duration: 0.200s, episode steps:  25, steps per second: 125, episode reward: 33.000, mean reward:  1.320 [-3.000, 32.010], mean action: 4.120 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  302/5000: episode: 13, duration: 0.127s, episode steps:  18, steps per second: 142, episode reward: -33.000, mean reward: -1.833 [-29.703,  2.360], mean action: 6.111 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  324/5000: episode: 14, duration: 0.167s, episode steps:  22, steps per second: 131, episode reward: 36.000, mean reward:  1.636 [-2.548, 32.210], mean action: 5.318 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  355/5000: episode: 15, duration: 0.227s, episode steps:  31, steps per second: 137, episode reward: 32.027, mean reward:  1.033 [-3.000, 32.400], mean action: 5.097 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  373/5000: episode: 16, duration: 0.130s, episode steps:  18, steps per second: 138, episode reward: 36.000, mean reward:  2.000 [-2.776, 32.200], mean action: 5.111 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  402/5000: episode: 17, duration: 0.198s, episode steps:  29, steps per second: 146, episode reward: 32.238, mean reward:  1.112 [-3.000, 32.562], mean action: 4.862 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  430/5000: episode: 18, duration: 0.205s, episode steps:  28, steps per second: 137, episode reward: 35.645, mean reward:  1.273 [-3.000, 31.695], mean action: 5.714 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  460/5000: episode: 19, duration: 0.207s, episode steps:  30, steps per second: 145, episode reward: 33.000, mean reward:  1.100 [-2.382, 32.160], mean action: 4.267 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  490/5000: episode: 20, duration: 0.213s, episode steps:  30, steps per second: 141, episode reward: 32.080, mean reward:  1.069 [-3.000, 31.686], mean action: 5.333 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  512/5000: episode: 21, duration: 0.151s, episode steps:  22, steps per second: 146, episode reward: 36.000, mean reward:  1.636 [-3.000, 32.080], mean action: 4.955 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  541/5000: episode: 22, duration: 0.210s, episode steps:  29, steps per second: 138, episode reward: 32.783, mean reward:  1.130 [-3.000, 32.063], mean action: 5.655 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  559/5000: episode: 23, duration: 0.136s, episode steps:  18, steps per second: 132, episode reward: 45.000, mean reward:  2.500 [-2.230, 32.100], mean action: 2.500 [0.000, 6.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  572/5000: episode: 24, duration: 0.119s, episode steps:  13, steps per second: 109, episode reward: 44.220, mean reward:  3.402 [-3.000, 32.450], mean action: 1.615 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  598/5000: episode: 25, duration: 0.180s, episode steps:  26, steps per second: 145, episode reward: 35.852, mean reward:  1.379 [-2.346, 32.360], mean action: 4.923 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  635/5000: episode: 26, duration: 0.244s, episode steps:  37, steps per second: 152, episode reward: 35.216, mean reward:  0.952 [-2.984, 32.660], mean action: 4.622 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  658/5000: episode: 27, duration: 0.157s, episode steps:  23, steps per second: 146, episode reward: 32.928, mean reward:  1.432 [-2.720, 33.000], mean action: 5.174 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  703/5000: episode: 28, duration: 0.306s, episode steps:  45, steps per second: 147, episode reward: -32.830, mean reward: -0.730 [-32.406,  2.905], mean action: 9.667 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  722/5000: episode: 29, duration: 0.133s, episode steps:  19, steps per second: 143, episode reward: -41.220, mean reward: -2.169 [-33.000,  2.109], mean action: 5.316 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  745/5000: episode: 30, duration: 0.160s, episode steps:  23, steps per second: 144, episode reward: 38.882, mean reward:  1.691 [-2.161, 32.200], mean action: 2.739 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  761/5000: episode: 31, duration: 0.118s, episode steps:  16, steps per second: 136, episode reward: 41.609, mean reward:  2.601 [-2.439, 32.128], mean action: 3.500 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  785/5000: episode: 32, duration: 0.180s, episode steps:  24, steps per second: 133, episode reward: -38.710, mean reward: -1.613 [-32.105,  2.450], mean action: 6.250 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  811/5000: episode: 33, duration: 0.186s, episode steps:  26, steps per second: 140, episode reward: 32.031, mean reward:  1.232 [-2.909, 32.150], mean action: 4.577 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  831/5000: episode: 34, duration: 0.140s, episode steps:  20, steps per second: 142, episode reward: -32.610, mean reward: -1.631 [-32.361,  2.651], mean action: 4.750 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  850/5000: episode: 35, duration: 0.143s, episode steps:  19, steps per second: 133, episode reward: 34.600, mean reward:  1.821 [-3.000, 31.626], mean action: 6.368 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  885/5000: episode: 36, duration: 0.257s, episode steps:  35, steps per second: 136, episode reward: -32.310, mean reward: -0.923 [-32.033,  2.340], mean action: 6.943 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  904/5000: episode: 37, duration: 0.151s, episode steps:  19, steps per second: 126, episode reward: 38.237, mean reward:  2.012 [-2.569, 31.813], mean action: 5.474 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  930/5000: episode: 38, duration: 0.184s, episode steps:  26, steps per second: 142, episode reward: 38.790, mean reward:  1.492 [-2.217, 32.220], mean action: 3.500 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  952/5000: episode: 39, duration: 0.175s, episode steps:  22, steps per second: 126, episode reward: 35.668, mean reward:  1.621 [-2.520, 31.718], mean action: 5.455 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  974/5000: episode: 40, duration: 0.158s, episode steps:  22, steps per second: 139, episode reward: -32.600, mean reward: -1.482 [-32.372,  3.000], mean action: 4.727 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  994/5000: episode: 41, duration: 0.152s, episode steps:  20, steps per second: 131, episode reward: 35.502, mean reward:  1.775 [-2.441, 32.204], mean action: 4.750 [0.000, 19.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1022/5000: episode: 42, duration: 0.359s, episode steps:  28, steps per second:  78, episode reward: -32.690, mean reward: -1.167 [-32.050,  2.230], mean action: 6.679 [0.000, 20.000],  loss: 0.022318, mae: 0.313290, mean_q: 0.542295, mean_eps: 0.000000
 1058/5000: episode: 43, duration: 0.535s, episode steps:  36, steps per second:  67, episode reward: 33.000, mean reward:  0.917 [-2.498, 32.370], mean action: 6.556 [0.000, 18.000],  loss: 0.022994, mae: 0.320234, mean_q: 0.530968, mean_eps: 0.000000
 1081/5000: episode: 44, duration: 0.521s, episode steps:  23, steps per second:  44, episode reward: -32.360, mean reward: -1.407 [-32.064,  2.723], mean action: 4.174 [0.000, 19.000],  loss: 0.022254, mae: 0.317428, mean_q: 0.539955, mean_eps: 0.000000
 1096/5000: episode: 45, duration: 0.242s, episode steps:  15, steps per second:  62, episode reward: 47.142, mean reward:  3.143 [-0.070, 32.360], mean action: 2.600 [0.000, 16.000],  loss: 0.021646, mae: 0.313216, mean_q: 0.528563, mean_eps: 0.000000
 1113/5000: episode: 46, duration: 0.254s, episode steps:  17, steps per second:  67, episode reward: 41.370, mean reward:  2.434 [-2.500, 32.541], mean action: 3.941 [2.000, 6.000],  loss: 0.019395, mae: 0.302684, mean_q: 0.527623, mean_eps: 0.000000
 1139/5000: episode: 47, duration: 16.948s, episode steps:  26, steps per second:   2, episode reward: 35.835, mean reward:  1.378 [-2.592, 32.026], mean action: 5.000 [0.000, 12.000],  loss: 0.018994, mae: 0.296319, mean_q: 0.551684, mean_eps: 0.000000
 1157/5000: episode: 48, duration: 0.278s, episode steps:  18, steps per second:  65, episode reward: 35.600, mean reward:  1.978 [-2.408, 32.659], mean action: 3.944 [0.000, 12.000],  loss: 0.019597, mae: 0.308181, mean_q: 0.557795, mean_eps: 0.000000
 1180/5000: episode: 49, duration: 0.354s, episode steps:  23, steps per second:  65, episode reward: 35.445, mean reward:  1.541 [-2.500, 31.645], mean action: 1.913 [0.000, 11.000],  loss: 0.016634, mae: 0.290948, mean_q: 0.563392, mean_eps: 0.000000
 1205/5000: episode: 50, duration: 0.380s, episode steps:  25, steps per second:  66, episode reward: 32.593, mean reward:  1.304 [-3.000, 32.330], mean action: 6.120 [0.000, 15.000],  loss: 0.018926, mae: 0.295156, mean_q: 0.517805, mean_eps: 0.000000
 1222/5000: episode: 51, duration: 0.272s, episode steps:  17, steps per second:  63, episode reward: 41.248, mean reward:  2.426 [-3.000, 33.000], mean action: 2.412 [0.000, 14.000],  loss: 0.015963, mae: 0.285833, mean_q: 0.564359, mean_eps: 0.000000
 1241/5000: episode: 52, duration: 0.298s, episode steps:  19, steps per second:  64, episode reward: -35.580, mean reward: -1.873 [-32.508,  2.900], mean action: 7.263 [0.000, 14.000],  loss: 0.020141, mae: 0.307030, mean_q: 0.592240, mean_eps: 0.000000
 1252/5000: episode: 53, duration: 0.187s, episode steps:  11, steps per second:  59, episode reward: 45.000, mean reward:  4.091 [-2.250, 32.020], mean action: 3.909 [0.000, 14.000],  loss: 0.018422, mae: 0.299081, mean_q: 0.604843, mean_eps: 0.000000
 1276/5000: episode: 54, duration: 0.367s, episode steps:  24, steps per second:  65, episode reward: -38.060, mean reward: -1.586 [-32.060,  2.340], mean action: 5.083 [0.000, 14.000],  loss: 0.018490, mae: 0.302009, mean_q: 0.575801, mean_eps: 0.000000
 1300/5000: episode: 55, duration: 0.345s, episode steps:  24, steps per second:  70, episode reward: 35.342, mean reward:  1.473 [-3.000, 32.330], mean action: 6.042 [1.000, 16.000],  loss: 0.018647, mae: 0.296662, mean_q: 0.588502, mean_eps: 0.000000
 1322/5000: episode: 56, duration: 0.337s, episode steps:  22, steps per second:  65, episode reward: 38.305, mean reward:  1.741 [-2.621, 32.481], mean action: 4.273 [0.000, 19.000],  loss: 0.021944, mae: 0.315485, mean_q: 0.576905, mean_eps: 0.000000
 1373/5000: episode: 57, duration: 0.715s, episode steps:  51, steps per second:  71, episode reward: -38.940, mean reward: -0.764 [-32.235,  2.910], mean action: 5.294 [0.000, 16.000],  loss: 0.021438, mae: 0.319733, mean_q: 0.568789, mean_eps: 0.000000
 1387/5000: episode: 58, duration: 0.222s, episode steps:  14, steps per second:  63, episode reward: 41.787, mean reward:  2.985 [-3.000, 32.480], mean action: 2.214 [0.000, 16.000],  loss: 0.024824, mae: 0.333715, mean_q: 0.537743, mean_eps: 0.000000
 1407/5000: episode: 59, duration: 0.317s, episode steps:  20, steps per second:  63, episode reward: 35.492, mean reward:  1.775 [-2.653, 32.300], mean action: 3.500 [0.000, 16.000],  loss: 0.019596, mae: 0.306153, mean_q: 0.525413, mean_eps: 0.000000
 1426/5000: episode: 60, duration: 0.314s, episode steps:  19, steps per second:  60, episode reward: 47.435, mean reward:  2.497 [ 0.110, 32.363], mean action: 1.263 [0.000, 3.000],  loss: 0.022682, mae: 0.322401, mean_q: 0.528456, mean_eps: 0.000000
 1446/5000: episode: 61, duration: 0.318s, episode steps:  20, steps per second:  63, episode reward: 38.263, mean reward:  1.913 [-2.393, 32.053], mean action: 3.200 [0.000, 15.000],  loss: 0.022100, mae: 0.317253, mean_q: 0.490604, mean_eps: 0.000000
 1463/5000: episode: 62, duration: 0.259s, episode steps:  17, steps per second:  66, episode reward: 35.886, mean reward:  2.111 [-2.900, 32.246], mean action: 3.941 [0.000, 15.000],  loss: 0.019662, mae: 0.310858, mean_q: 0.474073, mean_eps: 0.000000
 1491/5000: episode: 63, duration: 0.419s, episode steps:  28, steps per second:  67, episode reward: 41.045, mean reward:  1.466 [-2.414, 32.380], mean action: 2.321 [0.000, 11.000],  loss: 0.021300, mae: 0.311766, mean_q: 0.541761, mean_eps: 0.000000
 1516/5000: episode: 64, duration: 0.366s, episode steps:  25, steps per second:  68, episode reward: 39.000, mean reward:  1.560 [-2.353, 32.660], mean action: 3.320 [0.000, 11.000],  loss: 0.021659, mae: 0.312522, mean_q: 0.559955, mean_eps: 0.000000
 1534/5000: episode: 65, duration: 0.293s, episode steps:  18, steps per second:  61, episode reward: 39.000, mean reward:  2.167 [-3.000, 33.000], mean action: 2.889 [0.000, 12.000],  loss: 0.019904, mae: 0.302290, mean_q: 0.571397, mean_eps: 0.000000
 1551/5000: episode: 66, duration: 0.273s, episode steps:  17, steps per second:  62, episode reward: 38.714, mean reward:  2.277 [-2.526, 31.904], mean action: 5.588 [0.000, 14.000],  loss: 0.015291, mae: 0.278662, mean_q: 0.572767, mean_eps: 0.000000
 1574/5000: episode: 67, duration: 0.344s, episode steps:  23, steps per second:  67, episode reward: 35.552, mean reward:  1.546 [-2.473, 32.160], mean action: 4.957 [0.000, 20.000],  loss: 0.024108, mae: 0.324517, mean_q: 0.584796, mean_eps: 0.000000
 1599/5000: episode: 68, duration: 0.375s, episode steps:  25, steps per second:  67, episode reward: 32.510, mean reward:  1.300 [-3.000, 31.953], mean action: 2.960 [0.000, 12.000],  loss: 0.019073, mae: 0.306016, mean_q: 0.589335, mean_eps: 0.000000
 1625/5000: episode: 69, duration: 0.373s, episode steps:  26, steps per second:  70, episode reward: 32.525, mean reward:  1.251 [-3.000, 32.060], mean action: 6.346 [0.000, 14.000],  loss: 0.023343, mae: 0.325533, mean_q: 0.575171, mean_eps: 0.000000
 1644/5000: episode: 70, duration: 0.289s, episode steps:  19, steps per second:  66, episode reward: 41.031, mean reward:  2.160 [-2.725, 32.130], mean action: 2.789 [0.000, 11.000],  loss: 0.016759, mae: 0.289239, mean_q: 0.541793, mean_eps: 0.000000
 1665/5000: episode: 71, duration: 0.323s, episode steps:  21, steps per second:  65, episode reward: 33.000, mean reward:  1.571 [-2.900, 30.219], mean action: 5.238 [1.000, 19.000],  loss: 0.019766, mae: 0.304874, mean_q: 0.562157, mean_eps: 0.000000
 1692/5000: episode: 72, duration: 0.395s, episode steps:  27, steps per second:  68, episode reward: 32.157, mean reward:  1.191 [-2.496, 32.022], mean action: 3.926 [0.000, 19.000],  loss: 0.022068, mae: 0.315569, mean_q: 0.519385, mean_eps: 0.000000
 1708/5000: episode: 73, duration: 0.267s, episode steps:  16, steps per second:  60, episode reward: 38.702, mean reward:  2.419 [-2.590, 32.042], mean action: 4.562 [0.000, 19.000],  loss: 0.016806, mae: 0.289136, mean_q: 0.498852, mean_eps: 0.000000
 1726/5000: episode: 74, duration: 0.266s, episode steps:  18, steps per second:  68, episode reward: 37.668, mean reward:  2.093 [-3.000, 32.160], mean action: 5.556 [0.000, 20.000],  loss: 0.016140, mae: 0.285083, mean_q: 0.480041, mean_eps: 0.000000
 1744/5000: episode: 75, duration: 0.279s, episode steps:  18, steps per second:  65, episode reward: 41.131, mean reward:  2.285 [-2.514, 32.640], mean action: 2.500 [0.000, 12.000],  loss: 0.021346, mae: 0.313334, mean_q: 0.442081, mean_eps: 0.000000
 1764/5000: episode: 76, duration: 0.306s, episode steps:  20, steps per second:  65, episode reward: 38.179, mean reward:  1.909 [-2.498, 32.902], mean action: 4.600 [0.000, 14.000],  loss: 0.017881, mae: 0.290722, mean_q: 0.455871, mean_eps: 0.000000
 1784/5000: episode: 77, duration: 0.306s, episode steps:  20, steps per second:  65, episode reward: -35.440, mean reward: -1.772 [-32.168,  2.170], mean action: 3.750 [0.000, 12.000],  loss: 0.018168, mae: 0.290810, mean_q: 0.506745, mean_eps: 0.000000
 1808/5000: episode: 78, duration: 0.352s, episode steps:  24, steps per second:  68, episode reward: 32.768, mean reward:  1.365 [-3.000, 32.150], mean action: 3.375 [0.000, 14.000],  loss: 0.020415, mae: 0.304541, mean_q: 0.498601, mean_eps: 0.000000
 1837/5000: episode: 79, duration: 0.444s, episode steps:  29, steps per second:  65, episode reward: -35.590, mean reward: -1.227 [-33.000,  2.668], mean action: 4.828 [0.000, 20.000],  loss: 0.022232, mae: 0.314842, mean_q: 0.565498, mean_eps: 0.000000
 1858/5000: episode: 80, duration: 0.308s, episode steps:  21, steps per second:  68, episode reward: 40.644, mean reward:  1.935 [-2.230, 32.558], mean action: 2.905 [0.000, 9.000],  loss: 0.018117, mae: 0.294193, mean_q: 0.524777, mean_eps: 0.000000
 1880/5000: episode: 81, duration: 0.317s, episode steps:  22, steps per second:  69, episode reward: 33.000, mean reward:  1.500 [-3.000, 30.000], mean action: 2.773 [0.000, 12.000],  loss: 0.016319, mae: 0.281674, mean_q: 0.507061, mean_eps: 0.000000
 1899/5000: episode: 82, duration: 0.294s, episode steps:  19, steps per second:  65, episode reward: 36.000, mean reward:  1.895 [-2.702, 32.100], mean action: 4.316 [0.000, 12.000],  loss: 0.022356, mae: 0.313505, mean_q: 0.536584, mean_eps: 0.000000
 1923/5000: episode: 83, duration: 0.349s, episode steps:  24, steps per second:  69, episode reward: 38.419, mean reward:  1.601 [-3.000, 32.223], mean action: 4.250 [1.000, 19.000],  loss: 0.019475, mae: 0.301428, mean_q: 0.498734, mean_eps: 0.000000
 1941/5000: episode: 84, duration: 0.289s, episode steps:  18, steps per second:  62, episode reward: 33.000, mean reward:  1.833 [-2.558, 30.921], mean action: 4.667 [0.000, 19.000],  loss: 0.021277, mae: 0.311644, mean_q: 0.534788, mean_eps: 0.000000
 1966/5000: episode: 85, duration: 0.370s, episode steps:  25, steps per second:  68, episode reward: -41.710, mean reward: -1.668 [-32.230,  1.983], mean action: 3.600 [0.000, 19.000],  loss: 0.021400, mae: 0.307893, mean_q: 0.494158, mean_eps: 0.000000
 1976/5000: episode: 86, duration: 0.165s, episode steps:  10, steps per second:  60, episode reward: 44.005, mean reward:  4.400 [-2.582, 33.000], mean action: 1.800 [0.000, 11.000],  loss: 0.017886, mae: 0.299855, mean_q: 0.498825, mean_eps: 0.000000
 1996/5000: episode: 87, duration: 0.307s, episode steps:  20, steps per second:  65, episode reward: 33.000, mean reward:  1.650 [-3.000, 30.525], mean action: 6.400 [0.000, 16.000],  loss: 0.019678, mae: 0.302593, mean_q: 0.470304, mean_eps: 0.000000
 2005/5000: episode: 88, duration: 0.149s, episode steps:   9, steps per second:  60, episode reward: 47.430, mean reward:  5.270 [ 0.598, 33.000], mean action: 1.333 [0.000, 2.000],  loss: 0.017319, mae: 0.291671, mean_q: 0.488676, mean_eps: 0.000000
 2028/5000: episode: 89, duration: 0.355s, episode steps:  23, steps per second:  65, episode reward: 38.692, mean reward:  1.682 [-3.000, 31.865], mean action: 3.609 [0.000, 19.000],  loss: 0.023336, mae: 0.318002, mean_q: 0.494398, mean_eps: 0.000000
 2074/5000: episode: 90, duration: 0.681s, episode steps:  46, steps per second:  68, episode reward: 33.000, mean reward:  0.717 [-3.000, 30.172], mean action: 1.522 [0.000, 15.000],  loss: 0.021913, mae: 0.301838, mean_q: 0.518251, mean_eps: 0.000000
 2094/5000: episode: 91, duration: 0.316s, episode steps:  20, steps per second:  63, episode reward: 38.854, mean reward:  1.943 [-2.099, 33.000], mean action: 3.250 [0.000, 16.000],  loss: 0.023317, mae: 0.320942, mean_q: 0.487488, mean_eps: 0.000000
 2128/5000: episode: 92, duration: 0.515s, episode steps:  34, steps per second:  66, episode reward: 32.103, mean reward:  0.944 [-2.705, 32.490], mean action: 4.794 [0.000, 15.000],  loss: 0.022219, mae: 0.315392, mean_q: 0.542282, mean_eps: 0.000000
 2151/5000: episode: 93, duration: 0.347s, episode steps:  23, steps per second:  66, episode reward: -30.000, mean reward: -1.304 [-30.980,  2.320], mean action: 6.217 [0.000, 18.000],  loss: 0.022057, mae: 0.313064, mean_q: 0.599267, mean_eps: 0.000000
 2175/5000: episode: 94, duration: 0.370s, episode steps:  24, steps per second:  65, episode reward: -32.070, mean reward: -1.336 [-31.782,  2.903], mean action: 4.583 [0.000, 15.000],  loss: 0.019787, mae: 0.303619, mean_q: 0.586465, mean_eps: 0.000000
 2193/5000: episode: 95, duration: 0.299s, episode steps:  18, steps per second:  60, episode reward: 40.873, mean reward:  2.271 [-2.502, 32.160], mean action: 4.111 [0.000, 21.000],  loss: 0.019279, mae: 0.302811, mean_q: 0.563310, mean_eps: 0.000000
 2216/5000: episode: 96, duration: 0.359s, episode steps:  23, steps per second:  64, episode reward: 32.753, mean reward:  1.424 [-3.000, 32.753], mean action: 3.739 [0.000, 16.000],  loss: 0.019452, mae: 0.307742, mean_q: 0.519956, mean_eps: 0.000000
 2246/5000: episode: 97, duration: 0.426s, episode steps:  30, steps per second:  70, episode reward: -39.000, mean reward: -1.300 [-32.323,  2.220], mean action: 8.600 [0.000, 18.000],  loss: 0.021603, mae: 0.318038, mean_q: 0.560751, mean_eps: 0.000000
 2262/5000: episode: 98, duration: 0.246s, episode steps:  16, steps per second:  65, episode reward: 38.764, mean reward:  2.423 [-2.900, 32.474], mean action: 3.312 [1.000, 15.000],  loss: 0.019046, mae: 0.307389, mean_q: 0.567528, mean_eps: 0.000000
 2286/5000: episode: 99, duration: 0.349s, episode steps:  24, steps per second:  69, episode reward: -33.000, mean reward: -1.375 [-32.405,  2.441], mean action: 4.333 [0.000, 16.000],  loss: 0.023309, mae: 0.326161, mean_q: 0.487128, mean_eps: 0.000000
 2310/5000: episode: 100, duration: 0.360s, episode steps:  24, steps per second:  67, episode reward: 37.993, mean reward:  1.583 [-2.622, 32.544], mean action: 6.792 [0.000, 16.000],  loss: 0.021683, mae: 0.324972, mean_q: 0.450491, mean_eps: 0.000000
 2341/5000: episode: 101, duration: 0.450s, episode steps:  31, steps per second:  69, episode reward: -39.000, mean reward: -1.258 [-32.103,  2.564], mean action: 4.000 [0.000, 20.000],  loss: 0.023382, mae: 0.323260, mean_q: 0.537795, mean_eps: 0.000000
 2365/5000: episode: 102, duration: 0.363s, episode steps:  24, steps per second:  66, episode reward: 35.810, mean reward:  1.492 [-2.314, 32.620], mean action: 2.208 [0.000, 12.000],  loss: 0.022278, mae: 0.312231, mean_q: 0.594162, mean_eps: 0.000000
 2381/5000: episode: 103, duration: 0.256s, episode steps:  16, steps per second:  63, episode reward: 41.250, mean reward:  2.578 [-2.217, 32.016], mean action: 2.250 [0.000, 11.000],  loss: 0.017428, mae: 0.290084, mean_q: 0.559219, mean_eps: 0.000000
 2406/5000: episode: 104, duration: 0.365s, episode steps:  25, steps per second:  68, episode reward: -32.510, mean reward: -1.300 [-32.510,  2.734], mean action: 7.320 [0.000, 19.000],  loss: 0.020368, mae: 0.305678, mean_q: 0.505839, mean_eps: 0.000000
 2425/5000: episode: 105, duration: 0.289s, episode steps:  19, steps per second:  66, episode reward: 38.488, mean reward:  2.026 [-3.000, 32.034], mean action: 4.158 [0.000, 19.000],  loss: 0.023708, mae: 0.318197, mean_q: 0.473603, mean_eps: 0.000000
 2445/5000: episode: 106, duration: 0.315s, episode steps:  20, steps per second:  64, episode reward: 41.084, mean reward:  2.054 [-2.114, 32.440], mean action: 5.900 [0.000, 19.000],  loss: 0.018814, mae: 0.295183, mean_q: 0.499889, mean_eps: 0.000000
 2463/5000: episode: 107, duration: 0.300s, episode steps:  18, steps per second:  60, episode reward: 45.000, mean reward:  2.500 [-2.037, 32.170], mean action: 1.500 [0.000, 13.000],  loss: 0.023030, mae: 0.313366, mean_q: 0.519326, mean_eps: 0.000000
 2480/5000: episode: 108, duration: 0.280s, episode steps:  17, steps per second:  61, episode reward: 38.360, mean reward:  2.256 [-2.442, 32.739], mean action: 5.824 [0.000, 19.000],  loss: 0.019522, mae: 0.302148, mean_q: 0.566794, mean_eps: 0.000000
 2501/5000: episode: 109, duration: 0.350s, episode steps:  21, steps per second:  60, episode reward: 32.776, mean reward:  1.561 [-3.000, 31.956], mean action: 5.714 [0.000, 19.000],  loss: 0.018201, mae: 0.296068, mean_q: 0.523481, mean_eps: 0.000000
 2524/5000: episode: 110, duration: 0.348s, episode steps:  23, steps per second:  66, episode reward: 35.262, mean reward:  1.533 [-2.988, 31.502], mean action: 3.913 [0.000, 19.000],  loss: 0.019359, mae: 0.313118, mean_q: 0.456938, mean_eps: 0.000000
 2573/5000: episode: 111, duration: 0.712s, episode steps:  49, steps per second:  69, episode reward: 32.738, mean reward:  0.668 [-2.471, 32.072], mean action: 3.673 [0.000, 19.000],  loss: 0.021228, mae: 0.320299, mean_q: 0.513051, mean_eps: 0.000000
 2597/5000: episode: 112, duration: 0.354s, episode steps:  24, steps per second:  68, episode reward: 39.000, mean reward:  1.625 [-3.000, 32.350], mean action: 3.833 [0.000, 19.000],  loss: 0.023336, mae: 0.339411, mean_q: 0.513491, mean_eps: 0.000000
 2628/5000: episode: 113, duration: 0.466s, episode steps:  31, steps per second:  66, episode reward: 35.311, mean reward:  1.139 [-2.434, 32.400], mean action: 6.452 [1.000, 19.000],  loss: 0.016050, mae: 0.310222, mean_q: 0.516819, mean_eps: 0.000000
 2641/5000: episode: 114, duration: 0.218s, episode steps:  13, steps per second:  60, episode reward: 44.154, mean reward:  3.396 [-2.216, 31.753], mean action: 2.154 [1.000, 3.000],  loss: 0.020064, mae: 0.338268, mean_q: 0.473886, mean_eps: 0.000000
 2652/5000: episode: 115, duration: 0.194s, episode steps:  11, steps per second:  57, episode reward: 44.691, mean reward:  4.063 [-2.041, 31.837], mean action: 1.727 [0.000, 12.000],  loss: 0.020080, mae: 0.330677, mean_q: 0.543566, mean_eps: 0.000000
 2669/5000: episode: 116, duration: 0.279s, episode steps:  17, steps per second:  61, episode reward: 41.016, mean reward:  2.413 [-3.000, 32.310], mean action: 1.824 [0.000, 11.000],  loss: 0.016721, mae: 0.315423, mean_q: 0.526520, mean_eps: 0.000000
 2689/5000: episode: 117, duration: 0.310s, episode steps:  20, steps per second:  65, episode reward: 35.781, mean reward:  1.789 [-2.502, 31.991], mean action: 3.450 [0.000, 14.000],  loss: 0.021106, mae: 0.331729, mean_q: 0.523954, mean_eps: 0.000000
 2717/5000: episode: 118, duration: 0.462s, episode steps:  28, steps per second:  61, episode reward: -32.530, mean reward: -1.162 [-31.684,  2.368], mean action: 4.250 [2.000, 14.000],  loss: 0.017948, mae: 0.310257, mean_q: 0.501837, mean_eps: 0.000000
 2739/5000: episode: 119, duration: 0.343s, episode steps:  22, steps per second:  64, episode reward: 35.183, mean reward:  1.599 [-2.287, 32.298], mean action: 4.318 [0.000, 12.000],  loss: 0.021213, mae: 0.324801, mean_q: 0.605069, mean_eps: 0.000000
 2775/5000: episode: 120, duration: 0.532s, episode steps:  36, steps per second:  68, episode reward: 30.000, mean reward:  0.833 [-3.000, 30.000], mean action: 7.306 [1.000, 19.000],  loss: 0.019420, mae: 0.322700, mean_q: 0.575067, mean_eps: 0.000000
 2802/5000: episode: 121, duration: 0.390s, episode steps:  27, steps per second:  69, episode reward: 32.665, mean reward:  1.210 [-3.000, 31.735], mean action: 4.185 [0.000, 19.000],  loss: 0.019457, mae: 0.317602, mean_q: 0.513569, mean_eps: 0.000000
 2823/5000: episode: 122, duration: 0.330s, episode steps:  21, steps per second:  64, episode reward: 35.075, mean reward:  1.670 [-2.824, 32.075], mean action: 5.000 [1.000, 19.000],  loss: 0.023180, mae: 0.330719, mean_q: 0.539624, mean_eps: 0.000000
 2839/5000: episode: 123, duration: 0.263s, episode steps:  16, steps per second:  61, episode reward: 41.040, mean reward:  2.565 [-2.693, 32.290], mean action: 3.750 [0.000, 16.000],  loss: 0.017913, mae: 0.305892, mean_q: 0.530568, mean_eps: 0.000000
 2868/5000: episode: 124, duration: 0.435s, episode steps:  29, steps per second:  67, episode reward: -32.810, mean reward: -1.131 [-32.900,  2.247], mean action: 4.517 [0.000, 19.000],  loss: 0.021854, mae: 0.328634, mean_q: 0.567220, mean_eps: 0.000000
 2891/5000: episode: 125, duration: 0.507s, episode steps:  23, steps per second:  45, episode reward: 35.088, mean reward:  1.526 [-3.000, 32.130], mean action: 5.348 [0.000, 16.000],  loss: 0.021604, mae: 0.317596, mean_q: 0.547401, mean_eps: 0.000000
 2908/5000: episode: 126, duration: 0.262s, episode steps:  17, steps per second:  65, episode reward: 38.743, mean reward:  2.279 [-2.901, 32.743], mean action: 4.294 [0.000, 19.000],  loss: 0.023853, mae: 0.340735, mean_q: 0.521133, mean_eps: 0.000000
 2927/5000: episode: 127, duration: 0.301s, episode steps:  19, steps per second:  63, episode reward: 38.382, mean reward:  2.020 [-2.498, 32.200], mean action: 3.737 [0.000, 19.000],  loss: 0.017640, mae: 0.315245, mean_q: 0.506088, mean_eps: 0.000000
 2956/5000: episode: 128, duration: 0.449s, episode steps:  29, steps per second:  65, episode reward: -32.970, mean reward: -1.137 [-32.148,  2.892], mean action: 8.793 [0.000, 19.000],  loss: 0.021192, mae: 0.331451, mean_q: 0.470602, mean_eps: 0.000000
 2976/5000: episode: 129, duration: 0.294s, episode steps:  20, steps per second:  68, episode reward: 38.834, mean reward:  1.942 [-2.902, 33.000], mean action: 5.400 [0.000, 19.000],  loss: 0.018055, mae: 0.305908, mean_q: 0.472053, mean_eps: 0.000000
 2997/5000: episode: 130, duration: 0.323s, episode steps:  21, steps per second:  65, episode reward: 33.000, mean reward:  1.571 [-3.000, 32.230], mean action: 5.857 [0.000, 19.000],  loss: 0.016912, mae: 0.297884, mean_q: 0.420310, mean_eps: 0.000000
 3027/5000: episode: 131, duration: 0.436s, episode steps:  30, steps per second:  69, episode reward: 39.000, mean reward:  1.300 [-2.096, 32.750], mean action: 2.900 [0.000, 19.000],  loss: 0.018847, mae: 0.308992, mean_q: 0.459015, mean_eps: 0.000000
 3151/5000: episode: 132, duration: 1.764s, episode steps: 124, steps per second:  70, episode reward: 33.000, mean reward:  0.266 [-2.667, 32.180], mean action: 17.226 [0.000, 21.000],  loss: 0.022841, mae: 0.330593, mean_q: 0.506115, mean_eps: 0.000000
 3181/5000: episode: 133, duration: 0.430s, episode steps:  30, steps per second:  70, episode reward: 35.388, mean reward:  1.180 [-2.327, 32.072], mean action: 3.667 [0.000, 20.000],  loss: 0.022131, mae: 0.326546, mean_q: 0.578623, mean_eps: 0.000000
 3202/5000: episode: 134, duration: 0.317s, episode steps:  21, steps per second:  66, episode reward: 38.886, mean reward:  1.852 [-2.323, 32.096], mean action: 3.000 [0.000, 12.000],  loss: 0.017445, mae: 0.304569, mean_q: 0.547588, mean_eps: 0.000000
 3220/5000: episode: 135, duration: 0.276s, episode steps:  18, steps per second:  65, episode reward: 41.178, mean reward:  2.288 [-2.438, 31.923], mean action: 1.389 [0.000, 11.000],  loss: 0.022712, mae: 0.323344, mean_q: 0.537669, mean_eps: 0.000000
 3235/5000: episode: 136, duration: 0.237s, episode steps:  15, steps per second:  63, episode reward: 41.743, mean reward:  2.783 [-2.502, 31.843], mean action: 2.600 [0.000, 12.000],  loss: 0.017595, mae: 0.305675, mean_q: 0.523773, mean_eps: 0.000000
 3268/5000: episode: 137, duration: 0.475s, episode steps:  33, steps per second:  70, episode reward: 35.709, mean reward:  1.082 [-3.000, 32.630], mean action: 3.273 [0.000, 19.000],  loss: 0.017403, mae: 0.303020, mean_q: 0.579322, mean_eps: 0.000000
 3288/5000: episode: 138, duration: 0.299s, episode steps:  20, steps per second:  67, episode reward: 40.147, mean reward:  2.007 [-2.850, 32.400], mean action: 6.650 [0.000, 20.000],  loss: 0.018650, mae: 0.309345, mean_q: 0.614267, mean_eps: 0.000000
 3321/5000: episode: 139, duration: 0.472s, episode steps:  33, steps per second:  70, episode reward: 32.909, mean reward:  0.997 [-2.329, 32.420], mean action: 6.970 [0.000, 19.000],  loss: 0.021150, mae: 0.323584, mean_q: 0.578463, mean_eps: 0.000000
 3346/5000: episode: 140, duration: 0.367s, episode steps:  25, steps per second:  68, episode reward: 36.000, mean reward:  1.440 [-2.384, 32.330], mean action: 3.200 [0.000, 15.000],  loss: 0.018427, mae: 0.311435, mean_q: 0.591270, mean_eps: 0.000000
 3372/5000: episode: 141, duration: 0.414s, episode steps:  26, steps per second:  63, episode reward: -32.500, mean reward: -1.250 [-32.160,  2.409], mean action: 6.192 [0.000, 21.000],  loss: 0.018434, mae: 0.312742, mean_q: 0.598089, mean_eps: 0.000000
 3409/5000: episode: 142, duration: 0.535s, episode steps:  37, steps per second:  69, episode reward: 32.549, mean reward:  0.880 [-2.545, 31.887], mean action: 6.514 [0.000, 18.000],  loss: 0.020556, mae: 0.312570, mean_q: 0.561996, mean_eps: 0.000000
 3433/5000: episode: 143, duration: 0.369s, episode steps:  24, steps per second:  65, episode reward: 32.129, mean reward:  1.339 [-3.000, 32.550], mean action: 5.458 [0.000, 19.000],  loss: 0.023427, mae: 0.330402, mean_q: 0.539543, mean_eps: 0.000000
 3453/5000: episode: 144, duration: 0.309s, episode steps:  20, steps per second:  65, episode reward: 38.817, mean reward:  1.941 [-2.630, 32.300], mean action: 4.350 [0.000, 19.000],  loss: 0.019175, mae: 0.319277, mean_q: 0.562372, mean_eps: 0.000000
 3479/5000: episode: 145, duration: 0.389s, episode steps:  26, steps per second:  67, episode reward: 32.889, mean reward:  1.265 [-3.000, 32.110], mean action: 5.154 [0.000, 20.000],  loss: 0.019692, mae: 0.312338, mean_q: 0.550912, mean_eps: 0.000000
 3500/5000: episode: 146, duration: 0.313s, episode steps:  21, steps per second:  67, episode reward: -32.840, mean reward: -1.564 [-31.991,  2.380], mean action: 5.095 [0.000, 19.000],  loss: 0.023990, mae: 0.332895, mean_q: 0.548798, mean_eps: 0.000000
 3522/5000: episode: 147, duration: 0.324s, episode steps:  22, steps per second:  68, episode reward: 38.428, mean reward:  1.747 [-2.155, 31.869], mean action: 6.500 [0.000, 20.000],  loss: 0.023008, mae: 0.328955, mean_q: 0.562000, mean_eps: 0.000000
 3558/5000: episode: 148, duration: 0.519s, episode steps:  36, steps per second:  69, episode reward: -35.090, mean reward: -0.975 [-32.054,  2.290], mean action: 7.667 [0.000, 21.000],  loss: 0.021205, mae: 0.320620, mean_q: 0.575571, mean_eps: 0.000000
 3574/5000: episode: 149, duration: 0.259s, episode steps:  16, steps per second:  62, episode reward: 38.357, mean reward:  2.397 [-2.189, 32.354], mean action: 3.750 [0.000, 11.000],  loss: 0.021440, mae: 0.325651, mean_q: 0.549139, mean_eps: 0.000000
 3599/5000: episode: 150, duration: 0.351s, episode steps:  25, steps per second:  71, episode reward: 32.139, mean reward:  1.286 [-3.000, 32.304], mean action: 5.800 [0.000, 19.000],  loss: 0.019189, mae: 0.325047, mean_q: 0.542308, mean_eps: 0.000000
 3617/5000: episode: 151, duration: 0.270s, episode steps:  18, steps per second:  67, episode reward: 42.000, mean reward:  2.333 [-2.202, 33.000], mean action: 1.889 [0.000, 11.000],  loss: 0.020130, mae: 0.322831, mean_q: 0.532828, mean_eps: 0.000000
 3639/5000: episode: 152, duration: 0.328s, episode steps:  22, steps per second:  67, episode reward: 32.401, mean reward:  1.473 [-3.000, 31.761], mean action: 4.636 [0.000, 19.000],  loss: 0.021066, mae: 0.331181, mean_q: 0.547147, mean_eps: 0.000000
 3650/5000: episode: 153, duration: 0.191s, episode steps:  11, steps per second:  58, episode reward: 47.319, mean reward:  4.302 [ 0.000, 33.000], mean action: 2.545 [0.000, 12.000],  loss: 0.023977, mae: 0.340394, mean_q: 0.539898, mean_eps: 0.000000
 3671/5000: episode: 154, duration: 0.335s, episode steps:  21, steps per second:  63, episode reward: 38.712, mean reward:  1.843 [-2.443, 32.020], mean action: 4.857 [0.000, 19.000],  loss: 0.020849, mae: 0.327565, mean_q: 0.554343, mean_eps: 0.000000
 3691/5000: episode: 155, duration: 0.329s, episode steps:  20, steps per second:  61, episode reward: -35.910, mean reward: -1.795 [-32.807,  2.900], mean action: 3.750 [0.000, 12.000],  loss: 0.019209, mae: 0.311284, mean_q: 0.566409, mean_eps: 0.000000
 3716/5000: episode: 156, duration: 0.358s, episode steps:  25, steps per second:  70, episode reward: -32.550, mean reward: -1.302 [-31.663,  2.320], mean action: 3.480 [0.000, 12.000],  loss: 0.020863, mae: 0.325162, mean_q: 0.560073, mean_eps: 0.000000
 3736/5000: episode: 157, duration: 0.298s, episode steps:  20, steps per second:  67, episode reward: 33.000, mean reward:  1.650 [-3.000, 30.121], mean action: 4.850 [0.000, 14.000],  loss: 0.022900, mae: 0.330848, mean_q: 0.500150, mean_eps: 0.000000
 3766/5000: episode: 158, duration: 0.455s, episode steps:  30, steps per second:  66, episode reward: 38.098, mean reward:  1.270 [-2.494, 31.710], mean action: 7.900 [0.000, 19.000],  loss: 0.022922, mae: 0.332958, mean_q: 0.456841, mean_eps: 0.000000
 3810/5000: episode: 159, duration: 0.642s, episode steps:  44, steps per second:  69, episode reward: -36.000, mean reward: -0.818 [-32.358,  2.560], mean action: 4.750 [0.000, 13.000],  loss: 0.024113, mae: 0.324879, mean_q: 0.524549, mean_eps: 0.000000
 3829/5000: episode: 160, duration: 0.299s, episode steps:  19, steps per second:  64, episode reward: 38.483, mean reward:  2.025 [-2.240, 31.663], mean action: 1.947 [0.000, 12.000],  loss: 0.020490, mae: 0.309944, mean_q: 0.542685, mean_eps: 0.000000
 3850/5000: episode: 161, duration: 0.324s, episode steps:  21, steps per second:  65, episode reward: 35.752, mean reward:  1.702 [-2.903, 32.080], mean action: 3.762 [0.000, 19.000],  loss: 0.019893, mae: 0.306276, mean_q: 0.543936, mean_eps: 0.000000
 3872/5000: episode: 162, duration: 0.332s, episode steps:  22, steps per second:  66, episode reward: 33.808, mean reward:  1.537 [-3.000, 31.939], mean action: 7.045 [1.000, 19.000],  loss: 0.023085, mae: 0.320293, mean_q: 0.498779, mean_eps: 0.000000
 3896/5000: episode: 163, duration: 0.377s, episode steps:  24, steps per second:  64, episode reward: 41.396, mean reward:  1.725 [-2.237, 32.014], mean action: 2.625 [0.000, 19.000],  loss: 0.018328, mae: 0.290733, mean_q: 0.487747, mean_eps: 0.000000
 3912/5000: episode: 164, duration: 0.238s, episode steps:  16, steps per second:  67, episode reward: 35.566, mean reward:  2.223 [-3.000, 32.794], mean action: 6.812 [0.000, 19.000],  loss: 0.019109, mae: 0.298099, mean_q: 0.494305, mean_eps: 0.000000
 3941/5000: episode: 165, duration: 0.443s, episode steps:  29, steps per second:  65, episode reward: 35.273, mean reward:  1.216 [-2.385, 32.510], mean action: 6.690 [0.000, 19.000],  loss: 0.022955, mae: 0.318109, mean_q: 0.512322, mean_eps: 0.000000
 3965/5000: episode: 166, duration: 0.514s, episode steps:  24, steps per second:  47, episode reward: 32.795, mean reward:  1.366 [-2.696, 31.935], mean action: 5.208 [0.000, 19.000],  loss: 0.020550, mae: 0.309547, mean_q: 0.461533, mean_eps: 0.000000
 3984/5000: episode: 167, duration: 0.288s, episode steps:  19, steps per second:  66, episode reward: 38.735, mean reward:  2.039 [-3.000, 33.000], mean action: 3.947 [0.000, 19.000],  loss: 0.023066, mae: 0.319351, mean_q: 0.506076, mean_eps: 0.000000
 4006/5000: episode: 168, duration: 0.331s, episode steps:  22, steps per second:  66, episode reward: 35.131, mean reward:  1.597 [-2.436, 32.270], mean action: 8.227 [0.000, 20.000],  loss: 0.020794, mae: 0.308160, mean_q: 0.538983, mean_eps: 0.000000
 4025/5000: episode: 169, duration: 0.297s, episode steps:  19, steps per second:  64, episode reward: 35.798, mean reward:  1.884 [-2.367, 32.508], mean action: 3.947 [0.000, 12.000],  loss: 0.021390, mae: 0.308416, mean_q: 0.544762, mean_eps: 0.000000
 4042/5000: episode: 170, duration: 0.273s, episode steps:  17, steps per second:  62, episode reward: 38.684, mean reward:  2.276 [-3.000, 32.434], mean action: 4.000 [0.000, 11.000],  loss: 0.023848, mae: 0.315363, mean_q: 0.539379, mean_eps: 0.000000
 4071/5000: episode: 171, duration: 0.465s, episode steps:  29, steps per second:  62, episode reward: -36.000, mean reward: -1.241 [-32.231,  2.720], mean action: 6.690 [0.000, 19.000],  loss: 0.016938, mae: 0.292503, mean_q: 0.554065, mean_eps: 0.000000
 4096/5000: episode: 172, duration: 0.420s, episode steps:  25, steps per second:  60, episode reward: 35.645, mean reward:  1.426 [-2.903, 32.027], mean action: 4.880 [0.000, 13.000],  loss: 0.019692, mae: 0.305601, mean_q: 0.494507, mean_eps: 0.000000
 4122/5000: episode: 173, duration: 0.387s, episode steps:  26, steps per second:  67, episode reward: -32.940, mean reward: -1.267 [-32.307,  2.772], mean action: 6.231 [0.000, 19.000],  loss: 0.022129, mae: 0.318085, mean_q: 0.496749, mean_eps: 0.000000
 4141/5000: episode: 174, duration: 0.304s, episode steps:  19, steps per second:  62, episode reward: 33.000, mean reward:  1.737 [-3.000, 32.810], mean action: 8.211 [0.000, 21.000],  loss: 0.018220, mae: 0.299226, mean_q: 0.493770, mean_eps: 0.000000
 4158/5000: episode: 175, duration: 0.262s, episode steps:  17, steps per second:  65, episode reward: 41.186, mean reward:  2.423 [-2.130, 33.000], mean action: 5.176 [0.000, 15.000],  loss: 0.021138, mae: 0.315052, mean_q: 0.550448, mean_eps: 0.000000
 4185/5000: episode: 176, duration: 0.422s, episode steps:  27, steps per second:  64, episode reward: -35.360, mean reward: -1.310 [-32.104,  2.230], mean action: 7.778 [0.000, 15.000],  loss: 0.024831, mae: 0.328895, mean_q: 0.534315, mean_eps: 0.000000
 4211/5000: episode: 177, duration: 0.376s, episode steps:  26, steps per second:  69, episode reward: 32.322, mean reward:  1.243 [-3.000, 32.093], mean action: 4.462 [0.000, 15.000],  loss: 0.020736, mae: 0.307428, mean_q: 0.535877, mean_eps: 0.000000
 4232/5000: episode: 178, duration: 0.300s, episode steps:  21, steps per second:  70, episode reward: 33.000, mean reward:  1.571 [-3.000, 32.480], mean action: 5.381 [1.000, 15.000],  loss: 0.019562, mae: 0.301049, mean_q: 0.511580, mean_eps: 0.000000
 4255/5000: episode: 179, duration: 0.344s, episode steps:  23, steps per second:  67, episode reward: 38.389, mean reward:  1.669 [-2.347, 31.978], mean action: 5.000 [0.000, 13.000],  loss: 0.022460, mae: 0.320401, mean_q: 0.513234, mean_eps: 0.000000
 4270/5000: episode: 180, duration: 0.237s, episode steps:  15, steps per second:  63, episode reward: 38.106, mean reward:  2.540 [-3.000, 32.732], mean action: 4.333 [0.000, 11.000],  loss: 0.022882, mae: 0.321536, mean_q: 0.520663, mean_eps: 0.000000
 4287/5000: episode: 181, duration: 0.248s, episode steps:  17, steps per second:  68, episode reward: 35.438, mean reward:  2.085 [-3.000, 31.985], mean action: 6.529 [1.000, 16.000],  loss: 0.019952, mae: 0.307246, mean_q: 0.513058, mean_eps: 0.000000
 4304/5000: episode: 182, duration: 0.314s, episode steps:  17, steps per second:  54, episode reward: 38.097, mean reward:  2.241 [-2.398, 32.490], mean action: 3.294 [0.000, 16.000],  loss: 0.019048, mae: 0.300476, mean_q: 0.519442, mean_eps: 0.000000
 4333/5000: episode: 183, duration: 0.440s, episode steps:  29, steps per second:  66, episode reward: 32.438, mean reward:  1.119 [-3.000, 32.480], mean action: 5.586 [0.000, 14.000],  loss: 0.019408, mae: 0.306136, mean_q: 0.584452, mean_eps: 0.000000
 4353/5000: episode: 184, duration: 0.295s, episode steps:  20, steps per second:  68, episode reward: 41.318, mean reward:  2.066 [-3.000, 32.440], mean action: 3.950 [0.000, 19.000],  loss: 0.021112, mae: 0.311925, mean_q: 0.551734, mean_eps: 0.000000
 4380/5000: episode: 185, duration: 0.387s, episode steps:  27, steps per second:  70, episode reward: 35.659, mean reward:  1.321 [-2.330, 32.620], mean action: 6.370 [0.000, 19.000],  loss: 0.022125, mae: 0.319166, mean_q: 0.483782, mean_eps: 0.000000
 4401/5000: episode: 186, duration: 0.322s, episode steps:  21, steps per second:  65, episode reward: 44.195, mean reward:  2.105 [-2.230, 31.912], mean action: 3.000 [0.000, 15.000],  loss: 0.021945, mae: 0.318765, mean_q: 0.531685, mean_eps: 0.000000
 4427/5000: episode: 187, duration: 0.388s, episode steps:  26, steps per second:  67, episode reward: 32.763, mean reward:  1.260 [-2.449, 32.500], mean action: 7.346 [0.000, 21.000],  loss: 0.024728, mae: 0.329980, mean_q: 0.495033, mean_eps: 0.000000
 4441/5000: episode: 188, duration: 0.234s, episode steps:  14, steps per second:  60, episode reward: 41.726, mean reward:  2.980 [-2.631, 32.440], mean action: 3.071 [0.000, 12.000],  loss: 0.022282, mae: 0.319561, mean_q: 0.594545, mean_eps: 0.000000
 4465/5000: episode: 189, duration: 0.349s, episode steps:  24, steps per second:  69, episode reward: -32.050, mean reward: -1.335 [-31.951,  2.901], mean action: 5.958 [0.000, 12.000],  loss: 0.023096, mae: 0.328079, mean_q: 0.655237, mean_eps: 0.000000
 4486/5000: episode: 190, duration: 0.303s, episode steps:  21, steps per second:  69, episode reward: 36.000, mean reward:  1.714 [-2.276, 32.350], mean action: 5.190 [1.000, 20.000],  loss: 0.017219, mae: 0.305287, mean_q: 0.642210, mean_eps: 0.000000
 4520/5000: episode: 191, duration: 0.481s, episode steps:  34, steps per second:  71, episode reward: -41.600, mean reward: -1.224 [-31.800,  2.190], mean action: 4.559 [0.000, 12.000],  loss: 0.022319, mae: 0.330044, mean_q: 0.591457, mean_eps: 0.000000
 4551/5000: episode: 192, duration: 0.474s, episode steps:  31, steps per second:  65, episode reward: -30.000, mean reward: -0.968 [-30.210,  2.490], mean action: 7.968 [0.000, 17.000],  loss: 0.018878, mae: 0.313211, mean_q: 0.563525, mean_eps: 0.000000
 4574/5000: episode: 193, duration: 0.354s, episode steps:  23, steps per second:  65, episode reward: 38.105, mean reward:  1.657 [-2.344, 32.321], mean action: 3.652 [1.000, 11.000],  loss: 0.021385, mae: 0.321659, mean_q: 0.568292, mean_eps: 0.000000
 4598/5000: episode: 194, duration: 0.350s, episode steps:  24, steps per second:  69, episode reward: -38.790, mean reward: -1.616 [-32.208,  3.000], mean action: 9.708 [0.000, 19.000],  loss: 0.023154, mae: 0.332334, mean_q: 0.485333, mean_eps: 0.000000
 4627/5000: episode: 195, duration: 0.408s, episode steps:  29, steps per second:  71, episode reward: -32.610, mean reward: -1.124 [-32.463,  3.000], mean action: 6.414 [0.000, 19.000],  loss: 0.020490, mae: 0.313867, mean_q: 0.486884, mean_eps: 0.000000
 4668/5000: episode: 196, duration: 0.582s, episode steps:  41, steps per second:  70, episode reward: 34.700, mean reward:  0.846 [-3.000, 33.000], mean action: 3.683 [0.000, 17.000],  loss: 0.022326, mae: 0.330701, mean_q: 0.542342, mean_eps: 0.000000
 4693/5000: episode: 197, duration: 0.370s, episode steps:  25, steps per second:  68, episode reward: 32.130, mean reward:  1.285 [-3.000, 32.150], mean action: 3.680 [0.000, 19.000],  loss: 0.022384, mae: 0.328686, mean_q: 0.531087, mean_eps: 0.000000
 4713/5000: episode: 198, duration: 0.306s, episode steps:  20, steps per second:  65, episode reward: 35.207, mean reward:  1.760 [-3.000, 32.320], mean action: 3.800 [0.000, 11.000],  loss: 0.020951, mae: 0.322234, mean_q: 0.541119, mean_eps: 0.000000
 4731/5000: episode: 199, duration: 0.269s, episode steps:  18, steps per second:  67, episode reward: 35.641, mean reward:  1.980 [-2.355, 32.321], mean action: 4.111 [0.000, 11.000],  loss: 0.018250, mae: 0.313257, mean_q: 0.547168, mean_eps: 0.000000
 4757/5000: episode: 200, duration: 0.379s, episode steps:  26, steps per second:  69, episode reward: 35.903, mean reward:  1.381 [-2.400, 32.273], mean action: 3.500 [0.000, 19.000],  loss: 0.024781, mae: 0.343844, mean_q: 0.522917, mean_eps: 0.000000
 4779/5000: episode: 201, duration: 0.321s, episode steps:  22, steps per second:  68, episode reward: 35.500, mean reward:  1.614 [-2.614, 32.460], mean action: 3.227 [0.000, 11.000],  loss: 0.019760, mae: 0.310937, mean_q: 0.533940, mean_eps: 0.000000
 4804/5000: episode: 202, duration: 0.366s, episode steps:  25, steps per second:  68, episode reward: 38.624, mean reward:  1.545 [-3.000, 32.180], mean action: 4.520 [1.000, 14.000],  loss: 0.023348, mae: 0.324510, mean_q: 0.526743, mean_eps: 0.000000
 4835/5000: episode: 203, duration: 0.455s, episode steps:  31, steps per second:  68, episode reward: -32.780, mean reward: -1.057 [-32.268,  3.000], mean action: 5.097 [0.000, 19.000],  loss: 0.020906, mae: 0.315801, mean_q: 0.544509, mean_eps: 0.000000
 4855/5000: episode: 204, duration: 0.303s, episode steps:  20, steps per second:  66, episode reward: 34.452, mean reward:  1.723 [-3.000, 32.021], mean action: 5.250 [0.000, 19.000],  loss: 0.020174, mae: 0.311968, mean_q: 0.591120, mean_eps: 0.000000
 4874/5000: episode: 205, duration: 0.274s, episode steps:  19, steps per second:  69, episode reward: 33.000, mean reward:  1.737 [-3.000, 32.550], mean action: 5.737 [0.000, 19.000],  loss: 0.019334, mae: 0.312697, mean_q: 0.550081, mean_eps: 0.000000
 4902/5000: episode: 206, duration: 0.409s, episode steps:  28, steps per second:  68, episode reward: 40.900, mean reward:  1.461 [-2.444, 32.190], mean action: 5.000 [1.000, 20.000],  loss: 0.020496, mae: 0.312326, mean_q: 0.558477, mean_eps: 0.000000
 4920/5000: episode: 207, duration: 0.277s, episode steps:  18, steps per second:  65, episode reward: 35.438, mean reward:  1.969 [-2.564, 31.758], mean action: 6.000 [0.000, 19.000],  loss: 0.016677, mae: 0.296779, mean_q: 0.516099, mean_eps: 0.000000
 4945/5000: episode: 208, duration: 0.378s, episode steps:  25, steps per second:  66, episode reward: 35.012, mean reward:  1.400 [-3.000, 32.380], mean action: 4.760 [0.000, 20.000],  loss: 0.022109, mae: 0.314842, mean_q: 0.501909, mean_eps: 0.000000
 4967/5000: episode: 209, duration: 0.316s, episode steps:  22, steps per second:  70, episode reward: 33.000, mean reward:  1.500 [-3.000, 32.590], mean action: 7.545 [0.000, 19.000],  loss: 0.023378, mae: 0.325454, mean_q: 0.483063, mean_eps: 0.000000
 4991/5000: episode: 210, duration: 0.370s, episode steps:  24, steps per second:  65, episode reward: -33.000, mean reward: -1.375 [-32.405,  2.479], mean action: 7.208 [0.000, 19.000],  loss: 0.020187, mae: 0.312557, mean_q: 0.456917, mean_eps: 0.000000
done, took 84.894 seconds
DQN Evaluation: 13310 victories out of 15482 episodes
Results against random player:
DQN Evaluation: 483 victories out of 500 episodes

Results against max player:
DQN Evaluation: 387 victories out of 500 episodes
