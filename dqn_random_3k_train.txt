params 0
{'model_type': 'dqn_agent', 'l1_out': 128, 'l2_out': 64, 'gamma': 0.5, 'target_model_update': 1, 'delta_clip': 0.01, 'nb_steps_warmup': 1000, 'enable_dueling_dqn__': False, 'enable_double_dqn__': True, 'dueling_type__': 'avg'}
trial 0
Training for 30000 steps ...
    43/30000: episode: 1, duration: 0.512s, episode steps:  43, steps per second:  84, episode reward: -34.890, mean reward: -0.811 [-32.604,  2.901], mean action: 15.419 [3.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    89/30000: episode: 2, duration: 0.718s, episode steps:  46, steps per second:  64, episode reward: -32.660, mean reward: -0.710 [-32.176,  2.790], mean action: 15.348 [3.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   274/30000: episode: 3, duration: 1.055s, episode steps: 185, steps per second: 175, episode reward: 33.000, mean reward:  0.178 [-3.000, 32.234], mean action: 8.849 [1.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   338/30000: episode: 4, duration: 0.372s, episode steps:  64, steps per second: 172, episode reward: -34.810, mean reward: -0.544 [-32.420,  2.750], mean action: 14.016 [7.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   407/30000: episode: 5, duration: 0.382s, episode steps:  69, steps per second: 181, episode reward: -43.040, mean reward: -0.624 [-32.124,  2.620], mean action: 14.058 [6.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   462/30000: episode: 6, duration: 0.292s, episode steps:  55, steps per second: 188, episode reward: -40.630, mean reward: -0.739 [-31.996,  2.124], mean action: 14.073 [7.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   508/30000: episode: 7, duration: 0.464s, episode steps:  46, steps per second:  99, episode reward: 32.330, mean reward:  0.703 [-2.398, 32.275], mean action: 12.217 [3.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   603/30000: episode: 8, duration: 0.515s, episode steps:  95, steps per second: 185, episode reward: 32.700, mean reward:  0.344 [-2.351, 33.000], mean action: 13.716 [3.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   649/30000: episode: 9, duration: 0.254s, episode steps:  46, steps per second: 181, episode reward: 41.099, mean reward:  0.893 [-2.873, 31.953], mean action: 13.109 [3.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   769/30000: episode: 10, duration: 1.167s, episode steps: 120, steps per second: 103, episode reward: -32.650, mean reward: -0.272 [-32.009,  2.260], mean action: 16.583 [6.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   884/30000: episode: 11, duration: 0.664s, episode steps: 115, steps per second: 173, episode reward: -38.500, mean reward: -0.335 [-32.067,  2.880], mean action: 15.078 [1.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   949/30000: episode: 12, duration: 0.406s, episode steps:  65, steps per second: 160, episode reward: -37.480, mean reward: -0.577 [-32.067,  3.000], mean action: 15.846 [3.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   989/30000: episode: 13, duration: 0.276s, episode steps:  40, steps per second: 145, episode reward: -43.590, mean reward: -1.090 [-32.691,  2.020], mean action: 16.250 [10.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  1035/30000: episode: 14, duration: 1.153s, episode steps:  46, steps per second:  40, episode reward: 37.664, mean reward:  0.819 [-2.292, 34.700], mean action: 10.435 [1.000, 21.000],  loss: 0.009518, mae: 0.409807, mean_q: 0.361863, mean_eps: 0.000000
  1068/30000: episode: 15, duration: 0.403s, episode steps:  33, steps per second:  82, episode reward: 43.424, mean reward:  1.316 [-2.043, 32.350], mean action: 5.091 [1.000, 15.000],  loss: 0.008650, mae: 0.378404, mean_q: 0.227384, mean_eps: 0.000000
  1110/30000: episode: 16, duration: 0.440s, episode steps:  42, steps per second:  95, episode reward: 32.465, mean reward:  0.773 [-2.490, 32.270], mean action: 4.476 [1.000, 21.000],  loss: 0.008225, mae: 0.360766, mean_q: 0.238887, mean_eps: 0.000000
  1178/30000: episode: 17, duration: 0.928s, episode steps:  68, steps per second:  73, episode reward: 43.634, mean reward:  0.642 [-2.423, 32.460], mean action: 3.603 [1.000, 21.000],  loss: 0.008840, mae: 0.357859, mean_q: 0.209071, mean_eps: 0.000000
  1226/30000: episode: 18, duration: 0.828s, episode steps:  48, steps per second:  58, episode reward: -42.230, mean reward: -0.880 [-32.363,  2.219], mean action: 14.021 [1.000, 21.000],  loss: 0.007632, mae: 0.357749, mean_q: 0.184612, mean_eps: 0.000000
  1270/30000: episode: 19, duration: 0.724s, episode steps:  44, steps per second:  61, episode reward: 32.495, mean reward:  0.739 [-3.000, 32.140], mean action: 8.386 [1.000, 21.000],  loss: 0.009890, mae: 0.352752, mean_q: 0.199408, mean_eps: 0.000000
  1291/30000: episode: 20, duration: 0.246s, episode steps:  21, steps per second:  86, episode reward: 38.086, mean reward:  1.814 [-3.000, 31.940], mean action: 4.762 [1.000, 21.000],  loss: 0.009749, mae: 0.368945, mean_q: 0.168217, mean_eps: 0.000000
  1364/30000: episode: 21, duration: 1.020s, episode steps:  73, steps per second:  72, episode reward: 37.830, mean reward:  0.518 [-2.460, 32.133], mean action: 5.055 [1.000, 18.000],  loss: 0.007600, mae: 0.354377, mean_q: 0.194077, mean_eps: 0.000000
  1424/30000: episode: 22, duration: 0.648s, episode steps:  60, steps per second:  93, episode reward: 32.842, mean reward:  0.547 [-2.348, 32.150], mean action: 5.367 [1.000, 17.000],  loss: 0.008539, mae: 0.354321, mean_q: 0.206642, mean_eps: 0.000000
  1475/30000: episode: 23, duration: 1.637s, episode steps:  51, steps per second:  31, episode reward: -32.320, mean reward: -0.634 [-31.957,  2.761], mean action: 9.922 [1.000, 21.000],  loss: 0.007749, mae: 0.342809, mean_q: 0.213489, mean_eps: 0.000000
  1594/30000: episode: 24, duration: 1.818s, episode steps: 119, steps per second:  65, episode reward: -32.540, mean reward: -0.273 [-31.955,  2.376], mean action: 5.118 [1.000, 21.000],  loss: 0.008433, mae: 0.340367, mean_q: 0.217404, mean_eps: 0.000000
  1656/30000: episode: 25, duration: 0.946s, episode steps:  62, steps per second:  66, episode reward: -32.080, mean reward: -0.517 [-32.161,  2.623], mean action: 6.758 [1.000, 21.000],  loss: 0.007607, mae: 0.349598, mean_q: 0.197015, mean_eps: 0.000000
  1713/30000: episode: 26, duration: 0.580s, episode steps:  57, steps per second:  98, episode reward: -32.480, mean reward: -0.570 [-32.034,  2.623], mean action: 8.211 [1.000, 21.000],  loss: 0.009540, mae: 0.350263, mean_q: 0.183920, mean_eps: 0.000000
  1825/30000: episode: 27, duration: 1.151s, episode steps: 112, steps per second:  97, episode reward: -32.590, mean reward: -0.291 [-32.160,  3.000], mean action: 7.848 [1.000, 21.000],  loss: 0.009546, mae: 0.353406, mean_q: 0.167048, mean_eps: 0.000000
  1882/30000: episode: 28, duration: 0.757s, episode steps:  57, steps per second:  75, episode reward: 40.890, mean reward:  0.717 [-2.358, 32.110], mean action: 3.281 [0.000, 21.000],  loss: 0.009108, mae: 0.359251, mean_q: 0.163390, mean_eps: 0.000000
  1920/30000: episode: 29, duration: 0.417s, episode steps:  38, steps per second:  91, episode reward: 47.476, mean reward:  1.249 [-0.500, 32.063], mean action: 3.947 [3.000, 17.000],  loss: 0.008296, mae: 0.352536, mean_q: 0.170148, mean_eps: 0.000000
  1960/30000: episode: 30, duration: 0.452s, episode steps:  40, steps per second:  89, episode reward: 43.242, mean reward:  1.081 [-2.294, 31.816], mean action: 7.575 [0.000, 21.000],  loss: 0.009124, mae: 0.346075, mean_q: 0.197456, mean_eps: 0.000000
  1997/30000: episode: 31, duration: 0.601s, episode steps:  37, steps per second:  62, episode reward: 40.776, mean reward:  1.102 [-2.810, 32.370], mean action: 9.000 [3.000, 21.000],  loss: 0.009111, mae: 0.347021, mean_q: 0.175418, mean_eps: 0.000000
  2041/30000: episode: 32, duration: 0.664s, episode steps:  44, steps per second:  66, episode reward: 38.433, mean reward:  0.873 [-2.719, 32.180], mean action: 9.250 [0.000, 21.000],  loss: 0.010896, mae: 0.354962, mean_q: 0.156320, mean_eps: 0.000000
  2091/30000: episode: 33, duration: 0.955s, episode steps:  50, steps per second:  52, episode reward: -34.960, mean reward: -0.699 [-31.882,  2.300], mean action: 11.200 [3.000, 21.000],  loss: 0.010443, mae: 0.327526, mean_q: 0.211585, mean_eps: 0.000000
  2132/30000: episode: 34, duration: 0.969s, episode steps:  41, steps per second:  42, episode reward: 33.000, mean reward:  0.805 [-2.673, 32.940], mean action: 8.659 [0.000, 21.000],  loss: 0.009253, mae: 0.321576, mean_q: 0.214074, mean_eps: 0.000000
  2152/30000: episode: 35, duration: 0.585s, episode steps:  20, steps per second:  34, episode reward: 45.256, mean reward:  2.263 [-0.539, 32.292], mean action: 9.000 [0.000, 21.000],  loss: 0.009960, mae: 0.325958, mean_q: 0.197931, mean_eps: 0.000000
  2183/30000: episode: 36, duration: 0.547s, episode steps:  31, steps per second:  57, episode reward: 35.275, mean reward:  1.138 [-2.880, 32.150], mean action: 6.710 [1.000, 17.000],  loss: 0.008993, mae: 0.315478, mean_q: 0.236540, mean_eps: 0.000000
  2217/30000: episode: 37, duration: 0.677s, episode steps:  34, steps per second:  50, episode reward: 38.315, mean reward:  1.127 [-3.000, 32.290], mean action: 3.912 [0.000, 17.000],  loss: 0.010748, mae: 0.329853, mean_q: 0.228804, mean_eps: 0.000000
  2250/30000: episode: 38, duration: 0.412s, episode steps:  33, steps per second:  80, episode reward: 43.096, mean reward:  1.306 [-2.066, 31.973], mean action: 5.727 [1.000, 21.000],  loss: 0.011620, mae: 0.336459, mean_q: 0.209015, mean_eps: 0.000000
  2282/30000: episode: 39, duration: 0.752s, episode steps:  32, steps per second:  43, episode reward: 43.926, mean reward:  1.373 [-2.158, 31.937], mean action: 3.875 [0.000, 8.000],  loss: 0.009647, mae: 0.318691, mean_q: 0.237909, mean_eps: 0.000000
  2320/30000: episode: 40, duration: 0.726s, episode steps:  38, steps per second:  52, episode reward: -41.880, mean reward: -1.102 [-32.867,  2.272], mean action: 8.237 [0.000, 21.000],  loss: 0.010673, mae: 0.318473, mean_q: 0.259655, mean_eps: 0.000000
  2343/30000: episode: 41, duration: 0.333s, episode steps:  23, steps per second:  69, episode reward: 42.000, mean reward:  1.826 [-2.812, 32.230], mean action: 4.087 [0.000, 14.000],  loss: 0.007642, mae: 0.301839, mean_q: 0.233906, mean_eps: 0.000000
  2381/30000: episode: 42, duration: 0.470s, episode steps:  38, steps per second:  81, episode reward: -32.710, mean reward: -0.861 [-31.921,  2.519], mean action: 6.526 [1.000, 21.000],  loss: 0.009978, mae: 0.331308, mean_q: 0.232401, mean_eps: 0.000000
  2421/30000: episode: 43, duration: 0.581s, episode steps:  40, steps per second:  69, episode reward: 38.361, mean reward:  0.959 [-2.937, 32.320], mean action: 8.025 [1.000, 21.000],  loss: 0.010654, mae: 0.316490, mean_q: 0.225387, mean_eps: 0.000000
  2468/30000: episode: 44, duration: 0.698s, episode steps:  47, steps per second:  67, episode reward: 37.956, mean reward:  0.808 [-3.000, 34.310], mean action: 5.447 [0.000, 21.000],  loss: 0.009086, mae: 0.315753, mean_q: 0.241134, mean_eps: 0.000000
  2518/30000: episode: 45, duration: 0.502s, episode steps:  50, steps per second: 100, episode reward: 32.782, mean reward:  0.656 [-2.498, 32.092], mean action: 5.340 [1.000, 21.000],  loss: 0.010462, mae: 0.317004, mean_q: 0.254735, mean_eps: 0.000000
  2549/30000: episode: 46, duration: 0.486s, episode steps:  31, steps per second:  64, episode reward: -32.610, mean reward: -1.052 [-32.290,  3.000], mean action: 6.161 [1.000, 21.000],  loss: 0.009083, mae: 0.305127, mean_q: 0.278944, mean_eps: 0.000000
  2570/30000: episode: 47, duration: 0.291s, episode steps:  21, steps per second:  72, episode reward: 41.850, mean reward:  1.993 [-3.000, 32.290], mean action: 3.095 [1.000, 6.000],  loss: 0.010305, mae: 0.300262, mean_q: 0.275379, mean_eps: 0.000000
  2590/30000: episode: 48, duration: 0.245s, episode steps:  20, steps per second:  82, episode reward: 41.963, mean reward:  2.098 [-3.000, 32.180], mean action: 4.850 [3.000, 21.000],  loss: 0.011862, mae: 0.308612, mean_q: 0.282893, mean_eps: 0.000000
  2612/30000: episode: 49, duration: 0.475s, episode steps:  22, steps per second:  46, episode reward: 47.341, mean reward:  2.152 [-0.646, 31.665], mean action: 4.273 [3.000, 21.000],  loss: 0.008183, mae: 0.289817, mean_q: 0.262556, mean_eps: 0.000000
  2662/30000: episode: 50, duration: 0.504s, episode steps:  50, steps per second:  99, episode reward: 35.588, mean reward:  0.712 [-3.000, 31.882], mean action: 5.180 [1.000, 15.000],  loss: 0.009858, mae: 0.298780, mean_q: 0.288449, mean_eps: 0.000000
  2717/30000: episode: 51, duration: 0.569s, episode steps:  55, steps per second:  97, episode reward: 35.123, mean reward:  0.639 [-2.296, 32.458], mean action: 6.491 [1.000, 21.000],  loss: 0.009924, mae: 0.309654, mean_q: 0.261113, mean_eps: 0.000000
  2749/30000: episode: 52, duration: 0.338s, episode steps:  32, steps per second:  95, episode reward: 37.673, mean reward:  1.177 [-2.617, 32.110], mean action: 5.031 [1.000, 10.000],  loss: 0.010351, mae: 0.313415, mean_q: 0.246024, mean_eps: 0.000000
  2801/30000: episode: 53, duration: 0.521s, episode steps:  52, steps per second: 100, episode reward: -33.000, mean reward: -0.635 [-32.250,  2.941], mean action: 5.038 [1.000, 21.000],  loss: 0.008538, mae: 0.298223, mean_q: 0.265137, mean_eps: 0.000000
  2877/30000: episode: 54, duration: 0.756s, episode steps:  76, steps per second: 100, episode reward: 35.056, mean reward:  0.461 [-2.257, 32.280], mean action: 7.316 [0.000, 21.000],  loss: 0.010586, mae: 0.327042, mean_q: 0.241393, mean_eps: 0.000000
  2908/30000: episode: 55, duration: 0.343s, episode steps:  31, steps per second:  90, episode reward: 41.264, mean reward:  1.331 [-3.000, 36.200], mean action: 10.129 [1.000, 21.000],  loss: 0.010056, mae: 0.306705, mean_q: 0.256077, mean_eps: 0.000000
  2946/30000: episode: 56, duration: 0.395s, episode steps:  38, steps per second:  96, episode reward: 35.419, mean reward:  0.932 [-3.000, 32.770], mean action: 7.947 [1.000, 21.000],  loss: 0.010465, mae: 0.319654, mean_q: 0.206794, mean_eps: 0.000000
  2985/30000: episode: 57, duration: 0.400s, episode steps:  39, steps per second:  97, episode reward: 35.586, mean reward:  0.912 [-2.902, 32.195], mean action: 6.846 [1.000, 21.000],  loss: 0.010820, mae: 0.320511, mean_q: 0.257226, mean_eps: 0.000000
  3038/30000: episode: 58, duration: 0.552s, episode steps:  53, steps per second:  96, episode reward: 38.368, mean reward:  0.724 [-2.314, 32.510], mean action: 7.868 [1.000, 21.000],  loss: 0.009102, mae: 0.304600, mean_q: 0.261248, mean_eps: 0.000000
  3104/30000: episode: 59, duration: 0.736s, episode steps:  66, steps per second:  90, episode reward: -32.300, mean reward: -0.489 [-32.432,  2.483], mean action: 5.803 [1.000, 17.000],  loss: 0.011009, mae: 0.308041, mean_q: 0.261321, mean_eps: 0.000000
  3126/30000: episode: 60, duration: 0.257s, episode steps:  22, steps per second:  86, episode reward: 42.000, mean reward:  1.909 [-2.103, 33.000], mean action: 3.864 [1.000, 15.000],  loss: 0.011440, mae: 0.306657, mean_q: 0.259597, mean_eps: 0.000000
  3156/30000: episode: 61, duration: 0.498s, episode steps:  30, steps per second:  60, episode reward: 40.672, mean reward:  1.356 [-3.000, 31.659], mean action: 4.967 [3.000, 17.000],  loss: 0.011486, mae: 0.327744, mean_q: 0.219835, mean_eps: 0.000000
  3185/30000: episode: 62, duration: 0.339s, episode steps:  29, steps per second:  86, episode reward: 42.990, mean reward:  1.482 [-2.088, 31.734], mean action: 4.276 [1.000, 17.000],  loss: 0.007854, mae: 0.307329, mean_q: 0.224516, mean_eps: 0.000000
  3206/30000: episode: 63, duration: 0.221s, episode steps:  21, steps per second:  95, episode reward: 38.310, mean reward:  1.824 [-3.000, 31.955], mean action: 3.190 [1.000, 7.000],  loss: 0.009305, mae: 0.302774, mean_q: 0.259510, mean_eps: 0.000000
  3273/30000: episode: 64, duration: 0.803s, episode steps:  67, steps per second:  83, episode reward: 39.632, mean reward:  0.592 [-2.373, 32.080], mean action: 7.000 [1.000, 21.000],  loss: 0.012119, mae: 0.316762, mean_q: 0.246736, mean_eps: 0.000000
  3310/30000: episode: 65, duration: 0.410s, episode steps:  37, steps per second:  90, episode reward: -34.860, mean reward: -0.942 [-32.635,  2.260], mean action: 6.730 [1.000, 21.000],  loss: 0.011963, mae: 0.321527, mean_q: 0.259860, mean_eps: 0.000000
  3336/30000: episode: 66, duration: 0.299s, episode steps:  26, steps per second:  87, episode reward: 39.760, mean reward:  1.529 [-3.000, 32.110], mean action: 3.577 [1.000, 8.000],  loss: 0.011586, mae: 0.333059, mean_q: 0.248506, mean_eps: 0.000000
  3404/30000: episode: 67, duration: 0.796s, episode steps:  68, steps per second:  85, episode reward: -33.000, mean reward: -0.485 [-32.356,  3.000], mean action: 4.824 [0.000, 18.000],  loss: 0.012743, mae: 0.324459, mean_q: 0.275312, mean_eps: 0.000000
  3449/30000: episode: 68, duration: 0.497s, episode steps:  45, steps per second:  91, episode reward: 35.467, mean reward:  0.788 [-2.726, 32.183], mean action: 7.578 [1.000, 21.000],  loss: 0.011071, mae: 0.323923, mean_q: 0.230250, mean_eps: 0.000000
  3494/30000: episode: 69, duration: 0.495s, episode steps:  45, steps per second:  91, episode reward: 35.172, mean reward:  0.782 [-2.547, 32.260], mean action: 8.089 [1.000, 17.000],  loss: 0.012053, mae: 0.318169, mean_q: 0.267472, mean_eps: 0.000000
  3539/30000: episode: 70, duration: 0.469s, episode steps:  45, steps per second:  96, episode reward: -34.430, mean reward: -0.765 [-32.166,  2.900], mean action: 10.444 [1.000, 21.000],  loss: 0.010582, mae: 0.323749, mean_q: 0.239327, mean_eps: 0.000000
  3572/30000: episode: 71, duration: 0.418s, episode steps:  33, steps per second:  79, episode reward: 38.067, mean reward:  1.154 [-2.678, 32.300], mean action: 3.788 [1.000, 17.000],  loss: 0.010317, mae: 0.308097, mean_q: 0.244426, mean_eps: 0.000000
  3598/30000: episode: 72, duration: 0.269s, episode steps:  26, steps per second:  97, episode reward: 38.489, mean reward:  1.480 [-2.207, 31.983], mean action: 3.346 [1.000, 8.000],  loss: 0.008676, mae: 0.305480, mean_q: 0.236737, mean_eps: 0.000000
  3626/30000: episode: 73, duration: 0.302s, episode steps:  28, steps per second:  93, episode reward: 44.025, mean reward:  1.572 [-2.115, 32.010], mean action: 3.214 [1.000, 14.000],  loss: 0.009060, mae: 0.308102, mean_q: 0.266582, mean_eps: 0.000000
  3669/30000: episode: 74, duration: 0.438s, episode steps:  43, steps per second:  98, episode reward: 40.603, mean reward:  0.944 [-2.434, 31.965], mean action: 4.767 [1.000, 17.000],  loss: 0.013162, mae: 0.325450, mean_q: 0.270116, mean_eps: 0.000000
  3701/30000: episode: 75, duration: 0.325s, episode steps:  32, steps per second:  98, episode reward: 38.190, mean reward:  1.193 [-2.403, 32.570], mean action: 3.750 [1.000, 15.000],  loss: 0.010462, mae: 0.338875, mean_q: 0.226182, mean_eps: 0.000000
  3733/30000: episode: 76, duration: 0.328s, episode steps:  32, steps per second:  98, episode reward: 38.574, mean reward:  1.205 [-2.490, 32.410], mean action: 5.438 [1.000, 17.000],  loss: 0.010409, mae: 0.306716, mean_q: 0.268928, mean_eps: 0.000000
  3752/30000: episode: 77, duration: 0.202s, episode steps:  19, steps per second:  94, episode reward: 44.621, mean reward:  2.348 [-2.152, 32.520], mean action: 2.579 [1.000, 8.000],  loss: 0.015832, mae: 0.320066, mean_q: 0.312188, mean_eps: 0.000000
  3805/30000: episode: 78, duration: 0.526s, episode steps:  53, steps per second: 101, episode reward: 32.907, mean reward:  0.621 [-3.000, 32.207], mean action: 6.830 [1.000, 21.000],  loss: 0.011785, mae: 0.299599, mean_q: 0.288131, mean_eps: 0.000000
  3839/30000: episode: 79, duration: 0.359s, episode steps:  34, steps per second:  95, episode reward: 38.904, mean reward:  1.144 [-3.000, 32.034], mean action: 2.706 [1.000, 3.000],  loss: 0.011673, mae: 0.308598, mean_q: 0.263417, mean_eps: 0.000000
  3883/30000: episode: 80, duration: 0.453s, episode steps:  44, steps per second:  97, episode reward: 32.092, mean reward:  0.729 [-3.000, 32.080], mean action: 5.818 [1.000, 21.000],  loss: 0.011979, mae: 0.307787, mean_q: 0.285121, mean_eps: 0.000000
  3918/30000: episode: 81, duration: 0.365s, episode steps:  35, steps per second:  96, episode reward: 34.500, mean reward:  0.986 [-2.203, 32.021], mean action: 7.171 [1.000, 21.000],  loss: 0.010701, mae: 0.304955, mean_q: 0.288556, mean_eps: 0.000000
  3972/30000: episode: 82, duration: 0.535s, episode steps:  54, steps per second: 101, episode reward: 35.680, mean reward:  0.661 [-2.738, 31.998], mean action: 9.556 [1.000, 18.000],  loss: 0.010179, mae: 0.319786, mean_q: 0.254267, mean_eps: 0.000000
  4001/30000: episode: 83, duration: 0.305s, episode steps:  29, steps per second:  95, episode reward: 41.007, mean reward:  1.414 [-2.517, 32.070], mean action: 4.586 [1.000, 17.000],  loss: 0.010676, mae: 0.321636, mean_q: 0.206298, mean_eps: 0.000000
  4038/30000: episode: 84, duration: 0.387s, episode steps:  37, steps per second:  96, episode reward: 38.551, mean reward:  1.042 [-3.000, 32.270], mean action: 2.946 [0.000, 7.000],  loss: 0.014342, mae: 0.334060, mean_q: 0.248473, mean_eps: 0.000000
  4086/30000: episode: 85, duration: 0.493s, episode steps:  48, steps per second:  97, episode reward: 33.000, mean reward:  0.688 [-3.000, 32.500], mean action: 6.854 [1.000, 18.000],  loss: 0.007977, mae: 0.303488, mean_q: 0.228957, mean_eps: 0.000000
  4122/30000: episode: 86, duration: 0.368s, episode steps:  36, steps per second:  98, episode reward: 40.158, mean reward:  1.115 [-2.214, 31.811], mean action: 3.472 [1.000, 14.000],  loss: 0.011257, mae: 0.299516, mean_q: 0.268591, mean_eps: 0.000000
  4146/30000: episode: 87, duration: 0.255s, episode steps:  24, steps per second:  94, episode reward: 41.104, mean reward:  1.713 [-2.469, 32.515], mean action: 4.333 [1.000, 8.000],  loss: 0.012937, mae: 0.307192, mean_q: 0.291371, mean_eps: 0.000000
  4179/30000: episode: 88, duration: 0.337s, episode steps:  33, steps per second:  98, episode reward: 35.845, mean reward:  1.086 [-2.492, 32.330], mean action: 4.000 [1.000, 21.000],  loss: 0.012433, mae: 0.316414, mean_q: 0.251983, mean_eps: 0.000000
  4207/30000: episode: 89, duration: 0.292s, episode steps:  28, steps per second:  96, episode reward: 32.669, mean reward:  1.167 [-3.000, 32.050], mean action: 3.679 [0.000, 21.000],  loss: 0.014279, mae: 0.300713, mean_q: 0.296508, mean_eps: 0.000000
  4241/30000: episode: 90, duration: 0.357s, episode steps:  34, steps per second:  95, episode reward: 41.517, mean reward:  1.221 [-3.000, 32.270], mean action: 2.706 [1.000, 18.000],  loss: 0.009746, mae: 0.288897, mean_q: 0.259576, mean_eps: 0.000000
  4283/30000: episode: 91, duration: 0.508s, episode steps:  42, steps per second:  83, episode reward: 45.890, mean reward:  1.093 [-0.540, 32.140], mean action: 2.690 [1.000, 8.000],  loss: 0.012524, mae: 0.295363, mean_q: 0.280300, mean_eps: 0.000000
  4300/30000: episode: 92, duration: 0.182s, episode steps:  17, steps per second:  93, episode reward: 45.000, mean reward:  2.647 [ 0.000, 30.242], mean action: 4.118 [1.000, 10.000],  loss: 0.011804, mae: 0.318700, mean_q: 0.239100, mean_eps: 0.000000
  4327/30000: episode: 93, duration: 0.294s, episode steps:  27, steps per second:  92, episode reward: 35.855, mean reward:  1.328 [-2.480, 32.360], mean action: 2.556 [0.000, 10.000],  loss: 0.011286, mae: 0.295206, mean_q: 0.306607, mean_eps: 0.000000
  4382/30000: episode: 94, duration: 0.550s, episode steps:  55, steps per second: 100, episode reward: -32.470, mean reward: -0.590 [-32.538,  2.670], mean action: 6.236 [1.000, 21.000],  loss: 0.010122, mae: 0.304057, mean_q: 0.281060, mean_eps: 0.000000
  4413/30000: episode: 95, duration: 0.339s, episode steps:  31, steps per second:  92, episode reward: 41.027, mean reward:  1.323 [-2.345, 31.827], mean action: 4.065 [1.000, 21.000],  loss: 0.011666, mae: 0.314493, mean_q: 0.278347, mean_eps: 0.000000
  4446/30000: episode: 96, duration: 0.356s, episode steps:  33, steps per second:  93, episode reward: 43.684, mean reward:  1.324 [-2.078, 32.650], mean action: 4.000 [1.000, 15.000],  loss: 0.012938, mae: 0.319872, mean_q: 0.263066, mean_eps: 0.000000
  4496/30000: episode: 97, duration: 0.512s, episode steps:  50, steps per second:  98, episode reward: 37.539, mean reward:  0.751 [-2.431, 32.090], mean action: 7.320 [1.000, 21.000],  loss: 0.013477, mae: 0.319865, mean_q: 0.278938, mean_eps: 0.000000
  4526/30000: episode: 98, duration: 0.317s, episode steps:  30, steps per second:  95, episode reward: 38.852, mean reward:  1.295 [-2.579, 32.233], mean action: 2.400 [0.000, 15.000],  loss: 0.011826, mae: 0.312344, mean_q: 0.280035, mean_eps: 0.000000
  4559/30000: episode: 99, duration: 0.341s, episode steps:  33, steps per second:  97, episode reward: 39.876, mean reward:  1.208 [-2.875, 32.202], mean action: 4.697 [1.000, 17.000],  loss: 0.011521, mae: 0.305158, mean_q: 0.292332, mean_eps: 0.000000
  4587/30000: episode: 100, duration: 0.296s, episode steps:  28, steps per second:  95, episode reward: 42.558, mean reward:  1.520 [-2.081, 32.360], mean action: 7.321 [1.000, 21.000],  loss: 0.009969, mae: 0.298602, mean_q: 0.290441, mean_eps: 0.000000
  4646/30000: episode: 101, duration: 0.588s, episode steps:  59, steps per second: 100, episode reward: 32.108, mean reward:  0.544 [-2.789, 32.110], mean action: 8.458 [1.000, 21.000],  loss: 0.013635, mae: 0.305318, mean_q: 0.298879, mean_eps: 0.000000
  4681/30000: episode: 102, duration: 0.362s, episode steps:  35, steps per second:  97, episode reward: 40.982, mean reward:  1.171 [-2.516, 32.210], mean action: 3.629 [1.000, 16.000],  loss: 0.015051, mae: 0.323400, mean_q: 0.285390, mean_eps: 0.000000
  4743/30000: episode: 103, duration: 0.607s, episode steps:  62, steps per second: 102, episode reward: 35.163, mean reward:  0.567 [-2.931, 31.994], mean action: 5.452 [1.000, 21.000],  loss: 0.011513, mae: 0.307061, mean_q: 0.253426, mean_eps: 0.000000
  4772/30000: episode: 104, duration: 0.334s, episode steps:  29, steps per second:  87, episode reward: 43.472, mean reward:  1.499 [-2.055, 32.030], mean action: 3.172 [1.000, 17.000],  loss: 0.011090, mae: 0.302239, mean_q: 0.249694, mean_eps: 0.000000
  4793/30000: episode: 105, duration: 0.230s, episode steps:  21, steps per second:  91, episode reward: 38.780, mean reward:  1.847 [-2.902, 32.780], mean action: 3.048 [1.000, 10.000],  loss: 0.012459, mae: 0.299169, mean_q: 0.264074, mean_eps: 0.000000
  4833/30000: episode: 106, duration: 0.433s, episode steps:  40, steps per second:  92, episode reward: 32.940, mean reward:  0.823 [-3.000, 32.480], mean action: 10.075 [1.000, 21.000],  loss: 0.012124, mae: 0.300969, mean_q: 0.283253, mean_eps: 0.000000
  4865/30000: episode: 107, duration: 0.347s, episode steps:  32, steps per second:  92, episode reward: 35.267, mean reward:  1.102 [-2.731, 31.951], mean action: 3.625 [1.000, 15.000],  loss: 0.009959, mae: 0.294270, mean_q: 0.298188, mean_eps: 0.000000
  4895/30000: episode: 108, duration: 0.307s, episode steps:  30, steps per second:  98, episode reward: 43.388, mean reward:  1.446 [-2.281, 31.481], mean action: 4.667 [1.000, 10.000],  loss: 0.013246, mae: 0.308314, mean_q: 0.260922, mean_eps: 0.000000
  4934/30000: episode: 109, duration: 0.393s, episode steps:  39, steps per second:  99, episode reward: 32.125, mean reward:  0.824 [-2.865, 32.420], mean action: 5.462 [0.000, 17.000],  loss: 0.012822, mae: 0.294514, mean_q: 0.297673, mean_eps: 0.000000
  4977/30000: episode: 110, duration: 0.436s, episode steps:  43, steps per second:  99, episode reward: -30.000, mean reward: -0.698 [-30.950,  3.000], mean action: 3.674 [1.000, 21.000],  loss: 0.014850, mae: 0.315978, mean_q: 0.289990, mean_eps: 0.000000
  5002/30000: episode: 111, duration: 0.267s, episode steps:  25, steps per second:  93, episode reward: 46.756, mean reward:  1.870 [-0.403, 31.683], mean action: 4.720 [3.000, 8.000],  loss: 0.011181, mae: 0.295823, mean_q: 0.255403, mean_eps: 0.000000
  5044/30000: episode: 112, duration: 0.533s, episode steps:  42, steps per second:  79, episode reward: -32.030, mean reward: -0.763 [-32.025,  3.000], mean action: 6.929 [1.000, 21.000],  loss: 0.010889, mae: 0.284276, mean_q: 0.295689, mean_eps: 0.000000
  5080/30000: episode: 113, duration: 0.368s, episode steps:  36, steps per second:  98, episode reward: 32.639, mean reward:  0.907 [-2.696, 32.090], mean action: 5.722 [1.000, 21.000],  loss: 0.012223, mae: 0.292685, mean_q: 0.288604, mean_eps: 0.000000
  5100/30000: episode: 114, duration: 0.221s, episode steps:  20, steps per second:  90, episode reward: 44.833, mean reward:  2.242 [-2.101, 32.302], mean action: 3.050 [1.000, 6.000],  loss: 0.010433, mae: 0.291730, mean_q: 0.266954, mean_eps: 0.000000
  5159/30000: episode: 115, duration: 0.588s, episode steps:  59, steps per second: 100, episode reward: 40.883, mean reward:  0.693 [-3.000, 31.999], mean action: 3.814 [1.000, 14.000],  loss: 0.013247, mae: 0.300746, mean_q: 0.296113, mean_eps: 0.000000
  5204/30000: episode: 116, duration: 0.444s, episode steps:  45, steps per second: 101, episode reward: 38.481, mean reward:  0.855 [-2.373, 32.760], mean action: 7.667 [1.000, 21.000],  loss: 0.010228, mae: 0.296560, mean_q: 0.274030, mean_eps: 0.000000
  5223/30000: episode: 117, duration: 0.205s, episode steps:  19, steps per second:  93, episode reward: 44.326, mean reward:  2.333 [-2.787, 32.012], mean action: 4.947 [1.000, 14.000],  loss: 0.011905, mae: 0.302585, mean_q: 0.273322, mean_eps: 0.000000
  5261/30000: episode: 118, duration: 0.389s, episode steps:  38, steps per second:  98, episode reward: 38.559, mean reward:  1.015 [-2.296, 32.340], mean action: 5.605 [3.000, 17.000],  loss: 0.013275, mae: 0.313691, mean_q: 0.267711, mean_eps: 0.000000
  5311/30000: episode: 119, duration: 0.607s, episode steps:  50, steps per second:  82, episode reward: 44.123, mean reward:  0.882 [-2.462, 31.767], mean action: 2.580 [1.000, 17.000],  loss: 0.011969, mae: 0.291193, mean_q: 0.290865, mean_eps: 0.000000
  5382/30000: episode: 120, duration: 0.703s, episode steps:  71, steps per second: 101, episode reward: 32.608, mean reward:  0.459 [-3.000, 32.071], mean action: 5.268 [0.000, 17.000],  loss: 0.012369, mae: 0.288882, mean_q: 0.310312, mean_eps: 0.000000
  5419/30000: episode: 121, duration: 0.385s, episode steps:  37, steps per second:  96, episode reward: 36.941, mean reward:  0.998 [-2.710, 32.010], mean action: 9.378 [1.000, 16.000],  loss: 0.010466, mae: 0.286840, mean_q: 0.274746, mean_eps: 0.000000
  5516/30000: episode: 122, duration: 1.012s, episode steps:  97, steps per second:  96, episode reward: 34.692, mean reward:  0.358 [-2.756, 32.160], mean action: 7.299 [1.000, 21.000],  loss: 0.014190, mae: 0.302559, mean_q: 0.280276, mean_eps: 0.000000
  5543/30000: episode: 123, duration: 0.291s, episode steps:  27, steps per second:  93, episode reward: 41.783, mean reward:  1.548 [-2.898, 32.421], mean action: 3.407 [1.000, 15.000],  loss: 0.013602, mae: 0.300024, mean_q: 0.301056, mean_eps: 0.000000
  5594/30000: episode: 124, duration: 0.552s, episode steps:  51, steps per second:  92, episode reward: 36.000, mean reward:  0.706 [-3.000, 32.090], mean action: 2.118 [1.000, 10.000],  loss: 0.011916, mae: 0.286019, mean_q: 0.311579, mean_eps: 0.000000
  5630/30000: episode: 125, duration: 0.445s, episode steps:  36, steps per second:  81, episode reward: 35.628, mean reward:  0.990 [-2.218, 30.228], mean action: 4.694 [1.000, 21.000],  loss: 0.011240, mae: 0.292656, mean_q: 0.255101, mean_eps: 0.000000
  5653/30000: episode: 126, duration: 0.238s, episode steps:  23, steps per second:  97, episode reward: 44.753, mean reward:  1.946 [-2.220, 32.070], mean action: 3.217 [1.000, 13.000],  loss: 0.014525, mae: 0.290396, mean_q: 0.298885, mean_eps: 0.000000
  5707/30000: episode: 127, duration: 0.635s, episode steps:  54, steps per second:  85, episode reward: -35.560, mean reward: -0.659 [-32.130,  2.580], mean action: 5.556 [1.000, 17.000],  loss: 0.012213, mae: 0.285497, mean_q: 0.301285, mean_eps: 0.000000
  5743/30000: episode: 128, duration: 0.413s, episode steps:  36, steps per second:  87, episode reward: 43.943, mean reward:  1.221 [-2.364, 32.220], mean action: 2.528 [1.000, 7.000],  loss: 0.013129, mae: 0.285818, mean_q: 0.284131, mean_eps: 0.000000
  5786/30000: episode: 129, duration: 0.570s, episode steps:  43, steps per second:  75, episode reward: 42.444, mean reward:  0.987 [-2.163, 31.969], mean action: 6.907 [1.000, 21.000],  loss: 0.012533, mae: 0.294625, mean_q: 0.270602, mean_eps: 0.000000
  5824/30000: episode: 130, duration: 0.571s, episode steps:  38, steps per second:  67, episode reward: 44.619, mean reward:  1.174 [-2.203, 32.700], mean action: 2.500 [1.000, 8.000],  loss: 0.013533, mae: 0.292650, mean_q: 0.292048, mean_eps: 0.000000
  5877/30000: episode: 131, duration: 0.548s, episode steps:  53, steps per second:  97, episode reward: -32.270, mean reward: -0.609 [-31.951,  2.398], mean action: 6.302 [1.000, 17.000],  loss: 0.013127, mae: 0.289396, mean_q: 0.322942, mean_eps: 0.000000
  5910/30000: episode: 132, duration: 0.340s, episode steps:  33, steps per second:  97, episode reward: 40.888, mean reward:  1.239 [-2.616, 32.110], mean action: 4.212 [3.000, 21.000],  loss: 0.013402, mae: 0.280265, mean_q: 0.334910, mean_eps: 0.000000
  5940/30000: episode: 133, duration: 0.325s, episode steps:  30, steps per second:  92, episode reward: 44.399, mean reward:  1.480 [-3.000, 32.773], mean action: 2.167 [1.000, 9.000],  loss: 0.012058, mae: 0.281346, mean_q: 0.295001, mean_eps: 0.000000
  5981/30000: episode: 134, duration: 0.556s, episode steps:  41, steps per second:  74, episode reward: 40.450, mean reward:  0.987 [-2.137, 31.885], mean action: 5.829 [1.000, 17.000],  loss: 0.012136, mae: 0.281790, mean_q: 0.298180, mean_eps: 0.000000
  6027/30000: episode: 135, duration: 0.523s, episode steps:  46, steps per second:  88, episode reward: 41.190, mean reward:  0.895 [-2.085, 32.400], mean action: 3.130 [1.000, 17.000],  loss: 0.013885, mae: 0.296636, mean_q: 0.263540, mean_eps: 0.000000
  6076/30000: episode: 136, duration: 0.489s, episode steps:  49, steps per second: 100, episode reward: -33.000, mean reward: -0.673 [-33.000,  2.400], mean action: 4.653 [1.000, 14.000],  loss: 0.010267, mae: 0.278558, mean_q: 0.295678, mean_eps: 0.000000
  6115/30000: episode: 137, duration: 0.441s, episode steps:  39, steps per second:  88, episode reward: 44.824, mean reward:  1.149 [-2.016, 32.014], mean action: 2.744 [0.000, 14.000],  loss: 0.012570, mae: 0.286527, mean_q: 0.301728, mean_eps: 0.000000
  6135/30000: episode: 138, duration: 0.215s, episode steps:  20, steps per second:  93, episode reward: 44.136, mean reward:  2.207 [-2.226, 32.280], mean action: 2.700 [1.000, 15.000],  loss: 0.009730, mae: 0.273197, mean_q: 0.306742, mean_eps: 0.000000
  6162/30000: episode: 139, duration: 0.283s, episode steps:  27, steps per second:  95, episode reward: 35.496, mean reward:  1.315 [-2.948, 32.120], mean action: 3.185 [1.000, 15.000],  loss: 0.013270, mae: 0.278500, mean_q: 0.278237, mean_eps: 0.000000
  6209/30000: episode: 140, duration: 0.484s, episode steps:  47, steps per second:  97, episode reward: 37.796, mean reward:  0.804 [-2.722, 31.967], mean action: 5.404 [1.000, 16.000],  loss: 0.012097, mae: 0.272881, mean_q: 0.295237, mean_eps: 0.000000
  6247/30000: episode: 141, duration: 0.492s, episode steps:  38, steps per second:  77, episode reward: 35.059, mean reward:  0.923 [-3.000, 32.450], mean action: 12.105 [0.000, 18.000],  loss: 0.014417, mae: 0.296234, mean_q: 0.250436, mean_eps: 0.000000
  6286/30000: episode: 142, duration: 0.409s, episode steps:  39, steps per second:  95, episode reward: 38.251, mean reward:  0.981 [-2.392, 32.270], mean action: 5.282 [1.000, 15.000],  loss: 0.014845, mae: 0.292343, mean_q: 0.280823, mean_eps: 0.000000
  6304/30000: episode: 143, duration: 0.196s, episode steps:  18, steps per second:  92, episode reward: 41.876, mean reward:  2.326 [-2.068, 32.086], mean action: 4.556 [3.000, 15.000],  loss: 0.012746, mae: 0.278129, mean_q: 0.321907, mean_eps: 0.000000
  6350/30000: episode: 144, duration: 0.463s, episode steps:  46, steps per second:  99, episode reward: 38.550, mean reward:  0.838 [-2.940, 32.100], mean action: 2.804 [1.000, 15.000],  loss: 0.012606, mae: 0.296137, mean_q: 0.264745, mean_eps: 0.000000
  6379/30000: episode: 145, duration: 0.289s, episode steps:  29, steps per second: 100, episode reward: -45.310, mean reward: -1.562 [-32.900,  0.590], mean action: 11.103 [1.000, 21.000],  loss: 0.015175, mae: 0.290749, mean_q: 0.298821, mean_eps: 0.000000
  6408/30000: episode: 146, duration: 0.308s, episode steps:  29, steps per second:  94, episode reward: 38.409, mean reward:  1.324 [-2.307, 32.301], mean action: 4.586 [1.000, 15.000],  loss: 0.013407, mae: 0.286719, mean_q: 0.277191, mean_eps: 0.000000
  6430/30000: episode: 147, duration: 0.259s, episode steps:  22, steps per second:  85, episode reward: 44.819, mean reward:  2.037 [-2.273, 32.210], mean action: 3.636 [1.000, 10.000],  loss: 0.013311, mae: 0.285179, mean_q: 0.300251, mean_eps: 0.000000
  6481/30000: episode: 148, duration: 0.532s, episode steps:  51, steps per second:  96, episode reward: 40.032, mean reward:  0.785 [-2.664, 32.130], mean action: 5.098 [1.000, 15.000],  loss: 0.012107, mae: 0.276347, mean_q: 0.289054, mean_eps: 0.000000
  6504/30000: episode: 149, duration: 0.237s, episode steps:  23, steps per second:  97, episode reward: 38.936, mean reward:  1.693 [-2.900, 32.116], mean action: 2.348 [1.000, 4.000],  loss: 0.013565, mae: 0.274912, mean_q: 0.309660, mean_eps: 0.000000
  6558/30000: episode: 150, duration: 0.557s, episode steps:  54, steps per second:  97, episode reward: 32.231, mean reward:  0.597 [-2.386, 31.970], mean action: 5.907 [0.000, 21.000],  loss: 0.011967, mae: 0.281542, mean_q: 0.284228, mean_eps: 0.000000
  6584/30000: episode: 151, duration: 0.283s, episode steps:  26, steps per second:  92, episode reward: 40.594, mean reward:  1.561 [-2.317, 32.235], mean action: 6.423 [3.000, 21.000],  loss: 0.011630, mae: 0.277502, mean_q: 0.296800, mean_eps: 0.000000
  6611/30000: episode: 152, duration: 0.287s, episode steps:  27, steps per second:  94, episode reward: 41.303, mean reward:  1.530 [-2.247, 31.673], mean action: 3.037 [1.000, 16.000],  loss: 0.012466, mae: 0.291916, mean_q: 0.263979, mean_eps: 0.000000
  6644/30000: episode: 153, duration: 0.343s, episode steps:  33, steps per second:  96, episode reward: 35.194, mean reward:  1.066 [-2.700, 31.923], mean action: 2.788 [1.000, 16.000],  loss: 0.011678, mae: 0.281594, mean_q: 0.300003, mean_eps: 0.000000
  6671/30000: episode: 154, duration: 0.312s, episode steps:  27, steps per second:  86, episode reward: 36.000, mean reward:  1.333 [-3.000, 32.720], mean action: 4.222 [1.000, 16.000],  loss: 0.012879, mae: 0.285333, mean_q: 0.286230, mean_eps: 0.000000
  6710/30000: episode: 155, duration: 0.489s, episode steps:  39, steps per second:  80, episode reward: 34.178, mean reward:  0.876 [-3.000, 32.023], mean action: 4.667 [1.000, 16.000],  loss: 0.009807, mae: 0.268659, mean_q: 0.288259, mean_eps: 0.000000
  6747/30000: episode: 156, duration: 0.378s, episode steps:  37, steps per second:  98, episode reward: 34.907, mean reward:  0.943 [-3.000, 32.350], mean action: 4.541 [1.000, 16.000],  loss: 0.014127, mae: 0.294931, mean_q: 0.322802, mean_eps: 0.000000
  6824/30000: episode: 157, duration: 0.799s, episode steps:  77, steps per second:  96, episode reward: -34.910, mean reward: -0.453 [-31.822,  2.273], mean action: 2.922 [1.000, 16.000],  loss: 0.012636, mae: 0.280585, mean_q: 0.293271, mean_eps: 0.000000
  6860/30000: episode: 158, duration: 0.368s, episode steps:  36, steps per second:  98, episode reward: 38.243, mean reward:  1.062 [-3.000, 32.290], mean action: 6.444 [1.000, 21.000],  loss: 0.013712, mae: 0.284062, mean_q: 0.269316, mean_eps: 0.000000
  6921/30000: episode: 159, duration: 0.611s, episode steps:  61, steps per second: 100, episode reward: 43.181, mean reward:  0.708 [-2.883, 32.750], mean action: 5.656 [0.000, 21.000],  loss: 0.012266, mae: 0.284386, mean_q: 0.275243, mean_eps: 0.000000
  6958/30000: episode: 160, duration: 0.385s, episode steps:  37, steps per second:  96, episode reward: 38.746, mean reward:  1.047 [-2.618, 32.100], mean action: 4.135 [1.000, 16.000],  loss: 0.011571, mae: 0.274028, mean_q: 0.297608, mean_eps: 0.000000
  6977/30000: episode: 161, duration: 0.206s, episode steps:  19, steps per second:  92, episode reward: 41.324, mean reward:  2.175 [-2.424, 32.270], mean action: 5.526 [1.000, 16.000],  loss: 0.010697, mae: 0.272959, mean_q: 0.276852, mean_eps: 0.000000
  6995/30000: episode: 162, duration: 0.193s, episode steps:  18, steps per second:  93, episode reward: 41.939, mean reward:  2.330 [-3.000, 32.290], mean action: 4.056 [1.000, 16.000],  loss: 0.010669, mae: 0.271531, mean_q: 0.305374, mean_eps: 0.000000
  7021/30000: episode: 163, duration: 0.278s, episode steps:  26, steps per second:  93, episode reward: 35.801, mean reward:  1.377 [-2.695, 32.050], mean action: 4.154 [1.000, 16.000],  loss: 0.013204, mae: 0.294927, mean_q: 0.277244, mean_eps: 0.000000
  7052/30000: episode: 164, duration: 0.328s, episode steps:  31, steps per second:  95, episode reward: 35.573, mean reward:  1.148 [-2.642, 32.125], mean action: 3.484 [1.000, 17.000],  loss: 0.012754, mae: 0.289024, mean_q: 0.291657, mean_eps: 0.000000
  7108/30000: episode: 165, duration: 0.557s, episode steps:  56, steps per second: 101, episode reward: 40.257, mean reward:  0.719 [-2.204, 32.750], mean action: 5.232 [1.000, 21.000],  loss: 0.012478, mae: 0.271597, mean_q: 0.290316, mean_eps: 0.000000
  7155/30000: episode: 166, duration: 0.479s, episode steps:  47, steps per second:  98, episode reward: 39.647, mean reward:  0.844 [-2.213, 32.330], mean action: 4.213 [0.000, 16.000],  loss: 0.013241, mae: 0.276445, mean_q: 0.303703, mean_eps: 0.000000
  7176/30000: episode: 167, duration: 0.226s, episode steps:  21, steps per second:  93, episode reward: 43.987, mean reward:  2.095 [-2.188, 32.400], mean action: 3.429 [1.000, 7.000],  loss: 0.011050, mae: 0.282860, mean_q: 0.303971, mean_eps: 0.000000
  7204/30000: episode: 168, duration: 0.288s, episode steps:  28, steps per second:  97, episode reward: 35.781, mean reward:  1.278 [-2.807, 32.670], mean action: 6.607 [1.000, 16.000],  loss: 0.014086, mae: 0.312179, mean_q: 0.234973, mean_eps: 0.000000
  7232/30000: episode: 169, duration: 0.297s, episode steps:  28, steps per second:  94, episode reward: 35.207, mean reward:  1.257 [-2.362, 32.230], mean action: 3.429 [1.000, 15.000],  loss: 0.013297, mae: 0.284774, mean_q: 0.283126, mean_eps: 0.000000
  7261/30000: episode: 170, duration: 0.313s, episode steps:  29, steps per second:  93, episode reward: 43.809, mean reward:  1.511 [-2.307, 31.902], mean action: 3.828 [1.000, 17.000],  loss: 0.012850, mae: 0.291396, mean_q: 0.294725, mean_eps: 0.000000
  7289/30000: episode: 171, duration: 0.286s, episode steps:  28, steps per second:  98, episode reward: 38.426, mean reward:  1.372 [-3.000, 32.260], mean action: 3.321 [0.000, 15.000],  loss: 0.011917, mae: 0.282772, mean_q: 0.285592, mean_eps: 0.000000
  7327/30000: episode: 172, duration: 0.386s, episode steps:  38, steps per second:  98, episode reward: 38.590, mean reward:  1.016 [-2.371, 32.290], mean action: 4.000 [1.000, 16.000],  loss: 0.013142, mae: 0.289245, mean_q: 0.265007, mean_eps: 0.000000
  7348/30000: episode: 173, duration: 0.235s, episode steps:  21, steps per second:  90, episode reward: 38.855, mean reward:  1.850 [-3.000, 32.472], mean action: 4.190 [1.000, 16.000],  loss: 0.012902, mae: 0.287265, mean_q: 0.271300, mean_eps: 0.000000
  7375/30000: episode: 174, duration: 0.282s, episode steps:  27, steps per second:  96, episode reward: 37.727, mean reward:  1.397 [-2.547, 31.993], mean action: 4.000 [1.000, 16.000],  loss: 0.010980, mae: 0.267354, mean_q: 0.300097, mean_eps: 0.000000
  7415/30000: episode: 175, duration: 0.452s, episode steps:  40, steps per second:  89, episode reward: -35.230, mean reward: -0.881 [-32.990,  2.530], mean action: 7.600 [0.000, 16.000],  loss: 0.013749, mae: 0.287468, mean_q: 0.270777, mean_eps: 0.000000
  7450/30000: episode: 176, duration: 0.355s, episode steps:  35, steps per second:  99, episode reward: 40.588, mean reward:  1.160 [-2.281, 31.994], mean action: 7.543 [1.000, 21.000],  loss: 0.014710, mae: 0.282173, mean_q: 0.298172, mean_eps: 0.000000
  7478/30000: episode: 177, duration: 0.289s, episode steps:  28, steps per second:  97, episode reward: 41.241, mean reward:  1.473 [-3.000, 32.323], mean action: 5.036 [1.000, 16.000],  loss: 0.010646, mae: 0.271197, mean_q: 0.299322, mean_eps: 0.000000
  7507/30000: episode: 178, duration: 0.306s, episode steps:  29, steps per second:  95, episode reward: 44.588, mean reward:  1.538 [-2.017, 32.100], mean action: 3.621 [0.000, 16.000],  loss: 0.013602, mae: 0.287213, mean_q: 0.287931, mean_eps: 0.000000
  7535/30000: episode: 179, duration: 0.380s, episode steps:  28, steps per second:  74, episode reward: 43.003, mean reward:  1.536 [-2.324, 32.410], mean action: 4.429 [1.000, 16.000],  loss: 0.016583, mae: 0.292803, mean_q: 0.298719, mean_eps: 0.000000
  7556/30000: episode: 180, duration: 0.230s, episode steps:  21, steps per second:  91, episode reward: 44.368, mean reward:  2.113 [-0.178, 29.050], mean action: 3.095 [1.000, 7.000],  loss: 0.015712, mae: 0.300339, mean_q: 0.264262, mean_eps: 0.000000
  7581/30000: episode: 181, duration: 0.279s, episode steps:  25, steps per second:  90, episode reward: 41.717, mean reward:  1.669 [-2.206, 32.180], mean action: 4.000 [1.000, 16.000],  loss: 0.011841, mae: 0.267892, mean_q: 0.322467, mean_eps: 0.000000
  7619/30000: episode: 182, duration: 0.406s, episode steps:  38, steps per second:  93, episode reward: -32.700, mean reward: -0.861 [-32.316,  2.710], mean action: 5.842 [1.000, 16.000],  loss: 0.014999, mae: 0.283754, mean_q: 0.326372, mean_eps: 0.000000
  7642/30000: episode: 183, duration: 0.254s, episode steps:  23, steps per second:  91, episode reward: 41.611, mean reward:  1.809 [-2.070, 32.210], mean action: 4.348 [1.000, 15.000],  loss: 0.015061, mae: 0.276092, mean_q: 0.322189, mean_eps: 0.000000
  7681/30000: episode: 184, duration: 0.395s, episode steps:  39, steps per second:  99, episode reward: -34.970, mean reward: -0.897 [-32.383,  2.621], mean action: 5.231 [0.000, 18.000],  loss: 0.012699, mae: 0.265697, mean_q: 0.291457, mean_eps: 0.000000
  7717/30000: episode: 185, duration: 0.407s, episode steps:  36, steps per second:  88, episode reward: 32.818, mean reward:  0.912 [-2.801, 32.100], mean action: 5.722 [1.000, 20.000],  loss: 0.014299, mae: 0.264496, mean_q: 0.310592, mean_eps: 0.000000
  7741/30000: episode: 186, duration: 0.291s, episode steps:  24, steps per second:  83, episode reward: 43.533, mean reward:  1.814 [-2.529, 31.104], mean action: 6.792 [0.000, 18.000],  loss: 0.011727, mae: 0.256311, mean_q: 0.356056, mean_eps: 0.000000
  7791/30000: episode: 187, duration: 0.498s, episode steps:  50, steps per second: 100, episode reward: 32.009, mean reward:  0.640 [-3.000, 32.026], mean action: 4.580 [1.000, 16.000],  loss: 0.013960, mae: 0.267358, mean_q: 0.304951, mean_eps: 0.000000
  7823/30000: episode: 188, duration: 0.326s, episode steps:  32, steps per second:  98, episode reward: 36.000, mean reward:  1.125 [-3.000, 32.080], mean action: 3.156 [1.000, 14.000],  loss: 0.011345, mae: 0.252752, mean_q: 0.339907, mean_eps: 0.000000
  7849/30000: episode: 189, duration: 0.269s, episode steps:  26, steps per second:  97, episode reward: 35.938, mean reward:  1.382 [-2.660, 32.280], mean action: 2.538 [0.000, 4.000],  loss: 0.011239, mae: 0.255099, mean_q: 0.327670, mean_eps: 0.000000
  7872/30000: episode: 190, duration: 0.249s, episode steps:  23, steps per second:  92, episode reward: 41.515, mean reward:  1.805 [-2.536, 31.918], mean action: 2.609 [0.000, 16.000],  loss: 0.011428, mae: 0.253295, mean_q: 0.330330, mean_eps: 0.000000
  7907/30000: episode: 191, duration: 0.364s, episode steps:  35, steps per second:  96, episode reward: -37.810, mean reward: -1.080 [-32.225,  2.820], mean action: 8.086 [1.000, 21.000],  loss: 0.013013, mae: 0.258488, mean_q: 0.348165, mean_eps: 0.000000
  7935/30000: episode: 192, duration: 0.374s, episode steps:  28, steps per second:  75, episode reward: 38.855, mean reward:  1.388 [-2.559, 32.320], mean action: 3.000 [0.000, 20.000],  loss: 0.011985, mae: 0.264761, mean_q: 0.309478, mean_eps: 0.000000
  7960/30000: episode: 193, duration: 0.257s, episode steps:  25, steps per second:  97, episode reward: 38.682, mean reward:  1.547 [-3.000, 32.329], mean action: 4.800 [0.000, 15.000],  loss: 0.014045, mae: 0.280070, mean_q: 0.296320, mean_eps: 0.000000
  7987/30000: episode: 194, duration: 0.276s, episode steps:  27, steps per second:  98, episode reward: 33.000, mean reward:  1.222 [-2.802, 30.116], mean action: 2.407 [0.000, 15.000],  loss: 0.013096, mae: 0.265865, mean_q: 0.329178, mean_eps: 0.000000
  8002/30000: episode: 195, duration: 0.169s, episode steps:  15, steps per second:  89, episode reward: 47.012, mean reward:  3.134 [-0.120, 32.130], mean action: 3.533 [1.000, 6.000],  loss: 0.013148, mae: 0.268035, mean_q: 0.304684, mean_eps: 0.000000
  8036/30000: episode: 196, duration: 0.343s, episode steps:  34, steps per second:  99, episode reward: 38.115, mean reward:  1.121 [-2.532, 31.810], mean action: 2.735 [0.000, 15.000],  loss: 0.011676, mae: 0.262677, mean_q: 0.315343, mean_eps: 0.000000
  8091/30000: episode: 197, duration: 0.573s, episode steps:  55, steps per second:  96, episode reward: -32.330, mean reward: -0.588 [-32.022,  2.840], mean action: 3.327 [0.000, 16.000],  loss: 0.013428, mae: 0.257764, mean_q: 0.369186, mean_eps: 0.000000
  8126/30000: episode: 198, duration: 0.426s, episode steps:  35, steps per second:  82, episode reward: 40.310, mean reward:  1.152 [-2.578, 31.992], mean action: 3.114 [0.000, 21.000],  loss: 0.009542, mae: 0.235959, mean_q: 0.376139, mean_eps: 0.000000
  8158/30000: episode: 199, duration: 0.338s, episode steps:  32, steps per second:  95, episode reward: 38.497, mean reward:  1.203 [-2.500, 32.340], mean action: 4.844 [0.000, 15.000],  loss: 0.013062, mae: 0.266023, mean_q: 0.288476, mean_eps: 0.000000
  8179/30000: episode: 200, duration: 0.226s, episode steps:  21, steps per second:  93, episode reward: 47.156, mean reward:  2.246 [-0.185, 31.810], mean action: 4.000 [0.000, 8.000],  loss: 0.013777, mae: 0.279717, mean_q: 0.284609, mean_eps: 0.000000
  8221/30000: episode: 201, duration: 0.420s, episode steps:  42, steps per second: 100, episode reward: -32.410, mean reward: -0.772 [-32.430,  2.821], mean action: 4.976 [0.000, 16.000],  loss: 0.014536, mae: 0.280362, mean_q: 0.276284, mean_eps: 0.000000
  8235/30000: episode: 202, duration: 0.156s, episode steps:  14, steps per second:  90, episode reward: 44.786, mean reward:  3.199 [-2.134, 32.958], mean action: 2.429 [0.000, 4.000],  loss: 0.010183, mae: 0.255512, mean_q: 0.311077, mean_eps: 0.000000
  8267/30000: episode: 203, duration: 0.321s, episode steps:  32, steps per second: 100, episode reward: 40.588, mean reward:  1.268 [-2.469, 31.709], mean action: 3.344 [0.000, 21.000],  loss: 0.016306, mae: 0.288336, mean_q: 0.299747, mean_eps: 0.000000
  8301/30000: episode: 204, duration: 0.364s, episode steps:  34, steps per second:  93, episode reward: 35.900, mean reward:  1.056 [-3.000, 32.530], mean action: 4.088 [0.000, 17.000],  loss: 0.013972, mae: 0.266855, mean_q: 0.378440, mean_eps: 0.000000
  8330/30000: episode: 205, duration: 0.301s, episode steps:  29, steps per second:  96, episode reward: 41.131, mean reward:  1.418 [-2.751, 32.320], mean action: 4.241 [0.000, 16.000],  loss: 0.011965, mae: 0.265538, mean_q: 0.291988, mean_eps: 0.000000
  8384/30000: episode: 206, duration: 0.632s, episode steps:  54, steps per second:  86, episode reward: 38.092, mean reward:  0.705 [-2.333, 32.290], mean action: 2.148 [0.000, 15.000],  loss: 0.014134, mae: 0.263388, mean_q: 0.340909, mean_eps: 0.000000
  8439/30000: episode: 207, duration: 0.549s, episode steps:  55, steps per second: 100, episode reward: 35.445, mean reward:  0.644 [-2.876, 32.835], mean action: 4.273 [0.000, 15.000],  loss: 0.012018, mae: 0.258529, mean_q: 0.350739, mean_eps: 0.000000
  8492/30000: episode: 208, duration: 0.527s, episode steps:  53, steps per second: 101, episode reward: 41.709, mean reward:  0.787 [-3.000, 32.270], mean action: 2.774 [0.000, 14.000],  loss: 0.014811, mae: 0.270954, mean_q: 0.351966, mean_eps: 0.000000
  8524/30000: episode: 209, duration: 0.327s, episode steps:  32, steps per second:  98, episode reward: 33.000, mean reward:  1.031 [-3.000, 32.500], mean action: 5.844 [0.000, 16.000],  loss: 0.014539, mae: 0.275690, mean_q: 0.320088, mean_eps: 0.000000
  8544/30000: episode: 210, duration: 0.243s, episode steps:  20, steps per second:  82, episode reward: 44.114, mean reward:  2.206 [-2.445, 32.163], mean action: 2.300 [0.000, 6.000],  loss: 0.011516, mae: 0.264225, mean_q: 0.324792, mean_eps: 0.000000
  8584/30000: episode: 211, duration: 0.408s, episode steps:  40, steps per second:  98, episode reward: 39.718, mean reward:  0.993 [-2.725, 31.911], mean action: 3.600 [0.000, 10.000],  loss: 0.016026, mae: 0.290960, mean_q: 0.284219, mean_eps: 0.000000
  8603/30000: episode: 212, duration: 0.196s, episode steps:  19, steps per second:  97, episode reward: 43.807, mean reward:  2.306 [-2.207, 32.550], mean action: 7.316 [1.000, 21.000],  loss: 0.010886, mae: 0.245664, mean_q: 0.357825, mean_eps: 0.000000
  8651/30000: episode: 213, duration: 0.495s, episode steps:  48, steps per second:  97, episode reward: 38.751, mean reward:  0.807 [-2.585, 32.350], mean action: 3.271 [0.000, 16.000],  loss: 0.014909, mae: 0.278865, mean_q: 0.346530, mean_eps: 0.000000
  8709/30000: episode: 214, duration: 0.573s, episode steps:  58, steps per second: 101, episode reward: -35.780, mean reward: -0.617 [-32.234,  2.242], mean action: 4.500 [0.000, 16.000],  loss: 0.014433, mae: 0.273717, mean_q: 0.341066, mean_eps: 0.000000
  8747/30000: episode: 215, duration: 0.483s, episode steps:  38, steps per second:  79, episode reward: -32.900, mean reward: -0.866 [-32.191,  2.272], mean action: 4.474 [0.000, 15.000],  loss: 0.015631, mae: 0.276057, mean_q: 0.357191, mean_eps: 0.000000
  8770/30000: episode: 216, duration: 0.243s, episode steps:  23, steps per second:  95, episode reward: 42.000, mean reward:  1.826 [-2.378, 32.380], mean action: 3.348 [0.000, 16.000],  loss: 0.011156, mae: 0.262908, mean_q: 0.326004, mean_eps: 0.000000
  8784/30000: episode: 217, duration: 0.153s, episode steps:  14, steps per second:  91, episode reward: 44.030, mean reward:  3.145 [-2.355, 32.545], mean action: 3.286 [0.000, 16.000],  loss: 0.017612, mae: 0.286749, mean_q: 0.350953, mean_eps: 0.000000
  8809/30000: episode: 218, duration: 0.264s, episode steps:  25, steps per second:  95, episode reward: 35.704, mean reward:  1.428 [-2.421, 32.106], mean action: 4.480 [0.000, 17.000],  loss: 0.013319, mae: 0.257883, mean_q: 0.367956, mean_eps: 0.000000
  8830/30000: episode: 219, duration: 0.226s, episode steps:  21, steps per second:  93, episode reward: 38.803, mean reward:  1.848 [-3.000, 32.901], mean action: 3.190 [0.000, 17.000],  loss: 0.018140, mae: 0.286951, mean_q: 0.378651, mean_eps: 0.000000
  8862/30000: episode: 220, duration: 0.332s, episode steps:  32, steps per second:  96, episode reward: -35.590, mean reward: -1.112 [-32.227,  2.540], mean action: 9.094 [0.000, 18.000],  loss: 0.013474, mae: 0.257918, mean_q: 0.393411, mean_eps: 0.000000
  8883/30000: episode: 221, duration: 0.224s, episode steps:  21, steps per second:  94, episode reward: 43.345, mean reward:  2.064 [-2.094, 32.330], mean action: 1.714 [0.000, 16.000],  loss: 0.014909, mae: 0.273898, mean_q: 0.309557, mean_eps: 0.000000
  8910/30000: episode: 222, duration: 0.287s, episode steps:  27, steps per second:  94, episode reward: 42.000, mean reward:  1.556 [-2.103, 32.200], mean action: 2.741 [0.000, 15.000],  loss: 0.014276, mae: 0.268734, mean_q: 0.334117, mean_eps: 0.000000
  8939/30000: episode: 223, duration: 0.307s, episode steps:  29, steps per second:  95, episode reward: 36.000, mean reward:  1.241 [-2.421, 32.110], mean action: 1.241 [0.000, 4.000],  loss: 0.012334, mae: 0.261711, mean_q: 0.369816, mean_eps: 0.000000
  9014/30000: episode: 224, duration: 0.836s, episode steps:  75, steps per second:  90, episode reward: 38.422, mean reward:  0.512 [-2.257, 32.162], mean action: 2.827 [0.000, 16.000],  loss: 0.014047, mae: 0.275545, mean_q: 0.387982, mean_eps: 0.000000
  9035/30000: episode: 225, duration: 0.250s, episode steps:  21, steps per second:  84, episode reward: 42.000, mean reward:  2.000 [-2.954, 32.590], mean action: 3.286 [0.000, 16.000],  loss: 0.010460, mae: 0.269020, mean_q: 0.348888, mean_eps: 0.000000
  9052/30000: episode: 226, duration: 0.207s, episode steps:  17, steps per second:  82, episode reward: 47.248, mean reward:  2.779 [-0.030, 32.210], mean action: 2.000 [0.000, 3.000],  loss: 0.016455, mae: 0.299335, mean_q: 0.314689, mean_eps: 0.000000
  9125/30000: episode: 227, duration: 0.757s, episode steps:  73, steps per second:  96, episode reward: -33.000, mean reward: -0.452 [-32.350,  2.560], mean action: 4.740 [0.000, 16.000],  loss: 0.013436, mae: 0.265432, mean_q: 0.372617, mean_eps: 0.000000
  9142/30000: episode: 228, duration: 0.202s, episode steps:  17, steps per second:  84, episode reward: 41.195, mean reward:  2.423 [-2.600, 31.945], mean action: 2.824 [0.000, 16.000],  loss: 0.016089, mae: 0.280961, mean_q: 0.427858, mean_eps: 0.000000
  9166/30000: episode: 229, duration: 0.252s, episode steps:  24, steps per second:  95, episode reward: 39.000, mean reward:  1.625 [-2.519, 32.800], mean action: 2.333 [1.000, 6.000],  loss: 0.012490, mae: 0.261452, mean_q: 0.400056, mean_eps: 0.000000
  9194/30000: episode: 230, duration: 0.293s, episode steps:  28, steps per second:  96, episode reward: 42.000, mean reward:  1.500 [-2.600, 32.210], mean action: 2.821 [0.000, 16.000],  loss: 0.015025, mae: 0.268869, mean_q: 0.422832, mean_eps: 0.000000
  9217/30000: episode: 231, duration: 0.241s, episode steps:  23, steps per second:  96, episode reward: 38.385, mean reward:  1.669 [-2.489, 31.695], mean action: 3.652 [0.000, 16.000],  loss: 0.015742, mae: 0.273934, mean_q: 0.373123, mean_eps: 0.000000
  9237/30000: episode: 232, duration: 0.305s, episode steps:  20, steps per second:  66, episode reward: 44.442, mean reward:  2.222 [-2.062, 31.866], mean action: 1.350 [0.000, 16.000],  loss: 0.017152, mae: 0.276418, mean_q: 0.400968, mean_eps: 0.000000
  9276/30000: episode: 233, duration: 0.416s, episode steps:  39, steps per second:  94, episode reward: 39.000, mean reward:  1.000 [-2.594, 32.430], mean action: 1.436 [0.000, 14.000],  loss: 0.013282, mae: 0.269780, mean_q: 0.335541, mean_eps: 0.000000
  9304/30000: episode: 234, duration: 0.293s, episode steps:  28, steps per second:  95, episode reward: 39.000, mean reward:  1.393 [-2.662, 32.240], mean action: 3.071 [0.000, 16.000],  loss: 0.011862, mae: 0.247286, mean_q: 0.400213, mean_eps: 0.000000
  9335/30000: episode: 235, duration: 0.321s, episode steps:  31, steps per second:  97, episode reward: 35.846, mean reward:  1.156 [-2.439, 32.440], mean action: 1.226 [0.000, 12.000],  loss: 0.016478, mae: 0.266616, mean_q: 0.399053, mean_eps: 0.000000
  9363/30000: episode: 236, duration: 0.297s, episode steps:  28, steps per second:  94, episode reward: 38.711, mean reward:  1.383 [-3.000, 32.450], mean action: 1.000 [0.000, 6.000],  loss: 0.014435, mae: 0.262462, mean_q: 0.415892, mean_eps: 0.000000
  9395/30000: episode: 237, duration: 0.351s, episode steps:  32, steps per second:  91, episode reward: 32.900, mean reward:  1.028 [-3.000, 32.520], mean action: 5.000 [0.000, 21.000],  loss: 0.012667, mae: 0.257651, mean_q: 0.405285, mean_eps: 0.000000
  9414/30000: episode: 238, duration: 0.216s, episode steps:  19, steps per second:  88, episode reward: 41.778, mean reward:  2.199 [-2.173, 32.051], mean action: 1.789 [0.000, 16.000],  loss: 0.015617, mae: 0.275535, mean_q: 0.360647, mean_eps: 0.000000
  9455/30000: episode: 239, duration: 0.430s, episode steps:  41, steps per second:  95, episode reward: 35.262, mean reward:  0.860 [-3.000, 32.260], mean action: 3.024 [0.000, 18.000],  loss: 0.014788, mae: 0.265578, mean_q: 0.440528, mean_eps: 0.000000
  9491/30000: episode: 240, duration: 0.385s, episode steps:  36, steps per second:  94, episode reward: 43.384, mean reward:  1.205 [-3.000, 31.959], mean action: 3.167 [0.000, 17.000],  loss: 0.014015, mae: 0.268164, mean_q: 0.415224, mean_eps: 0.000000
  9519/30000: episode: 241, duration: 0.308s, episode steps:  28, steps per second:  91, episode reward: 41.903, mean reward:  1.497 [-2.416, 32.243], mean action: 2.071 [1.000, 16.000],  loss: 0.016859, mae: 0.283024, mean_q: 0.370998, mean_eps: 0.000000
  9550/30000: episode: 242, duration: 0.330s, episode steps:  31, steps per second:  94, episode reward: 41.167, mean reward:  1.328 [-3.000, 32.240], mean action: 2.065 [0.000, 6.000],  loss: 0.011229, mae: 0.253587, mean_q: 0.416275, mean_eps: 0.000000
  9604/30000: episode: 243, duration: 0.549s, episode steps:  54, steps per second:  98, episode reward: -32.230, mean reward: -0.597 [-31.963,  2.460], mean action: 3.278 [0.000, 18.000],  loss: 0.013052, mae: 0.260295, mean_q: 0.382130, mean_eps: 0.000000
  9631/30000: episode: 244, duration: 0.284s, episode steps:  27, steps per second:  95, episode reward: 41.873, mean reward:  1.551 [-2.197, 32.223], mean action: 2.630 [0.000, 16.000],  loss: 0.016547, mae: 0.285601, mean_q: 0.336426, mean_eps: 0.000000
  9661/30000: episode: 245, duration: 0.320s, episode steps:  30, steps per second:  94, episode reward: 36.000, mean reward:  1.200 [-2.336, 32.410], mean action: 3.267 [1.000, 15.000],  loss: 0.013194, mae: 0.263298, mean_q: 0.349898, mean_eps: 0.000000
  9698/30000: episode: 246, duration: 0.382s, episode steps:  37, steps per second:  97, episode reward: 35.442, mean reward:  0.958 [-2.608, 31.492], mean action: 3.459 [0.000, 17.000],  loss: 0.012799, mae: 0.265344, mean_q: 0.357665, mean_eps: 0.000000
  9722/30000: episode: 247, duration: 0.251s, episode steps:  24, steps per second:  96, episode reward: 38.614, mean reward:  1.609 [-2.487, 32.193], mean action: 3.125 [0.000, 17.000],  loss: 0.013080, mae: 0.263621, mean_q: 0.371616, mean_eps: 0.000000
  9760/30000: episode: 248, duration: 0.408s, episode steps:  38, steps per second:  93, episode reward: 35.180, mean reward:  0.926 [-3.000, 32.084], mean action: 5.789 [0.000, 18.000],  loss: 0.012481, mae: 0.261353, mean_q: 0.365341, mean_eps: 0.000000
  9775/30000: episode: 249, duration: 0.183s, episode steps:  15, steps per second:  82, episode reward: 41.668, mean reward:  2.778 [-3.000, 32.170], mean action: 2.667 [0.000, 6.000],  loss: 0.012941, mae: 0.261654, mean_q: 0.393467, mean_eps: 0.000000
  9809/30000: episode: 250, duration: 0.356s, episode steps:  34, steps per second:  95, episode reward: 44.047, mean reward:  1.296 [-2.150, 32.050], mean action: 4.882 [1.000, 18.000],  loss: 0.012884, mae: 0.268091, mean_q: 0.351510, mean_eps: 0.000000
  9839/30000: episode: 251, duration: 0.318s, episode steps:  30, steps per second:  94, episode reward: 38.792, mean reward:  1.293 [-3.000, 32.420], mean action: 4.900 [0.000, 21.000],  loss: 0.014069, mae: 0.267161, mean_q: 0.384520, mean_eps: 0.000000
  9874/30000: episode: 252, duration: 0.367s, episode steps:  35, steps per second:  95, episode reward: 38.048, mean reward:  1.087 [-2.572, 32.120], mean action: 3.514 [0.000, 18.000],  loss: 0.017273, mae: 0.285340, mean_q: 0.351622, mean_eps: 0.000000
  9904/30000: episode: 253, duration: 0.313s, episode steps:  30, steps per second:  96, episode reward: 41.345, mean reward:  1.378 [-2.800, 32.135], mean action: 3.633 [0.000, 12.000],  loss: 0.014680, mae: 0.272523, mean_q: 0.361580, mean_eps: 0.000000
  9930/30000: episode: 254, duration: 0.382s, episode steps:  26, steps per second:  68, episode reward: 39.000, mean reward:  1.500 [-3.000, 32.340], mean action: 3.231 [0.000, 12.000],  loss: 0.014867, mae: 0.265677, mean_q: 0.354484, mean_eps: 0.000000
  9952/30000: episode: 255, duration: 0.235s, episode steps:  22, steps per second:  94, episode reward: 41.136, mean reward:  1.870 [-2.390, 31.602], mean action: 1.364 [0.000, 12.000],  loss: 0.016179, mae: 0.270652, mean_q: 0.416446, mean_eps: 0.000000
  9975/30000: episode: 256, duration: 0.242s, episode steps:  23, steps per second:  95, episode reward: 47.378, mean reward:  2.060 [-0.124, 32.080], mean action: 0.870 [0.000, 13.000],  loss: 0.014306, mae: 0.265290, mean_q: 0.422370, mean_eps: 0.000000
 10003/30000: episode: 257, duration: 0.300s, episode steps:  28, steps per second:  93, episode reward: -32.200, mean reward: -1.150 [-33.000,  2.419], mean action: 6.893 [0.000, 18.000],  loss: 0.015677, mae: 0.275430, mean_q: 0.354130, mean_eps: 0.000000
 10029/30000: episode: 258, duration: 0.269s, episode steps:  26, steps per second:  97, episode reward: 39.000, mean reward:  1.500 [-2.356, 32.200], mean action: 3.038 [0.000, 17.000],  loss: 0.014305, mae: 0.264313, mean_q: 0.358987, mean_eps: 0.000000
 10054/30000: episode: 259, duration: 0.262s, episode steps:  25, steps per second:  95, episode reward: 35.970, mean reward:  1.439 [-3.000, 32.150], mean action: 4.800 [0.000, 16.000],  loss: 0.018644, mae: 0.284880, mean_q: 0.374146, mean_eps: 0.000000
 10083/30000: episode: 260, duration: 0.303s, episode steps:  29, steps per second:  96, episode reward: 42.000, mean reward:  1.448 [-2.614, 32.240], mean action: 2.172 [0.000, 12.000],  loss: 0.010939, mae: 0.256424, mean_q: 0.335499, mean_eps: 0.000000
 10099/30000: episode: 261, duration: 0.180s, episode steps:  16, steps per second:  89, episode reward: 47.277, mean reward:  2.955 [-0.168, 31.906], mean action: 1.000 [1.000, 1.000],  loss: 0.014947, mae: 0.266796, mean_q: 0.414578, mean_eps: 0.000000
 10133/30000: episode: 262, duration: 0.362s, episode steps:  34, steps per second:  94, episode reward: 39.000, mean reward:  1.147 [-2.340, 32.594], mean action: 3.382 [0.000, 13.000],  loss: 0.016259, mae: 0.266868, mean_q: 0.389705, mean_eps: 0.000000
 10157/30000: episode: 263, duration: 0.257s, episode steps:  24, steps per second:  93, episode reward: 41.498, mean reward:  1.729 [-2.337, 31.823], mean action: 3.708 [0.000, 12.000],  loss: 0.015356, mae: 0.272262, mean_q: 0.317522, mean_eps: 0.000000
 10193/30000: episode: 264, duration: 0.363s, episode steps:  36, steps per second:  99, episode reward: 32.191, mean reward:  0.894 [-3.000, 32.046], mean action: 7.306 [0.000, 18.000],  loss: 0.014707, mae: 0.263847, mean_q: 0.340979, mean_eps: 0.000000
 10229/30000: episode: 265, duration: 0.369s, episode steps:  36, steps per second:  98, episode reward: 41.095, mean reward:  1.142 [-2.142, 32.600], mean action: 3.500 [0.000, 15.000],  loss: 0.013705, mae: 0.267137, mean_q: 0.345356, mean_eps: 0.000000
 10253/30000: episode: 266, duration: 0.258s, episode steps:  24, steps per second:  93, episode reward: 37.574, mean reward:  1.566 [-3.000, 31.860], mean action: 4.958 [0.000, 17.000],  loss: 0.013231, mae: 0.257640, mean_q: 0.352148, mean_eps: 0.000000
 10282/30000: episode: 267, duration: 0.327s, episode steps:  29, steps per second:  89, episode reward: 41.268, mean reward:  1.423 [-2.429, 31.528], mean action: 2.483 [0.000, 16.000],  loss: 0.014098, mae: 0.267830, mean_q: 0.331352, mean_eps: 0.000000
 10305/30000: episode: 268, duration: 0.257s, episode steps:  23, steps per second:  90, episode reward: 43.618, mean reward:  1.896 [-2.157, 31.879], mean action: 3.478 [0.000, 15.000],  loss: 0.013240, mae: 0.254983, mean_q: 0.324399, mean_eps: 0.000000
 10338/30000: episode: 269, duration: 0.477s, episode steps:  33, steps per second:  69, episode reward: 44.245, mean reward:  1.341 [-2.064, 32.033], mean action: 2.424 [0.000, 12.000],  loss: 0.015110, mae: 0.268910, mean_q: 0.356343, mean_eps: 0.000000
 10369/30000: episode: 270, duration: 0.800s, episode steps:  31, steps per second:  39, episode reward: 44.940, mean reward:  1.450 [-2.058, 32.530], mean action: 2.000 [1.000, 12.000],  loss: 0.015719, mae: 0.266095, mean_q: 0.386978, mean_eps: 0.000000
 10389/30000: episode: 271, duration: 0.264s, episode steps:  20, steps per second:  76, episode reward: 38.773, mean reward:  1.939 [-2.847, 32.200], mean action: 3.300 [0.000, 13.000],  loss: 0.018094, mae: 0.288784, mean_q: 0.351216, mean_eps: 0.000000
 10415/30000: episode: 272, duration: 0.310s, episode steps:  26, steps per second:  84, episode reward: 35.521, mean reward:  1.366 [-2.876, 32.220], mean action: 3.423 [0.000, 12.000],  loss: 0.014819, mae: 0.263703, mean_q: 0.358773, mean_eps: 0.000000
 10435/30000: episode: 273, duration: 0.244s, episode steps:  20, steps per second:  82, episode reward: 38.307, mean reward:  1.915 [-3.000, 32.130], mean action: 4.500 [0.000, 21.000],  loss: 0.014671, mae: 0.264329, mean_q: 0.333784, mean_eps: 0.000000
 10449/30000: episode: 274, duration: 0.176s, episode steps:  14, steps per second:  80, episode reward: 44.152, mean reward:  3.154 [-2.601, 33.000], mean action: 3.929 [0.000, 15.000],  loss: 0.010651, mae: 0.252105, mean_q: 0.359508, mean_eps: 0.000000
 10504/30000: episode: 275, duration: 0.600s, episode steps:  55, steps per second:  92, episode reward: -32.450, mean reward: -0.590 [-32.456,  2.480], mean action: 1.618 [0.000, 12.000],  loss: 0.014337, mae: 0.262336, mean_q: 0.406311, mean_eps: 0.000000
 10528/30000: episode: 276, duration: 0.280s, episode steps:  24, steps per second:  86, episode reward: 41.233, mean reward:  1.718 [-2.194, 32.000], mean action: 2.542 [0.000, 12.000],  loss: 0.017753, mae: 0.270397, mean_q: 0.369865, mean_eps: 0.000000
 10551/30000: episode: 277, duration: 0.268s, episode steps:  23, steps per second:  86, episode reward: 38.940, mean reward:  1.693 [-3.000, 32.320], mean action: 3.957 [0.000, 15.000],  loss: 0.018306, mae: 0.280879, mean_q: 0.340173, mean_eps: 0.000000
 10588/30000: episode: 278, duration: 0.378s, episode steps:  37, steps per second:  98, episode reward: 38.583, mean reward:  1.043 [-2.757, 32.120], mean action: 4.784 [0.000, 16.000],  loss: 0.015431, mae: 0.264792, mean_q: 0.365682, mean_eps: 0.000000
 10624/30000: episode: 279, duration: 0.380s, episode steps:  36, steps per second:  95, episode reward: 36.000, mean reward:  1.000 [-3.000, 32.540], mean action: 4.444 [1.000, 12.000],  loss: 0.014707, mae: 0.263513, mean_q: 0.373862, mean_eps: 0.000000
 10646/30000: episode: 280, duration: 0.234s, episode steps:  22, steps per second:  94, episode reward: 41.357, mean reward:  1.880 [-2.668, 31.970], mean action: 3.182 [1.000, 12.000],  loss: 0.015341, mae: 0.272126, mean_q: 0.317960, mean_eps: 0.000000
 10679/30000: episode: 281, duration: 0.469s, episode steps:  33, steps per second:  70, episode reward: 41.752, mean reward:  1.265 [-2.056, 32.270], mean action: 2.939 [1.000, 16.000],  loss: 0.015065, mae: 0.264336, mean_q: 0.383178, mean_eps: 0.000000
 10730/30000: episode: 282, duration: 0.519s, episode steps:  51, steps per second:  98, episode reward: 40.215, mean reward:  0.789 [-2.545, 32.353], mean action: 4.039 [0.000, 19.000],  loss: 0.013335, mae: 0.256210, mean_q: 0.338946, mean_eps: 0.000000
 10765/30000: episode: 283, duration: 0.357s, episode steps:  35, steps per second:  98, episode reward: 35.721, mean reward:  1.021 [-3.000, 32.320], mean action: 4.057 [0.000, 15.000],  loss: 0.016676, mae: 0.262989, mean_q: 0.383346, mean_eps: 0.000000
 10793/30000: episode: 284, duration: 0.289s, episode steps:  28, steps per second:  97, episode reward: 32.322, mean reward:  1.154 [-2.938, 32.180], mean action: 5.214 [0.000, 21.000],  loss: 0.015745, mae: 0.261797, mean_q: 0.375630, mean_eps: 0.000000
 10814/30000: episode: 285, duration: 0.230s, episode steps:  21, steps per second:  91, episode reward: 44.676, mean reward:  2.127 [-2.288, 32.080], mean action: 1.857 [0.000, 10.000],  loss: 0.015515, mae: 0.256808, mean_q: 0.373874, mean_eps: 0.000000
 10852/30000: episode: 286, duration: 0.389s, episode steps:  38, steps per second:  98, episode reward: 35.668, mean reward:  0.939 [-2.564, 32.100], mean action: 5.711 [0.000, 17.000],  loss: 0.017034, mae: 0.267600, mean_q: 0.340845, mean_eps: 0.000000
 10873/30000: episode: 287, duration: 0.223s, episode steps:  21, steps per second:  94, episode reward: 44.634, mean reward:  2.125 [-2.224, 32.069], mean action: 1.238 [0.000, 3.000],  loss: 0.014363, mae: 0.249185, mean_q: 0.397562, mean_eps: 0.000000
 10911/30000: episode: 288, duration: 0.388s, episode steps:  38, steps per second:  98, episode reward: 35.705, mean reward:  0.940 [-2.844, 31.845], mean action: 4.368 [0.000, 15.000],  loss: 0.014844, mae: 0.255219, mean_q: 0.386232, mean_eps: 0.000000
 10930/30000: episode: 289, duration: 0.202s, episode steps:  19, steps per second:  94, episode reward: 38.561, mean reward:  2.030 [-2.844, 31.681], mean action: 2.211 [0.000, 12.000],  loss: 0.015564, mae: 0.259927, mean_q: 0.394866, mean_eps: 0.000000
 10951/30000: episode: 290, duration: 0.225s, episode steps:  21, steps per second:  93, episode reward: 47.094, mean reward:  2.243 [-0.061, 32.180], mean action: 2.571 [1.000, 12.000],  loss: 0.015650, mae: 0.263526, mean_q: 0.390980, mean_eps: 0.000000
 10967/30000: episode: 291, duration: 0.174s, episode steps:  16, steps per second:  92, episode reward: 41.374, mean reward:  2.586 [-2.637, 32.210], mean action: 3.125 [1.000, 16.000],  loss: 0.015549, mae: 0.268578, mean_q: 0.358965, mean_eps: 0.000000
 11000/30000: episode: 292, duration: 0.341s, episode steps:  33, steps per second:  97, episode reward: 32.243, mean reward:  0.977 [-3.000, 32.061], mean action: 3.667 [0.000, 15.000],  loss: 0.013803, mae: 0.257871, mean_q: 0.320370, mean_eps: 0.000000
 11043/30000: episode: 293, duration: 0.445s, episode steps:  43, steps per second:  97, episode reward: 38.122, mean reward:  0.887 [-2.900, 32.060], mean action: 3.465 [0.000, 15.000],  loss: 0.015774, mae: 0.265213, mean_q: 0.402585, mean_eps: 0.000000
 11091/30000: episode: 294, duration: 0.627s, episode steps:  48, steps per second:  77, episode reward: 40.795, mean reward:  0.850 [-3.000, 32.610], mean action: 2.958 [0.000, 12.000],  loss: 0.015085, mae: 0.273004, mean_q: 0.376888, mean_eps: 0.000000
 11119/30000: episode: 295, duration: 0.921s, episode steps:  28, steps per second:  30, episode reward: 41.041, mean reward:  1.466 [-2.381, 32.210], mean action: 5.250 [0.000, 21.000],  loss: 0.014296, mae: 0.264715, mean_q: 0.414523, mean_eps: 0.000000
 11174/30000: episode: 296, duration: 1.049s, episode steps:  55, steps per second:  52, episode reward: -32.690, mean reward: -0.594 [-32.296,  2.400], mean action: 3.000 [0.000, 15.000],  loss: 0.016119, mae: 0.278699, mean_q: 0.394143, mean_eps: 0.000000
 11208/30000: episode: 297, duration: 0.591s, episode steps:  34, steps per second:  58, episode reward: 35.832, mean reward:  1.054 [-3.000, 32.072], mean action: 2.500 [0.000, 12.000],  loss: 0.013907, mae: 0.263538, mean_q: 0.390925, mean_eps: 0.000000
 11228/30000: episode: 298, duration: 0.278s, episode steps:  20, steps per second:  72, episode reward: 38.651, mean reward:  1.933 [-2.803, 32.284], mean action: 2.350 [0.000, 12.000],  loss: 0.016170, mae: 0.279925, mean_q: 0.355117, mean_eps: 0.000000
 11258/30000: episode: 299, duration: 0.381s, episode steps:  30, steps per second:  79, episode reward: 41.184, mean reward:  1.373 [-2.261, 32.270], mean action: 2.533 [1.000, 12.000],  loss: 0.016849, mae: 0.274884, mean_q: 0.375965, mean_eps: 0.000000
 11290/30000: episode: 300, duration: 0.431s, episode steps:  32, steps per second:  74, episode reward: 40.960, mean reward:  1.280 [-2.123, 32.250], mean action: 4.062 [0.000, 15.000],  loss: 0.013364, mae: 0.268111, mean_q: 0.311048, mean_eps: 0.000000
 11339/30000: episode: 301, duration: 0.615s, episode steps:  49, steps per second:  80, episode reward: -32.030, mean reward: -0.654 [-32.089,  2.381], mean action: 10.633 [0.000, 18.000],  loss: 0.014643, mae: 0.271492, mean_q: 0.392475, mean_eps: 0.000000
 11371/30000: episode: 302, duration: 0.403s, episode steps:  32, steps per second:  79, episode reward: 38.225, mean reward:  1.195 [-2.879, 32.150], mean action: 2.688 [0.000, 12.000],  loss: 0.014230, mae: 0.266457, mean_q: 0.380472, mean_eps: 0.000000
 11402/30000: episode: 303, duration: 0.350s, episode steps:  31, steps per second:  88, episode reward: 38.900, mean reward:  1.255 [-3.000, 32.230], mean action: 3.355 [0.000, 12.000],  loss: 0.014056, mae: 0.257989, mean_q: 0.382883, mean_eps: 0.000000
 11434/30000: episode: 304, duration: 0.426s, episode steps:  32, steps per second:  75, episode reward: 38.880, mean reward:  1.215 [-2.403, 32.100], mean action: 2.656 [0.000, 15.000],  loss: 0.015359, mae: 0.257659, mean_q: 0.387245, mean_eps: 0.000000
 11496/30000: episode: 305, duration: 1.021s, episode steps:  62, steps per second:  61, episode reward: -35.550, mean reward: -0.573 [-32.082,  2.210], mean action: 2.839 [0.000, 15.000],  loss: 0.017121, mae: 0.268284, mean_q: 0.423524, mean_eps: 0.000000
 11518/30000: episode: 306, duration: 0.272s, episode steps:  22, steps per second:  81, episode reward: 38.751, mean reward:  1.761 [-2.535, 32.866], mean action: 2.318 [0.000, 12.000],  loss: 0.015609, mae: 0.259554, mean_q: 0.369326, mean_eps: 0.000000
 11559/30000: episode: 307, duration: 0.673s, episode steps:  41, steps per second:  61, episode reward: 35.931, mean reward:  0.876 [-2.808, 32.211], mean action: 3.561 [0.000, 15.000],  loss: 0.017366, mae: 0.264046, mean_q: 0.385696, mean_eps: 0.000000
 11593/30000: episode: 308, duration: 0.473s, episode steps:  34, steps per second:  72, episode reward: 32.805, mean reward:  0.965 [-3.000, 32.213], mean action: 2.912 [0.000, 16.000],  loss: 0.012301, mae: 0.246783, mean_q: 0.404984, mean_eps: 0.000000
 11627/30000: episode: 309, duration: 1.627s, episode steps:  34, steps per second:  21, episode reward: 35.594, mean reward:  1.047 [-2.477, 32.240], mean action: 4.676 [0.000, 15.000],  loss: 0.018208, mae: 0.277466, mean_q: 0.401248, mean_eps: 0.000000
 11663/30000: episode: 310, duration: 0.950s, episode steps:  36, steps per second:  38, episode reward: 38.269, mean reward:  1.063 [-3.000, 32.090], mean action: 1.889 [0.000, 15.000],  loss: 0.013841, mae: 0.260522, mean_q: 0.379931, mean_eps: 0.000000
 11687/30000: episode: 311, duration: 0.354s, episode steps:  24, steps per second:  68, episode reward: 35.450, mean reward:  1.477 [-2.407, 32.010], mean action: 7.750 [3.000, 16.000],  loss: 0.014806, mae: 0.258994, mean_q: 0.435967, mean_eps: 0.000000
 11715/30000: episode: 312, duration: 0.387s, episode steps:  28, steps per second:  72, episode reward: 47.911, mean reward:  1.711 [-0.558, 32.080], mean action: 2.429 [0.000, 12.000],  loss: 0.018769, mae: 0.280754, mean_q: 0.398773, mean_eps: 0.000000
 11738/30000: episode: 313, duration: 0.410s, episode steps:  23, steps per second:  56, episode reward: 41.690, mean reward:  1.813 [-2.324, 32.040], mean action: 5.217 [0.000, 16.000],  loss: 0.017281, mae: 0.269111, mean_q: 0.377359, mean_eps: 0.000000
 11765/30000: episode: 314, duration: 0.517s, episode steps:  27, steps per second:  52, episode reward: 44.958, mean reward:  1.665 [-2.497, 32.398], mean action: 2.370 [0.000, 15.000],  loss: 0.015525, mae: 0.262261, mean_q: 0.382323, mean_eps: 0.000000
 11786/30000: episode: 315, duration: 0.392s, episode steps:  21, steps per second:  54, episode reward: 42.000, mean reward:  2.000 [-2.894, 32.890], mean action: 4.381 [0.000, 17.000],  loss: 0.012275, mae: 0.244690, mean_q: 0.379544, mean_eps: 0.000000
 11807/30000: episode: 316, duration: 0.302s, episode steps:  21, steps per second:  70, episode reward: 41.634, mean reward:  1.983 [-3.000, 31.870], mean action: 1.619 [0.000, 12.000],  loss: 0.016166, mae: 0.272152, mean_q: 0.321100, mean_eps: 0.000000
 11826/30000: episode: 317, duration: 0.729s, episode steps:  19, steps per second:  26, episode reward: 38.806, mean reward:  2.042 [-3.000, 32.593], mean action: 2.895 [0.000, 16.000],  loss: 0.020503, mae: 0.289073, mean_q: 0.394466, mean_eps: 0.000000
 11868/30000: episode: 318, duration: 1.181s, episode steps:  42, steps per second:  36, episode reward: 41.042, mean reward:  0.977 [-2.712, 32.450], mean action: 2.095 [0.000, 16.000],  loss: 0.017051, mae: 0.269482, mean_q: 0.434539, mean_eps: 0.000000
 11883/30000: episode: 319, duration: 0.252s, episode steps:  15, steps per second:  60, episode reward: 41.707, mean reward:  2.780 [-2.129, 32.280], mean action: 1.200 [0.000, 3.000],  loss: 0.019758, mae: 0.293167, mean_q: 0.363752, mean_eps: 0.000000
 11911/30000: episode: 320, duration: 0.734s, episode steps:  28, steps per second:  38, episode reward: 44.361, mean reward:  1.584 [-2.382, 32.412], mean action: 1.286 [0.000, 12.000],  loss: 0.013243, mae: 0.256545, mean_q: 0.378313, mean_eps: 0.000000
 11933/30000: episode: 321, duration: 0.402s, episode steps:  22, steps per second:  55, episode reward: 42.000, mean reward:  1.909 [-2.871, 32.480], mean action: 1.364 [0.000, 12.000],  loss: 0.014604, mae: 0.270659, mean_q: 0.411350, mean_eps: 0.000000
 11964/30000: episode: 322, duration: 0.632s, episode steps:  31, steps per second:  49, episode reward: 41.356, mean reward:  1.334 [-2.728, 31.843], mean action: 3.129 [0.000, 12.000],  loss: 0.017663, mae: 0.283277, mean_q: 0.410952, mean_eps: 0.000000
 12000/30000: episode: 323, duration: 0.405s, episode steps:  36, steps per second:  89, episode reward: 41.004, mean reward:  1.139 [-2.083, 32.230], mean action: 1.389 [0.000, 8.000],  loss: 0.015748, mae: 0.285499, mean_q: 0.353346, mean_eps: 0.000000
 12041/30000: episode: 324, duration: 0.838s, episode steps:  41, steps per second:  49, episode reward: 38.641, mean reward:  0.942 [-3.000, 32.190], mean action: 4.146 [0.000, 16.000],  loss: 0.015349, mae: 0.271076, mean_q: 0.367904, mean_eps: 0.000000
 12061/30000: episode: 325, duration: 0.367s, episode steps:  20, steps per second:  54, episode reward: 41.806, mean reward:  2.090 [-2.550, 32.013], mean action: 1.950 [0.000, 6.000],  loss: 0.014603, mae: 0.264975, mean_q: 0.374765, mean_eps: 0.000000
 12087/30000: episode: 326, duration: 0.377s, episode steps:  26, steps per second:  69, episode reward: -32.270, mean reward: -1.241 [-32.758,  2.500], mean action: 3.654 [0.000, 16.000],  loss: 0.017486, mae: 0.281354, mean_q: 0.348379, mean_eps: 0.000000
 12122/30000: episode: 327, duration: 0.382s, episode steps:  35, steps per second:  92, episode reward: 37.971, mean reward:  1.085 [-2.938, 32.424], mean action: 4.629 [0.000, 21.000],  loss: 0.015532, mae: 0.272816, mean_q: 0.303269, mean_eps: 0.000000
 12161/30000: episode: 328, duration: 0.501s, episode steps:  39, steps per second:  78, episode reward: 32.357, mean reward:  0.830 [-2.822, 32.590], mean action: 7.513 [0.000, 17.000],  loss: 0.016764, mae: 0.274916, mean_q: 0.369553, mean_eps: 0.000000
 12181/30000: episode: 329, duration: 0.227s, episode steps:  20, steps per second:  88, episode reward: 44.490, mean reward:  2.224 [-2.188, 32.150], mean action: 1.100 [0.000, 12.000],  loss: 0.014641, mae: 0.261505, mean_q: 0.379290, mean_eps: 0.000000
 12207/30000: episode: 330, duration: 0.270s, episode steps:  26, steps per second:  96, episode reward: 38.672, mean reward:  1.487 [-3.000, 32.250], mean action: 1.423 [0.000, 12.000],  loss: 0.017536, mae: 0.273618, mean_q: 0.380554, mean_eps: 0.000000
 12263/30000: episode: 331, duration: 0.600s, episode steps:  56, steps per second:  93, episode reward: 38.778, mean reward:  0.692 [-2.417, 32.140], mean action: 3.875 [0.000, 21.000],  loss: 0.014891, mae: 0.255256, mean_q: 0.370993, mean_eps: 0.000000
 12303/30000: episode: 332, duration: 0.766s, episode steps:  40, steps per second:  52, episode reward: 42.000, mean reward:  1.050 [-2.878, 32.460], mean action: 2.450 [0.000, 16.000],  loss: 0.015755, mae: 0.259245, mean_q: 0.348980, mean_eps: 0.000000
 12346/30000: episode: 333, duration: 0.586s, episode steps:  43, steps per second:  73, episode reward: 37.707, mean reward:  0.877 [-2.378, 32.250], mean action: 3.209 [0.000, 16.000],  loss: 0.013796, mae: 0.253270, mean_q: 0.349691, mean_eps: 0.000000
 12376/30000: episode: 334, duration: 0.550s, episode steps:  30, steps per second:  55, episode reward: 35.044, mean reward:  1.168 [-3.000, 31.702], mean action: 3.967 [0.000, 18.000],  loss: 0.018002, mae: 0.275973, mean_q: 0.388734, mean_eps: 0.000000
 12401/30000: episode: 335, duration: 0.388s, episode steps:  25, steps per second:  64, episode reward: 38.201, mean reward:  1.528 [-2.614, 32.340], mean action: 1.800 [0.000, 12.000],  loss: 0.015560, mae: 0.264188, mean_q: 0.400460, mean_eps: 0.000000
 12423/30000: episode: 336, duration: 0.236s, episode steps:  22, steps per second:  93, episode reward: 45.000, mean reward:  2.045 [-2.202, 32.030], mean action: 2.864 [1.000, 4.000],  loss: 0.017523, mae: 0.280600, mean_q: 0.403858, mean_eps: 0.000000
 12452/30000: episode: 337, duration: 0.431s, episode steps:  29, steps per second:  67, episode reward: 35.812, mean reward:  1.235 [-2.813, 32.352], mean action: 2.793 [0.000, 15.000],  loss: 0.011644, mae: 0.261228, mean_q: 0.336952, mean_eps: 0.000000
 12471/30000: episode: 338, duration: 0.214s, episode steps:  19, steps per second:  89, episode reward: 44.763, mean reward:  2.356 [-2.172, 32.160], mean action: 3.000 [0.000, 6.000],  loss: 0.015380, mae: 0.281452, mean_q: 0.347311, mean_eps: 0.000000
 12500/30000: episode: 339, duration: 0.450s, episode steps:  29, steps per second:  64, episode reward: 37.843, mean reward:  1.305 [-2.502, 32.070], mean action: 2.621 [0.000, 16.000],  loss: 0.018153, mae: 0.283072, mean_q: 0.425493, mean_eps: 0.000000
 12521/30000: episode: 340, duration: 0.301s, episode steps:  21, steps per second:  70, episode reward: 36.000, mean reward:  1.714 [-3.000, 32.560], mean action: 3.857 [0.000, 12.000],  loss: 0.012022, mae: 0.257738, mean_q: 0.360238, mean_eps: 0.000000
 12542/30000: episode: 341, duration: 0.291s, episode steps:  21, steps per second:  72, episode reward: 41.085, mean reward:  1.956 [-2.618, 32.450], mean action: 2.286 [0.000, 12.000],  loss: 0.018399, mae: 0.279285, mean_q: 0.419497, mean_eps: 0.000000
 12596/30000: episode: 342, duration: 0.862s, episode steps:  54, steps per second:  63, episode reward: 35.940, mean reward:  0.666 [-3.000, 32.280], mean action: 1.944 [0.000, 17.000],  loss: 0.015251, mae: 0.273863, mean_q: 0.340960, mean_eps: 0.000000
 12618/30000: episode: 343, duration: 0.483s, episode steps:  22, steps per second:  46, episode reward: 41.380, mean reward:  1.881 [-2.167, 31.792], mean action: 1.955 [0.000, 12.000],  loss: 0.016940, mae: 0.278615, mean_q: 0.358902, mean_eps: 0.000000
 12683/30000: episode: 344, duration: 1.596s, episode steps:  65, steps per second:  41, episode reward: -32.370, mean reward: -0.498 [-32.249,  2.436], mean action: 10.708 [0.000, 18.000],  loss: 0.014663, mae: 0.270506, mean_q: 0.342171, mean_eps: 0.000000
 12732/30000: episode: 345, duration: 0.745s, episode steps:  49, steps per second:  66, episode reward: 38.873, mean reward:  0.793 [-3.000, 32.020], mean action: 3.449 [0.000, 16.000],  loss: 0.015654, mae: 0.272040, mean_q: 0.357842, mean_eps: 0.000000
 12751/30000: episode: 346, duration: 0.232s, episode steps:  19, steps per second:  82, episode reward: 41.348, mean reward:  2.176 [-3.000, 32.020], mean action: 2.632 [0.000, 17.000],  loss: 0.015718, mae: 0.269883, mean_q: 0.376337, mean_eps: 0.000000
 12776/30000: episode: 347, duration: 0.491s, episode steps:  25, steps per second:  51, episode reward: 42.000, mean reward:  1.680 [-2.353, 32.270], mean action: 2.640 [0.000, 12.000],  loss: 0.015140, mae: 0.263832, mean_q: 0.359703, mean_eps: 0.000000
 12795/30000: episode: 348, duration: 0.502s, episode steps:  19, steps per second:  38, episode reward: 41.937, mean reward:  2.207 [-3.000, 32.167], mean action: 2.526 [0.000, 12.000],  loss: 0.014332, mae: 0.251793, mean_q: 0.458924, mean_eps: 0.000000
 12823/30000: episode: 349, duration: 0.883s, episode steps:  28, steps per second:  32, episode reward: 45.000, mean reward:  1.607 [-2.266, 32.200], mean action: 2.286 [0.000, 16.000],  loss: 0.015524, mae: 0.277580, mean_q: 0.365868, mean_eps: 0.000000
 12850/30000: episode: 350, duration: 0.323s, episode steps:  27, steps per second:  84, episode reward: 38.250, mean reward:  1.417 [-2.510, 31.689], mean action: 2.704 [0.000, 16.000],  loss: 0.016520, mae: 0.285155, mean_q: 0.364836, mean_eps: 0.000000
 12894/30000: episode: 351, duration: 0.475s, episode steps:  44, steps per second:  93, episode reward: 35.568, mean reward:  0.808 [-2.998, 32.050], mean action: 4.455 [0.000, 21.000],  loss: 0.015252, mae: 0.280720, mean_q: 0.373233, mean_eps: 0.000000
 12932/30000: episode: 352, duration: 0.434s, episode steps:  38, steps per second:  88, episode reward: 38.970, mean reward:  1.026 [-2.557, 32.400], mean action: 2.053 [0.000, 15.000],  loss: 0.016581, mae: 0.274164, mean_q: 0.423113, mean_eps: 0.000000
 12961/30000: episode: 353, duration: 0.313s, episode steps:  29, steps per second:  93, episode reward: 44.075, mean reward:  1.520 [-2.100, 32.090], mean action: 2.517 [0.000, 3.000],  loss: 0.017142, mae: 0.279578, mean_q: 0.369699, mean_eps: 0.000000
 12992/30000: episode: 354, duration: 0.324s, episode steps:  31, steps per second:  96, episode reward: 38.296, mean reward:  1.235 [-2.701, 32.066], mean action: 5.935 [0.000, 15.000],  loss: 0.017566, mae: 0.283310, mean_q: 0.361075, mean_eps: 0.000000
 13025/30000: episode: 355, duration: 0.415s, episode steps:  33, steps per second:  79, episode reward: 37.945, mean reward:  1.150 [-3.000, 32.260], mean action: 4.152 [0.000, 21.000],  loss: 0.015965, mae: 0.273715, mean_q: 0.361586, mean_eps: 0.000000
 13073/30000: episode: 356, duration: 0.548s, episode steps:  48, steps per second:  88, episode reward: 45.000, mean reward:  0.938 [-0.568, 29.791], mean action: 1.917 [0.000, 4.000],  loss: 0.013693, mae: 0.255792, mean_q: 0.428432, mean_eps: 0.000000
 13106/30000: episode: 357, duration: 0.370s, episode steps:  33, steps per second:  89, episode reward: 38.791, mean reward:  1.175 [-3.000, 32.295], mean action: 2.152 [0.000, 12.000],  loss: 0.014207, mae: 0.260487, mean_q: 0.373172, mean_eps: 0.000000
 13121/30000: episode: 358, duration: 0.236s, episode steps:  15, steps per second:  64, episode reward: 47.726, mean reward:  3.182 [-0.512, 32.458], mean action: 2.067 [0.000, 10.000],  loss: 0.018629, mae: 0.285605, mean_q: 0.361770, mean_eps: 0.000000
 13152/30000: episode: 359, duration: 0.325s, episode steps:  31, steps per second:  95, episode reward: 38.671, mean reward:  1.247 [-2.616, 31.681], mean action: 2.871 [0.000, 12.000],  loss: 0.018212, mae: 0.272888, mean_q: 0.382845, mean_eps: 0.000000
 13191/30000: episode: 360, duration: 0.411s, episode steps:  39, steps per second:  95, episode reward: 41.876, mean reward:  1.074 [-2.118, 32.190], mean action: 0.974 [0.000, 4.000],  loss: 0.017226, mae: 0.272716, mean_q: 0.418785, mean_eps: 0.000000
 13219/30000: episode: 361, duration: 0.288s, episode steps:  28, steps per second:  97, episode reward: 35.601, mean reward:  1.271 [-3.000, 32.739], mean action: 3.643 [0.000, 16.000],  loss: 0.017048, mae: 0.282197, mean_q: 0.357276, mean_eps: 0.000000
 13245/30000: episode: 362, duration: 0.269s, episode steps:  26, steps per second:  97, episode reward: 33.000, mean reward:  1.269 [-3.000, 32.170], mean action: 6.038 [0.000, 16.000],  loss: 0.015543, mae: 0.278223, mean_q: 0.284072, mean_eps: 0.000000
 13269/30000: episode: 363, duration: 0.255s, episode steps:  24, steps per second:  94, episode reward: 38.163, mean reward:  1.590 [-2.777, 31.803], mean action: 2.583 [0.000, 14.000],  loss: 0.014989, mae: 0.278564, mean_q: 0.307448, mean_eps: 0.000000
 13305/30000: episode: 364, duration: 0.373s, episode steps:  36, steps per second:  97, episode reward: 38.653, mean reward:  1.074 [-2.712, 32.340], mean action: 2.361 [0.000, 16.000],  loss: 0.016480, mae: 0.275379, mean_q: 0.362713, mean_eps: 0.000000
 13330/30000: episode: 365, duration: 0.459s, episode steps:  25, steps per second:  54, episode reward: 38.804, mean reward:  1.552 [-2.270, 31.824], mean action: 2.800 [1.000, 16.000],  loss: 0.015402, mae: 0.277669, mean_q: 0.365114, mean_eps: 0.000000
 13375/30000: episode: 366, duration: 0.464s, episode steps:  45, steps per second:  97, episode reward: 39.000, mean reward:  0.867 [-2.172, 32.170], mean action: 2.244 [0.000, 15.000],  loss: 0.016212, mae: 0.278266, mean_q: 0.356085, mean_eps: 0.000000
 13402/30000: episode: 367, duration: 0.279s, episode steps:  27, steps per second:  97, episode reward: 33.000, mean reward:  1.222 [-3.000, 32.150], mean action: 3.963 [0.000, 12.000],  loss: 0.016897, mae: 0.273249, mean_q: 0.464634, mean_eps: 0.000000
 13423/30000: episode: 368, duration: 0.223s, episode steps:  21, steps per second:  94, episode reward: 44.265, mean reward:  2.108 [-2.695, 32.040], mean action: 3.048 [0.000, 12.000],  loss: 0.016361, mae: 0.279410, mean_q: 0.349367, mean_eps: 0.000000
 13449/30000: episode: 369, duration: 0.287s, episode steps:  26, steps per second:  91, episode reward: 41.244, mean reward:  1.586 [-2.204, 32.050], mean action: 2.615 [0.000, 16.000],  loss: 0.014396, mae: 0.271612, mean_q: 0.370462, mean_eps: 0.000000
 13480/30000: episode: 370, duration: 0.335s, episode steps:  31, steps per second:  92, episode reward: -32.620, mean reward: -1.052 [-32.148,  2.542], mean action: 6.484 [0.000, 18.000],  loss: 0.015623, mae: 0.274554, mean_q: 0.388709, mean_eps: 0.000000
 13513/30000: episode: 371, duration: 0.333s, episode steps:  33, steps per second:  99, episode reward: 35.399, mean reward:  1.073 [-2.802, 32.280], mean action: 3.939 [0.000, 17.000],  loss: 0.017886, mae: 0.282249, mean_q: 0.377486, mean_eps: 0.000000
 13546/30000: episode: 372, duration: 0.337s, episode steps:  33, steps per second:  98, episode reward: 41.926, mean reward:  1.270 [-2.583, 32.080], mean action: 3.121 [0.000, 21.000],  loss: 0.015701, mae: 0.274635, mean_q: 0.342430, mean_eps: 0.000000
 13586/30000: episode: 373, duration: 0.409s, episode steps:  40, steps per second:  98, episode reward: 35.427, mean reward:  0.886 [-2.826, 32.310], mean action: 4.700 [0.000, 17.000],  loss: 0.016983, mae: 0.280255, mean_q: 0.377197, mean_eps: 0.000000
 13655/30000: episode: 374, duration: 0.688s, episode steps:  69, steps per second: 100, episode reward: 35.755, mean reward:  0.518 [-3.000, 32.191], mean action: 2.739 [0.000, 17.000],  loss: 0.017749, mae: 0.281206, mean_q: 0.402875, mean_eps: 0.000000
 13691/30000: episode: 375, duration: 0.372s, episode steps:  36, steps per second:  97, episode reward: 41.873, mean reward:  1.163 [-2.522, 32.143], mean action: 2.083 [0.000, 16.000],  loss: 0.014212, mae: 0.266356, mean_q: 0.344401, mean_eps: 0.000000
 13738/30000: episode: 376, duration: 0.485s, episode steps:  47, steps per second:  97, episode reward: 35.209, mean reward:  0.749 [-2.531, 31.982], mean action: 2.447 [0.000, 15.000],  loss: 0.016515, mae: 0.276004, mean_q: 0.365806, mean_eps: 0.000000
 13769/30000: episode: 377, duration: 0.323s, episode steps:  31, steps per second:  96, episode reward: 35.126, mean reward:  1.133 [-2.643, 32.920], mean action: 4.871 [0.000, 16.000],  loss: 0.015553, mae: 0.258291, mean_q: 0.388772, mean_eps: 0.000000
 13801/30000: episode: 378, duration: 0.324s, episode steps:  32, steps per second:  99, episode reward: -32.380, mean reward: -1.012 [-32.338,  2.502], mean action: 3.000 [0.000, 12.000],  loss: 0.017081, mae: 0.262515, mean_q: 0.404193, mean_eps: 0.000000
 13831/30000: episode: 379, duration: 0.320s, episode steps:  30, steps per second:  94, episode reward: 38.691, mean reward:  1.290 [-2.233, 31.952], mean action: 1.567 [0.000, 12.000],  loss: 0.016523, mae: 0.265726, mean_q: 0.375787, mean_eps: 0.000000
 13860/30000: episode: 380, duration: 0.302s, episode steps:  29, steps per second:  96, episode reward: 38.940, mean reward:  1.343 [-2.636, 32.510], mean action: 3.310 [0.000, 16.000],  loss: 0.017034, mae: 0.263113, mean_q: 0.376642, mean_eps: 0.000000
 13892/30000: episode: 381, duration: 0.365s, episode steps:  32, steps per second:  88, episode reward: 36.000, mean reward:  1.125 [-3.000, 32.120], mean action: 2.594 [0.000, 12.000],  loss: 0.017885, mae: 0.266085, mean_q: 0.459988, mean_eps: 0.000000
 13919/30000: episode: 382, duration: 0.315s, episode steps:  27, steps per second:  86, episode reward: 41.947, mean reward:  1.554 [-2.400, 32.300], mean action: 1.926 [0.000, 12.000],  loss: 0.015496, mae: 0.261967, mean_q: 0.428223, mean_eps: 0.000000
 13950/30000: episode: 383, duration: 0.338s, episode steps:  31, steps per second:  92, episode reward: 41.745, mean reward:  1.347 [-2.830, 32.180], mean action: 1.226 [0.000, 12.000],  loss: 0.014845, mae: 0.261551, mean_q: 0.423535, mean_eps: 0.000000
 13974/30000: episode: 384, duration: 0.252s, episode steps:  24, steps per second:  95, episode reward: 38.428, mean reward:  1.601 [-2.562, 32.141], mean action: 5.042 [0.000, 18.000],  loss: 0.019773, mae: 0.289791, mean_q: 0.392125, mean_eps: 0.000000
 13988/30000: episode: 385, duration: 0.161s, episode steps:  14, steps per second:  87, episode reward: 42.000, mean reward:  3.000 [-2.272, 29.916], mean action: 2.357 [0.000, 12.000],  loss: 0.015797, mae: 0.263171, mean_q: 0.492872, mean_eps: 0.000000
 14024/30000: episode: 386, duration: 0.385s, episode steps:  36, steps per second:  93, episode reward: 35.339, mean reward:  0.982 [-2.653, 32.060], mean action: 3.361 [0.000, 18.000],  loss: 0.017272, mae: 0.270139, mean_q: 0.426875, mean_eps: 0.000000
 14062/30000: episode: 387, duration: 0.437s, episode steps:  38, steps per second:  87, episode reward: 37.393, mean reward:  0.984 [-2.633, 32.140], mean action: 2.395 [0.000, 12.000],  loss: 0.016033, mae: 0.268047, mean_q: 0.427598, mean_eps: 0.000000
 14080/30000: episode: 388, duration: 0.202s, episode steps:  18, steps per second:  89, episode reward: 46.998, mean reward:  2.611 [-0.062, 32.530], mean action: 3.889 [0.000, 10.000],  loss: 0.015973, mae: 0.309133, mean_q: 0.246835, mean_eps: 0.000000
 14121/30000: episode: 389, duration: 0.425s, episode steps:  41, steps per second:  96, episode reward: 35.716, mean reward:  0.871 [-2.915, 32.170], mean action: 4.098 [0.000, 16.000],  loss: 0.016001, mae: 0.274022, mean_q: 0.416291, mean_eps: 0.000000
 14143/30000: episode: 390, duration: 0.235s, episode steps:  22, steps per second:  94, episode reward: 39.000, mean reward:  1.773 [-2.243, 32.220], mean action: 4.000 [0.000, 15.000],  loss: 0.016124, mae: 0.284791, mean_q: 0.347278, mean_eps: 0.000000
 14178/30000: episode: 391, duration: 0.375s, episode steps:  35, steps per second:  93, episode reward: 42.000, mean reward:  1.200 [-3.000, 32.820], mean action: 4.457 [0.000, 16.000],  loss: 0.017256, mae: 0.280835, mean_q: 0.394881, mean_eps: 0.000000
 14220/30000: episode: 392, duration: 0.489s, episode steps:  42, steps per second:  86, episode reward: 38.639, mean reward:  0.920 [-3.000, 32.331], mean action: 3.024 [0.000, 16.000],  loss: 0.016707, mae: 0.271168, mean_q: 0.385573, mean_eps: 0.000000
 14244/30000: episode: 393, duration: 0.288s, episode steps:  24, steps per second:  83, episode reward: 44.713, mean reward:  1.863 [-2.255, 32.204], mean action: 1.625 [0.000, 6.000],  loss: 0.016019, mae: 0.269721, mean_q: 0.358050, mean_eps: 0.000000
 14287/30000: episode: 394, duration: 0.452s, episode steps:  43, steps per second:  95, episode reward: 38.276, mean reward:  0.890 [-2.522, 32.180], mean action: 4.372 [0.000, 16.000],  loss: 0.017603, mae: 0.276709, mean_q: 0.400447, mean_eps: 0.000000
 14314/30000: episode: 395, duration: 0.293s, episode steps:  27, steps per second:  92, episode reward: 40.737, mean reward:  1.509 [-2.354, 31.873], mean action: 3.852 [0.000, 21.000],  loss: 0.017121, mae: 0.265922, mean_q: 0.385139, mean_eps: 0.000000
 14331/30000: episode: 396, duration: 0.189s, episode steps:  17, steps per second:  90, episode reward: 48.000, mean reward:  2.824 [-0.350, 32.380], mean action: 1.588 [0.000, 3.000],  loss: 0.014962, mae: 0.261653, mean_q: 0.435507, mean_eps: 0.000000
 14378/30000: episode: 397, duration: 0.495s, episode steps:  47, steps per second:  95, episode reward: 32.416, mean reward:  0.690 [-2.377, 32.070], mean action: 9.213 [0.000, 17.000],  loss: 0.015859, mae: 0.265708, mean_q: 0.388999, mean_eps: 0.000000
 14405/30000: episode: 398, duration: 0.405s, episode steps:  27, steps per second:  67, episode reward: 43.858, mean reward:  1.624 [-2.858, 31.849], mean action: 2.333 [0.000, 16.000],  loss: 0.018054, mae: 0.273203, mean_q: 0.432108, mean_eps: 0.000000
 14454/30000: episode: 399, duration: 0.531s, episode steps:  49, steps per second:  92, episode reward: 37.718, mean reward:  0.770 [-2.244, 32.104], mean action: 5.612 [0.000, 21.000],  loss: 0.015790, mae: 0.262781, mean_q: 0.378174, mean_eps: 0.000000
 14488/30000: episode: 400, duration: 0.368s, episode steps:  34, steps per second:  92, episode reward: 33.000, mean reward:  0.971 [-2.479, 32.180], mean action: 2.118 [0.000, 15.000],  loss: 0.015332, mae: 0.266035, mean_q: 0.384404, mean_eps: 0.000000
 14512/30000: episode: 401, duration: 0.255s, episode steps:  24, steps per second:  94, episode reward: 43.923, mean reward:  1.830 [-2.283, 32.450], mean action: 1.875 [0.000, 12.000],  loss: 0.019142, mae: 0.276657, mean_q: 0.368236, mean_eps: 0.000000
 14533/30000: episode: 402, duration: 0.225s, episode steps:  21, steps per second:  93, episode reward: 40.706, mean reward:  1.938 [-2.707, 32.061], mean action: 7.667 [3.000, 21.000],  loss: 0.014631, mae: 0.258234, mean_q: 0.414570, mean_eps: 0.000000
 14583/30000: episode: 403, duration: 0.520s, episode steps:  50, steps per second:  96, episode reward: 37.850, mean reward:  0.757 [-3.000, 31.803], mean action: 2.560 [0.000, 16.000],  loss: 0.015386, mae: 0.274469, mean_q: 0.356250, mean_eps: 0.000000
 14607/30000: episode: 404, duration: 0.250s, episode steps:  24, steps per second:  96, episode reward: 44.336, mean reward:  1.847 [-2.256, 32.220], mean action: 2.750 [0.000, 12.000],  loss: 0.016852, mae: 0.270329, mean_q: 0.434389, mean_eps: 0.000000
 14628/30000: episode: 405, duration: 0.225s, episode steps:  21, steps per second:  93, episode reward: 32.820, mean reward:  1.563 [-3.000, 29.780], mean action: 4.714 [0.000, 16.000],  loss: 0.013873, mae: 0.265033, mean_q: 0.412249, mean_eps: 0.000000
 14659/30000: episode: 406, duration: 0.320s, episode steps:  31, steps per second:  97, episode reward: 35.318, mean reward:  1.139 [-2.823, 32.140], mean action: 3.710 [0.000, 18.000],  loss: 0.017505, mae: 0.278005, mean_q: 0.381151, mean_eps: 0.000000
 14682/30000: episode: 407, duration: 0.240s, episode steps:  23, steps per second:  96, episode reward: 38.938, mean reward:  1.693 [-2.035, 33.000], mean action: 2.870 [0.000, 16.000],  loss: 0.015931, mae: 0.268200, mean_q: 0.363416, mean_eps: 0.000000
 14721/30000: episode: 408, duration: 0.404s, episode steps:  39, steps per second:  97, episode reward: -32.440, mean reward: -0.832 [-33.000,  2.360], mean action: 3.744 [0.000, 17.000],  loss: 0.016044, mae: 0.275117, mean_q: 0.367356, mean_eps: 0.000000
 14748/30000: episode: 409, duration: 0.300s, episode steps:  27, steps per second:  90, episode reward: 41.580, mean reward:  1.540 [-2.238, 32.280], mean action: 2.556 [0.000, 12.000],  loss: 0.013489, mae: 0.255949, mean_q: 0.420429, mean_eps: 0.000000
 14781/30000: episode: 410, duration: 0.336s, episode steps:  33, steps per second:  98, episode reward: 36.000, mean reward:  1.091 [-2.779, 32.420], mean action: 3.242 [0.000, 16.000],  loss: 0.016826, mae: 0.269340, mean_q: 0.406059, mean_eps: 0.000000
 14833/30000: episode: 411, duration: 0.555s, episode steps:  52, steps per second:  94, episode reward: -32.600, mean reward: -0.627 [-32.122,  2.904], mean action: 2.769 [0.000, 16.000],  loss: 0.016680, mae: 0.272744, mean_q: 0.357808, mean_eps: 0.000000
 14872/30000: episode: 412, duration: 0.442s, episode steps:  39, steps per second:  88, episode reward: 32.743, mean reward:  0.840 [-3.000, 31.933], mean action: 2.564 [0.000, 13.000],  loss: 0.013489, mae: 0.255586, mean_q: 0.356701, mean_eps: 0.000000
 14900/30000: episode: 413, duration: 0.318s, episode steps:  28, steps per second:  88, episode reward: 38.488, mean reward:  1.375 [-3.000, 32.010], mean action: 1.821 [0.000, 17.000],  loss: 0.018551, mae: 0.285050, mean_q: 0.376484, mean_eps: 0.000000
 14927/30000: episode: 414, duration: 0.286s, episode steps:  27, steps per second:  94, episode reward: 41.579, mean reward:  1.540 [-3.000, 32.190], mean action: 3.296 [0.000, 17.000],  loss: 0.014621, mae: 0.257055, mean_q: 0.436844, mean_eps: 0.000000
 14953/30000: episode: 415, duration: 0.273s, episode steps:  26, steps per second:  95, episode reward: 43.448, mean reward:  1.671 [-2.199, 31.572], mean action: 2.038 [0.000, 7.000],  loss: 0.020650, mae: 0.289571, mean_q: 0.399033, mean_eps: 0.000000
 14988/30000: episode: 416, duration: 0.364s, episode steps:  35, steps per second:  96, episode reward: 41.032, mean reward:  1.172 [-2.761, 31.844], mean action: 2.114 [0.000, 21.000],  loss: 0.015755, mae: 0.271331, mean_q: 0.391262, mean_eps: 0.000000
 15014/30000: episode: 417, duration: 0.740s, episode steps:  26, steps per second:  35, episode reward: 39.000, mean reward:  1.500 [-2.297, 32.280], mean action: 1.692 [0.000, 6.000],  loss: 0.015692, mae: 0.261771, mean_q: 0.444054, mean_eps: 0.000000
 15037/30000: episode: 418, duration: 0.396s, episode steps:  23, steps per second:  58, episode reward: 38.071, mean reward:  1.655 [-2.785, 31.821], mean action: 2.348 [0.000, 15.000],  loss: 0.015972, mae: 0.263710, mean_q: 0.400373, mean_eps: 0.000000
 15059/30000: episode: 419, duration: 0.278s, episode steps:  22, steps per second:  79, episode reward: 38.030, mean reward:  1.729 [-3.000, 32.080], mean action: 3.227 [0.000, 21.000],  loss: 0.015926, mae: 0.282751, mean_q: 0.300448, mean_eps: 0.000000
 15082/30000: episode: 420, duration: 0.420s, episode steps:  23, steps per second:  55, episode reward: 42.000, mean reward:  1.826 [-2.086, 32.090], mean action: 2.652 [0.000, 6.000],  loss: 0.017600, mae: 0.275349, mean_q: 0.413607, mean_eps: 0.000000
 15135/30000: episode: 421, duration: 1.012s, episode steps:  53, steps per second:  52, episode reward: 38.467, mean reward:  0.726 [-3.000, 32.147], mean action: 3.113 [0.000, 21.000],  loss: 0.018854, mae: 0.277859, mean_q: 0.392958, mean_eps: 0.000000
 15159/30000: episode: 422, duration: 0.407s, episode steps:  24, steps per second:  59, episode reward: -35.260, mean reward: -1.469 [-32.746,  3.030], mean action: 6.542 [0.000, 16.000],  loss: 0.017447, mae: 0.267350, mean_q: 0.402464, mean_eps: 0.000000
 15174/30000: episode: 423, duration: 0.169s, episode steps:  15, steps per second:  89, episode reward: 42.000, mean reward:  2.800 [-3.000, 32.300], mean action: 5.333 [0.000, 21.000],  loss: 0.012478, mae: 0.249531, mean_q: 0.381911, mean_eps: 0.000000
 15206/30000: episode: 424, duration: 0.463s, episode steps:  32, steps per second:  69, episode reward: 41.118, mean reward:  1.285 [-2.348, 32.200], mean action: 3.781 [0.000, 21.000],  loss: 0.017552, mae: 0.274476, mean_q: 0.364995, mean_eps: 0.000000
 15223/30000: episode: 425, duration: 0.190s, episode steps:  17, steps per second:  90, episode reward: 41.417, mean reward:  2.436 [-2.448, 32.110], mean action: 3.941 [0.000, 15.000],  loss: 0.017952, mae: 0.272127, mean_q: 0.349430, mean_eps: 0.000000
 15247/30000: episode: 426, duration: 0.318s, episode steps:  24, steps per second:  75, episode reward: 38.939, mean reward:  1.622 [-2.235, 32.290], mean action: 1.750 [0.000, 16.000],  loss: 0.012402, mae: 0.243368, mean_q: 0.389815, mean_eps: 0.000000
 15291/30000: episode: 427, duration: 0.441s, episode steps:  44, steps per second: 100, episode reward: 41.319, mean reward:  0.939 [-2.242, 32.680], mean action: 2.477 [0.000, 21.000],  loss: 0.015242, mae: 0.266478, mean_q: 0.449362, mean_eps: 0.000000
 15316/30000: episode: 428, duration: 0.273s, episode steps:  25, steps per second:  92, episode reward: 44.675, mean reward:  1.787 [-2.162, 32.124], mean action: 2.280 [0.000, 15.000],  loss: 0.015082, mae: 0.261286, mean_q: 0.418571, mean_eps: 0.000000
 15359/30000: episode: 429, duration: 0.641s, episode steps:  43, steps per second:  67, episode reward: 35.878, mean reward:  0.834 [-2.435, 32.260], mean action: 7.535 [0.000, 21.000],  loss: 0.016008, mae: 0.261902, mean_q: 0.433922, mean_eps: 0.000000
 15381/30000: episode: 430, duration: 0.368s, episode steps:  22, steps per second:  60, episode reward: 38.484, mean reward:  1.749 [-2.659, 31.940], mean action: 2.500 [0.000, 12.000],  loss: 0.014475, mae: 0.261532, mean_q: 0.360347, mean_eps: 0.000000
 15410/30000: episode: 431, duration: 0.457s, episode steps:  29, steps per second:  63, episode reward: 42.000, mean reward:  1.448 [-2.027, 30.373], mean action: 3.069 [1.000, 15.000],  loss: 0.015688, mae: 0.261043, mean_q: 0.435553, mean_eps: 0.000000
 15429/30000: episode: 432, duration: 0.265s, episode steps:  19, steps per second:  72, episode reward: 47.224, mean reward:  2.485 [-0.280, 32.140], mean action: 2.947 [1.000, 8.000],  loss: 0.018370, mae: 0.280648, mean_q: 0.347939, mean_eps: 0.000000
 15451/30000: episode: 433, duration: 0.620s, episode steps:  22, steps per second:  35, episode reward: 41.175, mean reward:  1.872 [-3.000, 31.986], mean action: 3.955 [0.000, 15.000],  loss: 0.017332, mae: 0.270252, mean_q: 0.413463, mean_eps: 0.000000
 15476/30000: episode: 434, duration: 0.305s, episode steps:  25, steps per second:  82, episode reward: 44.017, mean reward:  1.761 [-2.040, 31.971], mean action: 2.920 [0.000, 15.000],  loss: 0.016862, mae: 0.280092, mean_q: 0.349098, mean_eps: 0.000000
 15493/30000: episode: 435, duration: 0.411s, episode steps:  17, steps per second:  41, episode reward: 44.605, mean reward:  2.624 [-2.422, 32.400], mean action: 3.353 [0.000, 15.000],  loss: 0.018357, mae: 0.280245, mean_q: 0.400638, mean_eps: 0.000000
 15519/30000: episode: 436, duration: 0.594s, episode steps:  26, steps per second:  44, episode reward: 43.973, mean reward:  1.691 [-2.528, 32.160], mean action: 3.000 [0.000, 15.000],  loss: 0.017787, mae: 0.283840, mean_q: 0.385348, mean_eps: 0.000000
 15550/30000: episode: 437, duration: 0.552s, episode steps:  31, steps per second:  56, episode reward: 35.878, mean reward:  1.157 [-3.000, 32.247], mean action: 5.968 [0.000, 16.000],  loss: 0.016093, mae: 0.263338, mean_q: 0.454436, mean_eps: 0.000000
 15563/30000: episode: 438, duration: 0.156s, episode steps:  13, steps per second:  83, episode reward: 41.111, mean reward:  3.162 [-3.000, 32.220], mean action: 3.000 [0.000, 16.000],  loss: 0.013673, mae: 0.261746, mean_q: 0.392153, mean_eps: 0.000000
 15593/30000: episode: 439, duration: 0.368s, episode steps:  30, steps per second:  82, episode reward: 44.358, mean reward:  1.479 [-2.144, 32.460], mean action: 0.767 [0.000, 17.000],  loss: 0.017591, mae: 0.272674, mean_q: 0.446523, mean_eps: 0.000000
 15611/30000: episode: 440, duration: 0.208s, episode steps:  18, steps per second:  87, episode reward: 44.013, mean reward:  2.445 [-2.004, 32.310], mean action: 3.000 [0.000, 17.000],  loss: 0.020620, mae: 0.286444, mean_q: 0.410883, mean_eps: 0.000000
 15634/30000: episode: 441, duration: 0.260s, episode steps:  23, steps per second:  89, episode reward: 46.823, mean reward:  2.036 [-0.389, 32.340], mean action: 3.478 [0.000, 15.000],  loss: 0.018785, mae: 0.272515, mean_q: 0.385688, mean_eps: 0.000000
 15650/30000: episode: 442, duration: 0.176s, episode steps:  16, steps per second:  91, episode reward: 41.093, mean reward:  2.568 [-3.000, 33.000], mean action: 3.000 [0.000, 15.000],  loss: 0.020775, mae: 0.288659, mean_q: 0.393761, mean_eps: 0.000000
 15692/30000: episode: 443, duration: 0.454s, episode steps:  42, steps per second:  92, episode reward: 35.562, mean reward:  0.847 [-2.600, 32.140], mean action: 3.643 [0.000, 17.000],  loss: 0.014427, mae: 0.253483, mean_q: 0.402648, mean_eps: 0.000000
 15718/30000: episode: 444, duration: 0.298s, episode steps:  26, steps per second:  87, episode reward: 39.000, mean reward:  1.500 [-3.000, 32.740], mean action: 4.654 [0.000, 17.000],  loss: 0.017594, mae: 0.272571, mean_q: 0.361947, mean_eps: 0.000000
 15768/30000: episode: 445, duration: 0.546s, episode steps:  50, steps per second:  92, episode reward: -43.560, mean reward: -0.871 [-32.007,  1.740], mean action: 12.060 [0.000, 18.000],  loss: 0.017813, mae: 0.272907, mean_q: 0.388788, mean_eps: 0.000000
 15804/30000: episode: 446, duration: 0.393s, episode steps:  36, steps per second:  92, episode reward: 41.580, mean reward:  1.155 [-2.794, 32.230], mean action: 3.472 [0.000, 15.000],  loss: 0.017759, mae: 0.267875, mean_q: 0.393682, mean_eps: 0.000000
 15824/30000: episode: 447, duration: 0.214s, episode steps:  20, steps per second:  93, episode reward: 42.000, mean reward:  2.100 [-2.286, 32.470], mean action: 6.000 [0.000, 16.000],  loss: 0.017562, mae: 0.268141, mean_q: 0.427929, mean_eps: 0.000000
 15853/30000: episode: 448, duration: 0.320s, episode steps:  29, steps per second:  91, episode reward: 41.163, mean reward:  1.419 [-2.132, 31.553], mean action: 3.483 [0.000, 21.000],  loss: 0.016812, mae: 0.270899, mean_q: 0.369669, mean_eps: 0.000000
 15901/30000: episode: 449, duration: 0.494s, episode steps:  48, steps per second:  97, episode reward: 35.650, mean reward:  0.743 [-3.000, 31.954], mean action: 5.771 [0.000, 21.000],  loss: 0.015904, mae: 0.267375, mean_q: 0.384262, mean_eps: 0.000000
 15921/30000: episode: 450, duration: 0.216s, episode steps:  20, steps per second:  93, episode reward: 44.581, mean reward:  2.229 [-2.262, 32.300], mean action: 3.950 [0.000, 14.000],  loss: 0.016110, mae: 0.264188, mean_q: 0.416910, mean_eps: 0.000000
 15943/30000: episode: 451, duration: 0.236s, episode steps:  22, steps per second:  93, episode reward: 43.867, mean reward:  1.994 [-2.676, 32.180], mean action: 2.545 [0.000, 6.000],  loss: 0.020697, mae: 0.305238, mean_q: 0.311613, mean_eps: 0.000000
 15973/30000: episode: 452, duration: 0.309s, episode steps:  30, steps per second:  97, episode reward: 41.187, mean reward:  1.373 [-2.637, 31.648], mean action: 0.833 [0.000, 6.000],  loss: 0.017240, mae: 0.276249, mean_q: 0.350671, mean_eps: 0.000000
 16012/30000: episode: 453, duration: 0.448s, episode steps:  39, steps per second:  87, episode reward: 39.971, mean reward:  1.025 [-2.279, 31.143], mean action: 2.128 [0.000, 15.000],  loss: 0.017675, mae: 0.276913, mean_q: 0.338737, mean_eps: 0.000000
 16048/30000: episode: 454, duration: 0.374s, episode steps:  36, steps per second:  96, episode reward: -32.230, mean reward: -0.895 [-32.810,  2.410], mean action: 3.750 [0.000, 17.000],  loss: 0.016572, mae: 0.265515, mean_q: 0.372887, mean_eps: 0.000000
 16077/30000: episode: 455, duration: 0.377s, episode steps:  29, steps per second:  77, episode reward: 36.000, mean reward:  1.241 [-2.907, 32.210], mean action: 1.345 [0.000, 6.000],  loss: 0.013713, mae: 0.252239, mean_q: 0.381491, mean_eps: 0.000000
 16098/30000: episode: 456, duration: 0.238s, episode steps:  21, steps per second:  88, episode reward: 44.681, mean reward:  2.128 [-2.024, 32.450], mean action: 2.333 [0.000, 7.000],  loss: 0.018307, mae: 0.273177, mean_q: 0.433475, mean_eps: 0.000000
 16185/30000: episode: 457, duration: 0.881s, episode steps:  87, steps per second:  99, episode reward: -35.880, mean reward: -0.412 [-32.113,  2.285], mean action: 5.747 [0.000, 17.000],  loss: 0.016031, mae: 0.268531, mean_q: 0.375318, mean_eps: 0.000000
 16197/30000: episode: 458, duration: 0.137s, episode steps:  12, steps per second:  88, episode reward: 47.502, mean reward:  3.958 [-0.116, 32.050], mean action: 3.833 [1.000, 15.000],  loss: 0.012676, mae: 0.245181, mean_q: 0.440495, mean_eps: 0.000000
 16208/30000: episode: 459, duration: 0.123s, episode steps:  11, steps per second:  89, episode reward: 44.293, mean reward:  4.027 [-2.044, 33.000], mean action: 1.545 [0.000, 6.000],  loss: 0.015828, mae: 0.269622, mean_q: 0.380260, mean_eps: 0.000000
 16225/30000: episode: 460, duration: 0.185s, episode steps:  17, steps per second:  92, episode reward: 46.553, mean reward:  2.738 [ 0.000, 31.797], mean action: 2.235 [0.000, 15.000],  loss: 0.022088, mae: 0.298950, mean_q: 0.348413, mean_eps: 0.000000
 16247/30000: episode: 461, duration: 0.238s, episode steps:  22, steps per second:  92, episode reward: 38.879, mean reward:  1.767 [-3.000, 32.689], mean action: 2.909 [0.000, 16.000],  loss: 0.016145, mae: 0.264724, mean_q: 0.356886, mean_eps: 0.000000
 16268/30000: episode: 462, duration: 0.219s, episode steps:  21, steps per second:  96, episode reward: 42.000, mean reward:  2.000 [-2.271, 32.520], mean action: 1.810 [0.000, 12.000],  loss: 0.016461, mae: 0.275204, mean_q: 0.324399, mean_eps: 0.000000
 16296/30000: episode: 463, duration: 0.321s, episode steps:  28, steps per second:  87, episode reward: 35.715, mean reward:  1.276 [-2.937, 32.150], mean action: 3.464 [0.000, 21.000],  loss: 0.017517, mae: 0.266503, mean_q: 0.380770, mean_eps: 0.000000
 16320/30000: episode: 464, duration: 0.259s, episode steps:  24, steps per second:  93, episode reward: 37.903, mean reward:  1.579 [-3.000, 32.230], mean action: 3.583 [0.000, 21.000],  loss: 0.015709, mae: 0.257492, mean_q: 0.413699, mean_eps: 0.000000
 16357/30000: episode: 465, duration: 0.394s, episode steps:  37, steps per second:  94, episode reward: 35.226, mean reward:  0.952 [-2.901, 32.480], mean action: 3.162 [0.000, 16.000],  loss: 0.016902, mae: 0.270284, mean_q: 0.389174, mean_eps: 0.000000
 16380/30000: episode: 466, duration: 0.257s, episode steps:  23, steps per second:  90, episode reward: 44.064, mean reward:  1.916 [-2.605, 32.229], mean action: 1.870 [0.000, 6.000],  loss: 0.018195, mae: 0.290917, mean_q: 0.377782, mean_eps: 0.000000
 16392/30000: episode: 467, duration: 0.137s, episode steps:  12, steps per second:  88, episode reward: 47.403, mean reward:  3.950 [-0.410, 32.560], mean action: 1.500 [0.000, 3.000],  loss: 0.013165, mae: 0.264401, mean_q: 0.367160, mean_eps: 0.000000
 16414/30000: episode: 468, duration: 0.257s, episode steps:  22, steps per second:  85, episode reward: 41.442, mean reward:  1.884 [-3.000, 32.160], mean action: 1.909 [0.000, 6.000],  loss: 0.018333, mae: 0.277154, mean_q: 0.407717, mean_eps: 0.000000
 16450/30000: episode: 469, duration: 0.365s, episode steps:  36, steps per second:  99, episode reward: 35.405, mean reward:  0.983 [-2.278, 31.505], mean action: 2.306 [0.000, 15.000],  loss: 0.017282, mae: 0.272144, mean_q: 0.388483, mean_eps: 0.000000
 16472/30000: episode: 470, duration: 0.254s, episode steps:  22, steps per second:  87, episode reward: 41.369, mean reward:  1.880 [-2.315, 32.280], mean action: 2.909 [1.000, 6.000],  loss: 0.016151, mae: 0.257000, mean_q: 0.439171, mean_eps: 0.000000
 16500/30000: episode: 471, duration: 0.293s, episode steps:  28, steps per second:  96, episode reward: 36.000, mean reward:  1.286 [-2.802, 32.320], mean action: 2.607 [0.000, 16.000],  loss: 0.017444, mae: 0.269443, mean_q: 0.409079, mean_eps: 0.000000
 16536/30000: episode: 472, duration: 0.388s, episode steps:  36, steps per second:  93, episode reward: 34.729, mean reward:  0.965 [-3.000, 31.852], mean action: 3.222 [0.000, 16.000],  loss: 0.019801, mae: 0.280481, mean_q: 0.395763, mean_eps: 0.000000
 16564/30000: episode: 473, duration: 0.309s, episode steps:  28, steps per second:  90, episode reward: 41.873, mean reward:  1.495 [-2.346, 32.013], mean action: 1.571 [0.000, 16.000],  loss: 0.015525, mae: 0.263579, mean_q: 0.394558, mean_eps: 0.000000
 16589/30000: episode: 474, duration: 0.430s, episode steps:  25, steps per second:  58, episode reward: 39.000, mean reward:  1.560 [-2.600, 32.450], mean action: 3.080 [0.000, 16.000],  loss: 0.017491, mae: 0.284303, mean_q: 0.347286, mean_eps: 0.000000
 16613/30000: episode: 475, duration: 0.318s, episode steps:  24, steps per second:  76, episode reward: 41.117, mean reward:  1.713 [-2.144, 32.260], mean action: 0.833 [0.000, 6.000],  loss: 0.017024, mae: 0.272833, mean_q: 0.398041, mean_eps: 0.000000
 16644/30000: episode: 476, duration: 0.360s, episode steps:  31, steps per second:  86, episode reward: 41.160, mean reward:  1.328 [-2.543, 32.100], mean action: 2.613 [0.000, 16.000],  loss: 0.020349, mae: 0.285302, mean_q: 0.398764, mean_eps: 0.000000
 16681/30000: episode: 477, duration: 0.614s, episode steps:  37, steps per second:  60, episode reward: 35.273, mean reward:  0.953 [-3.000, 32.230], mean action: 3.378 [0.000, 12.000],  loss: 0.013465, mae: 0.256606, mean_q: 0.430410, mean_eps: 0.000000
 16705/30000: episode: 478, duration: 0.606s, episode steps:  24, steps per second:  40, episode reward: 38.606, mean reward:  1.609 [-2.526, 32.260], mean action: 1.708 [0.000, 6.000],  loss: 0.017373, mae: 0.276248, mean_q: 0.391342, mean_eps: 0.000000
 16746/30000: episode: 479, duration: 0.877s, episode steps:  41, steps per second:  47, episode reward: -33.000, mean reward: -0.805 [-32.319,  2.990], mean action: 3.195 [0.000, 21.000],  loss: 0.015102, mae: 0.262378, mean_q: 0.398242, mean_eps: 0.000000
 16771/30000: episode: 480, duration: 0.359s, episode steps:  25, steps per second:  70, episode reward: 38.901, mean reward:  1.556 [-3.000, 32.471], mean action: 2.320 [0.000, 17.000],  loss: 0.014301, mae: 0.266485, mean_q: 0.339655, mean_eps: 0.000000
 16794/30000: episode: 481, duration: 0.516s, episode steps:  23, steps per second:  45, episode reward: 41.177, mean reward:  1.790 [-2.204, 32.210], mean action: 1.826 [0.000, 6.000],  loss: 0.018804, mae: 0.288664, mean_q: 0.369491, mean_eps: 0.000000
 16817/30000: episode: 482, duration: 0.729s, episode steps:  23, steps per second:  32, episode reward: 37.509, mean reward:  1.631 [-3.000, 32.126], mean action: 4.087 [0.000, 16.000],  loss: 0.019022, mae: 0.291449, mean_q: 0.402253, mean_eps: 0.000000
 16845/30000: episode: 483, duration: 0.420s, episode steps:  28, steps per second:  67, episode reward: 42.000, mean reward:  1.500 [-2.902, 32.190], mean action: 2.357 [0.000, 16.000],  loss: 0.017846, mae: 0.282887, mean_q: 0.421554, mean_eps: 0.000000
 16877/30000: episode: 484, duration: 0.397s, episode steps:  32, steps per second:  81, episode reward: 44.645, mean reward:  1.395 [-2.268, 32.100], mean action: 2.594 [0.000, 6.000],  loss: 0.014228, mae: 0.277824, mean_q: 0.324040, mean_eps: 0.000000
 16909/30000: episode: 485, duration: 0.366s, episode steps:  32, steps per second:  87, episode reward: 35.031, mean reward:  1.095 [-2.997, 32.740], mean action: 3.812 [0.000, 16.000],  loss: 0.017102, mae: 0.277886, mean_q: 0.402693, mean_eps: 0.000000
 16930/30000: episode: 486, duration: 0.388s, episode steps:  21, steps per second:  54, episode reward: 41.939, mean reward:  1.997 [-2.711, 32.100], mean action: 2.952 [0.000, 12.000],  loss: 0.017123, mae: 0.274622, mean_q: 0.396332, mean_eps: 0.000000
 16947/30000: episode: 487, duration: 0.246s, episode steps:  17, steps per second:  69, episode reward: 42.000, mean reward:  2.471 [-2.232, 30.794], mean action: 1.647 [0.000, 6.000],  loss: 0.017768, mae: 0.284320, mean_q: 0.389256, mean_eps: 0.000000
 16988/30000: episode: 488, duration: 0.417s, episode steps:  41, steps per second:  98, episode reward: 38.039, mean reward:  0.928 [-2.859, 32.081], mean action: 3.049 [0.000, 16.000],  loss: 0.016474, mae: 0.279758, mean_q: 0.410369, mean_eps: 0.000000
 17019/30000: episode: 489, duration: 0.327s, episode steps:  31, steps per second:  95, episode reward: 38.821, mean reward:  1.252 [-2.312, 32.270], mean action: 2.935 [0.000, 12.000],  loss: 0.014683, mae: 0.289291, mean_q: 0.330509, mean_eps: 0.000000
 17051/30000: episode: 490, duration: 0.423s, episode steps:  32, steps per second:  76, episode reward: 46.806, mean reward:  1.463 [-0.226, 32.080], mean action: 1.594 [0.000, 10.000],  loss: 0.019279, mae: 0.308739, mean_q: 0.328919, mean_eps: 0.000000
 17081/30000: episode: 491, duration: 0.443s, episode steps:  30, steps per second:  68, episode reward: 38.472, mean reward:  1.282 [-2.732, 32.040], mean action: 1.367 [0.000, 6.000],  loss: 0.016427, mae: 0.273291, mean_q: 0.400777, mean_eps: 0.000000
 17099/30000: episode: 492, duration: 0.268s, episode steps:  18, steps per second:  67, episode reward: 44.182, mean reward:  2.455 [-2.753, 32.501], mean action: 1.444 [0.000, 6.000],  loss: 0.015402, mae: 0.274295, mean_q: 0.426031, mean_eps: 0.000000
 17122/30000: episode: 493, duration: 0.263s, episode steps:  23, steps per second:  87, episode reward: 38.505, mean reward:  1.674 [-2.543, 31.834], mean action: 3.435 [0.000, 16.000],  loss: 0.021529, mae: 0.297889, mean_q: 0.408001, mean_eps: 0.000000
 17142/30000: episode: 494, duration: 0.245s, episode steps:  20, steps per second:  82, episode reward: 44.189, mean reward:  2.209 [-2.142, 32.380], mean action: 2.250 [1.000, 6.000],  loss: 0.017104, mae: 0.280274, mean_q: 0.407825, mean_eps: 0.000000
 17175/30000: episode: 495, duration: 0.714s, episode steps:  33, steps per second:  46, episode reward: 36.000, mean reward:  1.091 [-3.000, 32.810], mean action: 3.121 [0.000, 16.000],  loss: 0.021437, mae: 0.296956, mean_q: 0.433864, mean_eps: 0.000000
 17198/30000: episode: 496, duration: 0.433s, episode steps:  23, steps per second:  53, episode reward: 44.280, mean reward:  1.925 [-2.159, 31.920], mean action: 2.913 [0.000, 7.000],  loss: 0.019219, mae: 0.286077, mean_q: 0.434116, mean_eps: 0.000000
 17222/30000: episode: 497, duration: 0.269s, episode steps:  24, steps per second:  89, episode reward: 38.550, mean reward:  1.606 [-2.656, 32.020], mean action: 1.042 [0.000, 4.000],  loss: 0.020628, mae: 0.286180, mean_q: 0.443769, mean_eps: 0.000000
 17348/30000: episode: 498, duration: 1.666s, episode steps: 126, steps per second:  76, episode reward: -32.170, mean reward: -0.255 [-31.711,  3.059], mean action: 5.262 [0.000, 17.000],  loss: 0.018248, mae: 0.284317, mean_q: 0.399435, mean_eps: 0.000000
 17382/30000: episode: 499, duration: 0.385s, episode steps:  34, steps per second:  88, episode reward: 35.513, mean reward:  1.044 [-3.000, 32.223], mean action: 3.706 [0.000, 16.000],  loss: 0.015836, mae: 0.271067, mean_q: 0.388895, mean_eps: 0.000000
 17438/30000: episode: 500, duration: 0.573s, episode steps:  56, steps per second:  98, episode reward: 39.000, mean reward:  0.696 [-2.351, 32.110], mean action: 1.554 [0.000, 12.000],  loss: 0.016493, mae: 0.275030, mean_q: 0.425761, mean_eps: 0.000000
 17470/30000: episode: 501, duration: 0.333s, episode steps:  32, steps per second:  96, episode reward: 39.000, mean reward:  1.219 [-2.919, 32.300], mean action: 1.719 [0.000, 7.000],  loss: 0.017355, mae: 0.279165, mean_q: 0.419543, mean_eps: 0.000000
 17502/30000: episode: 502, duration: 0.342s, episode steps:  32, steps per second:  94, episode reward: 38.137, mean reward:  1.192 [-2.481, 34.430], mean action: 1.781 [1.000, 4.000],  loss: 0.014699, mae: 0.274510, mean_q: 0.388728, mean_eps: 0.000000
 17531/30000: episode: 503, duration: 0.299s, episode steps:  29, steps per second:  97, episode reward: 38.636, mean reward:  1.332 [-2.610, 32.485], mean action: 1.276 [0.000, 6.000],  loss: 0.017800, mae: 0.279078, mean_q: 0.424222, mean_eps: 0.000000
 17560/30000: episode: 504, duration: 0.312s, episode steps:  29, steps per second:  93, episode reward: 38.439, mean reward:  1.325 [-2.415, 32.120], mean action: 2.828 [0.000, 6.000],  loss: 0.017375, mae: 0.278051, mean_q: 0.424244, mean_eps: 0.000000
 17591/30000: episode: 505, duration: 0.327s, episode steps:  31, steps per second:  95, episode reward: 42.000, mean reward:  1.355 [-2.270, 32.870], mean action: 2.258 [1.000, 8.000],  loss: 0.014738, mae: 0.263441, mean_q: 0.378385, mean_eps: 0.000000
 17622/30000: episode: 506, duration: 0.325s, episode steps:  31, steps per second:  95, episode reward: 41.053, mean reward:  1.324 [-2.711, 31.223], mean action: 1.065 [0.000, 6.000],  loss: 0.018933, mae: 0.282541, mean_q: 0.430139, mean_eps: 0.000000
 17645/30000: episode: 507, duration: 0.240s, episode steps:  23, steps per second:  96, episode reward: 36.000, mean reward:  1.565 [-3.000, 32.340], mean action: 2.435 [0.000, 6.000],  loss: 0.013684, mae: 0.255276, mean_q: 0.417956, mean_eps: 0.000000
 17670/30000: episode: 508, duration: 0.266s, episode steps:  25, steps per second:  94, episode reward: 43.983, mean reward:  1.759 [-2.343, 32.370], mean action: 2.120 [0.000, 6.000],  loss: 0.018355, mae: 0.293147, mean_q: 0.361849, mean_eps: 0.000000
 17691/30000: episode: 509, duration: 0.229s, episode steps:  21, steps per second:  92, episode reward: 41.369, mean reward:  1.970 [-2.900, 32.064], mean action: 1.238 [0.000, 4.000],  loss: 0.014724, mae: 0.276163, mean_q: 0.398583, mean_eps: 0.000000
 17718/30000: episode: 510, duration: 0.281s, episode steps:  27, steps per second:  96, episode reward: 33.000, mean reward:  1.222 [-2.635, 32.230], mean action: 3.111 [0.000, 16.000],  loss: 0.016577, mae: 0.269098, mean_q: 0.443648, mean_eps: 0.000000
 17762/30000: episode: 511, duration: 1.064s, episode steps:  44, steps per second:  41, episode reward: -32.680, mean reward: -0.743 [-32.208,  2.500], mean action: 4.568 [0.000, 17.000],  loss: 0.018470, mae: 0.279746, mean_q: 0.426356, mean_eps: 0.000000
 17788/30000: episode: 512, duration: 0.659s, episode steps:  26, steps per second:  39, episode reward: 41.591, mean reward:  1.600 [-2.753, 32.330], mean action: 2.962 [0.000, 15.000],  loss: 0.018699, mae: 0.271750, mean_q: 0.439766, mean_eps: 0.000000
 17819/30000: episode: 513, duration: 0.369s, episode steps:  31, steps per second:  84, episode reward: 38.579, mean reward:  1.244 [-2.568, 31.659], mean action: 2.000 [0.000, 15.000],  loss: 0.015875, mae: 0.260627, mean_q: 0.388341, mean_eps: 0.000000
 17845/30000: episode: 514, duration: 0.282s, episode steps:  26, steps per second:  92, episode reward: 44.458, mean reward:  1.710 [-2.280, 32.500], mean action: 2.808 [0.000, 12.000],  loss: 0.016212, mae: 0.257934, mean_q: 0.424889, mean_eps: 0.000000
 17901/30000: episode: 515, duration: 0.952s, episode steps:  56, steps per second:  59, episode reward: 32.419, mean reward:  0.579 [-2.381, 32.040], mean action: 4.429 [0.000, 16.000],  loss: 0.016156, mae: 0.260336, mean_q: 0.402699, mean_eps: 0.000000
 17925/30000: episode: 516, duration: 0.282s, episode steps:  24, steps per second:  85, episode reward: 43.435, mean reward:  1.810 [-2.498, 31.702], mean action: 1.917 [0.000, 6.000],  loss: 0.017478, mae: 0.281460, mean_q: 0.344734, mean_eps: 0.000000
 17952/30000: episode: 517, duration: 0.409s, episode steps:  27, steps per second:  66, episode reward: 35.868, mean reward:  1.328 [-2.455, 31.898], mean action: 2.926 [0.000, 17.000],  loss: 0.016118, mae: 0.271859, mean_q: 0.356781, mean_eps: 0.000000
 17984/30000: episode: 518, duration: 0.366s, episode steps:  32, steps per second:  87, episode reward: 38.626, mean reward:  1.207 [-3.000, 32.426], mean action: 2.156 [0.000, 10.000],  loss: 0.016798, mae: 0.274044, mean_q: 0.418030, mean_eps: 0.000000
 18009/30000: episode: 519, duration: 0.315s, episode steps:  25, steps per second:  79, episode reward: 42.000, mean reward:  1.680 [-2.348, 32.640], mean action: 1.520 [0.000, 4.000],  loss: 0.016753, mae: 0.281922, mean_q: 0.394066, mean_eps: 0.000000
 18037/30000: episode: 520, duration: 0.293s, episode steps:  28, steps per second:  96, episode reward: 39.000, mean reward:  1.393 [-2.391, 32.010], mean action: 3.357 [0.000, 15.000],  loss: 0.015036, mae: 0.269093, mean_q: 0.401160, mean_eps: 0.000000
 18072/30000: episode: 521, duration: 0.532s, episode steps:  35, steps per second:  66, episode reward: 44.422, mean reward:  1.269 [-2.289, 32.030], mean action: 2.857 [1.000, 12.000],  loss: 0.016245, mae: 0.285463, mean_q: 0.352006, mean_eps: 0.000000
 18093/30000: episode: 522, duration: 0.268s, episode steps:  21, steps per second:  78, episode reward: 42.000, mean reward:  2.000 [-2.680, 32.270], mean action: 2.381 [0.000, 12.000],  loss: 0.018273, mae: 0.293980, mean_q: 0.354991, mean_eps: 0.000000
 18126/30000: episode: 523, duration: 0.545s, episode steps:  33, steps per second:  61, episode reward: 41.854, mean reward:  1.268 [-2.175, 32.055], mean action: 1.636 [0.000, 18.000],  loss: 0.013175, mae: 0.262816, mean_q: 0.364116, mean_eps: 0.000000
 18151/30000: episode: 524, duration: 0.450s, episode steps:  25, steps per second:  56, episode reward: 33.000, mean reward:  1.320 [-3.000, 32.390], mean action: 2.080 [0.000, 3.000],  loss: 0.016724, mae: 0.278430, mean_q: 0.380235, mean_eps: 0.000000
 18175/30000: episode: 525, duration: 0.785s, episode steps:  24, steps per second:  31, episode reward: 44.092, mean reward:  1.837 [-2.030, 32.010], mean action: 2.250 [0.000, 6.000],  loss: 0.020989, mae: 0.301418, mean_q: 0.354813, mean_eps: 0.000000
 18207/30000: episode: 526, duration: 0.547s, episode steps:  32, steps per second:  59, episode reward: 39.000, mean reward:  1.219 [-2.146, 32.020], mean action: 2.688 [0.000, 15.000],  loss: 0.019200, mae: 0.281336, mean_q: 0.384333, mean_eps: 0.000000
 18261/30000: episode: 527, duration: 1.116s, episode steps:  54, steps per second:  48, episode reward: 32.399, mean reward:  0.600 [-3.000, 32.080], mean action: 2.019 [0.000, 15.000],  loss: 0.017514, mae: 0.294857, mean_q: 0.332347, mean_eps: 0.000000
 18297/30000: episode: 528, duration: 0.509s, episode steps:  36, steps per second:  71, episode reward: 39.000, mean reward:  1.083 [-2.489, 32.300], mean action: 3.333 [0.000, 17.000],  loss: 0.017823, mae: 0.275851, mean_q: 0.442915, mean_eps: 0.000000
 18350/30000: episode: 529, duration: 0.533s, episode steps:  53, steps per second:  99, episode reward: 42.000, mean reward:  0.792 [-2.219, 33.000], mean action: 0.962 [0.000, 3.000],  loss: 0.017176, mae: 0.285735, mean_q: 0.348147, mean_eps: 0.000000
 18373/30000: episode: 530, duration: 0.247s, episode steps:  23, steps per second:  93, episode reward: 43.969, mean reward:  1.912 [-2.530, 32.030], mean action: 3.304 [0.000, 15.000],  loss: 0.017644, mae: 0.268308, mean_q: 0.396365, mean_eps: 0.000000
 18404/30000: episode: 531, duration: 0.330s, episode steps:  31, steps per second:  94, episode reward: 38.575, mean reward:  1.244 [-2.241, 32.130], mean action: 3.516 [0.000, 15.000],  loss: 0.013938, mae: 0.255170, mean_q: 0.388994, mean_eps: 0.000000
 18430/30000: episode: 532, duration: 0.287s, episode steps:  26, steps per second:  91, episode reward: 38.671, mean reward:  1.487 [-2.181, 32.030], mean action: 1.308 [0.000, 4.000],  loss: 0.020370, mae: 0.290048, mean_q: 0.386171, mean_eps: 0.000000
 18449/30000: episode: 533, duration: 0.208s, episode steps:  19, steps per second:  91, episode reward: 38.852, mean reward:  2.045 [-2.452, 32.142], mean action: 2.263 [0.000, 12.000],  loss: 0.020743, mae: 0.296912, mean_q: 0.373234, mean_eps: 0.000000
 18475/30000: episode: 534, duration: 0.280s, episode steps:  26, steps per second:  93, episode reward: 39.000, mean reward:  1.500 [-2.928, 32.250], mean action: 1.846 [0.000, 12.000],  loss: 0.017276, mae: 0.269471, mean_q: 0.421397, mean_eps: 0.000000
 18501/30000: episode: 535, duration: 0.278s, episode steps:  26, steps per second:  94, episode reward: 38.348, mean reward:  1.475 [-3.000, 31.498], mean action: 2.538 [0.000, 6.000],  loss: 0.018183, mae: 0.283254, mean_q: 0.391274, mean_eps: 0.000000
 18523/30000: episode: 536, duration: 0.237s, episode steps:  22, steps per second:  93, episode reward: 38.706, mean reward:  1.759 [-3.000, 32.357], mean action: 2.909 [0.000, 16.000],  loss: 0.018047, mae: 0.285441, mean_q: 0.375066, mean_eps: 0.000000
 18554/30000: episode: 537, duration: 0.318s, episode steps:  31, steps per second:  97, episode reward: 40.335, mean reward:  1.301 [-2.729, 32.080], mean action: 4.677 [0.000, 16.000],  loss: 0.020862, mae: 0.287772, mean_q: 0.458636, mean_eps: 0.000000
 18586/30000: episode: 538, duration: 0.343s, episode steps:  32, steps per second:  93, episode reward: 39.000, mean reward:  1.219 [-2.346, 32.440], mean action: 3.375 [0.000, 16.000],  loss: 0.018420, mae: 0.284141, mean_q: 0.389136, mean_eps: 0.000000
 18606/30000: episode: 539, duration: 0.210s, episode steps:  20, steps per second:  95, episode reward: 41.765, mean reward:  2.088 [-2.211, 32.070], mean action: 2.000 [0.000, 16.000],  loss: 0.017935, mae: 0.274130, mean_q: 0.392574, mean_eps: 0.000000
 18626/30000: episode: 540, duration: 0.217s, episode steps:  20, steps per second:  92, episode reward: 46.204, mean reward:  2.310 [-0.402, 32.340], mean action: 3.000 [0.000, 6.000],  loss: 0.013824, mae: 0.255595, mean_q: 0.429814, mean_eps: 0.000000
 18659/30000: episode: 541, duration: 0.337s, episode steps:  33, steps per second:  98, episode reward: 32.510, mean reward:  0.985 [-3.000, 31.964], mean action: 3.061 [0.000, 16.000],  loss: 0.016168, mae: 0.268554, mean_q: 0.433044, mean_eps: 0.000000
 18685/30000: episode: 542, duration: 0.293s, episode steps:  26, steps per second:  89, episode reward: 38.034, mean reward:  1.463 [-3.000, 32.040], mean action: 3.231 [0.000, 16.000],  loss: 0.015494, mae: 0.272025, mean_q: 0.375321, mean_eps: 0.000000
 18720/30000: episode: 543, duration: 0.354s, episode steps:  35, steps per second:  99, episode reward: 38.008, mean reward:  1.086 [-2.154, 32.382], mean action: 2.143 [0.000, 12.000],  loss: 0.019276, mae: 0.288817, mean_q: 0.420134, mean_eps: 0.000000
 18759/30000: episode: 544, duration: 0.418s, episode steps:  39, steps per second:  93, episode reward: 38.119, mean reward:  0.977 [-2.116, 32.070], mean action: 2.179 [0.000, 6.000],  loss: 0.016670, mae: 0.271729, mean_q: 0.400654, mean_eps: 0.000000
 18783/30000: episode: 545, duration: 0.255s, episode steps:  24, steps per second:  94, episode reward: 38.309, mean reward:  1.596 [-2.262, 31.589], mean action: 3.792 [1.000, 16.000],  loss: 0.019064, mae: 0.284381, mean_q: 0.411208, mean_eps: 0.000000
 18812/30000: episode: 546, duration: 0.304s, episode steps:  29, steps per second:  95, episode reward: 35.614, mean reward:  1.228 [-2.909, 32.260], mean action: 3.690 [0.000, 18.000],  loss: 0.015703, mae: 0.276139, mean_q: 0.400005, mean_eps: 0.000000
 18839/30000: episode: 547, duration: 0.286s, episode steps:  27, steps per second:  95, episode reward: 44.306, mean reward:  1.641 [-2.384, 31.868], mean action: 2.185 [1.000, 3.000],  loss: 0.018118, mae: 0.289689, mean_q: 0.440930, mean_eps: 0.000000
 18868/30000: episode: 548, duration: 0.296s, episode steps:  29, steps per second:  98, episode reward: 32.548, mean reward:  1.122 [-3.000, 31.628], mean action: 2.241 [0.000, 16.000],  loss: 0.016196, mae: 0.288691, mean_q: 0.376225, mean_eps: 0.000000
 18889/30000: episode: 549, duration: 0.238s, episode steps:  21, steps per second:  88, episode reward: 41.289, mean reward:  1.966 [-3.000, 32.301], mean action: 3.286 [0.000, 15.000],  loss: 0.019837, mae: 0.311207, mean_q: 0.352453, mean_eps: 0.000000
 18932/30000: episode: 550, duration: 0.598s, episode steps:  43, steps per second:  72, episode reward: 37.257, mean reward:  0.866 [-3.000, 31.943], mean action: 3.000 [0.000, 16.000],  loss: 0.019406, mae: 0.300236, mean_q: 0.382896, mean_eps: 0.000000
 18966/30000: episode: 551, duration: 0.483s, episode steps:  34, steps per second:  70, episode reward: 41.027, mean reward:  1.207 [-2.569, 31.914], mean action: 2.324 [0.000, 6.000],  loss: 0.018256, mae: 0.299547, mean_q: 0.434349, mean_eps: 0.000000
 18988/30000: episode: 552, duration: 0.247s, episode steps:  22, steps per second:  89, episode reward: 44.658, mean reward:  2.030 [-2.228, 31.845], mean action: 2.591 [0.000, 4.000],  loss: 0.018047, mae: 0.297285, mean_q: 0.393151, mean_eps: 0.000000
 19007/30000: episode: 553, duration: 0.207s, episode steps:  19, steps per second:  92, episode reward: 42.000, mean reward:  2.211 [-2.316, 32.010], mean action: 3.000 [1.000, 4.000],  loss: 0.014719, mae: 0.275430, mean_q: 0.365609, mean_eps: 0.000000
 19040/30000: episode: 554, duration: 0.830s, episode steps:  33, steps per second:  40, episode reward: 32.748, mean reward:  0.992 [-3.000, 32.550], mean action: 2.182 [0.000, 12.000],  loss: 0.019166, mae: 0.300266, mean_q: 0.376336, mean_eps: 0.000000
 19060/30000: episode: 555, duration: 0.431s, episode steps:  20, steps per second:  46, episode reward: 41.278, mean reward:  2.064 [-2.886, 32.250], mean action: 4.000 [0.000, 12.000],  loss: 0.023387, mae: 0.313501, mean_q: 0.430787, mean_eps: 0.000000
 19080/30000: episode: 556, duration: 0.297s, episode steps:  20, steps per second:  67, episode reward: 44.106, mean reward:  2.205 [-2.693, 32.100], mean action: 1.250 [0.000, 6.000],  loss: 0.019606, mae: 0.294571, mean_q: 0.447358, mean_eps: 0.000000
 19096/30000: episode: 557, duration: 0.569s, episode steps:  16, steps per second:  28, episode reward: 42.998, mean reward:  2.687 [-2.806, 32.743], mean action: 6.125 [0.000, 15.000],  loss: 0.014693, mae: 0.275305, mean_q: 0.378189, mean_eps: 0.000000
 19122/30000: episode: 558, duration: 0.851s, episode steps:  26, steps per second:  31, episode reward: 44.004, mean reward:  1.692 [-2.514, 32.473], mean action: 2.538 [1.000, 3.000],  loss: 0.016214, mae: 0.274718, mean_q: 0.383148, mean_eps: 0.000000
 19146/30000: episode: 559, duration: 0.313s, episode steps:  24, steps per second:  77, episode reward: 32.409, mean reward:  1.350 [-3.000, 31.749], mean action: 1.500 [0.000, 6.000],  loss: 0.017404, mae: 0.279445, mean_q: 0.442636, mean_eps: 0.000000
 19162/30000: episode: 560, duration: 0.233s, episode steps:  16, steps per second:  69, episode reward: 44.492, mean reward:  2.781 [-2.916, 32.050], mean action: 1.500 [0.000, 6.000],  loss: 0.017572, mae: 0.282427, mean_q: 0.437145, mean_eps: 0.000000
 19184/30000: episode: 561, duration: 0.319s, episode steps:  22, steps per second:  69, episode reward: 39.000, mean reward:  1.773 [-2.098, 29.946], mean action: 1.364 [0.000, 6.000],  loss: 0.015180, mae: 0.281232, mean_q: 0.388834, mean_eps: 0.000000
 19238/30000: episode: 562, duration: 0.663s, episode steps:  54, steps per second:  82, episode reward: 36.000, mean reward:  0.667 [-2.790, 32.070], mean action: 2.019 [0.000, 15.000],  loss: 0.019743, mae: 0.302687, mean_q: 0.398891, mean_eps: 0.000000
 19276/30000: episode: 563, duration: 0.575s, episode steps:  38, steps per second:  66, episode reward: 35.000, mean reward:  0.921 [-2.569, 31.275], mean action: 4.053 [0.000, 15.000],  loss: 0.016751, mae: 0.294552, mean_q: 0.362488, mean_eps: 0.000000
 19298/30000: episode: 564, duration: 0.361s, episode steps:  22, steps per second:  61, episode reward: 44.256, mean reward:  2.012 [-2.479, 32.178], mean action: 2.455 [0.000, 16.000],  loss: 0.019247, mae: 0.292145, mean_q: 0.424860, mean_eps: 0.000000
 19318/30000: episode: 565, duration: 0.374s, episode steps:  20, steps per second:  53, episode reward: 38.475, mean reward:  1.924 [-3.000, 31.883], mean action: 6.500 [0.000, 16.000],  loss: 0.020603, mae: 0.290576, mean_q: 0.489929, mean_eps: 0.000000
 19384/30000: episode: 566, duration: 1.133s, episode steps:  66, steps per second:  58, episode reward: -34.710, mean reward: -0.526 [-32.373,  2.964], mean action: 2.697 [0.000, 15.000],  loss: 0.017825, mae: 0.297674, mean_q: 0.385412, mean_eps: 0.000000
 19419/30000: episode: 567, duration: 1.044s, episode steps:  35, steps per second:  34, episode reward: 38.180, mean reward:  1.091 [-2.483, 31.862], mean action: 2.314 [0.000, 12.000],  loss: 0.018419, mae: 0.293212, mean_q: 0.397370, mean_eps: 0.000000
 19458/30000: episode: 568, duration: 0.947s, episode steps:  39, steps per second:  41, episode reward: 37.447, mean reward:  0.960 [-3.000, 32.142], mean action: 0.846 [0.000, 6.000],  loss: 0.016816, mae: 0.289749, mean_q: 0.368576, mean_eps: 0.000000
 19489/30000: episode: 569, duration: 0.662s, episode steps:  31, steps per second:  47, episode reward: 45.735, mean reward:  1.475 [-0.363, 32.480], mean action: 2.903 [0.000, 6.000],  loss: 0.017364, mae: 0.286196, mean_q: 0.357487, mean_eps: 0.000000
 19522/30000: episode: 570, duration: 0.652s, episode steps:  33, steps per second:  51, episode reward: 32.930, mean reward:  0.998 [-3.000, 34.370], mean action: 6.212 [0.000, 17.000],  loss: 0.015742, mae: 0.273178, mean_q: 0.397347, mean_eps: 0.000000
 19541/30000: episode: 571, duration: 0.381s, episode steps:  19, steps per second:  50, episode reward: 44.393, mean reward:  2.336 [-2.081, 32.370], mean action: 3.211 [1.000, 11.000],  loss: 0.013213, mae: 0.257952, mean_q: 0.422950, mean_eps: 0.000000
 19563/30000: episode: 572, duration: 0.816s, episode steps:  22, steps per second:  27, episode reward: 44.939, mean reward:  2.043 [-2.025, 33.000], mean action: 1.091 [0.000, 10.000],  loss: 0.020141, mae: 0.299914, mean_q: 0.382246, mean_eps: 0.000000
 19583/30000: episode: 573, duration: 0.419s, episode steps:  20, steps per second:  48, episode reward: 42.000, mean reward:  2.100 [-2.743, 30.341], mean action: 2.000 [0.000, 12.000],  loss: 0.014634, mae: 0.292325, mean_q: 0.329803, mean_eps: 0.000000
 19605/30000: episode: 574, duration: 0.366s, episode steps:  22, steps per second:  60, episode reward: 46.688, mean reward:  2.122 [-0.030, 31.606], mean action: 0.409 [0.000, 6.000],  loss: 0.020454, mae: 0.311656, mean_q: 0.389547, mean_eps: 0.000000
 19629/30000: episode: 575, duration: 0.447s, episode steps:  24, steps per second:  54, episode reward: 44.540, mean reward:  1.856 [-3.000, 32.030], mean action: 1.917 [0.000, 16.000],  loss: 0.020093, mae: 0.311150, mean_q: 0.405929, mean_eps: 0.000000
 19662/30000: episode: 576, duration: 0.538s, episode steps:  33, steps per second:  61, episode reward: 37.778, mean reward:  1.145 [-2.476, 32.180], mean action: 2.939 [0.000, 10.000],  loss: 0.017088, mae: 0.326679, mean_q: 0.345030, mean_eps: 0.000000
 19681/30000: episode: 577, duration: 0.313s, episode steps:  19, steps per second:  61, episode reward: 38.512, mean reward:  2.027 [-3.000, 32.140], mean action: 3.158 [1.000, 6.000],  loss: 0.015386, mae: 0.298686, mean_q: 0.422253, mean_eps: 0.000000
 19705/30000: episode: 578, duration: 0.498s, episode steps:  24, steps per second:  48, episode reward: 41.849, mean reward:  1.744 [-2.210, 32.340], mean action: 2.250 [0.000, 16.000],  loss: 0.015856, mae: 0.295963, mean_q: 0.442496, mean_eps: 0.000000
 19739/30000: episode: 579, duration: 0.518s, episode steps:  34, steps per second:  66, episode reward: 32.383, mean reward:  0.952 [-3.000, 32.050], mean action: 4.324 [0.000, 16.000],  loss: 0.015578, mae: 0.310704, mean_q: 0.366930, mean_eps: 0.000000
 19787/30000: episode: 580, duration: 1.345s, episode steps:  48, steps per second:  36, episode reward: 42.948, mean reward:  0.895 [-2.022, 32.190], mean action: 1.646 [0.000, 16.000],  loss: 0.020383, mae: 0.317184, mean_q: 0.396747, mean_eps: 0.000000
 19812/30000: episode: 581, duration: 0.444s, episode steps:  25, steps per second:  56, episode reward: 38.256, mean reward:  1.530 [-2.269, 31.774], mean action: 3.600 [0.000, 12.000],  loss: 0.017093, mae: 0.295637, mean_q: 0.386960, mean_eps: 0.000000
 19841/30000: episode: 582, duration: 0.538s, episode steps:  29, steps per second:  54, episode reward: 34.925, mean reward:  1.204 [-3.000, 31.303], mean action: 3.897 [0.000, 15.000],  loss: 0.016695, mae: 0.287115, mean_q: 0.403546, mean_eps: 0.000000
 19861/30000: episode: 583, duration: 0.330s, episode steps:  20, steps per second:  61, episode reward: 43.898, mean reward:  2.195 [-2.247, 32.366], mean action: 2.300 [0.000, 15.000],  loss: 0.022268, mae: 0.310770, mean_q: 0.349400, mean_eps: 0.000000
 19873/30000: episode: 584, duration: 0.202s, episode steps:  12, steps per second:  60, episode reward: 44.004, mean reward:  3.667 [-2.028, 32.228], mean action: 1.833 [0.000, 16.000],  loss: 0.021904, mae: 0.298165, mean_q: 0.405484, mean_eps: 0.000000
 19901/30000: episode: 585, duration: 0.514s, episode steps:  28, steps per second:  54, episode reward: 41.747, mean reward:  1.491 [-2.626, 31.917], mean action: 4.250 [0.000, 15.000],  loss: 0.017140, mae: 0.292946, mean_q: 0.384836, mean_eps: 0.000000
 19925/30000: episode: 586, duration: 0.473s, episode steps:  24, steps per second:  51, episode reward: 41.237, mean reward:  1.718 [-2.196, 31.871], mean action: 2.083 [0.000, 16.000],  loss: 0.021199, mae: 0.315507, mean_q: 0.372412, mean_eps: 0.000000
 19940/30000: episode: 587, duration: 0.559s, episode steps:  15, steps per second:  27, episode reward: 47.361, mean reward:  3.157 [-0.017, 32.420], mean action: 1.400 [1.000, 3.000],  loss: 0.016674, mae: 0.290602, mean_q: 0.366047, mean_eps: 0.000000
 20016/30000: episode: 588, duration: 2.384s, episode steps:  76, steps per second:  32, episode reward: -35.660, mean reward: -0.469 [-32.066,  2.574], mean action: 9.592 [0.000, 21.000],  loss: 0.016478, mae: 0.286471, mean_q: 0.389819, mean_eps: 0.000000
 20046/30000: episode: 589, duration: 0.733s, episode steps:  30, steps per second:  41, episode reward: 38.592, mean reward:  1.286 [-2.815, 32.427], mean action: 1.500 [0.000, 6.000],  loss: 0.018193, mae: 0.290400, mean_q: 0.452636, mean_eps: 0.000000
 20078/30000: episode: 590, duration: 0.642s, episode steps:  32, steps per second:  50, episode reward: 39.000, mean reward:  1.219 [-2.313, 32.120], mean action: 1.188 [0.000, 6.000],  loss: 0.016193, mae: 0.287042, mean_q: 0.467966, mean_eps: 0.000000
 20122/30000: episode: 591, duration: 0.805s, episode steps:  44, steps per second:  55, episode reward: 36.000, mean reward:  0.818 [-2.294, 32.760], mean action: 1.705 [0.000, 3.000],  loss: 0.018106, mae: 0.305375, mean_q: 0.429366, mean_eps: 0.000000
 20159/30000: episode: 592, duration: 0.906s, episode steps:  37, steps per second:  41, episode reward: 32.663, mean reward:  0.883 [-2.532, 32.163], mean action: 3.216 [0.000, 16.000],  loss: 0.015678, mae: 0.293439, mean_q: 0.418172, mean_eps: 0.000000
 20205/30000: episode: 593, duration: 0.759s, episode steps:  46, steps per second:  61, episode reward: 33.000, mean reward:  0.717 [-3.000, 32.290], mean action: 3.000 [0.000, 18.000],  loss: 0.016487, mae: 0.294288, mean_q: 0.410691, mean_eps: 0.000000
 20232/30000: episode: 594, duration: 0.430s, episode steps:  27, steps per second:  63, episode reward: 39.000, mean reward:  1.444 [-3.000, 32.050], mean action: 3.741 [0.000, 13.000],  loss: 0.017966, mae: 0.307280, mean_q: 0.374925, mean_eps: 0.000000
 20277/30000: episode: 595, duration: 0.653s, episode steps:  45, steps per second:  69, episode reward: -32.230, mean reward: -0.716 [-32.875,  2.470], mean action: 4.356 [0.000, 17.000],  loss: 0.016997, mae: 0.298020, mean_q: 0.370613, mean_eps: 0.000000
 20325/30000: episode: 596, duration: 0.649s, episode steps:  48, steps per second:  74, episode reward: 40.723, mean reward:  0.848 [-2.319, 32.100], mean action: 2.083 [0.000, 12.000],  loss: 0.016062, mae: 0.298596, mean_q: 0.387610, mean_eps: 0.000000
 20347/30000: episode: 597, duration: 0.295s, episode steps:  22, steps per second:  75, episode reward: 38.197, mean reward:  1.736 [-2.900, 32.150], mean action: 2.409 [0.000, 15.000],  loss: 0.014601, mae: 0.281876, mean_q: 0.444199, mean_eps: 0.000000
 20378/30000: episode: 598, duration: 0.380s, episode steps:  31, steps per second:  82, episode reward: 33.000, mean reward:  1.065 [-3.000, 32.030], mean action: 5.226 [0.000, 16.000],  loss: 0.014385, mae: 0.285195, mean_q: 0.430821, mean_eps: 0.000000
 20420/30000: episode: 599, duration: 1.281s, episode steps:  42, steps per second:  33, episode reward: 38.155, mean reward:  0.908 [-2.490, 32.080], mean action: 2.095 [0.000, 16.000],  loss: 0.023029, mae: 0.343335, mean_q: 0.373771, mean_eps: 0.000000
 20448/30000: episode: 600, duration: 0.478s, episode steps:  28, steps per second:  59, episode reward: 41.773, mean reward:  1.492 [-2.329, 32.100], mean action: 1.679 [0.000, 16.000],  loss: 0.021962, mae: 0.330606, mean_q: 0.388858, mean_eps: 0.000000
 20475/30000: episode: 601, duration: 0.732s, episode steps:  27, steps per second:  37, episode reward: 35.371, mean reward:  1.310 [-3.000, 31.451], mean action: 3.407 [0.000, 16.000],  loss: 0.020262, mae: 0.317530, mean_q: 0.369763, mean_eps: 0.000000
 20507/30000: episode: 602, duration: 1.082s, episode steps:  32, steps per second:  30, episode reward: 38.026, mean reward:  1.188 [-3.000, 32.079], mean action: 3.281 [0.000, 16.000],  loss: 0.017118, mae: 0.302725, mean_q: 0.392313, mean_eps: 0.000000
 20530/30000: episode: 603, duration: 0.332s, episode steps:  23, steps per second:  69, episode reward: 42.000, mean reward:  1.826 [-2.067, 30.315], mean action: 2.348 [0.000, 17.000],  loss: 0.018805, mae: 0.317644, mean_q: 0.341491, mean_eps: 0.000000
 20545/30000: episode: 604, duration: 0.216s, episode steps:  15, steps per second:  69, episode reward: 44.303, mean reward:  2.954 [-2.417, 32.206], mean action: 1.800 [0.000, 12.000],  loss: 0.020209, mae: 0.333189, mean_q: 0.321776, mean_eps: 0.000000
 20601/30000: episode: 605, duration: 1.051s, episode steps:  56, steps per second:  53, episode reward: 35.385, mean reward:  0.632 [-2.210, 32.200], mean action: 2.857 [0.000, 21.000],  loss: 0.017746, mae: 0.304902, mean_q: 0.388231, mean_eps: 0.000000
 20635/30000: episode: 606, duration: 0.549s, episode steps:  34, steps per second:  62, episode reward: 44.697, mean reward:  1.315 [-2.195, 32.320], mean action: 3.324 [0.000, 16.000],  loss: 0.018245, mae: 0.303673, mean_q: 0.404587, mean_eps: 0.000000
 20661/30000: episode: 607, duration: 0.376s, episode steps:  26, steps per second:  69, episode reward: 39.000, mean reward:  1.500 [-2.264, 32.130], mean action: 2.346 [0.000, 16.000],  loss: 0.017131, mae: 0.296221, mean_q: 0.414851, mean_eps: 0.000000
 20679/30000: episode: 608, duration: 0.241s, episode steps:  18, steps per second:  75, episode reward: 41.168, mean reward:  2.287 [-2.514, 31.400], mean action: 3.000 [0.000, 16.000],  loss: 0.014880, mae: 0.285066, mean_q: 0.398318, mean_eps: 0.000000
 20704/30000: episode: 609, duration: 0.319s, episode steps:  25, steps per second:  78, episode reward: 35.498, mean reward:  1.420 [-2.383, 29.200], mean action: 3.640 [0.000, 17.000],  loss: 0.015262, mae: 0.296201, mean_q: 0.363520, mean_eps: 0.000000
 20720/30000: episode: 610, duration: 0.207s, episode steps:  16, steps per second:  77, episode reward: 47.672, mean reward:  2.979 [-0.110, 32.590], mean action: 1.000 [1.000, 1.000],  loss: 0.013892, mae: 0.284980, mean_q: 0.376985, mean_eps: 0.000000
 20751/30000: episode: 611, duration: 0.421s, episode steps:  31, steps per second:  74, episode reward: 38.598, mean reward:  1.245 [-2.612, 31.655], mean action: 3.387 [0.000, 15.000],  loss: 0.020603, mae: 0.301556, mean_q: 0.451628, mean_eps: 0.000000
 20783/30000: episode: 612, duration: 0.608s, episode steps:  32, steps per second:  53, episode reward: 39.000, mean reward:  1.219 [-2.939, 32.180], mean action: 5.188 [0.000, 15.000],  loss: 0.016754, mae: 0.289852, mean_q: 0.414368, mean_eps: 0.000000
 20802/30000: episode: 613, duration: 0.269s, episode steps:  19, steps per second:  71, episode reward: 46.451, mean reward:  2.445 [-0.632, 32.261], mean action: 4.211 [0.000, 14.000],  loss: 0.017291, mae: 0.306793, mean_q: 0.358067, mean_eps: 0.000000
 20820/30000: episode: 614, duration: 0.274s, episode steps:  18, steps per second:  66, episode reward: 38.971, mean reward:  2.165 [-3.000, 32.041], mean action: 0.500 [0.000, 1.000],  loss: 0.014962, mae: 0.294854, mean_q: 0.359248, mean_eps: 0.000000
 20869/30000: episode: 615, duration: 0.663s, episode steps:  49, steps per second:  74, episode reward: 32.382, mean reward:  0.661 [-2.650, 32.123], mean action: 2.959 [0.000, 10.000],  loss: 0.016440, mae: 0.293316, mean_q: 0.392473, mean_eps: 0.000000
 20897/30000: episode: 616, duration: 0.378s, episode steps:  28, steps per second:  74, episode reward: 41.009, mean reward:  1.465 [-2.061, 31.895], mean action: 0.679 [0.000, 3.000],  loss: 0.013429, mae: 0.284352, mean_q: 0.395569, mean_eps: 0.000000
 20920/30000: episode: 617, duration: 0.508s, episode steps:  23, steps per second:  45, episode reward: 41.704, mean reward:  1.813 [-2.228, 32.365], mean action: 3.261 [0.000, 21.000],  loss: 0.018165, mae: 0.301765, mean_q: 0.385824, mean_eps: 0.000000
 20936/30000: episode: 618, duration: 0.205s, episode steps:  16, steps per second:  78, episode reward: 45.000, mean reward:  2.813 [-2.378, 32.450], mean action: 2.562 [0.000, 15.000],  loss: 0.012901, mae: 0.286694, mean_q: 0.367882, mean_eps: 0.000000
 20963/30000: episode: 619, duration: 0.318s, episode steps:  27, steps per second:  85, episode reward: 35.111, mean reward:  1.300 [-3.000, 31.896], mean action: 2.185 [1.000, 3.000],  loss: 0.016790, mae: 0.310372, mean_q: 0.354276, mean_eps: 0.000000
 20992/30000: episode: 620, duration: 0.361s, episode steps:  29, steps per second:  80, episode reward: 36.000, mean reward:  1.241 [-3.000, 32.220], mean action: 2.724 [0.000, 15.000],  loss: 0.015315, mae: 0.286391, mean_q: 0.404207, mean_eps: 0.000000
 21018/30000: episode: 621, duration: 0.320s, episode steps:  26, steps per second:  81, episode reward: 36.000, mean reward:  1.385 [-3.000, 32.090], mean action: 2.692 [0.000, 15.000],  loss: 0.022114, mae: 0.320203, mean_q: 0.364967, mean_eps: 0.000000
 21053/30000: episode: 622, duration: 0.441s, episode steps:  35, steps per second:  79, episode reward: 35.635, mean reward:  1.018 [-3.000, 32.100], mean action: 2.371 [0.000, 12.000],  loss: 0.020489, mae: 0.312465, mean_q: 0.408512, mean_eps: 0.000000
 21078/30000: episode: 623, duration: 0.278s, episode steps:  25, steps per second:  90, episode reward: 42.000, mean reward:  1.680 [-2.474, 33.000], mean action: 2.320 [0.000, 16.000],  loss: 0.015092, mae: 0.278551, mean_q: 0.445011, mean_eps: 0.000000
 21111/30000: episode: 624, duration: 0.390s, episode steps:  33, steps per second:  85, episode reward: 41.027, mean reward:  1.243 [-2.720, 32.400], mean action: 2.182 [0.000, 8.000],  loss: 0.021507, mae: 0.323530, mean_q: 0.393152, mean_eps: 0.000000
 21132/30000: episode: 625, duration: 0.253s, episode steps:  21, steps per second:  83, episode reward: 41.499, mean reward:  1.976 [-2.544, 32.354], mean action: 4.333 [0.000, 16.000],  loss: 0.016605, mae: 0.297631, mean_q: 0.418794, mean_eps: 0.000000
 21167/30000: episode: 626, duration: 0.510s, episode steps:  35, steps per second:  69, episode reward: 36.000, mean reward:  1.029 [-2.595, 32.270], mean action: 2.771 [0.000, 16.000],  loss: 0.021732, mae: 0.318927, mean_q: 0.408851, mean_eps: 0.000000
 21187/30000: episode: 627, duration: 0.246s, episode steps:  20, steps per second:  81, episode reward: 44.903, mean reward:  2.245 [-2.354, 32.003], mean action: 1.950 [1.000, 4.000],  loss: 0.019735, mae: 0.300316, mean_q: 0.438713, mean_eps: 0.000000
 21209/30000: episode: 628, duration: 0.281s, episode steps:  22, steps per second:  78, episode reward: 46.939, mean reward:  2.134 [ 0.000, 32.059], mean action: 2.227 [0.000, 4.000],  loss: 0.016809, mae: 0.298863, mean_q: 0.400344, mean_eps: 0.000000
 21226/30000: episode: 629, duration: 0.235s, episode steps:  17, steps per second:  72, episode reward: 41.459, mean reward:  2.439 [-2.194, 32.749], mean action: 0.941 [0.000, 4.000],  loss: 0.019298, mae: 0.303820, mean_q: 0.442116, mean_eps: 0.000000
 21247/30000: episode: 630, duration: 0.262s, episode steps:  21, steps per second:  80, episode reward: 41.367, mean reward:  1.970 [-3.000, 31.940], mean action: 3.000 [0.000, 12.000],  loss: 0.018418, mae: 0.300902, mean_q: 0.450000, mean_eps: 0.000000
 21271/30000: episode: 631, duration: 0.301s, episode steps:  24, steps per second:  80, episode reward: 41.365, mean reward:  1.724 [-2.498, 31.898], mean action: 1.667 [0.000, 12.000],  loss: 0.018406, mae: 0.302859, mean_q: 0.384023, mean_eps: 0.000000
 21290/30000: episode: 632, duration: 0.235s, episode steps:  19, steps per second:  81, episode reward: 41.879, mean reward:  2.204 [-2.498, 32.010], mean action: 4.105 [0.000, 15.000],  loss: 0.020301, mae: 0.315362, mean_q: 0.372425, mean_eps: 0.000000
 21311/30000: episode: 633, duration: 0.265s, episode steps:  21, steps per second:  79, episode reward: 38.315, mean reward:  1.825 [-3.000, 32.200], mean action: 3.476 [0.000, 15.000],  loss: 0.017759, mae: 0.290664, mean_q: 0.462046, mean_eps: 0.000000
 21335/30000: episode: 634, duration: 0.579s, episode steps:  24, steps per second:  41, episode reward: 35.973, mean reward:  1.499 [-3.000, 32.180], mean action: 3.167 [0.000, 16.000],  loss: 0.018465, mae: 0.296991, mean_q: 0.428211, mean_eps: 0.000000
 21368/30000: episode: 635, duration: 0.564s, episode steps:  33, steps per second:  59, episode reward: 38.103, mean reward:  1.155 [-2.398, 32.630], mean action: 2.879 [0.000, 12.000],  loss: 0.015881, mae: 0.289038, mean_q: 0.433056, mean_eps: 0.000000
 21395/30000: episode: 636, duration: 0.364s, episode steps:  27, steps per second:  74, episode reward: 47.210, mean reward:  1.749 [-0.298, 32.806], mean action: 5.074 [0.000, 15.000],  loss: 0.015620, mae: 0.289310, mean_q: 0.456599, mean_eps: 0.000000
 21438/30000: episode: 637, duration: 0.731s, episode steps:  43, steps per second:  59, episode reward: 32.625, mean reward:  0.759 [-3.000, 32.292], mean action: 8.930 [0.000, 17.000],  loss: 0.018316, mae: 0.303011, mean_q: 0.425929, mean_eps: 0.000000
 21494/30000: episode: 638, duration: 0.886s, episode steps:  56, steps per second:  63, episode reward: 35.802, mean reward:  0.639 [-2.411, 32.251], mean action: 1.821 [0.000, 16.000],  loss: 0.016638, mae: 0.295570, mean_q: 0.395549, mean_eps: 0.000000
 21512/30000: episode: 639, duration: 0.267s, episode steps:  18, steps per second:  67, episode reward: 41.616, mean reward:  2.312 [-2.599, 32.144], mean action: 1.889 [0.000, 6.000],  loss: 0.018495, mae: 0.308252, mean_q: 0.403631, mean_eps: 0.000000
 21541/30000: episode: 640, duration: 0.430s, episode steps:  29, steps per second:  67, episode reward: 39.000, mean reward:  1.345 [-2.626, 32.290], mean action: 1.483 [0.000, 3.000],  loss: 0.017984, mae: 0.296938, mean_q: 0.434128, mean_eps: 0.000000
 21610/30000: episode: 641, duration: 2.142s, episode steps:  69, steps per second:  32, episode reward: 32.567, mean reward:  0.472 [-2.460, 32.130], mean action: 1.420 [0.000, 16.000],  loss: 0.018194, mae: 0.303812, mean_q: 0.398633, mean_eps: 0.000000
 21627/30000: episode: 642, duration: 0.431s, episode steps:  17, steps per second:  39, episode reward: 47.173, mean reward:  2.775 [-0.455, 32.091], mean action: 0.529 [0.000, 1.000],  loss: 0.015494, mae: 0.283434, mean_q: 0.407365, mean_eps: 0.000000
 21667/30000: episode: 643, duration: 0.995s, episode steps:  40, steps per second:  40, episode reward: 41.637, mean reward:  1.041 [-2.423, 31.727], mean action: 1.300 [0.000, 6.000],  loss: 0.015927, mae: 0.293068, mean_q: 0.379792, mean_eps: 0.000000
 21685/30000: episode: 644, duration: 0.657s, episode steps:  18, steps per second:  27, episode reward: 41.083, mean reward:  2.282 [-2.053, 32.127], mean action: 1.833 [0.000, 6.000],  loss: 0.018953, mae: 0.307943, mean_q: 0.376457, mean_eps: 0.000000
 21727/30000: episode: 645, duration: 0.488s, episode steps:  42, steps per second:  86, episode reward: 35.535, mean reward:  0.846 [-3.000, 31.695], mean action: 5.524 [3.000, 15.000],  loss: 0.018712, mae: 0.295153, mean_q: 0.445781, mean_eps: 0.000000
 21748/30000: episode: 646, duration: 0.264s, episode steps:  21, steps per second:  80, episode reward: 41.073, mean reward:  1.956 [-2.478, 31.393], mean action: 2.333 [1.000, 6.000],  loss: 0.020073, mae: 0.302793, mean_q: 0.428695, mean_eps: 0.000000
 21777/30000: episode: 647, duration: 0.346s, episode steps:  29, steps per second:  84, episode reward: 38.028, mean reward:  1.311 [-2.332, 32.310], mean action: 3.517 [0.000, 15.000],  loss: 0.016098, mae: 0.282767, mean_q: 0.455340, mean_eps: 0.000000
 21832/30000: episode: 648, duration: 1.254s, episode steps:  55, steps per second:  44, episode reward: -33.000, mean reward: -0.600 [-33.000,  3.000], mean action: 3.000 [0.000, 16.000],  loss: 0.018237, mae: 0.290188, mean_q: 0.454302, mean_eps: 0.000000
 21854/30000: episode: 649, duration: 0.420s, episode steps:  22, steps per second:  52, episode reward: 44.200, mean reward:  2.009 [-2.224, 32.620], mean action: 1.955 [0.000, 15.000],  loss: 0.018408, mae: 0.296629, mean_q: 0.399942, mean_eps: 0.000000
 21882/30000: episode: 650, duration: 0.800s, episode steps:  28, steps per second:  35, episode reward: 38.207, mean reward:  1.365 [-2.261, 32.150], mean action: 3.179 [0.000, 16.000],  loss: 0.017068, mae: 0.305973, mean_q: 0.379367, mean_eps: 0.000000
 21904/30000: episode: 651, duration: 0.322s, episode steps:  22, steps per second:  68, episode reward: 44.500, mean reward:  2.023 [-2.332, 32.020], mean action: 1.409 [0.000, 15.000],  loss: 0.017809, mae: 0.299290, mean_q: 0.382861, mean_eps: 0.000000
 21944/30000: episode: 652, duration: 0.482s, episode steps:  40, steps per second:  83, episode reward: 36.000, mean reward:  0.900 [-3.000, 32.310], mean action: 2.700 [0.000, 6.000],  loss: 0.016092, mae: 0.288312, mean_q: 0.407722, mean_eps: 0.000000
 21977/30000: episode: 653, duration: 0.407s, episode steps:  33, steps per second:  81, episode reward: 41.073, mean reward:  1.245 [-2.629, 31.701], mean action: 1.939 [0.000, 16.000],  loss: 0.016550, mae: 0.285059, mean_q: 0.417112, mean_eps: 0.000000
 22010/30000: episode: 654, duration: 0.409s, episode steps:  33, steps per second:  81, episode reward: 38.613, mean reward:  1.170 [-2.904, 31.663], mean action: 3.121 [0.000, 16.000],  loss: 0.021901, mae: 0.308357, mean_q: 0.443535, mean_eps: 0.000000
 22032/30000: episode: 655, duration: 0.264s, episode steps:  22, steps per second:  83, episode reward: 39.000, mean reward:  1.773 [-2.431, 29.080], mean action: 2.636 [0.000, 6.000],  loss: 0.016236, mae: 0.280651, mean_q: 0.389510, mean_eps: 0.000000
 22052/30000: episode: 656, duration: 0.267s, episode steps:  20, steps per second:  75, episode reward: 38.901, mean reward:  1.945 [-2.899, 32.661], mean action: 2.650 [0.000, 16.000],  loss: 0.016982, mae: 0.289712, mean_q: 0.380307, mean_eps: 0.000000
 22075/30000: episode: 657, duration: 0.343s, episode steps:  23, steps per second:  67, episode reward: 41.792, mean reward:  1.817 [-2.546, 31.832], mean action: 2.391 [0.000, 16.000],  loss: 0.018813, mae: 0.291591, mean_q: 0.403168, mean_eps: 0.000000
 22126/30000: episode: 658, duration: 1.146s, episode steps:  51, steps per second:  45, episode reward: 41.527, mean reward:  0.814 [-2.156, 32.360], mean action: 2.294 [0.000, 21.000],  loss: 0.018012, mae: 0.295214, mean_q: 0.385545, mean_eps: 0.000000
 22168/30000: episode: 659, duration: 0.679s, episode steps:  42, steps per second:  62, episode reward: 33.000, mean reward:  0.786 [-2.731, 29.946], mean action: 3.643 [0.000, 16.000],  loss: 0.014774, mae: 0.282292, mean_q: 0.426073, mean_eps: 0.000000
 22187/30000: episode: 660, duration: 0.544s, episode steps:  19, steps per second:  35, episode reward: 41.554, mean reward:  2.187 [-3.000, 32.075], mean action: 3.263 [0.000, 15.000],  loss: 0.018149, mae: 0.309552, mean_q: 0.309481, mean_eps: 0.000000
 22220/30000: episode: 661, duration: 0.679s, episode steps:  33, steps per second:  49, episode reward: 40.596, mean reward:  1.230 [-3.000, 32.694], mean action: 2.273 [0.000, 16.000],  loss: 0.015063, mae: 0.284451, mean_q: 0.414330, mean_eps: 0.000000
 22249/30000: episode: 662, duration: 0.385s, episode steps:  29, steps per second:  75, episode reward: 47.036, mean reward:  1.622 [-0.264, 32.121], mean action: 1.828 [0.000, 6.000],  loss: 0.023843, mae: 0.316356, mean_q: 0.420341, mean_eps: 0.000000
 22267/30000: episode: 663, duration: 0.608s, episode steps:  18, steps per second:  30, episode reward: 44.196, mean reward:  2.455 [-2.230, 32.480], mean action: 4.000 [0.000, 14.000],  loss: 0.013961, mae: 0.263855, mean_q: 0.423953, mean_eps: 0.000000
 22305/30000: episode: 664, duration: 1.274s, episode steps:  38, steps per second:  30, episode reward: 40.727, mean reward:  1.072 [-2.590, 32.030], mean action: 2.184 [0.000, 14.000],  loss: 0.021792, mae: 0.309318, mean_q: 0.405495, mean_eps: 0.000000
 22324/30000: episode: 665, duration: 0.227s, episode steps:  19, steps per second:  84, episode reward: 44.633, mean reward:  2.349 [-2.067, 32.697], mean action: 1.789 [0.000, 16.000],  loss: 0.016534, mae: 0.285621, mean_q: 0.485521, mean_eps: 0.000000
 22347/30000: episode: 666, duration: 0.310s, episode steps:  23, steps per second:  74, episode reward: 38.940, mean reward:  1.693 [-3.000, 32.390], mean action: 2.957 [0.000, 16.000],  loss: 0.016817, mae: 0.280679, mean_q: 0.516857, mean_eps: 0.000000
 22377/30000: episode: 667, duration: 0.334s, episode steps:  30, steps per second:  90, episode reward: 37.908, mean reward:  1.264 [-2.405, 32.390], mean action: 4.533 [0.000, 16.000],  loss: 0.020665, mae: 0.316526, mean_q: 0.353327, mean_eps: 0.000000
 22406/30000: episode: 668, duration: 0.596s, episode steps:  29, steps per second:  49, episode reward: 47.596, mean reward:  1.641 [-0.430, 32.120], mean action: 3.517 [1.000, 15.000],  loss: 0.017602, mae: 0.297321, mean_q: 0.375851, mean_eps: 0.000000
 22442/30000: episode: 669, duration: 0.378s, episode steps:  36, steps per second:  95, episode reward: 37.728, mean reward:  1.048 [-2.840, 32.071], mean action: 2.972 [0.000, 16.000],  loss: 0.017412, mae: 0.293319, mean_q: 0.396832, mean_eps: 0.000000
 22500/30000: episode: 670, duration: 0.637s, episode steps:  58, steps per second:  91, episode reward: 40.607, mean reward:  0.700 [-2.282, 32.084], mean action: 2.724 [0.000, 6.000],  loss: 0.017087, mae: 0.297678, mean_q: 0.405573, mean_eps: 0.000000
 22521/30000: episode: 671, duration: 0.340s, episode steps:  21, steps per second:  62, episode reward: 43.242, mean reward:  2.059 [-2.787, 32.038], mean action: 2.905 [0.000, 15.000],  loss: 0.019057, mae: 0.289610, mean_q: 0.463602, mean_eps: 0.000000
 22543/30000: episode: 672, duration: 0.320s, episode steps:  22, steps per second:  69, episode reward: 44.968, mean reward:  2.044 [-2.577, 32.180], mean action: 1.636 [0.000, 6.000],  loss: 0.020929, mae: 0.306804, mean_q: 0.427019, mean_eps: 0.000000
 22575/30000: episode: 673, duration: 0.370s, episode steps:  32, steps per second:  87, episode reward: 35.786, mean reward:  1.118 [-2.563, 31.865], mean action: 5.281 [0.000, 16.000],  loss: 0.015952, mae: 0.285058, mean_q: 0.453261, mean_eps: 0.000000
 22603/30000: episode: 674, duration: 0.310s, episode steps:  28, steps per second:  90, episode reward: -32.560, mean reward: -1.163 [-32.192,  2.952], mean action: 5.071 [0.000, 15.000],  loss: 0.015398, mae: 0.304361, mean_q: 0.347704, mean_eps: 0.000000
 22622/30000: episode: 675, duration: 0.234s, episode steps:  19, steps per second:  81, episode reward: 41.609, mean reward:  2.190 [-2.231, 31.859], mean action: 2.211 [0.000, 6.000],  loss: 0.021988, mae: 0.334057, mean_q: 0.389619, mean_eps: 0.000000
 22645/30000: episode: 676, duration: 0.266s, episode steps:  23, steps per second:  87, episode reward: 38.903, mean reward:  1.691 [-2.670, 32.083], mean action: 1.826 [0.000, 6.000],  loss: 0.019186, mae: 0.305566, mean_q: 0.429236, mean_eps: 0.000000
 22665/30000: episode: 677, duration: 0.245s, episode steps:  20, steps per second:  82, episode reward: 42.000, mean reward:  2.100 [-2.280, 32.260], mean action: 2.100 [0.000, 12.000],  loss: 0.018724, mae: 0.304460, mean_q: 0.413952, mean_eps: 0.000000
 22691/30000: episode: 678, duration: 0.286s, episode steps:  26, steps per second:  91, episode reward: 46.564, mean reward:  1.791 [-0.673, 34.271], mean action: 3.885 [0.000, 10.000],  loss: 0.017002, mae: 0.301803, mean_q: 0.385290, mean_eps: 0.000000
 22727/30000: episode: 679, duration: 0.398s, episode steps:  36, steps per second:  90, episode reward: 43.883, mean reward:  1.219 [-0.417, 30.996], mean action: 2.278 [0.000, 14.000],  loss: 0.017430, mae: 0.297303, mean_q: 0.427646, mean_eps: 0.000000
 22747/30000: episode: 680, duration: 0.236s, episode steps:  20, steps per second:  85, episode reward: 41.871, mean reward:  2.094 [-2.278, 32.321], mean action: 2.400 [0.000, 16.000],  loss: 0.017135, mae: 0.286270, mean_q: 0.468204, mean_eps: 0.000000
 22784/30000: episode: 681, duration: 0.412s, episode steps:  37, steps per second:  90, episode reward: 38.392, mean reward:  1.038 [-2.444, 31.826], mean action: 2.811 [0.000, 16.000],  loss: 0.018166, mae: 0.294616, mean_q: 0.454513, mean_eps: 0.000000
 22819/30000: episode: 682, duration: 0.383s, episode steps:  35, steps per second:  91, episode reward: 32.582, mean reward:  0.931 [-3.000, 31.872], mean action: 0.657 [0.000, 4.000],  loss: 0.020217, mae: 0.312161, mean_q: 0.400745, mean_eps: 0.000000
 22853/30000: episode: 683, duration: 0.376s, episode steps:  34, steps per second:  91, episode reward: 38.790, mean reward:  1.141 [-3.000, 32.010], mean action: 3.324 [0.000, 20.000],  loss: 0.015364, mae: 0.285120, mean_q: 0.375859, mean_eps: 0.000000
 22923/30000: episode: 684, duration: 0.820s, episode steps:  70, steps per second:  85, episode reward: 35.275, mean reward:  0.504 [-2.167, 31.941], mean action: 1.586 [0.000, 16.000],  loss: 0.018810, mae: 0.293881, mean_q: 0.438512, mean_eps: 0.000000
 22960/30000: episode: 685, duration: 0.402s, episode steps:  37, steps per second:  92, episode reward: 32.794, mean reward:  0.886 [-3.000, 32.140], mean action: 5.270 [0.000, 20.000],  loss: 0.016676, mae: 0.286621, mean_q: 0.412033, mean_eps: 0.000000
 23007/30000: episode: 686, duration: 0.499s, episode steps:  47, steps per second:  94, episode reward: 40.895, mean reward:  0.870 [-2.750, 32.180], mean action: 4.511 [0.000, 20.000],  loss: 0.015945, mae: 0.291130, mean_q: 0.389176, mean_eps: 0.000000
 23055/30000: episode: 687, duration: 0.553s, episode steps:  48, steps per second:  87, episode reward: 33.000, mean reward:  0.688 [-2.452, 32.470], mean action: 3.729 [0.000, 16.000],  loss: 0.019045, mae: 0.303078, mean_q: 0.468749, mean_eps: 0.000000
 23085/30000: episode: 688, duration: 0.333s, episode steps:  30, steps per second:  90, episode reward: 41.121, mean reward:  1.371 [-2.709, 31.709], mean action: 1.667 [0.000, 6.000],  loss: 0.019450, mae: 0.307283, mean_q: 0.424455, mean_eps: 0.000000
 23104/30000: episode: 689, duration: 0.215s, episode steps:  19, steps per second:  88, episode reward: 44.584, mean reward:  2.347 [-2.137, 32.150], mean action: 3.789 [0.000, 14.000],  loss: 0.020278, mae: 0.304825, mean_q: 0.512863, mean_eps: 0.000000
 23144/30000: episode: 690, duration: 0.436s, episode steps:  40, steps per second:  92, episode reward: 38.106, mean reward:  0.953 [-2.631, 32.092], mean action: 1.450 [0.000, 6.000],  loss: 0.017839, mae: 0.299302, mean_q: 0.459460, mean_eps: 0.000000
 23176/30000: episode: 691, duration: 0.357s, episode steps:  32, steps per second:  90, episode reward: 38.748, mean reward:  1.211 [-3.000, 32.160], mean action: 2.406 [0.000, 20.000],  loss: 0.014869, mae: 0.295931, mean_q: 0.393976, mean_eps: 0.000000
 23207/30000: episode: 692, duration: 0.353s, episode steps:  31, steps per second:  88, episode reward: 40.601, mean reward:  1.310 [-2.632, 32.470], mean action: 1.645 [0.000, 8.000],  loss: 0.013798, mae: 0.281056, mean_q: 0.400469, mean_eps: 0.000000
 23234/30000: episode: 693, duration: 0.299s, episode steps:  27, steps per second:  90, episode reward: 39.000, mean reward:  1.444 [-2.680, 32.680], mean action: 2.556 [0.000, 10.000],  loss: 0.015183, mae: 0.284303, mean_q: 0.482093, mean_eps: 0.000000
 23260/30000: episode: 694, duration: 0.287s, episode steps:  26, steps per second:  91, episode reward: 45.000, mean reward:  1.731 [-2.082, 32.210], mean action: 1.346 [1.000, 7.000],  loss: 0.017322, mae: 0.299079, mean_q: 0.367293, mean_eps: 0.000000
 23298/30000: episode: 695, duration: 0.420s, episode steps:  38, steps per second:  90, episode reward: 37.209, mean reward:  0.979 [-2.078, 31.952], mean action: 1.158 [0.000, 12.000],  loss: 0.016576, mae: 0.289676, mean_q: 0.409687, mean_eps: 0.000000
 23327/30000: episode: 696, duration: 0.462s, episode steps:  29, steps per second:  63, episode reward: 40.228, mean reward:  1.387 [-2.614, 32.120], mean action: 4.034 [0.000, 21.000],  loss: 0.017856, mae: 0.302949, mean_q: 0.443505, mean_eps: 0.000000
 23360/30000: episode: 697, duration: 0.368s, episode steps:  33, steps per second:  90, episode reward: 38.110, mean reward:  1.155 [-2.770, 32.300], mean action: 2.242 [0.000, 19.000],  loss: 0.015521, mae: 0.297566, mean_q: 0.393239, mean_eps: 0.000000
 23379/30000: episode: 698, duration: 0.212s, episode steps:  19, steps per second:  90, episode reward: 41.121, mean reward:  2.164 [-2.458, 32.130], mean action: 1.211 [0.000, 3.000],  loss: 0.016916, mae: 0.308479, mean_q: 0.399883, mean_eps: 0.000000
 23405/30000: episode: 699, duration: 0.289s, episode steps:  26, steps per second:  90, episode reward: 37.717, mean reward:  1.451 [-2.233, 32.280], mean action: 2.192 [0.000, 15.000],  loss: 0.016495, mae: 0.303832, mean_q: 0.383533, mean_eps: 0.000000
 23428/30000: episode: 700, duration: 0.278s, episode steps:  23, steps per second:  83, episode reward: 44.529, mean reward:  1.936 [-2.057, 32.520], mean action: 1.391 [0.000, 15.000],  loss: 0.018081, mae: 0.312292, mean_q: 0.377021, mean_eps: 0.000000
 23472/30000: episode: 701, duration: 0.479s, episode steps:  44, steps per second:  92, episode reward: 35.040, mean reward:  0.796 [-2.335, 32.240], mean action: 3.659 [0.000, 16.000],  loss: 0.017990, mae: 0.306361, mean_q: 0.410818, mean_eps: 0.000000
 23505/30000: episode: 702, duration: 0.379s, episode steps:  33, steps per second:  87, episode reward: 38.485, mean reward:  1.166 [-2.142, 32.083], mean action: 4.970 [0.000, 16.000],  loss: 0.019097, mae: 0.313906, mean_q: 0.368660, mean_eps: 0.000000
 23531/30000: episode: 703, duration: 0.450s, episode steps:  26, steps per second:  58, episode reward: 38.074, mean reward:  1.464 [-3.000, 32.930], mean action: 3.077 [0.000, 16.000],  loss: 0.016266, mae: 0.294906, mean_q: 0.428124, mean_eps: 0.000000
 23564/30000: episode: 704, duration: 0.366s, episode steps:  33, steps per second:  90, episode reward: 38.506, mean reward:  1.167 [-2.373, 32.101], mean action: 4.000 [0.000, 16.000],  loss: 0.017241, mae: 0.296021, mean_q: 0.435079, mean_eps: 0.000000
 23582/30000: episode: 705, duration: 0.218s, episode steps:  18, steps per second:  83, episode reward: 44.094, mean reward:  2.450 [-2.791, 32.313], mean action: 1.556 [0.000, 6.000],  loss: 0.019320, mae: 0.327377, mean_q: 0.368899, mean_eps: 0.000000
 23616/30000: episode: 706, duration: 0.374s, episode steps:  34, steps per second:  91, episode reward: 35.615, mean reward:  1.048 [-3.000, 32.197], mean action: 3.676 [0.000, 16.000],  loss: 0.017849, mae: 0.315471, mean_q: 0.385031, mean_eps: 0.000000
 23641/30000: episode: 707, duration: 0.284s, episode steps:  25, steps per second:  88, episode reward: 44.146, mean reward:  1.766 [-2.187, 32.025], mean action: 2.000 [0.000, 16.000],  loss: 0.019721, mae: 0.305886, mean_q: 0.433940, mean_eps: 0.000000
 23652/30000: episode: 708, duration: 0.132s, episode steps:  11, steps per second:  83, episode reward: 45.000, mean reward:  4.091 [-2.079, 32.430], mean action: 1.636 [0.000, 15.000],  loss: 0.018000, mae: 0.301868, mean_q: 0.409617, mean_eps: 0.000000
 23677/30000: episode: 709, duration: 0.286s, episode steps:  25, steps per second:  87, episode reward: 40.610, mean reward:  1.624 [-3.000, 32.221], mean action: 3.440 [0.000, 19.000],  loss: 0.013723, mae: 0.286516, mean_q: 0.373001, mean_eps: 0.000000
 23703/30000: episode: 710, duration: 0.287s, episode steps:  26, steps per second:  91, episode reward: 38.392, mean reward:  1.477 [-3.000, 32.701], mean action: 4.154 [0.000, 16.000],  loss: 0.019942, mae: 0.305742, mean_q: 0.406092, mean_eps: 0.000000
 23738/30000: episode: 711, duration: 0.387s, episode steps:  35, steps per second:  90, episode reward: 38.619, mean reward:  1.103 [-3.000, 32.080], mean action: 1.857 [0.000, 16.000],  loss: 0.016800, mae: 0.297144, mean_q: 0.422307, mean_eps: 0.000000
 23761/30000: episode: 712, duration: 0.256s, episode steps:  23, steps per second:  90, episode reward: 38.626, mean reward:  1.679 [-3.000, 31.886], mean action: 3.043 [0.000, 16.000],  loss: 0.018724, mae: 0.301658, mean_q: 0.416968, mean_eps: 0.000000
 23807/30000: episode: 713, duration: 0.494s, episode steps:  46, steps per second:  93, episode reward: 32.137, mean reward:  0.699 [-2.266, 33.000], mean action: 5.326 [0.000, 20.000],  loss: 0.017889, mae: 0.300250, mean_q: 0.443718, mean_eps: 0.000000
 23832/30000: episode: 714, duration: 0.273s, episode steps:  25, steps per second:  92, episode reward: 35.412, mean reward:  1.416 [-2.937, 31.502], mean action: 1.960 [0.000, 16.000],  loss: 0.020709, mae: 0.321276, mean_q: 0.396052, mean_eps: 0.000000
 23851/30000: episode: 715, duration: 0.224s, episode steps:  19, steps per second:  85, episode reward: 41.356, mean reward:  2.177 [-2.559, 32.017], mean action: 2.526 [0.000, 16.000],  loss: 0.021547, mae: 0.324757, mean_q: 0.384300, mean_eps: 0.000000
 23876/30000: episode: 716, duration: 0.275s, episode steps:  25, steps per second:  91, episode reward: 44.558, mean reward:  1.782 [-2.273, 32.470], mean action: 3.600 [3.000, 7.000],  loss: 0.019814, mae: 0.313785, mean_q: 0.415986, mean_eps: 0.000000
 23896/30000: episode: 717, duration: 0.231s, episode steps:  20, steps per second:  87, episode reward: 45.000, mean reward:  2.250 [-2.004, 32.740], mean action: 2.550 [1.000, 6.000],  loss: 0.015530, mae: 0.292028, mean_q: 0.436942, mean_eps: 0.000000
 23937/30000: episode: 718, duration: 0.455s, episode steps:  41, steps per second:  90, episode reward: 41.709, mean reward:  1.017 [-3.000, 32.143], mean action: 2.976 [0.000, 20.000],  loss: 0.017791, mae: 0.307251, mean_q: 0.438353, mean_eps: 0.000000
 23960/30000: episode: 719, duration: 0.270s, episode steps:  23, steps per second:  85, episode reward: 38.300, mean reward:  1.665 [-2.923, 31.824], mean action: 2.609 [1.000, 6.000],  loss: 0.016728, mae: 0.316977, mean_q: 0.385166, mean_eps: 0.000000
 23986/30000: episode: 720, duration: 0.286s, episode steps:  26, steps per second:  91, episode reward: 41.749, mean reward:  1.606 [-2.177, 33.000], mean action: 3.154 [0.000, 16.000],  loss: 0.020545, mae: 0.332815, mean_q: 0.399437, mean_eps: 0.000000
 24018/30000: episode: 721, duration: 0.348s, episode steps:  32, steps per second:  92, episode reward: 44.910, mean reward:  1.403 [-2.403, 31.940], mean action: 2.344 [1.000, 6.000],  loss: 0.016435, mae: 0.299726, mean_q: 0.423027, mean_eps: 0.000000
 24042/30000: episode: 722, duration: 0.265s, episode steps:  24, steps per second:  91, episode reward: 35.732, mean reward:  1.489 [-2.521, 31.762], mean action: 2.250 [0.000, 16.000],  loss: 0.019390, mae: 0.308499, mean_q: 0.459935, mean_eps: 0.000000
 24060/30000: episode: 723, duration: 0.205s, episode steps:  18, steps per second:  88, episode reward: 47.054, mean reward:  2.614 [ 0.000, 31.912], mean action: 0.000 [0.000, 0.000],  loss: 0.020108, mae: 0.314119, mean_q: 0.468376, mean_eps: 0.000000
 24093/30000: episode: 724, duration: 0.368s, episode steps:  33, steps per second:  90, episode reward: 43.990, mean reward:  1.333 [-2.975, 32.026], mean action: 5.061 [0.000, 21.000],  loss: 0.018002, mae: 0.297896, mean_q: 0.420396, mean_eps: 0.000000
 24124/30000: episode: 725, duration: 0.365s, episode steps:  31, steps per second:  85, episode reward: 35.466, mean reward:  1.144 [-2.398, 31.729], mean action: 4.516 [0.000, 16.000],  loss: 0.014621, mae: 0.287078, mean_q: 0.452575, mean_eps: 0.000000
 24134/30000: episode: 726, duration: 0.133s, episode steps:  10, steps per second:  75, episode reward: 47.422, mean reward:  4.742 [-0.058, 33.000], mean action: 2.200 [1.000, 3.000],  loss: 0.021975, mae: 0.334124, mean_q: 0.391556, mean_eps: 0.000000
 24158/30000: episode: 727, duration: 0.255s, episode steps:  24, steps per second:  94, episode reward: 39.000, mean reward:  1.625 [-2.633, 32.910], mean action: 2.542 [0.000, 16.000],  loss: 0.016369, mae: 0.316581, mean_q: 0.367447, mean_eps: 0.000000
 24195/30000: episode: 728, duration: 0.389s, episode steps:  37, steps per second:  95, episode reward: 41.663, mean reward:  1.126 [-2.273, 31.970], mean action: 2.243 [0.000, 16.000],  loss: 0.019838, mae: 0.325660, mean_q: 0.425933, mean_eps: 0.000000
 24221/30000: episode: 729, duration: 0.296s, episode steps:  26, steps per second:  88, episode reward: 38.392, mean reward:  1.477 [-2.347, 32.260], mean action: 2.423 [0.000, 12.000],  loss: 0.017215, mae: 0.316529, mean_q: 0.363563, mean_eps: 0.000000
 24249/30000: episode: 730, duration: 0.302s, episode steps:  28, steps per second:  93, episode reward: 35.276, mean reward:  1.260 [-3.000, 32.060], mean action: 4.107 [0.000, 16.000],  loss: 0.016329, mae: 0.300181, mean_q: 0.384776, mean_eps: 0.000000
 24271/30000: episode: 731, duration: 0.245s, episode steps:  22, steps per second:  90, episode reward: 44.615, mean reward:  2.028 [-2.235, 32.200], mean action: 2.773 [0.000, 15.000],  loss: 0.021752, mae: 0.322984, mean_q: 0.389739, mean_eps: 0.000000
 24299/30000: episode: 732, duration: 0.309s, episode steps:  28, steps per second:  91, episode reward: 40.616, mean reward:  1.451 [-3.000, 32.170], mean action: 3.857 [0.000, 15.000],  loss: 0.019294, mae: 0.301955, mean_q: 0.443908, mean_eps: 0.000000
 24329/30000: episode: 733, duration: 0.335s, episode steps:  30, steps per second:  90, episode reward: 38.195, mean reward:  1.273 [-2.490, 32.250], mean action: 5.467 [0.000, 16.000],  loss: 0.017431, mae: 0.312522, mean_q: 0.407903, mean_eps: 0.000000
 24353/30000: episode: 734, duration: 0.274s, episode steps:  24, steps per second:  88, episode reward: 39.000, mean reward:  1.625 [-3.000, 32.320], mean action: 3.250 [0.000, 16.000],  loss: 0.015939, mae: 0.313929, mean_q: 0.420257, mean_eps: 0.000000
 24391/30000: episode: 735, duration: 0.411s, episode steps:  38, steps per second:  93, episode reward: 41.442, mean reward:  1.091 [-2.325, 31.952], mean action: 1.368 [0.000, 3.000],  loss: 0.017745, mae: 0.331472, mean_q: 0.414011, mean_eps: 0.000000
 24417/30000: episode: 736, duration: 0.297s, episode steps:  26, steps per second:  88, episode reward: 38.281, mean reward:  1.472 [-2.556, 31.666], mean action: 1.154 [0.000, 6.000],  loss: 0.015447, mae: 0.329657, mean_q: 0.380631, mean_eps: 0.000000
 24461/30000: episode: 737, duration: 0.493s, episode steps:  44, steps per second:  89, episode reward: 38.215, mean reward:  0.869 [-2.682, 31.355], mean action: 1.386 [0.000, 6.000],  loss: 0.019283, mae: 0.324787, mean_q: 0.436978, mean_eps: 0.000000
 24486/30000: episode: 738, duration: 0.284s, episode steps:  25, steps per second:  88, episode reward: 41.621, mean reward:  1.665 [-2.112, 32.093], mean action: 1.760 [0.000, 6.000],  loss: 0.015853, mae: 0.294631, mean_q: 0.440661, mean_eps: 0.000000
 24559/30000: episode: 739, duration: 0.943s, episode steps:  73, steps per second:  77, episode reward: -32.320, mean reward: -0.443 [-32.126,  2.290], mean action: 3.123 [0.000, 15.000],  loss: 0.017364, mae: 0.315779, mean_q: 0.392394, mean_eps: 0.000000
 24586/30000: episode: 740, duration: 0.309s, episode steps:  27, steps per second:  87, episode reward: 45.000, mean reward:  1.667 [-2.418, 32.210], mean action: 2.481 [0.000, 7.000],  loss: 0.018039, mae: 0.328210, mean_q: 0.373831, mean_eps: 0.000000
 24619/30000: episode: 741, duration: 0.349s, episode steps:  33, steps per second:  95, episode reward: 41.107, mean reward:  1.246 [-2.434, 32.290], mean action: 1.394 [0.000, 4.000],  loss: 0.020897, mae: 0.335863, mean_q: 0.418136, mean_eps: 0.000000
 24658/30000: episode: 742, duration: 0.415s, episode steps:  39, steps per second:  94, episode reward: -35.120, mean reward: -0.901 [-32.219,  2.630], mean action: 6.846 [0.000, 17.000],  loss: 0.018300, mae: 0.320574, mean_q: 0.385744, mean_eps: 0.000000
 24684/30000: episode: 743, duration: 0.303s, episode steps:  26, steps per second:  86, episode reward: 41.250, mean reward:  1.587 [-2.293, 32.385], mean action: 1.885 [0.000, 16.000],  loss: 0.018031, mae: 0.312399, mean_q: 0.411978, mean_eps: 0.000000
 24699/30000: episode: 744, duration: 0.185s, episode steps:  15, steps per second:  81, episode reward: 47.903, mean reward:  3.194 [-0.007, 32.910], mean action: 2.000 [0.000, 15.000],  loss: 0.017090, mae: 0.306654, mean_q: 0.415916, mean_eps: 0.000000
 24727/30000: episode: 745, duration: 0.317s, episode steps:  28, steps per second:  88, episode reward: 41.332, mean reward:  1.476 [-2.162, 32.037], mean action: 2.857 [0.000, 10.000],  loss: 0.014544, mae: 0.299805, mean_q: 0.386181, mean_eps: 0.000000
 24758/30000: episode: 746, duration: 0.345s, episode steps:  31, steps per second:  90, episode reward: 35.901, mean reward:  1.158 [-2.595, 32.211], mean action: 2.742 [0.000, 16.000],  loss: 0.016314, mae: 0.304504, mean_q: 0.399645, mean_eps: 0.000000
 24778/30000: episode: 747, duration: 0.359s, episode steps:  20, steps per second:  56, episode reward: 44.570, mean reward:  2.229 [-2.156, 32.010], mean action: 1.500 [0.000, 16.000],  loss: 0.016024, mae: 0.303498, mean_q: 0.416392, mean_eps: 0.000000
 24804/30000: episode: 748, duration: 0.334s, episode steps:  26, steps per second:  78, episode reward: 43.573, mean reward:  1.676 [-2.053, 32.060], mean action: 2.615 [0.000, 15.000],  loss: 0.017123, mae: 0.323577, mean_q: 0.411034, mean_eps: 0.000000
 24822/30000: episode: 749, duration: 0.198s, episode steps:  18, steps per second:  91, episode reward: 41.690, mean reward:  2.316 [-2.543, 32.130], mean action: 1.667 [0.000, 21.000],  loss: 0.020024, mae: 0.340610, mean_q: 0.427762, mean_eps: 0.000000
 24850/30000: episode: 750, duration: 0.319s, episode steps:  28, steps per second:  88, episode reward: 41.616, mean reward:  1.486 [-2.329, 32.880], mean action: 2.286 [0.000, 16.000],  loss: 0.015583, mae: 0.316973, mean_q: 0.430114, mean_eps: 0.000000
 24888/30000: episode: 751, duration: 0.397s, episode steps:  38, steps per second:  96, episode reward: 35.766, mean reward:  0.941 [-2.442, 32.054], mean action: 2.553 [0.000, 16.000],  loss: 0.017567, mae: 0.321027, mean_q: 0.428454, mean_eps: 0.000000
 24918/30000: episode: 752, duration: 0.325s, episode steps:  30, steps per second:  92, episode reward: 44.119, mean reward:  1.471 [-2.518, 32.130], mean action: 3.167 [0.000, 15.000],  loss: 0.017024, mae: 0.312427, mean_q: 0.418450, mean_eps: 0.000000
 24954/30000: episode: 753, duration: 0.389s, episode steps:  36, steps per second:  92, episode reward: 36.000, mean reward:  1.000 [-2.576, 32.060], mean action: 1.472 [0.000, 17.000],  loss: 0.022978, mae: 0.334634, mean_q: 0.473684, mean_eps: 0.000000
 24986/30000: episode: 754, duration: 0.339s, episode steps:  32, steps per second:  94, episode reward: 36.000, mean reward:  1.125 [-2.214, 32.070], mean action: 1.781 [0.000, 16.000],  loss: 0.015738, mae: 0.303699, mean_q: 0.466023, mean_eps: 0.000000
 25010/30000: episode: 755, duration: 0.269s, episode steps:  24, steps per second:  89, episode reward: 38.666, mean reward:  1.611 [-2.182, 32.726], mean action: 2.125 [0.000, 15.000],  loss: 0.016476, mae: 0.307854, mean_q: 0.434277, mean_eps: 0.000000
 25042/30000: episode: 756, duration: 0.344s, episode steps:  32, steps per second:  93, episode reward: 35.690, mean reward:  1.115 [-2.354, 32.031], mean action: 2.094 [0.000, 15.000],  loss: 0.015817, mae: 0.314089, mean_q: 0.398142, mean_eps: 0.000000
 25081/30000: episode: 757, duration: 0.416s, episode steps:  39, steps per second:  94, episode reward: 39.000, mean reward:  1.000 [-2.431, 32.110], mean action: 3.051 [0.000, 15.000],  loss: 0.015341, mae: 0.309233, mean_q: 0.406898, mean_eps: 0.000000
 25115/30000: episode: 758, duration: 0.378s, episode steps:  34, steps per second:  90, episode reward: 42.629, mean reward:  1.254 [-2.384, 32.280], mean action: 1.206 [0.000, 6.000],  loss: 0.019338, mae: 0.343089, mean_q: 0.384799, mean_eps: 0.000000
 25146/30000: episode: 759, duration: 0.336s, episode steps:  31, steps per second:  92, episode reward: 38.857, mean reward:  1.253 [-2.480, 32.477], mean action: 3.968 [0.000, 20.000],  loss: 0.021215, mae: 0.335383, mean_q: 0.445769, mean_eps: 0.000000
 25161/30000: episode: 760, duration: 0.174s, episode steps:  15, steps per second:  86, episode reward: 41.004, mean reward:  2.734 [-2.362, 32.230], mean action: 3.200 [0.000, 16.000],  loss: 0.018351, mae: 0.322568, mean_q: 0.417022, mean_eps: 0.000000
 25185/30000: episode: 761, duration: 0.255s, episode steps:  24, steps per second:  94, episode reward: 38.535, mean reward:  1.606 [-2.845, 32.570], mean action: 3.333 [0.000, 20.000],  loss: 0.019314, mae: 0.321495, mean_q: 0.447124, mean_eps: 0.000000
 25217/30000: episode: 762, duration: 0.426s, episode steps:  32, steps per second:  75, episode reward: 34.375, mean reward:  1.074 [-3.000, 32.140], mean action: 1.750 [0.000, 12.000],  loss: 0.018920, mae: 0.315705, mean_q: 0.483563, mean_eps: 0.000000
 25235/30000: episode: 763, duration: 0.201s, episode steps:  18, steps per second:  90, episode reward: 47.460, mean reward:  2.637 [ 0.000, 32.100], mean action: 2.556 [0.000, 10.000],  loss: 0.019105, mae: 0.316460, mean_q: 0.494282, mean_eps: 0.000000
 25280/30000: episode: 764, duration: 0.485s, episode steps:  45, steps per second:  93, episode reward: 32.239, mean reward:  0.716 [-2.744, 31.537], mean action: 2.378 [0.000, 15.000],  loss: 0.017555, mae: 0.316478, mean_q: 0.423869, mean_eps: 0.000000
 25317/30000: episode: 765, duration: 0.388s, episode steps:  37, steps per second:  95, episode reward: 41.861, mean reward:  1.131 [-2.281, 32.120], mean action: 1.784 [0.000, 6.000],  loss: 0.020084, mae: 0.339929, mean_q: 0.402560, mean_eps: 0.000000
 25337/30000: episode: 766, duration: 0.256s, episode steps:  20, steps per second:  78, episode reward: 41.295, mean reward:  2.065 [-2.465, 32.382], mean action: 3.900 [1.000, 15.000],  loss: 0.020572, mae: 0.332861, mean_q: 0.426825, mean_eps: 0.000000
 25362/30000: episode: 767, duration: 0.276s, episode steps:  25, steps per second:  91, episode reward: 41.814, mean reward:  1.673 [-3.000, 32.247], mean action: 3.400 [0.000, 20.000],  loss: 0.017625, mae: 0.311758, mean_q: 0.469416, mean_eps: 0.000000
 25388/30000: episode: 768, duration: 0.274s, episode steps:  26, steps per second:  95, episode reward: 35.941, mean reward:  1.382 [-3.000, 32.150], mean action: 3.808 [0.000, 15.000],  loss: 0.018225, mae: 0.317131, mean_q: 0.424720, mean_eps: 0.000000
 25411/30000: episode: 769, duration: 0.249s, episode steps:  23, steps per second:  93, episode reward: 38.333, mean reward:  1.667 [-2.326, 32.309], mean action: 3.696 [0.000, 15.000],  loss: 0.018030, mae: 0.314039, mean_q: 0.452130, mean_eps: 0.000000
 25435/30000: episode: 770, duration: 0.262s, episode steps:  24, steps per second:  92, episode reward: 40.696, mean reward:  1.696 [-3.000, 31.549], mean action: 5.542 [0.000, 20.000],  loss: 0.015279, mae: 0.314551, mean_q: 0.396675, mean_eps: 0.000000
 25458/30000: episode: 771, duration: 0.251s, episode steps:  23, steps per second:  92, episode reward: 41.612, mean reward:  1.809 [-2.695, 32.193], mean action: 2.913 [1.000, 20.000],  loss: 0.019873, mae: 0.333152, mean_q: 0.411868, mean_eps: 0.000000
 25490/30000: episode: 772, duration: 0.357s, episode steps:  32, steps per second:  90, episode reward: 35.432, mean reward:  1.107 [-2.876, 32.223], mean action: 2.000 [0.000, 20.000],  loss: 0.017885, mae: 0.314217, mean_q: 0.459067, mean_eps: 0.000000
 25552/30000: episode: 773, duration: 0.657s, episode steps:  62, steps per second:  94, episode reward: 32.040, mean reward:  0.517 [-3.000, 32.270], mean action: 3.129 [0.000, 20.000],  loss: 0.018140, mae: 0.318924, mean_q: 0.429922, mean_eps: 0.000000
 25592/30000: episode: 774, duration: 0.422s, episode steps:  40, steps per second:  95, episode reward: 33.000, mean reward:  0.825 [-3.000, 32.230], mean action: 4.900 [0.000, 20.000],  loss: 0.015546, mae: 0.313177, mean_q: 0.427457, mean_eps: 0.000000
 25629/30000: episode: 775, duration: 0.416s, episode steps:  37, steps per second:  89, episode reward: 35.497, mean reward:  0.959 [-3.000, 32.332], mean action: 3.649 [0.000, 17.000],  loss: 0.016731, mae: 0.323351, mean_q: 0.406721, mean_eps: 0.000000
 25658/30000: episode: 776, duration: 0.339s, episode steps:  29, steps per second:  86, episode reward: 41.601, mean reward:  1.435 [-2.339, 32.080], mean action: 2.828 [0.000, 16.000],  loss: 0.016754, mae: 0.308842, mean_q: 0.438078, mean_eps: 0.000000
 25684/30000: episode: 777, duration: 0.304s, episode steps:  26, steps per second:  86, episode reward: 42.913, mean reward:  1.651 [-2.333, 32.250], mean action: 1.885 [0.000, 6.000],  loss: 0.014550, mae: 0.311880, mean_q: 0.349041, mean_eps: 0.000000
 25710/30000: episode: 778, duration: 0.286s, episode steps:  26, steps per second:  91, episode reward: 41.269, mean reward:  1.587 [-2.514, 32.200], mean action: 5.154 [0.000, 15.000],  loss: 0.015251, mae: 0.317064, mean_q: 0.370216, mean_eps: 0.000000
 25747/30000: episode: 779, duration: 0.397s, episode steps:  37, steps per second:  93, episode reward: 36.000, mean reward:  0.973 [-3.000, 32.280], mean action: 4.270 [0.000, 16.000],  loss: 0.020170, mae: 0.336073, mean_q: 0.405221, mean_eps: 0.000000
 25768/30000: episode: 780, duration: 0.249s, episode steps:  21, steps per second:  84, episode reward: 41.552, mean reward:  1.979 [-2.061, 32.400], mean action: 2.381 [0.000, 15.000],  loss: 0.015049, mae: 0.311654, mean_q: 0.411171, mean_eps: 0.000000
 25799/30000: episode: 781, duration: 0.345s, episode steps:  31, steps per second:  90, episode reward: 41.174, mean reward:  1.328 [-2.662, 32.270], mean action: 1.548 [0.000, 16.000],  loss: 0.019718, mae: 0.338832, mean_q: 0.402092, mean_eps: 0.000000
 25809/30000: episode: 782, duration: 0.121s, episode steps:  10, steps per second:  83, episode reward: 46.833, mean reward:  4.683 [-0.129, 32.586], mean action: 2.000 [0.000, 12.000],  loss: 0.015846, mae: 0.330032, mean_q: 0.382035, mean_eps: 0.000000
 25827/30000: episode: 783, duration: 0.210s, episode steps:  18, steps per second:  86, episode reward: 41.677, mean reward:  2.315 [-2.280, 32.461], mean action: 2.056 [0.000, 12.000],  loss: 0.019729, mae: 0.332994, mean_q: 0.374099, mean_eps: 0.000000
 25862/30000: episode: 784, duration: 0.373s, episode steps:  35, steps per second:  94, episode reward: 35.843, mean reward:  1.024 [-3.000, 32.350], mean action: 2.486 [0.000, 15.000],  loss: 0.016831, mae: 0.306925, mean_q: 0.455583, mean_eps: 0.000000
 25890/30000: episode: 785, duration: 0.317s, episode steps:  28, steps per second:  88, episode reward: 41.454, mean reward:  1.480 [-2.376, 31.946], mean action: 2.000 [0.000, 16.000],  loss: 0.018314, mae: 0.316092, mean_q: 0.430144, mean_eps: 0.000000
 25923/30000: episode: 786, duration: 0.375s, episode steps:  33, steps per second:  88, episode reward: 32.368, mean reward:  0.981 [-2.554, 31.418], mean action: 8.212 [0.000, 17.000],  loss: 0.018794, mae: 0.310752, mean_q: 0.430393, mean_eps: 0.000000
 25976/30000: episode: 787, duration: 0.582s, episode steps:  53, steps per second:  91, episode reward: 43.892, mean reward:  0.828 [-2.314, 32.070], mean action: 3.075 [0.000, 15.000],  loss: 0.017038, mae: 0.308356, mean_q: 0.416071, mean_eps: 0.000000
 26016/30000: episode: 788, duration: 0.434s, episode steps:  40, steps per second:  92, episode reward: 35.424, mean reward:  0.886 [-2.628, 31.722], mean action: 1.975 [0.000, 12.000],  loss: 0.019192, mae: 0.313062, mean_q: 0.427793, mean_eps: 0.000000
 26038/30000: episode: 789, duration: 0.316s, episode steps:  22, steps per second:  70, episode reward: 39.000, mean reward:  1.773 [-3.000, 32.190], mean action: 2.545 [0.000, 12.000],  loss: 0.014267, mae: 0.291505, mean_q: 0.451414, mean_eps: 0.000000
 26070/30000: episode: 790, duration: 0.358s, episode steps:  32, steps per second:  89, episode reward: 38.775, mean reward:  1.212 [-2.616, 32.035], mean action: 1.781 [0.000, 16.000],  loss: 0.020104, mae: 0.328430, mean_q: 0.440404, mean_eps: 0.000000
 26100/30000: episode: 791, duration: 0.323s, episode steps:  30, steps per second:  93, episode reward: 38.110, mean reward:  1.270 [-3.000, 31.985], mean action: 6.100 [0.000, 20.000],  loss: 0.018510, mae: 0.318804, mean_q: 0.392625, mean_eps: 0.000000
 26124/30000: episode: 792, duration: 0.261s, episode steps:  24, steps per second:  92, episode reward: 41.531, mean reward:  1.730 [-2.329, 32.111], mean action: 0.958 [0.000, 12.000],  loss: 0.022259, mae: 0.339961, mean_q: 0.412300, mean_eps: 0.000000
 26159/30000: episode: 793, duration: 0.395s, episode steps:  35, steps per second:  89, episode reward: -32.460, mean reward: -0.927 [-31.930,  2.758], mean action: 5.029 [0.000, 17.000],  loss: 0.015994, mae: 0.298446, mean_q: 0.452020, mean_eps: 0.000000
 26192/30000: episode: 794, duration: 0.389s, episode steps:  33, steps per second:  85, episode reward: 41.519, mean reward:  1.258 [-3.000, 32.090], mean action: 4.030 [0.000, 15.000],  loss: 0.018032, mae: 0.320654, mean_q: 0.401481, mean_eps: 0.000000
 26208/30000: episode: 795, duration: 0.185s, episode steps:  16, steps per second:  87, episode reward: 44.463, mean reward:  2.779 [-2.410, 31.773], mean action: 1.875 [0.000, 6.000],  loss: 0.016187, mae: 0.303621, mean_q: 0.418345, mean_eps: 0.000000
 26233/30000: episode: 796, duration: 0.281s, episode steps:  25, steps per second:  89, episode reward: 38.234, mean reward:  1.529 [-2.597, 32.330], mean action: 1.880 [0.000, 15.000],  loss: 0.021271, mae: 0.326944, mean_q: 0.435566, mean_eps: 0.000000
 26246/30000: episode: 797, duration: 0.155s, episode steps:  13, steps per second:  84, episode reward: 44.067, mean reward:  3.390 [-2.292, 32.790], mean action: 1.231 [0.000, 3.000],  loss: 0.014700, mae: 0.311251, mean_q: 0.379618, mean_eps: 0.000000
 26269/30000: episode: 798, duration: 0.260s, episode steps:  23, steps per second:  89, episode reward: 38.574, mean reward:  1.677 [-2.553, 31.847], mean action: 1.783 [0.000, 12.000],  loss: 0.019842, mae: 0.334071, mean_q: 0.400297, mean_eps: 0.000000
 26309/30000: episode: 799, duration: 0.432s, episode steps:  40, steps per second:  93, episode reward: -32.600, mean reward: -0.815 [-32.327,  2.380], mean action: 3.775 [0.000, 17.000],  loss: 0.015234, mae: 0.303574, mean_q: 0.394493, mean_eps: 0.000000
 26329/30000: episode: 800, duration: 0.225s, episode steps:  20, steps per second:  89, episode reward: 38.900, mean reward:  1.945 [-2.756, 32.140], mean action: 1.300 [0.000, 3.000],  loss: 0.016461, mae: 0.317284, mean_q: 0.384093, mean_eps: 0.000000
 26363/30000: episode: 801, duration: 0.375s, episode steps:  34, steps per second:  91, episode reward: 38.539, mean reward:  1.134 [-2.594, 32.030], mean action: 2.265 [0.000, 15.000],  loss: 0.018166, mae: 0.317817, mean_q: 0.439590, mean_eps: 0.000000
 26401/30000: episode: 802, duration: 0.406s, episode steps:  38, steps per second:  94, episode reward: 38.263, mean reward:  1.007 [-2.156, 32.224], mean action: 3.237 [0.000, 15.000],  loss: 0.021001, mae: 0.321968, mean_q: 0.444831, mean_eps: 0.000000
 26430/30000: episode: 803, duration: 0.313s, episode steps:  29, steps per second:  93, episode reward: 39.000, mean reward:  1.345 [-2.806, 32.390], mean action: 5.241 [0.000, 15.000],  loss: 0.012068, mae: 0.291661, mean_q: 0.406646, mean_eps: 0.000000
 26448/30000: episode: 804, duration: 0.210s, episode steps:  18, steps per second:  86, episode reward: 45.000, mean reward:  2.500 [-2.996, 32.120], mean action: 3.611 [0.000, 15.000],  loss: 0.017435, mae: 0.332465, mean_q: 0.384140, mean_eps: 0.000000
 26487/30000: episode: 805, duration: 0.426s, episode steps:  39, steps per second:  91, episode reward: 46.658, mean reward:  1.196 [-0.707, 32.381], mean action: 2.462 [0.000, 6.000],  loss: 0.019121, mae: 0.332087, mean_q: 0.404768, mean_eps: 0.000000
 26516/30000: episode: 806, duration: 0.314s, episode steps:  29, steps per second:  92, episode reward: 43.414, mean reward:  1.497 [-2.653, 32.050], mean action: 3.103 [1.000, 16.000],  loss: 0.018749, mae: 0.323088, mean_q: 0.406255, mean_eps: 0.000000
 26531/30000: episode: 807, duration: 0.173s, episode steps:  15, steps per second:  87, episode reward: 45.000, mean reward:  3.000 [-2.176, 32.330], mean action: 3.733 [0.000, 16.000],  loss: 0.017935, mae: 0.307022, mean_q: 0.451763, mean_eps: 0.000000
 26564/30000: episode: 808, duration: 0.361s, episode steps:  33, steps per second:  91, episode reward: 35.333, mean reward:  1.071 [-2.876, 32.520], mean action: 1.697 [0.000, 16.000],  loss: 0.018443, mae: 0.318268, mean_q: 0.430203, mean_eps: 0.000000
 26618/30000: episode: 809, duration: 0.566s, episode steps:  54, steps per second:  95, episode reward: 32.667, mean reward:  0.605 [-3.000, 29.815], mean action: 3.519 [0.000, 15.000],  loss: 0.018130, mae: 0.317910, mean_q: 0.398792, mean_eps: 0.000000
 26666/30000: episode: 810, duration: 0.502s, episode steps:  48, steps per second:  96, episode reward: 32.190, mean reward:  0.671 [-2.612, 32.350], mean action: 1.667 [0.000, 15.000],  loss: 0.017659, mae: 0.319032, mean_q: 0.407870, mean_eps: 0.000000
 26704/30000: episode: 811, duration: 0.417s, episode steps:  38, steps per second:  91, episode reward: 39.000, mean reward:  1.026 [-2.040, 32.360], mean action: 1.868 [0.000, 16.000],  loss: 0.013327, mae: 0.294430, mean_q: 0.438508, mean_eps: 0.000000
 26724/30000: episode: 812, duration: 0.225s, episode steps:  20, steps per second:  89, episode reward: 41.552, mean reward:  2.078 [-2.174, 31.682], mean action: 2.600 [0.000, 15.000],  loss: 0.017007, mae: 0.328222, mean_q: 0.360440, mean_eps: 0.000000
 26751/30000: episode: 813, duration: 0.288s, episode steps:  27, steps per second:  94, episode reward: 35.519, mean reward:  1.316 [-3.000, 32.750], mean action: 0.630 [0.000, 3.000],  loss: 0.019930, mae: 0.329519, mean_q: 0.396903, mean_eps: 0.000000
 26788/30000: episode: 814, duration: 0.484s, episode steps:  37, steps per second:  76, episode reward: 42.000, mean reward:  1.135 [-2.239, 32.020], mean action: 1.351 [0.000, 20.000],  loss: 0.018111, mae: 0.309128, mean_q: 0.427961, mean_eps: 0.000000
 26816/30000: episode: 815, duration: 0.413s, episode steps:  28, steps per second:  68, episode reward: 42.000, mean reward:  1.500 [-2.414, 32.080], mean action: 3.750 [1.000, 20.000],  loss: 0.019312, mae: 0.319806, mean_q: 0.424629, mean_eps: 0.000000
 26843/30000: episode: 816, duration: 0.327s, episode steps:  27, steps per second:  82, episode reward: 38.826, mean reward:  1.438 [-2.561, 34.282], mean action: 1.333 [0.000, 12.000],  loss: 0.016449, mae: 0.314718, mean_q: 0.382399, mean_eps: 0.000000
 26872/30000: episode: 817, duration: 0.338s, episode steps:  29, steps per second:  86, episode reward: 41.211, mean reward:  1.421 [-3.000, 32.180], mean action: 1.759 [0.000, 12.000],  loss: 0.018065, mae: 0.324911, mean_q: 0.403207, mean_eps: 0.000000
 26910/30000: episode: 818, duration: 0.407s, episode steps:  38, steps per second:  93, episode reward: 35.248, mean reward:  0.928 [-2.283, 31.465], mean action: 0.842 [0.000, 3.000],  loss: 0.017133, mae: 0.323716, mean_q: 0.367524, mean_eps: 0.000000
 26937/30000: episode: 819, duration: 0.289s, episode steps:  27, steps per second:  93, episode reward: 41.100, mean reward:  1.522 [-3.000, 32.320], mean action: 2.778 [0.000, 7.000],  loss: 0.014857, mae: 0.333776, mean_q: 0.342313, mean_eps: 0.000000
 26976/30000: episode: 820, duration: 0.430s, episode steps:  39, steps per second:  91, episode reward: 36.000, mean reward:  0.923 [-2.435, 32.630], mean action: 2.179 [0.000, 16.000],  loss: 0.016293, mae: 0.307487, mean_q: 0.448444, mean_eps: 0.000000
 26995/30000: episode: 821, duration: 0.237s, episode steps:  19, steps per second:  80, episode reward: 42.000, mean reward:  2.211 [-2.446, 32.070], mean action: 1.895 [0.000, 12.000],  loss: 0.018574, mae: 0.321166, mean_q: 0.439440, mean_eps: 0.000000
 27030/30000: episode: 822, duration: 0.398s, episode steps:  35, steps per second:  88, episode reward: 38.111, mean reward:  1.089 [-2.421, 31.921], mean action: 3.000 [0.000, 15.000],  loss: 0.015820, mae: 0.303629, mean_q: 0.401697, mean_eps: 0.000000
 27067/30000: episode: 823, duration: 0.415s, episode steps:  37, steps per second:  89, episode reward: 41.343, mean reward:  1.117 [-3.000, 32.227], mean action: 3.189 [0.000, 14.000],  loss: 0.018319, mae: 0.310428, mean_q: 0.396747, mean_eps: 0.000000
 27100/30000: episode: 824, duration: 0.349s, episode steps:  33, steps per second:  95, episode reward: 38.171, mean reward:  1.157 [-2.455, 31.662], mean action: 3.030 [0.000, 16.000],  loss: 0.021153, mae: 0.323649, mean_q: 0.425822, mean_eps: 0.000000
 27146/30000: episode: 825, duration: 0.494s, episode steps:  46, steps per second:  93, episode reward: 42.000, mean reward:  0.913 [-2.485, 32.130], mean action: 1.674 [1.000, 4.000],  loss: 0.019797, mae: 0.330428, mean_q: 0.416723, mean_eps: 0.000000
 27194/30000: episode: 826, duration: 0.511s, episode steps:  48, steps per second:  94, episode reward: 40.722, mean reward:  0.848 [-3.000, 32.102], mean action: 2.688 [0.000, 16.000],  loss: 0.016117, mae: 0.312633, mean_q: 0.457133, mean_eps: 0.000000
 27219/30000: episode: 827, duration: 0.282s, episode steps:  25, steps per second:  89, episode reward: 38.772, mean reward:  1.551 [-3.000, 32.080], mean action: 4.800 [0.000, 21.000],  loss: 0.015255, mae: 0.298383, mean_q: 0.415592, mean_eps: 0.000000
 27242/30000: episode: 828, duration: 0.268s, episode steps:  23, steps per second:  86, episode reward: 41.136, mean reward:  1.789 [-3.000, 31.960], mean action: 6.000 [0.000, 18.000],  loss: 0.017025, mae: 0.317757, mean_q: 0.407950, mean_eps: 0.000000
 27263/30000: episode: 829, duration: 0.232s, episode steps:  21, steps per second:  90, episode reward: 39.000, mean reward:  1.857 [-3.000, 32.330], mean action: 3.476 [0.000, 16.000],  loss: 0.017125, mae: 0.314459, mean_q: 0.439394, mean_eps: 0.000000
 27284/30000: episode: 830, duration: 0.229s, episode steps:  21, steps per second:  92, episode reward: 38.313, mean reward:  1.824 [-3.000, 32.270], mean action: 2.333 [0.000, 15.000],  loss: 0.019811, mae: 0.324695, mean_q: 0.451384, mean_eps: 0.000000
 27337/30000: episode: 831, duration: 0.573s, episode steps:  53, steps per second:  92, episode reward: 32.353, mean reward:  0.610 [-3.000, 32.110], mean action: 3.358 [0.000, 16.000],  loss: 0.015696, mae: 0.314277, mean_q: 0.437576, mean_eps: 0.000000
 27361/30000: episode: 832, duration: 0.329s, episode steps:  24, steps per second:  73, episode reward: 38.414, mean reward:  1.601 [-3.000, 32.270], mean action: 1.417 [0.000, 12.000],  loss: 0.018889, mae: 0.330549, mean_q: 0.442186, mean_eps: 0.000000
 27410/30000: episode: 833, duration: 0.552s, episode steps:  49, steps per second:  89, episode reward: 41.277, mean reward:  0.842 [-2.822, 32.390], mean action: 2.531 [0.000, 20.000],  loss: 0.018154, mae: 0.329673, mean_q: 0.448335, mean_eps: 0.000000
 27449/30000: episode: 834, duration: 0.422s, episode steps:  39, steps per second:  92, episode reward: 39.342, mean reward:  1.009 [-2.379, 31.344], mean action: 4.436 [0.000, 20.000],  loss: 0.019254, mae: 0.331144, mean_q: 0.455590, mean_eps: 0.000000
 27470/30000: episode: 835, duration: 0.238s, episode steps:  21, steps per second:  88, episode reward: 41.622, mean reward:  1.982 [-2.393, 31.771], mean action: 1.952 [0.000, 12.000],  loss: 0.019342, mae: 0.328667, mean_q: 0.430615, mean_eps: 0.000000
 27492/30000: episode: 836, duration: 0.239s, episode steps:  22, steps per second:  92, episode reward: 38.573, mean reward:  1.753 [-3.000, 32.160], mean action: 6.636 [0.000, 20.000],  loss: 0.020491, mae: 0.343280, mean_q: 0.403709, mean_eps: 0.000000
 27543/30000: episode: 837, duration: 0.548s, episode steps:  51, steps per second:  93, episode reward: -35.610, mean reward: -0.698 [-31.906,  2.500], mean action: 5.176 [0.000, 17.000],  loss: 0.016385, mae: 0.319426, mean_q: 0.430829, mean_eps: 0.000000
 27579/30000: episode: 838, duration: 0.378s, episode steps:  36, steps per second:  95, episode reward: 35.345, mean reward:  0.982 [-2.901, 31.634], mean action: 4.500 [0.000, 14.000],  loss: 0.014998, mae: 0.318508, mean_q: 0.381075, mean_eps: 0.000000
 27605/30000: episode: 839, duration: 0.280s, episode steps:  26, steps per second:  93, episode reward: 39.000, mean reward:  1.500 [-3.000, 32.110], mean action: 4.769 [0.000, 16.000],  loss: 0.015190, mae: 0.308007, mean_q: 0.439449, mean_eps: 0.000000
 27623/30000: episode: 840, duration: 0.194s, episode steps:  18, steps per second:  93, episode reward: 44.212, mean reward:  2.456 [-2.017, 31.752], mean action: 1.278 [0.000, 6.000],  loss: 0.016388, mae: 0.314538, mean_q: 0.442034, mean_eps: 0.000000
 27656/30000: episode: 841, duration: 0.366s, episode steps:  33, steps per second:  90, episode reward: 40.905, mean reward:  1.240 [-2.349, 32.100], mean action: 3.606 [0.000, 11.000],  loss: 0.016605, mae: 0.312737, mean_q: 0.411565, mean_eps: 0.000000
 27679/30000: episode: 842, duration: 0.249s, episode steps:  23, steps per second:  92, episode reward: 38.902, mean reward:  1.691 [-3.000, 31.942], mean action: 4.087 [0.000, 16.000],  loss: 0.016675, mae: 0.316688, mean_q: 0.389308, mean_eps: 0.000000
 27705/30000: episode: 843, duration: 0.297s, episode steps:  26, steps per second:  88, episode reward: 38.204, mean reward:  1.469 [-2.823, 32.460], mean action: 1.385 [0.000, 6.000],  loss: 0.018517, mae: 0.329244, mean_q: 0.412989, mean_eps: 0.000000
 27730/30000: episode: 844, duration: 0.284s, episode steps:  25, steps per second:  88, episode reward: 42.000, mean reward:  1.680 [-2.368, 32.170], mean action: 1.560 [0.000, 12.000],  loss: 0.018889, mae: 0.324404, mean_q: 0.410959, mean_eps: 0.000000
 27767/30000: episode: 845, duration: 0.381s, episode steps:  37, steps per second:  97, episode reward: -38.610, mean reward: -1.044 [-32.496,  2.220], mean action: 9.135 [0.000, 16.000],  loss: 0.016209, mae: 0.316275, mean_q: 0.410139, mean_eps: 0.000000
 27800/30000: episode: 846, duration: 0.368s, episode steps:  33, steps per second:  90, episode reward: 44.644, mean reward:  1.353 [-2.122, 32.410], mean action: 1.545 [0.000, 17.000],  loss: 0.016960, mae: 0.333882, mean_q: 0.391648, mean_eps: 0.000000
 27816/30000: episode: 847, duration: 0.179s, episode steps:  16, steps per second:  89, episode reward: 42.000, mean reward:  2.625 [-2.709, 32.330], mean action: 3.375 [0.000, 12.000],  loss: 0.019699, mae: 0.337366, mean_q: 0.413108, mean_eps: 0.000000
 27835/30000: episode: 848, duration: 0.234s, episode steps:  19, steps per second:  81, episode reward: 44.521, mean reward:  2.343 [-3.000, 32.380], mean action: 2.842 [0.000, 10.000],  loss: 0.020692, mae: 0.347529, mean_q: 0.369281, mean_eps: 0.000000
 27866/30000: episode: 849, duration: 0.332s, episode steps:  31, steps per second:  93, episode reward: 36.000, mean reward:  1.161 [-3.000, 32.110], mean action: 3.161 [0.000, 15.000],  loss: 0.018030, mae: 0.343301, mean_q: 0.366053, mean_eps: 0.000000
 27883/30000: episode: 850, duration: 0.190s, episode steps:  17, steps per second:  90, episode reward: 42.000, mean reward:  2.471 [-2.293, 32.250], mean action: 2.412 [0.000, 15.000],  loss: 0.014319, mae: 0.316860, mean_q: 0.385173, mean_eps: 0.000000
 27912/30000: episode: 851, duration: 0.314s, episode steps:  29, steps per second:  92, episode reward: 35.790, mean reward:  1.234 [-3.000, 32.860], mean action: 5.138 [1.000, 15.000],  loss: 0.017995, mae: 0.326894, mean_q: 0.434585, mean_eps: 0.000000
 27943/30000: episode: 852, duration: 0.346s, episode steps:  31, steps per second:  89, episode reward: 35.714, mean reward:  1.152 [-2.810, 32.010], mean action: 1.548 [0.000, 12.000],  loss: 0.016462, mae: 0.318299, mean_q: 0.440443, mean_eps: 0.000000
 27963/30000: episode: 853, duration: 0.231s, episode steps:  20, steps per second:  87, episode reward: 48.000, mean reward:  2.400 [ 0.000, 32.240], mean action: 1.900 [0.000, 6.000],  loss: 0.014605, mae: 0.311991, mean_q: 0.418155, mean_eps: 0.000000
 27978/30000: episode: 854, duration: 0.178s, episode steps:  15, steps per second:  84, episode reward: 44.686, mean reward:  2.979 [-2.131, 32.280], mean action: 3.200 [0.000, 15.000],  loss: 0.015783, mae: 0.303686, mean_q: 0.477159, mean_eps: 0.000000
 28009/30000: episode: 855, duration: 0.420s, episode steps:  31, steps per second:  74, episode reward: 39.000, mean reward:  1.258 [-2.400, 32.270], mean action: 1.129 [0.000, 6.000],  loss: 0.018361, mae: 0.319750, mean_q: 0.439601, mean_eps: 0.000000
 28053/30000: episode: 856, duration: 0.467s, episode steps:  44, steps per second:  94, episode reward: 32.526, mean reward:  0.739 [-2.903, 32.123], mean action: 1.818 [0.000, 16.000],  loss: 0.019613, mae: 0.342865, mean_q: 0.391768, mean_eps: 0.000000
 28103/30000: episode: 857, duration: 0.534s, episode steps:  50, steps per second:  94, episode reward: 41.610, mean reward:  0.832 [-2.437, 32.070], mean action: 3.020 [0.000, 16.000],  loss: 0.018034, mae: 0.324749, mean_q: 0.444814, mean_eps: 0.000000
 28138/30000: episode: 858, duration: 0.381s, episode steps:  35, steps per second:  92, episode reward: 38.107, mean reward:  1.089 [-3.000, 32.240], mean action: 4.571 [0.000, 17.000],  loss: 0.020736, mae: 0.347353, mean_q: 0.417395, mean_eps: 0.000000
 28184/30000: episode: 859, duration: 0.503s, episode steps:  46, steps per second:  91, episode reward: 32.661, mean reward:  0.710 [-2.365, 32.078], mean action: 6.174 [0.000, 17.000],  loss: 0.017565, mae: 0.335640, mean_q: 0.447718, mean_eps: 0.000000
 28214/30000: episode: 860, duration: 0.336s, episode steps:  30, steps per second:  89, episode reward: 38.507, mean reward:  1.284 [-2.717, 31.917], mean action: 2.767 [0.000, 15.000],  loss: 0.017212, mae: 0.342642, mean_q: 0.400230, mean_eps: 0.000000
 28272/30000: episode: 861, duration: 0.594s, episode steps:  58, steps per second:  98, episode reward: 44.774, mean reward:  0.772 [-2.320, 32.220], mean action: 2.086 [0.000, 15.000],  loss: 0.015553, mae: 0.314379, mean_q: 0.399436, mean_eps: 0.000000
 28308/30000: episode: 862, duration: 0.385s, episode steps:  36, steps per second:  94, episode reward: 40.093, mean reward:  1.114 [-2.465, 31.794], mean action: 3.167 [0.000, 18.000],  loss: 0.015173, mae: 0.313224, mean_q: 0.415179, mean_eps: 0.000000
 28333/30000: episode: 863, duration: 0.312s, episode steps:  25, steps per second:  80, episode reward: 38.130, mean reward:  1.525 [-3.000, 32.210], mean action: 1.840 [0.000, 12.000],  loss: 0.016248, mae: 0.329109, mean_q: 0.358491, mean_eps: 0.000000
 28357/30000: episode: 864, duration: 0.277s, episode steps:  24, steps per second:  87, episode reward: 44.450, mean reward:  1.852 [-2.048, 32.017], mean action: 2.042 [0.000, 3.000],  loss: 0.018136, mae: 0.336834, mean_q: 0.381717, mean_eps: 0.000000
 28378/30000: episode: 865, duration: 0.232s, episode steps:  21, steps per second:  91, episode reward: 41.767, mean reward:  1.989 [-2.287, 32.130], mean action: 3.095 [0.000, 16.000],  loss: 0.014878, mae: 0.335232, mean_q: 0.363120, mean_eps: 0.000000
 28393/30000: episode: 866, duration: 0.173s, episode steps:  15, steps per second:  87, episode reward: 44.652, mean reward:  2.977 [-2.130, 32.750], mean action: 2.267 [0.000, 7.000],  loss: 0.017968, mae: 0.349422, mean_q: 0.385374, mean_eps: 0.000000
 28418/30000: episode: 867, duration: 0.279s, episode steps:  25, steps per second:  90, episode reward: 42.000, mean reward:  1.680 [-2.009, 30.010], mean action: 1.040 [0.000, 3.000],  loss: 0.016867, mae: 0.339033, mean_q: 0.374450, mean_eps: 0.000000
 28454/30000: episode: 868, duration: 0.386s, episode steps:  36, steps per second:  93, episode reward: 41.903, mean reward:  1.164 [-2.217, 31.943], mean action: 3.472 [0.000, 16.000],  loss: 0.021126, mae: 0.358751, mean_q: 0.403984, mean_eps: 0.000000
 28483/30000: episode: 869, duration: 0.324s, episode steps:  29, steps per second:  90, episode reward: 35.941, mean reward:  1.239 [-2.858, 32.170], mean action: 3.586 [0.000, 16.000],  loss: 0.016722, mae: 0.328432, mean_q: 0.446709, mean_eps: 0.000000
 28527/30000: episode: 870, duration: 0.470s, episode steps:  44, steps per second:  94, episode reward: 32.614, mean reward:  0.741 [-3.000, 32.214], mean action: 3.909 [0.000, 15.000],  loss: 0.020248, mae: 0.343989, mean_q: 0.443071, mean_eps: 0.000000
 28568/30000: episode: 871, duration: 0.506s, episode steps:  41, steps per second:  81, episode reward: 32.903, mean reward:  0.803 [-3.000, 32.213], mean action: 3.366 [0.000, 15.000],  loss: 0.016304, mae: 0.323711, mean_q: 0.421013, mean_eps: 0.000000
 28592/30000: episode: 872, duration: 0.296s, episode steps:  24, steps per second:  81, episode reward: 41.394, mean reward:  1.725 [-2.192, 32.248], mean action: 1.750 [0.000, 15.000],  loss: 0.018707, mae: 0.359420, mean_q: 0.385220, mean_eps: 0.000000
 28623/30000: episode: 873, duration: 0.325s, episode steps:  31, steps per second:  95, episode reward: 38.679, mean reward:  1.248 [-3.000, 32.019], mean action: 2.161 [0.000, 16.000],  loss: 0.014713, mae: 0.314793, mean_q: 0.422279, mean_eps: 0.000000
 28636/30000: episode: 874, duration: 0.198s, episode steps:  13, steps per second:  66, episode reward: 44.185, mean reward:  3.399 [-2.005, 32.193], mean action: 3.154 [0.000, 16.000],  loss: 0.015097, mae: 0.316055, mean_q: 0.413865, mean_eps: 0.000000
 28657/30000: episode: 875, duration: 0.235s, episode steps:  21, steps per second:  89, episode reward: 41.858, mean reward:  1.993 [-2.721, 32.028], mean action: 1.143 [0.000, 16.000],  loss: 0.017147, mae: 0.336044, mean_q: 0.359520, mean_eps: 0.000000
 28686/30000: episode: 876, duration: 0.322s, episode steps:  29, steps per second:  90, episode reward: 39.000, mean reward:  1.345 [-2.778, 32.130], mean action: 4.621 [0.000, 16.000],  loss: 0.018228, mae: 0.335431, mean_q: 0.386449, mean_eps: 0.000000
 28713/30000: episode: 877, duration: 0.308s, episode steps:  27, steps per second:  88, episode reward: 38.489, mean reward:  1.426 [-2.814, 32.321], mean action: 1.667 [0.000, 14.000],  loss: 0.017659, mae: 0.330633, mean_q: 0.395474, mean_eps: 0.000000
 28739/30000: episode: 878, duration: 0.284s, episode steps:  26, steps per second:  91, episode reward: 41.337, mean reward:  1.590 [-2.450, 32.720], mean action: 1.538 [0.000, 3.000],  loss: 0.016049, mae: 0.339035, mean_q: 0.391572, mean_eps: 0.000000
 28764/30000: episode: 879, duration: 0.269s, episode steps:  25, steps per second:  93, episode reward: 39.000, mean reward:  1.560 [-2.806, 32.250], mean action: 3.440 [0.000, 15.000],  loss: 0.018025, mae: 0.332374, mean_q: 0.427064, mean_eps: 0.000000
 28822/30000: episode: 880, duration: 0.746s, episode steps:  58, steps per second:  78, episode reward: 35.154, mean reward:  0.606 [-2.556, 31.738], mean action: 3.310 [0.000, 15.000],  loss: 0.017437, mae: 0.321903, mean_q: 0.436716, mean_eps: 0.000000
 28850/30000: episode: 881, duration: 0.335s, episode steps:  28, steps per second:  84, episode reward: -37.690, mean reward: -1.346 [-32.831,  3.000], mean action: 6.893 [0.000, 21.000],  loss: 0.017681, mae: 0.330473, mean_q: 0.400853, mean_eps: 0.000000
 28877/30000: episode: 882, duration: 0.294s, episode steps:  27, steps per second:  92, episode reward: 44.505, mean reward:  1.648 [-2.296, 32.430], mean action: 1.963 [0.000, 10.000],  loss: 0.017693, mae: 0.328371, mean_q: 0.408795, mean_eps: 0.000000
 28907/30000: episode: 883, duration: 0.328s, episode steps:  30, steps per second:  91, episode reward: 43.345, mean reward:  1.445 [-2.335, 32.062], mean action: 2.367 [0.000, 12.000],  loss: 0.017092, mae: 0.326638, mean_q: 0.430654, mean_eps: 0.000000
 28937/30000: episode: 884, duration: 0.314s, episode steps:  30, steps per second:  95, episode reward: 36.000, mean reward:  1.200 [-2.903, 32.030], mean action: 2.333 [0.000, 15.000],  loss: 0.021334, mae: 0.341606, mean_q: 0.490437, mean_eps: 0.000000
 28965/30000: episode: 885, duration: 0.294s, episode steps:  28, steps per second:  95, episode reward: -32.310, mean reward: -1.154 [-32.195,  2.746], mean action: 3.643 [0.000, 12.000],  loss: 0.017388, mae: 0.335040, mean_q: 0.391156, mean_eps: 0.000000
 28981/30000: episode: 886, duration: 0.177s, episode steps:  16, steps per second:  90, episode reward: 40.373, mean reward:  2.523 [-3.000, 32.723], mean action: 3.125 [0.000, 12.000],  loss: 0.017306, mae: 0.334872, mean_q: 0.390470, mean_eps: 0.000000
 29007/30000: episode: 887, duration: 0.292s, episode steps:  26, steps per second:  89, episode reward: 41.135, mean reward:  1.582 [-2.463, 32.100], mean action: 1.038 [0.000, 4.000],  loss: 0.016944, mae: 0.335926, mean_q: 0.388860, mean_eps: 0.000000
 29027/30000: episode: 888, duration: 0.227s, episode steps:  20, steps per second:  88, episode reward: 44.880, mean reward:  2.244 [-2.185, 32.560], mean action: 2.800 [1.000, 3.000],  loss: 0.019884, mae: 0.350349, mean_q: 0.406893, mean_eps: 0.000000
 29063/30000: episode: 889, duration: 0.376s, episode steps:  36, steps per second:  96, episode reward: 38.733, mean reward:  1.076 [-3.000, 32.045], mean action: 3.472 [0.000, 16.000],  loss: 0.014950, mae: 0.317513, mean_q: 0.455559, mean_eps: 0.000000
 29097/30000: episode: 890, duration: 0.381s, episode steps:  34, steps per second:  89, episode reward: 38.310, mean reward:  1.127 [-2.475, 31.674], mean action: 2.529 [0.000, 16.000],  loss: 0.018111, mae: 0.338411, mean_q: 0.396743, mean_eps: 0.000000
 29114/30000: episode: 891, duration: 0.194s, episode steps:  17, steps per second:  88, episode reward: 35.518, mean reward:  2.089 [-3.000, 31.568], mean action: 5.000 [0.000, 17.000],  loss: 0.018640, mae: 0.332633, mean_q: 0.452154, mean_eps: 0.000000
 29134/30000: episode: 892, duration: 0.227s, episode steps:  20, steps per second:  88, episode reward: 47.267, mean reward:  2.363 [-0.004, 32.140], mean action: 2.350 [0.000, 7.000],  loss: 0.018418, mae: 0.318669, mean_q: 0.480840, mean_eps: 0.000000
 29167/30000: episode: 893, duration: 0.359s, episode steps:  33, steps per second:  92, episode reward: 44.229, mean reward:  1.340 [-2.412, 32.010], mean action: 2.212 [0.000, 20.000],  loss: 0.017800, mae: 0.333820, mean_q: 0.389612, mean_eps: 0.000000
 29205/30000: episode: 894, duration: 0.605s, episode steps:  38, steps per second:  63, episode reward: 44.217, mean reward:  1.164 [-2.078, 32.754], mean action: 2.316 [0.000, 16.000],  loss: 0.016314, mae: 0.336254, mean_q: 0.371403, mean_eps: 0.000000
 29235/30000: episode: 895, duration: 0.313s, episode steps:  30, steps per second:  96, episode reward: 38.395, mean reward:  1.280 [-2.488, 32.268], mean action: 4.533 [0.000, 16.000],  loss: 0.016134, mae: 0.327089, mean_q: 0.395661, mean_eps: 0.000000
 29269/30000: episode: 896, duration: 0.350s, episode steps:  34, steps per second:  97, episode reward: 39.000, mean reward:  1.147 [-2.512, 32.250], mean action: 5.971 [0.000, 20.000],  loss: 0.020210, mae: 0.345525, mean_q: 0.419015, mean_eps: 0.000000
 29293/30000: episode: 897, duration: 0.256s, episode steps:  24, steps per second:  94, episode reward: 41.333, mean reward:  1.722 [-2.385, 31.703], mean action: 2.750 [0.000, 14.000],  loss: 0.016885, mae: 0.326206, mean_q: 0.417934, mean_eps: 0.000000
 29316/30000: episode: 898, duration: 0.242s, episode steps:  23, steps per second:  95, episode reward: 44.456, mean reward:  1.933 [-2.510, 32.060], mean action: 1.826 [1.000, 12.000],  loss: 0.013133, mae: 0.319519, mean_q: 0.418852, mean_eps: 0.000000
 29343/30000: episode: 899, duration: 0.292s, episode steps:  27, steps per second:  92, episode reward: 43.832, mean reward:  1.623 [-2.104, 31.995], mean action: 3.741 [0.000, 10.000],  loss: 0.018798, mae: 0.346972, mean_q: 0.404477, mean_eps: 0.000000
 29369/30000: episode: 900, duration: 0.273s, episode steps:  26, steps per second:  95, episode reward: 39.000, mean reward:  1.500 [-3.000, 32.070], mean action: 1.654 [0.000, 3.000],  loss: 0.014633, mae: 0.317226, mean_q: 0.390091, mean_eps: 0.000000
 29385/30000: episode: 901, duration: 0.175s, episode steps:  16, steps per second:  91, episode reward: 41.527, mean reward:  2.595 [-2.374, 31.982], mean action: 0.625 [0.000, 3.000],  loss: 0.020537, mae: 0.351389, mean_q: 0.379075, mean_eps: 0.000000
 29408/30000: episode: 902, duration: 0.257s, episode steps:  23, steps per second:  90, episode reward: 44.632, mean reward:  1.941 [-2.309, 32.540], mean action: 2.565 [0.000, 17.000],  loss: 0.013555, mae: 0.325360, mean_q: 0.425874, mean_eps: 0.000000
 29428/30000: episode: 903, duration: 0.231s, episode steps:  20, steps per second:  87, episode reward: 41.244, mean reward:  2.062 [-2.751, 32.163], mean action: 2.400 [0.000, 16.000],  loss: 0.016015, mae: 0.349260, mean_q: 0.348778, mean_eps: 0.000000
 29459/30000: episode: 904, duration: 0.348s, episode steps:  31, steps per second:  89, episode reward: 36.000, mean reward:  1.161 [-3.000, 32.240], mean action: 2.387 [0.000, 12.000],  loss: 0.020408, mae: 0.347575, mean_q: 0.457872, mean_eps: 0.000000
 29497/30000: episode: 905, duration: 0.490s, episode steps:  38, steps per second:  78, episode reward: 35.941, mean reward:  0.946 [-3.000, 33.000], mean action: 4.000 [0.000, 20.000],  loss: 0.020907, mae: 0.354161, mean_q: 0.407038, mean_eps: 0.000000
 29525/30000: episode: 906, duration: 0.297s, episode steps:  28, steps per second:  94, episode reward: 40.941, mean reward:  1.462 [-2.466, 32.152], mean action: 2.786 [0.000, 15.000],  loss: 0.018382, mae: 0.350779, mean_q: 0.422116, mean_eps: 0.000000
 29565/30000: episode: 907, duration: 0.417s, episode steps:  40, steps per second:  96, episode reward: 41.576, mean reward:  1.039 [-3.000, 32.290], mean action: 5.000 [0.000, 15.000],  loss: 0.017417, mae: 0.352559, mean_q: 0.416935, mean_eps: 0.000000
 29587/30000: episode: 908, duration: 0.238s, episode steps:  22, steps per second:  92, episode reward: 38.091, mean reward:  1.731 [-2.236, 32.520], mean action: 4.182 [0.000, 15.000],  loss: 0.016788, mae: 0.337044, mean_q: 0.433421, mean_eps: 0.000000
 29612/30000: episode: 909, duration: 0.274s, episode steps:  25, steps per second:  91, episode reward: 44.414, mean reward:  1.777 [-2.076, 32.160], mean action: 1.920 [0.000, 16.000],  loss: 0.017083, mae: 0.330030, mean_q: 0.417439, mean_eps: 0.000000
 29639/30000: episode: 910, duration: 0.321s, episode steps:  27, steps per second:  84, episode reward: 44.506, mean reward:  1.648 [-2.187, 32.320], mean action: 1.148 [0.000, 15.000],  loss: 0.017393, mae: 0.349342, mean_q: 0.384306, mean_eps: 0.000000
 29667/30000: episode: 911, duration: 0.290s, episode steps:  28, steps per second:  97, episode reward: 36.000, mean reward:  1.286 [-2.669, 32.100], mean action: 1.107 [0.000, 3.000],  loss: 0.017124, mae: 0.333877, mean_q: 0.474769, mean_eps: 0.000000
 29688/30000: episode: 912, duration: 0.232s, episode steps:  21, steps per second:  91, episode reward: 46.218, mean reward:  2.201 [-0.650, 32.530], mean action: 2.524 [0.000, 20.000],  loss: 0.017958, mae: 0.337343, mean_q: 0.471942, mean_eps: 0.000000
 29719/30000: episode: 913, duration: 0.326s, episode steps:  31, steps per second:  95, episode reward: 38.852, mean reward:  1.253 [-3.000, 32.240], mean action: 5.484 [0.000, 16.000],  loss: 0.014969, mae: 0.338808, mean_q: 0.377032, mean_eps: 0.000000
 29751/30000: episode: 914, duration: 0.359s, episode steps:  32, steps per second:  89, episode reward: 47.088, mean reward:  1.472 [-0.427, 31.933], mean action: 4.031 [3.000, 15.000],  loss: 0.018734, mae: 0.331843, mean_q: 0.484642, mean_eps: 0.000000
 29771/30000: episode: 915, duration: 0.220s, episode steps:  20, steps per second:  91, episode reward: 41.314, mean reward:  2.066 [-2.405, 31.904], mean action: 3.500 [1.000, 16.000],  loss: 0.019026, mae: 0.347013, mean_q: 0.416074, mean_eps: 0.000000
 29795/30000: episode: 916, duration: 0.259s, episode steps:  24, steps per second:  93, episode reward: 41.557, mean reward:  1.732 [-2.402, 32.080], mean action: 1.833 [0.000, 3.000],  loss: 0.018104, mae: 0.352009, mean_q: 0.396824, mean_eps: 0.000000
 29824/30000: episode: 917, duration: 0.302s, episode steps:  29, steps per second:  96, episode reward: 33.000, mean reward:  1.138 [-2.966, 30.178], mean action: 4.655 [0.000, 17.000],  loss: 0.017524, mae: 0.327697, mean_q: 0.449346, mean_eps: 0.000000
 29865/30000: episode: 918, duration: 0.422s, episode steps:  41, steps per second:  97, episode reward: 43.982, mean reward:  1.073 [-2.207, 32.140], mean action: 3.000 [0.000, 18.000],  loss: 0.015181, mae: 0.324809, mean_q: 0.450513, mean_eps: 0.000000
 29889/30000: episode: 919, duration: 0.252s, episode steps:  24, steps per second:  95, episode reward: 38.796, mean reward:  1.616 [-3.000, 31.866], mean action: 2.708 [0.000, 12.000],  loss: 0.015827, mae: 0.327419, mean_q: 0.430510, mean_eps: 0.000000
 29914/30000: episode: 920, duration: 0.271s, episode steps:  25, steps per second:  92, episode reward: 42.000, mean reward:  1.680 [-2.349, 33.000], mean action: 0.800 [0.000, 12.000],  loss: 0.021169, mae: 0.343607, mean_q: 0.453224, mean_eps: 0.000000
done, took 381.971 seconds
Training for 30000 steps ...
    19/30000: episode: 1, duration: 0.153s, episode steps:  19, steps per second: 124, episode reward: 41.842, mean reward:  2.202 [-2.434, 32.330], mean action: 0.895 [0.000, 4.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    40/30000: episode: 2, duration: 0.143s, episode steps:  21, steps per second: 147, episode reward: 43.957, mean reward:  2.093 [-2.148, 31.756], mean action: 2.333 [0.000, 14.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    61/30000: episode: 3, duration: 0.136s, episode steps:  21, steps per second: 154, episode reward: 41.259, mean reward:  1.965 [-2.004, 32.124], mean action: 1.524 [0.000, 4.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    85/30000: episode: 4, duration: 0.156s, episode steps:  24, steps per second: 154, episode reward: 32.521, mean reward:  1.355 [-3.000, 32.121], mean action: 4.417 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   111/30000: episode: 5, duration: 0.184s, episode steps:  26, steps per second: 141, episode reward: 44.564, mean reward:  1.714 [-2.181, 32.204], mean action: 0.615 [0.000, 4.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   139/30000: episode: 6, duration: 0.170s, episode steps:  28, steps per second: 165, episode reward: 38.567, mean reward:  1.377 [-3.000, 32.130], mean action: 3.179 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   158/30000: episode: 7, duration: 0.136s, episode steps:  19, steps per second: 140, episode reward: 43.399, mean reward:  2.284 [-2.179, 32.795], mean action: 3.947 [0.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   194/30000: episode: 8, duration: 0.228s, episode steps:  36, steps per second: 158, episode reward: 35.763, mean reward:  0.993 [-3.000, 32.110], mean action: 1.722 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   262/30000: episode: 9, duration: 0.407s, episode steps:  68, steps per second: 167, episode reward: -33.000, mean reward: -0.485 [-32.143,  2.500], mean action: 10.588 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   294/30000: episode: 10, duration: 0.228s, episode steps:  32, steps per second: 140, episode reward: 44.308, mean reward:  1.385 [-2.354, 32.090], mean action: 2.500 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   312/30000: episode: 11, duration: 0.125s, episode steps:  18, steps per second: 144, episode reward: 44.004, mean reward:  2.445 [-2.344, 32.070], mean action: 1.944 [0.000, 4.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   335/30000: episode: 12, duration: 0.151s, episode steps:  23, steps per second: 153, episode reward: 44.682, mean reward:  1.943 [-2.377, 32.080], mean action: 1.522 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   357/30000: episode: 13, duration: 0.139s, episode steps:  22, steps per second: 159, episode reward: 41.189, mean reward:  1.872 [-3.000, 32.110], mean action: 2.545 [1.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   394/30000: episode: 14, duration: 0.251s, episode steps:  37, steps per second: 148, episode reward: 38.047, mean reward:  1.028 [-2.471, 32.360], mean action: 2.649 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   414/30000: episode: 15, duration: 0.138s, episode steps:  20, steps per second: 145, episode reward: 41.156, mean reward:  2.058 [-2.842, 32.350], mean action: 2.400 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   437/30000: episode: 16, duration: 0.148s, episode steps:  23, steps per second: 156, episode reward: 44.673, mean reward:  1.942 [-2.454, 32.080], mean action: 2.739 [0.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   465/30000: episode: 17, duration: 0.181s, episode steps:  28, steps per second: 155, episode reward: 38.428, mean reward:  1.372 [-2.780, 32.070], mean action: 3.000 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   496/30000: episode: 18, duration: 0.195s, episode steps:  31, steps per second: 159, episode reward: 44.293, mean reward:  1.429 [-2.442, 32.540], mean action: 2.000 [0.000, 12.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   523/30000: episode: 19, duration: 0.167s, episode steps:  27, steps per second: 161, episode reward: 37.749, mean reward:  1.398 [-3.000, 32.220], mean action: 4.296 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   559/30000: episode: 20, duration: 0.219s, episode steps:  36, steps per second: 164, episode reward: 38.747, mean reward:  1.076 [-2.294, 32.850], mean action: 6.917 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   581/30000: episode: 21, duration: 0.148s, episode steps:  22, steps per second: 149, episode reward: 39.000, mean reward:  1.773 [-3.000, 32.070], mean action: 3.636 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   605/30000: episode: 22, duration: 0.171s, episode steps:  24, steps per second: 140, episode reward: 38.793, mean reward:  1.616 [-3.000, 32.380], mean action: 3.708 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   634/30000: episode: 23, duration: 0.189s, episode steps:  29, steps per second: 153, episode reward: 35.929, mean reward:  1.239 [-3.000, 32.290], mean action: 1.862 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   672/30000: episode: 24, duration: 0.230s, episode steps:  38, steps per second: 166, episode reward: 32.038, mean reward:  0.843 [-2.637, 31.580], mean action: 2.263 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   695/30000: episode: 25, duration: 0.156s, episode steps:  23, steps per second: 148, episode reward: 42.000, mean reward:  1.826 [-2.525, 32.160], mean action: 1.652 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   719/30000: episode: 26, duration: 0.231s, episode steps:  24, steps per second: 104, episode reward: 38.389, mean reward:  1.600 [-3.000, 32.270], mean action: 2.125 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   780/30000: episode: 27, duration: 0.365s, episode steps:  61, steps per second: 167, episode reward: 35.532, mean reward:  0.582 [-3.000, 32.340], mean action: 3.590 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   803/30000: episode: 28, duration: 0.149s, episode steps:  23, steps per second: 154, episode reward: 38.903, mean reward:  1.691 [-3.000, 32.393], mean action: 2.957 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   846/30000: episode: 29, duration: 0.259s, episode steps:  43, steps per second: 166, episode reward: 38.135, mean reward:  0.887 [-2.262, 32.360], mean action: 6.744 [0.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   875/30000: episode: 30, duration: 0.182s, episode steps:  29, steps per second: 159, episode reward: 43.540, mean reward:  1.501 [-2.341, 31.997], mean action: 4.069 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   895/30000: episode: 31, duration: 0.133s, episode steps:  20, steps per second: 150, episode reward: 37.949, mean reward:  1.897 [-2.850, 32.046], mean action: 3.500 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   937/30000: episode: 32, duration: 0.247s, episode steps:  42, steps per second: 170, episode reward: 32.667, mean reward:  0.778 [-2.251, 31.787], mean action: 3.714 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   973/30000: episode: 33, duration: 0.212s, episode steps:  36, steps per second: 170, episode reward: 38.836, mean reward:  1.079 [-2.708, 32.320], mean action: 2.889 [1.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   998/30000: episode: 34, duration: 0.157s, episode steps:  25, steps per second: 160, episode reward: 33.000, mean reward:  1.320 [-3.000, 32.350], mean action: 2.640 [0.000, 16.000],  loss: --, mae: --, mean_q: --, mean_eps: --
