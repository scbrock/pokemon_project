params 0
{'model_type': 'dqn_agent', 'l1_out': 128, 'l2_out': 64, 'gamma': 0.5, 'target_model_update': 1, 'delta_clip': 0.01, 'nb_steps_warmup': 1000, 'enable_dueling_dqn__': False, 'enable_double_dqn__': True, 'dueling_type__': 'avg'}
trial 0
Training for 10000 steps ...
   28/10000: episode: 1, duration: 1.732s, episode steps:  28, steps per second:  16, episode reward: -41.510, mean reward: -1.482 [-32.058,  2.430], mean action: 16.214 [14.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   66/10000: episode: 2, duration: 0.390s, episode steps:  38, steps per second:  98, episode reward: -44.650, mean reward: -1.175 [-32.115,  1.790], mean action: 16.658 [7.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   95/10000: episode: 3, duration: 0.242s, episode steps:  29, steps per second: 120, episode reward: -35.560, mean reward: -1.226 [-31.669,  2.920], mean action: 16.931 [7.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  116/10000: episode: 4, duration: 0.201s, episode steps:  21, steps per second: 104, episode reward: -38.910, mean reward: -1.853 [-32.910,  3.000], mean action: 14.476 [7.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  143/10000: episode: 5, duration: 0.222s, episode steps:  27, steps per second: 121, episode reward: -41.730, mean reward: -1.546 [-32.432,  2.540], mean action: 15.185 [7.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  169/10000: episode: 6, duration: 0.208s, episode steps:  26, steps per second: 125, episode reward: -45.000, mean reward: -1.731 [-33.000,  2.320], mean action: 15.692 [10.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  199/10000: episode: 7, duration: 0.371s, episode steps:  30, steps per second:  81, episode reward: -44.570, mean reward: -1.486 [-32.004,  2.060], mean action: 15.900 [14.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  231/10000: episode: 8, duration: 0.291s, episode steps:  32, steps per second: 110, episode reward: -32.450, mean reward: -1.014 [-32.259,  3.000], mean action: 15.656 [7.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  252/10000: episode: 9, duration: 0.158s, episode steps:  21, steps per second: 133, episode reward: -39.000, mean reward: -1.857 [-32.301,  2.371], mean action: 14.619 [7.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  286/10000: episode: 10, duration: 0.256s, episode steps:  34, steps per second: 133, episode reward: -41.290, mean reward: -1.214 [-31.881,  2.950], mean action: 15.471 [7.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  318/10000: episode: 11, duration: 0.223s, episode steps:  32, steps per second: 143, episode reward: -42.000, mean reward: -1.312 [-32.663,  2.320], mean action: 13.719 [1.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  348/10000: episode: 12, duration: 0.256s, episode steps:  30, steps per second: 117, episode reward: -39.000, mean reward: -1.300 [-32.265,  2.232], mean action: 15.100 [7.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  372/10000: episode: 13, duration: 0.215s, episode steps:  24, steps per second: 112, episode reward: -41.080, mean reward: -1.712 [-31.873,  2.240], mean action: 13.375 [7.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  409/10000: episode: 14, duration: 0.297s, episode steps:  37, steps per second: 124, episode reward: -39.000, mean reward: -1.054 [-32.197,  2.840], mean action: 13.216 [3.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  441/10000: episode: 15, duration: 0.270s, episode steps:  32, steps per second: 118, episode reward: -38.340, mean reward: -1.198 [-32.042,  2.220], mean action: 15.438 [10.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  485/10000: episode: 16, duration: 0.342s, episode steps:  44, steps per second: 129, episode reward: -38.180, mean reward: -0.868 [-32.089,  2.430], mean action: 14.909 [3.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  517/10000: episode: 17, duration: 0.257s, episode steps:  32, steps per second: 124, episode reward: -35.880, mean reward: -1.121 [-32.239,  2.700], mean action: 15.312 [7.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  563/10000: episode: 18, duration: 0.374s, episode steps:  46, steps per second: 123, episode reward: -41.210, mean reward: -0.896 [-31.980,  2.502], mean action: 12.717 [1.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  614/10000: episode: 19, duration: 0.456s, episode steps:  51, steps per second: 112, episode reward: -44.380, mean reward: -0.870 [-31.974,  1.824], mean action: 15.902 [7.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  713/10000: episode: 20, duration: 0.787s, episode steps:  99, steps per second: 126, episode reward: -44.910, mean reward: -0.454 [-32.285,  2.460], mean action: 16.485 [7.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  748/10000: episode: 21, duration: 0.393s, episode steps:  35, steps per second:  89, episode reward: -44.830, mean reward: -1.281 [-32.683,  2.120], mean action: 13.714 [7.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  776/10000: episode: 22, duration: 0.278s, episode steps:  28, steps per second: 101, episode reward: -44.470, mean reward: -1.588 [-32.086,  2.230], mean action: 14.321 [7.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  799/10000: episode: 23, duration: 0.234s, episode steps:  23, steps per second:  98, episode reward: -39.000, mean reward: -1.696 [-30.042,  2.351], mean action: 13.304 [7.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  844/10000: episode: 24, duration: 0.394s, episode steps:  45, steps per second: 114, episode reward: -36.000, mean reward: -0.800 [-32.333,  2.739], mean action: 8.844 [3.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  893/10000: episode: 25, duration: 0.496s, episode steps:  49, steps per second:  99, episode reward: -42.000, mean reward: -0.857 [-32.139,  2.160], mean action: 14.878 [3.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  926/10000: episode: 26, duration: 0.271s, episode steps:  33, steps per second: 122, episode reward: -44.980, mean reward: -1.363 [-32.214,  2.004], mean action: 13.939 [7.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  972/10000: episode: 27, duration: 0.402s, episode steps:  46, steps per second: 114, episode reward: 36.941, mean reward:  0.803 [-2.454, 32.344], mean action: 14.717 [3.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1021/10000: episode: 28, duration: 1.607s, episode steps:  49, steps per second:  31, episode reward: -39.000, mean reward: -0.796 [-32.184,  2.550], mean action: 16.694 [3.000, 21.000],  loss: 0.016827, mae: 0.438837, mean_q: 0.455937, mean_eps: 0.000000
 1050/10000: episode: 29, duration: 0.528s, episode steps:  29, steps per second:  55, episode reward: -38.340, mean reward: -1.322 [-32.104,  2.810], mean action: 13.172 [1.000, 21.000],  loss: 0.012629, mae: 0.537899, mean_q: 0.021895, mean_eps: 0.000000
 1084/10000: episode: 30, duration: 0.488s, episode steps:  34, steps per second:  70, episode reward: -42.000, mean reward: -1.235 [-32.169,  2.169], mean action: 12.353 [7.000, 21.000],  loss: 0.013303, mae: 0.568113, mean_q: -0.031862, mean_eps: 0.000000
 1117/10000: episode: 31, duration: 0.445s, episode steps:  33, steps per second:  74, episode reward: -35.850, mean reward: -1.086 [-32.310,  2.270], mean action: 7.212 [3.000, 21.000],  loss: 0.013407, mae: 0.535682, mean_q: 0.035018, mean_eps: 0.000000
 1149/10000: episode: 32, duration: 0.582s, episode steps:  32, steps per second:  55, episode reward: -38.760, mean reward: -1.211 [-32.605,  2.520], mean action: 18.125 [3.000, 21.000],  loss: 0.012211, mae: 0.592192, mean_q: -0.000303, mean_eps: 0.000000
 1194/10000: episode: 33, duration: 0.713s, episode steps:  45, steps per second:  63, episode reward: -38.170, mean reward: -0.848 [-32.016,  2.512], mean action: 15.067 [1.000, 21.000],  loss: 0.013667, mae: 0.649432, mean_q: -0.042814, mean_eps: 0.000000
 1253/10000: episode: 34, duration: 0.869s, episode steps:  59, steps per second:  68, episode reward: -32.910, mean reward: -0.558 [-32.213,  3.000], mean action: 9.678 [1.000, 21.000],  loss: 0.014412, mae: 0.650777, mean_q: -0.018453, mean_eps: 0.000000
 1284/10000: episode: 35, duration: 0.478s, episode steps:  31, steps per second:  65, episode reward: -35.760, mean reward: -1.154 [-32.564,  2.370], mean action: 7.323 [1.000, 21.000],  loss: 0.014533, mae: 0.688251, mean_q: -0.072997, mean_eps: 0.000000
 1321/10000: episode: 36, duration: 0.615s, episode steps:  37, steps per second:  60, episode reward: -35.170, mean reward: -0.951 [-31.976,  3.000], mean action: 5.703 [1.000, 21.000],  loss: 0.014654, mae: 0.636354, mean_q: -0.015197, mean_eps: 0.000000
 1343/10000: episode: 37, duration: 0.303s, episode steps:  22, steps per second:  73, episode reward: -38.830, mean reward: -1.765 [-32.028,  3.000], mean action: 11.182 [1.000, 16.000],  loss: 0.014380, mae: 0.630670, mean_q: -0.001878, mean_eps: 0.000000
 1412/10000: episode: 38, duration: 1.064s, episode steps:  69, steps per second:  65, episode reward: -35.910, mean reward: -0.520 [-32.228,  2.660], mean action: 10.435 [1.000, 21.000],  loss: 0.014950, mae: 0.679498, mean_q: -0.030593, mean_eps: 0.000000
 1444/10000: episode: 39, duration: 0.501s, episode steps:  32, steps per second:  64, episode reward: 33.000, mean reward:  1.031 [-2.422, 32.340], mean action: 7.688 [1.000, 21.000],  loss: 0.014419, mae: 0.636442, mean_q: 0.027439, mean_eps: 0.000000
 1478/10000: episode: 40, duration: 0.465s, episode steps:  34, steps per second:  73, episode reward: -39.000, mean reward: -1.147 [-32.010,  2.290], mean action: 8.294 [1.000, 21.000],  loss: 0.013679, mae: 0.639860, mean_q: 0.015486, mean_eps: 0.000000
 1502/10000: episode: 41, duration: 0.339s, episode steps:  24, steps per second:  71, episode reward: -41.430, mean reward: -1.726 [-32.117,  2.189], mean action: 10.542 [1.000, 16.000],  loss: 0.015400, mae: 0.661072, mean_q: -0.014894, mean_eps: 0.000000
 1531/10000: episode: 42, duration: 0.463s, episode steps:  29, steps per second:  63, episode reward: -33.000, mean reward: -1.138 [-33.000,  2.788], mean action: 15.966 [3.000, 21.000],  loss: 0.012487, mae: 0.669225, mean_q: -0.007290, mean_eps: 0.000000
 1558/10000: episode: 43, duration: 0.344s, episode steps:  27, steps per second:  78, episode reward: -44.940, mean reward: -1.664 [-32.184,  2.003], mean action: 14.926 [3.000, 21.000],  loss: 0.015937, mae: 0.699994, mean_q: -0.040004, mean_eps: 0.000000
 1588/10000: episode: 44, duration: 0.417s, episode steps:  30, steps per second:  72, episode reward: -35.910, mean reward: -1.197 [-32.284,  2.901], mean action: 12.300 [1.000, 21.000],  loss: 0.013510, mae: 0.699200, mean_q: -0.044410, mean_eps: 0.000000
 1616/10000: episode: 45, duration: 0.392s, episode steps:  28, steps per second:  71, episode reward: -38.600, mean reward: -1.379 [-32.243,  2.441], mean action: 15.750 [3.000, 21.000],  loss: 0.012521, mae: 0.699103, mean_q: -0.038665, mean_eps: 0.000000
 1649/10000: episode: 46, duration: 0.444s, episode steps:  33, steps per second:  74, episode reward: 33.000, mean reward:  1.000 [-3.000, 33.000], mean action: 15.152 [1.000, 21.000],  loss: 0.014808, mae: 0.736398, mean_q: -0.072695, mean_eps: 0.000000
 1674/10000: episode: 47, duration: 0.318s, episode steps:  25, steps per second:  78, episode reward: -35.350, mean reward: -1.414 [-32.250,  3.000], mean action: 9.040 [1.000, 21.000],  loss: 0.019596, mae: 0.733091, mean_q: -0.048762, mean_eps: 0.000000
 1705/10000: episode: 48, duration: 0.412s, episode steps:  31, steps per second:  75, episode reward: 35.571, mean reward:  1.147 [-3.000, 32.211], mean action: 15.323 [3.000, 21.000],  loss: 0.012762, mae: 0.701297, mean_q: -0.044705, mean_eps: 0.000000
 1738/10000: episode: 49, duration: 0.484s, episode steps:  33, steps per second:  68, episode reward: -36.000, mean reward: -1.091 [-32.358,  2.418], mean action: 15.273 [1.000, 21.000],  loss: 0.016542, mae: 0.725810, mean_q: -0.037702, mean_eps: 0.000000
 1789/10000: episode: 50, duration: 0.708s, episode steps:  51, steps per second:  72, episode reward: 32.972, mean reward:  0.647 [-3.000, 32.061], mean action: 14.824 [1.000, 21.000],  loss: 0.013745, mae: 0.700334, mean_q: -0.027176, mean_eps: 0.000000
 1814/10000: episode: 51, duration: 0.341s, episode steps:  25, steps per second:  73, episode reward: -39.000, mean reward: -1.560 [-32.190,  2.641], mean action: 15.520 [3.000, 21.000],  loss: 0.015180, mae: 0.684492, mean_q: 0.011234, mean_eps: 0.000000
 1842/10000: episode: 52, duration: 0.415s, episode steps:  28, steps per second:  67, episode reward: 32.225, mean reward:  1.151 [-2.484, 32.205], mean action: 12.500 [1.000, 21.000],  loss: 0.016810, mae: 0.714633, mean_q: -0.003945, mean_eps: 0.000000
 1868/10000: episode: 53, duration: 0.379s, episode steps:  26, steps per second:  69, episode reward: 32.609, mean reward:  1.254 [-3.000, 32.902], mean action: 14.577 [1.000, 21.000],  loss: 0.015998, mae: 0.692895, mean_q: 0.020602, mean_eps: 0.000000
 1908/10000: episode: 54, duration: 0.557s, episode steps:  40, steps per second:  72, episode reward: -41.620, mean reward: -1.040 [-32.187,  2.202], mean action: 9.700 [1.000, 21.000],  loss: 0.015221, mae: 0.672186, mean_q: 0.022837, mean_eps: 0.000000
 1930/10000: episode: 55, duration: 0.332s, episode steps:  22, steps per second:  66, episode reward: -38.210, mean reward: -1.737 [-31.887,  2.220], mean action: 11.273 [1.000, 17.000],  loss: 0.018271, mae: 0.691670, mean_q: 0.018168, mean_eps: 0.000000
 1951/10000: episode: 56, duration: 0.289s, episode steps:  21, steps per second:  73, episode reward: 37.620, mean reward:  1.791 [-2.273, 32.562], mean action: 13.857 [1.000, 21.000],  loss: 0.015597, mae: 0.711401, mean_q: -0.036642, mean_eps: 0.000000
 1989/10000: episode: 57, duration: 0.547s, episode steps:  38, steps per second:  69, episode reward: -33.000, mean reward: -0.868 [-32.611,  2.903], mean action: 11.500 [1.000, 21.000],  loss: 0.014132, mae: 0.681481, mean_q: 0.033769, mean_eps: 0.000000
 2036/10000: episode: 58, duration: 0.680s, episode steps:  47, steps per second:  69, episode reward: 32.087, mean reward:  0.683 [-2.856, 32.060], mean action: 5.745 [1.000, 21.000],  loss: 0.015845, mae: 0.707344, mean_q: 0.025544, mean_eps: 0.000000
 2070/10000: episode: 59, duration: 0.439s, episode steps:  34, steps per second:  77, episode reward: -35.020, mean reward: -1.030 [-32.151,  2.510], mean action: 14.882 [1.000, 21.000],  loss: 0.013829, mae: 0.703776, mean_q: 0.021318, mean_eps: 0.000000
 2093/10000: episode: 60, duration: 0.307s, episode steps:  23, steps per second:  75, episode reward: -38.950, mean reward: -1.693 [-32.333,  2.517], mean action: 12.000 [1.000, 17.000],  loss: 0.012603, mae: 0.642667, mean_q: 0.119905, mean_eps: 0.000000
 2125/10000: episode: 61, duration: 0.438s, episode steps:  32, steps per second:  73, episode reward: 32.612, mean reward:  1.019 [-2.870, 32.306], mean action: 10.531 [1.000, 16.000],  loss: 0.015611, mae: 0.699275, mean_q: 0.062844, mean_eps: 0.000000
 2147/10000: episode: 62, duration: 0.317s, episode steps:  22, steps per second:  69, episode reward: 32.540, mean reward:  1.479 [-3.000, 32.718], mean action: 11.409 [1.000, 21.000],  loss: 0.015846, mae: 0.715074, mean_q: 0.052872, mean_eps: 0.000000
 2181/10000: episode: 63, duration: 0.435s, episode steps:  34, steps per second:  78, episode reward: -35.340, mean reward: -1.039 [-32.074,  2.537], mean action: 8.235 [1.000, 17.000],  loss: 0.015508, mae: 0.712619, mean_q: 0.033923, mean_eps: 0.000000
 2211/10000: episode: 64, duration: 0.410s, episode steps:  30, steps per second:  73, episode reward: -35.290, mean reward: -1.176 [-31.795,  3.031], mean action: 10.933 [1.000, 17.000],  loss: 0.014106, mae: 0.672822, mean_q: 0.071328, mean_eps: 0.000000
 2251/10000: episode: 65, duration: 0.521s, episode steps:  40, steps per second:  77, episode reward: 30.000, mean reward:  0.750 [-2.796, 30.201], mean action: 15.100 [1.000, 21.000],  loss: 0.016448, mae: 0.730575, mean_q: 0.026367, mean_eps: 0.000000
 2293/10000: episode: 66, duration: 0.528s, episode steps:  42, steps per second:  80, episode reward: -32.970, mean reward: -0.785 [-32.501,  2.522], mean action: 13.952 [3.000, 21.000],  loss: 0.016893, mae: 0.729600, mean_q: 0.061821, mean_eps: 0.000000
 2323/10000: episode: 67, duration: 0.430s, episode steps:  30, steps per second:  70, episode reward: -35.650, mean reward: -1.188 [-32.337,  2.720], mean action: 10.200 [1.000, 17.000],  loss: 0.014868, mae: 0.758177, mean_q: 0.021729, mean_eps: 0.000000
 2351/10000: episode: 68, duration: 0.443s, episode steps:  28, steps per second:  63, episode reward: -38.270, mean reward: -1.367 [-31.885,  2.290], mean action: 10.571 [1.000, 21.000],  loss: 0.018177, mae: 0.767531, mean_q: 0.011734, mean_eps: 0.000000
 2379/10000: episode: 69, duration: 0.386s, episode steps:  28, steps per second:  73, episode reward: -35.910, mean reward: -1.282 [-32.404,  2.622], mean action: 10.607 [1.000, 21.000],  loss: 0.014816, mae: 0.751040, mean_q: 0.013465, mean_eps: 0.000000
 2398/10000: episode: 70, duration: 0.257s, episode steps:  19, steps per second:  74, episode reward: -41.180, mean reward: -2.167 [-33.000,  2.162], mean action: 14.684 [3.000, 21.000],  loss: 0.015814, mae: 0.768974, mean_q: -0.007475, mean_eps: 0.000000
 2430/10000: episode: 71, duration: 0.455s, episode steps:  32, steps per second:  70, episode reward: -41.200, mean reward: -1.288 [-32.125,  2.510], mean action: 8.031 [1.000, 17.000],  loss: 0.016985, mae: 0.746270, mean_q: 0.002516, mean_eps: 0.000000
 2458/10000: episode: 72, duration: 0.379s, episode steps:  28, steps per second:  74, episode reward: -41.520, mean reward: -1.483 [-32.141,  3.038], mean action: 12.393 [1.000, 17.000],  loss: 0.015727, mae: 0.739891, mean_q: 0.023756, mean_eps: 0.000000
 2492/10000: episode: 73, duration: 0.471s, episode steps:  34, steps per second:  72, episode reward: -41.400, mean reward: -1.218 [-32.172,  2.176], mean action: 13.206 [1.000, 21.000],  loss: 0.015392, mae: 0.713757, mean_q: 0.064590, mean_eps: 0.000000
 2524/10000: episode: 74, duration: 0.467s, episode steps:  32, steps per second:  68, episode reward: 32.601, mean reward:  1.019 [-3.000, 30.654], mean action: 16.375 [1.000, 21.000],  loss: 0.017359, mae: 0.756336, mean_q: 0.022541, mean_eps: 0.000000
 2549/10000: episode: 75, duration: 0.315s, episode steps:  25, steps per second:  79, episode reward: 34.388, mean reward:  1.376 [-3.000, 32.760], mean action: 9.600 [1.000, 21.000],  loss: 0.019958, mae: 0.737007, mean_q: 0.041286, mean_eps: 0.000000
 2576/10000: episode: 76, duration: 0.398s, episode steps:  27, steps per second:  68, episode reward: 34.206, mean reward:  1.267 [-2.358, 32.011], mean action: 10.667 [1.000, 21.000],  loss: 0.016739, mae: 0.730408, mean_q: 0.026307, mean_eps: 0.000000
 2607/10000: episode: 77, duration: 0.538s, episode steps:  31, steps per second:  58, episode reward: 35.159, mean reward:  1.134 [-2.296, 31.954], mean action: 6.806 [1.000, 21.000],  loss: 0.014810, mae: 0.728547, mean_q: 0.042947, mean_eps: 0.000000
 2632/10000: episode: 78, duration: 0.378s, episode steps:  25, steps per second:  66, episode reward: -44.140, mean reward: -1.766 [-32.555,  1.672], mean action: 13.440 [1.000, 17.000],  loss: 0.012018, mae: 0.677069, mean_q: 0.096648, mean_eps: 0.000000
 2660/10000: episode: 79, duration: 0.367s, episode steps:  28, steps per second:  76, episode reward: -47.060, mean reward: -1.681 [-32.298,  0.005], mean action: 15.143 [1.000, 21.000],  loss: 0.015685, mae: 0.755887, mean_q: 0.000012, mean_eps: 0.000000
 2691/10000: episode: 80, duration: 0.442s, episode steps:  31, steps per second:  70, episode reward: 35.035, mean reward:  1.130 [-2.410, 32.170], mean action: 8.452 [1.000, 21.000],  loss: 0.016967, mae: 0.728147, mean_q: 0.029771, mean_eps: 0.000000
 2718/10000: episode: 81, duration: 0.393s, episode steps:  27, steps per second:  69, episode reward: -41.530, mean reward: -1.538 [-31.779,  2.740], mean action: 16.074 [3.000, 21.000],  loss: 0.018511, mae: 0.781050, mean_q: -0.016645, mean_eps: 0.000000
 2759/10000: episode: 82, duration: 0.695s, episode steps:  41, steps per second:  59, episode reward: -32.760, mean reward: -0.799 [-31.958,  2.450], mean action: 7.268 [1.000, 21.000],  loss: 0.014695, mae: 0.753243, mean_q: 0.006808, mean_eps: 0.000000
 2793/10000: episode: 83, duration: 0.528s, episode steps:  34, steps per second:  64, episode reward: -32.740, mean reward: -0.963 [-32.181,  2.560], mean action: 6.882 [1.000, 21.000],  loss: 0.016457, mae: 0.757070, mean_q: 0.006531, mean_eps: 0.000000
 2827/10000: episode: 84, duration: 0.587s, episode steps:  34, steps per second:  58, episode reward: 34.724, mean reward:  1.021 [-2.867, 33.000], mean action: 10.206 [1.000, 21.000],  loss: 0.016594, mae: 0.724611, mean_q: 0.063142, mean_eps: 0.000000
 2887/10000: episode: 85, duration: 0.815s, episode steps:  60, steps per second:  74, episode reward: 32.419, mean reward:  0.540 [-3.000, 31.953], mean action: 5.483 [1.000, 21.000],  loss: 0.016092, mae: 0.744898, mean_q: 0.017951, mean_eps: 0.000000
 2916/10000: episode: 86, duration: 0.378s, episode steps:  29, steps per second:  77, episode reward: 38.003, mean reward:  1.310 [-3.000, 33.000], mean action: 10.793 [1.000, 21.000],  loss: 0.014521, mae: 0.707196, mean_q: 0.050640, mean_eps: 0.000000
 2958/10000: episode: 87, duration: 0.585s, episode steps:  42, steps per second:  72, episode reward: -33.000, mean reward: -0.786 [-32.300,  2.810], mean action: 11.405 [1.000, 21.000],  loss: 0.013370, mae: 0.701928, mean_q: 0.068438, mean_eps: 0.000000
 3024/10000: episode: 88, duration: 0.891s, episode steps:  66, steps per second:  74, episode reward: 32.030, mean reward:  0.485 [-3.000, 32.250], mean action: 13.106 [3.000, 17.000],  loss: 0.014534, mae: 0.733230, mean_q: 0.034792, mean_eps: 0.000000
 3049/10000: episode: 89, duration: 0.340s, episode steps:  25, steps per second:  73, episode reward: -38.350, mean reward: -1.534 [-31.928,  2.490], mean action: 10.760 [1.000, 21.000],  loss: 0.011062, mae: 0.699216, mean_q: 0.050302, mean_eps: 0.000000
 3074/10000: episode: 90, duration: 0.321s, episode steps:  25, steps per second:  78, episode reward: -38.810, mean reward: -1.552 [-31.990,  2.093], mean action: 12.840 [3.000, 21.000],  loss: 0.012518, mae: 0.704915, mean_q: 0.052447, mean_eps: 0.000000
 3102/10000: episode: 91, duration: 0.410s, episode steps:  28, steps per second:  68, episode reward: -33.000, mean reward: -1.179 [-32.357,  3.000], mean action: 6.357 [1.000, 17.000],  loss: 0.014074, mae: 0.775370, mean_q: -0.012233, mean_eps: 0.000000
 3125/10000: episode: 92, duration: 0.335s, episode steps:  23, steps per second:  69, episode reward: -32.430, mean reward: -1.410 [-31.575,  2.902], mean action: 8.957 [1.000, 17.000],  loss: 0.018821, mae: 0.791808, mean_q: -0.005203, mean_eps: 0.000000
 3158/10000: episode: 93, duration: 0.498s, episode steps:  33, steps per second:  66, episode reward: -32.850, mean reward: -0.995 [-31.945,  2.380], mean action: 13.394 [1.000, 21.000],  loss: 0.016482, mae: 0.736360, mean_q: 0.065838, mean_eps: 0.000000
 3240/10000: episode: 94, duration: 1.152s, episode steps:  82, steps per second:  71, episode reward: -36.000, mean reward: -0.439 [-32.029,  2.400], mean action: 15.280 [1.000, 21.000],  loss: 0.016262, mae: 0.754017, mean_q: 0.033489, mean_eps: 0.000000
 3269/10000: episode: 95, duration: 0.416s, episode steps:  29, steps per second:  70, episode reward: -32.520, mean reward: -1.121 [-31.551,  3.120], mean action: 12.931 [1.000, 21.000],  loss: 0.014322, mae: 0.733031, mean_q: 0.044751, mean_eps: 0.000000
 3301/10000: episode: 96, duration: 0.445s, episode steps:  32, steps per second:  72, episode reward: -41.120, mean reward: -1.285 [-31.712,  2.671], mean action: 14.594 [1.000, 21.000],  loss: 0.017133, mae: 0.784012, mean_q: 0.032623, mean_eps: 0.000000
 3338/10000: episode: 97, duration: 0.537s, episode steps:  37, steps per second:  69, episode reward: 35.865, mean reward:  0.969 [-2.261, 32.045], mean action: 11.973 [1.000, 21.000],  loss: 0.015180, mae: 0.783834, mean_q: 0.016301, mean_eps: 0.000000
 3360/10000: episode: 98, duration: 0.289s, episode steps:  22, steps per second:  76, episode reward: -35.520, mean reward: -1.615 [-32.047,  2.535], mean action: 8.364 [1.000, 21.000],  loss: 0.015244, mae: 0.736689, mean_q: 0.057845, mean_eps: 0.000000
 3391/10000: episode: 99, duration: 0.443s, episode steps:  31, steps per second:  70, episode reward: -32.540, mean reward: -1.050 [-31.998,  2.735], mean action: 12.161 [1.000, 21.000],  loss: 0.016772, mae: 0.771275, mean_q: 0.016036, mean_eps: 0.000000
 3430/10000: episode: 100, duration: 0.502s, episode steps:  39, steps per second:  78, episode reward: -32.780, mean reward: -0.841 [-32.264,  2.950], mean action: 9.051 [3.000, 17.000],  loss: 0.014526, mae: 0.753461, mean_q: 0.043737, mean_eps: 0.000000
 3476/10000: episode: 101, duration: 0.605s, episode steps:  46, steps per second:  76, episode reward: -35.930, mean reward: -0.781 [-32.127,  2.592], mean action: 7.087 [1.000, 21.000],  loss: 0.021372, mae: 0.788128, mean_q: 0.036096, mean_eps: 0.000000
 3497/10000: episode: 102, duration: 0.314s, episode steps:  21, steps per second:  67, episode reward: -32.990, mean reward: -1.571 [-31.994,  3.000], mean action: 12.524 [1.000, 21.000],  loss: 0.016794, mae: 0.820608, mean_q: -0.041570, mean_eps: 0.000000
 3526/10000: episode: 103, duration: 0.376s, episode steps:  29, steps per second:  77, episode reward: -32.500, mean reward: -1.121 [-32.071,  2.567], mean action: 8.966 [1.000, 21.000],  loss: 0.014287, mae: 0.761171, mean_q: 0.042338, mean_eps: 0.000000
 3542/10000: episode: 104, duration: 0.195s, episode steps:  16, steps per second:  82, episode reward: -42.000, mean reward: -2.625 [-33.000,  2.712], mean action: 13.562 [3.000, 21.000],  loss: 0.014991, mae: 0.761022, mean_q: 0.053263, mean_eps: 0.000000
 3572/10000: episode: 105, duration: 0.402s, episode steps:  30, steps per second:  75, episode reward: -35.620, mean reward: -1.187 [-32.255,  2.495], mean action: 10.067 [1.000, 17.000],  loss: 0.016603, mae: 0.750487, mean_q: 0.056044, mean_eps: 0.000000
 3591/10000: episode: 106, duration: 0.385s, episode steps:  19, steps per second:  49, episode reward: -44.070, mean reward: -2.319 [-33.000,  2.231], mean action: 12.316 [3.000, 21.000],  loss: 0.017721, mae: 0.818032, mean_q: -0.040209, mean_eps: 0.000000
 3621/10000: episode: 107, duration: 0.383s, episode steps:  30, steps per second:  78, episode reward: -38.870, mean reward: -1.296 [-32.328,  2.416], mean action: 11.167 [1.000, 21.000],  loss: 0.018082, mae: 0.777215, mean_q: 0.035054, mean_eps: 0.000000
 3696/10000: episode: 108, duration: 1.045s, episode steps:  75, steps per second:  72, episode reward: 35.517, mean reward:  0.474 [-2.308, 32.130], mean action: 3.973 [1.000, 21.000],  loss: 0.015577, mae: 0.768619, mean_q: 0.056144, mean_eps: 0.000000
 3728/10000: episode: 109, duration: 0.458s, episode steps:  32, steps per second:  70, episode reward: -33.000, mean reward: -1.031 [-32.838,  2.527], mean action: 13.906 [1.000, 21.000],  loss: 0.015795, mae: 0.752665, mean_q: 0.078299, mean_eps: 0.000000
 3764/10000: episode: 110, duration: 0.470s, episode steps:  36, steps per second:  77, episode reward: -35.560, mean reward: -0.988 [-31.893,  3.658], mean action: 15.583 [8.000, 21.000],  loss: 0.013885, mae: 0.734358, mean_q: 0.088448, mean_eps: 0.000000
 3790/10000: episode: 111, duration: 0.374s, episode steps:  26, steps per second:  70, episode reward: 35.117, mean reward:  1.351 [-3.000, 32.150], mean action: 10.885 [1.000, 21.000],  loss: 0.017502, mae: 0.788345, mean_q: 0.049772, mean_eps: 0.000000
 3834/10000: episode: 112, duration: 0.607s, episode steps:  44, steps per second:  73, episode reward: 34.835, mean reward:  0.792 [-2.456, 32.055], mean action: 12.864 [3.000, 21.000],  loss: 0.015562, mae: 0.781629, mean_q: 0.020759, mean_eps: 0.000000
 3868/10000: episode: 113, duration: 0.483s, episode steps:  34, steps per second:  70, episode reward: 35.174, mean reward:  1.035 [-3.000, 32.100], mean action: 12.676 [3.000, 21.000],  loss: 0.015690, mae: 0.780624, mean_q: 0.023098, mean_eps: 0.000000
 3908/10000: episode: 114, duration: 0.530s, episode steps:  40, steps per second:  75, episode reward: -38.030, mean reward: -0.951 [-31.974,  2.203], mean action: 11.525 [1.000, 17.000],  loss: 0.014618, mae: 0.761327, mean_q: 0.063067, mean_eps: 0.000000
 3932/10000: episode: 115, duration: 0.355s, episode steps:  24, steps per second:  68, episode reward: 45.652, mean reward:  1.902 [-0.373, 32.335], mean action: 5.083 [1.000, 21.000],  loss: 0.015982, mae: 0.762030, mean_q: 0.058454, mean_eps: 0.000000
 3962/10000: episode: 116, duration: 0.414s, episode steps:  30, steps per second:  72, episode reward: -33.000, mean reward: -1.100 [-32.892,  3.000], mean action: 10.100 [3.000, 21.000],  loss: 0.015135, mae: 0.775930, mean_q: 0.035144, mean_eps: 0.000000
 3996/10000: episode: 117, duration: 0.477s, episode steps:  34, steps per second:  71, episode reward: -36.000, mean reward: -1.059 [-32.322,  2.431], mean action: 13.059 [1.000, 21.000],  loss: 0.015164, mae: 0.731957, mean_q: 0.143199, mean_eps: 0.000000
 4046/10000: episode: 118, duration: 0.745s, episode steps:  50, steps per second:  67, episode reward: 37.904, mean reward:  0.758 [-3.000, 32.300], mean action: 9.040 [1.000, 21.000],  loss: 0.014925, mae: 0.759814, mean_q: 0.099152, mean_eps: 0.000000
 4073/10000: episode: 119, duration: 0.345s, episode steps:  27, steps per second:  78, episode reward: 32.778, mean reward:  1.214 [-3.000, 32.940], mean action: 12.704 [1.000, 17.000],  loss: 0.015046, mae: 0.767490, mean_q: 0.054567, mean_eps: 0.000000
 4132/10000: episode: 120, duration: 0.903s, episode steps:  59, steps per second:  65, episode reward: -38.880, mean reward: -0.659 [-32.150,  2.592], mean action: 7.068 [1.000, 21.000],  loss: 0.014087, mae: 0.790224, mean_q: 0.020689, mean_eps: 0.000000
 4155/10000: episode: 121, duration: 0.347s, episode steps:  23, steps per second:  66, episode reward: -38.410, mean reward: -1.670 [-32.530,  2.830], mean action: 11.261 [1.000, 21.000],  loss: 0.016925, mae: 0.748574, mean_q: 0.095856, mean_eps: 0.000000
 4180/10000: episode: 122, duration: 0.332s, episode steps:  25, steps per second:  75, episode reward: 35.065, mean reward:  1.403 [-2.628, 32.240], mean action: 12.240 [1.000, 21.000],  loss: 0.013809, mae: 0.728885, mean_q: 0.100447, mean_eps: 0.000000
 4203/10000: episode: 123, duration: 0.318s, episode steps:  23, steps per second:  72, episode reward: -45.000, mean reward: -1.957 [-32.311,  1.694], mean action: 13.043 [1.000, 21.000],  loss: 0.018300, mae: 0.776037, mean_q: 0.064779, mean_eps: 0.000000
 4230/10000: episode: 124, duration: 0.408s, episode steps:  27, steps per second:  66, episode reward: -35.770, mean reward: -1.325 [-32.232,  2.460], mean action: 11.148 [1.000, 16.000],  loss: 0.019230, mae: 0.749765, mean_q: 0.113086, mean_eps: 0.000000
 4256/10000: episode: 125, duration: 0.385s, episode steps:  26, steps per second:  68, episode reward: -35.820, mean reward: -1.378 [-32.194,  3.000], mean action: 14.577 [1.000, 21.000],  loss: 0.016616, mae: 0.737163, mean_q: 0.113088, mean_eps: 0.000000
 4283/10000: episode: 126, duration: 0.332s, episode steps:  27, steps per second:  81, episode reward: -33.000, mean reward: -1.222 [-32.292,  3.000], mean action: 12.481 [1.000, 21.000],  loss: 0.017406, mae: 0.763224, mean_q: 0.065464, mean_eps: 0.000000
 4309/10000: episode: 127, duration: 0.352s, episode steps:  26, steps per second:  74, episode reward: -38.840, mean reward: -1.494 [-32.159,  2.562], mean action: 13.269 [3.000, 21.000],  loss: 0.017795, mae: 0.807733, mean_q: 0.032849, mean_eps: 0.000000
 4339/10000: episode: 128, duration: 0.438s, episode steps:  30, steps per second:  69, episode reward: -32.170, mean reward: -1.072 [-31.955,  2.755], mean action: 7.767 [1.000, 17.000],  loss: 0.014865, mae: 0.779411, mean_q: 0.047946, mean_eps: 0.000000
 4369/10000: episode: 129, duration: 0.452s, episode steps:  30, steps per second:  66, episode reward: 37.568, mean reward:  1.252 [-3.000, 32.680], mean action: 4.633 [1.000, 16.000],  loss: 0.016055, mae: 0.732649, mean_q: 0.098109, mean_eps: 0.000000
 4416/10000: episode: 130, duration: 0.637s, episode steps:  47, steps per second:  74, episode reward: 35.017, mean reward:  0.745 [-2.474, 32.215], mean action: 7.872 [1.000, 21.000],  loss: 0.015964, mae: 0.761190, mean_q: 0.100199, mean_eps: 0.000000
 4452/10000: episode: 131, duration: 0.510s, episode steps:  36, steps per second:  71, episode reward: 32.716, mean reward:  0.909 [-3.000, 32.370], mean action: 9.500 [1.000, 21.000],  loss: 0.019131, mae: 0.744646, mean_q: 0.140278, mean_eps: 0.000000
 4478/10000: episode: 132, duration: 0.389s, episode steps:  26, steps per second:  67, episode reward: -38.970, mean reward: -1.499 [-32.031,  2.227], mean action: 10.000 [1.000, 17.000],  loss: 0.018744, mae: 0.772751, mean_q: 0.111854, mean_eps: 0.000000
 4522/10000: episode: 133, duration: 0.653s, episode steps:  44, steps per second:  67, episode reward: 37.494, mean reward:  0.852 [-2.158, 32.312], mean action: 12.795 [1.000, 21.000],  loss: 0.015656, mae: 0.780608, mean_q: 0.079486, mean_eps: 0.000000
 4559/10000: episode: 134, duration: 0.501s, episode steps:  37, steps per second:  74, episode reward: -33.000, mean reward: -0.892 [-32.019,  2.330], mean action: 7.595 [1.000, 16.000],  loss: 0.017593, mae: 0.782515, mean_q: 0.121120, mean_eps: 0.000000
 4591/10000: episode: 135, duration: 0.432s, episode steps:  32, steps per second:  74, episode reward: 35.313, mean reward:  1.104 [-2.183, 32.120], mean action: 11.156 [1.000, 21.000],  loss: 0.013295, mae: 0.695485, mean_q: 0.162850, mean_eps: 0.000000
 4606/10000: episode: 136, duration: 0.205s, episode steps:  15, steps per second:  73, episode reward: 41.608, mean reward:  2.774 [-2.489, 32.440], mean action: 5.333 [1.000, 17.000],  loss: 0.016412, mae: 0.711136, mean_q: 0.182023, mean_eps: 0.000000
 4639/10000: episode: 137, duration: 0.470s, episode steps:  33, steps per second:  70, episode reward: -38.730, mean reward: -1.174 [-32.524,  2.903], mean action: 12.636 [3.000, 17.000],  loss: 0.017043, mae: 0.735525, mean_q: 0.144420, mean_eps: 0.000000
 4668/10000: episode: 138, duration: 0.430s, episode steps:  29, steps per second:  67, episode reward: 38.143, mean reward:  1.315 [-3.000, 32.290], mean action: 6.483 [1.000, 21.000],  loss: 0.015623, mae: 0.738133, mean_q: 0.106023, mean_eps: 0.000000
 4700/10000: episode: 139, duration: 0.435s, episode steps:  32, steps per second:  74, episode reward: -32.250, mean reward: -1.008 [-31.445,  2.210], mean action: 9.875 [1.000, 17.000],  loss: 0.015135, mae: 0.757501, mean_q: 0.058852, mean_eps: 0.000000
 4730/10000: episode: 140, duration: 0.466s, episode steps:  30, steps per second:  64, episode reward: 33.000, mean reward:  1.100 [-3.000, 32.760], mean action: 7.867 [1.000, 21.000],  loss: 0.017452, mae: 0.721043, mean_q: 0.132022, mean_eps: 0.000000
 4762/10000: episode: 141, duration: 0.456s, episode steps:  32, steps per second:  70, episode reward: 32.492, mean reward:  1.015 [-3.000, 32.290], mean action: 13.094 [1.000, 21.000],  loss: 0.017960, mae: 0.750892, mean_q: 0.111852, mean_eps: 0.000000
 4792/10000: episode: 142, duration: 0.377s, episode steps:  30, steps per second:  80, episode reward: -36.000, mean reward: -1.200 [-32.137,  2.950], mean action: 7.400 [1.000, 17.000],  loss: 0.021329, mae: 0.785582, mean_q: 0.094824, mean_eps: 0.000000
 4824/10000: episode: 143, duration: 0.416s, episode steps:  32, steps per second:  77, episode reward: 41.230, mean reward:  1.288 [-2.356, 32.080], mean action: 7.688 [1.000, 21.000],  loss: 0.013648, mae: 0.739622, mean_q: 0.099602, mean_eps: 0.000000
 4844/10000: episode: 144, duration: 0.309s, episode steps:  20, steps per second:  65, episode reward: 34.970, mean reward:  1.749 [-2.708, 32.504], mean action: 8.200 [1.000, 17.000],  loss: 0.019849, mae: 0.780944, mean_q: 0.092465, mean_eps: 0.000000
 4870/10000: episode: 145, duration: 0.398s, episode steps:  26, steps per second:  65, episode reward: 35.752, mean reward:  1.375 [-3.000, 33.000], mean action: 7.231 [1.000, 16.000],  loss: 0.013333, mae: 0.726898, mean_q: 0.114185, mean_eps: 0.000000
 4924/10000: episode: 146, duration: 0.681s, episode steps:  54, steps per second:  79, episode reward: 38.108, mean reward:  0.706 [-2.184, 32.108], mean action: 4.185 [1.000, 16.000],  loss: 0.018984, mae: 0.769794, mean_q: 0.101307, mean_eps: 0.000000
 4976/10000: episode: 147, duration: 0.711s, episode steps:  52, steps per second:  73, episode reward: -41.040, mean reward: -0.789 [-31.978,  2.160], mean action: 15.346 [1.000, 17.000],  loss: 0.017135, mae: 0.758156, mean_q: 0.095238, mean_eps: 0.000000
 4998/10000: episode: 148, duration: 0.325s, episode steps:  22, steps per second:  68, episode reward: -38.320, mean reward: -1.742 [-32.345,  2.520], mean action: 8.182 [1.000, 17.000],  loss: 0.009529, mae: 0.728350, mean_q: 0.134553, mean_eps: 0.000000
 5041/10000: episode: 149, duration: 0.586s, episode steps:  43, steps per second:  73, episode reward: 34.858, mean reward:  0.811 [-2.770, 32.160], mean action: 10.279 [1.000, 21.000],  loss: 0.018070, mae: 0.736309, mean_q: 0.148461, mean_eps: 0.000000
 5077/10000: episode: 150, duration: 0.490s, episode steps:  36, steps per second:  73, episode reward: 32.511, mean reward:  0.903 [-3.000, 32.060], mean action: 12.472 [1.000, 17.000],  loss: 0.016833, mae: 0.748166, mean_q: 0.139205, mean_eps: 0.000000
 5118/10000: episode: 151, duration: 0.558s, episode steps:  41, steps per second:  74, episode reward: -32.200, mean reward: -0.785 [-31.493,  3.120], mean action: 8.390 [1.000, 21.000],  loss: 0.015728, mae: 0.723647, mean_q: 0.149011, mean_eps: 0.000000
 5138/10000: episode: 152, duration: 0.306s, episode steps:  20, steps per second:  65, episode reward: 40.878, mean reward:  2.044 [-2.267, 32.910], mean action: 7.400 [1.000, 21.000],  loss: 0.013632, mae: 0.700639, mean_q: 0.112226, mean_eps: 0.000000
 5161/10000: episode: 153, duration: 0.311s, episode steps:  23, steps per second:  74, episode reward: 37.456, mean reward:  1.629 [-2.266, 32.485], mean action: 6.957 [1.000, 16.000],  loss: 0.018112, mae: 0.728590, mean_q: 0.156437, mean_eps: 0.000000
 5184/10000: episode: 154, duration: 0.343s, episode steps:  23, steps per second:  67, episode reward: -35.260, mean reward: -1.533 [-31.940,  3.061], mean action: 11.174 [1.000, 21.000],  loss: 0.015996, mae: 0.712291, mean_q: 0.136850, mean_eps: 0.000000
 5240/10000: episode: 155, duration: 0.898s, episode steps:  56, steps per second:  62, episode reward: -33.000, mean reward: -0.589 [-32.111,  3.000], mean action: 5.518 [1.000, 17.000],  loss: 0.016576, mae: 0.704075, mean_q: 0.149772, mean_eps: 0.000000
 5279/10000: episode: 156, duration: 0.539s, episode steps:  39, steps per second:  72, episode reward: -32.940, mean reward: -0.845 [-32.066,  2.640], mean action: 14.538 [1.000, 17.000],  loss: 0.016067, mae: 0.729734, mean_q: 0.111265, mean_eps: 0.000000
 5308/10000: episode: 157, duration: 0.423s, episode steps:  29, steps per second:  68, episode reward: 32.712, mean reward:  1.128 [-2.624, 32.220], mean action: 8.310 [1.000, 21.000],  loss: 0.017066, mae: 0.712383, mean_q: 0.149821, mean_eps: 0.000000
 5342/10000: episode: 158, duration: 0.572s, episode steps:  34, steps per second:  59, episode reward: -38.590, mean reward: -1.135 [-32.230,  2.410], mean action: 8.176 [1.000, 21.000],  loss: 0.015861, mae: 0.728856, mean_q: 0.125766, mean_eps: 0.000000
 5374/10000: episode: 159, duration: 0.411s, episode steps:  32, steps per second:  78, episode reward: 32.305, mean reward:  1.010 [-2.454, 32.170], mean action: 8.469 [1.000, 17.000],  loss: 0.018437, mae: 0.701715, mean_q: 0.176095, mean_eps: 0.000000
 5414/10000: episode: 160, duration: 0.663s, episode steps:  40, steps per second:  60, episode reward: -35.680, mean reward: -0.892 [-31.745,  2.250], mean action: 11.100 [1.000, 17.000],  loss: 0.018026, mae: 0.691793, mean_q: 0.152954, mean_eps: 0.000000
 5441/10000: episode: 161, duration: 0.469s, episode steps:  27, steps per second:  58, episode reward: -38.870, mean reward: -1.440 [-32.140,  2.238], mean action: 10.333 [3.000, 17.000],  loss: 0.017442, mae: 0.696730, mean_q: 0.138948, mean_eps: 0.000000
 5482/10000: episode: 162, duration: 0.645s, episode steps:  41, steps per second:  64, episode reward: 41.917, mean reward:  1.022 [-2.242, 32.327], mean action: 10.366 [1.000, 17.000],  loss: 0.015045, mae: 0.723328, mean_q: 0.084737, mean_eps: 0.000000
 5515/10000: episode: 163, duration: 0.477s, episode steps:  33, steps per second:  69, episode reward: -32.440, mean reward: -0.983 [-31.551,  2.262], mean action: 11.061 [1.000, 21.000],  loss: 0.015120, mae: 0.671724, mean_q: 0.163979, mean_eps: 0.000000
 5539/10000: episode: 164, duration: 0.319s, episode steps:  24, steps per second:  75, episode reward: -41.310, mean reward: -1.721 [-32.215,  2.470], mean action: 12.833 [1.000, 21.000],  loss: 0.017093, mae: 0.701916, mean_q: 0.127215, mean_eps: 0.000000
 5561/10000: episode: 165, duration: 0.337s, episode steps:  22, steps per second:  65, episode reward: -38.010, mean reward: -1.728 [-31.348,  3.000], mean action: 7.227 [1.000, 17.000],  loss: 0.018769, mae: 0.755846, mean_q: 0.085824, mean_eps: 0.000000
 5639/10000: episode: 166, duration: 1.100s, episode steps:  78, steps per second:  71, episode reward: -35.720, mean reward: -0.458 [-31.970,  2.498], mean action: 15.077 [1.000, 21.000],  loss: 0.015073, mae: 0.693518, mean_q: 0.145165, mean_eps: 0.000000
 5672/10000: episode: 167, duration: 0.454s, episode steps:  33, steps per second:  73, episode reward: -32.260, mean reward: -0.978 [-30.042,  2.961], mean action: 10.394 [1.000, 17.000],  loss: 0.016655, mae: 0.710495, mean_q: 0.160531, mean_eps: 0.000000
 5694/10000: episode: 168, duration: 0.285s, episode steps:  22, steps per second:  77, episode reward: -35.910, mean reward: -1.632 [-31.989,  2.612], mean action: 8.955 [1.000, 21.000],  loss: 0.013983, mae: 0.695315, mean_q: 0.155133, mean_eps: 0.000000
 5715/10000: episode: 169, duration: 0.277s, episode steps:  21, steps per second:  76, episode reward: 38.673, mean reward:  1.842 [-2.633, 32.380], mean action: 9.048 [1.000, 21.000],  loss: 0.016408, mae: 0.701647, mean_q: 0.172575, mean_eps: 0.000000
 5736/10000: episode: 170, duration: 0.275s, episode steps:  21, steps per second:  76, episode reward: -44.120, mean reward: -2.101 [-32.022,  2.072], mean action: 13.762 [3.000, 17.000],  loss: 0.018575, mae: 0.711571, mean_q: 0.176339, mean_eps: 0.000000
 5758/10000: episode: 171, duration: 0.312s, episode steps:  22, steps per second:  70, episode reward: 44.397, mean reward:  2.018 [-2.068, 32.420], mean action: 5.545 [1.000, 21.000],  loss: 0.017379, mae: 0.744083, mean_q: 0.123091, mean_eps: 0.000000
 5777/10000: episode: 172, duration: 0.278s, episode steps:  19, steps per second:  68, episode reward: -35.490, mean reward: -1.868 [-33.000,  3.000], mean action: 10.895 [1.000, 16.000],  loss: 0.017637, mae: 0.673063, mean_q: 0.192230, mean_eps: 0.000000
 5813/10000: episode: 173, duration: 0.454s, episode steps:  36, steps per second:  79, episode reward: -32.570, mean reward: -0.905 [-31.937,  2.744], mean action: 10.611 [1.000, 21.000],  loss: 0.019277, mae: 0.741860, mean_q: 0.110278, mean_eps: 0.000000
 5851/10000: episode: 174, duration: 0.555s, episode steps:  38, steps per second:  68, episode reward: -38.560, mean reward: -1.015 [-32.244,  2.872], mean action: 8.132 [1.000, 17.000],  loss: 0.017782, mae: 0.732727, mean_q: 0.113324, mean_eps: 0.000000
 5875/10000: episode: 175, duration: 0.331s, episode steps:  24, steps per second:  72, episode reward: -32.630, mean reward: -1.360 [-32.139,  3.000], mean action: 11.333 [1.000, 17.000],  loss: 0.018659, mae: 0.749852, mean_q: 0.119422, mean_eps: 0.000000
 5911/10000: episode: 176, duration: 0.506s, episode steps:  36, steps per second:  71, episode reward: 39.000, mean reward:  1.083 [-2.363, 32.020], mean action: 5.917 [1.000, 17.000],  loss: 0.019599, mae: 0.759493, mean_q: 0.108687, mean_eps: 0.000000
 5941/10000: episode: 177, duration: 0.455s, episode steps:  30, steps per second:  66, episode reward: -32.240, mean reward: -1.075 [-32.037,  2.927], mean action: 7.233 [1.000, 21.000],  loss: 0.016770, mae: 0.721259, mean_q: 0.129102, mean_eps: 0.000000
 6002/10000: episode: 178, duration: 0.837s, episode steps:  61, steps per second:  73, episode reward: -33.000, mean reward: -0.541 [-32.286,  3.000], mean action: 4.820 [1.000, 21.000],  loss: 0.018503, mae: 0.701889, mean_q: 0.146351, mean_eps: 0.000000
 6038/10000: episode: 179, duration: 0.474s, episode steps:  36, steps per second:  76, episode reward: -32.340, mean reward: -0.898 [-32.098,  3.000], mean action: 5.750 [1.000, 21.000],  loss: 0.018642, mae: 0.715311, mean_q: 0.133224, mean_eps: 0.000000
 6059/10000: episode: 180, duration: 0.255s, episode steps:  21, steps per second:  82, episode reward: -40.790, mean reward: -1.942 [-32.385,  2.260], mean action: 11.476 [1.000, 21.000],  loss: 0.022311, mae: 0.717363, mean_q: 0.145390, mean_eps: 0.000000
 6074/10000: episode: 181, duration: 0.244s, episode steps:  15, steps per second:  61, episode reward: 41.445, mean reward:  2.763 [-2.449, 33.000], mean action: 3.267 [1.000, 16.000],  loss: 0.014748, mae: 0.685069, mean_q: 0.157517, mean_eps: 0.000000
 6114/10000: episode: 182, duration: 0.529s, episode steps:  40, steps per second:  76, episode reward: 33.000, mean reward:  0.825 [-3.000, 32.520], mean action: 4.925 [1.000, 21.000],  loss: 0.016661, mae: 0.671872, mean_q: 0.190955, mean_eps: 0.000000
 6141/10000: episode: 183, duration: 0.425s, episode steps:  27, steps per second:  63, episode reward: -35.450, mean reward: -1.313 [-31.631,  2.540], mean action: 7.556 [1.000, 16.000],  loss: 0.013462, mae: 0.650539, mean_q: 0.186629, mean_eps: 0.000000
 6243/10000: episode: 184, duration: 1.286s, episode steps: 102, steps per second:  79, episode reward: -35.030, mean reward: -0.343 [-31.846,  2.903], mean action: 3.118 [1.000, 21.000],  loss: 0.017056, mae: 0.654989, mean_q: 0.200412, mean_eps: 0.000000
 6268/10000: episode: 185, duration: 0.336s, episode steps:  25, steps per second:  74, episode reward: -41.880, mean reward: -1.675 [-32.457,  2.500], mean action: 11.480 [1.000, 21.000],  loss: 0.016379, mae: 0.692457, mean_q: 0.162219, mean_eps: 0.000000
 6294/10000: episode: 186, duration: 0.325s, episode steps:  26, steps per second:  80, episode reward: -32.880, mean reward: -1.265 [-32.617,  2.567], mean action: 13.423 [1.000, 21.000],  loss: 0.014965, mae: 0.666588, mean_q: 0.192358, mean_eps: 0.000000
 6317/10000: episode: 187, duration: 0.326s, episode steps:  23, steps per second:  70, episode reward: 38.751, mean reward:  1.685 [-2.325, 32.270], mean action: 8.087 [1.000, 17.000],  loss: 0.016298, mae: 0.722929, mean_q: 0.112157, mean_eps: 0.000000
 6359/10000: episode: 188, duration: 0.642s, episode steps:  42, steps per second:  65, episode reward: -32.350, mean reward: -0.770 [-31.956,  2.606], mean action: 12.262 [1.000, 21.000],  loss: 0.015571, mae: 0.695052, mean_q: 0.145548, mean_eps: 0.000000
 6386/10000: episode: 189, duration: 0.386s, episode steps:  27, steps per second:  70, episode reward: -32.130, mean reward: -1.190 [-32.018,  2.354], mean action: 8.185 [1.000, 21.000],  loss: 0.016892, mae: 0.707848, mean_q: 0.117294, mean_eps: 0.000000
 6417/10000: episode: 190, duration: 0.459s, episode steps:  31, steps per second:  68, episode reward: 35.653, mean reward:  1.150 [-3.000, 31.723], mean action: 7.774 [1.000, 21.000],  loss: 0.015362, mae: 0.672441, mean_q: 0.203265, mean_eps: 0.000000
 6444/10000: episode: 191, duration: 0.410s, episode steps:  27, steps per second:  66, episode reward: 32.322, mean reward:  1.197 [-2.431, 31.949], mean action: 7.519 [1.000, 21.000],  loss: 0.015691, mae: 0.659743, mean_q: 0.225892, mean_eps: 0.000000
 6467/10000: episode: 192, duration: 0.277s, episode steps:  23, steps per second:  83, episode reward: -41.640, mean reward: -1.810 [-31.835,  2.140], mean action: 14.174 [3.000, 21.000],  loss: 0.013867, mae: 0.665012, mean_q: 0.206196, mean_eps: 0.000000
 6495/10000: episode: 193, duration: 0.388s, episode steps:  28, steps per second:  72, episode reward: -32.370, mean reward: -1.156 [-32.049,  2.490], mean action: 12.214 [1.000, 21.000],  loss: 0.013511, mae: 0.662464, mean_q: 0.206014, mean_eps: 0.000000
 6524/10000: episode: 194, duration: 0.395s, episode steps:  29, steps per second:  73, episode reward: 32.449, mean reward:  1.119 [-3.000, 32.040], mean action: 10.276 [1.000, 21.000],  loss: 0.016516, mae: 0.679358, mean_q: 0.220963, mean_eps: 0.000000
 6538/10000: episode: 195, duration: 0.180s, episode steps:  14, steps per second:  78, episode reward: 44.197, mean reward:  3.157 [-2.227, 32.370], mean action: 2.357 [1.000, 16.000],  loss: 0.013535, mae: 0.651895, mean_q: 0.264426, mean_eps: 0.000000
 6574/10000: episode: 196, duration: 0.482s, episode steps:  36, steps per second:  75, episode reward: 32.087, mean reward:  0.891 [-2.231, 32.230], mean action: 9.583 [1.000, 21.000],  loss: 0.015312, mae: 0.685879, mean_q: 0.176843, mean_eps: 0.000000
 6597/10000: episode: 197, duration: 0.320s, episode steps:  23, steps per second:  72, episode reward: 35.302, mean reward:  1.535 [-2.496, 32.230], mean action: 13.217 [1.000, 21.000],  loss: 0.012573, mae: 0.685421, mean_q: 0.146122, mean_eps: 0.000000
 6622/10000: episode: 198, duration: 0.313s, episode steps:  25, steps per second:  80, episode reward: 35.633, mean reward:  1.425 [-2.473, 32.171], mean action: 7.280 [1.000, 21.000],  loss: 0.014842, mae: 0.662185, mean_q: 0.180915, mean_eps: 0.000000
 6655/10000: episode: 199, duration: 0.444s, episode steps:  33, steps per second:  74, episode reward: -32.330, mean reward: -0.980 [-32.028,  2.740], mean action: 7.364 [1.000, 17.000],  loss: 0.017723, mae: 0.710585, mean_q: 0.179406, mean_eps: 0.000000
 6676/10000: episode: 200, duration: 0.309s, episode steps:  21, steps per second:  68, episode reward: 34.505, mean reward:  1.643 [-2.350, 31.753], mean action: 6.190 [1.000, 17.000],  loss: 0.017795, mae: 0.766219, mean_q: 0.087981, mean_eps: 0.000000
 6701/10000: episode: 201, duration: 0.406s, episode steps:  25, steps per second:  62, episode reward: 32.080, mean reward:  1.283 [-2.390, 32.020], mean action: 9.920 [1.000, 21.000],  loss: 0.020132, mae: 0.720878, mean_q: 0.137008, mean_eps: 0.000000
 6722/10000: episode: 202, duration: 0.284s, episode steps:  21, steps per second:  74, episode reward: 38.471, mean reward:  1.832 [-3.000, 32.140], mean action: 7.762 [1.000, 21.000],  loss: 0.012673, mae: 0.658249, mean_q: 0.178078, mean_eps: 0.000000
 6747/10000: episode: 203, duration: 0.369s, episode steps:  25, steps per second:  68, episode reward: 37.504, mean reward:  1.500 [-2.956, 32.072], mean action: 3.720 [1.000, 16.000],  loss: 0.015278, mae: 0.664363, mean_q: 0.179851, mean_eps: 0.000000
 6771/10000: episode: 204, duration: 0.337s, episode steps:  24, steps per second:  71, episode reward: -35.490, mean reward: -1.479 [-32.365,  3.000], mean action: 12.250 [1.000, 17.000],  loss: 0.016959, mae: 0.693394, mean_q: 0.160415, mean_eps: 0.000000
 6796/10000: episode: 205, duration: 0.325s, episode steps:  25, steps per second:  77, episode reward: 40.814, mean reward:  1.633 [-2.128, 32.430], mean action: 4.080 [1.000, 21.000],  loss: 0.016497, mae: 0.689894, mean_q: 0.137465, mean_eps: 0.000000
 6824/10000: episode: 206, duration: 0.420s, episode steps:  28, steps per second:  67, episode reward: -33.000, mean reward: -1.179 [-32.490,  2.970], mean action: 6.036 [1.000, 17.000],  loss: 0.019383, mae: 0.664550, mean_q: 0.246632, mean_eps: 0.000000
 6852/10000: episode: 207, duration: 0.544s, episode steps:  28, steps per second:  51, episode reward: -35.410, mean reward: -1.265 [-32.044,  3.000], mean action: 7.429 [1.000, 16.000],  loss: 0.019001, mae: 0.691608, mean_q: 0.225434, mean_eps: 0.000000
 6881/10000: episode: 208, duration: 0.431s, episode steps:  29, steps per second:  67, episode reward: -32.910, mean reward: -1.135 [-32.058,  3.000], mean action: 10.000 [1.000, 21.000],  loss: 0.019540, mae: 0.710149, mean_q: 0.158050, mean_eps: 0.000000
 6902/10000: episode: 209, duration: 0.289s, episode steps:  21, steps per second:  73, episode reward: -38.530, mean reward: -1.835 [-33.000,  2.383], mean action: 10.286 [1.000, 17.000],  loss: 0.023160, mae: 0.693741, mean_q: 0.218036, mean_eps: 0.000000
 6928/10000: episode: 210, duration: 0.371s, episode steps:  26, steps per second:  70, episode reward: 32.680, mean reward:  1.257 [-2.278, 32.490], mean action: 8.769 [1.000, 21.000],  loss: 0.015825, mae: 0.677827, mean_q: 0.176366, mean_eps: 0.000000
 6958/10000: episode: 211, duration: 0.433s, episode steps:  30, steps per second:  69, episode reward: 33.000, mean reward:  1.100 [-3.000, 32.140], mean action: 11.600 [1.000, 17.000],  loss: 0.017898, mae: 0.664130, mean_q: 0.236794, mean_eps: 0.000000
 6991/10000: episode: 212, duration: 0.465s, episode steps:  33, steps per second:  71, episode reward: 38.216, mean reward:  1.158 [-3.000, 31.840], mean action: 5.273 [1.000, 16.000],  loss: 0.012058, mae: 0.607071, mean_q: 0.307115, mean_eps: 0.000000
 7018/10000: episode: 213, duration: 0.379s, episode steps:  27, steps per second:  71, episode reward: -35.410, mean reward: -1.311 [-32.411,  2.620], mean action: 6.704 [1.000, 17.000],  loss: 0.016282, mae: 0.683322, mean_q: 0.190888, mean_eps: 0.000000
 7061/10000: episode: 214, duration: 0.617s, episode steps:  43, steps per second:  70, episode reward: -35.740, mean reward: -0.831 [-32.094,  2.560], mean action: 4.442 [1.000, 17.000],  loss: 0.018722, mae: 0.710159, mean_q: 0.171492, mean_eps: 0.000000
 7083/10000: episode: 215, duration: 0.319s, episode steps:  22, steps per second:  69, episode reward: 35.371, mean reward:  1.608 [-2.675, 32.250], mean action: 5.273 [1.000, 17.000],  loss: 0.015801, mae: 0.690348, mean_q: 0.162269, mean_eps: 0.000000
 7106/10000: episode: 216, duration: 0.361s, episode steps:  23, steps per second:  64, episode reward: 38.095, mean reward:  1.656 [-2.361, 32.569], mean action: 6.174 [1.000, 21.000],  loss: 0.016823, mae: 0.710724, mean_q: 0.110119, mean_eps: 0.000000
 7140/10000: episode: 217, duration: 0.503s, episode steps:  34, steps per second:  68, episode reward: -32.910, mean reward: -0.968 [-32.015,  2.773], mean action: 8.853 [1.000, 21.000],  loss: 0.015220, mae: 0.673385, mean_q: 0.180702, mean_eps: 0.000000
 7159/10000: episode: 218, duration: 0.300s, episode steps:  19, steps per second:  63, episode reward: 38.707, mean reward:  2.037 [-2.552, 32.060], mean action: 5.842 [1.000, 16.000],  loss: 0.020783, mae: 0.748998, mean_q: 0.095701, mean_eps: 0.000000
 7187/10000: episode: 219, duration: 0.352s, episode steps:  28, steps per second:  80, episode reward: -35.400, mean reward: -1.264 [-31.855,  2.963], mean action: 5.357 [1.000, 17.000],  loss: 0.016777, mae: 0.679232, mean_q: 0.216908, mean_eps: 0.000000
 7220/10000: episode: 220, duration: 0.426s, episode steps:  33, steps per second:  77, episode reward: -32.660, mean reward: -0.990 [-32.296,  2.809], mean action: 9.424 [1.000, 21.000],  loss: 0.014737, mae: 0.653160, mean_q: 0.205702, mean_eps: 0.000000
 7253/10000: episode: 221, duration: 0.435s, episode steps:  33, steps per second:  76, episode reward: -35.750, mean reward: -1.083 [-32.006,  3.000], mean action: 11.545 [3.000, 17.000],  loss: 0.019070, mae: 0.706058, mean_q: 0.161030, mean_eps: 0.000000
 7281/10000: episode: 222, duration: 0.380s, episode steps:  28, steps per second:  74, episode reward: -38.140, mean reward: -1.362 [-32.650,  2.290], mean action: 5.000 [1.000, 17.000],  loss: 0.018428, mae: 0.691742, mean_q: 0.182244, mean_eps: 0.000000
 7316/10000: episode: 223, duration: 0.475s, episode steps:  35, steps per second:  74, episode reward: 34.672, mean reward:  0.991 [-2.273, 32.390], mean action: 5.486 [1.000, 17.000],  loss: 0.016536, mae: 0.695824, mean_q: 0.148315, mean_eps: 0.000000
 7348/10000: episode: 224, duration: 0.412s, episode steps:  32, steps per second:  78, episode reward: 33.000, mean reward:  1.031 [-3.000, 32.110], mean action: 9.344 [1.000, 21.000],  loss: 0.018411, mae: 0.700954, mean_q: 0.165879, mean_eps: 0.000000
 7381/10000: episode: 225, duration: 0.431s, episode steps:  33, steps per second:  77, episode reward: 32.762, mean reward:  0.993 [-2.877, 32.100], mean action: 11.909 [1.000, 17.000],  loss: 0.016315, mae: 0.674175, mean_q: 0.180202, mean_eps: 0.000000
 7405/10000: episode: 226, duration: 0.376s, episode steps:  24, steps per second:  64, episode reward: -32.300, mean reward: -1.346 [-30.630,  3.000], mean action: 8.458 [1.000, 17.000],  loss: 0.018914, mae: 0.669678, mean_q: 0.199844, mean_eps: 0.000000
 7446/10000: episode: 227, duration: 0.531s, episode steps:  41, steps per second:  77, episode reward: -32.530, mean reward: -0.793 [-32.066,  2.765], mean action: 10.561 [1.000, 17.000],  loss: 0.019142, mae: 0.657250, mean_q: 0.224971, mean_eps: 0.000000
 7473/10000: episode: 228, duration: 0.376s, episode steps:  27, steps per second:  72, episode reward: 30.000, mean reward:  1.111 [-2.388, 30.083], mean action: 9.074 [1.000, 17.000],  loss: 0.015944, mae: 0.666170, mean_q: 0.194144, mean_eps: 0.000000
 7503/10000: episode: 229, duration: 0.472s, episode steps:  30, steps per second:  64, episode reward: -38.140, mean reward: -1.271 [-32.001,  3.059], mean action: 6.933 [1.000, 17.000],  loss: 0.014187, mae: 0.656959, mean_q: 0.206966, mean_eps: 0.000000
 7524/10000: episode: 230, duration: 0.267s, episode steps:  21, steps per second:  79, episode reward: 32.047, mean reward:  1.526 [-2.641, 29.220], mean action: 11.000 [1.000, 21.000],  loss: 0.013573, mae: 0.651875, mean_q: 0.203538, mean_eps: 0.000000
 7564/10000: episode: 231, duration: 0.622s, episode steps:  40, steps per second:  64, episode reward: -35.710, mean reward: -0.893 [-32.027,  2.700], mean action: 7.475 [1.000, 17.000],  loss: 0.018164, mae: 0.675435, mean_q: 0.178588, mean_eps: 0.000000
 7587/10000: episode: 232, duration: 0.325s, episode steps:  23, steps per second:  71, episode reward: 37.558, mean reward:  1.633 [-3.000, 33.000], mean action: 8.478 [1.000, 21.000],  loss: 0.017783, mae: 0.699078, mean_q: 0.151216, mean_eps: 0.000000
 7620/10000: episode: 233, duration: 0.466s, episode steps:  33, steps per second:  71, episode reward: -35.160, mean reward: -1.065 [-32.040,  3.000], mean action: 9.545 [1.000, 21.000],  loss: 0.016633, mae: 0.642450, mean_q: 0.195478, mean_eps: 0.000000
 7646/10000: episode: 234, duration: 0.374s, episode steps:  26, steps per second:  69, episode reward: 42.726, mean reward:  1.643 [-2.315, 32.303], mean action: 3.654 [1.000, 16.000],  loss: 0.015194, mae: 0.671749, mean_q: 0.160779, mean_eps: 0.000000
 7669/10000: episode: 235, duration: 0.300s, episode steps:  23, steps per second:  77, episode reward: -36.000, mean reward: -1.565 [-32.182,  3.000], mean action: 11.435 [1.000, 21.000],  loss: 0.017644, mae: 0.671530, mean_q: 0.212489, mean_eps: 0.000000
 7699/10000: episode: 236, duration: 0.404s, episode steps:  30, steps per second:  74, episode reward: 32.129, mean reward:  1.071 [-3.000, 32.280], mean action: 10.600 [1.000, 17.000],  loss: 0.017095, mae: 0.661409, mean_q: 0.164066, mean_eps: 0.000000
 7727/10000: episode: 237, duration: 0.406s, episode steps:  28, steps per second:  69, episode reward: -33.000, mean reward: -1.179 [-32.309,  2.580], mean action: 8.393 [1.000, 17.000],  loss: 0.016341, mae: 0.673010, mean_q: 0.172013, mean_eps: 0.000000
 7749/10000: episode: 238, duration: 0.348s, episode steps:  22, steps per second:  63, episode reward: -32.540, mean reward: -1.479 [-32.418,  3.000], mean action: 11.409 [3.000, 21.000],  loss: 0.017911, mae: 0.707229, mean_q: 0.131265, mean_eps: 0.000000
 7777/10000: episode: 239, duration: 0.353s, episode steps:  28, steps per second:  79, episode reward: 38.667, mean reward:  1.381 [-3.000, 32.370], mean action: 3.321 [1.000, 16.000],  loss: 0.015468, mae: 0.635190, mean_q: 0.275778, mean_eps: 0.000000
 7807/10000: episode: 240, duration: 0.409s, episode steps:  30, steps per second:  73, episode reward: 36.990, mean reward:  1.233 [-2.280, 33.000], mean action: 5.867 [1.000, 21.000],  loss: 0.015508, mae: 0.668543, mean_q: 0.197150, mean_eps: 0.000000
 7828/10000: episode: 241, duration: 0.303s, episode steps:  21, steps per second:  69, episode reward: 32.323, mean reward:  1.539 [-3.000, 32.080], mean action: 6.952 [1.000, 17.000],  loss: 0.019349, mae: 0.662401, mean_q: 0.213993, mean_eps: 0.000000
 7863/10000: episode: 242, duration: 0.461s, episode steps:  35, steps per second:  76, episode reward: 35.429, mean reward:  1.012 [-2.701, 32.098], mean action: 7.429 [1.000, 21.000],  loss: 0.013429, mae: 0.626353, mean_q: 0.252092, mean_eps: 0.000000
 7884/10000: episode: 243, duration: 0.315s, episode steps:  21, steps per second:  67, episode reward: 38.919, mean reward:  1.853 [-2.412, 33.000], mean action: 6.714 [1.000, 17.000],  loss: 0.018333, mae: 0.663545, mean_q: 0.240312, mean_eps: 0.000000
 7905/10000: episode: 244, duration: 0.318s, episode steps:  21, steps per second:  66, episode reward: 36.000, mean reward:  1.714 [-2.900, 32.450], mean action: 6.524 [1.000, 16.000],  loss: 0.013760, mae: 0.674599, mean_q: 0.162455, mean_eps: 0.000000
 7929/10000: episode: 245, duration: 0.376s, episode steps:  24, steps per second:  64, episode reward: 38.798, mean reward:  1.617 [-2.689, 32.150], mean action: 5.708 [3.000, 21.000],  loss: 0.018587, mae: 0.670926, mean_q: 0.173188, mean_eps: 0.000000
 7953/10000: episode: 246, duration: 0.335s, episode steps:  24, steps per second:  72, episode reward: -32.550, mean reward: -1.356 [-31.819,  2.340], mean action: 6.292 [1.000, 17.000],  loss: 0.016627, mae: 0.670511, mean_q: 0.177885, mean_eps: 0.000000
 7979/10000: episode: 247, duration: 0.378s, episode steps:  26, steps per second:  69, episode reward: 38.902, mean reward:  1.496 [-2.175, 32.152], mean action: 4.269 [1.000, 21.000],  loss: 0.015972, mae: 0.680795, mean_q: 0.138444, mean_eps: 0.000000
 8001/10000: episode: 248, duration: 0.329s, episode steps:  22, steps per second:  67, episode reward: 40.063, mean reward:  1.821 [-2.494, 32.218], mean action: 5.318 [1.000, 21.000],  loss: 0.017049, mae: 0.636174, mean_q: 0.257255, mean_eps: 0.000000
 8038/10000: episode: 249, duration: 0.514s, episode steps:  37, steps per second:  72, episode reward: -32.280, mean reward: -0.872 [-32.263,  2.262], mean action: 11.243 [1.000, 16.000],  loss: 0.014859, mae: 0.618110, mean_q: 0.244011, mean_eps: 0.000000
 8073/10000: episode: 250, duration: 0.458s, episode steps:  35, steps per second:  76, episode reward: -38.740, mean reward: -1.107 [-32.692,  2.110], mean action: 6.343 [1.000, 17.000],  loss: 0.017813, mae: 0.639140, mean_q: 0.235817, mean_eps: 0.000000
 8098/10000: episode: 251, duration: 0.333s, episode steps:  25, steps per second:  75, episode reward: -32.710, mean reward: -1.308 [-32.299,  2.991], mean action: 8.440 [1.000, 21.000],  loss: 0.020076, mae: 0.632363, mean_q: 0.289640, mean_eps: 0.000000
 8121/10000: episode: 252, duration: 0.359s, episode steps:  23, steps per second:  64, episode reward: 37.574, mean reward:  1.634 [-2.418, 32.229], mean action: 4.739 [1.000, 21.000],  loss: 0.018007, mae: 0.660677, mean_q: 0.193703, mean_eps: 0.000000
 8147/10000: episode: 253, duration: 0.374s, episode steps:  26, steps per second:  70, episode reward: 34.968, mean reward:  1.345 [-2.359, 32.890], mean action: 8.154 [1.000, 17.000],  loss: 0.015359, mae: 0.600255, mean_q: 0.318073, mean_eps: 0.000000
 8172/10000: episode: 254, duration: 0.311s, episode steps:  25, steps per second:  80, episode reward: 32.451, mean reward:  1.298 [-2.499, 32.451], mean action: 8.880 [3.000, 16.000],  loss: 0.021868, mae: 0.659974, mean_q: 0.272707, mean_eps: 0.000000
 8210/10000: episode: 255, duration: 0.497s, episode steps:  38, steps per second:  76, episode reward: 32.057, mean reward:  0.844 [-3.000, 31.873], mean action: 8.579 [1.000, 16.000],  loss: 0.017179, mae: 0.684912, mean_q: 0.142577, mean_eps: 0.000000
 8234/10000: episode: 256, duration: 0.353s, episode steps:  24, steps per second:  68, episode reward: 38.340, mean reward:  1.598 [-2.344, 32.090], mean action: 6.208 [1.000, 21.000],  loss: 0.017145, mae: 0.595866, mean_q: 0.352851, mean_eps: 0.000000
 8280/10000: episode: 257, duration: 0.917s, episode steps:  46, steps per second:  50, episode reward: -35.520, mean reward: -0.772 [-32.109,  2.627], mean action: 5.109 [1.000, 21.000],  loss: 0.018037, mae: 0.586253, mean_q: 0.366925, mean_eps: 0.000000
 8312/10000: episode: 258, duration: 0.440s, episode steps:  32, steps per second:  73, episode reward: -32.370, mean reward: -1.012 [-32.071,  2.713], mean action: 9.062 [1.000, 17.000],  loss: 0.018723, mae: 0.634927, mean_q: 0.326621, mean_eps: 0.000000
 8344/10000: episode: 259, duration: 0.425s, episode steps:  32, steps per second:  75, episode reward: -32.640, mean reward: -1.020 [-32.023,  2.295], mean action: 8.438 [1.000, 17.000],  loss: 0.017178, mae: 0.648830, mean_q: 0.240174, mean_eps: 0.000000
 8370/10000: episode: 260, duration: 0.394s, episode steps:  26, steps per second:  66, episode reward: 32.663, mean reward:  1.256 [-3.000, 32.781], mean action: 6.308 [1.000, 17.000],  loss: 0.016045, mae: 0.654619, mean_q: 0.180907, mean_eps: 0.000000
 8397/10000: episode: 261, duration: 0.424s, episode steps:  27, steps per second:  64, episode reward: 36.915, mean reward:  1.367 [-3.000, 32.508], mean action: 6.667 [1.000, 17.000],  loss: 0.021407, mae: 0.609185, mean_q: 0.318295, mean_eps: 0.000000
 8432/10000: episode: 262, duration: 0.482s, episode steps:  35, steps per second:  73, episode reward: 35.162, mean reward:  1.005 [-2.668, 32.650], mean action: 11.143 [1.000, 21.000],  loss: 0.019535, mae: 0.647888, mean_q: 0.234386, mean_eps: 0.000000
 8458/10000: episode: 263, duration: 0.377s, episode steps:  26, steps per second:  69, episode reward: 35.429, mean reward:  1.363 [-2.527, 32.630], mean action: 7.923 [1.000, 21.000],  loss: 0.015173, mae: 0.622474, mean_q: 0.227370, mean_eps: 0.000000
 8483/10000: episode: 264, duration: 0.318s, episode steps:  25, steps per second:  79, episode reward: 37.605, mean reward:  1.504 [-2.505, 32.170], mean action: 5.280 [1.000, 21.000],  loss: 0.015772, mae: 0.656557, mean_q: 0.206339, mean_eps: 0.000000
 8516/10000: episode: 265, duration: 0.447s, episode steps:  33, steps per second:  74, episode reward: 35.468, mean reward:  1.075 [-2.493, 32.560], mean action: 5.758 [1.000, 17.000],  loss: 0.018849, mae: 0.634897, mean_q: 0.265317, mean_eps: 0.000000
 8545/10000: episode: 266, duration: 0.421s, episode steps:  29, steps per second:  69, episode reward: -35.520, mean reward: -1.225 [-31.930,  3.000], mean action: 5.414 [0.000, 17.000],  loss: 0.017317, mae: 0.634355, mean_q: 0.261781, mean_eps: 0.000000
 8571/10000: episode: 267, duration: 0.402s, episode steps:  26, steps per second:  65, episode reward: 32.172, mean reward:  1.237 [-2.282, 32.538], mean action: 7.000 [1.000, 21.000],  loss: 0.019501, mae: 0.654317, mean_q: 0.258454, mean_eps: 0.000000
 8588/10000: episode: 268, duration: 0.226s, episode steps:  17, steps per second:  75, episode reward: -41.830, mean reward: -2.461 [-32.118,  2.630], mean action: 6.529 [1.000, 17.000],  loss: 0.018733, mae: 0.597561, mean_q: 0.367234, mean_eps: 0.000000
 8612/10000: episode: 269, duration: 0.375s, episode steps:  24, steps per second:  64, episode reward: -35.620, mean reward: -1.484 [-32.453,  2.142], mean action: 9.333 [1.000, 21.000],  loss: 0.019537, mae: 0.624722, mean_q: 0.360320, mean_eps: 0.000000
 8647/10000: episode: 270, duration: 0.524s, episode steps:  35, steps per second:  67, episode reward: 33.000, mean reward:  0.943 [-2.471, 32.200], mean action: 11.343 [1.000, 21.000],  loss: 0.019257, mae: 0.649197, mean_q: 0.273482, mean_eps: 0.000000
 8674/10000: episode: 271, duration: 0.430s, episode steps:  27, steps per second:  63, episode reward: 33.000, mean reward:  1.222 [-3.000, 33.000], mean action: 10.667 [1.000, 17.000],  loss: 0.018548, mae: 0.656491, mean_q: 0.239175, mean_eps: 0.000000
 8690/10000: episode: 272, duration: 0.211s, episode steps:  16, steps per second:  76, episode reward: -35.720, mean reward: -2.232 [-32.910,  3.000], mean action: 9.438 [1.000, 21.000],  loss: 0.019559, mae: 0.630590, mean_q: 0.286402, mean_eps: 0.000000
 8706/10000: episode: 273, duration: 0.287s, episode steps:  16, steps per second:  56, episode reward: 44.438, mean reward:  2.777 [-2.093, 32.855], mean action: 6.062 [3.000, 21.000],  loss: 0.017188, mae: 0.622943, mean_q: 0.239481, mean_eps: 0.000000
 8731/10000: episode: 274, duration: 0.357s, episode steps:  25, steps per second:  70, episode reward: -35.270, mean reward: -1.411 [-31.862,  3.002], mean action: 10.240 [1.000, 21.000],  loss: 0.017768, mae: 0.659462, mean_q: 0.155154, mean_eps: 0.000000
 8758/10000: episode: 275, duration: 0.465s, episode steps:  27, steps per second:  58, episode reward: 32.525, mean reward:  1.205 [-2.431, 32.435], mean action: 6.074 [0.000, 21.000],  loss: 0.015437, mae: 0.651984, mean_q: 0.172402, mean_eps: 0.000000
 8778/10000: episode: 276, duration: 0.520s, episode steps:  20, steps per second:  38, episode reward: 38.720, mean reward:  1.936 [-2.413, 32.630], mean action: 5.750 [1.000, 21.000],  loss: 0.012866, mae: 0.599998, mean_q: 0.260621, mean_eps: 0.000000
 8801/10000: episode: 277, duration: 0.317s, episode steps:  23, steps per second:  73, episode reward: 35.270, mean reward:  1.533 [-2.901, 32.179], mean action: 5.696 [0.000, 17.000],  loss: 0.016940, mae: 0.591252, mean_q: 0.329558, mean_eps: 0.000000
 8834/10000: episode: 278, duration: 0.553s, episode steps:  33, steps per second:  60, episode reward: 32.041, mean reward:  0.971 [-2.224, 32.110], mean action: 6.879 [1.000, 17.000],  loss: 0.015669, mae: 0.602227, mean_q: 0.305982, mean_eps: 0.000000
 8863/10000: episode: 279, duration: 0.419s, episode steps:  29, steps per second:  69, episode reward: 38.543, mean reward:  1.329 [-2.534, 32.120], mean action: 3.828 [0.000, 17.000],  loss: 0.018657, mae: 0.655217, mean_q: 0.221525, mean_eps: 0.000000
 8883/10000: episode: 280, duration: 0.333s, episode steps:  20, steps per second:  60, episode reward: 36.000, mean reward:  1.800 [-2.152, 32.240], mean action: 7.100 [0.000, 16.000],  loss: 0.016754, mae: 0.636296, mean_q: 0.241562, mean_eps: 0.000000
 8914/10000: episode: 281, duration: 0.502s, episode steps:  31, steps per second:  62, episode reward: -38.780, mean reward: -1.251 [-32.249,  2.679], mean action: 10.903 [0.000, 21.000],  loss: 0.019636, mae: 0.609113, mean_q: 0.303303, mean_eps: 0.000000
 8934/10000: episode: 282, duration: 0.290s, episode steps:  20, steps per second:  69, episode reward: 35.567, mean reward:  1.778 [-3.000, 32.741], mean action: 6.200 [0.000, 17.000],  loss: 0.015360, mae: 0.570263, mean_q: 0.301920, mean_eps: 0.000000
 8959/10000: episode: 283, duration: 0.363s, episode steps:  25, steps per second:  69, episode reward: -41.380, mean reward: -1.655 [-32.029,  2.200], mean action: 7.280 [0.000, 17.000],  loss: 0.018413, mae: 0.620785, mean_q: 0.215559, mean_eps: 0.000000
 8983/10000: episode: 284, duration: 0.393s, episode steps:  24, steps per second:  61, episode reward: 35.601, mean reward:  1.483 [-2.457, 32.190], mean action: 5.833 [0.000, 17.000],  loss: 0.017540, mae: 0.564669, mean_q: 0.388316, mean_eps: 0.000000
 8998/10000: episode: 285, duration: 0.226s, episode steps:  15, steps per second:  66, episode reward: 40.859, mean reward:  2.724 [-2.658, 32.344], mean action: 5.667 [0.000, 17.000],  loss: 0.015047, mae: 0.569501, mean_q: 0.358404, mean_eps: 0.000000
 9032/10000: episode: 286, duration: 0.556s, episode steps:  34, steps per second:  61, episode reward: -35.440, mean reward: -1.042 [-31.859,  2.425], mean action: 5.118 [0.000, 21.000],  loss: 0.019217, mae: 0.586255, mean_q: 0.330143, mean_eps: 0.000000
 9064/10000: episode: 287, duration: 0.511s, episode steps:  32, steps per second:  63, episode reward: 37.834, mean reward:  1.182 [-2.320, 32.324], mean action: 6.625 [0.000, 21.000],  loss: 0.014221, mae: 0.545659, mean_q: 0.383578, mean_eps: 0.000000
 9091/10000: episode: 288, duration: 0.423s, episode steps:  27, steps per second:  64, episode reward: 35.918, mean reward:  1.330 [-2.808, 33.000], mean action: 6.185 [0.000, 17.000],  loss: 0.018303, mae: 0.610979, mean_q: 0.292739, mean_eps: 0.000000
 9110/10000: episode: 289, duration: 0.308s, episode steps:  19, steps per second:  62, episode reward: 38.754, mean reward:  2.040 [-2.287, 31.774], mean action: 4.263 [0.000, 17.000],  loss: 0.017896, mae: 0.617688, mean_q: 0.250810, mean_eps: 0.000000
 9134/10000: episode: 290, duration: 0.360s, episode steps:  24, steps per second:  67, episode reward: -38.480, mean reward: -1.603 [-31.923,  2.904], mean action: 9.833 [1.000, 17.000],  loss: 0.020073, mae: 0.630243, mean_q: 0.246776, mean_eps: 0.000000
 9166/10000: episode: 291, duration: 0.468s, episode steps:  32, steps per second:  68, episode reward: 39.000, mean reward:  1.219 [-2.222, 32.200], mean action: 9.406 [0.000, 21.000],  loss: 0.019082, mae: 0.596194, mean_q: 0.297936, mean_eps: 0.000000
 9211/10000: episode: 292, duration: 0.637s, episode steps:  45, steps per second:  71, episode reward: -35.220, mean reward: -0.783 [-32.505,  2.930], mean action: 11.911 [1.000, 21.000],  loss: 0.016078, mae: 0.584097, mean_q: 0.314885, mean_eps: 0.000000
 9238/10000: episode: 293, duration: 0.365s, episode steps:  27, steps per second:  74, episode reward: -32.380, mean reward: -1.199 [-31.859,  2.657], mean action: 10.630 [0.000, 21.000],  loss: 0.014167, mae: 0.565620, mean_q: 0.305692, mean_eps: 0.000000
 9277/10000: episode: 294, duration: 0.597s, episode steps:  39, steps per second:  65, episode reward: -35.680, mean reward: -0.915 [-32.468,  2.343], mean action: 7.872 [0.000, 17.000],  loss: 0.015461, mae: 0.596492, mean_q: 0.255605, mean_eps: 0.000000
 9305/10000: episode: 295, duration: 0.390s, episode steps:  28, steps per second:  72, episode reward: 32.208, mean reward:  1.150 [-2.487, 32.020], mean action: 6.286 [0.000, 17.000],  loss: 0.019762, mae: 0.603804, mean_q: 0.305557, mean_eps: 0.000000
 9328/10000: episode: 296, duration: 0.321s, episode steps:  23, steps per second:  72, episode reward: 38.208, mean reward:  1.661 [-2.469, 33.000], mean action: 4.478 [0.000, 16.000],  loss: 0.015605, mae: 0.653619, mean_q: 0.184344, mean_eps: 0.000000
 9388/10000: episode: 297, duration: 0.861s, episode steps:  60, steps per second:  70, episode reward: -38.150, mean reward: -0.636 [-32.182,  2.230], mean action: 11.317 [3.000, 21.000],  loss: 0.017207, mae: 0.585194, mean_q: 0.337628, mean_eps: 0.000000
 9411/10000: episode: 298, duration: 0.310s, episode steps:  23, steps per second:  74, episode reward: 38.126, mean reward:  1.658 [-2.209, 32.340], mean action: 4.000 [0.000, 17.000],  loss: 0.017413, mae: 0.623009, mean_q: 0.255477, mean_eps: 0.000000
 9431/10000: episode: 299, duration: 0.305s, episode steps:  20, steps per second:  66, episode reward: 37.710, mean reward:  1.886 [-2.286, 32.620], mean action: 5.250 [1.000, 17.000],  loss: 0.017608, mae: 0.629524, mean_q: 0.214985, mean_eps: 0.000000
 9451/10000: episode: 300, duration: 0.249s, episode steps:  20, steps per second:  80, episode reward: 35.706, mean reward:  1.785 [-3.000, 32.706], mean action: 6.100 [1.000, 17.000],  loss: 0.016908, mae: 0.660077, mean_q: 0.181334, mean_eps: 0.000000
 9477/10000: episode: 301, duration: 0.540s, episode steps:  26, steps per second:  48, episode reward: 38.708, mean reward:  1.489 [-2.622, 32.130], mean action: 4.192 [1.000, 16.000],  loss: 0.016221, mae: 0.584263, mean_q: 0.339785, mean_eps: 0.000000
 9506/10000: episode: 302, duration: 0.473s, episode steps:  29, steps per second:  61, episode reward: 32.547, mean reward:  1.122 [-3.000, 32.530], mean action: 7.724 [0.000, 21.000],  loss: 0.018725, mae: 0.626144, mean_q: 0.272601, mean_eps: 0.000000
 9531/10000: episode: 303, duration: 0.361s, episode steps:  25, steps per second:  69, episode reward: 35.641, mean reward:  1.426 [-2.475, 32.120], mean action: 6.600 [0.000, 17.000],  loss: 0.017410, mae: 0.638286, mean_q: 0.237003, mean_eps: 0.000000
 9565/10000: episode: 304, duration: 0.518s, episode steps:  34, steps per second:  66, episode reward: -35.140, mean reward: -1.034 [-32.075,  2.241], mean action: 4.765 [1.000, 17.000],  loss: 0.017635, mae: 0.615444, mean_q: 0.279658, mean_eps: 0.000000
 9588/10000: episode: 305, duration: 0.411s, episode steps:  23, steps per second:  56, episode reward: 33.000, mean reward:  1.435 [-3.000, 32.230], mean action: 9.696 [0.000, 21.000],  loss: 0.018294, mae: 0.604539, mean_q: 0.292874, mean_eps: 0.000000
 9610/10000: episode: 306, duration: 0.316s, episode steps:  22, steps per second:  70, episode reward: 35.320, mean reward:  1.605 [-2.722, 32.620], mean action: 11.273 [0.000, 21.000],  loss: 0.021560, mae: 0.614030, mean_q: 0.308295, mean_eps: 0.000000
 9641/10000: episode: 307, duration: 0.529s, episode steps:  31, steps per second:  59, episode reward: -35.410, mean reward: -1.142 [-31.851,  2.270], mean action: 8.677 [0.000, 21.000],  loss: 0.018895, mae: 0.608437, mean_q: 0.296826, mean_eps: 0.000000
 9666/10000: episode: 308, duration: 0.412s, episode steps:  25, steps per second:  61, episode reward: -32.660, mean reward: -1.306 [-31.725,  2.282], mean action: 9.840 [1.000, 17.000],  loss: 0.018046, mae: 0.573799, mean_q: 0.348619, mean_eps: 0.000000
 9698/10000: episode: 309, duration: 0.491s, episode steps:  32, steps per second:  65, episode reward: -38.260, mean reward: -1.196 [-29.540,  2.142], mean action: 9.906 [0.000, 21.000],  loss: 0.018070, mae: 0.593438, mean_q: 0.278559, mean_eps: 0.000000
 9731/10000: episode: 310, duration: 0.567s, episode steps:  33, steps per second:  58, episode reward: -32.670, mean reward: -0.990 [-31.848,  2.210], mean action: 5.364 [1.000, 17.000],  loss: 0.016006, mae: 0.570092, mean_q: 0.325554, mean_eps: 0.000000
 9777/10000: episode: 311, duration: 0.684s, episode steps:  46, steps per second:  67, episode reward: -35.670, mean reward: -0.775 [-32.016,  2.780], mean action: 4.043 [0.000, 17.000],  loss: 0.017885, mae: 0.613288, mean_q: 0.278767, mean_eps: 0.000000
 9801/10000: episode: 312, duration: 0.315s, episode steps:  24, steps per second:  76, episode reward: 43.878, mean reward:  1.828 [-2.418, 33.000], mean action: 3.500 [0.000, 16.000],  loss: 0.019398, mae: 0.594652, mean_q: 0.314956, mean_eps: 0.000000
 9833/10000: episode: 313, duration: 0.442s, episode steps:  32, steps per second:  72, episode reward: -32.390, mean reward: -1.012 [-31.950,  2.810], mean action: 6.469 [0.000, 17.000],  loss: 0.016520, mae: 0.600866, mean_q: 0.272968, mean_eps: 0.000000
 9858/10000: episode: 314, duration: 0.340s, episode steps:  25, steps per second:  74, episode reward: -38.960, mean reward: -1.558 [-32.157,  2.424], mean action: 9.840 [0.000, 17.000],  loss: 0.018386, mae: 0.620041, mean_q: 0.287180, mean_eps: 0.000000
 9884/10000: episode: 315, duration: 0.437s, episode steps:  26, steps per second:  60, episode reward: 35.531, mean reward:  1.367 [-2.386, 31.891], mean action: 4.385 [0.000, 17.000],  loss: 0.016782, mae: 0.620623, mean_q: 0.240325, mean_eps: 0.000000
 9922/10000: episode: 316, duration: 0.528s, episode steps:  38, steps per second:  72, episode reward: 41.661, mean reward:  1.096 [-2.254, 32.680], mean action: 4.684 [0.000, 16.000],  loss: 0.016386, mae: 0.583488, mean_q: 0.268599, mean_eps: 0.000000
 9957/10000: episode: 317, duration: 0.469s, episode steps:  35, steps per second:  75, episode reward: 35.524, mean reward:  1.015 [-2.454, 32.013], mean action: 8.171 [0.000, 17.000],  loss: 0.017885, mae: 0.607621, mean_q: 0.272165, mean_eps: 0.000000
done, took 141.129 seconds
Results against random player:
DQN Evaluation: 278 victories out of 300 episodes

Results against max player:
DQN Evaluation: 174 victories out of 300 episodes
Training for 10000 steps ...
   26/10000: episode: 1, duration: 0.385s, episode steps:  26, steps per second:  67, episode reward: -41.040, mean reward: -1.578 [-31.766,  2.260], mean action: 7.769 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   68/10000: episode: 2, duration: 0.323s, episode steps:  42, steps per second: 130, episode reward: -41.750, mean reward: -0.994 [-32.238,  2.289], mean action: 8.429 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  110/10000: episode: 3, duration: 0.344s, episode steps:  42, steps per second: 122, episode reward: -33.000, mean reward: -0.786 [-32.026,  2.572], mean action: 15.262 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  143/10000: episode: 4, duration: 0.241s, episode steps:  33, steps per second: 137, episode reward: 32.329, mean reward:  0.980 [-3.000, 32.054], mean action: 12.455 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  171/10000: episode: 5, duration: 0.181s, episode steps:  28, steps per second: 154, episode reward: -47.320, mean reward: -1.690 [-32.067,  0.170], mean action: 11.714 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  249/10000: episode: 6, duration: 0.631s, episode steps:  78, steps per second: 124, episode reward: -38.930, mean reward: -0.499 [-32.308,  2.360], mean action: 12.231 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  298/10000: episode: 7, duration: 0.331s, episode steps:  49, steps per second: 148, episode reward: -33.000, mean reward: -0.673 [-32.433,  3.060], mean action: 13.245 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  323/10000: episode: 8, duration: 0.160s, episode steps:  25, steps per second: 157, episode reward: -38.240, mean reward: -1.530 [-32.444,  2.522], mean action: 10.920 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  356/10000: episode: 9, duration: 0.210s, episode steps:  33, steps per second: 157, episode reward: 32.445, mean reward:  0.983 [-2.606, 32.040], mean action: 8.212 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  385/10000: episode: 10, duration: 0.191s, episode steps:  29, steps per second: 152, episode reward: -39.000, mean reward: -1.345 [-29.077,  2.231], mean action: 11.724 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  417/10000: episode: 11, duration: 0.213s, episode steps:  32, steps per second: 150, episode reward: 32.250, mean reward:  1.008 [-2.606, 32.500], mean action: 12.281 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  451/10000: episode: 12, duration: 0.297s, episode steps:  34, steps per second: 114, episode reward: 35.930, mean reward:  1.057 [-3.000, 32.510], mean action: 10.794 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  482/10000: episode: 13, duration: 0.234s, episode steps:  31, steps per second: 132, episode reward: 37.741, mean reward:  1.217 [-2.477, 32.331], mean action: 14.097 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  513/10000: episode: 14, duration: 0.191s, episode steps:  31, steps per second: 162, episode reward: -44.620, mean reward: -1.439 [-31.837,  2.190], mean action: 15.839 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  548/10000: episode: 15, duration: 0.254s, episode steps:  35, steps per second: 138, episode reward: -35.740, mean reward: -1.021 [-32.031,  2.821], mean action: 12.914 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  572/10000: episode: 16, duration: 0.155s, episode steps:  24, steps per second: 155, episode reward: -44.580, mean reward: -1.857 [-32.009,  2.190], mean action: 8.375 [2.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  606/10000: episode: 17, duration: 0.221s, episode steps:  34, steps per second: 154, episode reward: -38.130, mean reward: -1.121 [-31.956,  3.000], mean action: 13.882 [4.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  638/10000: episode: 18, duration: 0.241s, episode steps:  32, steps per second: 133, episode reward: -35.880, mean reward: -1.121 [-32.115,  2.753], mean action: 12.094 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  671/10000: episode: 19, duration: 0.216s, episode steps:  33, steps per second: 152, episode reward: -44.570, mean reward: -1.351 [-31.954,  2.160], mean action: 14.091 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  718/10000: episode: 20, duration: 0.340s, episode steps:  47, steps per second: 138, episode reward: -32.710, mean reward: -0.696 [-32.106,  2.571], mean action: 12.404 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  747/10000: episode: 21, duration: 0.237s, episode steps:  29, steps per second: 122, episode reward: -44.190, mean reward: -1.524 [-32.122,  2.055], mean action: 12.586 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  771/10000: episode: 22, duration: 0.160s, episode steps:  24, steps per second: 150, episode reward: -41.400, mean reward: -1.725 [-32.251,  2.502], mean action: 12.875 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  791/10000: episode: 23, duration: 0.124s, episode steps:  20, steps per second: 162, episode reward: -41.710, mean reward: -2.086 [-31.735,  2.901], mean action: 13.050 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  825/10000: episode: 24, duration: 0.226s, episode steps:  34, steps per second: 150, episode reward: -36.000, mean reward: -1.059 [-32.059,  2.440], mean action: 13.382 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  859/10000: episode: 25, duration: 0.206s, episode steps:  34, steps per second: 165, episode reward: -38.490, mean reward: -1.132 [-32.132,  2.132], mean action: 12.265 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  887/10000: episode: 26, duration: 0.209s, episode steps:  28, steps per second: 134, episode reward: -38.940, mean reward: -1.391 [-32.181,  2.283], mean action: 12.750 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  958/10000: episode: 27, duration: 0.456s, episode steps:  71, steps per second: 156, episode reward: -41.100, mean reward: -0.579 [-32.352,  2.630], mean action: 7.606 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  992/10000: episode: 28, duration: 0.231s, episode steps:  34, steps per second: 147, episode reward: -41.860, mean reward: -1.231 [-32.309,  2.400], mean action: 14.941 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1030/10000: episode: 29, duration: 1.186s, episode steps:  38, steps per second:  32, episode reward: -41.500, mean reward: -1.092 [-32.003,  3.000], mean action: 15.421 [4.000, 20.000],  loss: 0.017168, mae: 0.513302, mean_q: 0.338921, mean_eps: 0.000000
 1063/10000: episode: 30, duration: 0.379s, episode steps:  33, steps per second:  87, episode reward: -41.080, mean reward: -1.245 [-32.129,  2.440], mean action: 13.697 [2.000, 20.000],  loss: 0.014217, mae: 0.614171, mean_q: 0.007640, mean_eps: 0.000000
 1112/10000: episode: 31, duration: 0.539s, episode steps:  49, steps per second:  91, episode reward: -32.450, mean reward: -0.662 [-32.042,  2.701], mean action: 14.816 [2.000, 18.000],  loss: 0.014209, mae: 0.628775, mean_q: -0.027449, mean_eps: 0.000000
 1140/10000: episode: 32, duration: 0.318s, episode steps:  28, steps per second:  88, episode reward: -40.130, mean reward: -1.433 [-32.119,  2.530], mean action: 14.286 [2.000, 18.000],  loss: 0.014884, mae: 0.605351, mean_q: 0.016821, mean_eps: 0.000000
 1184/10000: episode: 33, duration: 0.500s, episode steps:  44, steps per second:  88, episode reward: -41.510, mean reward: -0.943 [-32.105,  2.572], mean action: 9.227 [1.000, 19.000],  loss: 0.015030, mae: 0.601224, mean_q: 0.014772, mean_eps: 0.000000
 1217/10000: episode: 34, duration: 0.383s, episode steps:  33, steps per second:  86, episode reward: -38.100, mean reward: -1.155 [-32.097,  2.149], mean action: 14.182 [7.000, 20.000],  loss: 0.013677, mae: 0.598180, mean_q: -0.064103, mean_eps: 0.000000
 1249/10000: episode: 35, duration: 0.362s, episode steps:  32, steps per second:  88, episode reward: 32.208, mean reward:  1.006 [-2.431, 31.883], mean action: 15.875 [10.000, 20.000],  loss: 0.016103, mae: 0.630958, mean_q: -0.052058, mean_eps: 0.000000
 1273/10000: episode: 36, duration: 0.284s, episode steps:  24, steps per second:  84, episode reward: -41.150, mean reward: -1.715 [-32.094,  2.310], mean action: 13.292 [0.000, 20.000],  loss: 0.017564, mae: 0.663191, mean_q: -0.060256, mean_eps: 0.000000
 1317/10000: episode: 37, duration: 0.550s, episode steps:  44, steps per second:  80, episode reward: 32.784, mean reward:  0.745 [-2.606, 32.230], mean action: 14.682 [11.000, 15.000],  loss: 0.016860, mae: 0.648927, mean_q: -0.024261, mean_eps: 0.000000
 1356/10000: episode: 38, duration: 0.456s, episode steps:  39, steps per second:  86, episode reward: 33.000, mean reward:  0.846 [-2.248, 30.187], mean action: 13.949 [2.000, 20.000],  loss: 0.015086, mae: 0.640896, mean_q: -0.029363, mean_eps: 0.000000
 1385/10000: episode: 39, duration: 0.347s, episode steps:  29, steps per second:  84, episode reward: -38.660, mean reward: -1.333 [-32.366,  3.000], mean action: 14.966 [2.000, 20.000],  loss: 0.014477, mae: 0.605941, mean_q: -0.004221, mean_eps: 0.000000
 1410/10000: episode: 40, duration: 0.291s, episode steps:  25, steps per second:  86, episode reward: -44.080, mean reward: -1.763 [-32.004,  2.453], mean action: 13.600 [2.000, 20.000],  loss: 0.015024, mae: 0.587372, mean_q: 0.064155, mean_eps: 0.000000
 1440/10000: episode: 41, duration: 0.323s, episode steps:  30, steps per second:  93, episode reward: -41.080, mean reward: -1.369 [-32.007,  2.374], mean action: 15.733 [13.000, 20.000],  loss: 0.017107, mae: 0.643474, mean_q: -0.006663, mean_eps: 0.000000
 1476/10000: episode: 42, duration: 0.410s, episode steps:  36, steps per second:  88, episode reward: -35.750, mean reward: -0.993 [-32.401,  2.266], mean action: 14.556 [2.000, 20.000],  loss: 0.017135, mae: 0.633001, mean_q: 0.005217, mean_eps: 0.000000
 1514/10000: episode: 43, duration: 0.432s, episode steps:  38, steps per second:  88, episode reward: -35.270, mean reward: -0.928 [-32.020,  2.941], mean action: 13.421 [2.000, 20.000],  loss: 0.015931, mae: 0.642785, mean_q: -0.031772, mean_eps: 0.000000
 1538/10000: episode: 44, duration: 0.354s, episode steps:  24, steps per second:  68, episode reward: 35.368, mean reward:  1.474 [-3.000, 32.078], mean action: 10.125 [0.000, 15.000],  loss: 0.010972, mae: 0.616542, mean_q: -0.019408, mean_eps: 0.000000
 1570/10000: episode: 45, duration: 0.384s, episode steps:  32, steps per second:  83, episode reward: 34.961, mean reward:  1.093 [-2.903, 31.870], mean action: 10.969 [2.000, 20.000],  loss: 0.012434, mae: 0.594640, mean_q: 0.001621, mean_eps: 0.000000
 1602/10000: episode: 46, duration: 0.387s, episode steps:  32, steps per second:  83, episode reward: -35.040, mean reward: -1.095 [-31.758,  2.870], mean action: 13.750 [2.000, 20.000],  loss: 0.015689, mae: 0.657354, mean_q: -0.091643, mean_eps: 0.000000
 1624/10000: episode: 47, duration: 0.323s, episode steps:  22, steps per second:  68, episode reward: 37.868, mean reward:  1.721 [-3.000, 33.000], mean action: 9.682 [2.000, 16.000],  loss: 0.011516, mae: 0.604047, mean_q: 0.015436, mean_eps: 0.000000
 1645/10000: episode: 48, duration: 0.317s, episode steps:  21, steps per second:  66, episode reward: -38.570, mean reward: -1.837 [-32.053,  2.440], mean action: 10.667 [2.000, 20.000],  loss: 0.013943, mae: 0.665907, mean_q: -0.085770, mean_eps: 0.000000
 1679/10000: episode: 49, duration: 0.408s, episode steps:  34, steps per second:  83, episode reward: -41.560, mean reward: -1.222 [-31.834,  2.210], mean action: 17.176 [13.000, 20.000],  loss: 0.014284, mae: 0.599392, mean_q: -0.015619, mean_eps: 0.000000
 1709/10000: episode: 50, duration: 0.361s, episode steps:  30, steps per second:  83, episode reward: -33.000, mean reward: -1.100 [-32.451,  2.910], mean action: 11.333 [2.000, 18.000],  loss: 0.016373, mae: 0.611706, mean_q: 0.030253, mean_eps: 0.000000
 1746/10000: episode: 51, duration: 0.453s, episode steps:  37, steps per second:  82, episode reward: -39.000, mean reward: -1.054 [-32.566,  2.436], mean action: 8.703 [2.000, 20.000],  loss: 0.014790, mae: 0.620783, mean_q: -0.028457, mean_eps: 0.000000
 1792/10000: episode: 52, duration: 1.470s, episode steps:  46, steps per second:  31, episode reward: -38.200, mean reward: -0.830 [-32.034,  3.000], mean action: 13.652 [2.000, 16.000],  loss: 0.015166, mae: 0.645158, mean_q: -0.038129, mean_eps: 0.000000
 1820/10000: episode: 53, duration: 0.374s, episode steps:  28, steps per second:  75, episode reward: -41.060, mean reward: -1.466 [-32.213,  2.378], mean action: 11.036 [2.000, 20.000],  loss: 0.018575, mae: 0.702471, mean_q: -0.108207, mean_eps: 0.000000
 1852/10000: episode: 54, duration: 0.412s, episode steps:  32, steps per second:  78, episode reward: -39.000, mean reward: -1.219 [-32.569,  2.590], mean action: 13.875 [2.000, 20.000],  loss: 0.016405, mae: 0.658864, mean_q: -0.049205, mean_eps: 0.000000
 1873/10000: episode: 55, duration: 0.252s, episode steps:  21, steps per second:  83, episode reward: -41.280, mean reward: -1.966 [-31.921,  2.696], mean action: 11.000 [2.000, 15.000],  loss: 0.014411, mae: 0.683786, mean_q: -0.091068, mean_eps: 0.000000
 1893/10000: episode: 56, duration: 0.233s, episode steps:  20, steps per second:  86, episode reward: -41.310, mean reward: -2.066 [-32.493,  2.853], mean action: 13.000 [2.000, 20.000],  loss: 0.018288, mae: 0.695338, mean_q: -0.078253, mean_eps: 0.000000
 1924/10000: episode: 57, duration: 0.355s, episode steps:  31, steps per second:  87, episode reward: -38.550, mean reward: -1.244 [-32.057,  2.230], mean action: 13.774 [2.000, 20.000],  loss: 0.015236, mae: 0.704059, mean_q: -0.082429, mean_eps: 0.000000
 1981/10000: episode: 58, duration: 0.645s, episode steps:  57, steps per second:  88, episode reward: 32.368, mean reward:  0.568 [-2.349, 32.270], mean action: 13.544 [2.000, 19.000],  loss: 0.014899, mae: 0.649657, mean_q: -0.002598, mean_eps: 0.000000
 2028/10000: episode: 59, duration: 0.539s, episode steps:  47, steps per second:  87, episode reward: -35.430, mean reward: -0.754 [-31.930,  2.140], mean action: 10.404 [2.000, 16.000],  loss: 0.016769, mae: 0.711652, mean_q: -0.029080, mean_eps: 0.000000
 2052/10000: episode: 60, duration: 0.287s, episode steps:  24, steps per second:  84, episode reward: 42.935, mean reward:  1.789 [-2.262, 32.130], mean action: 12.792 [2.000, 18.000],  loss: 0.018347, mae: 0.683757, mean_q: 0.012482, mean_eps: 0.000000
 2083/10000: episode: 61, duration: 0.370s, episode steps:  31, steps per second:  84, episode reward: -32.320, mean reward: -1.043 [-32.507,  2.386], mean action: 11.516 [2.000, 16.000],  loss: 0.014398, mae: 0.674449, mean_q: -0.027807, mean_eps: 0.000000
 2103/10000: episode: 62, duration: 0.273s, episode steps:  20, steps per second:  73, episode reward: -44.750, mean reward: -2.237 [-30.159,  2.174], mean action: 13.650 [2.000, 16.000],  loss: 0.014555, mae: 0.657841, mean_q: -0.005323, mean_eps: 0.000000
 2136/10000: episode: 63, duration: 0.416s, episode steps:  33, steps per second:  79, episode reward: -38.390, mean reward: -1.163 [-32.102,  2.596], mean action: 13.727 [2.000, 18.000],  loss: 0.015525, mae: 0.700810, mean_q: -0.050994, mean_eps: 0.000000
 2164/10000: episode: 64, duration: 0.903s, episode steps:  28, steps per second:  31, episode reward: 32.730, mean reward:  1.169 [-2.509, 32.393], mean action: 12.393 [2.000, 20.000],  loss: 0.017737, mae: 0.737815, mean_q: -0.124528, mean_eps: 0.000000
 2188/10000: episode: 65, duration: 0.308s, episode steps:  24, steps per second:  78, episode reward: -35.310, mean reward: -1.471 [-32.351,  2.830], mean action: 11.875 [2.000, 16.000],  loss: 0.014920, mae: 0.644580, mean_q: 0.026623, mean_eps: 0.000000
 2227/10000: episode: 66, duration: 0.556s, episode steps:  39, steps per second:  70, episode reward: -32.390, mean reward: -0.831 [-32.013,  2.939], mean action: 11.154 [2.000, 15.000],  loss: 0.018522, mae: 0.691439, mean_q: -0.021444, mean_eps: 0.000000
 2266/10000: episode: 67, duration: 0.568s, episode steps:  39, steps per second:  69, episode reward: -38.500, mean reward: -0.987 [-31.630,  2.227], mean action: 13.769 [2.000, 20.000],  loss: 0.015958, mae: 0.715326, mean_q: -0.048710, mean_eps: 0.000000
 2294/10000: episode: 68, duration: 0.375s, episode steps:  28, steps per second:  75, episode reward: -38.820, mean reward: -1.386 [-31.900,  2.081], mean action: 14.143 [7.000, 18.000],  loss: 0.017705, mae: 0.727902, mean_q: -0.053588, mean_eps: 0.000000
 2374/10000: episode: 69, duration: 0.944s, episode steps:  80, steps per second:  85, episode reward: -33.000, mean reward: -0.412 [-32.249,  2.710], mean action: 10.838 [2.000, 20.000],  loss: 0.017595, mae: 0.730748, mean_q: -0.036238, mean_eps: 0.000000
 2432/10000: episode: 70, duration: 0.628s, episode steps:  58, steps per second:  92, episode reward: -44.300, mean reward: -0.764 [-32.196,  2.010], mean action: 6.483 [2.000, 20.000],  loss: 0.014794, mae: 0.687942, mean_q: -0.035844, mean_eps: 0.000000
 2470/10000: episode: 71, duration: 0.454s, episode steps:  38, steps per second:  84, episode reward: -32.910, mean reward: -0.866 [-32.234,  2.640], mean action: 12.842 [2.000, 20.000],  loss: 0.017902, mae: 0.710152, mean_q: -0.020042, mean_eps: 0.000000
 2502/10000: episode: 72, duration: 0.360s, episode steps:  32, steps per second:  89, episode reward: -35.130, mean reward: -1.098 [-32.173,  2.500], mean action: 13.781 [2.000, 16.000],  loss: 0.018332, mae: 0.675425, mean_q: 0.010808, mean_eps: 0.000000
 2538/10000: episode: 73, duration: 0.426s, episode steps:  36, steps per second:  85, episode reward: -36.000, mean reward: -1.000 [-32.043,  3.000], mean action: 14.361 [6.000, 19.000],  loss: 0.014954, mae: 0.680959, mean_q: 0.001248, mean_eps: 0.000000
 2565/10000: episode: 74, duration: 0.313s, episode steps:  27, steps per second:  86, episode reward: -32.840, mean reward: -1.216 [-31.901,  2.842], mean action: 11.593 [2.000, 20.000],  loss: 0.014906, mae: 0.696038, mean_q: -0.009644, mean_eps: 0.000000
 2595/10000: episode: 75, duration: 0.329s, episode steps:  30, steps per second:  91, episode reward: -39.000, mean reward: -1.300 [-32.207,  2.879], mean action: 16.000 [2.000, 20.000],  loss: 0.015877, mae: 0.700486, mean_q: -0.012624, mean_eps: 0.000000
 2639/10000: episode: 76, duration: 0.488s, episode steps:  44, steps per second:  90, episode reward: -41.450, mean reward: -0.942 [-32.744,  2.660], mean action: 12.295 [2.000, 20.000],  loss: 0.014865, mae: 0.679857, mean_q: -0.024095, mean_eps: 0.000000
 2673/10000: episode: 77, duration: 0.399s, episode steps:  34, steps per second:  85, episode reward: -38.780, mean reward: -1.141 [-32.187,  2.729], mean action: 12.000 [2.000, 20.000],  loss: 0.017806, mae: 0.689213, mean_q: -0.009808, mean_eps: 0.000000
 2711/10000: episode: 78, duration: 0.433s, episode steps:  38, steps per second:  88, episode reward: -38.770, mean reward: -1.020 [-32.263,  2.280], mean action: 11.105 [2.000, 16.000],  loss: 0.015138, mae: 0.703653, mean_q: -0.005288, mean_eps: 0.000000
 2748/10000: episode: 79, duration: 0.539s, episode steps:  37, steps per second:  69, episode reward: -35.350, mean reward: -0.955 [-31.587,  3.286], mean action: 13.216 [2.000, 20.000],  loss: 0.015814, mae: 0.671061, mean_q: 0.003800, mean_eps: 0.000000
 2786/10000: episode: 80, duration: 0.421s, episode steps:  38, steps per second:  90, episode reward: -41.850, mean reward: -1.101 [-32.263,  2.643], mean action: 13.579 [2.000, 18.000],  loss: 0.014261, mae: 0.679803, mean_q: -0.002121, mean_eps: 0.000000
 2832/10000: episode: 81, duration: 0.526s, episode steps:  46, steps per second:  88, episode reward: 37.340, mean reward:  0.812 [-2.222, 32.113], mean action: 8.348 [2.000, 18.000],  loss: 0.016571, mae: 0.719842, mean_q: -0.034357, mean_eps: 0.000000
 2856/10000: episode: 82, duration: 0.270s, episode steps:  24, steps per second:  89, episode reward: -44.640, mean reward: -1.860 [-32.537,  2.199], mean action: 15.333 [7.000, 20.000],  loss: 0.013868, mae: 0.717797, mean_q: -0.046651, mean_eps: 0.000000
 2873/10000: episode: 83, duration: 0.203s, episode steps:  17, steps per second:  84, episode reward: 38.170, mean reward:  2.245 [-3.000, 29.180], mean action: 7.059 [2.000, 18.000],  loss: 0.016001, mae: 0.733880, mean_q: -0.006303, mean_eps: 0.000000
 2912/10000: episode: 84, duration: 0.447s, episode steps:  39, steps per second:  87, episode reward: -44.210, mean reward: -1.134 [-32.068,  2.059], mean action: 13.846 [7.000, 15.000],  loss: 0.016385, mae: 0.741743, mean_q: -0.030540, mean_eps: 0.000000
 2941/10000: episode: 85, duration: 0.376s, episode steps:  29, steps per second:  77, episode reward: -41.890, mean reward: -1.444 [-32.017,  2.250], mean action: 10.483 [2.000, 16.000],  loss: 0.015464, mae: 0.719998, mean_q: -0.010084, mean_eps: 0.000000
 2973/10000: episode: 86, duration: 0.374s, episode steps:  32, steps per second:  86, episode reward: -41.390, mean reward: -1.293 [-31.700,  1.999], mean action: 14.062 [2.000, 20.000],  loss: 0.014717, mae: 0.710790, mean_q: -0.026009, mean_eps: 0.000000
 3013/10000: episode: 87, duration: 0.461s, episode steps:  40, steps per second:  87, episode reward: -32.440, mean reward: -0.811 [-32.574,  2.520], mean action: 12.975 [2.000, 18.000],  loss: 0.016190, mae: 0.733483, mean_q: -0.050726, mean_eps: 0.000000
 3036/10000: episode: 88, duration: 0.265s, episode steps:  23, steps per second:  87, episode reward: -36.000, mean reward: -1.565 [-32.166,  2.441], mean action: 9.130 [2.000, 20.000],  loss: 0.013673, mae: 0.677324, mean_q: 0.003827, mean_eps: 0.000000
 3071/10000: episode: 89, duration: 0.404s, episode steps:  35, steps per second:  87, episode reward: 37.957, mean reward:  1.084 [-2.423, 32.870], mean action: 14.714 [2.000, 19.000],  loss: 0.014978, mae: 0.743404, mean_q: -0.059763, mean_eps: 0.000000
 3100/10000: episode: 90, duration: 0.417s, episode steps:  29, steps per second:  70, episode reward: 32.328, mean reward:  1.115 [-2.762, 32.760], mean action: 12.000 [2.000, 15.000],  loss: 0.018384, mae: 0.713217, mean_q: 0.002463, mean_eps: 0.000000
 3145/10000: episode: 91, duration: 0.515s, episode steps:  45, steps per second:  87, episode reward: -32.780, mean reward: -0.728 [-32.154,  2.530], mean action: 14.556 [10.000, 15.000],  loss: 0.017426, mae: 0.731329, mean_q: 0.005203, mean_eps: 0.000000
 3188/10000: episode: 92, duration: 0.510s, episode steps:  43, steps per second:  84, episode reward: 32.189, mean reward:  0.749 [-2.282, 32.120], mean action: 11.698 [2.000, 20.000],  loss: 0.018528, mae: 0.727066, mean_q: -0.017015, mean_eps: 0.000000
 3215/10000: episode: 93, duration: 0.322s, episode steps:  27, steps per second:  84, episode reward: -35.550, mean reward: -1.317 [-32.423,  2.145], mean action: 7.963 [2.000, 15.000],  loss: 0.016294, mae: 0.751019, mean_q: -0.052287, mean_eps: 0.000000
 3243/10000: episode: 94, duration: 0.318s, episode steps:  28, steps per second:  88, episode reward: -41.750, mean reward: -1.491 [-32.071,  2.200], mean action: 12.714 [2.000, 20.000],  loss: 0.013459, mae: 0.745329, mean_q: -0.060075, mean_eps: 0.000000
 3264/10000: episode: 95, duration: 0.232s, episode steps:  21, steps per second:  90, episode reward: -41.310, mean reward: -1.967 [-32.203,  2.380], mean action: 12.619 [2.000, 15.000],  loss: 0.016390, mae: 0.744738, mean_q: -0.028610, mean_eps: 0.000000
 3294/10000: episode: 96, duration: 0.335s, episode steps:  30, steps per second:  90, episode reward: -36.000, mean reward: -1.200 [-32.449,  2.570], mean action: 14.300 [2.000, 20.000],  loss: 0.014209, mae: 0.731296, mean_q: -0.042295, mean_eps: 0.000000
 3338/10000: episode: 97, duration: 0.520s, episode steps:  44, steps per second:  85, episode reward: 35.380, mean reward:  0.804 [-2.243, 31.867], mean action: 11.250 [2.000, 15.000],  loss: 0.015476, mae: 0.734417, mean_q: -0.019696, mean_eps: 0.000000
 3381/10000: episode: 98, duration: 0.482s, episode steps:  43, steps per second:  89, episode reward: 32.773, mean reward:  0.762 [-3.000, 31.948], mean action: 10.372 [2.000, 19.000],  loss: 0.015098, mae: 0.697594, mean_q: 0.007556, mean_eps: 0.000000
 3413/10000: episode: 99, duration: 0.359s, episode steps:  32, steps per second:  89, episode reward: -36.000, mean reward: -1.125 [-32.611,  2.282], mean action: 9.969 [2.000, 15.000],  loss: 0.015924, mae: 0.741544, mean_q: 0.013755, mean_eps: 0.000000
 3453/10000: episode: 100, duration: 0.565s, episode steps:  40, steps per second:  71, episode reward: -39.000, mean reward: -0.975 [-32.124,  2.720], mean action: 12.600 [2.000, 20.000],  loss: 0.016242, mae: 0.714833, mean_q: -0.002108, mean_eps: 0.000000
 3489/10000: episode: 101, duration: 0.405s, episode steps:  36, steps per second:  89, episode reward: -32.670, mean reward: -0.908 [-32.189,  2.963], mean action: 12.472 [2.000, 20.000],  loss: 0.014008, mae: 0.703358, mean_q: 0.019894, mean_eps: 0.000000
 3516/10000: episode: 102, duration: 0.298s, episode steps:  27, steps per second:  91, episode reward: -41.410, mean reward: -1.534 [-32.207,  2.242], mean action: 12.667 [2.000, 20.000],  loss: 0.014735, mae: 0.706233, mean_q: 0.010704, mean_eps: 0.000000
 3553/10000: episode: 103, duration: 0.490s, episode steps:  37, steps per second:  75, episode reward: 32.122, mean reward:  0.868 [-2.886, 32.270], mean action: 13.216 [2.000, 15.000],  loss: 0.017199, mae: 0.756177, mean_q: -0.032542, mean_eps: 0.000000
 3578/10000: episode: 104, duration: 0.296s, episode steps:  25, steps per second:  84, episode reward: -35.470, mean reward: -1.419 [-31.841,  2.950], mean action: 12.480 [2.000, 16.000],  loss: 0.014986, mae: 0.730064, mean_q: 0.030744, mean_eps: 0.000000
 3619/10000: episode: 105, duration: 0.467s, episode steps:  41, steps per second:  88, episode reward: -35.580, mean reward: -0.868 [-32.155,  2.592], mean action: 12.976 [2.000, 20.000],  loss: 0.015581, mae: 0.699082, mean_q: 0.032853, mean_eps: 0.000000
 3656/10000: episode: 106, duration: 0.427s, episode steps:  37, steps per second:  87, episode reward: -32.340, mean reward: -0.874 [-32.017,  3.000], mean action: 11.622 [2.000, 15.000],  loss: 0.016286, mae: 0.772035, mean_q: -0.036331, mean_eps: 0.000000
 3704/10000: episode: 107, duration: 0.530s, episode steps:  48, steps per second:  91, episode reward: 32.672, mean reward:  0.681 [-2.319, 32.110], mean action: 11.542 [2.000, 15.000],  loss: 0.013982, mae: 0.739214, mean_q: 0.002584, mean_eps: 0.000000
 3734/10000: episode: 108, duration: 0.328s, episode steps:  30, steps per second:  91, episode reward: -41.960, mean reward: -1.399 [-32.775,  2.472], mean action: 13.433 [6.000, 20.000],  loss: 0.015504, mae: 0.755159, mean_q: -0.002634, mean_eps: 0.000000
 3770/10000: episode: 109, duration: 0.412s, episode steps:  36, steps per second:  87, episode reward: -41.710, mean reward: -1.159 [-32.148,  2.246], mean action: 12.861 [2.000, 18.000],  loss: 0.017977, mae: 0.752143, mean_q: 0.011274, mean_eps: 0.000000
 3811/10000: episode: 110, duration: 0.459s, episode steps:  41, steps per second:  89, episode reward: 37.731, mean reward:  0.920 [-2.193, 32.420], mean action: 13.805 [2.000, 20.000],  loss: 0.015764, mae: 0.758953, mean_q: 0.002117, mean_eps: 0.000000
 3835/10000: episode: 111, duration: 0.276s, episode steps:  24, steps per second:  87, episode reward: -33.000, mean reward: -1.375 [-29.989,  3.000], mean action: 12.042 [2.000, 20.000],  loss: 0.015638, mae: 0.707992, mean_q: 0.063030, mean_eps: 0.000000
 3875/10000: episode: 112, duration: 0.447s, episode steps:  40, steps per second:  90, episode reward: -32.780, mean reward: -0.820 [-32.181,  3.000], mean action: 14.750 [2.000, 20.000],  loss: 0.014075, mae: 0.733724, mean_q: -0.001050, mean_eps: 0.000000
 3895/10000: episode: 113, duration: 0.243s, episode steps:  20, steps per second:  82, episode reward: 39.459, mean reward:  1.973 [-2.514, 32.260], mean action: 7.500 [2.000, 20.000],  loss: 0.020010, mae: 0.726728, mean_q: 0.017735, mean_eps: 0.000000
 3923/10000: episode: 114, duration: 0.320s, episode steps:  28, steps per second:  88, episode reward: 34.476, mean reward:  1.231 [-2.245, 32.240], mean action: 10.286 [2.000, 15.000],  loss: 0.014788, mae: 0.726460, mean_q: -0.000819, mean_eps: 0.000000
 3958/10000: episode: 115, duration: 0.406s, episode steps:  35, steps per second:  86, episode reward: -39.000, mean reward: -1.114 [-32.200,  2.590], mean action: 8.000 [2.000, 18.000],  loss: 0.014057, mae: 0.693579, mean_q: 0.029345, mean_eps: 0.000000
 3986/10000: episode: 116, duration: 0.342s, episode steps:  28, steps per second:  82, episode reward: 32.084, mean reward:  1.146 [-2.620, 32.450], mean action: 9.893 [2.000, 20.000],  loss: 0.015199, mae: 0.715325, mean_q: 0.014283, mean_eps: 0.000000
 4015/10000: episode: 117, duration: 0.425s, episode steps:  29, steps per second:  68, episode reward: -32.580, mean reward: -1.123 [-31.925,  2.500], mean action: 12.000 [2.000, 18.000],  loss: 0.016966, mae: 0.718647, mean_q: 0.009419, mean_eps: 0.000000
 4044/10000: episode: 118, duration: 0.370s, episode steps:  29, steps per second:  78, episode reward: 32.285, mean reward:  1.113 [-2.426, 32.430], mean action: 10.069 [2.000, 19.000],  loss: 0.016571, mae: 0.666504, mean_q: 0.057446, mean_eps: 0.000000
 4074/10000: episode: 119, duration: 0.458s, episode steps:  30, steps per second:  65, episode reward: -38.500, mean reward: -1.283 [-32.400,  2.340], mean action: 10.133 [2.000, 20.000],  loss: 0.017381, mae: 0.704304, mean_q: 0.051755, mean_eps: 0.000000
 4104/10000: episode: 120, duration: 0.362s, episode steps:  30, steps per second:  83, episode reward: -38.030, mean reward: -1.268 [-31.781,  2.430], mean action: 15.233 [2.000, 20.000],  loss: 0.013804, mae: 0.771293, mean_q: -0.034468, mean_eps: 0.000000
 4139/10000: episode: 121, duration: 0.445s, episode steps:  35, steps per second:  79, episode reward: 35.018, mean reward:  1.001 [-3.000, 31.759], mean action: 12.286 [2.000, 19.000],  loss: 0.015352, mae: 0.734338, mean_q: 0.002012, mean_eps: 0.000000
 4182/10000: episode: 122, duration: 0.539s, episode steps:  43, steps per second:  80, episode reward: -33.000, mean reward: -0.767 [-32.163,  2.907], mean action: 14.116 [2.000, 18.000],  loss: 0.017568, mae: 0.758958, mean_q: 0.004284, mean_eps: 0.000000
 4211/10000: episode: 123, duration: 0.335s, episode steps:  29, steps per second:  87, episode reward: -39.000, mean reward: -1.345 [-32.878,  3.000], mean action: 14.069 [2.000, 18.000],  loss: 0.017565, mae: 0.775097, mean_q: -0.012293, mean_eps: 0.000000
 4238/10000: episode: 124, duration: 0.349s, episode steps:  27, steps per second:  77, episode reward: -38.180, mean reward: -1.414 [-32.830,  2.527], mean action: 11.593 [2.000, 20.000],  loss: 0.014958, mae: 0.753453, mean_q: -0.004147, mean_eps: 0.000000
 4281/10000: episode: 125, duration: 0.644s, episode steps:  43, steps per second:  67, episode reward: 32.945, mean reward:  0.766 [-2.490, 32.332], mean action: 4.628 [2.000, 15.000],  loss: 0.018063, mae: 0.763733, mean_q: -0.014905, mean_eps: 0.000000
 4303/10000: episode: 126, duration: 0.265s, episode steps:  22, steps per second:  83, episode reward: 35.165, mean reward:  1.598 [-2.797, 32.209], mean action: 12.000 [2.000, 19.000],  loss: 0.014029, mae: 0.720254, mean_q: 0.035140, mean_eps: 0.000000
 4338/10000: episode: 127, duration: 0.407s, episode steps:  35, steps per second:  86, episode reward: 39.000, mean reward:  1.114 [-2.337, 32.110], mean action: 4.229 [2.000, 16.000],  loss: 0.016397, mae: 0.743456, mean_q: 0.020652, mean_eps: 0.000000
 4367/10000: episode: 128, duration: 0.386s, episode steps:  29, steps per second:  75, episode reward: 37.341, mean reward:  1.288 [-2.532, 32.112], mean action: 11.000 [2.000, 15.000],  loss: 0.013792, mae: 0.763404, mean_q: -0.008740, mean_eps: 0.000000
 4398/10000: episode: 129, duration: 0.351s, episode steps:  31, steps per second:  88, episode reward: -38.370, mean reward: -1.238 [-31.864,  2.208], mean action: 12.226 [2.000, 19.000],  loss: 0.014060, mae: 0.748503, mean_q: 0.011122, mean_eps: 0.000000
 4425/10000: episode: 130, duration: 0.321s, episode steps:  27, steps per second:  84, episode reward: 38.308, mean reward:  1.419 [-2.440, 32.220], mean action: 14.111 [2.000, 15.000],  loss: 0.017663, mae: 0.822494, mean_q: -0.041897, mean_eps: 0.000000
 4457/10000: episode: 131, duration: 0.381s, episode steps:  32, steps per second:  84, episode reward: -38.700, mean reward: -1.209 [-31.808,  2.110], mean action: 11.781 [2.000, 15.000],  loss: 0.018132, mae: 0.784092, mean_q: 0.025028, mean_eps: 0.000000
 4491/10000: episode: 132, duration: 0.383s, episode steps:  34, steps per second:  89, episode reward: -36.000, mean reward: -1.059 [-32.456,  2.332], mean action: 15.706 [2.000, 20.000],  loss: 0.017090, mae: 0.755999, mean_q: 0.021795, mean_eps: 0.000000
 4526/10000: episode: 133, duration: 0.385s, episode steps:  35, steps per second:  91, episode reward: -35.620, mean reward: -1.018 [-31.855,  2.350], mean action: 14.343 [2.000, 20.000],  loss: 0.014345, mae: 0.766055, mean_q: -0.010802, mean_eps: 0.000000
 4572/10000: episode: 134, duration: 0.532s, episode steps:  46, steps per second:  86, episode reward: 32.214, mean reward:  0.700 [-2.285, 31.966], mean action: 13.478 [2.000, 16.000],  loss: 0.014676, mae: 0.760337, mean_q: 0.034302, mean_eps: 0.000000
 4605/10000: episode: 135, duration: 0.396s, episode steps:  33, steps per second:  83, episode reward: -39.000, mean reward: -1.182 [-32.306,  2.470], mean action: 13.848 [2.000, 15.000],  loss: 0.017685, mae: 0.779847, mean_q: 0.020432, mean_eps: 0.000000
 4633/10000: episode: 136, duration: 0.334s, episode steps:  28, steps per second:  84, episode reward: -35.880, mean reward: -1.281 [-32.258,  2.210], mean action: 15.750 [2.000, 20.000],  loss: 0.015717, mae: 0.742079, mean_q: 0.050333, mean_eps: 0.000000
 4668/10000: episode: 137, duration: 0.394s, episode steps:  35, steps per second:  89, episode reward: -38.370, mean reward: -1.096 [-32.288,  2.280], mean action: 11.229 [2.000, 18.000],  loss: 0.016805, mae: 0.783833, mean_q: -0.002626, mean_eps: 0.000000
 4719/10000: episode: 138, duration: 0.573s, episode steps:  51, steps per second:  89, episode reward: 32.397, mean reward:  0.635 [-2.266, 31.601], mean action: 13.902 [2.000, 20.000],  loss: 0.016861, mae: 0.785787, mean_q: -0.002425, mean_eps: 0.000000
 4737/10000: episode: 139, duration: 0.199s, episode steps:  18, steps per second:  90, episode reward: -42.000, mean reward: -2.333 [-32.037,  2.490], mean action: 12.944 [2.000, 20.000],  loss: 0.015198, mae: 0.812017, mean_q: -0.036078, mean_eps: 0.000000
 4768/10000: episode: 140, duration: 0.349s, episode steps:  31, steps per second:  89, episode reward: -35.370, mean reward: -1.141 [-31.483,  2.303], mean action: 13.032 [2.000, 20.000],  loss: 0.014014, mae: 0.776852, mean_q: 0.003675, mean_eps: 0.000000
 4805/10000: episode: 141, duration: 0.436s, episode steps:  37, steps per second:  85, episode reward: -35.870, mean reward: -0.969 [-32.126,  2.561], mean action: 10.784 [2.000, 20.000],  loss: 0.016327, mae: 0.756372, mean_q: 0.033333, mean_eps: 0.000000
 4832/10000: episode: 142, duration: 0.356s, episode steps:  27, steps per second:  76, episode reward: -44.020, mean reward: -1.630 [-32.004,  2.270], mean action: 14.185 [2.000, 20.000],  loss: 0.016886, mae: 0.785751, mean_q: 0.005026, mean_eps: 0.000000
 4871/10000: episode: 143, duration: 0.429s, episode steps:  39, steps per second:  91, episode reward: 32.468, mean reward:  0.833 [-2.734, 32.570], mean action: 12.385 [2.000, 16.000],  loss: 0.011891, mae: 0.751361, mean_q: 0.032410, mean_eps: 0.000000
 4896/10000: episode: 144, duration: 0.286s, episode steps:  25, steps per second:  87, episode reward: 40.842, mean reward:  1.634 [-2.233, 31.910], mean action: 8.840 [2.000, 15.000],  loss: 0.016203, mae: 0.799387, mean_q: -0.026396, mean_eps: 0.000000
 4935/10000: episode: 145, duration: 0.447s, episode steps:  39, steps per second:  87, episode reward: 32.781, mean reward:  0.841 [-2.903, 32.200], mean action: 14.410 [2.000, 20.000],  loss: 0.014579, mae: 0.767151, mean_q: -0.000496, mean_eps: 0.000000
 4966/10000: episode: 146, duration: 0.365s, episode steps:  31, steps per second:  85, episode reward: -32.400, mean reward: -1.045 [-32.204,  2.292], mean action: 9.323 [2.000, 20.000],  loss: 0.017041, mae: 0.749695, mean_q: 0.086341, mean_eps: 0.000000
 5001/10000: episode: 147, duration: 0.603s, episode steps:  35, steps per second:  58, episode reward: -32.850, mean reward: -0.939 [-32.298,  2.772], mean action: 10.457 [2.000, 20.000],  loss: 0.016383, mae: 0.759754, mean_q: 0.034529, mean_eps: 0.000000
 5039/10000: episode: 148, duration: 0.429s, episode steps:  38, steps per second:  89, episode reward: -35.560, mean reward: -0.936 [-32.058,  3.000], mean action: 14.974 [2.000, 20.000],  loss: 0.017218, mae: 0.788861, mean_q: 0.028166, mean_eps: 0.000000
 5060/10000: episode: 149, duration: 0.240s, episode steps:  21, steps per second:  87, episode reward: -39.000, mean reward: -1.857 [-32.374,  2.332], mean action: 12.667 [2.000, 20.000],  loss: 0.015032, mae: 0.818973, mean_q: -0.010061, mean_eps: 0.000000
 5102/10000: episode: 150, duration: 0.472s, episode steps:  42, steps per second:  89, episode reward: -41.780, mean reward: -0.995 [-32.175,  2.224], mean action: 13.976 [2.000, 20.000],  loss: 0.015285, mae: 0.842051, mean_q: -0.035886, mean_eps: 0.000000
 5129/10000: episode: 151, duration: 0.329s, episode steps:  27, steps per second:  82, episode reward: 38.152, mean reward:  1.413 [-2.498, 31.922], mean action: 6.741 [2.000, 15.000],  loss: 0.017421, mae: 0.838189, mean_q: -0.060634, mean_eps: 0.000000
 5157/10000: episode: 152, duration: 0.315s, episode steps:  28, steps per second:  89, episode reward: -41.670, mean reward: -1.488 [-32.097,  2.320], mean action: 13.786 [2.000, 20.000],  loss: 0.014462, mae: 0.812897, mean_q: -0.017083, mean_eps: 0.000000
 5183/10000: episode: 153, duration: 0.295s, episode steps:  26, steps per second:  88, episode reward: 38.409, mean reward:  1.477 [-2.318, 32.020], mean action: 10.500 [2.000, 16.000],  loss: 0.018246, mae: 0.842493, mean_q: 0.017871, mean_eps: 0.000000
 5232/10000: episode: 154, duration: 0.551s, episode steps:  49, steps per second:  89, episode reward: 32.935, mean reward:  0.672 [-3.000, 32.060], mean action: 14.612 [2.000, 20.000],  loss: 0.017087, mae: 0.792104, mean_q: 0.035808, mean_eps: 0.000000
 5252/10000: episode: 155, duration: 0.229s, episode steps:  20, steps per second:  87, episode reward: -35.390, mean reward: -1.770 [-32.042,  2.964], mean action: 10.950 [2.000, 15.000],  loss: 0.016108, mae: 0.853277, mean_q: 0.012499, mean_eps: 0.000000
 5284/10000: episode: 156, duration: 0.362s, episode steps:  32, steps per second:  88, episode reward: -32.730, mean reward: -1.023 [-32.096,  2.873], mean action: 7.719 [2.000, 19.000],  loss: 0.016306, mae: 0.799948, mean_q: 0.046035, mean_eps: 0.000000
 5307/10000: episode: 157, duration: 0.288s, episode steps:  23, steps per second:  80, episode reward: 35.585, mean reward:  1.547 [-2.505, 30.622], mean action: 6.696 [2.000, 16.000],  loss: 0.016467, mae: 0.777146, mean_q: 0.070647, mean_eps: 0.000000
 5334/10000: episode: 158, duration: 0.315s, episode steps:  27, steps per second:  86, episode reward: 37.893, mean reward:  1.403 [-2.375, 32.189], mean action: 12.519 [2.000, 15.000],  loss: 0.018318, mae: 0.863830, mean_q: -0.053529, mean_eps: 0.000000
 5355/10000: episode: 159, duration: 0.247s, episode steps:  21, steps per second:  85, episode reward: -39.000, mean reward: -1.857 [-32.412,  2.410], mean action: 9.095 [2.000, 15.000],  loss: 0.015748, mae: 0.833522, mean_q: -0.009345, mean_eps: 0.000000
 5387/10000: episode: 160, duration: 0.474s, episode steps:  32, steps per second:  68, episode reward: -39.000, mean reward: -1.219 [-32.262,  2.810], mean action: 15.781 [6.000, 20.000],  loss: 0.015652, mae: 0.782029, mean_q: 0.020440, mean_eps: 0.000000
 5417/10000: episode: 161, duration: 0.366s, episode steps:  30, steps per second:  82, episode reward: -35.610, mean reward: -1.187 [-32.223,  2.480], mean action: 9.367 [2.000, 16.000],  loss: 0.014118, mae: 0.791851, mean_q: 0.013340, mean_eps: 0.000000
 5444/10000: episode: 162, duration: 0.361s, episode steps:  27, steps per second:  75, episode reward: -41.400, mean reward: -1.533 [-32.374,  2.541], mean action: 10.519 [2.000, 19.000],  loss: 0.017086, mae: 0.808609, mean_q: 0.036091, mean_eps: 0.000000
 5472/10000: episode: 163, duration: 0.395s, episode steps:  28, steps per second:  71, episode reward: 37.448, mean reward:  1.337 [-2.254, 32.023], mean action: 10.821 [2.000, 15.000],  loss: 0.015293, mae: 0.801110, mean_q: 0.011952, mean_eps: 0.000000
 5501/10000: episode: 164, duration: 0.340s, episode steps:  29, steps per second:  85, episode reward: -38.540, mean reward: -1.329 [-32.130,  2.610], mean action: 10.103 [2.000, 20.000],  loss: 0.014651, mae: 0.782094, mean_q: 0.038339, mean_eps: 0.000000
 5533/10000: episode: 165, duration: 0.348s, episode steps:  32, steps per second:  92, episode reward: -41.430, mean reward: -1.295 [-32.285,  2.200], mean action: 13.562 [2.000, 20.000],  loss: 0.015975, mae: 0.805448, mean_q: 0.040362, mean_eps: 0.000000
 5561/10000: episode: 166, duration: 0.317s, episode steps:  28, steps per second:  88, episode reward: -35.660, mean reward: -1.274 [-32.660,  2.430], mean action: 12.607 [2.000, 20.000],  loss: 0.015124, mae: 0.749155, mean_q: 0.066840, mean_eps: 0.000000
 5582/10000: episode: 167, duration: 0.243s, episode steps:  21, steps per second:  87, episode reward: -38.200, mean reward: -1.819 [-31.451,  2.433], mean action: 12.762 [2.000, 20.000],  loss: 0.019451, mae: 0.836058, mean_q: -0.011374, mean_eps: 0.000000
 5617/10000: episode: 168, duration: 0.423s, episode steps:  35, steps per second:  83, episode reward: 35.069, mean reward:  1.002 [-2.717, 32.201], mean action: 6.343 [2.000, 20.000],  loss: 0.016446, mae: 0.826047, mean_q: 0.040721, mean_eps: 0.000000
 5641/10000: episode: 169, duration: 0.326s, episode steps:  24, steps per second:  74, episode reward: 32.230, mean reward:  1.343 [-3.000, 32.240], mean action: 10.500 [2.000, 19.000],  loss: 0.015783, mae: 0.782796, mean_q: 0.098346, mean_eps: 0.000000
 5660/10000: episode: 170, duration: 0.233s, episode steps:  19, steps per second:  82, episode reward: -38.300, mean reward: -2.016 [-32.187,  2.340], mean action: 10.526 [2.000, 20.000],  loss: 0.016872, mae: 0.821208, mean_q: -0.004693, mean_eps: 0.000000
 5696/10000: episode: 171, duration: 0.431s, episode steps:  36, steps per second:  84, episode reward: -39.000, mean reward: -1.083 [-32.025,  2.600], mean action: 14.333 [2.000, 20.000],  loss: 0.018332, mae: 0.835444, mean_q: 0.036688, mean_eps: 0.000000
 5745/10000: episode: 172, duration: 0.552s, episode steps:  49, steps per second:  89, episode reward: 35.292, mean reward:  0.720 [-2.591, 32.280], mean action: 10.633 [2.000, 15.000],  loss: 0.015695, mae: 0.834220, mean_q: -0.010775, mean_eps: 0.000000
 5780/10000: episode: 173, duration: 0.420s, episode steps:  35, steps per second:  83, episode reward: 37.959, mean reward:  1.085 [-2.551, 32.540], mean action: 11.314 [2.000, 16.000],  loss: 0.016874, mae: 0.841296, mean_q: 0.031457, mean_eps: 0.000000
 5804/10000: episode: 174, duration: 0.285s, episode steps:  24, steps per second:  84, episode reward: -38.280, mean reward: -1.595 [-31.993,  2.164], mean action: 10.125 [2.000, 15.000],  loss: 0.017938, mae: 0.852970, mean_q: 0.035334, mean_eps: 0.000000
 5833/10000: episode: 175, duration: 0.477s, episode steps:  29, steps per second:  61, episode reward: -39.000, mean reward: -1.345 [-32.367,  2.950], mean action: 11.483 [2.000, 20.000],  loss: 0.015523, mae: 0.852795, mean_q: -0.016501, mean_eps: 0.000000
 5867/10000: episode: 176, duration: 0.396s, episode steps:  34, steps per second:  86, episode reward: -35.530, mean reward: -1.045 [-31.966,  2.530], mean action: 13.912 [2.000, 20.000],  loss: 0.019656, mae: 0.860909, mean_q: -0.026528, mean_eps: 0.000000
 5890/10000: episode: 177, duration: 0.272s, episode steps:  23, steps per second:  85, episode reward: -41.910, mean reward: -1.822 [-32.561,  2.410], mean action: 14.870 [2.000, 20.000],  loss: 0.014542, mae: 0.776461, mean_q: 0.084513, mean_eps: 0.000000
 5922/10000: episode: 178, duration: 0.370s, episode steps:  32, steps per second:  86, episode reward: -35.330, mean reward: -1.104 [-31.684,  2.845], mean action: 14.000 [2.000, 20.000],  loss: 0.015521, mae: 0.794947, mean_q: 0.031567, mean_eps: 0.000000
 5944/10000: episode: 179, duration: 0.252s, episode steps:  22, steps per second:  87, episode reward: -38.920, mean reward: -1.769 [-32.142,  2.450], mean action: 8.364 [2.000, 15.000],  loss: 0.017537, mae: 0.833044, mean_q: -0.007018, mean_eps: 0.000000
 5982/10000: episode: 180, duration: 0.440s, episode steps:  38, steps per second:  86, episode reward: 37.478, mean reward:  0.986 [-2.449, 32.130], mean action: 12.184 [2.000, 16.000],  loss: 0.016317, mae: 0.819762, mean_q: 0.015337, mean_eps: 0.000000
 6009/10000: episode: 181, duration: 0.323s, episode steps:  27, steps per second:  84, episode reward: 34.590, mean reward:  1.281 [-2.742, 32.740], mean action: 9.370 [2.000, 18.000],  loss: 0.016701, mae: 0.775091, mean_q: 0.036537, mean_eps: 0.000000
 6033/10000: episode: 182, duration: 0.281s, episode steps:  24, steps per second:  85, episode reward: -38.820, mean reward: -1.617 [-32.106,  2.450], mean action: 8.958 [2.000, 20.000],  loss: 0.019303, mae: 0.771415, mean_q: 0.074537, mean_eps: 0.000000
 6045/10000: episode: 183, duration: 0.155s, episode steps:  12, steps per second:  77, episode reward: 44.070, mean reward:  3.673 [-2.270, 32.060], mean action: 2.667 [2.000, 10.000],  loss: 0.013535, mae: 0.738703, mean_q: 0.077665, mean_eps: 0.000000
 6071/10000: episode: 184, duration: 0.314s, episode steps:  26, steps per second:  83, episode reward: -42.000, mean reward: -1.615 [-32.078,  2.315], mean action: 13.385 [2.000, 20.000],  loss: 0.017080, mae: 0.792064, mean_q: 0.057881, mean_eps: 0.000000
 6105/10000: episode: 185, duration: 0.377s, episode steps:  34, steps per second:  90, episode reward: -35.270, mean reward: -1.037 [-31.492,  3.000], mean action: 13.059 [2.000, 19.000],  loss: 0.016505, mae: 0.853524, mean_q: 0.017325, mean_eps: 0.000000
 6153/10000: episode: 186, duration: 0.553s, episode steps:  48, steps per second:  87, episode reward: -39.000, mean reward: -0.812 [-32.150,  2.475], mean action: 13.146 [2.000, 20.000],  loss: 0.015551, mae: 0.790860, mean_q: 0.051892, mean_eps: 0.000000
 6193/10000: episode: 187, duration: 0.448s, episode steps:  40, steps per second:  89, episode reward: 38.017, mean reward:  0.950 [-3.000, 31.822], mean action: 12.150 [2.000, 18.000],  loss: 0.018122, mae: 0.842401, mean_q: -0.011263, mean_eps: 0.000000
 6233/10000: episode: 188, duration: 0.437s, episode steps:  40, steps per second:  92, episode reward: -40.830, mean reward: -1.021 [-32.144,  2.349], mean action: 13.675 [2.000, 20.000],  loss: 0.022464, mae: 0.852976, mean_q: 0.014319, mean_eps: 0.000000
 6266/10000: episode: 189, duration: 0.360s, episode steps:  33, steps per second:  92, episode reward: -35.450, mean reward: -1.074 [-32.109,  3.000], mean action: 12.727 [2.000, 20.000],  loss: 0.017022, mae: 0.829480, mean_q: 0.008884, mean_eps: 0.000000
 6289/10000: episode: 190, duration: 0.261s, episode steps:  23, steps per second:  88, episode reward: 35.601, mean reward:  1.548 [-2.497, 31.621], mean action: 10.174 [2.000, 20.000],  loss: 0.016280, mae: 0.811544, mean_q: 0.020127, mean_eps: 0.000000
 6346/10000: episode: 191, duration: 0.641s, episode steps:  57, steps per second:  89, episode reward: -35.840, mean reward: -0.629 [-32.313,  2.600], mean action: 13.719 [2.000, 20.000],  loss: 0.015835, mae: 0.867840, mean_q: 0.013382, mean_eps: 0.000000
 6372/10000: episode: 192, duration: 0.304s, episode steps:  26, steps per second:  85, episode reward: -36.000, mean reward: -1.385 [-32.349,  3.000], mean action: 12.538 [2.000, 20.000],  loss: 0.013743, mae: 0.807191, mean_q: 0.074908, mean_eps: 0.000000
 6393/10000: episode: 193, duration: 0.245s, episode steps:  21, steps per second:  86, episode reward: 40.555, mean reward:  1.931 [-2.187, 32.742], mean action: 8.000 [2.000, 15.000],  loss: 0.013288, mae: 0.852106, mean_q: -0.030088, mean_eps: 0.000000
 6442/10000: episode: 194, duration: 0.551s, episode steps:  49, steps per second:  89, episode reward: -38.190, mean reward: -0.779 [-31.927,  2.341], mean action: 9.571 [2.000, 20.000],  loss: 0.020299, mae: 0.870523, mean_q: 0.001014, mean_eps: 0.000000
 6459/10000: episode: 195, duration: 0.203s, episode steps:  17, steps per second:  84, episode reward: 35.547, mean reward:  2.091 [-3.000, 33.000], mean action: 8.706 [2.000, 20.000],  loss: 0.017444, mae: 0.838809, mean_q: 0.007295, mean_eps: 0.000000
 6488/10000: episode: 196, duration: 0.490s, episode steps:  29, steps per second:  59, episode reward: 38.232, mean reward:  1.318 [-2.392, 32.829], mean action: 6.310 [2.000, 16.000],  loss: 0.016447, mae: 0.830376, mean_q: 0.001646, mean_eps: 0.000000
 6538/10000: episode: 197, duration: 0.569s, episode steps:  50, steps per second:  88, episode reward: 32.186, mean reward:  0.644 [-2.275, 32.124], mean action: 13.620 [2.000, 16.000],  loss: 0.016415, mae: 0.849899, mean_q: -0.000175, mean_eps: 0.000000
 6571/10000: episode: 198, duration: 0.378s, episode steps:  33, steps per second:  87, episode reward: -32.690, mean reward: -0.991 [-32.378,  2.708], mean action: 11.727 [2.000, 19.000],  loss: 0.015976, mae: 0.783231, mean_q: 0.068547, mean_eps: 0.000000
 6616/10000: episode: 199, duration: 0.648s, episode steps:  45, steps per second:  69, episode reward: 37.404, mean reward:  0.831 [-2.529, 32.150], mean action: 12.489 [2.000, 16.000],  loss: 0.019629, mae: 0.848180, mean_q: 0.064046, mean_eps: 0.000000
 6644/10000: episode: 200, duration: 0.340s, episode steps:  28, steps per second:  82, episode reward: 38.345, mean reward:  1.369 [-2.164, 32.160], mean action: 4.500 [2.000, 15.000],  loss: 0.017383, mae: 0.875886, mean_q: 0.031519, mean_eps: 0.000000
 6667/10000: episode: 201, duration: 0.286s, episode steps:  23, steps per second:  80, episode reward: 37.534, mean reward:  1.632 [-2.549, 31.559], mean action: 8.261 [2.000, 16.000],  loss: 0.016668, mae: 0.846950, mean_q: 0.007628, mean_eps: 0.000000
 6692/10000: episode: 202, duration: 0.318s, episode steps:  25, steps per second:  79, episode reward: -38.200, mean reward: -1.528 [-32.644,  2.320], mean action: 9.720 [2.000, 20.000],  loss: 0.014696, mae: 0.827133, mean_q: 0.022369, mean_eps: 0.000000
 6728/10000: episode: 203, duration: 0.524s, episode steps:  36, steps per second:  69, episode reward: -35.380, mean reward: -0.983 [-31.877,  2.239], mean action: 10.861 [2.000, 20.000],  loss: 0.018411, mae: 0.835885, mean_q: 0.026015, mean_eps: 0.000000
 6770/10000: episode: 204, duration: 0.484s, episode steps:  42, steps per second:  87, episode reward: -33.000, mean reward: -0.786 [-32.671,  2.950], mean action: 9.571 [2.000, 15.000],  loss: 0.015399, mae: 0.773473, mean_q: 0.076776, mean_eps: 0.000000
 6796/10000: episode: 205, duration: 0.446s, episode steps:  26, steps per second:  58, episode reward: 38.285, mean reward:  1.472 [-2.365, 32.300], mean action: 6.077 [2.000, 15.000],  loss: 0.014119, mae: 0.778122, mean_q: 0.075750, mean_eps: 0.000000
 6823/10000: episode: 206, duration: 0.499s, episode steps:  27, steps per second:  54, episode reward: -38.740, mean reward: -1.435 [-31.971,  2.396], mean action: 10.519 [2.000, 20.000],  loss: 0.017166, mae: 0.770151, mean_q: 0.097676, mean_eps: 0.000000
 6849/10000: episode: 207, duration: 0.412s, episode steps:  26, steps per second:  63, episode reward: -34.440, mean reward: -1.325 [-32.266,  2.720], mean action: 9.962 [0.000, 20.000],  loss: 0.017099, mae: 0.807609, mean_q: 0.089695, mean_eps: 0.000000
 6882/10000: episode: 208, duration: 0.409s, episode steps:  33, steps per second:  81, episode reward: 32.067, mean reward:  0.972 [-3.000, 32.250], mean action: 12.121 [2.000, 20.000],  loss: 0.015776, mae: 0.833007, mean_q: 0.051383, mean_eps: 0.000000
 6908/10000: episode: 209, duration: 0.329s, episode steps:  26, steps per second:  79, episode reward: -35.370, mean reward: -1.360 [-32.223,  3.000], mean action: 11.269 [2.000, 20.000],  loss: 0.018945, mae: 0.850340, mean_q: 0.017841, mean_eps: 0.000000
 6977/10000: episode: 210, duration: 0.876s, episode steps:  69, steps per second:  79, episode reward: -35.300, mean reward: -0.512 [-31.646,  2.492], mean action: 8.420 [2.000, 20.000],  loss: 0.015036, mae: 0.821465, mean_q: 0.050730, mean_eps: 0.000000
 6995/10000: episode: 211, duration: 0.221s, episode steps:  18, steps per second:  82, episode reward: 41.741, mean reward:  2.319 [-2.511, 32.190], mean action: 4.889 [2.000, 16.000],  loss: 0.017965, mae: 0.777676, mean_q: 0.129546, mean_eps: 0.000000
 7016/10000: episode: 212, duration: 0.255s, episode steps:  21, steps per second:  82, episode reward: 40.599, mean reward:  1.933 [-2.416, 31.932], mean action: 8.714 [2.000, 15.000],  loss: 0.019378, mae: 0.801328, mean_q: 0.072494, mean_eps: 0.000000
 7048/10000: episode: 213, duration: 0.352s, episode steps:  32, steps per second:  91, episode reward: 32.500, mean reward:  1.016 [-3.000, 32.150], mean action: 10.875 [2.000, 15.000],  loss: 0.017354, mae: 0.762747, mean_q: 0.135505, mean_eps: 0.000000
 7073/10000: episode: 214, duration: 0.292s, episode steps:  25, steps per second:  86, episode reward: 35.970, mean reward:  1.439 [-2.494, 32.970], mean action: 5.240 [2.000, 15.000],  loss: 0.017117, mae: 0.755328, mean_q: 0.135628, mean_eps: 0.000000
 7100/10000: episode: 215, duration: 0.298s, episode steps:  27, steps per second:  91, episode reward: -35.330, mean reward: -1.309 [-32.144,  2.869], mean action: 9.778 [2.000, 15.000],  loss: 0.016005, mae: 0.804264, mean_q: 0.073993, mean_eps: 0.000000
 7140/10000: episode: 216, duration: 0.438s, episode steps:  40, steps per second:  91, episode reward: -35.100, mean reward: -0.878 [-31.914,  2.280], mean action: 9.000 [2.000, 19.000],  loss: 0.015696, mae: 0.803413, mean_q: 0.048853, mean_eps: 0.000000
 7172/10000: episode: 217, duration: 0.363s, episode steps:  32, steps per second:  88, episode reward: 32.734, mean reward:  1.023 [-2.261, 33.000], mean action: 12.062 [2.000, 15.000],  loss: 0.017694, mae: 0.804785, mean_q: 0.028155, mean_eps: 0.000000
 7229/10000: episode: 218, duration: 0.624s, episode steps:  57, steps per second:  91, episode reward: 32.748, mean reward:  0.575 [-2.998, 32.748], mean action: 8.526 [2.000, 19.000],  loss: 0.017164, mae: 0.813605, mean_q: 0.076474, mean_eps: 0.000000
 7251/10000: episode: 219, duration: 0.288s, episode steps:  22, steps per second:  76, episode reward: 37.687, mean reward:  1.713 [-2.415, 32.895], mean action: 10.636 [2.000, 16.000],  loss: 0.017267, mae: 0.871431, mean_q: -0.003328, mean_eps: 0.000000
 7271/10000: episode: 220, duration: 0.288s, episode steps:  20, steps per second:  69, episode reward: 35.963, mean reward:  1.798 [-2.900, 32.120], mean action: 9.650 [2.000, 20.000],  loss: 0.019460, mae: 0.818024, mean_q: 0.078487, mean_eps: 0.000000
 7304/10000: episode: 221, duration: 0.439s, episode steps:  33, steps per second:  75, episode reward: -38.340, mean reward: -1.162 [-32.081,  2.263], mean action: 11.848 [2.000, 20.000],  loss: 0.018814, mae: 0.834081, mean_q: 0.046959, mean_eps: 0.000000
 7327/10000: episode: 222, duration: 0.269s, episode steps:  23, steps per second:  85, episode reward: -44.120, mean reward: -1.918 [-32.043,  2.028], mean action: 14.522 [2.000, 20.000],  loss: 0.018179, mae: 0.803858, mean_q: 0.051291, mean_eps: 0.000000
 7357/10000: episode: 223, duration: 0.351s, episode steps:  30, steps per second:  85, episode reward: -32.130, mean reward: -1.071 [-31.907,  2.980], mean action: 10.767 [2.000, 16.000],  loss: 0.017137, mae: 0.798291, mean_q: 0.054026, mean_eps: 0.000000
 7377/10000: episode: 224, duration: 0.258s, episode steps:  20, steps per second:  77, episode reward: -44.420, mean reward: -2.221 [-32.203,  2.010], mean action: 12.000 [2.000, 15.000],  loss: 0.017913, mae: 0.773729, mean_q: 0.119020, mean_eps: 0.000000
 7396/10000: episode: 225, duration: 0.264s, episode steps:  19, steps per second:  72, episode reward: 38.678, mean reward:  2.036 [-2.336, 32.270], mean action: 6.895 [2.000, 16.000],  loss: 0.016638, mae: 0.805459, mean_q: 0.073453, mean_eps: 0.000000
 7424/10000: episode: 226, duration: 0.830s, episode steps:  28, steps per second:  34, episode reward: -32.270, mean reward: -1.153 [-32.188,  3.000], mean action: 8.571 [2.000, 15.000],  loss: 0.015557, mae: 0.768224, mean_q: 0.110093, mean_eps: 0.000000
 7454/10000: episode: 227, duration: 0.365s, episode steps:  30, steps per second:  82, episode reward: -38.910, mean reward: -1.297 [-32.375,  2.325], mean action: 13.000 [2.000, 20.000],  loss: 0.014121, mae: 0.821659, mean_q: 0.039633, mean_eps: 0.000000
 7488/10000: episode: 228, duration: 0.451s, episode steps:  34, steps per second:  75, episode reward: -38.200, mean reward: -1.124 [-32.278,  3.060], mean action: 13.147 [2.000, 20.000],  loss: 0.018190, mae: 0.816113, mean_q: 0.085448, mean_eps: 0.000000
 7519/10000: episode: 229, duration: 0.389s, episode steps:  31, steps per second:  80, episode reward: -32.120, mean reward: -1.036 [-31.748,  2.490], mean action: 7.387 [2.000, 16.000],  loss: 0.014823, mae: 0.827169, mean_q: 0.002677, mean_eps: 0.000000
 7558/10000: episode: 230, duration: 0.480s, episode steps:  39, steps per second:  81, episode reward: -32.910, mean reward: -0.844 [-31.914,  2.574], mean action: 11.410 [2.000, 20.000],  loss: 0.015185, mae: 0.805603, mean_q: 0.065814, mean_eps: 0.000000
 7600/10000: episode: 231, duration: 0.652s, episode steps:  42, steps per second:  64, episode reward: 32.129, mean reward:  0.765 [-3.000, 32.129], mean action: 10.952 [2.000, 15.000],  loss: 0.016117, mae: 0.804256, mean_q: 0.051345, mean_eps: 0.000000
 7623/10000: episode: 232, duration: 0.293s, episode steps:  23, steps per second:  79, episode reward: 41.586, mean reward:  1.808 [-2.600, 31.686], mean action: 6.043 [2.000, 16.000],  loss: 0.014497, mae: 0.792811, mean_q: 0.075148, mean_eps: 0.000000
 7652/10000: episode: 233, duration: 0.334s, episode steps:  29, steps per second:  87, episode reward: 34.685, mean reward:  1.196 [-2.544, 32.640], mean action: 11.828 [2.000, 15.000],  loss: 0.017811, mae: 0.828780, mean_q: 0.116808, mean_eps: 0.000000
 7682/10000: episode: 234, duration: 0.335s, episode steps:  30, steps per second:  90, episode reward: -41.700, mean reward: -1.390 [-31.919,  2.390], mean action: 13.900 [2.000, 20.000],  loss: 0.019048, mae: 0.842795, mean_q: 0.085233, mean_eps: 0.000000
 7710/10000: episode: 235, duration: 0.333s, episode steps:  28, steps per second:  84, episode reward: -39.000, mean reward: -1.393 [-32.477,  2.200], mean action: 7.393 [2.000, 20.000],  loss: 0.015397, mae: 0.815768, mean_q: 0.091299, mean_eps: 0.000000
 7735/10000: episode: 236, duration: 0.309s, episode steps:  25, steps per second:  81, episode reward: -36.000, mean reward: -1.440 [-32.075,  2.275], mean action: 11.280 [2.000, 20.000],  loss: 0.015430, mae: 0.805138, mean_q: 0.087532, mean_eps: 0.000000
 7775/10000: episode: 237, duration: 0.487s, episode steps:  40, steps per second:  82, episode reward: 33.000, mean reward:  0.825 [-2.475, 29.514], mean action: 6.950 [0.000, 19.000],  loss: 0.015467, mae: 0.865392, mean_q: 0.032328, mean_eps: 0.000000
 7794/10000: episode: 238, duration: 0.250s, episode steps:  19, steps per second:  76, episode reward: 33.000, mean reward:  1.737 [-3.000, 33.000], mean action: 6.737 [2.000, 20.000],  loss: 0.017341, mae: 0.816126, mean_q: 0.084815, mean_eps: 0.000000
 7812/10000: episode: 239, duration: 0.244s, episode steps:  18, steps per second:  74, episode reward: 40.862, mean reward:  2.270 [-3.000, 32.831], mean action: 8.500 [2.000, 20.000],  loss: 0.015807, mae: 0.802805, mean_q: 0.065694, mean_eps: 0.000000
 7835/10000: episode: 240, duration: 0.331s, episode steps:  23, steps per second:  69, episode reward: -35.180, mean reward: -1.530 [-31.863,  2.900], mean action: 7.478 [2.000, 20.000],  loss: 0.019269, mae: 0.820723, mean_q: 0.064924, mean_eps: 0.000000
 7860/10000: episode: 241, duration: 0.337s, episode steps:  25, steps per second:  74, episode reward: -38.920, mean reward: -1.557 [-32.130,  2.887], mean action: 13.880 [2.000, 20.000],  loss: 0.013933, mae: 0.788175, mean_q: 0.057815, mean_eps: 0.000000
 7899/10000: episode: 242, duration: 0.464s, episode steps:  39, steps per second:  84, episode reward: 32.211, mean reward:  0.826 [-2.502, 32.130], mean action: 11.564 [2.000, 20.000],  loss: 0.014998, mae: 0.793501, mean_q: 0.044946, mean_eps: 0.000000
 7924/10000: episode: 243, duration: 0.313s, episode steps:  25, steps per second:  80, episode reward: 40.541, mean reward:  1.622 [-2.213, 32.420], mean action: 10.880 [2.000, 20.000],  loss: 0.018090, mae: 0.827607, mean_q: 0.037483, mean_eps: 0.000000
 7962/10000: episode: 244, duration: 0.429s, episode steps:  38, steps per second:  88, episode reward: -41.760, mean reward: -1.099 [-32.064,  2.414], mean action: 12.026 [2.000, 20.000],  loss: 0.015914, mae: 0.781850, mean_q: 0.142782, mean_eps: 0.000000
 7982/10000: episode: 245, duration: 0.236s, episode steps:  20, steps per second:  85, episode reward: -39.000, mean reward: -1.950 [-32.247,  3.000], mean action: 10.750 [2.000, 15.000],  loss: 0.018254, mae: 0.776837, mean_q: 0.177701, mean_eps: 0.000000
 8022/10000: episode: 246, duration: 0.434s, episode steps:  40, steps per second:  92, episode reward: -32.150, mean reward: -0.804 [-31.796,  3.000], mean action: 12.075 [2.000, 20.000],  loss: 0.016737, mae: 0.814712, mean_q: 0.069918, mean_eps: 0.000000
 8067/10000: episode: 247, duration: 0.539s, episode steps:  45, steps per second:  83, episode reward: -39.000, mean reward: -0.867 [-32.269,  3.000], mean action: 13.733 [2.000, 20.000],  loss: 0.016025, mae: 0.818363, mean_q: 0.039233, mean_eps: 0.000000
 8082/10000: episode: 248, duration: 0.202s, episode steps:  15, steps per second:  74, episode reward: -44.320, mean reward: -2.955 [-31.989,  2.270], mean action: 11.867 [2.000, 20.000],  loss: 0.018381, mae: 0.784791, mean_q: 0.127946, mean_eps: 0.000000
 8113/10000: episode: 249, duration: 0.347s, episode steps:  31, steps per second:  89, episode reward: -33.000, mean reward: -1.065 [-32.086,  2.910], mean action: 7.097 [2.000, 15.000],  loss: 0.016226, mae: 0.817913, mean_q: 0.079033, mean_eps: 0.000000
 8138/10000: episode: 250, duration: 0.302s, episode steps:  25, steps per second:  83, episode reward: -32.510, mean reward: -1.300 [-32.410,  2.500], mean action: 8.080 [2.000, 20.000],  loss: 0.018557, mae: 0.868214, mean_q: 0.010306, mean_eps: 0.000000
 8167/10000: episode: 251, duration: 0.342s, episode steps:  29, steps per second:  85, episode reward: 32.522, mean reward:  1.121 [-2.587, 32.270], mean action: 10.759 [2.000, 15.000],  loss: 0.017007, mae: 0.834953, mean_q: 0.064364, mean_eps: 0.000000
 8193/10000: episode: 252, duration: 0.312s, episode steps:  26, steps per second:  83, episode reward: 38.134, mean reward:  1.467 [-2.359, 30.996], mean action: 9.731 [2.000, 15.000],  loss: 0.016251, mae: 0.825628, mean_q: 0.075337, mean_eps: 0.000000
 8218/10000: episode: 253, duration: 0.295s, episode steps:  25, steps per second:  85, episode reward: 38.700, mean reward:  1.548 [-2.730, 31.980], mean action: 8.480 [2.000, 15.000],  loss: 0.014913, mae: 0.809596, mean_q: 0.083611, mean_eps: 0.000000
 8254/10000: episode: 254, duration: 0.437s, episode steps:  36, steps per second:  82, episode reward: -35.370, mean reward: -0.982 [-32.352,  2.291], mean action: 10.556 [0.000, 19.000],  loss: 0.015043, mae: 0.774420, mean_q: 0.133613, mean_eps: 0.000000
 8283/10000: episode: 255, duration: 0.333s, episode steps:  29, steps per second:  87, episode reward: 34.686, mean reward:  1.196 [-3.000, 31.861], mean action: 11.345 [2.000, 16.000],  loss: 0.018091, mae: 0.832810, mean_q: 0.037416, mean_eps: 0.000000
 8312/10000: episode: 256, duration: 0.333s, episode steps:  29, steps per second:  87, episode reward: -46.870, mean reward: -1.616 [-31.556,  0.000], mean action: 16.310 [2.000, 20.000],  loss: 0.014177, mae: 0.793707, mean_q: 0.029594, mean_eps: 0.000000
 8345/10000: episode: 257, duration: 0.383s, episode steps:  33, steps per second:  86, episode reward: -35.110, mean reward: -1.064 [-32.252,  2.901], mean action: 8.091 [2.000, 15.000],  loss: 0.017644, mae: 0.788963, mean_q: 0.079206, mean_eps: 0.000000
 8363/10000: episode: 258, duration: 0.209s, episode steps:  18, steps per second:  86, episode reward: -38.770, mean reward: -2.154 [-32.289,  2.200], mean action: 10.444 [2.000, 18.000],  loss: 0.015075, mae: 0.798424, mean_q: 0.087329, mean_eps: 0.000000
 8399/10000: episode: 259, duration: 0.733s, episode steps:  36, steps per second:  49, episode reward: 37.524, mean reward:  1.042 [-2.215, 32.623], mean action: 10.750 [2.000, 20.000],  loss: 0.018454, mae: 0.818944, mean_q: 0.095666, mean_eps: 0.000000
 8424/10000: episode: 260, duration: 0.302s, episode steps:  25, steps per second:  83, episode reward: -38.270, mean reward: -1.531 [-31.339,  2.460], mean action: 11.120 [2.000, 20.000],  loss: 0.016612, mae: 0.779113, mean_q: 0.167594, mean_eps: 0.000000
 8448/10000: episode: 261, duration: 0.274s, episode steps:  24, steps per second:  88, episode reward: -35.320, mean reward: -1.472 [-32.636,  2.494], mean action: 7.333 [2.000, 15.000],  loss: 0.018378, mae: 0.806764, mean_q: 0.072356, mean_eps: 0.000000
 8468/10000: episode: 262, duration: 0.231s, episode steps:  20, steps per second:  87, episode reward: 38.201, mean reward:  1.910 [-2.839, 32.400], mean action: 6.900 [2.000, 15.000],  loss: 0.017049, mae: 0.795308, mean_q: 0.094200, mean_eps: 0.000000
 8492/10000: episode: 263, duration: 0.357s, episode steps:  24, steps per second:  67, episode reward: 35.106, mean reward:  1.463 [-3.000, 32.898], mean action: 4.375 [2.000, 13.000],  loss: 0.017884, mae: 0.822270, mean_q: 0.072788, mean_eps: 0.000000
 8514/10000: episode: 264, duration: 0.268s, episode steps:  22, steps per second:  82, episode reward: -32.910, mean reward: -1.496 [-32.149,  3.000], mean action: 13.455 [2.000, 20.000],  loss: 0.016294, mae: 0.795867, mean_q: 0.093328, mean_eps: 0.000000
 8541/10000: episode: 265, duration: 0.312s, episode steps:  27, steps per second:  87, episode reward: -32.660, mean reward: -1.210 [-32.413,  2.730], mean action: 8.926 [2.000, 15.000],  loss: 0.019485, mae: 0.840822, mean_q: 0.046716, mean_eps: 0.000000
 8565/10000: episode: 266, duration: 0.284s, episode steps:  24, steps per second:  85, episode reward: 35.015, mean reward:  1.459 [-2.707, 31.863], mean action: 9.667 [2.000, 20.000],  loss: 0.014262, mae: 0.788312, mean_q: 0.038821, mean_eps: 0.000000
 8607/10000: episode: 267, duration: 0.480s, episode steps:  42, steps per second:  87, episode reward: 40.027, mean reward:  0.953 [-2.433, 32.120], mean action: 5.095 [2.000, 15.000],  loss: 0.017440, mae: 0.796148, mean_q: 0.077227, mean_eps: 0.000000
 8628/10000: episode: 268, duration: 0.231s, episode steps:  21, steps per second:  91, episode reward: -44.510, mean reward: -2.120 [-32.247,  2.050], mean action: 16.000 [6.000, 19.000],  loss: 0.016840, mae: 0.830818, mean_q: 0.042772, mean_eps: 0.000000
 8662/10000: episode: 269, duration: 0.402s, episode steps:  34, steps per second:  84, episode reward: -35.880, mean reward: -1.055 [-32.046,  3.000], mean action: 10.441 [2.000, 20.000],  loss: 0.018635, mae: 0.812908, mean_q: 0.061685, mean_eps: 0.000000
 8689/10000: episode: 270, duration: 0.307s, episode steps:  27, steps per second:  88, episode reward: -35.910, mean reward: -1.330 [-32.030,  3.000], mean action: 9.963 [2.000, 20.000],  loss: 0.013438, mae: 0.724512, mean_q: 0.132748, mean_eps: 0.000000
 8721/10000: episode: 271, duration: 0.362s, episode steps:  32, steps per second:  88, episode reward: 32.265, mean reward:  1.008 [-2.920, 32.150], mean action: 8.125 [2.000, 19.000],  loss: 0.017994, mae: 0.816434, mean_q: 0.072136, mean_eps: 0.000000
 8757/10000: episode: 272, duration: 0.420s, episode steps:  36, steps per second:  86, episode reward: 37.886, mean reward:  1.052 [-2.273, 32.172], mean action: 10.583 [2.000, 19.000],  loss: 0.018824, mae: 0.784339, mean_q: 0.091068, mean_eps: 0.000000
 8780/10000: episode: 273, duration: 0.293s, episode steps:  23, steps per second:  79, episode reward: -35.180, mean reward: -1.530 [-31.726,  2.600], mean action: 11.043 [2.000, 20.000],  loss: 0.017525, mae: 0.807316, mean_q: 0.030276, mean_eps: 0.000000
 8814/10000: episode: 274, duration: 0.405s, episode steps:  34, steps per second:  84, episode reward: -35.280, mean reward: -1.038 [-31.642,  2.339], mean action: 10.088 [2.000, 19.000],  loss: 0.017424, mae: 0.784896, mean_q: 0.071722, mean_eps: 0.000000
 8859/10000: episode: 275, duration: 0.617s, episode steps:  45, steps per second:  73, episode reward: -32.680, mean reward: -0.726 [-32.119,  2.670], mean action: 7.422 [2.000, 19.000],  loss: 0.018889, mae: 0.797398, mean_q: 0.039205, mean_eps: 0.000000
 8891/10000: episode: 276, duration: 0.436s, episode steps:  32, steps per second:  73, episode reward: -33.000, mean reward: -1.031 [-32.529,  2.202], mean action: 10.188 [2.000, 19.000],  loss: 0.016640, mae: 0.755444, mean_q: 0.098435, mean_eps: 0.000000
 8919/10000: episode: 277, duration: 0.378s, episode steps:  28, steps per second:  74, episode reward: -32.630, mean reward: -1.165 [-32.081,  2.903], mean action: 9.714 [2.000, 20.000],  loss: 0.015801, mae: 0.781846, mean_q: 0.067795, mean_eps: 0.000000
 8956/10000: episode: 278, duration: 0.441s, episode steps:  37, steps per second:  84, episode reward: 32.412, mean reward:  0.876 [-2.442, 32.030], mean action: 10.784 [2.000, 16.000],  loss: 0.016534, mae: 0.789177, mean_q: 0.079393, mean_eps: 0.000000
 8980/10000: episode: 279, duration: 0.286s, episode steps:  24, steps per second:  84, episode reward: -35.450, mean reward: -1.477 [-32.213,  2.537], mean action: 12.125 [2.000, 18.000],  loss: 0.017592, mae: 0.819884, mean_q: 0.004972, mean_eps: 0.000000
 9007/10000: episode: 280, duration: 0.366s, episode steps:  27, steps per second:  74, episode reward: 36.604, mean reward:  1.356 [-2.179, 32.200], mean action: 7.593 [2.000, 19.000],  loss: 0.015339, mae: 0.772431, mean_q: 0.080683, mean_eps: 0.000000
 9030/10000: episode: 281, duration: 0.284s, episode steps:  23, steps per second:  81, episode reward: -41.840, mean reward: -1.819 [-32.053,  3.000], mean action: 13.913 [2.000, 20.000],  loss: 0.018794, mae: 0.753686, mean_q: 0.115279, mean_eps: 0.000000
 9088/10000: episode: 282, duration: 1.274s, episode steps:  58, steps per second:  46, episode reward: -35.830, mean reward: -0.618 [-32.345,  2.407], mean action: 6.759 [2.000, 18.000],  loss: 0.019139, mae: 0.780443, mean_q: 0.111198, mean_eps: 0.000000
 9117/10000: episode: 283, duration: 0.594s, episode steps:  29, steps per second:  49, episode reward: -44.860, mean reward: -1.547 [-32.005,  2.500], mean action: 13.448 [1.000, 19.000],  loss: 0.019345, mae: 0.778138, mean_q: 0.135807, mean_eps: 0.000000
 9144/10000: episode: 284, duration: 0.375s, episode steps:  27, steps per second:  72, episode reward: 33.000, mean reward:  1.222 [-2.169, 32.540], mean action: 6.370 [2.000, 16.000],  loss: 0.015811, mae: 0.787212, mean_q: 0.095422, mean_eps: 0.000000
 9176/10000: episode: 285, duration: 0.445s, episode steps:  32, steps per second:  72, episode reward: -35.580, mean reward: -1.112 [-32.003,  2.502], mean action: 10.094 [2.000, 16.000],  loss: 0.018048, mae: 0.825252, mean_q: 0.026323, mean_eps: 0.000000
 9237/10000: episode: 286, duration: 0.758s, episode steps:  61, steps per second:  81, episode reward: -33.000, mean reward: -0.541 [-32.255,  2.442], mean action: 13.148 [2.000, 20.000],  loss: 0.017112, mae: 0.808659, mean_q: 0.086565, mean_eps: 0.000000
 9261/10000: episode: 287, duration: 0.313s, episode steps:  24, steps per second:  77, episode reward: -35.810, mean reward: -1.492 [-32.662,  2.610], mean action: 11.667 [2.000, 20.000],  loss: 0.018562, mae: 0.857225, mean_q: 0.012212, mean_eps: 0.000000
 9294/10000: episode: 288, duration: 0.437s, episode steps:  33, steps per second:  75, episode reward: -35.550, mean reward: -1.077 [-32.223,  2.436], mean action: 12.273 [2.000, 15.000],  loss: 0.016391, mae: 0.864714, mean_q: 0.021297, mean_eps: 0.000000
 9331/10000: episode: 289, duration: 0.462s, episode steps:  37, steps per second:  80, episode reward: -35.810, mean reward: -0.968 [-32.239,  2.700], mean action: 11.378 [2.000, 15.000],  loss: 0.016752, mae: 0.867921, mean_q: 0.033116, mean_eps: 0.000000
 9380/10000: episode: 290, duration: 0.588s, episode steps:  49, steps per second:  83, episode reward: 32.727, mean reward:  0.668 [-2.816, 31.897], mean action: 13.143 [2.000, 20.000],  loss: 0.016854, mae: 0.797617, mean_q: 0.103854, mean_eps: 0.000000
 9404/10000: episode: 291, duration: 0.302s, episode steps:  24, steps per second:  79, episode reward: -35.070, mean reward: -1.461 [-32.057,  2.940], mean action: 7.625 [2.000, 15.000],  loss: 0.016635, mae: 0.823835, mean_q: 0.062007, mean_eps: 0.000000
 9431/10000: episode: 292, duration: 0.337s, episode steps:  27, steps per second:  80, episode reward: -36.000, mean reward: -1.333 [-32.199,  3.000], mean action: 12.037 [2.000, 20.000],  loss: 0.019178, mae: 0.838266, mean_q: 0.070712, mean_eps: 0.000000
 9478/10000: episode: 293, duration: 0.561s, episode steps:  47, steps per second:  84, episode reward: -36.000, mean reward: -0.766 [-32.062,  3.000], mean action: 9.447 [2.000, 20.000],  loss: 0.019612, mae: 0.831014, mean_q: 0.085320, mean_eps: 0.000000
 9506/10000: episode: 294, duration: 0.464s, episode steps:  28, steps per second:  60, episode reward: 38.584, mean reward:  1.378 [-2.735, 32.310], mean action: 10.357 [2.000, 15.000],  loss: 0.018804, mae: 0.870508, mean_q: 0.052081, mean_eps: 0.000000
 9528/10000: episode: 295, duration: 0.328s, episode steps:  22, steps per second:  67, episode reward: 39.815, mean reward:  1.810 [-2.553, 31.755], mean action: 7.591 [2.000, 15.000],  loss: 0.016428, mae: 0.862249, mean_q: 0.063000, mean_eps: 0.000000
 9552/10000: episode: 296, duration: 0.320s, episode steps:  24, steps per second:  75, episode reward: -38.770, mean reward: -1.615 [-32.139,  2.390], mean action: 12.708 [2.000, 15.000],  loss: 0.016079, mae: 0.861864, mean_q: 0.079762, mean_eps: 0.000000
 9573/10000: episode: 297, duration: 0.292s, episode steps:  21, steps per second:  72, episode reward: -38.190, mean reward: -1.819 [-32.770,  2.240], mean action: 10.429 [2.000, 16.000],  loss: 0.012042, mae: 0.835981, mean_q: 0.058600, mean_eps: 0.000000
 9590/10000: episode: 298, duration: 0.194s, episode steps:  17, steps per second:  88, episode reward: -41.900, mean reward: -2.465 [-31.922,  2.310], mean action: 12.529 [2.000, 20.000],  loss: 0.021387, mae: 0.902778, mean_q: 0.011459, mean_eps: 0.000000
 9610/10000: episode: 299, duration: 0.234s, episode steps:  20, steps per second:  85, episode reward: -36.000, mean reward: -1.800 [-32.225,  3.000], mean action: 9.000 [2.000, 20.000],  loss: 0.014950, mae: 0.805041, mean_q: 0.072624, mean_eps: 0.000000
 9629/10000: episode: 300, duration: 0.228s, episode steps:  19, steps per second:  84, episode reward: 38.454, mean reward:  2.024 [-2.408, 32.311], mean action: 8.526 [2.000, 15.000],  loss: 0.015121, mae: 0.807636, mean_q: 0.104743, mean_eps: 0.000000
 9658/10000: episode: 301, duration: 0.336s, episode steps:  29, steps per second:  86, episode reward: -33.000, mean reward: -1.138 [-32.106,  2.521], mean action: 10.103 [2.000, 20.000],  loss: 0.017051, mae: 0.827006, mean_q: 0.062581, mean_eps: 0.000000
 9685/10000: episode: 302, duration: 0.308s, episode steps:  27, steps per second:  88, episode reward: -32.910, mean reward: -1.219 [-32.015,  3.000], mean action: 9.222 [2.000, 20.000],  loss: 0.014858, mae: 0.781212, mean_q: 0.169656, mean_eps: 0.000000
 9722/10000: episode: 303, duration: 0.423s, episode steps:  37, steps per second:  88, episode reward: -33.000, mean reward: -0.892 [-32.041,  2.819], mean action: 9.568 [2.000, 15.000],  loss: 0.014949, mae: 0.770154, mean_q: 0.107647, mean_eps: 0.000000
 9753/10000: episode: 304, duration: 0.368s, episode steps:  31, steps per second:  84, episode reward: 32.734, mean reward:  1.056 [-2.709, 32.320], mean action: 11.097 [2.000, 16.000],  loss: 0.016036, mae: 0.816743, mean_q: 0.040992, mean_eps: 0.000000
 9774/10000: episode: 305, duration: 0.252s, episode steps:  21, steps per second:  83, episode reward: -35.630, mean reward: -1.697 [-31.778,  2.743], mean action: 11.667 [2.000, 15.000],  loss: 0.017922, mae: 0.796631, mean_q: 0.076658, mean_eps: 0.000000
 9810/10000: episode: 306, duration: 0.431s, episode steps:  36, steps per second:  83, episode reward: -35.180, mean reward: -0.977 [-32.380,  2.840], mean action: 12.194 [2.000, 20.000],  loss: 0.019610, mae: 0.849785, mean_q: 0.040169, mean_eps: 0.000000
 9839/10000: episode: 307, duration: 0.330s, episode steps:  29, steps per second:  88, episode reward: -41.160, mean reward: -1.419 [-32.125,  3.000], mean action: 12.379 [1.000, 15.000],  loss: 0.018521, mae: 0.832850, mean_q: 0.058741, mean_eps: 0.000000
 9863/10000: episode: 308, duration: 0.279s, episode steps:  24, steps per second:  86, episode reward: 33.000, mean reward:  1.375 [-3.000, 32.470], mean action: 9.583 [2.000, 18.000],  loss: 0.021647, mae: 0.816039, mean_q: 0.080678, mean_eps: 0.000000
 9900/10000: episode: 309, duration: 0.407s, episode steps:  37, steps per second:  91, episode reward: -38.350, mean reward: -1.036 [-32.152,  2.200], mean action: 9.838 [2.000, 18.000],  loss: 0.016009, mae: 0.782796, mean_q: 0.090495, mean_eps: 0.000000
 9934/10000: episode: 310, duration: 0.386s, episode steps:  34, steps per second:  88, episode reward: -32.910, mean reward: -0.968 [-32.300,  3.000], mean action: 10.471 [2.000, 20.000],  loss: 0.018117, mae: 0.800907, mean_q: 0.051938, mean_eps: 0.000000
 9956/10000: episode: 311, duration: 0.238s, episode steps:  22, steps per second:  92, episode reward: -41.370, mean reward: -1.880 [-31.449,  2.250], mean action: 13.636 [2.000, 20.000],  loss: 0.015307, mae: 0.803798, mean_q: 0.073090, mean_eps: 0.000000
 9987/10000: episode: 312, duration: 0.358s, episode steps:  31, steps per second:  87, episode reward: -35.360, mean reward: -1.141 [-32.193,  3.000], mean action: 9.226 [2.000, 20.000],  loss: 0.020642, mae: 0.826993, mean_q: 0.060557, mean_eps: 0.000000
done, took 120.573 seconds
Results against random player:
DQN Evaluation: 222 victories out of 300 episodes

Results against max player:
DQN Evaluation: 113 victories out of 300 episodes
