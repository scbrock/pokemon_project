params 0
{'model_type': 'dqn_agent', 'l1_out': 128, 'l2_out': 64, 'gamma': 0.5, 'target_model_update': 1, 'delta_clip': 0.01, 'nb_steps_warmup': 1000, 'enable_dueling_dqn__': False, 'enable_double_dqn__': True, 'dueling_type__': 'avg'}
trial 0
Training for 10000 steps ...
   34/10000: episode: 1, duration: 1.277s, episode steps:  34, steps per second:  27, episode reward: -39.000, mean reward: -1.147 [-32.079,  3.000], mean action: 14.941 [7.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   53/10000: episode: 2, duration: 0.156s, episode steps:  19, steps per second: 122, episode reward: -39.000, mean reward: -2.053 [-32.704,  2.250], mean action: 14.316 [3.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   81/10000: episode: 3, duration: 0.196s, episode steps:  28, steps per second: 143, episode reward: -41.910, mean reward: -1.497 [-32.040,  2.700], mean action: 14.607 [7.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  102/10000: episode: 4, duration: 0.138s, episode steps:  21, steps per second: 152, episode reward: -42.000, mean reward: -2.000 [-32.219,  2.172], mean action: 13.571 [7.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  132/10000: episode: 5, duration: 0.206s, episode steps:  30, steps per second: 146, episode reward: -38.910, mean reward: -1.297 [-32.210,  2.272], mean action: 15.100 [1.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  161/10000: episode: 6, duration: 0.198s, episode steps:  29, steps per second: 147, episode reward: -38.710, mean reward: -1.335 [-32.061,  2.620], mean action: 13.517 [3.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  195/10000: episode: 7, duration: 0.217s, episode steps:  34, steps per second: 156, episode reward: -32.130, mean reward: -0.945 [-32.148,  2.900], mean action: 15.206 [6.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  249/10000: episode: 8, duration: 0.348s, episode steps:  54, steps per second: 155, episode reward: 32.841, mean reward:  0.608 [-2.566, 32.330], mean action: 13.389 [3.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  279/10000: episode: 9, duration: 0.204s, episode steps:  30, steps per second: 147, episode reward: -35.480, mean reward: -1.183 [-31.553,  2.410], mean action: 15.533 [3.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  312/10000: episode: 10, duration: 0.235s, episode steps:  33, steps per second: 140, episode reward: -41.860, mean reward: -1.268 [-32.047,  2.570], mean action: 14.727 [7.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  356/10000: episode: 11, duration: 0.350s, episode steps:  44, steps per second: 126, episode reward: -47.190, mean reward: -1.073 [-32.045,  0.470], mean action: 16.114 [7.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  386/10000: episode: 12, duration: 0.196s, episode steps:  30, steps per second: 153, episode reward: -35.810, mean reward: -1.194 [-32.285,  3.000], mean action: 11.667 [3.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  415/10000: episode: 13, duration: 0.173s, episode steps:  29, steps per second: 167, episode reward: -41.630, mean reward: -1.436 [-32.412,  2.370], mean action: 15.172 [7.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  445/10000: episode: 14, duration: 0.189s, episode steps:  30, steps per second: 158, episode reward: -47.340, mean reward: -1.578 [-32.545,  0.224], mean action: 14.000 [7.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  471/10000: episode: 15, duration: 0.155s, episode steps:  26, steps per second: 168, episode reward: -41.810, mean reward: -1.608 [-32.335,  2.112], mean action: 16.462 [7.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  493/10000: episode: 16, duration: 0.140s, episode steps:  22, steps per second: 158, episode reward: -44.050, mean reward: -2.002 [-32.084,  2.066], mean action: 17.000 [7.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  521/10000: episode: 17, duration: 0.163s, episode steps:  28, steps per second: 172, episode reward: -38.230, mean reward: -1.365 [-31.837,  3.046], mean action: 13.679 [3.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  546/10000: episode: 18, duration: 0.174s, episode steps:  25, steps per second: 144, episode reward: -44.470, mean reward: -1.779 [-32.389,  2.300], mean action: 15.640 [7.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  584/10000: episode: 19, duration: 0.235s, episode steps:  38, steps per second: 162, episode reward: -41.650, mean reward: -1.096 [-31.943,  2.184], mean action: 13.974 [3.000, 18.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  611/10000: episode: 20, duration: 0.160s, episode steps:  27, steps per second: 169, episode reward: -38.830, mean reward: -1.438 [-32.211,  2.171], mean action: 15.000 [7.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  673/10000: episode: 21, duration: 0.380s, episode steps:  62, steps per second: 163, episode reward: -35.280, mean reward: -0.569 [-31.502,  2.833], mean action: 11.871 [3.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  695/10000: episode: 22, duration: 0.126s, episode steps:  22, steps per second: 174, episode reward: -41.110, mean reward: -1.869 [-32.303,  2.810], mean action: 15.682 [7.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  730/10000: episode: 23, duration: 0.226s, episode steps:  35, steps per second: 155, episode reward: -44.380, mean reward: -1.268 [-31.850,  2.091], mean action: 12.343 [7.000, 17.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  776/10000: episode: 24, duration: 0.278s, episode steps:  46, steps per second: 166, episode reward: -44.090, mean reward: -0.958 [-32.047,  2.080], mean action: 16.043 [7.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  838/10000: episode: 25, duration: 0.383s, episode steps:  62, steps per second: 162, episode reward: -35.790, mean reward: -0.577 [-32.225,  2.270], mean action: 15.839 [14.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  873/10000: episode: 26, duration: 0.221s, episode steps:  35, steps per second: 158, episode reward: -42.000, mean reward: -1.200 [-32.221,  2.421], mean action: 15.514 [6.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  921/10000: episode: 27, duration: 0.279s, episode steps:  48, steps per second: 172, episode reward: -32.930, mean reward: -0.686 [-32.342,  3.059], mean action: 15.771 [3.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  953/10000: episode: 28, duration: 0.185s, episode steps:  32, steps per second: 173, episode reward: -41.450, mean reward: -1.295 [-32.206,  2.220], mean action: 16.500 [15.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  996/10000: episode: 29, duration: 0.241s, episode steps:  43, steps per second: 178, episode reward: -44.010, mean reward: -1.023 [-32.103,  2.450], mean action: 16.279 [10.000, 21.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1033/10000: episode: 30, duration: 1.135s, episode steps:  37, steps per second:  33, episode reward: -41.910, mean reward: -1.133 [-32.202,  2.257], mean action: 12.459 [1.000, 18.000],  loss: 0.015073, mae: 0.475610, mean_q: 0.342564, mean_eps: 0.000000
 1063/10000: episode: 31, duration: 0.319s, episode steps:  30, steps per second:  94, episode reward: 33.000, mean reward:  1.100 [-2.467, 32.450], mean action: 7.967 [1.000, 18.000],  loss: 0.016262, mae: 0.494812, mean_q: 0.097692, mean_eps: 0.000000
 1094/10000: episode: 32, duration: 0.328s, episode steps:  31, steps per second:  95, episode reward: 37.505, mean reward:  1.210 [-2.491, 32.290], mean action: 5.097 [1.000, 15.000],  loss: 0.014685, mae: 0.561639, mean_q: 0.003320, mean_eps: 0.000000
 1118/10000: episode: 33, duration: 0.244s, episode steps:  24, steps per second:  98, episode reward: -38.470, mean reward: -1.603 [-32.061,  2.090], mean action: 9.125 [1.000, 21.000],  loss: 0.011515, mae: 0.553697, mean_q: 0.011850, mean_eps: 0.000000
 1147/10000: episode: 34, duration: 0.309s, episode steps:  29, steps per second:  94, episode reward: -35.070, mean reward: -1.209 [-32.188,  2.292], mean action: 9.310 [1.000, 18.000],  loss: 0.016729, mae: 0.560375, mean_q: 0.033119, mean_eps: 0.000000
 1183/10000: episode: 35, duration: 0.387s, episode steps:  36, steps per second:  93, episode reward: -33.000, mean reward: -0.917 [-32.197,  3.000], mean action: 4.972 [1.000, 18.000],  loss: 0.013971, mae: 0.526848, mean_q: 0.062964, mean_eps: 0.000000
 1215/10000: episode: 36, duration: 0.373s, episode steps:  32, steps per second:  86, episode reward: -38.110, mean reward: -1.191 [-32.252,  2.749], mean action: 15.281 [1.000, 18.000],  loss: 0.014877, mae: 0.520389, mean_q: 0.071183, mean_eps: 0.000000
 1232/10000: episode: 37, duration: 0.212s, episode steps:  17, steps per second:  80, episode reward: -44.310, mean reward: -2.606 [-33.000,  2.520], mean action: 10.706 [1.000, 18.000],  loss: 0.015570, mae: 0.503626, mean_q: 0.103416, mean_eps: 0.000000
 1258/10000: episode: 38, duration: 0.364s, episode steps:  26, steps per second:  71, episode reward: -35.380, mean reward: -1.361 [-32.228,  3.310], mean action: 9.692 [1.000, 18.000],  loss: 0.018732, mae: 0.551526, mean_q: 0.077036, mean_eps: 0.000000
 1289/10000: episode: 39, duration: 0.339s, episode steps:  31, steps per second:  91, episode reward: 32.252, mean reward:  1.040 [-2.232, 32.264], mean action: 7.290 [1.000, 18.000],  loss: 0.016732, mae: 0.519208, mean_q: 0.108921, mean_eps: 0.000000
 1316/10000: episode: 40, duration: 0.307s, episode steps:  27, steps per second:  88, episode reward: -42.000, mean reward: -1.556 [-32.145,  2.233], mean action: 8.148 [1.000, 17.000],  loss: 0.015259, mae: 0.551568, mean_q: 0.031996, mean_eps: 0.000000
 1366/10000: episode: 41, duration: 0.605s, episode steps:  50, steps per second:  83, episode reward: -32.860, mean reward: -0.657 [-32.020,  2.320], mean action: 12.320 [1.000, 21.000],  loss: 0.015437, mae: 0.572613, mean_q: -0.002589, mean_eps: 0.000000
 1386/10000: episode: 42, duration: 0.217s, episode steps:  20, steps per second:  92, episode reward: -38.880, mean reward: -1.944 [-32.125,  2.620], mean action: 12.800 [1.000, 17.000],  loss: 0.020278, mae: 0.589663, mean_q: 0.038176, mean_eps: 0.000000
 1422/10000: episode: 43, duration: 0.402s, episode steps:  36, steps per second:  90, episode reward: -35.100, mean reward: -0.975 [-31.960,  2.291], mean action: 7.889 [1.000, 21.000],  loss: 0.017572, mae: 0.554670, mean_q: 0.077295, mean_eps: 0.000000
 1442/10000: episode: 44, duration: 0.245s, episode steps:  20, steps per second:  82, episode reward: -36.000, mean reward: -1.800 [-32.401,  3.000], mean action: 11.000 [1.000, 18.000],  loss: 0.013010, mae: 0.558726, mean_q: 0.031931, mean_eps: 0.000000
 1472/10000: episode: 45, duration: 0.341s, episode steps:  30, steps per second:  88, episode reward: -33.000, mean reward: -1.100 [-32.663,  3.060], mean action: 11.267 [1.000, 18.000],  loss: 0.015396, mae: 0.545956, mean_q: 0.064085, mean_eps: 0.000000
 1516/10000: episode: 46, duration: 0.484s, episode steps:  44, steps per second:  91, episode reward: 32.516, mean reward:  0.739 [-2.404, 32.676], mean action: 10.932 [1.000, 21.000],  loss: 0.016264, mae: 0.517820, mean_q: 0.108194, mean_eps: 0.000000
 1550/10000: episode: 47, duration: 0.395s, episode steps:  34, steps per second:  86, episode reward: -36.000, mean reward: -1.059 [-32.077,  2.036], mean action: 8.412 [1.000, 18.000],  loss: 0.018569, mae: 0.535846, mean_q: 0.070563, mean_eps: 0.000000
 1581/10000: episode: 48, duration: 0.372s, episode steps:  31, steps per second:  83, episode reward: -35.590, mean reward: -1.148 [-31.978,  3.000], mean action: 11.710 [1.000, 20.000],  loss: 0.016180, mae: 0.506951, mean_q: 0.118206, mean_eps: 0.000000
 1623/10000: episode: 49, duration: 0.465s, episode steps:  42, steps per second:  90, episode reward: -39.000, mean reward: -0.929 [-32.017,  2.910], mean action: 10.190 [1.000, 17.000],  loss: 0.017858, mae: 0.515253, mean_q: 0.118150, mean_eps: 0.000000
 1652/10000: episode: 50, duration: 0.310s, episode steps:  29, steps per second:  94, episode reward: -35.720, mean reward: -1.232 [-32.300,  2.703], mean action: 14.207 [1.000, 16.000],  loss: 0.015421, mae: 0.475689, mean_q: 0.195876, mean_eps: 0.000000
 1683/10000: episode: 51, duration: 0.441s, episode steps:  31, steps per second:  70, episode reward: -33.000, mean reward: -1.065 [-32.450,  3.000], mean action: 6.903 [1.000, 15.000],  loss: 0.019152, mae: 0.529456, mean_q: 0.133753, mean_eps: 0.000000
 1720/10000: episode: 52, duration: 0.411s, episode steps:  37, steps per second:  90, episode reward: -44.840, mean reward: -1.212 [-32.114,  2.170], mean action: 6.649 [1.000, 20.000],  loss: 0.015533, mae: 0.491839, mean_q: 0.164993, mean_eps: 0.000000
 1790/10000: episode: 53, duration: 0.747s, episode steps:  70, steps per second:  94, episode reward: 34.626, mean reward:  0.495 [-2.226, 32.714], mean action: 6.257 [1.000, 18.000],  loss: 0.014602, mae: 0.499172, mean_q: 0.106001, mean_eps: 0.000000
 1822/10000: episode: 54, duration: 0.349s, episode steps:  32, steps per second:  92, episode reward: -47.180, mean reward: -1.474 [-31.794,  0.317], mean action: 10.500 [1.000, 16.000],  loss: 0.015701, mae: 0.518939, mean_q: 0.112617, mean_eps: 0.000000
 1865/10000: episode: 55, duration: 0.455s, episode steps:  43, steps per second:  94, episode reward: -38.750, mean reward: -0.901 [-31.990,  2.938], mean action: 12.767 [1.000, 18.000],  loss: 0.015950, mae: 0.524912, mean_q: 0.070144, mean_eps: 0.000000
 1904/10000: episode: 56, duration: 0.428s, episode steps:  39, steps per second:  91, episode reward: -32.810, mean reward: -0.841 [-32.702,  2.165], mean action: 11.769 [1.000, 21.000],  loss: 0.015277, mae: 0.499833, mean_q: 0.104747, mean_eps: 0.000000
 1946/10000: episode: 57, duration: 0.486s, episode steps:  42, steps per second:  86, episode reward: -32.840, mean reward: -0.782 [-32.020,  3.080], mean action: 9.024 [1.000, 16.000],  loss: 0.017008, mae: 0.577744, mean_q: 0.029716, mean_eps: 0.000000
 1974/10000: episode: 58, duration: 0.302s, episode steps:  28, steps per second:  93, episode reward: -38.290, mean reward: -1.367 [-32.084,  2.402], mean action: 10.643 [1.000, 18.000],  loss: 0.014411, mae: 0.505602, mean_q: 0.063427, mean_eps: 0.000000
 2003/10000: episode: 59, duration: 0.362s, episode steps:  29, steps per second:  80, episode reward: -41.750, mean reward: -1.440 [-32.006,  2.229], mean action: 10.897 [7.000, 15.000],  loss: 0.015428, mae: 0.479878, mean_q: 0.101536, mean_eps: 0.000000
 2024/10000: episode: 60, duration: 0.221s, episode steps:  21, steps per second:  95, episode reward: -42.000, mean reward: -2.000 [-32.382,  2.110], mean action: 9.286 [1.000, 18.000],  loss: 0.019559, mae: 0.517807, mean_q: 0.100811, mean_eps: 0.000000
 2068/10000: episode: 61, duration: 0.474s, episode steps:  44, steps per second:  93, episode reward: -33.000, mean reward: -0.750 [-32.161,  3.000], mean action: 8.364 [1.000, 18.000],  loss: 0.014309, mae: 0.499979, mean_q: 0.098275, mean_eps: 0.000000
 2104/10000: episode: 62, duration: 0.406s, episode steps:  36, steps per second:  89, episode reward: -35.250, mean reward: -0.979 [-32.306,  3.000], mean action: 14.278 [1.000, 20.000],  loss: 0.012738, mae: 0.506502, mean_q: 0.075254, mean_eps: 0.000000
 2133/10000: episode: 63, duration: 0.664s, episode steps:  29, steps per second:  44, episode reward: 32.606, mean reward:  1.124 [-2.599, 32.280], mean action: 12.966 [1.000, 17.000],  loss: 0.015223, mae: 0.553028, mean_q: 0.058590, mean_eps: 0.000000
 2172/10000: episode: 64, duration: 0.466s, episode steps:  39, steps per second:  84, episode reward: -33.000, mean reward: -0.846 [-32.335,  3.000], mean action: 7.385 [1.000, 20.000],  loss: 0.012093, mae: 0.518025, mean_q: 0.063764, mean_eps: 0.000000
 2190/10000: episode: 65, duration: 0.202s, episode steps:  18, steps per second:  89, episode reward: 40.977, mean reward:  2.276 [-2.400, 32.533], mean action: 6.500 [1.000, 14.000],  loss: 0.018325, mae: 0.521109, mean_q: 0.095177, mean_eps: 0.000000
 2238/10000: episode: 66, duration: 0.545s, episode steps:  48, steps per second:  88, episode reward: -35.240, mean reward: -0.734 [-31.759,  2.580], mean action: 10.000 [1.000, 16.000],  loss: 0.016937, mae: 0.527452, mean_q: 0.097576, mean_eps: 0.000000
 2276/10000: episode: 67, duration: 0.443s, episode steps:  38, steps per second:  86, episode reward: -47.140, mean reward: -1.241 [-32.086,  0.216], mean action: 11.474 [1.000, 21.000],  loss: 0.015536, mae: 0.516149, mean_q: 0.083899, mean_eps: 0.000000
 2309/10000: episode: 68, duration: 0.425s, episode steps:  33, steps per second:  78, episode reward: -41.610, mean reward: -1.261 [-32.012,  2.263], mean action: 12.455 [1.000, 18.000],  loss: 0.017342, mae: 0.506827, mean_q: 0.142090, mean_eps: 0.000000
 2334/10000: episode: 69, duration: 0.303s, episode steps:  25, steps per second:  82, episode reward: -47.620, mean reward: -1.905 [-32.710,  0.445], mean action: 13.360 [5.000, 17.000],  loss: 0.018576, mae: 0.562768, mean_q: 0.084823, mean_eps: 0.000000
 2373/10000: episode: 70, duration: 0.396s, episode steps:  39, steps per second:  99, episode reward: -39.000, mean reward: -1.000 [-32.497,  2.593], mean action: 11.026 [1.000, 18.000],  loss: 0.015938, mae: 0.546076, mean_q: 0.078904, mean_eps: 0.000000
 2399/10000: episode: 71, duration: 0.870s, episode steps:  26, steps per second:  30, episode reward: -36.000, mean reward: -1.385 [-32.173,  2.903], mean action: 11.000 [1.000, 18.000],  loss: 0.020067, mae: 0.602810, mean_q: 0.063454, mean_eps: 0.000000
 2455/10000: episode: 72, duration: 0.697s, episode steps:  56, steps per second:  80, episode reward: -42.000, mean reward: -0.750 [-32.084,  2.370], mean action: 9.857 [1.000, 21.000],  loss: 0.016093, mae: 0.549884, mean_q: 0.072911, mean_eps: 0.000000
 2492/10000: episode: 73, duration: 0.396s, episode steps:  37, steps per second:  93, episode reward: 32.316, mean reward:  0.873 [-2.195, 32.350], mean action: 8.919 [1.000, 18.000],  loss: 0.015527, mae: 0.558662, mean_q: 0.039988, mean_eps: 0.000000
 2529/10000: episode: 74, duration: 0.398s, episode steps:  37, steps per second:  93, episode reward: -38.940, mean reward: -1.052 [-32.071,  2.230], mean action: 9.108 [1.000, 18.000],  loss: 0.017310, mae: 0.530400, mean_q: 0.088554, mean_eps: 0.000000
 2555/10000: episode: 75, duration: 0.320s, episode steps:  26, steps per second:  81, episode reward: -32.330, mean reward: -1.243 [-32.051,  2.490], mean action: 9.615 [1.000, 16.000],  loss: 0.018654, mae: 0.578783, mean_q: 0.041562, mean_eps: 0.000000
 2589/10000: episode: 76, duration: 0.362s, episode steps:  34, steps per second:  94, episode reward: -33.000, mean reward: -0.971 [-32.233,  2.682], mean action: 10.118 [1.000, 16.000],  loss: 0.012467, mae: 0.507967, mean_q: 0.098581, mean_eps: 0.000000
 2617/10000: episode: 77, duration: 0.508s, episode steps:  28, steps per second:  55, episode reward: -35.810, mean reward: -1.279 [-32.005,  2.318], mean action: 10.679 [1.000, 17.000],  loss: 0.017869, mae: 0.586696, mean_q: 0.014906, mean_eps: 0.000000
 2658/10000: episode: 78, duration: 0.493s, episode steps:  41, steps per second:  83, episode reward: 32.443, mean reward:  0.791 [-2.230, 32.650], mean action: 7.634 [1.000, 17.000],  loss: 0.014887, mae: 0.555682, mean_q: 0.044667, mean_eps: 0.000000
 2690/10000: episode: 79, duration: 0.391s, episode steps:  32, steps per second:  82, episode reward: -38.820, mean reward: -1.213 [-32.037,  2.440], mean action: 7.844 [1.000, 18.000],  loss: 0.015563, mae: 0.579304, mean_q: 0.019752, mean_eps: 0.000000
 2728/10000: episode: 80, duration: 0.428s, episode steps:  38, steps per second:  89, episode reward: -32.270, mean reward: -0.849 [-32.143,  2.784], mean action: 8.947 [1.000, 17.000],  loss: 0.015737, mae: 0.584185, mean_q: 0.027055, mean_eps: 0.000000
 2772/10000: episode: 81, duration: 0.472s, episode steps:  44, steps per second:  93, episode reward: -32.940, mean reward: -0.749 [-31.967,  3.000], mean action: 8.568 [1.000, 16.000],  loss: 0.016371, mae: 0.631473, mean_q: -0.009315, mean_eps: 0.000000
 2804/10000: episode: 82, duration: 0.354s, episode steps:  32, steps per second:  90, episode reward: 41.528, mean reward:  1.298 [-2.523, 32.139], mean action: 9.000 [1.000, 17.000],  loss: 0.011792, mae: 0.567819, mean_q: 0.060947, mean_eps: 0.000000
 2850/10000: episode: 83, duration: 0.493s, episode steps:  46, steps per second:  93, episode reward: 37.487, mean reward:  0.815 [-2.490, 32.120], mean action: 10.000 [1.000, 18.000],  loss: 0.015774, mae: 0.606742, mean_q: 0.005610, mean_eps: 0.000000
 2924/10000: episode: 84, duration: 0.916s, episode steps:  74, steps per second:  81, episode reward: -38.540, mean reward: -0.521 [-32.060,  2.500], mean action: 12.892 [1.000, 21.000],  loss: 0.017354, mae: 0.598403, mean_q: 0.043931, mean_eps: 0.000000
 2945/10000: episode: 85, duration: 0.237s, episode steps:  21, steps per second:  89, episode reward: 37.758, mean reward:  1.798 [-2.444, 31.848], mean action: 7.286 [1.000, 16.000],  loss: 0.013575, mae: 0.587401, mean_q: 0.058098, mean_eps: 0.000000
 2979/10000: episode: 86, duration: 0.450s, episode steps:  34, steps per second:  76, episode reward: -38.430, mean reward: -1.130 [-32.001,  2.302], mean action: 12.559 [1.000, 21.000],  loss: 0.014561, mae: 0.584790, mean_q: 0.043960, mean_eps: 0.000000
 3000/10000: episode: 87, duration: 0.243s, episode steps:  21, steps per second:  86, episode reward: -39.000, mean reward: -1.857 [-33.000,  3.000], mean action: 12.952 [1.000, 18.000],  loss: 0.016431, mae: 0.589506, mean_q: 0.035814, mean_eps: 0.000000
 3033/10000: episode: 88, duration: 0.427s, episode steps:  33, steps per second:  77, episode reward: -32.220, mean reward: -0.976 [-31.237,  2.330], mean action: 10.788 [1.000, 18.000],  loss: 0.015851, mae: 0.570377, mean_q: 0.066013, mean_eps: 0.000000
 3066/10000: episode: 89, duration: 0.353s, episode steps:  33, steps per second:  93, episode reward: -38.810, mean reward: -1.176 [-32.217,  2.320], mean action: 13.455 [1.000, 18.000],  loss: 0.017030, mae: 0.575403, mean_q: 0.032800, mean_eps: 0.000000
 3100/10000: episode: 90, duration: 0.427s, episode steps:  34, steps per second:  80, episode reward: -36.000, mean reward: -1.059 [-32.697,  2.540], mean action: 11.059 [1.000, 21.000],  loss: 0.016519, mae: 0.609139, mean_q: 0.018556, mean_eps: 0.000000
 3147/10000: episode: 91, duration: 0.579s, episode steps:  47, steps per second:  81, episode reward: 32.579, mean reward:  0.693 [-2.330, 32.270], mean action: 10.404 [1.000, 16.000],  loss: 0.013916, mae: 0.551344, mean_q: 0.066828, mean_eps: 0.000000
 3187/10000: episode: 92, duration: 0.421s, episode steps:  40, steps per second:  95, episode reward: -35.130, mean reward: -0.878 [-31.639,  2.904], mean action: 14.475 [1.000, 21.000],  loss: 0.015218, mae: 0.581665, mean_q: 0.022036, mean_eps: 0.000000
 3210/10000: episode: 93, duration: 0.247s, episode steps:  23, steps per second:  93, episode reward: 32.284, mean reward:  1.404 [-2.559, 32.310], mean action: 7.913 [1.000, 15.000],  loss: 0.014591, mae: 0.572526, mean_q: 0.026512, mean_eps: 0.000000
 3242/10000: episode: 94, duration: 0.382s, episode steps:  32, steps per second:  84, episode reward: 40.075, mean reward:  1.252 [-2.301, 33.000], mean action: 6.844 [1.000, 17.000],  loss: 0.014629, mae: 0.558361, mean_q: 0.072703, mean_eps: 0.000000
 3264/10000: episode: 95, duration: 0.239s, episode steps:  22, steps per second:  92, episode reward: 32.335, mean reward:  1.470 [-3.000, 32.887], mean action: 7.955 [1.000, 16.000],  loss: 0.014902, mae: 0.533393, mean_q: 0.109538, mean_eps: 0.000000
 3296/10000: episode: 96, duration: 0.402s, episode steps:  32, steps per second:  80, episode reward: -32.610, mean reward: -1.019 [-32.860,  2.990], mean action: 8.375 [1.000, 18.000],  loss: 0.015528, mae: 0.572166, mean_q: 0.065261, mean_eps: 0.000000
 3326/10000: episode: 97, duration: 0.365s, episode steps:  30, steps per second:  82, episode reward: -35.910, mean reward: -1.197 [-32.561,  2.923], mean action: 12.333 [1.000, 18.000],  loss: 0.016032, mae: 0.526289, mean_q: 0.098111, mean_eps: 0.000000
 3346/10000: episode: 98, duration: 0.307s, episode steps:  20, steps per second:  65, episode reward: -42.000, mean reward: -2.100 [-32.415,  2.586], mean action: 8.200 [1.000, 16.000],  loss: 0.016335, mae: 0.564160, mean_q: 0.081118, mean_eps: 0.000000
 3367/10000: episode: 99, duration: 0.310s, episode steps:  21, steps per second:  68, episode reward: -41.670, mean reward: -1.984 [-32.028,  2.110], mean action: 10.810 [1.000, 17.000],  loss: 0.013787, mae: 0.580818, mean_q: 0.027440, mean_eps: 0.000000
 3390/10000: episode: 100, duration: 0.264s, episode steps:  23, steps per second:  87, episode reward: -41.390, mean reward: -1.800 [-32.063,  1.953], mean action: 9.696 [1.000, 21.000],  loss: 0.017724, mae: 0.629683, mean_q: 0.003220, mean_eps: 0.000000
 3414/10000: episode: 101, duration: 0.248s, episode steps:  24, steps per second:  97, episode reward: -41.730, mean reward: -1.739 [-32.271,  2.620], mean action: 10.583 [1.000, 17.000],  loss: 0.015048, mae: 0.606961, mean_q: 0.011316, mean_eps: 0.000000
 3468/10000: episode: 102, duration: 0.580s, episode steps:  54, steps per second:  93, episode reward: -38.540, mean reward: -0.714 [-32.083,  2.320], mean action: 10.759 [1.000, 18.000],  loss: 0.017086, mae: 0.586802, mean_q: 0.037163, mean_eps: 0.000000
 3491/10000: episode: 103, duration: 0.268s, episode steps:  23, steps per second:  86, episode reward: 43.004, mean reward:  1.870 [-2.028, 32.320], mean action: 5.304 [1.000, 17.000],  loss: 0.018082, mae: 0.616041, mean_q: 0.042311, mean_eps: 0.000000
 3532/10000: episode: 104, duration: 0.492s, episode steps:  41, steps per second:  83, episode reward: -33.000, mean reward: -0.805 [-30.000,  2.411], mean action: 5.049 [1.000, 18.000],  loss: 0.016962, mae: 0.634314, mean_q: 0.008998, mean_eps: 0.000000
 3564/10000: episode: 105, duration: 0.542s, episode steps:  32, steps per second:  59, episode reward: -33.000, mean reward: -1.031 [-29.559,  2.850], mean action: 7.188 [1.000, 18.000],  loss: 0.015197, mae: 0.592931, mean_q: 0.012560, mean_eps: 0.000000
 3601/10000: episode: 106, duration: 0.454s, episode steps:  37, steps per second:  82, episode reward: -33.000, mean reward: -0.892 [-32.004,  2.390], mean action: 12.649 [1.000, 18.000],  loss: 0.015217, mae: 0.613519, mean_q: -0.001253, mean_eps: 0.000000
 3626/10000: episode: 107, duration: 0.303s, episode steps:  25, steps per second:  83, episode reward: 32.903, mean reward:  1.316 [-2.682, 32.713], mean action: 7.440 [1.000, 18.000],  loss: 0.017894, mae: 0.571802, mean_q: 0.061343, mean_eps: 0.000000
 3649/10000: episode: 108, duration: 0.236s, episode steps:  23, steps per second:  97, episode reward: -44.350, mean reward: -1.928 [-32.154,  2.010], mean action: 13.826 [1.000, 18.000],  loss: 0.018846, mae: 0.633627, mean_q: 0.013915, mean_eps: 0.000000
 3677/10000: episode: 109, duration: 0.318s, episode steps:  28, steps per second:  88, episode reward: -38.620, mean reward: -1.379 [-31.634,  2.308], mean action: 12.000 [1.000, 17.000],  loss: 0.014085, mae: 0.561429, mean_q: 0.089779, mean_eps: 0.000000
 3695/10000: episode: 110, duration: 0.215s, episode steps:  18, steps per second:  84, episode reward: 40.311, mean reward:  2.239 [-3.000, 32.904], mean action: 8.556 [1.000, 16.000],  loss: 0.016297, mae: 0.562421, mean_q: 0.092020, mean_eps: 0.000000
 3721/10000: episode: 111, duration: 0.290s, episode steps:  26, steps per second:  90, episode reward: -35.720, mean reward: -1.374 [-31.793,  2.903], mean action: 12.115 [1.000, 16.000],  loss: 0.015745, mae: 0.601663, mean_q: 0.021371, mean_eps: 0.000000
 3747/10000: episode: 112, duration: 0.317s, episode steps:  26, steps per second:  82, episode reward: -38.220, mean reward: -1.470 [-32.110,  3.061], mean action: 12.769 [1.000, 18.000],  loss: 0.018960, mae: 0.641016, mean_q: -0.021859, mean_eps: 0.000000
 3771/10000: episode: 113, duration: 0.284s, episode steps:  24, steps per second:  84, episode reward: -35.760, mean reward: -1.490 [-31.853,  2.483], mean action: 10.042 [1.000, 18.000],  loss: 0.016120, mae: 0.596185, mean_q: 0.037149, mean_eps: 0.000000
 3801/10000: episode: 114, duration: 0.459s, episode steps:  30, steps per second:  65, episode reward: -32.660, mean reward: -1.089 [-32.046,  2.902], mean action: 8.733 [1.000, 15.000],  loss: 0.012803, mae: 0.632402, mean_q: -0.038700, mean_eps: 0.000000
 3830/10000: episode: 115, duration: 0.339s, episode steps:  29, steps per second:  85, episode reward: -41.470, mean reward: -1.430 [-32.199,  2.223], mean action: 10.345 [1.000, 18.000],  loss: 0.014315, mae: 0.565114, mean_q: 0.030406, mean_eps: 0.000000
 3870/10000: episode: 116, duration: 0.406s, episode steps:  40, steps per second:  98, episode reward: -38.530, mean reward: -0.963 [-31.634,  2.560], mean action: 7.600 [1.000, 21.000],  loss: 0.015832, mae: 0.616597, mean_q: -0.005174, mean_eps: 0.000000
 3901/10000: episode: 117, duration: 0.411s, episode steps:  31, steps per second:  75, episode reward: 38.066, mean reward:  1.228 [-2.166, 31.963], mean action: 5.645 [1.000, 16.000],  loss: 0.015850, mae: 0.649162, mean_q: -0.033632, mean_eps: 0.000000
 3926/10000: episode: 118, duration: 0.285s, episode steps:  25, steps per second:  88, episode reward: -39.000, mean reward: -1.560 [-32.449,  2.810], mean action: 10.280 [1.000, 18.000],  loss: 0.012545, mae: 0.539476, mean_q: 0.057237, mean_eps: 0.000000
 3948/10000: episode: 119, duration: 0.317s, episode steps:  22, steps per second:  69, episode reward: 30.000, mean reward:  1.364 [-3.000, 30.216], mean action: 12.500 [7.000, 20.000],  loss: 0.019082, mae: 0.608392, mean_q: 0.001698, mean_eps: 0.000000
 3960/10000: episode: 120, duration: 0.214s, episode steps:  12, steps per second:  56, episode reward: 43.808, mean reward:  3.651 [-2.648, 32.203], mean action: 6.167 [1.000, 16.000],  loss: 0.012422, mae: 0.630061, mean_q: -0.048568, mean_eps: 0.000000
 3985/10000: episode: 121, duration: 0.264s, episode steps:  25, steps per second:  95, episode reward: -32.380, mean reward: -1.295 [-32.377,  2.531], mean action: 9.720 [1.000, 16.000],  loss: 0.017009, mae: 0.575244, mean_q: 0.074154, mean_eps: 0.000000
 4014/10000: episode: 122, duration: 0.332s, episode steps:  29, steps per second:  87, episode reward: 34.902, mean reward:  1.204 [-2.883, 32.680], mean action: 8.379 [1.000, 16.000],  loss: 0.016768, mae: 0.602305, mean_q: 0.033676, mean_eps: 0.000000
 4036/10000: episode: 123, duration: 0.227s, episode steps:  22, steps per second:  97, episode reward: -44.600, mean reward: -2.027 [-32.135,  1.923], mean action: 10.864 [1.000, 18.000],  loss: 0.016067, mae: 0.591870, mean_q: 0.027315, mean_eps: 0.000000
 4064/10000: episode: 124, duration: 0.316s, episode steps:  28, steps per second:  89, episode reward: -38.920, mean reward: -1.390 [-32.243,  2.030], mean action: 9.714 [1.000, 20.000],  loss: 0.014135, mae: 0.592791, mean_q: -0.007295, mean_eps: 0.000000
 4108/10000: episode: 125, duration: 0.462s, episode steps:  44, steps per second:  95, episode reward: -35.230, mean reward: -0.801 [-32.154,  2.210], mean action: 11.795 [1.000, 20.000],  loss: 0.016017, mae: 0.598982, mean_q: 0.023224, mean_eps: 0.000000
 4172/10000: episode: 126, duration: 0.636s, episode steps:  64, steps per second: 101, episode reward: 35.363, mean reward:  0.553 [-3.000, 31.941], mean action: 6.141 [1.000, 17.000],  loss: 0.016210, mae: 0.596555, mean_q: 0.034138, mean_eps: 0.000000
 4200/10000: episode: 127, duration: 0.564s, episode steps:  28, steps per second:  50, episode reward: -42.000, mean reward: -1.500 [-32.909,  1.939], mean action: 12.929 [1.000, 21.000],  loss: 0.016020, mae: 0.564506, mean_q: 0.055350, mean_eps: 0.000000
 4245/10000: episode: 128, duration: 0.488s, episode steps:  45, steps per second:  92, episode reward: 38.207, mean reward:  0.849 [-3.000, 31.988], mean action: 5.400 [1.000, 16.000],  loss: 0.018197, mae: 0.605789, mean_q: 0.029782, mean_eps: 0.000000
 4293/10000: episode: 129, duration: 0.519s, episode steps:  48, steps per second:  93, episode reward: -32.140, mean reward: -0.670 [-32.205,  3.000], mean action: 7.917 [1.000, 18.000],  loss: 0.016484, mae: 0.593076, mean_q: 0.029057, mean_eps: 0.000000
 4326/10000: episode: 130, duration: 0.380s, episode steps:  33, steps per second:  87, episode reward: -32.590, mean reward: -0.988 [-31.985,  2.715], mean action: 8.424 [1.000, 20.000],  loss: 0.018018, mae: 0.591310, mean_q: 0.053853, mean_eps: 0.000000
 4344/10000: episode: 131, duration: 0.199s, episode steps:  18, steps per second:  91, episode reward: -44.500, mean reward: -2.472 [-32.189,  2.110], mean action: 13.722 [1.000, 21.000],  loss: 0.018885, mae: 0.588618, mean_q: 0.058881, mean_eps: 0.000000
 4372/10000: episode: 132, duration: 0.337s, episode steps:  28, steps per second:  83, episode reward: -41.360, mean reward: -1.477 [-31.463,  2.150], mean action: 11.000 [1.000, 20.000],  loss: 0.015192, mae: 0.580494, mean_q: 0.053135, mean_eps: 0.000000
 4403/10000: episode: 133, duration: 0.375s, episode steps:  31, steps per second:  83, episode reward: 34.342, mean reward:  1.108 [-2.294, 32.576], mean action: 10.935 [1.000, 20.000],  loss: 0.016185, mae: 0.618525, mean_q: 0.036406, mean_eps: 0.000000
 4429/10000: episode: 134, duration: 0.316s, episode steps:  26, steps per second:  82, episode reward: 37.794, mean reward:  1.454 [-2.516, 33.000], mean action: 5.885 [1.000, 16.000],  loss: 0.018368, mae: 0.643116, mean_q: 0.031367, mean_eps: 0.000000
 4451/10000: episode: 135, duration: 0.257s, episode steps:  22, steps per second:  85, episode reward: 41.793, mean reward:  1.900 [-2.303, 31.913], mean action: 1.000 [1.000, 1.000],  loss: 0.022525, mae: 0.677572, mean_q: 0.017759, mean_eps: 0.000000
 4486/10000: episode: 136, duration: 0.372s, episode steps:  35, steps per second:  94, episode reward: 32.941, mean reward:  0.941 [-3.000, 32.250], mean action: 6.914 [1.000, 20.000],  loss: 0.013727, mae: 0.595641, mean_q: 0.038347, mean_eps: 0.000000
 4514/10000: episode: 137, duration: 0.289s, episode steps:  28, steps per second:  97, episode reward: -32.800, mean reward: -1.171 [-31.929,  2.711], mean action: 10.571 [1.000, 20.000],  loss: 0.019660, mae: 0.579145, mean_q: 0.084754, mean_eps: 0.000000
 4538/10000: episode: 138, duration: 0.266s, episode steps:  24, steps per second:  90, episode reward: -33.000, mean reward: -1.375 [-32.089,  2.736], mean action: 11.625 [1.000, 17.000],  loss: 0.016651, mae: 0.573179, mean_q: 0.138706, mean_eps: 0.000000
 4558/10000: episode: 139, duration: 0.214s, episode steps:  20, steps per second:  93, episode reward: -38.410, mean reward: -1.920 [-32.302,  3.000], mean action: 10.000 [1.000, 20.000],  loss: 0.016883, mae: 0.608530, mean_q: 0.114799, mean_eps: 0.000000
 4597/10000: episode: 140, duration: 0.435s, episode steps:  39, steps per second:  90, episode reward: -41.270, mean reward: -1.058 [-32.152,  2.201], mean action: 7.974 [1.000, 18.000],  loss: 0.018535, mae: 0.603296, mean_q: 0.095444, mean_eps: 0.000000
 4630/10000: episode: 141, duration: 0.371s, episode steps:  33, steps per second:  89, episode reward: 32.562, mean reward:  0.987 [-2.835, 32.020], mean action: 10.788 [1.000, 16.000],  loss: 0.015761, mae: 0.582865, mean_q: 0.063270, mean_eps: 0.000000
 4670/10000: episode: 142, duration: 0.419s, episode steps:  40, steps per second:  96, episode reward: -41.700, mean reward: -1.042 [-32.126,  2.021], mean action: 13.450 [1.000, 21.000],  loss: 0.017161, mae: 0.587392, mean_q: 0.061585, mean_eps: 0.000000
 4702/10000: episode: 143, duration: 0.336s, episode steps:  32, steps per second:  95, episode reward: -35.590, mean reward: -1.112 [-32.617,  2.452], mean action: 11.250 [1.000, 17.000],  loss: 0.016914, mae: 0.580751, mean_q: 0.082866, mean_eps: 0.000000
 4730/10000: episode: 144, duration: 0.317s, episode steps:  28, steps per second:  88, episode reward: 34.610, mean reward:  1.236 [-2.964, 33.000], mean action: 12.321 [1.000, 20.000],  loss: 0.017780, mae: 0.624056, mean_q: 0.032881, mean_eps: 0.000000
 4753/10000: episode: 145, duration: 0.259s, episode steps:  23, steps per second:  89, episode reward: 38.763, mean reward:  1.685 [-3.000, 32.766], mean action: 6.304 [1.000, 20.000],  loss: 0.016405, mae: 0.615080, mean_q: 0.017470, mean_eps: 0.000000
 4775/10000: episode: 146, duration: 0.263s, episode steps:  22, steps per second:  84, episode reward: 37.054, mean reward:  1.684 [-3.000, 32.083], mean action: 6.909 [1.000, 15.000],  loss: 0.016203, mae: 0.641381, mean_q: -0.009902, mean_eps: 0.000000
 4811/10000: episode: 147, duration: 0.466s, episode steps:  36, steps per second:  77, episode reward: -35.620, mean reward: -0.989 [-31.708,  2.451], mean action: 12.694 [1.000, 17.000],  loss: 0.016030, mae: 0.614351, mean_q: 0.022487, mean_eps: 0.000000
 4871/10000: episode: 148, duration: 0.649s, episode steps:  60, steps per second:  92, episode reward: 32.898, mean reward:  0.548 [-2.378, 32.038], mean action: 8.850 [1.000, 16.000],  loss: 0.016630, mae: 0.575414, mean_q: 0.090395, mean_eps: 0.000000
 4891/10000: episode: 149, duration: 0.206s, episode steps:  20, steps per second:  97, episode reward: -44.820, mean reward: -2.241 [-32.451,  1.631], mean action: 10.100 [1.000, 18.000],  loss: 0.016037, mae: 0.592867, mean_q: 0.044147, mean_eps: 0.000000
 4921/10000: episode: 150, duration: 0.333s, episode steps:  30, steps per second:  90, episode reward: 32.349, mean reward:  1.078 [-3.000, 32.380], mean action: 9.933 [1.000, 21.000],  loss: 0.020647, mae: 0.614837, mean_q: 0.040870, mean_eps: 0.000000
 4947/10000: episode: 151, duration: 0.288s, episode steps:  26, steps per second:  90, episode reward: -35.540, mean reward: -1.367 [-32.120,  2.420], mean action: 13.231 [1.000, 18.000],  loss: 0.015447, mae: 0.553071, mean_q: 0.084489, mean_eps: 0.000000
 4979/10000: episode: 152, duration: 0.329s, episode steps:  32, steps per second:  97, episode reward: 32.826, mean reward:  1.026 [-2.304, 32.490], mean action: 5.812 [1.000, 16.000],  loss: 0.015105, mae: 0.584556, mean_q: 0.067000, mean_eps: 0.000000
 5008/10000: episode: 153, duration: 0.322s, episode steps:  29, steps per second:  90, episode reward: 35.746, mean reward:  1.233 [-2.327, 32.060], mean action: 6.138 [1.000, 16.000],  loss: 0.016134, mae: 0.584090, mean_q: 0.071918, mean_eps: 0.000000
 5038/10000: episode: 154, duration: 0.331s, episode steps:  30, steps per second:  91, episode reward: -32.070, mean reward: -1.069 [-32.061,  3.000], mean action: 9.600 [1.000, 16.000],  loss: 0.017217, mae: 0.575672, mean_q: 0.089271, mean_eps: 0.000000
 5059/10000: episode: 155, duration: 0.217s, episode steps:  21, steps per second:  97, episode reward: -38.390, mean reward: -1.828 [-33.000,  3.000], mean action: 7.286 [1.000, 16.000],  loss: 0.013070, mae: 0.530836, mean_q: 0.097310, mean_eps: 0.000000
 5081/10000: episode: 156, duration: 0.229s, episode steps:  22, steps per second:  96, episode reward: -35.190, mean reward: -1.600 [-32.279,  2.390], mean action: 6.727 [1.000, 16.000],  loss: 0.015002, mae: 0.515433, mean_q: 0.124204, mean_eps: 0.000000
 5148/10000: episode: 157, duration: 0.698s, episode steps:  67, steps per second:  96, episode reward: 40.894, mean reward:  0.610 [-2.298, 32.530], mean action: 2.985 [1.000, 12.000],  loss: 0.014329, mae: 0.611397, mean_q: 0.012734, mean_eps: 0.000000
 5185/10000: episode: 158, duration: 0.410s, episode steps:  37, steps per second:  90, episode reward: 32.626, mean reward:  0.882 [-2.446, 32.510], mean action: 12.622 [1.000, 17.000],  loss: 0.015698, mae: 0.587711, mean_q: 0.061845, mean_eps: 0.000000
 5234/10000: episode: 159, duration: 0.884s, episode steps:  49, steps per second:  55, episode reward: 32.908, mean reward:  0.672 [-2.459, 32.190], mean action: 9.184 [1.000, 15.000],  loss: 0.015620, mae: 0.618184, mean_q: 0.020722, mean_eps: 0.000000
 5272/10000: episode: 160, duration: 0.502s, episode steps:  38, steps per second:  76, episode reward: 35.546, mean reward:  0.935 [-2.913, 32.070], mean action: 9.421 [1.000, 16.000],  loss: 0.018528, mae: 0.610683, mean_q: 0.070446, mean_eps: 0.000000
 5326/10000: episode: 161, duration: 0.687s, episode steps:  54, steps per second:  79, episode reward: -32.170, mean reward: -0.596 [-31.968,  3.000], mean action: 11.407 [1.000, 17.000],  loss: 0.017128, mae: 0.607779, mean_q: 0.061223, mean_eps: 0.000000
 5355/10000: episode: 162, duration: 0.302s, episode steps:  29, steps per second:  96, episode reward: -39.000, mean reward: -1.345 [-32.174,  3.051], mean action: 10.931 [1.000, 17.000],  loss: 0.017276, mae: 0.577328, mean_q: 0.100235, mean_eps: 0.000000
 5376/10000: episode: 163, duration: 0.247s, episode steps:  21, steps per second:  85, episode reward: 41.078, mean reward:  1.956 [-2.430, 32.040], mean action: 3.048 [1.000, 17.000],  loss: 0.013741, mae: 0.588427, mean_q: 0.079407, mean_eps: 0.000000
 5403/10000: episode: 164, duration: 0.280s, episode steps:  27, steps per second:  96, episode reward: -33.000, mean reward: -1.222 [-32.204,  2.295], mean action: 6.889 [1.000, 15.000],  loss: 0.019088, mae: 0.580316, mean_q: 0.105284, mean_eps: 0.000000
 5428/10000: episode: 165, duration: 0.289s, episode steps:  25, steps per second:  86, episode reward: -36.000, mean reward: -1.440 [-32.185,  2.920], mean action: 7.280 [1.000, 17.000],  loss: 0.016532, mae: 0.530060, mean_q: 0.146088, mean_eps: 0.000000
 5484/10000: episode: 166, duration: 0.566s, episode steps:  56, steps per second:  99, episode reward: -35.760, mean reward: -0.639 [-32.260,  2.900], mean action: 12.304 [1.000, 15.000],  loss: 0.016755, mae: 0.598707, mean_q: 0.042953, mean_eps: 0.000000
 5516/10000: episode: 167, duration: 0.358s, episode steps:  32, steps per second:  89, episode reward: -34.610, mean reward: -1.082 [-32.168,  2.830], mean action: 9.875 [1.000, 17.000],  loss: 0.016974, mae: 0.571714, mean_q: 0.117105, mean_eps: 0.000000
 5547/10000: episode: 168, duration: 0.331s, episode steps:  31, steps per second:  94, episode reward: -33.000, mean reward: -1.065 [-32.322,  3.000], mean action: 10.871 [1.000, 18.000],  loss: 0.017129, mae: 0.598602, mean_q: 0.086654, mean_eps: 0.000000
 5573/10000: episode: 169, duration: 0.301s, episode steps:  26, steps per second:  86, episode reward: -35.580, mean reward: -1.368 [-31.750,  3.000], mean action: 9.538 [1.000, 16.000],  loss: 0.015283, mae: 0.588220, mean_q: 0.059517, mean_eps: 0.000000
 5599/10000: episode: 170, duration: 0.296s, episode steps:  26, steps per second:  88, episode reward: -39.000, mean reward: -1.500 [-32.890,  2.390], mean action: 12.731 [1.000, 18.000],  loss: 0.015015, mae: 0.624342, mean_q: 0.006061, mean_eps: 0.000000
 5631/10000: episode: 171, duration: 0.383s, episode steps:  32, steps per second:  84, episode reward: 32.171, mean reward:  1.005 [-2.615, 32.812], mean action: 11.219 [1.000, 15.000],  loss: 0.015642, mae: 0.616996, mean_q: 0.045775, mean_eps: 0.000000
 5669/10000: episode: 172, duration: 0.403s, episode steps:  38, steps per second:  94, episode reward: -41.530, mean reward: -1.093 [-32.520,  2.240], mean action: 11.737 [1.000, 16.000],  loss: 0.018086, mae: 0.601750, mean_q: 0.109422, mean_eps: 0.000000
 5703/10000: episode: 173, duration: 0.440s, episode steps:  34, steps per second:  77, episode reward: 34.896, mean reward:  1.026 [-2.466, 30.380], mean action: 4.588 [1.000, 15.000],  loss: 0.018235, mae: 0.622656, mean_q: 0.051597, mean_eps: 0.000000
 5737/10000: episode: 174, duration: 0.379s, episode steps:  34, steps per second:  90, episode reward: -32.680, mean reward: -0.961 [-32.375,  2.700], mean action: 7.882 [1.000, 20.000],  loss: 0.016788, mae: 0.571375, mean_q: 0.118810, mean_eps: 0.000000
 5770/10000: episode: 175, duration: 0.346s, episode steps:  33, steps per second:  95, episode reward: -32.680, mean reward: -0.990 [-32.236,  3.000], mean action: 7.273 [1.000, 20.000],  loss: 0.016058, mae: 0.584432, mean_q: 0.087459, mean_eps: 0.000000
 5805/10000: episode: 176, duration: 0.372s, episode steps:  35, steps per second:  94, episode reward: -32.900, mean reward: -0.940 [-32.695,  3.000], mean action: 7.829 [1.000, 20.000],  loss: 0.015935, mae: 0.594519, mean_q: 0.065579, mean_eps: 0.000000
 5844/10000: episode: 177, duration: 0.416s, episode steps:  39, steps per second:  94, episode reward: -36.000, mean reward: -0.923 [-32.004,  2.774], mean action: 13.385 [1.000, 21.000],  loss: 0.016925, mae: 0.590816, mean_q: 0.081680, mean_eps: 0.000000
 5863/10000: episode: 178, duration: 0.211s, episode steps:  19, steps per second:  90, episode reward: -41.300, mean reward: -2.174 [-32.171,  2.410], mean action: 8.000 [1.000, 16.000],  loss: 0.020552, mae: 0.617870, mean_q: 0.053680, mean_eps: 0.000000
 5890/10000: episode: 179, duration: 0.308s, episode steps:  27, steps per second:  88, episode reward: 34.816, mean reward:  1.289 [-3.000, 32.780], mean action: 7.333 [1.000, 15.000],  loss: 0.017011, mae: 0.547223, mean_q: 0.149414, mean_eps: 0.000000
 5923/10000: episode: 180, duration: 0.363s, episode steps:  33, steps per second:  91, episode reward: -41.430, mean reward: -1.255 [-31.609,  2.140], mean action: 13.394 [1.000, 16.000],  loss: 0.019875, mae: 0.611562, mean_q: 0.071548, mean_eps: 0.000000
 5984/10000: episode: 181, duration: 0.648s, episode steps:  61, steps per second:  94, episode reward: 32.025, mean reward:  0.525 [-2.406, 32.513], mean action: 7.230 [1.000, 15.000],  loss: 0.018649, mae: 0.608422, mean_q: 0.073245, mean_eps: 0.000000
 6016/10000: episode: 182, duration: 0.326s, episode steps:  32, steps per second:  98, episode reward: -36.000, mean reward: -1.125 [-32.415,  2.510], mean action: 7.781 [1.000, 18.000],  loss: 0.016718, mae: 0.611669, mean_q: 0.049127, mean_eps: 0.000000
 6039/10000: episode: 183, duration: 0.248s, episode steps:  23, steps per second:  93, episode reward: 36.000, mean reward:  1.565 [-3.000, 32.430], mean action: 6.609 [1.000, 16.000],  loss: 0.017038, mae: 0.651883, mean_q: 0.040987, mean_eps: 0.000000
 6080/10000: episode: 184, duration: 0.464s, episode steps:  41, steps per second:  88, episode reward: -35.760, mean reward: -0.872 [-31.886,  2.220], mean action: 8.293 [1.000, 17.000],  loss: 0.015667, mae: 0.564100, mean_q: 0.102198, mean_eps: 0.000000
 6132/10000: episode: 185, duration: 0.619s, episode steps:  52, steps per second:  84, episode reward: -38.530, mean reward: -0.741 [-32.089,  2.590], mean action: 5.288 [1.000, 18.000],  loss: 0.018678, mae: 0.600843, mean_q: 0.083686, mean_eps: 0.000000
 6167/10000: episode: 186, duration: 0.375s, episode steps:  35, steps per second:  93, episode reward: -35.290, mean reward: -1.008 [-32.410,  2.690], mean action: 6.543 [1.000, 18.000],  loss: 0.014906, mae: 0.583239, mean_q: 0.077459, mean_eps: 0.000000
 6189/10000: episode: 187, duration: 0.251s, episode steps:  22, steps per second:  88, episode reward: 43.552, mean reward:  1.980 [-2.134, 32.053], mean action: 4.318 [1.000, 15.000],  loss: 0.015717, mae: 0.597597, mean_q: 0.086575, mean_eps: 0.000000
 6221/10000: episode: 188, duration: 0.365s, episode steps:  32, steps per second:  88, episode reward: -35.130, mean reward: -1.098 [-32.010,  2.745], mean action: 12.188 [1.000, 21.000],  loss: 0.015951, mae: 0.593646, mean_q: 0.069263, mean_eps: 0.000000
 6268/10000: episode: 189, duration: 0.525s, episode steps:  47, steps per second:  90, episode reward: -35.830, mean reward: -0.762 [-32.089,  2.270], mean action: 4.255 [1.000, 16.000],  loss: 0.018038, mae: 0.642407, mean_q: 0.019293, mean_eps: 0.000000
 6308/10000: episode: 190, duration: 0.430s, episode steps:  40, steps per second:  93, episode reward: -39.000, mean reward: -0.975 [-32.694,  3.000], mean action: 13.000 [1.000, 20.000],  loss: 0.015321, mae: 0.586381, mean_q: 0.088585, mean_eps: 0.000000
 6342/10000: episode: 191, duration: 0.383s, episode steps:  34, steps per second:  89, episode reward: 43.689, mean reward:  1.285 [-2.082, 32.350], mean action: 3.882 [1.000, 18.000],  loss: 0.017076, mae: 0.615227, mean_q: 0.040614, mean_eps: 0.000000
 6379/10000: episode: 192, duration: 0.381s, episode steps:  37, steps per second:  97, episode reward: -33.000, mean reward: -0.892 [-32.354,  2.465], mean action: 6.865 [1.000, 15.000],  loss: 0.020100, mae: 0.653046, mean_q: -0.000573, mean_eps: 0.000000
 6408/10000: episode: 193, duration: 0.314s, episode steps:  29, steps per second:  92, episode reward: -41.940, mean reward: -1.446 [-32.296,  2.292], mean action: 12.586 [1.000, 20.000],  loss: 0.017732, mae: 0.635088, mean_q: 0.037682, mean_eps: 0.000000
 6443/10000: episode: 194, duration: 0.373s, episode steps:  35, steps per second:  94, episode reward: 32.665, mean reward:  0.933 [-2.886, 32.310], mean action: 5.600 [1.000, 17.000],  loss: 0.015418, mae: 0.584931, mean_q: 0.074647, mean_eps: 0.000000
 6475/10000: episode: 195, duration: 0.347s, episode steps:  32, steps per second:  92, episode reward: -36.000, mean reward: -1.125 [-32.111,  2.280], mean action: 9.438 [1.000, 17.000],  loss: 0.017595, mae: 0.624237, mean_q: 0.048377, mean_eps: 0.000000
 6503/10000: episode: 196, duration: 0.330s, episode steps:  28, steps per second:  85, episode reward: -38.350, mean reward: -1.370 [-32.167,  2.200], mean action: 8.929 [1.000, 16.000],  loss: 0.017503, mae: 0.604230, mean_q: 0.067958, mean_eps: 0.000000
 6551/10000: episode: 197, duration: 0.558s, episode steps:  48, steps per second:  86, episode reward: 32.212, mean reward:  0.671 [-3.000, 32.910], mean action: 10.354 [1.000, 17.000],  loss: 0.016204, mae: 0.557928, mean_q: 0.113101, mean_eps: 0.000000
 6586/10000: episode: 198, duration: 0.430s, episode steps:  35, steps per second:  81, episode reward: -44.110, mean reward: -1.260 [-32.183,  3.000], mean action: 8.314 [1.000, 15.000],  loss: 0.015736, mae: 0.616729, mean_q: 0.051053, mean_eps: 0.000000
 6621/10000: episode: 199, duration: 0.379s, episode steps:  35, steps per second:  92, episode reward: -35.320, mean reward: -1.009 [-31.742,  2.590], mean action: 9.714 [1.000, 18.000],  loss: 0.016646, mae: 0.548746, mean_q: 0.118303, mean_eps: 0.000000
 6646/10000: episode: 200, duration: 0.274s, episode steps:  25, steps per second:  91, episode reward: 33.000, mean reward:  1.320 [-3.000, 29.857], mean action: 9.360 [1.000, 17.000],  loss: 0.018129, mae: 0.629852, mean_q: 0.009787, mean_eps: 0.000000
 6693/10000: episode: 201, duration: 0.625s, episode steps:  47, steps per second:  75, episode reward: -35.210, mean reward: -0.749 [-31.877,  2.522], mean action: 10.277 [1.000, 21.000],  loss: 0.017205, mae: 0.588812, mean_q: 0.089437, mean_eps: 0.000000
 6753/10000: episode: 202, duration: 0.650s, episode steps:  60, steps per second:  92, episode reward: -39.000, mean reward: -0.650 [-32.102,  2.212], mean action: 8.917 [1.000, 17.000],  loss: 0.015831, mae: 0.588788, mean_q: 0.069316, mean_eps: 0.000000
 6787/10000: episode: 203, duration: 0.392s, episode steps:  34, steps per second:  87, episode reward: -33.000, mean reward: -0.971 [-32.311,  3.000], mean action: 11.294 [1.000, 18.000],  loss: 0.016416, mae: 0.609082, mean_q: 0.050336, mean_eps: 0.000000
 6813/10000: episode: 204, duration: 0.342s, episode steps:  26, steps per second:  76, episode reward: 40.564, mean reward:  1.560 [-2.187, 32.270], mean action: 7.038 [1.000, 15.000],  loss: 0.022212, mae: 0.634072, mean_q: 0.032411, mean_eps: 0.000000
 6847/10000: episode: 205, duration: 0.423s, episode steps:  34, steps per second:  80, episode reward: -35.170, mean reward: -1.034 [-32.289,  3.000], mean action: 9.941 [1.000, 21.000],  loss: 0.018175, mae: 0.603448, mean_q: 0.047730, mean_eps: 0.000000
 6874/10000: episode: 206, duration: 0.342s, episode steps:  27, steps per second:  79, episode reward: 38.252, mean reward:  1.417 [-3.000, 32.833], mean action: 3.815 [1.000, 16.000],  loss: 0.015432, mae: 0.606682, mean_q: 0.050391, mean_eps: 0.000000
 6913/10000: episode: 207, duration: 0.394s, episode steps:  39, steps per second:  99, episode reward: -38.310, mean reward: -0.982 [-32.181,  2.181], mean action: 8.333 [1.000, 17.000],  loss: 0.017245, mae: 0.663289, mean_q: 0.007777, mean_eps: 0.000000
 6942/10000: episode: 208, duration: 0.330s, episode steps:  29, steps per second:  88, episode reward: -32.650, mean reward: -1.126 [-31.902,  2.431], mean action: 11.483 [1.000, 15.000],  loss: 0.015238, mae: 0.594052, mean_q: 0.073485, mean_eps: 0.000000
 6990/10000: episode: 209, duration: 0.602s, episode steps:  48, steps per second:  80, episode reward: -32.610, mean reward: -0.679 [-31.979,  2.604], mean action: 7.125 [1.000, 20.000],  loss: 0.014200, mae: 0.573667, mean_q: 0.058791, mean_eps: 0.000000
 7025/10000: episode: 210, duration: 0.401s, episode steps:  35, steps per second:  87, episode reward: -32.280, mean reward: -0.922 [-31.443,  2.473], mean action: 8.143 [1.000, 18.000],  loss: 0.015502, mae: 0.580742, mean_q: 0.049248, mean_eps: 0.000000
 7053/10000: episode: 211, duration: 0.321s, episode steps:  28, steps per second:  87, episode reward: -41.090, mean reward: -1.468 [-32.145,  2.120], mean action: 8.571 [1.000, 18.000],  loss: 0.014968, mae: 0.591421, mean_q: 0.013826, mean_eps: 0.000000
 7105/10000: episode: 212, duration: 0.535s, episode steps:  52, steps per second:  97, episode reward: -35.330, mean reward: -0.679 [-31.524,  2.211], mean action: 7.827 [1.000, 18.000],  loss: 0.014717, mae: 0.599052, mean_q: 0.066016, mean_eps: 0.000000
 7142/10000: episode: 213, duration: 0.385s, episode steps:  37, steps per second:  96, episode reward: -35.050, mean reward: -0.947 [-31.400,  2.860], mean action: 12.541 [1.000, 17.000],  loss: 0.013157, mae: 0.522727, mean_q: 0.112412, mean_eps: 0.000000
 7170/10000: episode: 214, duration: 0.305s, episode steps:  28, steps per second:  92, episode reward: -36.000, mean reward: -1.286 [-32.426,  2.383], mean action: 7.714 [1.000, 20.000],  loss: 0.019958, mae: 0.612086, mean_q: 0.035324, mean_eps: 0.000000
 7193/10000: episode: 215, duration: 0.256s, episode steps:  23, steps per second:  90, episode reward: -38.550, mean reward: -1.676 [-32.457,  2.530], mean action: 9.348 [1.000, 20.000],  loss: 0.021701, mae: 0.629911, mean_q: 0.049837, mean_eps: 0.000000
 7226/10000: episode: 216, duration: 0.350s, episode steps:  33, steps per second:  94, episode reward: -38.640, mean reward: -1.171 [-32.128,  2.392], mean action: 10.545 [1.000, 20.000],  loss: 0.019881, mae: 0.606176, mean_q: 0.044263, mean_eps: 0.000000
 7248/10000: episode: 217, duration: 0.234s, episode steps:  22, steps per second:  94, episode reward: -41.270, mean reward: -1.876 [-32.139,  2.010], mean action: 10.955 [1.000, 20.000],  loss: 0.017751, mae: 0.636400, mean_q: 0.040341, mean_eps: 0.000000
 7279/10000: episode: 218, duration: 0.334s, episode steps:  31, steps per second:  93, episode reward: 35.277, mean reward:  1.138 [-2.270, 32.320], mean action: 4.226 [1.000, 20.000],  loss: 0.014887, mae: 0.594349, mean_q: 0.040141, mean_eps: 0.000000
 7316/10000: episode: 219, duration: 0.404s, episode steps:  37, steps per second:  92, episode reward: -32.360, mean reward: -0.875 [-32.246,  2.770], mean action: 9.270 [1.000, 15.000],  loss: 0.018187, mae: 0.626335, mean_q: 0.060734, mean_eps: 0.000000
 7339/10000: episode: 220, duration: 0.260s, episode steps:  23, steps per second:  88, episode reward: -38.410, mean reward: -1.670 [-31.750,  2.710], mean action: 13.696 [1.000, 16.000],  loss: 0.015149, mae: 0.616336, mean_q: 0.040308, mean_eps: 0.000000
 7364/10000: episode: 221, duration: 0.253s, episode steps:  25, steps per second:  99, episode reward: -36.000, mean reward: -1.440 [-32.373,  2.901], mean action: 12.000 [1.000, 20.000],  loss: 0.014102, mae: 0.625896, mean_q: 0.024954, mean_eps: 0.000000
 7414/10000: episode: 222, duration: 0.538s, episode steps:  50, steps per second:  93, episode reward: -41.880, mean reward: -0.838 [-32.091,  2.231], mean action: 14.380 [6.000, 18.000],  loss: 0.018765, mae: 0.644356, mean_q: 0.017995, mean_eps: 0.000000
 7435/10000: episode: 223, duration: 0.272s, episode steps:  21, steps per second:  77, episode reward: -41.510, mean reward: -1.977 [-32.221,  2.131], mean action: 14.714 [1.000, 20.000],  loss: 0.013455, mae: 0.609237, mean_q: -0.004455, mean_eps: 0.000000
 7454/10000: episode: 224, duration: 0.205s, episode steps:  19, steps per second:  93, episode reward: -38.440, mean reward: -2.023 [-32.169,  2.261], mean action: 12.579 [1.000, 20.000],  loss: 0.013031, mae: 0.558971, mean_q: 0.053071, mean_eps: 0.000000
 7482/10000: episode: 225, duration: 0.303s, episode steps:  28, steps per second:  92, episode reward: -44.950, mean reward: -1.605 [-32.621,  2.370], mean action: 13.036 [1.000, 21.000],  loss: 0.017973, mae: 0.605307, mean_q: 0.026954, mean_eps: 0.000000
 7522/10000: episode: 226, duration: 0.451s, episode steps:  40, steps per second:  89, episode reward: -36.000, mean reward: -0.900 [-32.275,  2.736], mean action: 11.150 [1.000, 17.000],  loss: 0.017823, mae: 0.574486, mean_q: 0.077565, mean_eps: 0.000000
 7608/10000: episode: 227, duration: 0.896s, episode steps:  86, steps per second:  96, episode reward: -41.810, mean reward: -0.486 [-32.212,  2.311], mean action: 11.663 [1.000, 15.000],  loss: 0.015799, mae: 0.613609, mean_q: 0.035462, mean_eps: 0.000000
 7626/10000: episode: 228, duration: 0.219s, episode steps:  18, steps per second:  82, episode reward: 41.794, mean reward:  2.322 [-2.287, 32.033], mean action: 2.056 [1.000, 15.000],  loss: 0.013160, mae: 0.633547, mean_q: -0.007321, mean_eps: 0.000000
 7680/10000: episode: 229, duration: 0.564s, episode steps:  54, steps per second:  96, episode reward: 32.404, mean reward:  0.600 [-2.355, 31.927], mean action: 11.278 [1.000, 21.000],  loss: 0.016723, mae: 0.638618, mean_q: 0.007581, mean_eps: 0.000000
 7712/10000: episode: 230, duration: 0.347s, episode steps:  32, steps per second:  92, episode reward: -33.000, mean reward: -1.031 [-32.299,  2.795], mean action: 11.594 [1.000, 18.000],  loss: 0.016637, mae: 0.610225, mean_q: 0.028692, mean_eps: 0.000000
 7736/10000: episode: 231, duration: 0.255s, episode steps:  24, steps per second:  94, episode reward: 38.509, mean reward:  1.605 [-2.512, 32.520], mean action: 2.292 [1.000, 12.000],  loss: 0.017913, mae: 0.657866, mean_q: -0.000548, mean_eps: 0.000000
 7774/10000: episode: 232, duration: 0.399s, episode steps:  38, steps per second:  95, episode reward: 35.192, mean reward:  0.926 [-2.549, 32.080], mean action: 5.947 [1.000, 17.000],  loss: 0.016650, mae: 0.591834, mean_q: 0.050361, mean_eps: 0.000000
 7809/10000: episode: 233, duration: 0.361s, episode steps:  35, steps per second:  97, episode reward: -36.000, mean reward: -1.029 [-32.067,  2.910], mean action: 5.857 [1.000, 17.000],  loss: 0.015732, mae: 0.601905, mean_q: 0.063771, mean_eps: 0.000000
 7855/10000: episode: 234, duration: 0.476s, episode steps:  46, steps per second:  97, episode reward: -38.550, mean reward: -0.838 [-32.888,  2.312], mean action: 7.870 [1.000, 17.000],  loss: 0.017566, mae: 0.592399, mean_q: 0.043040, mean_eps: 0.000000
 7886/10000: episode: 235, duration: 0.336s, episode steps:  31, steps per second:  92, episode reward: -32.930, mean reward: -1.062 [-32.216,  3.120], mean action: 9.645 [1.000, 21.000],  loss: 0.015890, mae: 0.635083, mean_q: -0.022890, mean_eps: 0.000000
 7954/10000: episode: 236, duration: 0.913s, episode steps:  68, steps per second:  75, episode reward: 34.408, mean reward:  0.506 [-3.000, 32.903], mean action: 6.632 [1.000, 20.000],  loss: 0.016625, mae: 0.585925, mean_q: 0.052463, mean_eps: 0.000000
 8008/10000: episode: 237, duration: 0.588s, episode steps:  54, steps per second:  92, episode reward: -33.000, mean reward: -0.611 [-32.900,  2.780], mean action: 8.667 [1.000, 17.000],  loss: 0.015217, mae: 0.577274, mean_q: 0.041611, mean_eps: 0.000000
 8047/10000: episode: 238, duration: 0.445s, episode steps:  39, steps per second:  88, episode reward: 32.728, mean reward:  0.839 [-3.000, 32.708], mean action: 11.590 [1.000, 17.000],  loss: 0.016820, mae: 0.618020, mean_q: 0.017547, mean_eps: 0.000000
 8076/10000: episode: 239, duration: 0.338s, episode steps:  29, steps per second:  86, episode reward: -32.810, mean reward: -1.131 [-32.110,  3.000], mean action: 9.897 [1.000, 17.000],  loss: 0.015217, mae: 0.614763, mean_q: 0.022726, mean_eps: 0.000000
 8128/10000: episode: 240, duration: 0.556s, episode steps:  52, steps per second:  94, episode reward: -35.200, mean reward: -0.677 [-32.284,  2.340], mean action: 13.250 [1.000, 16.000],  loss: 0.014551, mae: 0.541862, mean_q: 0.082812, mean_eps: 0.000000
 8159/10000: episode: 241, duration: 0.321s, episode steps:  31, steps per second:  97, episode reward: 38.160, mean reward:  1.231 [-2.162, 32.114], mean action: 7.161 [1.000, 17.000],  loss: 0.014946, mae: 0.600403, mean_q: 0.025650, mean_eps: 0.000000
 8215/10000: episode: 242, duration: 0.580s, episode steps:  56, steps per second:  97, episode reward: 32.597, mean reward:  0.582 [-2.524, 32.597], mean action: 11.536 [1.000, 17.000],  loss: 0.017865, mae: 0.590219, mean_q: 0.052535, mean_eps: 0.000000
 8242/10000: episode: 243, duration: 0.285s, episode steps:  27, steps per second:  95, episode reward: -41.510, mean reward: -1.537 [-32.449,  2.017], mean action: 13.148 [1.000, 20.000],  loss: 0.012349, mae: 0.566940, mean_q: 0.042346, mean_eps: 0.000000
 8270/10000: episode: 244, duration: 0.312s, episode steps:  28, steps per second:  90, episode reward: 40.822, mean reward:  1.458 [-2.183, 30.010], mean action: 4.750 [1.000, 20.000],  loss: 0.014980, mae: 0.562479, mean_q: 0.038786, mean_eps: 0.000000
 8287/10000: episode: 245, duration: 0.190s, episode steps:  17, steps per second:  89, episode reward: 41.148, mean reward:  2.420 [-2.221, 32.414], mean action: 1.941 [1.000, 12.000],  loss: 0.013798, mae: 0.621942, mean_q: -0.002789, mean_eps: 0.000000
 8320/10000: episode: 246, duration: 0.354s, episode steps:  33, steps per second:  93, episode reward: -38.740, mean reward: -1.174 [-32.352,  2.660], mean action: 10.061 [1.000, 16.000],  loss: 0.015303, mae: 0.559642, mean_q: 0.075210, mean_eps: 0.000000
 8352/10000: episode: 247, duration: 0.326s, episode steps:  32, steps per second:  98, episode reward: -35.650, mean reward: -1.114 [-32.403,  2.420], mean action: 9.031 [1.000, 17.000],  loss: 0.017329, mae: 0.616955, mean_q: 0.018579, mean_eps: 0.000000
 8377/10000: episode: 248, duration: 0.331s, episode steps:  25, steps per second:  76, episode reward: -41.910, mean reward: -1.676 [-32.039,  2.580], mean action: 14.560 [6.000, 18.000],  loss: 0.018489, mae: 0.572442, mean_q: 0.088124, mean_eps: 0.000000
 8406/10000: episode: 249, duration: 0.315s, episode steps:  29, steps per second:  92, episode reward: -41.400, mean reward: -1.428 [-32.329,  3.000], mean action: 8.069 [1.000, 18.000],  loss: 0.015298, mae: 0.540273, mean_q: 0.083584, mean_eps: 0.000000
 8468/10000: episode: 250, duration: 0.723s, episode steps:  62, steps per second:  86, episode reward: -32.880, mean reward: -0.530 [-32.062,  2.529], mean action: 5.919 [1.000, 17.000],  loss: 0.017217, mae: 0.580560, mean_q: 0.066274, mean_eps: 0.000000
 8499/10000: episode: 251, duration: 0.345s, episode steps:  31, steps per second:  90, episode reward: 32.858, mean reward:  1.060 [-2.492, 32.880], mean action: 4.548 [1.000, 17.000],  loss: 0.015982, mae: 0.597523, mean_q: 0.051564, mean_eps: 0.000000
 8528/10000: episode: 252, duration: 0.310s, episode steps:  29, steps per second:  93, episode reward: -38.650, mean reward: -1.333 [-32.196,  2.191], mean action: 10.931 [1.000, 17.000],  loss: 0.017536, mae: 0.559508, mean_q: 0.063170, mean_eps: 0.000000
 8555/10000: episode: 253, duration: 0.290s, episode steps:  27, steps per second:  93, episode reward: -38.260, mean reward: -1.417 [-32.178,  3.000], mean action: 10.556 [1.000, 21.000],  loss: 0.017930, mae: 0.578119, mean_q: 0.033321, mean_eps: 0.000000
 8602/10000: episode: 254, duration: 0.511s, episode steps:  47, steps per second:  92, episode reward: 32.213, mean reward:  0.685 [-2.939, 32.470], mean action: 8.340 [1.000, 18.000],  loss: 0.015834, mae: 0.544950, mean_q: 0.064999, mean_eps: 0.000000
 8623/10000: episode: 255, duration: 0.215s, episode steps:  21, steps per second:  98, episode reward: -38.180, mean reward: -1.818 [-32.035,  2.583], mean action: 12.571 [1.000, 17.000],  loss: 0.021284, mae: 0.589952, mean_q: 0.038482, mean_eps: 0.000000
 8666/10000: episode: 256, duration: 0.472s, episode steps:  43, steps per second:  91, episode reward: -41.810, mean reward: -0.972 [-32.203,  2.459], mean action: 15.070 [6.000, 20.000],  loss: 0.015309, mae: 0.578346, mean_q: 0.027445, mean_eps: 0.000000
 8697/10000: episode: 257, duration: 0.357s, episode steps:  31, steps per second:  87, episode reward: -38.150, mean reward: -1.231 [-32.182,  2.180], mean action: 11.806 [1.000, 20.000],  loss: 0.015839, mae: 0.527940, mean_q: 0.103320, mean_eps: 0.000000
 8722/10000: episode: 258, duration: 0.284s, episode steps:  25, steps per second:  88, episode reward: -38.760, mean reward: -1.550 [-32.613,  2.560], mean action: 11.440 [1.000, 20.000],  loss: 0.014888, mae: 0.561085, mean_q: 0.037237, mean_eps: 0.000000
 8756/10000: episode: 259, duration: 0.375s, episode steps:  34, steps per second:  91, episode reward: -32.420, mean reward: -0.954 [-32.171,  2.859], mean action: 13.647 [1.000, 18.000],  loss: 0.014309, mae: 0.546085, mean_q: 0.060448, mean_eps: 0.000000
 8789/10000: episode: 260, duration: 0.363s, episode steps:  33, steps per second:  91, episode reward: 36.000, mean reward:  1.091 [-2.776, 32.250], mean action: 6.970 [1.000, 18.000],  loss: 0.016197, mae: 0.588080, mean_q: -0.010371, mean_eps: 0.000000
 8817/10000: episode: 261, duration: 0.335s, episode steps:  28, steps per second:  84, episode reward: -33.000, mean reward: -1.179 [-29.880,  3.000], mean action: 7.179 [1.000, 17.000],  loss: 0.017102, mae: 0.608161, mean_q: 0.006356, mean_eps: 0.000000
 8840/10000: episode: 262, duration: 0.307s, episode steps:  23, steps per second:  75, episode reward: -41.420, mean reward: -1.801 [-32.586,  2.590], mean action: 8.957 [1.000, 16.000],  loss: 0.023508, mae: 0.630806, mean_q: 0.016698, mean_eps: 0.000000
 8866/10000: episode: 263, duration: 0.331s, episode steps:  26, steps per second:  78, episode reward: 41.720, mean reward:  1.605 [-2.998, 31.972], mean action: 3.077 [1.000, 16.000],  loss: 0.015052, mae: 0.557324, mean_q: 0.042761, mean_eps: 0.000000
 8887/10000: episode: 264, duration: 0.229s, episode steps:  21, steps per second:  92, episode reward: -38.910, mean reward: -1.853 [-32.318,  2.453], mean action: 12.000 [1.000, 18.000],  loss: 0.015869, mae: 0.541134, mean_q: 0.080905, mean_eps: 0.000000
 8925/10000: episode: 265, duration: 0.437s, episode steps:  38, steps per second:  87, episode reward: -42.000, mean reward: -1.105 [-32.134,  2.320], mean action: 12.789 [1.000, 18.000],  loss: 0.016330, mae: 0.603244, mean_q: 0.019653, mean_eps: 0.000000
 8952/10000: episode: 266, duration: 0.329s, episode steps:  27, steps per second:  82, episode reward: -36.000, mean reward: -1.333 [-29.491,  2.450], mean action: 10.333 [1.000, 15.000],  loss: 0.015496, mae: 0.541742, mean_q: 0.119082, mean_eps: 0.000000
 8981/10000: episode: 267, duration: 0.346s, episode steps:  29, steps per second:  84, episode reward: -44.310, mean reward: -1.528 [-31.509,  2.229], mean action: 14.828 [11.000, 21.000],  loss: 0.014106, mae: 0.552263, mean_q: 0.030149, mean_eps: 0.000000
 9000/10000: episode: 268, duration: 0.214s, episode steps:  19, steps per second:  89, episode reward: -39.000, mean reward: -2.053 [-32.806,  2.393], mean action: 10.684 [1.000, 21.000],  loss: 0.015823, mae: 0.565772, mean_q: 0.051037, mean_eps: 0.000000
 9020/10000: episode: 269, duration: 0.243s, episode steps:  20, steps per second:  82, episode reward: 41.433, mean reward:  2.072 [-2.249, 32.180], mean action: 5.000 [1.000, 17.000],  loss: 0.019226, mae: 0.586462, mean_q: 0.064633, mean_eps: 0.000000
 9039/10000: episode: 270, duration: 0.226s, episode steps:  19, steps per second:  84, episode reward: -41.600, mean reward: -2.189 [-32.136,  2.130], mean action: 13.053 [1.000, 21.000],  loss: 0.017533, mae: 0.588062, mean_q: 0.054346, mean_eps: 0.000000
 9069/10000: episode: 271, duration: 0.361s, episode steps:  30, steps per second:  83, episode reward: 36.785, mean reward:  1.226 [-2.415, 32.490], mean action: 11.900 [1.000, 20.000],  loss: 0.012893, mae: 0.580624, mean_q: 0.010447, mean_eps: 0.000000
 9096/10000: episode: 272, duration: 0.285s, episode steps:  27, steps per second:  95, episode reward: -38.630, mean reward: -1.431 [-31.996,  2.180], mean action: 9.333 [1.000, 21.000],  loss: 0.015012, mae: 0.564692, mean_q: 0.013844, mean_eps: 0.000000
 9129/10000: episode: 273, duration: 0.372s, episode steps:  33, steps per second:  89, episode reward: -38.810, mean reward: -1.176 [-32.722,  2.830], mean action: 12.273 [1.000, 20.000],  loss: 0.015548, mae: 0.561952, mean_q: 0.049710, mean_eps: 0.000000
 9206/10000: episode: 274, duration: 0.945s, episode steps:  77, steps per second:  82, episode reward: 38.092, mean reward:  0.495 [-2.971, 33.000], mean action: 12.610 [1.000, 15.000],  loss: 0.018361, mae: 0.587860, mean_q: 0.056989, mean_eps: 0.000000
 9230/10000: episode: 275, duration: 0.268s, episode steps:  24, steps per second:  90, episode reward: 35.279, mean reward:  1.470 [-2.443, 32.410], mean action: 7.250 [1.000, 15.000],  loss: 0.014584, mae: 0.559102, mean_q: 0.071324, mean_eps: 0.000000
 9280/10000: episode: 276, duration: 0.589s, episode steps:  50, steps per second:  85, episode reward: -32.320, mean reward: -0.646 [-33.000,  2.655], mean action: 12.720 [1.000, 17.000],  loss: 0.017595, mae: 0.577634, mean_q: 0.028357, mean_eps: 0.000000
 9319/10000: episode: 277, duration: 0.435s, episode steps:  39, steps per second:  90, episode reward: -32.880, mean reward: -0.843 [-32.127,  2.292], mean action: 10.359 [1.000, 16.000],  loss: 0.015276, mae: 0.531615, mean_q: 0.069121, mean_eps: 0.000000
 9375/10000: episode: 278, duration: 0.586s, episode steps:  56, steps per second:  96, episode reward: -36.000, mean reward: -0.643 [-32.162,  2.670], mean action: 6.250 [1.000, 17.000],  loss: 0.017185, mae: 0.562111, mean_q: 0.054180, mean_eps: 0.000000
 9401/10000: episode: 279, duration: 0.279s, episode steps:  26, steps per second:  93, episode reward: -35.670, mean reward: -1.372 [-32.004,  2.964], mean action: 8.038 [1.000, 17.000],  loss: 0.017809, mae: 0.570390, mean_q: 0.039535, mean_eps: 0.000000
 9433/10000: episode: 280, duration: 0.338s, episode steps:  32, steps per second:  95, episode reward: -38.580, mean reward: -1.206 [-32.139,  2.430], mean action: 9.781 [1.000, 21.000],  loss: 0.017618, mae: 0.588357, mean_q: 0.026607, mean_eps: 0.000000
 9458/10000: episode: 281, duration: 0.265s, episode steps:  25, steps per second:  94, episode reward: 32.410, mean reward:  1.296 [-3.000, 30.725], mean action: 11.440 [1.000, 16.000],  loss: 0.015450, mae: 0.598208, mean_q: 0.041549, mean_eps: 0.000000
 9518/10000: episode: 282, duration: 0.749s, episode steps:  60, steps per second:  80, episode reward: 32.697, mean reward:  0.545 [-3.000, 32.924], mean action: 6.117 [1.000, 17.000],  loss: 0.015804, mae: 0.525658, mean_q: 0.100141, mean_eps: 0.000000
 9540/10000: episode: 283, duration: 0.233s, episode steps:  22, steps per second:  94, episode reward: 37.786, mean reward:  1.718 [-2.188, 32.040], mean action: 7.864 [1.000, 18.000],  loss: 0.012475, mae: 0.623240, mean_q: -0.034039, mean_eps: 0.000000
 9586/10000: episode: 284, duration: 0.478s, episode steps:  46, steps per second:  96, episode reward: 41.676, mean reward:  0.906 [-2.210, 32.186], mean action: 2.804 [1.000, 15.000],  loss: 0.017343, mae: 0.557817, mean_q: 0.079510, mean_eps: 0.000000
 9629/10000: episode: 285, duration: 0.449s, episode steps:  43, steps per second:  96, episode reward: -41.040, mean reward: -0.954 [-32.124,  2.500], mean action: 8.349 [1.000, 20.000],  loss: 0.016729, mae: 0.568672, mean_q: 0.029293, mean_eps: 0.000000
 9656/10000: episode: 286, duration: 0.301s, episode steps:  27, steps per second:  90, episode reward: -38.120, mean reward: -1.412 [-32.708,  2.110], mean action: 9.481 [1.000, 20.000],  loss: 0.015289, mae: 0.579762, mean_q: 0.026505, mean_eps: 0.000000
 9667/10000: episode: 287, duration: 0.146s, episode steps:  11, steps per second:  75, episode reward: 45.956, mean reward:  4.178 [-0.138, 32.230], mean action: 5.091 [1.000, 15.000],  loss: 0.017416, mae: 0.563321, mean_q: 0.052647, mean_eps: 0.000000
 9704/10000: episode: 288, duration: 0.444s, episode steps:  37, steps per second:  83, episode reward: 43.452, mean reward:  1.174 [-2.430, 32.160], mean action: 7.865 [1.000, 17.000],  loss: 0.018019, mae: 0.557442, mean_q: 0.057989, mean_eps: 0.000000
 9737/10000: episode: 289, duration: 0.395s, episode steps:  33, steps per second:  84, episode reward: -39.000, mean reward: -1.182 [-32.146,  2.712], mean action: 11.727 [1.000, 18.000],  loss: 0.014580, mae: 0.570220, mean_q: 0.036545, mean_eps: 0.000000
 9770/10000: episode: 290, duration: 0.358s, episode steps:  33, steps per second:  92, episode reward: -30.000, mean reward: -0.909 [-29.934,  2.430], mean action: 9.848 [1.000, 17.000],  loss: 0.016254, mae: 0.549643, mean_q: 0.059398, mean_eps: 0.000000
 9830/10000: episode: 291, duration: 0.644s, episode steps:  60, steps per second:  93, episode reward: -35.860, mean reward: -0.598 [-32.511,  2.310], mean action: 9.167 [1.000, 17.000],  loss: 0.016105, mae: 0.558562, mean_q: 0.073733, mean_eps: 0.000000
 9869/10000: episode: 292, duration: 0.404s, episode steps:  39, steps per second:  96, episode reward: -44.660, mean reward: -1.145 [-32.308,  2.200], mean action: 13.846 [10.000, 18.000],  loss: 0.016654, mae: 0.533929, mean_q: 0.107791, mean_eps: 0.000000
 9901/10000: episode: 293, duration: 0.335s, episode steps:  32, steps per second:  96, episode reward: 36.000, mean reward:  1.125 [-3.000, 32.340], mean action: 4.594 [1.000, 16.000],  loss: 0.015397, mae: 0.555002, mean_q: 0.079658, mean_eps: 0.000000
 9925/10000: episode: 294, duration: 0.250s, episode steps:  24, steps per second:  96, episode reward: 35.414, mean reward:  1.476 [-2.417, 32.230], mean action: 6.208 [1.000, 14.000],  loss: 0.016898, mae: 0.580072, mean_q: 0.050769, mean_eps: 0.000000
 9955/10000: episode: 295, duration: 0.319s, episode steps:  30, steps per second:  94, episode reward: -36.000, mean reward: -1.200 [-29.669,  2.150], mean action: 14.133 [1.000, 20.000],  loss: 0.017951, mae: 0.547989, mean_q: 0.105030, mean_eps: 0.000000
done, took 111.906 seconds
Results against random player:
DQN Evaluation: 215 victories out of 300 episodes

Results against max player:
DQN Evaluation: 91 victories out of 300 episodes
Training for 10000 steps ...
   63/10000: episode: 1, duration: 0.604s, episode steps:  63, steps per second: 104, episode reward: -32.740, mean reward: -0.520 [-32.535,  3.061], mean action: 7.460 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  108/10000: episode: 2, duration: 0.279s, episode steps:  45, steps per second: 162, episode reward: 32.446, mean reward:  0.721 [-2.321, 32.080], mean action: 12.600 [4.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  148/10000: episode: 3, duration: 0.232s, episode steps:  40, steps per second: 173, episode reward: -36.000, mean reward: -0.900 [-32.137,  2.213], mean action: 13.425 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  200/10000: episode: 4, duration: 0.305s, episode steps:  52, steps per second: 170, episode reward: -32.610, mean reward: -0.627 [-31.911,  2.547], mean action: 13.962 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  228/10000: episode: 5, duration: 0.176s, episode steps:  28, steps per second: 159, episode reward: -35.750, mean reward: -1.277 [-32.006,  2.863], mean action: 11.571 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  254/10000: episode: 6, duration: 0.160s, episode steps:  26, steps per second: 163, episode reward: -41.130, mean reward: -1.582 [-32.061,  2.342], mean action: 14.615 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  287/10000: episode: 7, duration: 0.206s, episode steps:  33, steps per second: 160, episode reward: -35.630, mean reward: -1.080 [-31.743,  3.000], mean action: 15.121 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  318/10000: episode: 8, duration: 0.171s, episode steps:  31, steps per second: 181, episode reward: -44.500, mean reward: -1.435 [-32.210,  2.120], mean action: 12.484 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  383/10000: episode: 9, duration: 0.597s, episode steps:  65, steps per second: 109, episode reward: -38.860, mean reward: -0.598 [-32.122,  2.210], mean action: 7.523 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  474/10000: episode: 10, duration: 1.574s, episode steps:  91, steps per second:  58, episode reward: -33.000, mean reward: -0.363 [-33.000,  3.000], mean action: 7.802 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  496/10000: episode: 11, duration: 0.511s, episode steps:  22, steps per second:  43, episode reward: -38.160, mean reward: -1.735 [-31.222,  2.151], mean action: 16.727 [4.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  547/10000: episode: 12, duration: 0.531s, episode steps:  51, steps per second:  96, episode reward: -44.340, mean reward: -0.869 [-32.209,  2.340], mean action: 12.255 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  586/10000: episode: 13, duration: 0.696s, episode steps:  39, steps per second:  56, episode reward: -38.630, mean reward: -0.991 [-31.862,  3.062], mean action: 8.897 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  627/10000: episode: 14, duration: 0.658s, episode steps:  41, steps per second:  62, episode reward: -39.000, mean reward: -0.951 [-33.000,  2.351], mean action: 13.146 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  672/10000: episode: 15, duration: 0.340s, episode steps:  45, steps per second: 132, episode reward: -41.380, mean reward: -0.920 [-32.900,  2.430], mean action: 7.511 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  716/10000: episode: 16, duration: 0.301s, episode steps:  44, steps per second: 146, episode reward: -41.180, mean reward: -0.936 [-32.037,  2.920], mean action: 8.000 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  754/10000: episode: 17, duration: 0.215s, episode steps:  38, steps per second: 177, episode reward: -35.780, mean reward: -0.942 [-31.987,  2.670], mean action: 13.816 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  810/10000: episode: 18, duration: 0.310s, episode steps:  56, steps per second: 180, episode reward: 32.204, mean reward:  0.575 [-2.900, 32.600], mean action: 6.339 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  848/10000: episode: 19, duration: 0.213s, episode steps:  38, steps per second: 178, episode reward: -35.640, mean reward: -0.938 [-31.789,  2.734], mean action: 10.237 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  875/10000: episode: 20, duration: 0.164s, episode steps:  27, steps per second: 164, episode reward: -32.410, mean reward: -1.200 [-32.651,  2.874], mean action: 9.667 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  943/10000: episode: 21, duration: 0.364s, episode steps:  68, steps per second: 187, episode reward: -38.170, mean reward: -0.561 [-32.206,  2.330], mean action: 6.029 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  975/10000: episode: 22, duration: 0.187s, episode steps:  32, steps per second: 171, episode reward: -38.940, mean reward: -1.217 [-32.022,  2.113], mean action: 11.438 [2.000, 20.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  998/10000: episode: 23, duration: 0.132s, episode steps:  23, steps per second: 174, episode reward: -35.410, mean reward: -1.540 [-32.134,  2.280], mean action: 10.261 [2.000, 15.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 1064/10000: episode: 24, duration: 1.556s, episode steps:  66, steps per second:  42, episode reward: -32.940, mean reward: -0.499 [-32.145,  2.904], mean action: 8.318 [1.000, 20.000],  loss: 0.014346, mae: 0.493840, mean_q: 0.199017, mean_eps: 0.000000
 1099/10000: episode: 25, duration: 0.354s, episode steps:  35, steps per second:  99, episode reward: -36.000, mean reward: -1.029 [-32.049,  2.180], mean action: 8.971 [2.000, 20.000],  loss: 0.010308, mae: 0.537529, mean_q: 0.008612, mean_eps: 0.000000
 1130/10000: episode: 26, duration: 0.329s, episode steps:  31, steps per second:  94, episode reward: -35.730, mean reward: -1.153 [-31.825,  2.549], mean action: 8.419 [2.000, 20.000],  loss: 0.014517, mae: 0.621079, mean_q: -0.038542, mean_eps: 0.000000
 1158/10000: episode: 27, duration: 0.293s, episode steps:  28, steps per second:  96, episode reward: -32.880, mean reward: -1.174 [-32.281,  2.610], mean action: 9.679 [2.000, 20.000],  loss: 0.013761, mae: 0.569238, mean_q: 0.032450, mean_eps: 0.000000
 1189/10000: episode: 28, duration: 0.338s, episode steps:  31, steps per second:  92, episode reward: -39.000, mean reward: -1.258 [-32.061,  3.000], mean action: 6.194 [2.000, 20.000],  loss: 0.013663, mae: 0.579277, mean_q: -0.006134, mean_eps: 0.000000
 1250/10000: episode: 29, duration: 0.614s, episode steps:  61, steps per second:  99, episode reward: 32.644, mean reward:  0.535 [-2.855, 32.611], mean action: 9.000 [2.000, 18.000],  loss: 0.013809, mae: 0.584563, mean_q: -0.009714, mean_eps: 0.000000
 1278/10000: episode: 30, duration: 0.392s, episode steps:  28, steps per second:  71, episode reward: -45.000, mean reward: -1.607 [-32.405,  1.947], mean action: 9.036 [2.000, 20.000],  loss: 0.014281, mae: 0.573209, mean_q: 0.004998, mean_eps: 0.000000
 1319/10000: episode: 31, duration: 0.445s, episode steps:  41, steps per second:  92, episode reward: -33.000, mean reward: -0.805 [-32.582,  2.450], mean action: 11.098 [2.000, 18.000],  loss: 0.009951, mae: 0.531759, mean_q: 0.042545, mean_eps: 0.000000
 1338/10000: episode: 32, duration: 0.186s, episode steps:  19, steps per second: 102, episode reward: -47.320, mean reward: -2.491 [-32.121,  0.360], mean action: 13.263 [2.000, 18.000],  loss: 0.012158, mae: 0.570455, mean_q: 0.005292, mean_eps: 0.000000
 1387/10000: episode: 33, duration: 0.545s, episode steps:  49, steps per second:  90, episode reward: -39.000, mean reward: -0.796 [-32.220,  2.132], mean action: 8.714 [2.000, 20.000],  loss: 0.012973, mae: 0.583550, mean_q: 0.004401, mean_eps: 0.000000
 1411/10000: episode: 34, duration: 0.242s, episode steps:  24, steps per second:  99, episode reward: -44.050, mean reward: -1.835 [-32.168,  1.835], mean action: 13.375 [2.000, 20.000],  loss: 0.014870, mae: 0.598509, mean_q: -0.002755, mean_eps: 0.000000
 1449/10000: episode: 35, duration: 0.382s, episode steps:  38, steps per second:  99, episode reward: -32.520, mean reward: -0.856 [-32.013,  3.062], mean action: 7.737 [2.000, 20.000],  loss: 0.012133, mae: 0.586269, mean_q: -0.002572, mean_eps: 0.000000
 1495/10000: episode: 36, duration: 0.473s, episode steps:  46, steps per second:  97, episode reward: 35.576, mean reward:  0.773 [-2.269, 32.050], mean action: 5.130 [2.000, 18.000],  loss: 0.010496, mae: 0.574157, mean_q: 0.022450, mean_eps: 0.000000
 1530/10000: episode: 37, duration: 0.359s, episode steps:  35, steps per second:  98, episode reward: -38.290, mean reward: -1.094 [-32.020,  2.336], mean action: 10.771 [2.000, 18.000],  loss: 0.013178, mae: 0.593192, mean_q: 0.010103, mean_eps: 0.000000
 1596/10000: episode: 38, duration: 0.658s, episode steps:  66, steps per second: 100, episode reward: 32.024, mean reward:  0.485 [-2.112, 31.977], mean action: 8.879 [2.000, 20.000],  loss: 0.013665, mae: 0.646592, mean_q: -0.038469, mean_eps: 0.000000
 1630/10000: episode: 39, duration: 0.440s, episode steps:  34, steps per second:  77, episode reward: 35.082, mean reward:  1.032 [-2.531, 32.140], mean action: 8.265 [2.000, 20.000],  loss: 0.014896, mae: 0.612919, mean_q: 0.035147, mean_eps: 0.000000
 1684/10000: episode: 40, duration: 0.556s, episode steps:  54, steps per second:  97, episode reward: 37.298, mean reward:  0.691 [-3.000, 33.000], mean action: 7.907 [2.000, 18.000],  loss: 0.012823, mae: 0.611968, mean_q: 0.012268, mean_eps: 0.000000
 1726/10000: episode: 41, duration: 0.509s, episode steps:  42, steps per second:  82, episode reward: -41.280, mean reward: -0.983 [-31.597,  2.422], mean action: 5.214 [2.000, 15.000],  loss: 0.013270, mae: 0.616485, mean_q: 0.015778, mean_eps: 0.000000
 1755/10000: episode: 42, duration: 0.349s, episode steps:  29, steps per second:  83, episode reward: -41.330, mean reward: -1.425 [-32.174,  2.432], mean action: 11.655 [2.000, 15.000],  loss: 0.015711, mae: 0.635310, mean_q: 0.037683, mean_eps: 0.000000
 1798/10000: episode: 43, duration: 0.498s, episode steps:  43, steps per second:  86, episode reward: -36.000, mean reward: -0.837 [-32.074,  2.903], mean action: 3.791 [2.000, 15.000],  loss: 0.012522, mae: 0.601817, mean_q: 0.058035, mean_eps: 0.000000
 1831/10000: episode: 44, duration: 0.373s, episode steps:  33, steps per second:  88, episode reward: -35.170, mean reward: -1.066 [-31.596,  2.776], mean action: 9.576 [2.000, 15.000],  loss: 0.012471, mae: 0.645129, mean_q: 0.048746, mean_eps: 0.000000
 1848/10000: episode: 45, duration: 0.191s, episode steps:  17, steps per second:  89, episode reward: -44.500, mean reward: -2.618 [-31.985,  1.950], mean action: 12.235 [2.000, 15.000],  loss: 0.010453, mae: 0.628607, mean_q: 0.036759, mean_eps: 0.000000
 1943/10000: episode: 46, duration: 0.984s, episode steps:  95, steps per second:  97, episode reward: -32.680, mean reward: -0.344 [-32.535,  2.952], mean action: 3.042 [2.000, 16.000],  loss: 0.013130, mae: 0.649594, mean_q: 0.033177, mean_eps: 0.000000
 1973/10000: episode: 47, duration: 0.319s, episode steps:  30, steps per second:  94, episode reward: -41.810, mean reward: -1.394 [-32.082,  2.841], mean action: 9.067 [2.000, 16.000],  loss: 0.015035, mae: 0.682494, mean_q: 0.026464, mean_eps: 0.000000
 2002/10000: episode: 48, duration: 0.315s, episode steps:  29, steps per second:  92, episode reward: -32.400, mean reward: -1.117 [-32.004,  2.884], mean action: 8.483 [2.000, 19.000],  loss: 0.014243, mae: 0.676786, mean_q: 0.037994, mean_eps: 0.000000
 2042/10000: episode: 49, duration: 0.437s, episode steps:  40, steps per second:  92, episode reward: 32.489, mean reward:  0.812 [-2.677, 32.220], mean action: 6.500 [2.000, 20.000],  loss: 0.016211, mae: 0.709347, mean_q: 0.017038, mean_eps: 0.000000
 2104/10000: episode: 50, duration: 0.657s, episode steps:  62, steps per second:  94, episode reward: -32.520, mean reward: -0.525 [-31.831,  2.901], mean action: 7.694 [2.000, 20.000],  loss: 0.014058, mae: 0.692218, mean_q: 0.019591, mean_eps: 0.000000
 2135/10000: episode: 51, duration: 0.357s, episode steps:  31, steps per second:  87, episode reward: 32.806, mean reward:  1.058 [-3.000, 32.053], mean action: 4.839 [2.000, 15.000],  loss: 0.013093, mae: 0.663457, mean_q: 0.026040, mean_eps: 0.000000
 2183/10000: episode: 52, duration: 0.500s, episode steps:  48, steps per second:  96, episode reward: -32.580, mean reward: -0.679 [-32.580,  3.000], mean action: 8.917 [2.000, 20.000],  loss: 0.012566, mae: 0.656765, mean_q: 0.036499, mean_eps: 0.000000
 2207/10000: episode: 53, duration: 0.251s, episode steps:  24, steps per second:  95, episode reward: 35.666, mean reward:  1.486 [-2.386, 32.481], mean action: 5.208 [2.000, 20.000],  loss: 0.011261, mae: 0.652290, mean_q: 0.044360, mean_eps: 0.000000
 2255/10000: episode: 54, duration: 0.486s, episode steps:  48, steps per second:  99, episode reward: -35.290, mean reward: -0.735 [-32.534,  2.250], mean action: 8.479 [2.000, 20.000],  loss: 0.014260, mae: 0.666408, mean_q: 0.045781, mean_eps: 0.000000
 2305/10000: episode: 55, duration: 0.510s, episode steps:  50, steps per second:  98, episode reward: 35.375, mean reward:  0.707 [-2.513, 32.428], mean action: 4.880 [2.000, 20.000],  loss: 0.012529, mae: 0.667878, mean_q: 0.020040, mean_eps: 0.000000
 2343/10000: episode: 56, duration: 0.381s, episode steps:  38, steps per second: 100, episode reward: -36.000, mean reward: -0.947 [-32.038,  2.500], mean action: 6.211 [2.000, 20.000],  loss: 0.012374, mae: 0.660556, mean_q: 0.030517, mean_eps: 0.000000
 2377/10000: episode: 57, duration: 0.436s, episode steps:  34, steps per second:  78, episode reward: -39.000, mean reward: -1.147 [-32.217,  2.067], mean action: 9.559 [2.000, 20.000],  loss: 0.014002, mae: 0.672889, mean_q: 0.020845, mean_eps: 0.000000
 2409/10000: episode: 58, duration: 0.341s, episode steps:  32, steps per second:  94, episode reward: -32.460, mean reward: -1.014 [-32.070,  2.742], mean action: 9.781 [2.000, 20.000],  loss: 0.015415, mae: 0.677510, mean_q: 0.012516, mean_eps: 0.000000
 2431/10000: episode: 59, duration: 0.244s, episode steps:  22, steps per second:  90, episode reward: -32.950, mean reward: -1.498 [-30.190,  2.750], mean action: 13.500 [4.000, 20.000],  loss: 0.017649, mae: 0.657760, mean_q: 0.112180, mean_eps: 0.000000
 2514/10000: episode: 60, duration: 0.820s, episode steps:  83, steps per second: 101, episode reward: -35.910, mean reward: -0.433 [-31.918,  2.540], mean action: 6.181 [2.000, 20.000],  loss: 0.012917, mae: 0.664615, mean_q: 0.047527, mean_eps: 0.000000
 2540/10000: episode: 61, duration: 0.260s, episode steps:  26, steps per second: 100, episode reward: -44.140, mean reward: -1.698 [-32.160,  2.360], mean action: 9.308 [2.000, 15.000],  loss: 0.013548, mae: 0.695235, mean_q: -0.025909, mean_eps: 0.000000
 2563/10000: episode: 62, duration: 0.240s, episode steps:  23, steps per second:  96, episode reward: -38.910, mean reward: -1.692 [-32.012,  1.969], mean action: 8.696 [2.000, 15.000],  loss: 0.012240, mae: 0.673517, mean_q: 0.020598, mean_eps: 0.000000
 2587/10000: episode: 63, duration: 0.253s, episode steps:  24, steps per second:  95, episode reward: -45.000, mean reward: -1.875 [-32.128,  2.099], mean action: 11.292 [2.000, 15.000],  loss: 0.015731, mae: 0.717952, mean_q: -0.014895, mean_eps: 0.000000
 2609/10000: episode: 64, duration: 0.223s, episode steps:  22, steps per second:  99, episode reward: -36.000, mean reward: -1.636 [-32.194,  2.620], mean action: 4.045 [2.000, 10.000],  loss: 0.011489, mae: 0.689684, mean_q: 0.012534, mean_eps: 0.000000
 2631/10000: episode: 65, duration: 0.250s, episode steps:  22, steps per second:  88, episode reward: 41.010, mean reward:  1.864 [-3.000, 32.393], mean action: 3.000 [2.000, 15.000],  loss: 0.014134, mae: 0.715285, mean_q: 0.002053, mean_eps: 0.000000
 2721/10000: episode: 66, duration: 0.887s, episode steps:  90, steps per second: 101, episode reward: 32.035, mean reward:  0.356 [-2.623, 31.809], mean action: 7.056 [2.000, 20.000],  loss: 0.013359, mae: 0.672155, mean_q: 0.040153, mean_eps: 0.000000
 2776/10000: episode: 67, duration: 0.541s, episode steps:  55, steps per second: 102, episode reward: -38.330, mean reward: -0.697 [-32.092,  2.190], mean action: 7.091 [2.000, 15.000],  loss: 0.012347, mae: 0.692980, mean_q: 0.021476, mean_eps: 0.000000
 2806/10000: episode: 68, duration: 0.296s, episode steps:  30, steps per second: 101, episode reward: -39.000, mean reward: -1.300 [-32.494,  2.315], mean action: 10.367 [2.000, 15.000],  loss: 0.013527, mae: 0.691492, mean_q: 0.028332, mean_eps: 0.000000
 2832/10000: episode: 69, duration: 0.278s, episode steps:  26, steps per second:  94, episode reward: -38.820, mean reward: -1.493 [-32.139,  2.580], mean action: 8.231 [2.000, 15.000],  loss: 0.014997, mae: 0.665485, mean_q: 0.073331, mean_eps: 0.000000
 2905/10000: episode: 70, duration: 0.727s, episode steps:  73, steps per second: 100, episode reward: 34.894, mean reward:  0.478 [-2.939, 32.080], mean action: 5.699 [2.000, 20.000],  loss: 0.013370, mae: 0.649719, mean_q: 0.061578, mean_eps: 0.000000
 2938/10000: episode: 71, duration: 0.331s, episode steps:  33, steps per second: 100, episode reward: -35.240, mean reward: -1.068 [-31.913,  2.980], mean action: 8.485 [2.000, 20.000],  loss: 0.013396, mae: 0.647621, mean_q: 0.056373, mean_eps: 0.000000
 2957/10000: episode: 72, duration: 0.196s, episode steps:  19, steps per second:  97, episode reward: -41.650, mean reward: -2.192 [-32.015,  1.819], mean action: 7.368 [0.000, 15.000],  loss: 0.011012, mae: 0.679568, mean_q: 0.025677, mean_eps: 0.000000
 2985/10000: episode: 73, duration: 0.299s, episode steps:  28, steps per second:  94, episode reward: 39.415, mean reward:  1.408 [-2.387, 31.986], mean action: 5.821 [2.000, 15.000],  loss: 0.012328, mae: 0.660597, mean_q: 0.100678, mean_eps: 0.000000
 3014/10000: episode: 74, duration: 0.294s, episode steps:  29, steps per second:  99, episode reward: -35.240, mean reward: -1.215 [-31.869,  2.530], mean action: 9.241 [2.000, 20.000],  loss: 0.012694, mae: 0.680341, mean_q: 0.034088, mean_eps: 0.000000
 3042/10000: episode: 75, duration: 0.294s, episode steps:  28, steps per second:  95, episode reward: 32.030, mean reward:  1.144 [-3.000, 32.170], mean action: 6.321 [2.000, 15.000],  loss: 0.015275, mae: 0.722248, mean_q: 0.026693, mean_eps: 0.000000
 3087/10000: episode: 76, duration: 0.452s, episode steps:  45, steps per second: 100, episode reward: -41.040, mean reward: -0.912 [-32.181,  2.130], mean action: 8.422 [2.000, 20.000],  loss: 0.013201, mae: 0.677644, mean_q: 0.060178, mean_eps: 0.000000
 3103/10000: episode: 77, duration: 0.169s, episode steps:  16, steps per second:  94, episode reward: 41.191, mean reward:  2.574 [-2.386, 32.011], mean action: 4.312 [2.000, 15.000],  loss: 0.013866, mae: 0.682141, mean_q: 0.030912, mean_eps: 0.000000
 3128/10000: episode: 78, duration: 0.269s, episode steps:  25, steps per second:  93, episode reward: -35.910, mean reward: -1.436 [-32.165,  2.892], mean action: 9.520 [2.000, 20.000],  loss: 0.012843, mae: 0.685466, mean_q: 0.033489, mean_eps: 0.000000
 3152/10000: episode: 79, duration: 0.353s, episode steps:  24, steps per second:  68, episode reward: -44.120, mean reward: -1.838 [-32.309,  1.927], mean action: 10.083 [2.000, 18.000],  loss: 0.012001, mae: 0.704494, mean_q: -0.015117, mean_eps: 0.000000
 3223/10000: episode: 80, duration: 0.728s, episode steps:  71, steps per second:  98, episode reward: -32.550, mean reward: -0.458 [-32.404,  2.820], mean action: 6.014 [2.000, 15.000],  loss: 0.015380, mae: 0.710494, mean_q: 0.019517, mean_eps: 0.000000
 3248/10000: episode: 81, duration: 0.260s, episode steps:  25, steps per second:  96, episode reward: -35.330, mean reward: -1.413 [-32.004,  2.859], mean action: 9.160 [2.000, 18.000],  loss: 0.014800, mae: 0.692135, mean_q: 0.045285, mean_eps: 0.000000
 3275/10000: episode: 82, duration: 0.275s, episode steps:  27, steps per second:  98, episode reward: -38.530, mean reward: -1.427 [-31.724,  2.092], mean action: 6.852 [2.000, 18.000],  loss: 0.014196, mae: 0.732095, mean_q: 0.016822, mean_eps: 0.000000
 3301/10000: episode: 83, duration: 0.269s, episode steps:  26, steps per second:  97, episode reward: -35.880, mean reward: -1.380 [-32.087,  2.479], mean action: 9.923 [2.000, 18.000],  loss: 0.017197, mae: 0.729276, mean_q: 0.033503, mean_eps: 0.000000
 3382/10000: episode: 84, duration: 0.815s, episode steps:  81, steps per second:  99, episode reward: -32.600, mean reward: -0.402 [-31.952,  2.580], mean action: 5.346 [2.000, 18.000],  loss: 0.015237, mae: 0.729721, mean_q: 0.009131, mean_eps: 0.000000
 3405/10000: episode: 85, duration: 0.240s, episode steps:  23, steps per second:  96, episode reward: -41.480, mean reward: -1.803 [-32.045,  2.453], mean action: 10.826 [2.000, 15.000],  loss: 0.011149, mae: 0.659763, mean_q: 0.051675, mean_eps: 0.000000
 3432/10000: episode: 86, duration: 0.307s, episode steps:  27, steps per second:  88, episode reward: 35.475, mean reward:  1.314 [-2.815, 32.280], mean action: 6.741 [2.000, 13.000],  loss: 0.011163, mae: 0.698012, mean_q: 0.013143, mean_eps: 0.000000
 3487/10000: episode: 87, duration: 0.565s, episode steps:  55, steps per second:  97, episode reward: 33.000, mean reward:  0.600 [-2.578, 33.000], mean action: 4.000 [2.000, 15.000],  loss: 0.011898, mae: 0.684931, mean_q: 0.036865, mean_eps: 0.000000
 3529/10000: episode: 88, duration: 0.422s, episode steps:  42, steps per second:  99, episode reward: -36.000, mean reward: -0.857 [-32.013,  2.671], mean action: 5.095 [2.000, 18.000],  loss: 0.016983, mae: 0.704338, mean_q: 0.058944, mean_eps: 0.000000
 3572/10000: episode: 89, duration: 0.429s, episode steps:  43, steps per second: 100, episode reward: -38.510, mean reward: -0.896 [-31.928,  2.280], mean action: 8.884 [2.000, 18.000],  loss: 0.012963, mae: 0.721794, mean_q: 0.014125, mean_eps: 0.000000
 3592/10000: episode: 90, duration: 0.207s, episode steps:  20, steps per second:  97, episode reward: -41.940, mean reward: -2.097 [-32.399,  2.183], mean action: 7.450 [2.000, 18.000],  loss: 0.013743, mae: 0.736383, mean_q: -0.001937, mean_eps: 0.000000
 3615/10000: episode: 91, duration: 0.252s, episode steps:  23, steps per second:  91, episode reward: -38.150, mean reward: -1.659 [-32.340,  2.241], mean action: 7.087 [2.000, 15.000],  loss: 0.013330, mae: 0.747605, mean_q: -0.024849, mean_eps: 0.000000
 3672/10000: episode: 92, duration: 0.605s, episode steps:  57, steps per second:  94, episode reward: 32.554, mean reward:  0.571 [-2.903, 32.780], mean action: 5.175 [2.000, 20.000],  loss: 0.016116, mae: 0.730179, mean_q: 0.014206, mean_eps: 0.000000
 3732/10000: episode: 93, duration: 0.592s, episode steps:  60, steps per second: 101, episode reward: -41.200, mean reward: -0.687 [-32.580,  2.410], mean action: 8.833 [2.000, 18.000],  loss: 0.015126, mae: 0.742147, mean_q: 0.015001, mean_eps: 0.000000
 3756/10000: episode: 94, duration: 0.255s, episode steps:  24, steps per second:  94, episode reward: 34.569, mean reward:  1.440 [-2.498, 32.002], mean action: 9.833 [2.000, 15.000],  loss: 0.013086, mae: 0.673770, mean_q: 0.069293, mean_eps: 0.000000
 3812/10000: episode: 95, duration: 0.552s, episode steps:  56, steps per second: 101, episode reward: -30.000, mean reward: -0.536 [-30.240,  2.422], mean action: 5.804 [2.000, 20.000],  loss: 0.011532, mae: 0.697909, mean_q: 0.029918, mean_eps: 0.000000
 3845/10000: episode: 96, duration: 0.348s, episode steps:  33, steps per second:  95, episode reward: 37.443, mean reward:  1.135 [-2.598, 32.929], mean action: 7.061 [2.000, 18.000],  loss: 0.015634, mae: 0.734149, mean_q: 0.029967, mean_eps: 0.000000
 3874/10000: episode: 97, duration: 0.292s, episode steps:  29, steps per second:  99, episode reward: -32.310, mean reward: -1.114 [-31.676,  2.753], mean action: 6.966 [2.000, 20.000],  loss: 0.015113, mae: 0.720177, mean_q: 0.081055, mean_eps: 0.000000
 3941/10000: episode: 98, duration: 0.661s, episode steps:  67, steps per second: 101, episode reward: -35.280, mean reward: -0.527 [-32.047,  2.904], mean action: 4.597 [2.000, 18.000],  loss: 0.014916, mae: 0.764212, mean_q: 0.020801, mean_eps: 0.000000
 3978/10000: episode: 99, duration: 0.393s, episode steps:  37, steps per second:  94, episode reward: -35.320, mean reward: -0.955 [-31.441,  3.000], mean action: 8.081 [2.000, 18.000],  loss: 0.013053, mae: 0.770524, mean_q: -0.013195, mean_eps: 0.000000
 4007/10000: episode: 100, duration: 0.311s, episode steps:  29, steps per second:  93, episode reward: 35.111, mean reward:  1.211 [-2.673, 32.050], mean action: 7.483 [2.000, 15.000],  loss: 0.011551, mae: 0.728327, mean_q: 0.040586, mean_eps: 0.000000
 4035/10000: episode: 101, duration: 0.276s, episode steps:  28, steps per second: 102, episode reward: -41.130, mean reward: -1.469 [-31.677,  2.433], mean action: 11.714 [2.000, 20.000],  loss: 0.012784, mae: 0.763824, mean_q: 0.022323, mean_eps: 0.000000
 4063/10000: episode: 102, duration: 0.277s, episode steps:  28, steps per second: 101, episode reward: -44.660, mean reward: -1.595 [-32.212,  1.893], mean action: 7.500 [2.000, 15.000],  loss: 0.017443, mae: 0.783378, mean_q: 0.026650, mean_eps: 0.000000
 4099/10000: episode: 103, duration: 0.362s, episode steps:  36, steps per second:  99, episode reward: -32.580, mean reward: -0.905 [-32.116,  2.810], mean action: 8.167 [2.000, 16.000],  loss: 0.012155, mae: 0.758881, mean_q: 0.023716, mean_eps: 0.000000
 4143/10000: episode: 104, duration: 0.568s, episode steps:  44, steps per second:  77, episode reward: -35.800, mean reward: -0.814 [-32.503,  2.293], mean action: 7.023 [2.000, 19.000],  loss: 0.014007, mae: 0.753954, mean_q: 0.013643, mean_eps: 0.000000
 4186/10000: episode: 105, duration: 0.442s, episode steps:  43, steps per second:  97, episode reward: -38.780, mean reward: -0.902 [-32.085,  2.810], mean action: 7.930 [2.000, 18.000],  loss: 0.012148, mae: 0.734465, mean_q: 0.011359, mean_eps: 0.000000
 4219/10000: episode: 106, duration: 0.330s, episode steps:  33, steps per second: 100, episode reward: -44.660, mean reward: -1.353 [-31.809,  2.090], mean action: 9.455 [2.000, 15.000],  loss: 0.012903, mae: 0.706997, mean_q: 0.066424, mean_eps: 0.000000
 4251/10000: episode: 107, duration: 0.329s, episode steps:  32, steps per second:  97, episode reward: -36.000, mean reward: -1.125 [-32.006,  3.236], mean action: 6.219 [2.000, 15.000],  loss: 0.012353, mae: 0.696991, mean_q: 0.034523, mean_eps: 0.000000
 4329/10000: episode: 108, duration: 0.751s, episode steps:  78, steps per second: 104, episode reward: -39.000, mean reward: -0.500 [-32.278,  2.466], mean action: 11.128 [2.000, 18.000],  loss: 0.014503, mae: 0.727723, mean_q: 0.024836, mean_eps: 0.000000
 4365/10000: episode: 109, duration: 0.366s, episode steps:  36, steps per second:  98, episode reward: -35.910, mean reward: -0.997 [-32.157,  2.951], mean action: 12.556 [2.000, 16.000],  loss: 0.012521, mae: 0.720268, mean_q: 0.015177, mean_eps: 0.000000
 4403/10000: episode: 110, duration: 0.381s, episode steps:  38, steps per second: 100, episode reward: -36.000, mean reward: -0.947 [-32.060,  2.759], mean action: 8.947 [2.000, 16.000],  loss: 0.013719, mae: 0.782636, mean_q: -0.026044, mean_eps: 0.000000
 4434/10000: episode: 111, duration: 0.316s, episode steps:  31, steps per second:  98, episode reward: -38.260, mean reward: -1.234 [-31.981,  2.175], mean action: 9.452 [2.000, 20.000],  loss: 0.012499, mae: 0.753225, mean_q: 0.013944, mean_eps: 0.000000
 4469/10000: episode: 112, duration: 0.382s, episode steps:  35, steps per second:  92, episode reward: 37.839, mean reward:  1.081 [-3.000, 31.723], mean action: 4.743 [2.000, 20.000],  loss: 0.014640, mae: 0.823311, mean_q: -0.071713, mean_eps: 0.000000
 4499/10000: episode: 113, duration: 0.327s, episode steps:  30, steps per second:  92, episode reward: -42.000, mean reward: -1.400 [-32.367,  2.813], mean action: 7.367 [2.000, 20.000],  loss: 0.015584, mae: 0.743857, mean_q: 0.030262, mean_eps: 0.000000
 4522/10000: episode: 114, duration: 0.261s, episode steps:  23, steps per second:  88, episode reward: 35.643, mean reward:  1.550 [-2.432, 32.880], mean action: 8.348 [2.000, 16.000],  loss: 0.014187, mae: 0.789130, mean_q: -0.027674, mean_eps: 0.000000
 4546/10000: episode: 115, duration: 0.264s, episode steps:  24, steps per second:  91, episode reward: -38.630, mean reward: -1.610 [-32.081,  2.163], mean action: 12.417 [2.000, 16.000],  loss: 0.013551, mae: 0.755183, mean_q: 0.018361, mean_eps: 0.000000
 4586/10000: episode: 116, duration: 0.472s, episode steps:  40, steps per second:  85, episode reward: -38.160, mean reward: -0.954 [-31.698,  2.229], mean action: 4.050 [2.000, 15.000],  loss: 0.014231, mae: 0.787595, mean_q: -0.017652, mean_eps: 0.000000
 4601/10000: episode: 117, duration: 0.168s, episode steps:  15, steps per second:  89, episode reward: 38.701, mean reward:  2.580 [-3.000, 32.900], mean action: 3.800 [2.000, 13.000],  loss: 0.012082, mae: 0.750234, mean_q: -0.017443, mean_eps: 0.000000
 4658/10000: episode: 118, duration: 0.569s, episode steps:  57, steps per second: 100, episode reward: -39.000, mean reward: -0.684 [-32.067,  2.500], mean action: 7.386 [2.000, 16.000],  loss: 0.013271, mae: 0.718845, mean_q: 0.007389, mean_eps: 0.000000
 4719/10000: episode: 119, duration: 0.623s, episode steps:  61, steps per second:  98, episode reward: -35.000, mean reward: -0.574 [-30.436,  2.627], mean action: 4.000 [2.000, 20.000],  loss: 0.014712, mae: 0.779470, mean_q: 0.001708, mean_eps: 0.000000
 4800/10000: episode: 120, duration: 0.849s, episode steps:  81, steps per second:  95, episode reward: 34.265, mean reward:  0.423 [-3.000, 31.555], mean action: 4.802 [2.000, 20.000],  loss: 0.014305, mae: 0.789050, mean_q: -0.015051, mean_eps: 0.000000
 4856/10000: episode: 121, duration: 0.540s, episode steps:  56, steps per second: 104, episode reward: -32.710, mean reward: -0.584 [-32.900,  3.000], mean action: 4.393 [2.000, 18.000],  loss: 0.016343, mae: 0.790368, mean_q: -0.023327, mean_eps: 0.000000
 4882/10000: episode: 122, duration: 0.276s, episode steps:  26, steps per second:  94, episode reward: 32.012, mean reward:  1.231 [-3.000, 31.839], mean action: 9.769 [2.000, 18.000],  loss: 0.013583, mae: 0.754868, mean_q: 0.003004, mean_eps: 0.000000
 4914/10000: episode: 123, duration: 0.322s, episode steps:  32, steps per second: 100, episode reward: 32.509, mean reward:  1.016 [-3.000, 32.300], mean action: 7.688 [2.000, 20.000],  loss: 0.016781, mae: 0.819740, mean_q: -0.016607, mean_eps: 0.000000
 4945/10000: episode: 124, duration: 0.320s, episode steps:  31, steps per second:  97, episode reward: 32.456, mean reward:  1.047 [-3.000, 32.170], mean action: 8.677 [2.000, 20.000],  loss: 0.011116, mae: 0.782861, mean_q: 0.003322, mean_eps: 0.000000
 4976/10000: episode: 125, duration: 0.317s, episode steps:  31, steps per second:  98, episode reward: -35.520, mean reward: -1.146 [-32.136,  2.586], mean action: 7.645 [2.000, 18.000],  loss: 0.016419, mae: 0.842851, mean_q: -0.061312, mean_eps: 0.000000
 5013/10000: episode: 126, duration: 0.473s, episode steps:  37, steps per second:  78, episode reward: 32.032, mean reward:  0.866 [-2.279, 32.200], mean action: 7.027 [2.000, 16.000],  loss: 0.014547, mae: 0.793327, mean_q: 0.001016, mean_eps: 0.000000
 5039/10000: episode: 127, duration: 0.270s, episode steps:  26, steps per second:  96, episode reward: -38.860, mean reward: -1.495 [-32.101,  2.893], mean action: 10.000 [2.000, 15.000],  loss: 0.014740, mae: 0.756518, mean_q: 0.045227, mean_eps: 0.000000
 5071/10000: episode: 128, duration: 0.346s, episode steps:  32, steps per second:  93, episode reward: -38.980, mean reward: -1.218 [-32.016,  2.709], mean action: 11.469 [2.000, 18.000],  loss: 0.015291, mae: 0.758358, mean_q: 0.023648, mean_eps: 0.000000
 5098/10000: episode: 129, duration: 0.289s, episode steps:  27, steps per second:  93, episode reward: -35.230, mean reward: -1.305 [-32.279,  2.623], mean action: 9.074 [2.000, 16.000],  loss: 0.016953, mae: 0.738206, mean_q: 0.054627, mean_eps: 0.000000
 5138/10000: episode: 130, duration: 0.418s, episode steps:  40, steps per second:  96, episode reward: -35.670, mean reward: -0.892 [-31.926,  2.212], mean action: 5.900 [2.000, 15.000],  loss: 0.014736, mae: 0.794954, mean_q: -0.017958, mean_eps: 0.000000
 5161/10000: episode: 131, duration: 0.233s, episode steps:  23, steps per second:  99, episode reward: -38.070, mean reward: -1.655 [-32.227,  2.340], mean action: 7.043 [2.000, 15.000],  loss: 0.014988, mae: 0.787801, mean_q: 0.005830, mean_eps: 0.000000
 5211/10000: episode: 132, duration: 0.532s, episode steps:  50, steps per second:  94, episode reward: 35.683, mean reward:  0.714 [-2.360, 32.890], mean action: 3.940 [2.000, 15.000],  loss: 0.015382, mae: 0.763558, mean_q: 0.005435, mean_eps: 0.000000
 5243/10000: episode: 133, duration: 0.688s, episode steps:  32, steps per second:  46, episode reward: -38.210, mean reward: -1.194 [-32.014,  2.710], mean action: 5.375 [2.000, 16.000],  loss: 0.012996, mae: 0.782746, mean_q: -0.012416, mean_eps: 0.000000
 5293/10000: episode: 134, duration: 0.506s, episode steps:  50, steps per second:  99, episode reward: 33.000, mean reward:  0.660 [-3.000, 33.000], mean action: 4.120 [2.000, 15.000],  loss: 0.016024, mae: 0.804619, mean_q: -0.014526, mean_eps: 0.000000
 5330/10000: episode: 135, duration: 0.388s, episode steps:  37, steps per second:  95, episode reward: -36.000, mean reward: -0.973 [-32.077,  2.700], mean action: 5.838 [2.000, 15.000],  loss: 0.013995, mae: 0.795328, mean_q: 0.005953, mean_eps: 0.000000
 5359/10000: episode: 136, duration: 0.301s, episode steps:  29, steps per second:  96, episode reward: -32.840, mean reward: -1.132 [-32.207,  2.567], mean action: 8.241 [2.000, 16.000],  loss: 0.011552, mae: 0.728457, mean_q: 0.032239, mean_eps: 0.000000
 5382/10000: episode: 137, duration: 0.243s, episode steps:  23, steps per second:  95, episode reward: 32.174, mean reward:  1.399 [-3.000, 32.032], mean action: 11.217 [2.000, 18.000],  loss: 0.015894, mae: 0.746771, mean_q: 0.033497, mean_eps: 0.000000
 5438/10000: episode: 138, duration: 0.563s, episode steps:  56, steps per second: 100, episode reward: -39.000, mean reward: -0.696 [-32.350,  3.009], mean action: 5.125 [2.000, 18.000],  loss: 0.015273, mae: 0.780669, mean_q: -0.011872, mean_eps: 0.000000
 5471/10000: episode: 139, duration: 0.331s, episode steps:  33, steps per second: 100, episode reward: -35.680, mean reward: -1.081 [-32.225,  2.562], mean action: 10.333 [2.000, 20.000],  loss: 0.014205, mae: 0.747459, mean_q: 0.028943, mean_eps: 0.000000
 5503/10000: episode: 140, duration: 0.334s, episode steps:  32, steps per second:  96, episode reward: 32.363, mean reward:  1.011 [-2.498, 32.170], mean action: 9.750 [2.000, 16.000],  loss: 0.014104, mae: 0.772772, mean_q: 0.011628, mean_eps: 0.000000
 5522/10000: episode: 141, duration: 0.205s, episode steps:  19, steps per second:  93, episode reward: 42.945, mean reward:  2.260 [-2.367, 33.000], mean action: 8.158 [2.000, 15.000],  loss: 0.014844, mae: 0.772987, mean_q: 0.029697, mean_eps: 0.000000
 5553/10000: episode: 142, duration: 0.319s, episode steps:  31, steps per second:  97, episode reward: -35.580, mean reward: -1.148 [-32.133,  2.810], mean action: 11.129 [2.000, 16.000],  loss: 0.011492, mae: 0.734108, mean_q: 0.051235, mean_eps: 0.000000
 5569/10000: episode: 143, duration: 0.220s, episode steps:  16, steps per second:  73, episode reward: 38.349, mean reward:  2.397 [-2.712, 32.134], mean action: 5.688 [2.000, 16.000],  loss: 0.013286, mae: 0.736438, mean_q: 0.040222, mean_eps: 0.000000
 5599/10000: episode: 144, duration: 0.409s, episode steps:  30, steps per second:  73, episode reward: -36.000, mean reward: -1.200 [-32.337,  3.000], mean action: 11.367 [2.000, 20.000],  loss: 0.013901, mae: 0.748654, mean_q: 0.000439, mean_eps: 0.000000
 5645/10000: episode: 145, duration: 0.464s, episode steps:  46, steps per second:  99, episode reward: -35.570, mean reward: -0.773 [-31.928,  2.380], mean action: 6.109 [2.000, 18.000],  loss: 0.015651, mae: 0.785067, mean_q: -0.011640, mean_eps: 0.000000
 5684/10000: episode: 146, duration: 0.422s, episode steps:  39, steps per second:  93, episode reward: -32.150, mean reward: -0.824 [-31.546,  2.374], mean action: 5.154 [2.000, 15.000],  loss: 0.013776, mae: 0.774737, mean_q: -0.010083, mean_eps: 0.000000
 5707/10000: episode: 147, duration: 0.254s, episode steps:  23, steps per second:  90, episode reward: 35.636, mean reward:  1.549 [-3.000, 32.580], mean action: 7.174 [2.000, 18.000],  loss: 0.016450, mae: 0.793568, mean_q: 0.019125, mean_eps: 0.000000
 5751/10000: episode: 148, duration: 0.497s, episode steps:  44, steps per second:  88, episode reward: -47.570, mean reward: -1.081 [-32.020,  0.200], mean action: 5.955 [2.000, 15.000],  loss: 0.015848, mae: 0.749823, mean_q: 0.037903, mean_eps: 0.000000
 5784/10000: episode: 149, duration: 0.346s, episode steps:  33, steps per second:  95, episode reward: -44.470, mean reward: -1.348 [-33.000,  1.926], mean action: 7.030 [2.000, 15.000],  loss: 0.013750, mae: 0.766276, mean_q: 0.051068, mean_eps: 0.000000
 5816/10000: episode: 150, duration: 0.332s, episode steps:  32, steps per second:  96, episode reward: -38.010, mean reward: -1.188 [-31.810,  2.302], mean action: 7.812 [2.000, 20.000],  loss: 0.015013, mae: 0.770072, mean_q: 0.025745, mean_eps: 0.000000
 5842/10000: episode: 151, duration: 0.268s, episode steps:  26, steps per second:  97, episode reward: -35.910, mean reward: -1.381 [-32.056,  2.690], mean action: 5.731 [2.000, 15.000],  loss: 0.013382, mae: 0.792443, mean_q: 0.034683, mean_eps: 0.000000
 5860/10000: episode: 152, duration: 0.263s, episode steps:  18, steps per second:  68, episode reward: 39.000, mean reward:  2.167 [-2.555, 32.470], mean action: 7.389 [2.000, 18.000],  loss: 0.013746, mae: 0.786804, mean_q: 0.013576, mean_eps: 0.000000
 5878/10000: episode: 153, duration: 0.199s, episode steps:  18, steps per second:  90, episode reward: -41.160, mean reward: -2.287 [-32.377,  2.840], mean action: 4.000 [2.000, 18.000],  loss: 0.015508, mae: 0.833159, mean_q: -0.002284, mean_eps: 0.000000
 5908/10000: episode: 154, duration: 0.324s, episode steps:  30, steps per second:  93, episode reward: -44.420, mean reward: -1.481 [-31.826,  2.076], mean action: 10.367 [2.000, 18.000],  loss: 0.013580, mae: 0.801329, mean_q: 0.027313, mean_eps: 0.000000
 5923/10000: episode: 155, duration: 0.159s, episode steps:  15, steps per second:  95, episode reward: -47.330, mean reward: -3.155 [-33.000,  0.056], mean action: 11.000 [2.000, 15.000],  loss: 0.012347, mae: 0.780068, mean_q: 0.035724, mean_eps: 0.000000
 5963/10000: episode: 156, duration: 0.460s, episode steps:  40, steps per second:  87, episode reward: -32.490, mean reward: -0.812 [-32.141,  2.634], mean action: 6.075 [2.000, 16.000],  loss: 0.016675, mae: 0.792696, mean_q: 0.009227, mean_eps: 0.000000
 5991/10000: episode: 157, duration: 0.288s, episode steps:  28, steps per second:  97, episode reward: -38.960, mean reward: -1.391 [-32.084,  2.350], mean action: 10.964 [2.000, 16.000],  loss: 0.016918, mae: 0.799114, mean_q: 0.008189, mean_eps: 0.000000
 6028/10000: episode: 158, duration: 0.430s, episode steps:  37, steps per second:  86, episode reward: 40.812, mean reward:  1.103 [-2.463, 32.060], mean action: 8.703 [2.000, 16.000],  loss: 0.012083, mae: 0.800486, mean_q: -0.015347, mean_eps: 0.000000
 6084/10000: episode: 159, duration: 0.585s, episode steps:  56, steps per second:  96, episode reward: -44.490, mean reward: -0.794 [-32.187,  2.310], mean action: 4.500 [2.000, 16.000],  loss: 0.013784, mae: 0.796568, mean_q: 0.016175, mean_eps: 0.000000
 6109/10000: episode: 160, duration: 0.270s, episode steps:  25, steps per second:  93, episode reward: 32.027, mean reward:  1.281 [-3.000, 32.280], mean action: 8.400 [2.000, 20.000],  loss: 0.016417, mae: 0.787461, mean_q: 0.029286, mean_eps: 0.000000
 6157/10000: episode: 161, duration: 0.516s, episode steps:  48, steps per second:  93, episode reward: 34.909, mean reward:  0.727 [-2.901, 32.180], mean action: 7.396 [2.000, 16.000],  loss: 0.015400, mae: 0.779246, mean_q: 0.033814, mean_eps: 0.000000
 6185/10000: episode: 162, duration: 0.299s, episode steps:  28, steps per second:  94, episode reward: -38.740, mean reward: -1.384 [-32.163,  3.000], mean action: 10.393 [2.000, 15.000],  loss: 0.012313, mae: 0.765102, mean_q: 0.023277, mean_eps: 0.000000
 6215/10000: episode: 163, duration: 0.325s, episode steps:  30, steps per second:  92, episode reward: -38.960, mean reward: -1.299 [-32.104,  2.033], mean action: 9.300 [2.000, 15.000],  loss: 0.013640, mae: 0.808591, mean_q: -0.025068, mean_eps: 0.000000
 6251/10000: episode: 164, duration: 0.395s, episode steps:  36, steps per second:  91, episode reward: -32.590, mean reward: -0.905 [-32.893,  2.235], mean action: 8.444 [2.000, 18.000],  loss: 0.014580, mae: 0.810321, mean_q: -0.003097, mean_eps: 0.000000
 6295/10000: episode: 165, duration: 0.474s, episode steps:  44, steps per second:  93, episode reward: -32.090, mean reward: -0.729 [-32.339,  2.550], mean action: 4.295 [2.000, 18.000],  loss: 0.013554, mae: 0.775259, mean_q: 0.004183, mean_eps: 0.000000
 6318/10000: episode: 166, duration: 0.247s, episode steps:  23, steps per second:  93, episode reward: -33.000, mean reward: -1.435 [-32.177,  2.938], mean action: 11.217 [2.000, 16.000],  loss: 0.018119, mae: 0.830912, mean_q: 0.002260, mean_eps: 0.000000
 6349/10000: episode: 167, duration: 0.343s, episode steps:  31, steps per second:  90, episode reward: -32.420, mean reward: -1.046 [-31.785,  3.000], mean action: 7.645 [2.000, 20.000],  loss: 0.013636, mae: 0.740806, mean_q: 0.069768, mean_eps: 0.000000
 6408/10000: episode: 168, duration: 0.615s, episode steps:  59, steps per second:  96, episode reward: -32.890, mean reward: -0.557 [-32.023,  2.700], mean action: 5.186 [2.000, 18.000],  loss: 0.014026, mae: 0.756866, mean_q: 0.036328, mean_eps: 0.000000
 6441/10000: episode: 169, duration: 0.365s, episode steps:  33, steps per second:  90, episode reward: 41.596, mean reward:  1.260 [-2.093, 32.100], mean action: 5.273 [2.000, 16.000],  loss: 0.015319, mae: 0.806830, mean_q: 0.003503, mean_eps: 0.000000
 6482/10000: episode: 170, duration: 0.450s, episode steps:  41, steps per second:  91, episode reward: -32.470, mean reward: -0.792 [-32.111,  3.000], mean action: 6.366 [2.000, 16.000],  loss: 0.013689, mae: 0.828900, mean_q: -0.024550, mean_eps: 0.000000
 6510/10000: episode: 171, duration: 0.310s, episode steps:  28, steps per second:  90, episode reward: 34.998, mean reward:  1.250 [-2.595, 31.904], mean action: 6.536 [2.000, 16.000],  loss: 0.017034, mae: 0.831243, mean_q: -0.021752, mean_eps: 0.000000
 6535/10000: episode: 172, duration: 0.283s, episode steps:  25, steps per second:  88, episode reward: 34.498, mean reward:  1.380 [-2.442, 32.340], mean action: 3.680 [2.000, 13.000],  loss: 0.015734, mae: 0.804739, mean_q: 0.004711, mean_eps: 0.000000
 6579/10000: episode: 173, duration: 0.477s, episode steps:  44, steps per second:  92, episode reward: 37.985, mean reward:  0.863 [-3.000, 31.415], mean action: 3.477 [2.000, 16.000],  loss: 0.014990, mae: 0.817110, mean_q: -0.022985, mean_eps: 0.000000
 6600/10000: episode: 174, duration: 0.235s, episode steps:  21, steps per second:  89, episode reward: -38.460, mean reward: -1.831 [-32.093,  2.530], mean action: 9.905 [2.000, 20.000],  loss: 0.013618, mae: 0.782056, mean_q: -0.003649, mean_eps: 0.000000
 6645/10000: episode: 175, duration: 0.559s, episode steps:  45, steps per second:  81, episode reward: -35.180, mean reward: -0.782 [-31.698,  2.760], mean action: 6.889 [2.000, 15.000],  loss: 0.015834, mae: 0.799752, mean_q: 0.006385, mean_eps: 0.000000
 6678/10000: episode: 176, duration: 0.333s, episode steps:  33, steps per second:  99, episode reward: -44.240, mean reward: -1.341 [-31.902,  1.891], mean action: 7.697 [2.000, 15.000],  loss: 0.013271, mae: 0.790218, mean_q: -0.002058, mean_eps: 0.000000
 6701/10000: episode: 177, duration: 0.245s, episode steps:  23, steps per second:  94, episode reward: -38.160, mean reward: -1.659 [-31.282,  2.122], mean action: 9.739 [2.000, 16.000],  loss: 0.012983, mae: 0.770474, mean_q: 0.004955, mean_eps: 0.000000
 6726/10000: episode: 178, duration: 0.254s, episode steps:  25, steps per second:  98, episode reward: -38.500, mean reward: -1.540 [-32.438,  2.457], mean action: 12.200 [2.000, 20.000],  loss: 0.017486, mae: 0.794185, mean_q: 0.030874, mean_eps: 0.000000
 6752/10000: episode: 179, duration: 0.273s, episode steps:  26, steps per second:  95, episode reward: -33.000, mean reward: -1.269 [-32.193,  2.798], mean action: 9.538 [2.000, 18.000],  loss: 0.011217, mae: 0.786416, mean_q: 0.006498, mean_eps: 0.000000
 6794/10000: episode: 180, duration: 0.469s, episode steps:  42, steps per second:  89, episode reward: -38.180, mean reward: -0.909 [-32.090,  2.130], mean action: 6.476 [2.000, 18.000],  loss: 0.014631, mae: 0.816615, mean_q: 0.002478, mean_eps: 0.000000
 6835/10000: episode: 181, duration: 0.406s, episode steps:  41, steps per second: 101, episode reward: -35.430, mean reward: -0.864 [-31.941,  2.374], mean action: 8.000 [2.000, 18.000],  loss: 0.015466, mae: 0.781420, mean_q: 0.046756, mean_eps: 0.000000
 6867/10000: episode: 182, duration: 0.345s, episode steps:  32, steps per second:  93, episode reward: -35.260, mean reward: -1.102 [-32.738,  2.903], mean action: 6.938 [2.000, 16.000],  loss: 0.017654, mae: 0.839067, mean_q: -0.002928, mean_eps: 0.000000
 6890/10000: episode: 183, duration: 0.259s, episode steps:  23, steps per second:  89, episode reward: 32.657, mean reward:  1.420 [-3.000, 32.140], mean action: 7.652 [2.000, 20.000],  loss: 0.015453, mae: 0.826787, mean_q: 0.009911, mean_eps: 0.000000
 6927/10000: episode: 184, duration: 0.396s, episode steps:  37, steps per second:  94, episode reward: -35.550, mean reward: -0.961 [-32.037,  3.080], mean action: 6.919 [2.000, 20.000],  loss: 0.013416, mae: 0.765724, mean_q: 0.015396, mean_eps: 0.000000
 6969/10000: episode: 185, duration: 0.431s, episode steps:  42, steps per second:  97, episode reward: 34.733, mean reward:  0.827 [-2.231, 33.000], mean action: 5.619 [2.000, 16.000],  loss: 0.017593, mae: 0.785424, mean_q: 0.044921, mean_eps: 0.000000
 7022/10000: episode: 186, duration: 0.543s, episode steps:  53, steps per second:  98, episode reward: -32.930, mean reward: -0.621 [-32.193,  2.644], mean action: 5.302 [2.000, 15.000],  loss: 0.013881, mae: 0.772699, mean_q: 0.031180, mean_eps: 0.000000
 7037/10000: episode: 187, duration: 0.167s, episode steps:  15, steps per second:  90, episode reward: 41.926, mean reward:  2.795 [-2.308, 33.000], mean action: 4.267 [2.000, 10.000],  loss: 0.015051, mae: 0.813390, mean_q: -0.039547, mean_eps: 0.000000
 7122/10000: episode: 188, duration: 0.824s, episode steps:  85, steps per second: 103, episode reward: -38.370, mean reward: -0.451 [-32.312,  2.833], mean action: 4.800 [2.000, 18.000],  loss: 0.014369, mae: 0.801694, mean_q: -0.006978, mean_eps: 0.000000
 7143/10000: episode: 189, duration: 0.217s, episode steps:  21, steps per second:  97, episode reward: -42.000, mean reward: -2.000 [-30.430,  1.909], mean action: 14.000 [2.000, 20.000],  loss: 0.015177, mae: 0.759910, mean_q: 0.035973, mean_eps: 0.000000
 7187/10000: episode: 190, duration: 0.458s, episode steps:  44, steps per second:  96, episode reward: 32.506, mean reward:  0.739 [-2.366, 32.060], mean action: 5.545 [2.000, 18.000],  loss: 0.013172, mae: 0.774775, mean_q: 0.023074, mean_eps: 0.000000
 7218/10000: episode: 191, duration: 0.318s, episode steps:  31, steps per second:  97, episode reward: -33.000, mean reward: -1.065 [-32.022,  2.282], mean action: 6.806 [2.000, 16.000],  loss: 0.011645, mae: 0.763533, mean_q: 0.018519, mean_eps: 0.000000
 7256/10000: episode: 192, duration: 0.382s, episode steps:  38, steps per second: 100, episode reward: 37.208, mean reward:  0.979 [-2.425, 32.140], mean action: 7.500 [2.000, 18.000],  loss: 0.013051, mae: 0.767025, mean_q: 0.015200, mean_eps: 0.000000
 7299/10000: episode: 193, duration: 0.440s, episode steps:  43, steps per second:  98, episode reward: 35.531, mean reward:  0.826 [-2.228, 31.787], mean action: 4.674 [2.000, 18.000],  loss: 0.015535, mae: 0.774963, mean_q: -0.009461, mean_eps: 0.000000
 7338/10000: episode: 194, duration: 0.443s, episode steps:  39, steps per second:  88, episode reward: 38.378, mean reward:  0.984 [-2.144, 33.000], mean action: 7.974 [2.000, 18.000],  loss: 0.015836, mae: 0.790246, mean_q: -0.000495, mean_eps: 0.000000
 7384/10000: episode: 195, duration: 0.523s, episode steps:  46, steps per second:  88, episode reward: 32.459, mean reward:  0.706 [-3.000, 32.631], mean action: 5.022 [2.000, 15.000],  loss: 0.015320, mae: 0.752625, mean_q: 0.038023, mean_eps: 0.000000
 7407/10000: episode: 196, duration: 0.241s, episode steps:  23, steps per second:  96, episode reward: 34.820, mean reward:  1.514 [-3.000, 32.180], mean action: 4.652 [2.000, 15.000],  loss: 0.014946, mae: 0.757128, mean_q: 0.061638, mean_eps: 0.000000
 7426/10000: episode: 197, duration: 0.206s, episode steps:  19, steps per second:  92, episode reward: 37.933, mean reward:  1.996 [-2.357, 32.583], mean action: 7.053 [2.000, 15.000],  loss: 0.015428, mae: 0.754316, mean_q: 0.026027, mean_eps: 0.000000
 7447/10000: episode: 198, duration: 0.269s, episode steps:  21, steps per second:  78, episode reward: 36.000, mean reward:  1.714 [-2.903, 32.250], mean action: 5.476 [2.000, 16.000],  loss: 0.013965, mae: 0.788093, mean_q: 0.011455, mean_eps: 0.000000
 7492/10000: episode: 199, duration: 0.522s, episode steps:  45, steps per second:  86, episode reward: -33.000, mean reward: -0.733 [-32.060,  3.000], mean action: 9.711 [2.000, 15.000],  loss: 0.014062, mae: 0.810740, mean_q: -0.029586, mean_eps: 0.000000
 7524/10000: episode: 200, duration: 0.328s, episode steps:  32, steps per second:  98, episode reward: -38.300, mean reward: -1.197 [-32.094,  2.400], mean action: 6.750 [2.000, 15.000],  loss: 0.013778, mae: 0.747987, mean_q: 0.038681, mean_eps: 0.000000
 7552/10000: episode: 201, duration: 0.286s, episode steps:  28, steps per second:  98, episode reward: -35.910, mean reward: -1.282 [-31.936,  2.214], mean action: 9.214 [2.000, 16.000],  loss: 0.014731, mae: 0.730616, mean_q: 0.038422, mean_eps: 0.000000
 7576/10000: episode: 202, duration: 0.244s, episode steps:  24, steps per second:  98, episode reward: -38.670, mean reward: -1.611 [-32.403,  2.590], mean action: 11.667 [2.000, 16.000],  loss: 0.015060, mae: 0.739472, mean_q: 0.065687, mean_eps: 0.000000
 7612/10000: episode: 203, duration: 0.404s, episode steps:  36, steps per second:  89, episode reward: -39.000, mean reward: -1.083 [-32.353,  2.460], mean action: 8.500 [2.000, 16.000],  loss: 0.015474, mae: 0.810388, mean_q: -0.015089, mean_eps: 0.000000
 7635/10000: episode: 204, duration: 0.247s, episode steps:  23, steps per second:  93, episode reward: 38.041, mean reward:  1.654 [-2.320, 32.710], mean action: 5.565 [2.000, 16.000],  loss: 0.014614, mae: 0.853057, mean_q: -0.038622, mean_eps: 0.000000
 7657/10000: episode: 205, duration: 0.270s, episode steps:  22, steps per second:  82, episode reward: 40.029, mean reward:  1.819 [-2.260, 32.348], mean action: 7.318 [2.000, 18.000],  loss: 0.013611, mae: 0.793110, mean_q: 0.008189, mean_eps: 0.000000
 7687/10000: episode: 206, duration: 0.310s, episode steps:  30, steps per second:  97, episode reward: -32.230, mean reward: -1.074 [-31.845,  2.300], mean action: 9.867 [2.000, 18.000],  loss: 0.014803, mae: 0.773848, mean_q: 0.008770, mean_eps: 0.000000
 7730/10000: episode: 207, duration: 0.443s, episode steps:  43, steps per second:  97, episode reward: 35.806, mean reward:  0.833 [-3.000, 32.150], mean action: 5.233 [2.000, 20.000],  loss: 0.012700, mae: 0.744522, mean_q: 0.030487, mean_eps: 0.000000
 7755/10000: episode: 208, duration: 0.258s, episode steps:  25, steps per second:  97, episode reward: -39.000, mean reward: -1.560 [-32.804,  2.322], mean action: 13.560 [2.000, 16.000],  loss: 0.012921, mae: 0.708236, mean_q: 0.056458, mean_eps: 0.000000
 7798/10000: episode: 209, duration: 0.443s, episode steps:  43, steps per second:  97, episode reward: -44.400, mean reward: -1.033 [-32.050,  2.002], mean action: 13.535 [2.000, 20.000],  loss: 0.014485, mae: 0.736650, mean_q: 0.029722, mean_eps: 0.000000
 7858/10000: episode: 210, duration: 0.652s, episode steps:  60, steps per second:  92, episode reward: 35.004, mean reward:  0.583 [-2.590, 32.270], mean action: 3.700 [2.000, 18.000],  loss: 0.016370, mae: 0.756500, mean_q: 0.020065, mean_eps: 0.000000
 7878/10000: episode: 211, duration: 0.219s, episode steps:  20, steps per second:  91, episode reward: 37.891, mean reward:  1.895 [-2.472, 32.390], mean action: 7.900 [2.000, 16.000],  loss: 0.016030, mae: 0.754155, mean_q: 0.018318, mean_eps: 0.000000
 7918/10000: episode: 212, duration: 0.429s, episode steps:  40, steps per second:  93, episode reward: -32.490, mean reward: -0.812 [-31.808,  2.450], mean action: 5.000 [2.000, 16.000],  loss: 0.014045, mae: 0.745498, mean_q: 0.032365, mean_eps: 0.000000
 7945/10000: episode: 213, duration: 0.274s, episode steps:  27, steps per second:  98, episode reward: -44.160, mean reward: -1.636 [-32.015,  2.530], mean action: 11.556 [2.000, 18.000],  loss: 0.016140, mae: 0.776721, mean_q: -0.001126, mean_eps: 0.000000
 7966/10000: episode: 214, duration: 0.206s, episode steps:  21, steps per second: 102, episode reward: -44.640, mean reward: -2.126 [-33.000,  2.047], mean action: 10.143 [2.000, 20.000],  loss: 0.013567, mae: 0.707553, mean_q: 0.071952, mean_eps: 0.000000
 8000/10000: episode: 215, duration: 0.343s, episode steps:  34, steps per second:  99, episode reward: -38.250, mean reward: -1.125 [-32.466,  2.261], mean action: 5.941 [2.000, 15.000],  loss: 0.015294, mae: 0.796689, mean_q: -0.015335, mean_eps: 0.000000
 8034/10000: episode: 216, duration: 0.354s, episode steps:  34, steps per second:  96, episode reward: -32.840, mean reward: -0.966 [-32.368,  2.769], mean action: 9.412 [2.000, 16.000],  loss: 0.016083, mae: 0.734452, mean_q: 0.042662, mean_eps: 0.000000
 8069/10000: episode: 217, duration: 0.360s, episode steps:  35, steps per second:  97, episode reward: 32.304, mean reward:  0.923 [-3.000, 32.296], mean action: 4.257 [2.000, 18.000],  loss: 0.017135, mae: 0.765050, mean_q: 0.030539, mean_eps: 0.000000
 8099/10000: episode: 218, duration: 0.311s, episode steps:  30, steps per second:  96, episode reward: -32.560, mean reward: -1.085 [-31.877,  2.811], mean action: 7.100 [2.000, 18.000],  loss: 0.015474, mae: 0.820193, mean_q: -0.017017, mean_eps: 0.000000
 8122/10000: episode: 219, duration: 0.251s, episode steps:  23, steps per second:  92, episode reward: 38.157, mean reward:  1.659 [-2.245, 32.446], mean action: 3.391 [2.000, 18.000],  loss: 0.018058, mae: 0.792819, mean_q: -0.001319, mean_eps: 0.000000
 8154/10000: episode: 220, duration: 0.332s, episode steps:  32, steps per second:  96, episode reward: -32.110, mean reward: -1.003 [-32.133,  2.410], mean action: 7.594 [2.000, 15.000],  loss: 0.017270, mae: 0.747799, mean_q: 0.021294, mean_eps: 0.000000
 8193/10000: episode: 221, duration: 0.406s, episode steps:  39, steps per second:  96, episode reward: 39.000, mean reward:  1.000 [-2.310, 32.760], mean action: 3.154 [2.000, 18.000],  loss: 0.015585, mae: 0.772967, mean_q: 0.019170, mean_eps: 0.000000
 8217/10000: episode: 222, duration: 0.256s, episode steps:  24, steps per second:  94, episode reward: 33.000, mean reward:  1.375 [-3.000, 29.939], mean action: 5.583 [2.000, 18.000],  loss: 0.014852, mae: 0.747223, mean_q: 0.054770, mean_eps: 0.000000
 8243/10000: episode: 223, duration: 0.262s, episode steps:  26, steps per second:  99, episode reward: -39.000, mean reward: -1.500 [-32.323,  2.670], mean action: 9.385 [2.000, 18.000],  loss: 0.011627, mae: 0.737808, mean_q: 0.033586, mean_eps: 0.000000
 8263/10000: episode: 224, duration: 0.325s, episode steps:  20, steps per second:  62, episode reward: -32.650, mean reward: -1.632 [-32.201,  2.903], mean action: 6.950 [2.000, 16.000],  loss: 0.013680, mae: 0.702386, mean_q: 0.056264, mean_eps: 0.000000
 8341/10000: episode: 225, duration: 0.793s, episode steps:  78, steps per second:  98, episode reward: 38.335, mean reward:  0.491 [-2.416, 32.425], mean action: 3.821 [2.000, 20.000],  loss: 0.014702, mae: 0.728970, mean_q: 0.058148, mean_eps: 0.000000
 8365/10000: episode: 226, duration: 0.251s, episode steps:  24, steps per second:  96, episode reward: -38.200, mean reward: -1.592 [-32.083,  2.530], mean action: 9.958 [2.000, 20.000],  loss: 0.014902, mae: 0.758121, mean_q: 0.021568, mean_eps: 0.000000
 8401/10000: episode: 227, duration: 0.420s, episode steps:  36, steps per second:  86, episode reward: 32.675, mean reward:  0.908 [-2.279, 32.604], mean action: 9.333 [2.000, 20.000],  loss: 0.015016, mae: 0.758308, mean_q: 0.047454, mean_eps: 0.000000
 8427/10000: episode: 228, duration: 0.284s, episode steps:  26, steps per second:  91, episode reward: 34.347, mean reward:  1.321 [-2.747, 32.128], mean action: 5.577 [2.000, 10.000],  loss: 0.017288, mae: 0.760596, mean_q: 0.052326, mean_eps: 0.000000
 8471/10000: episode: 229, duration: 0.446s, episode steps:  44, steps per second:  99, episode reward: 32.539, mean reward:  0.740 [-2.521, 32.150], mean action: 10.568 [2.000, 16.000],  loss: 0.012030, mae: 0.719096, mean_q: 0.067840, mean_eps: 0.000000
 8563/10000: episode: 230, duration: 0.911s, episode steps:  92, steps per second: 101, episode reward: 38.195, mean reward:  0.415 [-2.904, 32.054], mean action: 3.076 [2.000, 18.000],  loss: 0.014422, mae: 0.805995, mean_q: 0.003319, mean_eps: 0.000000
 8600/10000: episode: 231, duration: 0.372s, episode steps:  37, steps per second:  99, episode reward: -39.000, mean reward: -1.054 [-32.045,  2.020], mean action: 8.027 [2.000, 20.000],  loss: 0.012097, mae: 0.779555, mean_q: 0.020006, mean_eps: 0.000000
 8653/10000: episode: 232, duration: 0.540s, episode steps:  53, steps per second:  98, episode reward: -36.000, mean reward: -0.679 [-32.704,  2.180], mean action: 3.717 [2.000, 15.000],  loss: 0.014002, mae: 0.787863, mean_q: 0.024424, mean_eps: 0.000000
 8693/10000: episode: 233, duration: 0.409s, episode steps:  40, steps per second:  98, episode reward: -32.280, mean reward: -0.807 [-31.629,  2.798], mean action: 6.900 [2.000, 20.000],  loss: 0.013034, mae: 0.752466, mean_q: 0.049788, mean_eps: 0.000000
 8713/10000: episode: 234, duration: 0.205s, episode steps:  20, steps per second:  97, episode reward: -47.300, mean reward: -2.365 [-33.000,  0.305], mean action: 7.900 [2.000, 15.000],  loss: 0.014455, mae: 0.799772, mean_q: 0.020449, mean_eps: 0.000000
 8764/10000: episode: 235, duration: 0.516s, episode steps:  51, steps per second:  99, episode reward: -35.850, mean reward: -0.703 [-32.476,  2.424], mean action: 7.863 [2.000, 15.000],  loss: 0.013839, mae: 0.743498, mean_q: 0.065744, mean_eps: 0.000000
 8785/10000: episode: 236, duration: 0.228s, episode steps:  21, steps per second:  92, episode reward: -36.000, mean reward: -1.714 [-32.061,  3.000], mean action: 9.905 [2.000, 18.000],  loss: 0.015054, mae: 0.737937, mean_q: 0.076888, mean_eps: 0.000000
 8848/10000: episode: 237, duration: 0.639s, episode steps:  63, steps per second:  99, episode reward: 33.000, mean reward:  0.524 [-2.205, 32.030], mean action: 6.333 [2.000, 20.000],  loss: 0.016124, mae: 0.766575, mean_q: 0.038902, mean_eps: 0.000000
 8877/10000: episode: 238, duration: 0.301s, episode steps:  29, steps per second:  97, episode reward: -36.000, mean reward: -1.241 [-32.718,  2.321], mean action: 9.069 [2.000, 20.000],  loss: 0.014135, mae: 0.733257, mean_q: 0.053422, mean_eps: 0.000000
 8916/10000: episode: 239, duration: 0.407s, episode steps:  39, steps per second:  96, episode reward: 32.108, mean reward:  0.823 [-2.344, 32.320], mean action: 7.282 [2.000, 20.000],  loss: 0.014228, mae: 0.748771, mean_q: 0.047046, mean_eps: 0.000000
 8942/10000: episode: 240, duration: 0.268s, episode steps:  26, steps per second:  97, episode reward: -38.620, mean reward: -1.485 [-32.119,  2.417], mean action: 5.615 [2.000, 20.000],  loss: 0.016128, mae: 0.768483, mean_q: 0.038021, mean_eps: 0.000000
 8962/10000: episode: 241, duration: 0.217s, episode steps:  20, steps per second:  92, episode reward: 35.543, mean reward:  1.777 [-3.000, 30.826], mean action: 6.350 [2.000, 20.000],  loss: 0.017147, mae: 0.737413, mean_q: 0.076277, mean_eps: 0.000000
 8993/10000: episode: 242, duration: 0.339s, episode steps:  31, steps per second:  91, episode reward: -32.200, mean reward: -1.039 [-31.861,  2.360], mean action: 5.258 [2.000, 15.000],  loss: 0.017096, mae: 0.784682, mean_q: 0.006261, mean_eps: 0.000000
 9037/10000: episode: 243, duration: 0.460s, episode steps:  44, steps per second:  96, episode reward: 37.561, mean reward:  0.854 [-2.418, 31.930], mean action: 3.750 [2.000, 20.000],  loss: 0.015105, mae: 0.745817, mean_q: 0.062441, mean_eps: 0.000000
 9082/10000: episode: 244, duration: 0.594s, episode steps:  45, steps per second:  76, episode reward: -38.770, mean reward: -0.862 [-32.262,  2.367], mean action: 7.556 [2.000, 15.000],  loss: 0.014630, mae: 0.792828, mean_q: 0.013898, mean_eps: 0.000000
 9116/10000: episode: 245, duration: 0.354s, episode steps:  34, steps per second:  96, episode reward: 32.309, mean reward:  0.950 [-3.000, 33.000], mean action: 5.500 [2.000, 18.000],  loss: 0.015335, mae: 0.784552, mean_q: 0.009898, mean_eps: 0.000000
 9152/10000: episode: 246, duration: 0.372s, episode steps:  36, steps per second:  97, episode reward: 38.357, mean reward:  1.065 [-2.550, 32.214], mean action: 4.889 [2.000, 16.000],  loss: 0.014430, mae: 0.745392, mean_q: 0.080966, mean_eps: 0.000000
 9180/10000: episode: 247, duration: 0.293s, episode steps:  28, steps per second:  95, episode reward: -35.600, mean reward: -1.271 [-32.690,  2.941], mean action: 4.393 [2.000, 15.000],  loss: 0.016532, mae: 0.777066, mean_q: 0.048183, mean_eps: 0.000000
 9210/10000: episode: 248, duration: 0.302s, episode steps:  30, steps per second:  99, episode reward: -44.210, mean reward: -1.474 [-32.425,  2.088], mean action: 7.967 [2.000, 15.000],  loss: 0.017003, mae: 0.780784, mean_q: 0.055425, mean_eps: 0.000000
 9238/10000: episode: 249, duration: 0.284s, episode steps:  28, steps per second:  99, episode reward: -42.000, mean reward: -1.500 [-32.197,  2.010], mean action: 10.714 [2.000, 20.000],  loss: 0.012522, mae: 0.752193, mean_q: 0.037100, mean_eps: 0.000000
 9249/10000: episode: 250, duration: 0.128s, episode steps:  11, steps per second:  86, episode reward: 44.185, mean reward:  4.017 [-2.364, 32.606], mean action: 6.909 [2.000, 20.000],  loss: 0.011221, mae: 0.756513, mean_q: 0.007428, mean_eps: 0.000000
 9298/10000: episode: 251, duration: 0.490s, episode steps:  49, steps per second: 100, episode reward: 32.763, mean reward:  0.669 [-2.778, 32.212], mean action: 7.612 [2.000, 20.000],  loss: 0.015184, mae: 0.798690, mean_q: 0.016465, mean_eps: 0.000000
 9337/10000: episode: 252, duration: 0.398s, episode steps:  39, steps per second:  98, episode reward: 35.548, mean reward:  0.911 [-3.000, 32.180], mean action: 5.256 [2.000, 16.000],  loss: 0.015820, mae: 0.763293, mean_q: 0.039699, mean_eps: 0.000000
 9368/10000: episode: 253, duration: 0.404s, episode steps:  31, steps per second:  77, episode reward: -36.000, mean reward: -1.161 [-32.176,  2.637], mean action: 8.742 [2.000, 18.000],  loss: 0.015349, mae: 0.760825, mean_q: 0.059127, mean_eps: 0.000000
 9421/10000: episode: 254, duration: 0.538s, episode steps:  53, steps per second:  98, episode reward: 33.000, mean reward:  0.623 [-2.338, 32.060], mean action: 11.132 [2.000, 20.000],  loss: 0.014325, mae: 0.777841, mean_q: 0.033276, mean_eps: 0.000000
 9459/10000: episode: 255, duration: 0.385s, episode steps:  38, steps per second:  99, episode reward: 34.540, mean reward:  0.909 [-2.331, 32.470], mean action: 7.158 [2.000, 20.000],  loss: 0.014362, mae: 0.774743, mean_q: 0.044261, mean_eps: 0.000000
 9528/10000: episode: 256, duration: 0.697s, episode steps:  69, steps per second:  99, episode reward: -32.070, mean reward: -0.465 [-32.125,  3.000], mean action: 13.942 [2.000, 20.000],  loss: 0.012677, mae: 0.771782, mean_q: 0.035033, mean_eps: 0.000000
 9564/10000: episode: 257, duration: 0.370s, episode steps:  36, steps per second:  97, episode reward: 38.706, mean reward:  1.075 [-2.584, 32.146], mean action: 3.583 [2.000, 20.000],  loss: 0.015768, mae: 0.793475, mean_q: 0.015120, mean_eps: 0.000000
 9596/10000: episode: 258, duration: 0.324s, episode steps:  32, steps per second:  99, episode reward: -35.650, mean reward: -1.114 [-32.104,  2.380], mean action: 7.938 [2.000, 16.000],  loss: 0.014212, mae: 0.743906, mean_q: 0.060693, mean_eps: 0.000000
 9627/10000: episode: 259, duration: 0.335s, episode steps:  31, steps per second:  92, episode reward: 32.490, mean reward:  1.048 [-2.406, 32.300], mean action: 7.065 [2.000, 15.000],  loss: 0.011789, mae: 0.717762, mean_q: 0.082939, mean_eps: 0.000000
 9650/10000: episode: 260, duration: 0.243s, episode steps:  23, steps per second:  95, episode reward: 38.478, mean reward:  1.673 [-2.400, 32.904], mean action: 3.913 [2.000, 15.000],  loss: 0.014503, mae: 0.779939, mean_q: 0.021207, mean_eps: 0.000000
 9671/10000: episode: 261, duration: 0.217s, episode steps:  21, steps per second:  97, episode reward: -39.000, mean reward: -1.857 [-29.438,  2.150], mean action: 12.286 [2.000, 15.000],  loss: 0.015802, mae: 0.789765, mean_q: 0.017787, mean_eps: 0.000000
 9704/10000: episode: 262, duration: 0.343s, episode steps:  33, steps per second:  96, episode reward: -32.440, mean reward: -0.983 [-32.112,  2.909], mean action: 9.242 [2.000, 16.000],  loss: 0.014744, mae: 0.733006, mean_q: 0.067956, mean_eps: 0.000000
 9774/10000: episode: 263, duration: 0.724s, episode steps:  70, steps per second:  97, episode reward: -35.090, mean reward: -0.501 [-31.940,  2.563], mean action: 3.457 [2.000, 15.000],  loss: 0.014479, mae: 0.746041, mean_q: 0.063156, mean_eps: 0.000000
 9797/10000: episode: 264, duration: 0.239s, episode steps:  23, steps per second:  96, episode reward: -35.610, mean reward: -1.548 [-31.812,  2.910], mean action: 8.478 [2.000, 20.000],  loss: 0.014256, mae: 0.757064, mean_q: 0.030218, mean_eps: 0.000000
 9832/10000: episode: 265, duration: 0.365s, episode steps:  35, steps per second:  96, episode reward: -37.030, mean reward: -1.058 [-31.549,  2.591], mean action: 6.571 [2.000, 16.000],  loss: 0.013719, mae: 0.749296, mean_q: 0.059331, mean_eps: 0.000000
 9867/10000: episode: 266, duration: 0.362s, episode steps:  35, steps per second:  97, episode reward: 39.000, mean reward:  1.114 [-2.342, 32.360], mean action: 4.629 [2.000, 20.000],  loss: 0.014111, mae: 0.738413, mean_q: 0.056250, mean_eps: 0.000000
 9902/10000: episode: 267, duration: 0.367s, episode steps:  35, steps per second:  95, episode reward: 35.234, mean reward:  1.007 [-2.292, 31.845], mean action: 8.943 [2.000, 18.000],  loss: 0.014033, mae: 0.739434, mean_q: 0.057712, mean_eps: 0.000000
 9938/10000: episode: 268, duration: 0.370s, episode steps:  36, steps per second:  97, episode reward: 32.443, mean reward:  0.901 [-2.615, 32.405], mean action: 6.028 [2.000, 18.000],  loss: 0.014857, mae: 0.765288, mean_q: 0.024460, mean_eps: 0.000000
 9969/10000: episode: 269, duration: 0.314s, episode steps:  31, steps per second:  99, episode reward: -41.500, mean reward: -1.339 [-32.277,  1.924], mean action: 9.516 [2.000, 18.000],  loss: 0.015517, mae: 0.768067, mean_q: 0.031245, mean_eps: 0.000000
 9999/10000: episode: 270, duration: 0.299s, episode steps:  30, steps per second: 100, episode reward: -39.000, mean reward: -1.300 [-32.037,  3.000], mean action: 7.800 [2.000, 14.000],  loss: 0.015673, mae: 0.770012, mean_q: 0.004367, mean_eps: 0.000000
done, took 105.497 seconds
Results against random player:
DQN Evaluation: 209 victories out of 300 episodes

Results against max player:
DQN Evaluation: 83 victories out of 300 episodes
